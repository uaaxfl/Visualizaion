2020.aacl-main.22,C18-1139,0,0.0459533,"Missing"
2020.aacl-main.22,N19-1423,0,0.110489,"eam tasks, such as entity linking (Luo et al., 2015), relation extraction (Feldman and Rosenfeld, 2006) and question answering (Lee et al., 2006). NER is usually modeled as a sentence-level sequence labeling task in previous work. For example, Lample et al. (2016) used long-short term memory (LSTM) (Gers et al., 2000) for capturing contextual word representations and conditional random ﬁeid (CRF) (Lafferty et al., 2001) for jointly label decoding. In recent years, language models (LMs) were introduced to this task to learn better contextual representations of words (Peters et al., 2017, 2018; Devlin et al., 2019). However, these methods only consider the contexts within a sentence, which is insufﬁcient. Our work is motivated by the observation that the contextual information beyond sentences can mitigate the negative effects of the ambiguous and limited sentence contexts. The sentences within a document are highly related, and the interactions between them can provide document-level contextual information. For example, in Figure 1, sentence 1 is ambiguous because it can be either his mother called Elizabeth Trump or a business called Elizabeth Trump and Son. But another sentence in this document expli"
2020.aacl-main.22,W06-1656,0,0.0506255,"es of document- and word-level contextual evidence. Blue italic and red underlined entities are the names of organizations and persons respectively. Green and orange arrows indicate the document- and word-level contextual evidence respectively. Introduction Named Entity Recognition (NER) is deﬁned as automatically identifying and classifying named entities into speciﬁc categories (e.g., person, location, organization) in text. It is a critical task in Natural Language Processing (NLP) and a prerequisite for many downstream tasks, such as entity linking (Luo et al., 2015), relation extraction (Feldman and Rosenfeld, 2006) and question answering (Lee et al., 2006). NER is usually modeled as a sentence-level sequence labeling task in previous work. For example, Lample et al. (2016) used long-short term memory (LSTM) (Gers et al., 2000) for capturing contextual word representations and conditional random ﬁeid (CRF) (Lafferty et al., 2001) for jointly label decoding. In recent years, language models (LMs) were introduced to this task to learn better contextual representations of words (Peters et al., 2017, 2018; Devlin et al., 2019). However, these methods only consider the contexts within a sentence, which is ins"
2020.aacl-main.22,N16-1030,0,0.493173,"ows indicate the document- and word-level contextual evidence respectively. Introduction Named Entity Recognition (NER) is deﬁned as automatically identifying and classifying named entities into speciﬁc categories (e.g., person, location, organization) in text. It is a critical task in Natural Language Processing (NLP) and a prerequisite for many downstream tasks, such as entity linking (Luo et al., 2015), relation extraction (Feldman and Rosenfeld, 2006) and question answering (Lee et al., 2006). NER is usually modeled as a sentence-level sequence labeling task in previous work. For example, Lample et al. (2016) used long-short term memory (LSTM) (Gers et al., 2000) for capturing contextual word representations and conditional random ﬁeid (CRF) (Lafferty et al., 2001) for jointly label decoding. In recent years, language models (LMs) were introduced to this task to learn better contextual representations of words (Peters et al., 2017, 2018; Devlin et al., 2019). However, these methods only consider the contexts within a sentence, which is insufﬁcient. Our work is motivated by the observation that the contextual information beyond sentences can mitigate the negative effects of the ambiguous and limite"
2020.aacl-main.22,K18-2005,0,0.0239259,"Missing"
2020.aacl-main.22,Q16-1026,0,0.0213156,"example, Passos et al. (2014) trained phrase vectors in their lexicon-infused skip-gram model. Lin and Wu (2009) used a linear chain CRF and added phrase cluster features extracted from the web data. However, these methods require heavy feature engineering, which necessities massive domain knowledge. In addition, these methods cannot make full use of contextual information within texts. In recent years, many neural networks were applied to the NER task. Collobert et al. (2011) ﬁrst adopted CNNs to learn word representations. Recently, BiLSTM was widely used for long distance context modeling (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Additionally, Chiu and Nichols (2016) employed CNNs to capture morphological word representations; Lample et al. (2016) utilized CRF to model the dependencies between adjacent tags; Ma and Hovy (2016) proposed LSTM-CNNs-CRF model to combine the strengths of these components. Besides, Strubell et al. (2017) proposed iterated-dilated CNNs for higher efﬁciency than BiLSTM and better capacity with large context than vanilla CNNs. Recent work proved that the context-sensitive representations captured by language models are useful in NER systems. Peters et"
2020.aacl-main.22,P09-1116,0,0.0109506,"t by capturing the interactions between sentences within a document with the multi-head self attention mechanism. • We propose to mine the word-level context with an auxiliary word classiﬁcation task to learn the words’ preferences of entity type and relative position from the entities. • We conduct experiments on several benchmark datasets, and the results validate the effectiveness of our method. 2 Related Work In traditional NER methods, contexts are usually modeled via hand-crafted features. For example, Passos et al. (2014) trained phrase vectors in their lexicon-infused skip-gram model. Lin and Wu (2009) used a linear chain CRF and added phrase cluster features extracted from the web data. However, these methods require heavy feature engineering, which necessities massive domain knowledge. In addition, these methods cannot make full use of contextual information within texts. In recent years, many neural networks were applied to the NER task. Collobert et al. (2011) ﬁrst adopted CNNs to learn word representations. Recently, BiLSTM was widely used for long distance context modeling (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Additionally, Chiu and Nichols (2016) employed"
2020.aacl-main.22,P19-1524,0,0.0244358,"Missing"
2020.aacl-main.22,D15-1104,0,0.0210484,"g officer. × Sentence 2 Figure 1: Examples of document- and word-level contextual evidence. Blue italic and red underlined entities are the names of organizations and persons respectively. Green and orange arrows indicate the document- and word-level contextual evidence respectively. Introduction Named Entity Recognition (NER) is deﬁned as automatically identifying and classifying named entities into speciﬁc categories (e.g., person, location, organization) in text. It is a critical task in Natural Language Processing (NLP) and a prerequisite for many downstream tasks, such as entity linking (Luo et al., 2015), relation extraction (Feldman and Rosenfeld, 2006) and question answering (Lee et al., 2006). NER is usually modeled as a sentence-level sequence labeling task in previous work. For example, Lample et al. (2016) used long-short term memory (LSTM) (Gers et al., 2000) for capturing contextual word representations and conditional random ﬁeid (CRF) (Lafferty et al., 2001) for jointly label decoding. In recent years, language models (LMs) were introduced to this task to learn better contextual representations of words (Peters et al., 2017, 2018; Devlin et al., 2019). However, these methods only co"
2020.aacl-main.22,W09-1119,0,0.0311585,"f all labels denoted as |C|). During training, we use plabel to compute k the loss function for word classiﬁcation, which is formulated as cross-entropy loss: n  (11) where λ is the weight of word classiﬁcation loss. (8) where pk is obtained by looking up a randomlyinitialized embedding matrix and tuned during training. Then xk is fed into the two-layer classiﬁer to predict label distribution: tanh(Wc1 xk L(θ) = LCRF (θ) + λLW C (θ), 4.2 Experimental Settings In our experiments, we use the BIOES labeling scheme for output tags, which was proven to outperform other options in previous work (Ratinov and Roth, 2009). Under this tagging scheme, the number of labels |C |= 17 ([B,I,E,S]× 1 The CoNLL-2002 dataset contains Dutch and Spanish data. But the Spanish data lacks the marks of doucument boundaries. Thus we only conduct experiments on the Dutch data. (10) k=1 185 Hyper-parameter Word embedding dim. (dwe ) Character embedding dim. (dce ) Position embedding dim. (dpe ) Character hidden state dim. (dch ) Word hidden state dim. (dwh ) Sentence hidden state dim. (dsh ) Sequence hidden state dim. (dsqh ) Neural attention subspace dim. (dna ) Self attention subspace dim. (dsa ) Label classiﬁer hidden dim. (d"
2020.aacl-main.22,P16-1101,0,0.0216239,"vectors in their lexicon-infused skip-gram model. Lin and Wu (2009) used a linear chain CRF and added phrase cluster features extracted from the web data. However, these methods require heavy feature engineering, which necessities massive domain knowledge. In addition, these methods cannot make full use of contextual information within texts. In recent years, many neural networks were applied to the NER task. Collobert et al. (2011) ﬁrst adopted CNNs to learn word representations. Recently, BiLSTM was widely used for long distance context modeling (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Additionally, Chiu and Nichols (2016) employed CNNs to capture morphological word representations; Lample et al. (2016) utilized CRF to model the dependencies between adjacent tags; Ma and Hovy (2016) proposed LSTM-CNNs-CRF model to combine the strengths of these components. Besides, Strubell et al. (2017) proposed iterated-dilated CNNs for higher efﬁciency than BiLSTM and better capacity with large context than vanilla CNNs. Recent work proved that the context-sensitive representations captured by language models are useful in NER systems. Peters et al. (2017) proposed TagLM model and intro"
2020.aacl-main.22,P18-2039,0,0.011775,". . . , win , and feed the sentence into TagLM’s bottom BiGRU to compute [hword , hword , . . . , hword i0 i1 in ]. Next we compute the document representation di and replace hword with i0 it (requires dwh = dsh ). Then we feed them into 184 the top BiGRU. The input of the top BiGRU contains document- and sentence-level contextual representations simultaneously. Thus its output hidden states act as the fusion of the two contexts. 3.3 To incorporate the word-level context, we conwith the original CRF input hseq catenate plabel ik ik to enrich word representations with the label distributions (Seyler et al., 2018). The CRF takes the enhanced word representations as input and decodes the best label sequence. Our framework is jointly trained on the original NER and the auxiliary classiﬁcation task via multi-task learning: Word-level Context In natural language, words themselves have different preferences on different entity types and relative positions from the entities. These preferences provide word-level contextual information for the NER task. For example, in the sentence “With only one match before New Year, Real will spend Christmas ahead of others”, the type of the entity Real is uncertain because"
2020.aacl-main.22,E99-1001,0,0.0674449,"that the context-sensitive representations captured by language models are useful in NER systems. Peters et al. (2017) proposed TagLM model and introduced LM embeddings in this task. Afterwards, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) were proposed for better contextual representations. However, these methods focused only on the context within a sentence, so their performance is substantially hurt by the ambiguity and limitation of sentence context. To combine contexts beyond sentences, several methods were proposed to mine document-level information, such as logical rules (Mikheev et al., 1999), global attention (Xu et al., 2018; Zhang et al., 2018; Hu et al., 2020) and memory mechanisms (Gui et al., 2020). But these methods ignored the sequential characteristics of the sentences within a document, which may be sub-optimal. We observe that contextual associations between sentences in a document have the potential of improving the NER performance. Moreover, the words’ preferences of entity type and relative position from the entities 182 O O … O O S O O O S O S O O S O tanh 2-layer Neural Network … Character embedding Word embedding Character representation (ு) ு ߙଵଵ … ߙଵ … … … ଵ …"
2020.aacl-main.22,W14-1609,0,0.0177081,"he NER task with a uniﬁed framework. • We propose to exploit the document-level context by capturing the interactions between sentences within a document with the multi-head self attention mechanism. • We propose to mine the word-level context with an auxiliary word classiﬁcation task to learn the words’ preferences of entity type and relative position from the entities. • We conduct experiments on several benchmark datasets, and the results validate the effectiveness of our method. 2 Related Work In traditional NER methods, contexts are usually modeled via hand-crafted features. For example, Passos et al. (2014) trained phrase vectors in their lexicon-infused skip-gram model. Lin and Wu (2009) used a linear chain CRF and added phrase cluster features extracted from the web data. However, these methods require heavy feature engineering, which necessities massive domain knowledge. In addition, these methods cannot make full use of contextual information within texts. In recent years, many neural networks were applied to the NER task. Collobert et al. (2011) ﬁrst adopted CNNs to learn word representations. Recently, BiLSTM was widely used for long distance context modeling (Chiu and Nichols, 2016; Lampl"
2020.aacl-main.22,P17-1161,0,0.35782,"erequisite for many downstream tasks, such as entity linking (Luo et al., 2015), relation extraction (Feldman and Rosenfeld, 2006) and question answering (Lee et al., 2006). NER is usually modeled as a sentence-level sequence labeling task in previous work. For example, Lample et al. (2016) used long-short term memory (LSTM) (Gers et al., 2000) for capturing contextual word representations and conditional random ﬁeid (CRF) (Lafferty et al., 2001) for jointly label decoding. In recent years, language models (LMs) were introduced to this task to learn better contextual representations of words (Peters et al., 2017, 2018; Devlin et al., 2019). However, these methods only consider the contexts within a sentence, which is insufﬁcient. Our work is motivated by the observation that the contextual information beyond sentences can mitigate the negative effects of the ambiguous and limited sentence contexts. The sentences within a document are highly related, and the interactions between them can provide document-level contextual information. For example, in Figure 1, sentence 1 is ambiguous because it can be either his mother called Elizabeth Trump or a business called Elizabeth Trump and Son. But another sen"
2020.aacl-main.22,N18-1202,0,0.0490263,"s to capture morphological word representations; Lample et al. (2016) utilized CRF to model the dependencies between adjacent tags; Ma and Hovy (2016) proposed LSTM-CNNs-CRF model to combine the strengths of these components. Besides, Strubell et al. (2017) proposed iterated-dilated CNNs for higher efﬁciency than BiLSTM and better capacity with large context than vanilla CNNs. Recent work proved that the context-sensitive representations captured by language models are useful in NER systems. Peters et al. (2017) proposed TagLM model and introduced LM embeddings in this task. Afterwards, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) were proposed for better contextual representations. However, these methods focused only on the context within a sentence, so their performance is substantially hurt by the ambiguity and limitation of sentence context. To combine contexts beyond sentences, several methods were proposed to mine document-level information, such as logical rules (Mikheev et al., 1999), global attention (Xu et al., 2018; Zhang et al., 2018; Hu et al., 2020) and memory mechanisms (Gui et al., 2020). But these methods ignored the sequential characteristics of the sentences within a do"
2020.aacl-main.22,D17-1283,0,0.011877,"of contextual information within texts. In recent years, many neural networks were applied to the NER task. Collobert et al. (2011) ﬁrst adopted CNNs to learn word representations. Recently, BiLSTM was widely used for long distance context modeling (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016). Additionally, Chiu and Nichols (2016) employed CNNs to capture morphological word representations; Lample et al. (2016) utilized CRF to model the dependencies between adjacent tags; Ma and Hovy (2016) proposed LSTM-CNNs-CRF model to combine the strengths of these components. Besides, Strubell et al. (2017) proposed iterated-dilated CNNs for higher efﬁciency than BiLSTM and better capacity with large context than vanilla CNNs. Recent work proved that the context-sensitive representations captured by language models are useful in NER systems. Peters et al. (2017) proposed TagLM model and introduced LM embeddings in this task. Afterwards, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) were proposed for better contextual representations. However, these methods focused only on the context within a sentence, so their performance is substantially hurt by the ambiguity and limitation of sent"
2020.aacl-main.22,K18-1009,0,0.0237893,"nguage models are useful in NER systems. Peters et al. (2017) proposed TagLM model and introduced LM embeddings in this task. Afterwards, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) were proposed for better contextual representations. However, these methods focused only on the context within a sentence, so their performance is substantially hurt by the ambiguity and limitation of sentence context. To combine contexts beyond sentences, several methods were proposed to mine document-level information, such as logical rules (Mikheev et al., 1999), global attention (Xu et al., 2018; Zhang et al., 2018; Hu et al., 2020) and memory mechanisms (Gui et al., 2020). But these methods ignored the sequential characteristics of the sentences within a document, which may be sub-optimal. We observe that contextual associations between sentences in a document have the potential of improving the NER performance. Moreover, the words’ preferences of entity type and relative position from the entities 182 O O … O O S O O O S O S O O S O tanh 2-layer Neural Network … Character embedding Word embedding Character representation (ு) ு ߙଵଵ … ߙଵ … … … ଵ … … …… ߙଵ … … … ு ଵ ߙ ߙଵ ……  … … ଵ ߙଵ … … ଵ ଵ ߙଵ …"
2020.aacl-main.6,P19-1033,1,0.652179,"categories, user profiles and features extracted from user behavior histories. They ranked candidate news based on the click scores computed by a neural factorization machine. However, these methods rely on manual feature engineering to build news and user representations, which usually necessitate massive expertise. In addition, handcrafted features may not be optimal in representing news content and user interest. In recent years, several news recommendation methods based on deep learning techniques are proposed (Okura et al., 2017; Khattar et al., 2018; Wang et al., 2018; Wu et al., 2019a; An et al., 2019; Wu et al., 2019b,c; Ge et al., 2020). For example, Okura et al. (2017) proposed to learn first news representations from news bodies using autoencoders, and then learn representations of users from their clicked news with a GRU network. Candidate news are ranked based on the click scores computed by the dot products between news and user representations. Wang et al. (2018) proposed to learn news representations from news titles and • To the best of our knowledge, this is the first work that explores to improve the sentiment diversity of news recommendation. • We propose a sentiment-aware new"
2020.aacl-main.6,D19-1671,1,0.675314,"Missing"
2020.aacl-main.6,D14-1162,0,0.0841738,"i=1 10 X i ), sci ), (8) sci ), i=1 where C is the number of candidate news in an impression, sci denotes the sentiment score of the candidate news with the i-th highest click score. In these metrics, higher scores indicate that the recommendation results are less diverse from the browsed news in their sentiment.9 We repeated each experiment 10 times and reported the average results over all impressions in terms of the recommendation performance and sentiment diversity. Following Wu et al. (2019b), in our experiments the word embeddings were initialized by the 300dimensional Glove embeddings (Pennington et al., 2014). The negative sampling ratio K was set to 4. Adam (Kingma and Ba, 2014) was chosen as the optimizer and the size of a minibatch was 30. In addition, the loss weights λ and µ were respectively set to 0.4 and 10. These hyperparameters were tuned on the validation set. To evaluate the performance of news recommendation, we use metrics including AUC, MRR, nDCG@5 and nDCG@108 . 4.2 Performance Evaluation We evaluate the recommendation performance and sentiment diversity of our approach by comparing it with several baseline methods, including: (1) LibFM (Rendle, 2012), a feature-based recommendatio"
2020.acl-main.267,D16-1171,0,0.0144082,"e representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text representations. There are also other popular pooling methods that can select salient features to learn more informative text representations, such as max pooling (Kim, 2014; Zhang et al., 2015) and attentive pooling (Yang et al., 2016), which are employed by many neural NLP methods (Collobert et al., 2011; Kim, 2014; Huang et al., 2012; Yang et al., 2016; Chen et al., 2016; Zhou et al., 2016; Du et al., 2017; Li et al., 2018; Wu et al., 2019a; Tao et al., 2019; Devlin et al., 2019; Wu et al., 2019b). For example, Collobert et al. (2011) proposed to learn representations of contexts within each window using feed forward neural networks, and used max pooling to build final text representations. Kim (2014) proposed to apply max pooling over time to the contextual word representations learned by multiple CNN filters. Huang et al. (2012) proposed to build representations of the entire document using the summation of word representations weighted by their TF-IDF scor"
2020.acl-main.267,P17-1055,0,0.0312344,"ing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations"
2020.acl-main.267,N19-1423,0,0.215536,"ion (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations, which may not be op"
2020.acl-main.267,P12-1092,0,0.0717581,"rage pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text representations. There are also other popular pooling methods that can select salient features to learn more informative text representations, such as max pooling (Kim, 2014; Zhang et al., 2015) and attentive pooling (Yang et al., 2016), which are employed by many neural NLP methods (Collobert et al., 2011; Kim, 2014; Huang et al., 2012; Yang et al., 2016; Chen et al., 2016; Zhou et al., 2016; Du et al., 2017; Li et al., 2018; Wu et al., 2019a; Tao et al., 2019; Devlin et al., 2019; Wu et al., 2019b). For example, Collobert et al. (2011) proposed to learn representations of contexts within each window using feed forward neural networks, and used max pooling to build final text representations. Kim (2014) proposed to apply max pooling over time to the contextual word representations learned by multiple CNN filters. Huang et al. (2012) proposed to build representations of the entire document using the summation of word represe"
2020.acl-main.267,D14-1181,0,0.0924972,"esentation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average an"
2020.acl-main.267,P15-1098,0,0.128348,"of text representation. 1 The visualized weights of max pooling are summations of the maximum elements over time for each word. 2 https://github.com/wuch15/ACL2020-APLN 2 Related Work Neural networks are widely used to learn text representations from contexts (Peng et al., 2018). Pooling is usually an essential step in these methods to build contextual representations by summarizing the information of input features (LeCun et al., 2015). The simplest pooling method is average pooling, which is used in many approaches to construct text representations (Tang et al., 2014, 2015a,b). For example, Tang et al. (2015a) proposed to apply average pooling to the output of CNN filters to capture global contexts in a sentence. In addition, they also proposed to average the sentence representations learned by parallel CNN networks with different window sizes. In their another work (Tang et al., 2015b), they proposed to apply average pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text represen"
2020.acl-main.267,P14-1146,0,0.305955,"urately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build th"
2020.acl-main.267,N19-1180,1,0.904864,"). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build text representations, which may not be optimal when handli"
2020.acl-main.267,D14-1162,0,0.0975115,"ts with different characteristics. The first one is AG’s News5 , which is a news topic classification dataset. Following (Zhang et al., 2015), we only use the title and description fields in this dataset. The second one is IMDB6 (Diao et al., 2014), which is a dataset with movie reviews and ratings. The third one is Amazon Electronics 4 In experiments on a machine with a GTX1080ti GPU, the computation of xp is accelerated by more than 10 times. 5 https://www.di.unipi.it/en/ 6 https://github.com/nihalb/JMARS In our experiments, the word embeddings were 300-dimensional and initialized by Glove (Pennington et al., 2014)7 . In our comparative experiments, the CNN networks had 400 filters, and their window size was 3. The dimension of LSTM hidden states was 200. The attention query vectors were 200-dimensional. The initial pooling norm p was set to 1, which is consistent with the vanilla attentive pooling. Adam (Kingma and Ba, 2014) was used as the optimizer, and the 7 We do not use language models such as ELMo and BERT since our work focuses on facilitating the pooling technique rather than boosting the performance of our approach against the state-of-the-art methods. 2965 Methods CNN-Avg CNN-Max CNN-Att CNN-"
2020.acl-main.267,N16-1174,0,0.84745,"kes Heavy Toll on Wildlife Max Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife Attentive Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife News Topic Classification Figure 1: The pooling weights of several different pooling methods on the representations produced by an LSTM network. Darker colors indicate higher weights. that can select salient features accurately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson"
2020.acl-main.267,N18-1202,0,0.0275927,"ildlife Max Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife Attentive Pooling Fire on Queensland Island Takes Heavy Toll on Wildlife News Topic Classification Figure 1: The pooling weights of several different pooling methods on the representations produced by an LSTM network. Darker colors indicate higher weights. that can select salient features accurately will facilitate many NLP methods (Ma et al., 2017). Introduction In recent years, neural network based methods are widely used in the natural language processing (NLP) field to learn text representations (Yang et al., 2016; Peters et al., 2018). In these methods, pooling is a core technique to build the text representation vector from a collection of input feature vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thu"
2020.acl-main.267,D15-1167,0,0.205443,"of text representation. 1 The visualized weights of max pooling are summations of the maximum elements over time for each word. 2 https://github.com/wuch15/ACL2020-APLN 2 Related Work Neural networks are widely used to learn text representations from contexts (Peng et al., 2018). Pooling is usually an essential step in these methods to build contextual representations by summarizing the information of input features (LeCun et al., 2015). The simplest pooling method is average pooling, which is used in many approaches to construct text representations (Tang et al., 2014, 2015a,b). For example, Tang et al. (2015a) proposed to apply average pooling to the output of CNN filters to capture global contexts in a sentence. In addition, they also proposed to average the sentence representations learned by parallel CNN networks with different window sizes. In their another work (Tang et al., 2015b), they proposed to apply average pooling to the sequence of sentence representations to build the representations of an entire document. Although average pooling is computationally efficient, it cannot distinguish important contexts from unimportant ones, which may not be optimal for learning accurate text represen"
2020.acl-main.267,D16-1024,0,0.33966,"vectors by summarizing their information (Lai et al., 2015). Thus, an effective pooling method Among existing pooling methods, average pooling is a representative one which takes the average of the L1 norm of input features (Tang et al., 2014, 2015a,b). However, average pooling equally regards the input representation vector at each position and ignores their different informativeness for learning text representation, which may not be optimal (Johnson and Zhang, 2015). Thus, other pooling methods such as max pooling (Collobert et al., 2011; Kim, 2014) and attentive pooling (Yang et al., 2016; Zhou et al., 2016; Cui et al., 2017; Devlin et al., 2019; Wu et al., 2019b) are widely used in neural NLP models. For example, Kim (2014) proposed to apply max pooling to the contextual word representations learned by CNN networks to build the representations of the entire sentence. Yang et al. (2016) proposed to use attentive pooling at both word and sentence levels to learn informative sentence and document representations by selecting important words and sentences. However, these pooling methods use fixed average norms, i.e., L1 norm for average and attentive pooling and L∞ norm for max pooling, to build te"
2020.ccl-1.106,W17-4215,0,0.0257596,"na National Conference on Computational Linguistics, pages 1143-1154, Hainan, China, October 30 - Novermber 1, 2020. (c) Technical Committee on Computational Linguistics, Chinese Information Processing Society of China Computational Linguistics 2 Related Work CC L 20 20 Automatic detection of clickbaits is important for online platforms to purify their web content and improve user experience. Traditional clickbait detection methods usually rely on handcrafted features to build representations of webpages (Chen et al., 2015; Biyani et al., 2016; Potthast et al., 2016; Chakraborty et al., 2016; Bourgonje et al., 2017; Cao et al., 2017; Indurthi and Oota, 2017; Gec¸kil et al., 2018). For example, Chen et al. (2015) proposed to represent news articles with semantic features (e.g., unresolved pronouns, affective words, suspenseful language and overuse numerals), syntax features (e.g., forward reference and reverse narrative) and image features (e.g., image placement and emotional content). In addition, they incorporate users’ behaviors on news, like reading time, sharing and commenting, to enhance news representation. They use various classification models like Naive Bayes and SVM to identify clickbaits base"
2020.ccl-1.106,D14-1162,0,0.0879086,"Missing"
2020.ccl-1.106,N16-1174,0,0.0595971,"fective neural architecture for context modeling. Thus, we apply two independent Transformers to learn hidden representations of words in title and body by modeling their contexts. We denote the hidden representation sequences of words in title and body as Et = [et1 , et2 , ..., etN ] and Eb = [eb1 , eb2 , ..., ebP ], respectively. Different words in a title or body may have different importance for modeling the content. For instance, the word “MUST” in Fig. 1 is more important than the word “About” in learning title representation for clickbait detection. Thus, we apply attention mechanisms (Yang et al., 2016) to select words in the title and body to form unified representations for them (denoted as et and eb ), which are respectively formulated as follows: 3.2 et = Attention([et1 , et2 , ..., etN ]), (1) eb = Attention([eb1 , eb2 , ..., ebP ]). (2) Style Modeling The style modeling module is used to capture the stylistic patterns in the title to better identify clickbaits. Usually, there are some common patterns on the style of clickbait titles. For example, many clickbaits use all-capital words (e.g., “MUST”, “NOT” and “THIS”), exclamation marks, and numeric characters to attract users’ attention"
2020.ccl-1.85,C18-1139,0,0.0222805,"as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach name"
2020.ccl-1.85,P19-1587,0,0.0193625,"e entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted fea"
2020.ccl-1.85,Q16-1026,0,0.477774,"aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, ma"
2020.ccl-1.85,D18-1217,0,0.0142394,"italization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a lang"
2020.ccl-1.85,J81-4005,0,0.704455,"Missing"
2020.ccl-1.85,N19-1423,0,0.37551,"o incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a language model on a large corpus to provide contextualized word r"
2020.ccl-1.85,D19-1096,0,0.0194451,"nowledge to design these dictionary based features, and these handcrafted features may not be optimal. Different from these methods, in our approach we introduce a term-level classification task to exploit the useful information in entity dictionary without manual feature engineering. We jointly train our model in both the NER and term classification tasks to enhance the performance of NER model in an end-to-end manner. There are also a few methods that explore to incorporate dictionary knowledge into Chinese NER models in an end-to-end manner by using graph neural networks (Sui et al., 2019; Gui et al., 2019). For example, Sui et al. (2019) propose a character-based collaborative graph neural network to learn the representations of characters and words matched by dictionaries from three word-character graphs, i.e., a containing graph that describes the connection between characters and matched words, a transition graph that builds the connections between characters and the nearest contextual matched words, and a Lattice graph that connects each word with its boundary characters. However, these methods mainly model the interactions between matched entities and their local contexts, while ignore the"
2020.ccl-1.85,C12-1071,0,0.115466,"CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases"
2020.ccl-1.85,N16-1030,0,0.0305721,"many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) prop"
2020.ccl-1.85,W06-0115,0,0.0603833,"a context-dictionary attention. In addition, we propose an auxiliary term classification task to predict the types of the matched entity names, and jointly train it with the NER model to fuse both contexts and dictionary knowledge into NER. Extensive experiments on the CoNLL-2003 benchmark dataset validate the effectiveness of our approach in exploiting entity dictionaries to improve the performance of various NER models. CC L Named entity recognition (NER) aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations fro"
2020.ccl-1.85,D17-1282,0,0.0144347,"features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et"
2020.ccl-1.85,D18-1226,0,0.019237,"s, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by"
2020.ccl-1.85,P16-1200,0,0.0384072,"contexts and dictionary knowledge into NER. Extensive experiments on the CoNLL-2003 benchmark dataset validate the effectiveness of our approach in exploiting entity dictionaries to improve the performance of various NER models. CC L Named entity recognition (NER) aims to extract entity names from texts and classify them into several pre-defined categories, such as person, location and organization (Levow, 2006). It is an important task in natural language processing, and a prerequisite for many downstream applications such as entity linking (Derczynski et al., 2015) and relation extraction (Lin et al., 2016; Luo et al., 2018; Zeng et al., 2018). Thus, NER is a hot research topic. In this paper, we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to a"
2020.ccl-1.85,P19-1524,0,0.0603402,"n addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different contexts. Thus, it is not optimal to directly apply entity dictionaries to NER without considering the contexts. 1 https://www.geonames.org Proceedings of the 19th China National Conference on Computational Linguistics, pages 915-926, Hainan, Chi"
2020.ccl-1.85,W19-5807,0,0.0181192,"performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different contexts. Thus, it is not optimal to directly apply entity dictionaries to NER without considering the contexts. 1 https://www.geonames.org Proceedings of the 19th China National Conference on Computational Linguistics, pages 915-926, Hainan, China, October 30 - Novermbe"
2020.ccl-1.85,E14-1048,0,0.0135247,"and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NER (Liu et al., 2019; Magnolini et al., 2019) and most of them are based on dictionary matching features. For example, Wang et al. (2019) proposed to combine token matching features with token embeddings and LSTM outputs. However, in many cases entities are context-dependent. For instance, in Table 1, the word “Jordan” can be a person name or a location name in different c"
2020.ccl-1.85,W14-1609,0,0.142949,"n model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing t"
2020.ccl-1.85,P19-1231,0,0.0191113,"we focus on the English NER task. Many methods have been proposed for English NER, and most of them model this task as a word-level sequence labeling problem (Chiu and Nichols, 2016). For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for English NER. They used CNN to learn word representations from characters, LSTM to model the contexts of words, and CRF to decode labels. These existing NER methods usually rely on massive labeled data for model training, which is costly and time-consuming to annotate. When training data is scarce, their performance usually significantly declines (Peng et al., 2019). In addition, their performance on recognizing entities that rarely or do not appear in training data is usually unsatisfactory (Wang et al., 2019). Fortunately, many large-scale entity dictionaries such as Wikipedia (Higashinaka et al., 2012) and Geonames1 are off-the-shelf, and they can be easily derived from knowledge bases and webpages (Neelakantan and Collins, 2014). These entity dictionaries contain both popular and rare entity names, and can provide important information for NER models to identify these entity names. There are a few researches on incorporating entity dictionary into NE"
2020.ccl-1.85,P17-1161,0,0.341377,"istories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better perfo"
2020.ccl-1.85,N18-1202,0,0.21204,"4) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supe"
2020.ccl-1.85,W09-1119,0,0.57015,"d the term classification model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word represe"
2020.ccl-1.85,P17-1194,0,0.0173022,"et al. (2014) used features such as character n-grams, word types, capitalization pattern and lexicon matching features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017)"
2020.ccl-1.85,S13-2058,0,0.0739984,"Missing"
2020.ccl-1.85,D19-1396,0,0.017795,"s rely on domain knowledge to design these dictionary based features, and these handcrafted features may not be optimal. Different from these methods, in our approach we introduce a term-level classification task to exploit the useful information in entity dictionary without manual feature engineering. We jointly train our model in both the NER and term classification tasks to enhance the performance of NER model in an end-to-end manner. There are also a few methods that explore to incorporate dictionary knowledge into Chinese NER models in an end-to-end manner by using graph neural networks (Sui et al., 2019; Gui et al., 2019). For example, Sui et al. (2019) propose a character-based collaborative graph neural network to learn the representations of characters and words matched by dictionaries from three word-character graphs, i.e., a containing graph that describes the connection between characters and matched words, a transition graph that builds the connections between characters and the nearest contextual matched words, and a Lattice graph that connects each word with its boundary characters. However, these methods mainly model the interactions between matched entities and their local context"
2020.ccl-1.85,W03-0419,0,0.540528,"Missing"
2020.ccl-1.85,I11-1096,0,0.0346375,"on, we propose an auxiliary term classification task to predict the types of the matched entity names in different contexts. Besides, we propose a unified framework to jointly train the NER model and the term classification model to incorporate entity dictionary knowledge and contextual information into the NER model. Extensive experiments show our approach can effectively exploit entity dictionaries to improve the performance of various NER models and reduce their dependence on labeled data. 2 Related Work CC L 20 20 Named entity recognition is usually modeled as a sequence labeling problem (Wan et al., 2011). Many traditional NER methods are based on statistical sequence modeling methods, such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Cohen and Sarawagi, 2004; Ratinov and Roth, 2009; Passos et al., 2014; Arora et al., 2019). Usually, a core problem in these methods is how to build the feature vector for each word, and these features are traditionally constructed via manual feature engineering (Ratinov and Roth, 2009). For example, Ratinov and Roth (2009) used many features such as word n-grams, gazetteers and prediction histories as the word features. Passos et al. (2014)"
2020.ccl-1.85,I08-4016,0,0.0439182,"ctional pre-trained language model named BERT, which can empower downstream tasks like NER by using deep Transformers (Vaswani et al., 2017) to model contexts accurately. However, these neural network based methods heavily rely on labeled sentences to train NER models, which need heavy effort of manual annotation. In addition, their performance on recognizing entities which rarely or do not appear in labeled data is usually unsatisfactory (Wang et al., 2019). There are several approaches on utilizing entity dictionaries for named entity recognition (Cohen and Sarawagi, 2004; Lin et al., 2007; Yu et al., 2008; Rockt¨aschel et al., 2013; Passos et al., 2014; Song et al., 2015; Wang et al., 2019; Liu et al., 2019). In traditional methods, dictionaries are often incorporated as additional features. For example, Cohen et al. (2004) proposed to extract dictionary features based on entity matching and similarities, and they incorporated these features into an HMM based model. There are also a few methods to incorporate dictionary knowledge into neural NER models (Chiu and Nichols, 2016; Wang et al., 2019; Liu et al., 2019). For example, Wang et al. (2019) proposed to incorporate dictionaries into neural"
2020.ccl-1.85,N19-1342,0,0.0720066,"g features. They also incorporated lexicon embedding learned by skip-gram model to enhance the word representations. Designing these hand-crafted features usually needs a huge amount of domain knowledge. In addition, the feature vectors may be very sparse and their dimensions can be huge. In recent years, many neural network based NER methods have been proposed (Collobert et al., 2011; Lample et al., 2016; Chiu and Nichols, 2016; Ma and Hovy, 2016; Peters et al., 2017; Li et al., 2017; Rei, 2017; Peters et al., 2018; Akbik et al., 2018; Lin and Lu, 2018; Clark et al., 2018; Chen et al., 2019; Zhu and Wang, 2019; Devlin et al., 2019). For example, Lample et al. (2016) proposed to use LSTM to learn the contextual representation of each token based on global context in sentences and use CRF for joint label decoding. Chiu and Nichols (2016) proposed to use CNN to learn word representations from original characters and then learn contextual word representation using Bi-LSTM. Ma and Hovy (2016) proposed to combine the CNN-LSTM framework with CRF for better performance. Peters et al. (2017) proposed a semi-supervised approach named TagLM for NER by pre-training a language model on a large corpus to provide"
2020.findings-emnlp.128,P19-1033,1,0.441834,"ssive news articles are posted online every day, users of online news services face heavy information overload (Zheng et al., 2018). Different users usually prefer different news information. Thus, personalized news recommendation, which aims to display news articles to users based on their personal interest, is a useful technique to improve user experience and has been widely used in many online news services (Wu et al., 2019b). The research of news recommendation has attracted many attentions from both academic and industrial fields (Okura et al., 2017; Wang et al., 2018; Lian et al., 2018; An et al., 2019; Wu et al., 2019a). Many news recommendation methods have been proposed in recent years (Wang et al., 2018; Wu et al., 2019b; Zhu et al., 2019b). These methods usually recommend news based on the matching between the news representation learned from news content and the user interest representation learned from historical user behaviors on news. For example, Okura et al. (2017) proposed to learn news representations from the content of news articles via autoencoder, and learn user interest representations from the clicked news articles via Gated Recurrent Unit (GRU) network. They ranked the c"
2020.findings-emnlp.128,W14-4012,0,0.0583608,"Missing"
2020.findings-emnlp.128,2020.acl-main.392,1,0.848151,"i.e., the strength of the noise), we can achieve a smaller privacy budget  which means better privacy protection. However, strong noise will hurt the accuracy of aggregated gradients. Thus, λ should be selected based on the trade-off between privacy protection and model performance. 4 Experiment 4.1 Dataset and Experimental Settings Our experiments were conducted on a public news recommendation dataset (named Adressa) collected from a Norwegian news website (Gulla et al., 2017) and another real-world dataset collected from Microsoft News4 (named MSN-News).5 For the Adressa dataset, following Hu et al. (2020), we used user logs in the first five days to construct 4 https://www.msn.com/en-us Our dataset and codes will be publicly available in https://github.com/JulySinceAndrew/FedNewsRec-EMNLPFindings-2020. 5 # users # news # impressions # positive behaviors # negative behaviors avg. # title length MSN-News 100,000 118,325 1,341,853 2,006,289 48,051,601 11.52 Adressa 528,514 16,004 2,411,187 6.60 Table 1: The statistical information of the dataset. users’ click history, used logs in the 6-th day for model training, and used logs in the 7-th day for model evaluation. Since the Adressa dataset does n"
2020.findings-emnlp.128,D19-1493,1,0.566924,"Missing"
2020.findings-emnlp.128,D19-1671,1,0.695969,"Missing"
2020.findings-emnlp.174,N19-1423,0,0.146638,"s users from user behaviors on the e-commerce platform based on their relevance to the candidate ads. Okura et al. (2017) proposed to use a GRU network for news recommendation, which models users from their clicked news. However, these methods mainly rely on sufficient labeled data to train user models, and their performance may be not optimal when training data is scarce. In addition, they only model task-specific user information and do not exploit the universal user information encoded in user behaviors. In recent years, pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019) have achieved great success in many NLP tasks, such as reading comprehension and machine translation. Many language models are pre-trained on a large unlabeled corpus via self-supervision tasks such as masked LM and next sentence prediction to model the contexts (Devlin et al., 2019). These language models can learn universal language representations from large unlabeled corpus and empower many different downstream tasks when the labeled data for these tasks is insufficient (Qiu et al., 2020). Motivated by pre-trained language models, in this paper we propose pre"
2020.findings-emnlp.174,N16-1174,0,0.0221708,"plete hyperparameter settings and analysis are included in supplements. To evaluate the performance of different methods, we used accuracy and macro F-score on the Demo dataset, and used AUC and AP scores on the CTR dataset. Each experiment was repeated 10 times independently. 3.2 Performance Evaluation In this section, we verify the effectiveness of our proposed PTUM method for user model pretraining. We choose several state-of-the-art user models and compare their performance with their variants pre-trained by our PTUM method. On the Demo dataset, the models to be compared include: (1) HAN (Yang et al., 2016), hierarchical attention network, which uses attentional LSTM to learn behavior and user representations. (2) HURA (Wu et al., 2019c), hierarchical user representation with attention model, which uses CNN and attention networks to learn behavior and user representations. (3) HSA (Wu et al., 2019b), using hierarchical multi-head self-attention to learn behavior and user representations. On the CTR dataset, the models to be compared include: (1) GRU4Rec (Hidasi et al., 2016), using GRU networks to learn behavior and user representations. (2) NativeCTR (An et al., 2019), using CNN and attention n"
2020.findings-emnlp.174,N18-1202,0,0.0189005,"CTR) prediction, which models users from user behaviors on the e-commerce platform based on their relevance to the candidate ads. Okura et al. (2017) proposed to use a GRU network for news recommendation, which models users from their clicked news. However, these methods mainly rely on sufficient labeled data to train user models, and their performance may be not optimal when training data is scarce. In addition, they only model task-specific user information and do not exploit the universal user information encoded in user behaviors. In recent years, pre-trained language models such as ELMo (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019) have achieved great success in many NLP tasks, such as reading comprehension and machine translation. Many language models are pre-trained on a large unlabeled corpus via self-supervision tasks such as masked LM and next sentence prediction to model the contexts (Devlin et al., 2019). These language models can learn universal language representations from large unlabeled corpus and empower many different downstream tasks when the labeled data for these tasks is insufficient (Qiu et al., 2020). Motivated by pre-trained language models,"
2020.findings-emnlp.174,D19-1671,1,0.463046,"Missing"
2021.acl-long.423,P19-1033,1,0.862875,"News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user interests, and used an attentive pooling network to obtain a unified user representation. However, user interest is usually diverse and multigrained. For example, as shown in Fig. 1, a user may have interest in movies, sports, finance and health at the same time. In addition, for users who are interested in sports, some of them may have general inter"
2021.acl-long.423,D14-1162,0,0.0893212,"ree weeks to construct test data. Since Feeds only contains topic label of news, we implement a simplified version of HieRec with only user- and topiclevel interest representations on Feeds. Besides, following Wu et al. (2020d), users in Feeds were anonymized via hash algorithms and de-linked from the production system to protect user privacy. Detailed information is summarized in Table 1. Next, we introduce experimental settings and hyper-parameters of HieRec. We use the first 30 words and 5 entities of news titles and users’ recent 50 clicked news in experiments. We adopt pre-trained glove (Pennington et al., 2014) word embeddings and TransE entity embeddings (Bordes et al., 2013) for initialization. In HieRec, the word and entity self-attention network output 400and 100-dimensional vectors, respectively. Besides, the unified news representation is 400-dimensional. Attention networks (i.e., φs (·), φt (·), and φg (·)) are implemented by single-layer dense networks. Besides, dimensions of topic and subtopic embeddings are 400, both of which are randomly initialized and fine-tuned. The hyper-parameters for combining different interest scores, i.e. λt and λs , are set to 0.15 and 0.7 respectively. Moreover"
2021.acl-long.423,2020.findings-emnlp.128,1,0.884748,"ained, which are difficult to be accurately modeled by a single user embedding. Different from these methods, we propose a hierarchical user interest modeling framework to model user interests in different aspects and granularities. In addition, we propose a hierarchical user interest matching framework to understand user interest in candidate news from different interest granularities for more accurate user interest targeting. 2 3 Related Work Personalized news recommendation is an important intelligent application and is widely studied in recent years (Bansal et al., 2015; Wu et al., 2019c; Qi et al., 2020; Ge et al., 2020). Existing methods usually model news from its content, model user interest from user’s clicked news, and recommend candidate news based on their relevance with user interests (Okura et al., 2017). For example, Okura et al. (2017) utilized an auto-encoder to learn news representations from news bodies. They applied a GRU network to capture user interests from the sequence of users’ historical clicks and used the last hidden state vector of GRU as user interest representation. Besides, they proposed to model relevance between user interest and candidate news based on the dot p"
2021.acl-long.423,2020.acl-main.77,1,0.699389,"ked news via a GRU network and long-term user interests from user-news interactions via user ID embeddings. (7) NRMS (Wu et al., 2019e): applying multi-head self-attention networks to learn news representations and user representations. (8) KRED (Liu et al., 2020): proposing a knowledge graph attention network to learn news representations from texts and entities of news titles. (9) GNewsRec (Hu et al., 2020): modeling short-term user interests from clicked news sequences via an attentive GRU network and long-term user interests from user-news click graph via a graph neural network. (10) FIM (Wang et al., 2020): modeling user interests in candidate news from semantic relevance of user’s clicked news and candidate news 2 https://github.com/JulySinceAndrew/HieRec via a 3-D CNN network. Each experiment is repeated 5 times. The average results and standard deviations are listed in Table 2, from which we have several observations. First, HieRec significantly outperforms other baseline methods which learn a single user embedding to model overall user interests, such as NRMS, NPA, and NAML. This is because user interests are usually diverse and multi-grained. However, it is difficult for a single represent"
2021.acl-long.423,P19-1110,1,0.821408,"Missing"
2021.acl-long.423,D19-1493,1,0.872578,"Missing"
2021.acl-long.423,D19-1671,1,0.877679,"Missing"
2021.acl-long.423,2020.aacl-main.6,1,0.882116,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.423,2020.findings-emnlp.174,1,0.904662,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.423,2020.acl-main.331,1,0.86944,"experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation. 1 Figure 1: Click and non-click logs of an example user. Introduction Recently, massive people are habituated to reading news articles on online news platforms, such as Google News and Microsoft News (Khattar et al., 2018; Das et al., 2007). To help users efficiently obtain their interested news information, personalized news recommendation technique that aims to recommend news according to user interests, is widely used by these platforms (Wu et al., 2020a; Liu et al., 2010; Lin et al., 2014). User interest modeling is a critical step for personalized news recommendation (Wu et al., 2021; Zheng et al., 2018; Wu et al., 2020c). Existing methods usually learn a single representation vector to model overall user interests from users’ clicked news (Okura et al., 2017; Wu et al., 2020b; An et al., 2019). For example, Okura et al. (2017) used a GRU network to model user interests from clicked news. They used the latest hidden state of GRU as the user interest representation. Wu et al. (2019e) used multi-head self-attention network to capture user in"
2021.acl-long.424,P19-1033,1,0.823497,"n visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to learn news embeddings from news titles and categories, and model both long-term and short-term user interests from news click behaviors. However, these personalized news recommendation methods usually have difficulties in making accurate recommendations to cold-start users, since the behaviors of these users are very sparse and it is difficult to model their interest (Trevisiol et al., 2014). Besides, these methods tend to recommend similar news with those users have read (Nguyen et al., 2014), which may hurt user experience and is not beneficial for them to rec"
2021.acl-long.424,D14-1162,0,0.0884303,"For both datasets, we randomly sample 500k impressions for model training, 100k impressions for validation, and 500k impressions for evaluation, respectively. The detailed statistics are listed in Table 1. Following previous works (Wu et al., 2019a; An et al., 2019), we use AUC, MRR, nDCG@5, and nDCG@10 to evaluate recommendation performance. MSN Feeds # News 161,013 4,117,562 # Users 490,522 98,866 # Impressions 1,100,000 1,100,000 # Clicks 1,675,084 2,384,976 Table 1: Statistics of the datasets. In our experiments, word embeddings are 300dimensional and initialized by the Glove embeddings (Pennington et al., 2014). The entity embeddings are 100-dimensional vectors pre-trained on knowledge tuples extracted from WikiData via TransE (Bordes et al., 2013). We use clicked and unclicked impressions in the recent one hour to compute the near real-time CTR. The recency and popularity embeddings are set to 100 dimensions and initialized randomly. All multi-head attention networks are set to have 20 attention heads and the output dimension of each head is 20. All gate networks are implemented by a two-layer dense network with 100-dimensional hidden vectors. Dropout approach (Srivastava et al., 2014) is applied t"
2021.acl-long.424,2020.findings-emnlp.128,1,0.744287,"er of views and comments (Yang, 2016; Tatar et al., 2014; Lee et al., 2010). For example, Yang (2016) proposed to use the frequency of views to measure news popularity. Tatar et al. (2014) proposed to predict news popularity based Related Work 2.1 Personalized News Recommendation Personalized news recommendation are widely used in online news platforms (Liu et al., 2010; Bansal et al., 2015; Wu et al., 2020d,c, 2019d). Existing personalized news recommendation methods usually rank candidate news for a target user based on the matching between news content and user interest (Wang et al., 2018; Qi et al., 2020; Wu et al., 2020a, 2019c). For example, Okura et al. (2017) learned news embeddings from news bodies via an auto-encoder and modeled user interests from the clicked news via a GRU network. The matching between news and user is formulated as the dot product of their embeddings. Wu et al. (2019e) used multi-head self-attention networks to generate 1 https://github.com/JulySinceAndrew/PP-Rec 2.2 5458 Popularity-based News Recommendation Figure 2: The overall framework of PP-Rec. on the number of comments of news via a linear model. Li et al. (2011) proposed to use the number of clicks on news to"
2021.acl-long.424,P19-1110,1,0.871435,"Missing"
2021.acl-long.424,D19-1493,1,0.883569,"Missing"
2021.acl-long.424,D19-1671,1,0.871688,"Missing"
2021.acl-long.424,2020.aacl-main.6,1,0.864496,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-long.424,2020.findings-emnlp.174,1,0.792811,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-long.424,2020.acl-main.331,1,0.920608,"odeling. Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation. 1 6.1 magnitude quake rattles Alaska Returning to normal is not simple for everyone Biden aims to rebuild and expand legal immigration Russia diplomat warns US ahead of summit Man accused of plotting Walmart attack arrested Black Wall Street was shattered 100 years ago Figure 1: Several example popular news. Introduction Personalized news recommendation is a useful technique to help users alleviate information overload when visiting online news platforms (Wu et al., 2020d,b, 2021; Ge et al., 2020). Existing personalized news recommendation methods usually recommend news to a target user based on the matching between the content of candidate news and user interest inferred from previous behaviors (Zhu et al., 2019; Wu et al., 2019f). For example, Wu et al. (2019e) proposed to model news content from news title based on multi-head self-attention. In addition, they modeled user interest from the previously clicked news articles with multi-head self-attention to capture the relatedness between different behaviors. An et al. (2019) proposed to use CNN network to l"
2021.acl-short.107,N19-1423,0,0.0271475,"e use a document Transformer to model the global document context from these sentence representations. Next, we use another sentence Transformer to enhance sentence modeling using the global document context. Finally, we use hierarchical pooling method to obtain document embedding. Extensive experiments on three benchmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transfo"
2021.acl-short.107,2020.acl-main.267,1,0.904363,"hmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal"
2021.acl-short.107,2020.acl-main.331,1,0.818817,"hmark datasets validate the efficiency and effectiveness of HiTransformer in long document modeling. 1 Introduction Transformer (Vaswani et al., 2017) is an effective architecture for text modeling, and has been an essential component in many state-of-the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal"
2021.acl-short.107,D14-1162,0,0.111777,"ments are conducted on three benchmark document modeling datasets. The first one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is for product review rating prediction.3 The second one is IMDB (Diao et al., 2014), a widely used dataset for movie review rating prediction.4 The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale dataset for news intelligence.5 We use the content based news topic classification task on this dataset. The detailed dataset statistics are shown in Table 1. In our experiments, we use the 300-dimensional pre-trained Glove (Pennington et al., 2014) embeddings for initializing word embeddings. We use two Hi-Transformers layers in our approach and two Transformer layers in other baseline methods.6 We use attentive pooling (Yang et al., 2016) to implement the hierarchical pooling module. The hidden dimension is set to 256, i.e., 8 self-attention heads in total and the output dimension of each head is 32. Due to the limitation of GPU memory, the input sequence lengths of vanilla Transformer and its variants for long documents are 512 and 2048, respectively. The dropout (Srivastava et al., 2014) ratio is 0.2. The optimizer is Adam (Bengio an"
2021.acl-short.107,2020.findings-emnlp.232,0,0.0283646,"the-art NLP models like BERT (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Wu et al., 2021). The standard Transformer needs to compute a dense self-attention matrix based on the interactions between each pair of tokens in text, where the computational complexity is proportional to the square of text length (Vaswani et al., 2017; Wu et al., 2020b). Thus, it is difficult for Transformer to model long documents efficiently (Child et al., 2019). There are several methods to accelerate Transformer for long document modeling (Wu et al., 2019; Kitaev et al., 2019; Wang et al., 2020; Qiu et al., 2020). One direction is using Transformer in a hierarchical manner to reduce sequence length, e.g., first learn sentence representations and then learn document representations from sentence representations (Zhang et al., 2019; Yang et al., 2020). However, the modeling of sentences is agnostic to the global document context, which may be suboptimal because the local context within sentence is usually insufficient. Another direction is using a sparse self-attention matrix instead of a dense one. For example, Beltagy et al. (2020) proposed to combine local self-attention with a dilated sliding window"
2021.acl-short.107,N16-1174,0,0.0594138,"cond one is IMDB (Diao et al., 2014), a widely used dataset for movie review rating prediction.4 The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale dataset for news intelligence.5 We use the content based news topic classification task on this dataset. The detailed dataset statistics are shown in Table 1. In our experiments, we use the 300-dimensional pre-trained Glove (Pennington et al., 2014) embeddings for initializing word embeddings. We use two Hi-Transformers layers in our approach and two Transformer layers in other baseline methods.6 We use attentive pooling (Yang et al., 2016) to implement the hierarchical pooling module. The hidden dimension is set to 256, i.e., 8 self-attention heads in total and the output dimension of each head is 32. Due to the limitation of GPU memory, the input sequence lengths of vanilla Transformer and its variants for long documents are 512 and 2048, respectively. The dropout (Srivastava et al., 2014) ratio is 0.2. The optimizer is Adam (Bengio and LeCun, 2015), and the learning rate is 1e-4. The maximum training epoch is 3. The models are implemented using the Keras library with Tensorflow backend. The GPU we used is GeForce GTX 1080 Ti"
2021.findings-acl.268,P19-1422,0,0.216584,", 2008)), where secret information is encoded by slightly modifying an existing covertext, the options for modification can be very limited to keep the text fluent enough so as not to arouse suspicions. In recent years, powered by the advanced technology of deep learning and natural language processing, language models based on neural networks have made significant progress in generating fluent text (Radford et al., 2019; Brown et al., 2020), which bring new vitality to linguistic steganography and facilitate the investigation of generationbased methods (Fang et al., 2017; Yang et al., 2018a; Dai and Cai, 2019; Ziegler et al., 2019; Yang et al., 2020a; Zhou et al., 2021). The generative linguistic steganography directly transform secret information into innocuous-looking steganographic text (stegotext) without any covertext. Using an off-the-shelf language model, secret information can be encoded in the selection of token at each time step autoregressively during the generation procedure, which greatly alleviates the drawback of low embedding capacity. However, previous methods inevitably introduce distortions during generation. The imperceptibility of generative linguistic steganography still need"
2021.findings-acl.268,P17-3017,0,0.0543408,"Missing"
2021.findings-acl.268,W02-0109,0,0.345258,"amely “Large Movie Review Dataset” (Movie) (Maas et al., 2011), “All the News” (News)1 and “Sentiment140” (Tweet) (Go et al., 2009). Large movie review dataset is originally built for binary sentiment classification, containing 100,000 movie reviews in total crawled from IMDb2 . “All the news” is a collection of publications of mainstream news media. Sentiment140 is also used in sentiment analysis tasks, which contains 1,600,000 tweets extracted from Twitter3 . We converted the raw text to lowercase and removed HTML tags and most punctuations, then segmented it into sentences with NLTK tools (Loper and Bird, 2002). We filtered out sentences with length below 5 or above 200. For the convenience of training and evaluation, any token occurring less than 10 times was mapped to a special token “ UNK”. We also added “ BOS” and “ EOS” at the beginning and end of each sentence to help training. Sentences in a batch were padded to the same length with a special padding token “ PAD”. Finally, we divided the preprocessed corpora into training set and test set according to the ratio of 9:1. Statistics are demonstrated in Table 3. 5.2 Implementation Details In experiments, we utilized LSTMs (Hochreiter and Schmidhu"
2021.findings-acl.268,P11-1015,0,0.0214793,"ed to the recursive manner of ADG, which means the proposed method is provably secure. 4.3 Experimental Results and Analysis Information Extraction The extraction algorithm is basically the inverse process of the embedding algorithm. For an exactly successful extraction, Alice and Bob have to share the same language model, vocabulary and grouping algorithm. At each time step, Bob is supposed to recursively operate the same grouping algorithm as Alice do, and then select the group Datasets We evaluated the performance of ADG on three public corpora, namely “Large Movie Review Dataset” (Movie) (Maas et al., 2011), “All the News” (News)1 and “Sentiment140” (Tweet) (Go et al., 2009). Large movie review dataset is originally built for binary sentiment classification, containing 100,000 movie reviews in total crawled from IMDb2 . “All the news” is a collection of publications of mainstream news media. Sentiment140 is also used in sentiment analysis tasks, which contains 1,600,000 tweets extracted from Twitter3 . We converted the raw text to lowercase and removed HTML tags and most punctuations, then segmented it into sentences with NLTK tools (Loper and Bird, 2002). We filtered out sentences with length b"
2021.findings-acl.268,D19-1115,0,0.120641,"hnology of hiding secret information within an innocent natural carrier (such as image (Hussain et al., 2018), audio (Mishra et al., 2018), video (Liu et al., 2019), text (Krishnan et al., 2017), etc) in order to avoid eavesdropping. Steganography differs from cryptography in that cryptography only conceals the content of secret information, whereas steganography even conceals its very existence, which makes it more secure and reliable in some scenarios (Anderson and Petitcolas, 1998). Natural language is suitable as a carrier of steganography by virtue of its high robustness in transmission (Ziegler et al., 2019). Unlike digital images or digital audio which is sensitive to distortions like compression, cropping, blurring or pixel-wise dropout, text can usually be transmitted losslessly through different kinds of public channels. Nevertheless, text generally has low entropy and lacks sufficient redundancy for information hiding (Sharma et al., 2016), which often results in low embedding capacity of linguistic steganography. For example, in traditional modificationbased methods (such as synonym substitution (Xiang et al., 2014, 2018) and spelling transformation (Shirali-Shahreza, 2008)), where secret i"
2021.findings-acl.387,N19-1423,0,0.228095,"jointly finetune multiple teacher PLMs in downstream tasks with shared pooling and prediction layers to align their output space for better collaborative teaching. In addition, we propose a multi-teacher hidden loss and a multi-teacher distillation loss to transfer the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a"
2021.findings-acl.387,D13-1170,0,0.00424023,"ain higher weight. Following (Tang et al., 2019; Lu et al., 2020), we also incorporate gold labels to compute the taskspecific loss LT ask based on the predictions of the student model, i.e., LT ask = CE(y, ys ). The final loss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 ver"
2021.findings-acl.387,D19-1441,0,0.533019,"sets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowledge and supervision provided by a single teacher model may be insufficient to learn an accurate student model, and the student model may also i"
2021.findings-acl.387,2020.acl-main.267,1,0.901542,"ss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings"
2021.findings-acl.387,2021.findings-emnlp.280,1,0.717538,"ge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowledge and supervisio"
2021.findings-acl.387,2020.findings-emnlp.372,0,0.608909,"the useful knowledge in both hidden states and soft labels from multiple teacher PLMs to the student model. Experiments on three benchmark datasets validate the effectiveness of MTBERT in compressing PLMs. 1 Introduction Pre-trained language models (PLMs) such as BERT and RoBERTa have achieved notable success in various NLP tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019). However, many PLMs have a huge model size and computational complexity, making it difficult to deploy them to low-latency and high-concurrence online systems or devices with limited computational resources (Jiao et al., 2020; Wu et al., 2021). Knowledge distillation is a widely used technique for compressing large-scale pre-trained language models (Sun et al., 2019; Wang et al., 2020). For example, Sanh et al. (2019) proposed DistilBERT to compress BERT by transferring knowledge from the soft labels predicted by the teacher model to student model with a distillation loss. Jiao et al. (2020) proposed TinyBERT, which aligns the hidden states and the attention heatmaps between student and teacher models. These methods usually learn the student model from a single teacher model (Gou et al., 2020). However, the knowle"
2021.findings-acl.387,2020.acl-main.331,1,0.924376,"ss function L for learning the student model is a summation of the multi-teacher hidden loss, multiteacher distillation loss and the task-specific loss, which is formulated as follows: L = LM T −Hid + LM T −Dis + LT ask . 3 (3) Experiments 3.1 Datasets and Experimental Settings We conduct experiments on three benchmark datasets with different sizes. The first one is SST2 (Socher et al., 2013), which is a benchmark for text sentiment classification. The second one is RTE (Bentivogli et al., 2009), which is a widely used dataset for natural language inference. The third one is the MIND dataset (Wu et al., 2020c), which is a large-scale public English news dataset.5 We perform the news topic classification task on this dataset. The detailed statistics of the three datasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings"
2021.findings-acl.387,2021.ccl-1.108,0,0.0743917,"Missing"
2021.findings-acl.387,N16-1174,0,0.0726725,"atasets are shown in Table 1. In our experiments, we use the pre-trained 12layer BERT, RoBERTa and UniLM (Bao et al., 2020)6 models as the teachers to distill a 6-layer 5 6 https://msnews.github.io/ We used the UniLMv2 version. Dataset SST-2 RTE MIND #Train 67k 2.5k 102k #Dev 872 276 2.6k #Test 1.8k 3.0k 26k #Class 2 2 18 Table 1: The statistics of the three datasets. and a 4-layer student models respectively. We use the token embeddings and the first 4 or 6 Transformer layers of UniLM to initialize the parameters of the student model. The pooling layer is implemented by an attention network (Yang et al., 2016; Wu et al., 2020a). The temperature coefficient t is set to 1. The attention query dimension in the attentive pooling layer is 200. The optimizer we use is Adam (Bengio and LeCun, 2015). The teacher model learning rate is 2e-6 while the student model learning rate is 5e-6. The batch size is 64. Following (Jiao et al., 2020), we report the accuracy score on the SST-2 and RTE datasets. In addition, since the news topics in the MIND dataset are highly imbalanced, following (Wu et al., 2020b) we report both accuracy and macro-F1 scores. Each experiment is independently repeated 5 times and the av"
2021.findings-emnlp.124,P19-1033,1,0.897943,"mendation to encrypt the uploaded gradients. Qi et al. (2020) Personalized news recommendation is an important proposed to apply federated learning technique to research problem and has been widely studied in train neural news recommendation models on derecent years (Konstan et al., 1997; Wang and Blei, centralized user data. They used local differential 2011; Liu et al., 2010; Bansal et al., 2015; Wu et al., privacy technique (Ren et al., 2018) to protect the 2020b; Qi et al., 2021c; Wu et al., 2020d, 2021d; uploaded gradients from leaking user privacy. In Wang et al., 2020; Ge et al., 2020; An et al., 2019). brief, most of these methods focus on training a Existing news recommendation methods aim to recommendation model for ranking candidate news match candidate news content with user prefer- in a privacy-preserving way. However, how to genences mined from users’ historical behaviors (Khat- erate candidate news from news pool according tar et al., 2018; Wu et al., 2021b,f; Ge et al., 2020; to user interest and serve users with decentralized Qi et al., 2021a; Wu et al., 2019d; An et al., 2019). user behavior data are still unsolved problems. DifFor example, Okura et al. (2017) proposed to learn f"
2021.findings-emnlp.124,2021.acl-long.424,1,0.218472,"ool, which are further distributed to the user client at the ranking stage for personalized news display. In addition, we propose an interest decomposer-aggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy. 1 Introduction 2019c,b, 2020c; Qi et al., 2021a; Ge et al., 2020; Wu et al., 2021c). For example, Okura et al. (2017) employed a GRU network to build user embeddings from browsed news. Wu et al. (2019a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns"
2021.findings-emnlp.124,2020.findings-emnlp.128,1,0.832885,"19a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns on privacy leakage and violate some privacy protection regulations such as GDPR1 . A few methods explore to recommend news in a privacy-preserving way (Qi et al., 2020). For instance, Qi et al. (2020) proposed to store user data in user clients and applied federated learning technique (McMahan et al., 2017) to train news recommendation models on decentralized data. In general, these methods usually focus on developing privacy-preserving model training approaches based on decentralized user behavior data for ranking candidate news. However, how to generate candidate news and serve users in a privacy-preserving way remains an open problem. In this paper, we propose a unified news recommendation framework based on federated learning (named Uni-FedRec), which ca"
2021.findings-emnlp.124,2020.acl-main.77,1,0.886971,"u et al., 2021e; Pal et al., 2020), UniFedRec contains a recall stage for personalized candidate news generation and a ranking stage for candidate news ranking. In the recall stage, the user client first locally learns multiple interest representations from clicked news to model diverse Online news platforms usually rely on personalized news recommendation techniques to help users obtain their interested news information (Qi et al., 2021b; Wu et al., 2019c). Existing news recommendation models usually exploit users’ historical behavior data to model user interests for matching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private use"
2021.findings-emnlp.124,P19-1110,1,0.784467,"Missing"
2021.findings-emnlp.124,D19-1493,1,0.858774,"Missing"
2021.findings-emnlp.124,D19-1671,1,0.883767,"Missing"
2021.findings-emnlp.124,N19-1180,1,0.15532,"ggregator method with perturbation noise to better protect private user information encoded in user interest representations. Besides, we collaboratively train both recall and ranking models on the data decentralized in a large number of user clients in a privacy-preserving way. Experiments on two real-world news datasets show that our method can outperform baseline methods and effectively protect user privacy. 1 Introduction 2019c,b, 2020c; Qi et al., 2021a; Ge et al., 2020; Wu et al., 2021c). For example, Okura et al. (2017) employed a GRU network to build user embeddings from browsed news. Wu et al. (2019a) employed an attention network to build user embeddings by aggregating different clicked news. Both of them match candidate news and user interests via the inner product of their embeddings. In short, most of these methods rely on centralized storage of user behavior data to train models and serve users. However, user behavior data is usually highly privacysensitive (Chai et al., 2019), and centrally storing them may arouse users’ concerns on privacy leakage and violate some privacy protection regulations such as GDPR1 . A few methods explore to recommend news in a privacy-preserving way (Qi"
2021.findings-emnlp.124,2020.aacl-main.6,1,0.865683,"atching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private user information encoded in interest representations, we propose an interest decomposer-aggregator method with perturbation noise to synthesize interest representations with a group of basic interest embeddings. In addition, Uni-FedRec utilizes user data decentralized in a large number of user clients to collaboratively train the recall and ranking model in a privacy-preserving way. Extensive experiments on two real-world datasets verify that our method can significantly outperform baseline methods and effectively protect user privacy. In summary, our contributions are a"
2021.findings-emnlp.124,2020.findings-emnlp.174,1,0.838032,"atching candidate news (Wang et al., 2020; Wu et al., 1438 1 https://gdpr-info.eu/ Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1438–1448 November 7–11, 2021. ©2021 Association for Computational Linguistics user interests. These representations are further uploaded to the server to recall a small number of candidate news (e.g., 100) from a large news pool. In the ranking stage, recalled candidate news are distributed to the user client and locally ranked for news personalized display. Bedsides, user interest representations may encode user privacy information. (Wu et al., 2020a). To protect private user information encoded in interest representations, we propose an interest decomposer-aggregator method with perturbation noise to synthesize interest representations with a group of basic interest embeddings. In addition, Uni-FedRec utilizes user data decentralized in a large number of user clients to collaboratively train the recall and ranking model in a privacy-preserving way. Extensive experiments on two real-world datasets verify that our method can significantly outperform baseline methods and effectively protect user privacy. In summary, our contributions are a"
2021.findings-emnlp.124,2020.acl-main.331,1,0.838068,"Missing"
2021.findings-emnlp.280,N19-1423,0,0.234401,"horough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 November 7–11, 2021. ©2021 Association for Computational L"
2021.findings-emnlp.280,P84-1044,0,0.122405,"Missing"
2021.findings-emnlp.280,2020.findings-emnlp.372,0,0.634244,"earned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 November 7–11, 2021. ©2021"
2021.findings-emnlp.280,D16-1139,0,0.0323948,"Missing"
2021.findings-emnlp.280,2021.ccl-1.108,0,0.0964318,"Missing"
2021.findings-emnlp.280,D14-1162,0,0.0890374,"o fitting the self-supervision tasks (e.g., masked token prediction) while the hidden representations of intermediate layers have better generalization ability, which is also validated by (Chi et al., 2021). Fourth, compared with TwinBERT, the results of TinyBERT and NewsBERT are usually better. This 4.2 Performance Evaluation may be because the TwinBERT method only distills In this section, we compare the performance of the teacher model based on the output soft labels, our NewsBERT approach with many baseline meth- while the other two methods can also align the hidods, including: (1) Glove (Pennington et al., 2014), den representations learned by intermediate layers, which is a widely used pre-trained word embed- which can help the student model better imitate the ding. We used Glove to initialize the word embed- teacher model. Fifth, our NewsBERT approach outdings in a Transformer (Vaswani et al., 2017) model performs all other compared baseline methods, and for news topic classification and the NRMS (Wu our further t-test results show the improvements are et al., 2019d) model for news recommendation. (2) significant at p < 0.01 (by comparing the models BERT (Devlin et al., 2019), a popular PLM with bi"
2021.findings-emnlp.280,D19-1441,0,0.313093,"rm many baseline methods for PLM distillation. The main contributions of this work include: • We propose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for"
2021.findings-emnlp.280,2020.acl-main.195,0,0.0179758,"tion. The main contributions of this work include: • We propose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which d"
2021.findings-emnlp.280,2020.acl-main.77,1,0.902044,"technique that can learned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of the 1 Introduction original performance (Sanh et al., 2019; Sun et al., Pre-trained language models (PLMs) like 2019; Wang et al., 2020b; Jiao et al., 2020). For BERT (Devlin et al., 2019) and GPT (Radford example, Sanh et al. (2019) proposed a Distilet al., 2019) have achieved remarkable success in BERT approach, which learns the student model various NLP applications (Liu et al., 2019; Yang from the soft target probabilities of the teacher et al., 2019). These PLMs are usually in huge model by using a distillation loss with softmaxsize with hundreds of millions of parameters (Qiu temperature (Jang et al., 2016), and they regular3285 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3285–3295 Novem"
2021.findings-emnlp.280,2021.findings-acl.188,0,0.0115905,"ropose a momentum distillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s outpu"
2021.findings-emnlp.280,P19-1110,1,0.873346,"Missing"
2021.findings-emnlp.280,D19-1671,1,0.892909,"Missing"
2021.findings-emnlp.280,2021.findings-acl.387,1,0.717538,"d which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s output soft probabilities and hidden st"
2021.findings-emnlp.280,2020.acl-main.331,1,0.95966,"rich textual content (Wang et al., In our approach, we design a teacher-student 2020a). Thus, these applications would benefit a joint learning and distillation framework to collaboratively learn both teacher and student lot from the powerful language understanding abilmodels, where the student model can learn ity of PLMs if they could be incorporated in an from the learning experience of the teacher efficient way, which further has the potential to immodel. In addition, we propose a momentum prove the news reading experience of millions of distillation method by incorporating the gradiusers (Wu et al., 2020). ents of teacher model into the update of student model to better transfer the knowledge Knowledge distillation is a technique that can learned by the teacher model. Thorough expercompress a cumbersome teacher model into a iments on two real-world datasets with three lighter-weight student model by transferring usetasks show that NewsBERT can empower varful knowledge (Hinton et al., 2015; Kim and ious intelligent news applications with much Rush, 2016). It has been employed to compress smaller models. many huge pre-trained language models into much smaller versions and meanwhile keep most of"
2021.findings-emnlp.280,2020.emnlp-main.633,0,0.0112104,"istillation method which uses the gradient of the teacher model to boost the learning of student model in a momentum manner. • Extensive experiments on real-world datasets validate that our method can effectively improve the performance of various intelligent news applications in an efficient way. 2 Related Work In recent years, many researchers explore to use knowledge distillation techniques to compress large-scale PLMs into smaller ones (Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019; Mirzadeh et al., 2020; Sun et al., 2020; Wang et al., 2020b; Jiao et al., 2020; Wang et al., 2021; Xu et al., 2020; Wu et al., 2021a). For example, Tang et al. (2019) proposed a BiLSTMSOFT method that distills the BERT model into a single layer BiLSTM using the distillation loss in downstream tasks. Sanh et al. (2019) proposed a DistilBERT approach, which distills the student model at the pre-training stage using the distillation loss and a cosine embedding loss that aligns the hidden states of teacher and student models. Sun et al. (2019) proposed a patient knowledge distillation method for BERT compression named BERT-PKD, which distills the student model by learning from teacher’s output soft probabilit"
2021.findings-emnlp.280,N16-1174,0,0.0194224,".1 Teacher-Student Joint Learning and Distillation Framework several Transformer (Vaswani et al., 2017) layers. We assume that the teacher model has N K Transformer layers on the top of the embedding layer and the student model contains N Transformer layers on the embedding layer. Thus, the inference speed of the student model is approximately K times faster than the teacher. We first use the teacher and student models to separately process the input news text (denoted as x) through their Transformer layers and obtain the hidden representation of each token. We use a shared attentive pooling (Yang et al., 2016) layer (with parameter set Θp ) to convert the hidden representation sequences output by the teacher and student models into unified news embeddings, and finally use a shared dense layer (with parameter set Θd ) to predict the classification probability scores based on the news embedding. By sharing the parameters of the top pooling and dense layers, the student model can get richer supervision information from the teacher, and the teacher can also be aware of student’s learning status. Thus, the teacher and student can be reciprocally learned by sharing useful knowledge encoded by them, which"
2021.naacl-main.166,D15-1075,0,0.0274437,"tecture. 4 Experiments 4.1 Datasets and Experimental Settings Our experiments are conducted on five benchmark datasets for different tasks. Four of them are benchmark NLP datasets. The first one is AG’s News2 (denoted as AG), which is a news topic classification dataset. The second one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is a dataset for review rating prediction. The third one is Stanford Sentiment Treebank (Socher et al., 2013) (denoted as SST). We use the binary classification version of this dataset. The fourth one is Stanford Natural Language Inference (Bowman et al., 2015) (SNLI) dataset, which is a widely used natural language inference dataset. The detailed statistics of these datasets are summarized in Table 1. In addition, we also conduct experiments on a benchmark news recommendation dataset named MIND (Wu et al., 2020c), aiming to validate the effectiveness of our approach in both text and user modeling. It contains the news impression logs of 1 million users from Microsoft News3 from October 12 to November 22, 2019. The training set contains the logs in the first five weeks except those on the last day which are used for validation. The rest logs are use"
2021.naacl-main.166,P19-1285,0,0.0499028,"Missing"
2021.naacl-main.166,N19-1423,0,0.167352,"anilla Transformer and its several variants. the precise information of the real distance, which may not be beneficial for the Transformer to cap1 Introduction ture word orders and the context relations. Transformer (Vaswani et al., 2017) has achieved In this paper, we propose a distance-aware Transhuge success in the NLP field in recent former (DA-Transformer), which can explicitly exyears (Kobayashi et al., 2020). It serves as the ba- ploit real token distance information to enhance sic architecture of various state-of-the-art models context modeling by leveraging the relative dislike BERT (Devlin et al., 2019) and GPT (Rad- tances between different tokens to re-scale the raw ford et al., 2019), and boosts the performance of attention weights before softmax normalization. 2059 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059–2068 June 6–11, 2021. ©2021 Association for Computational Linguistics More specifically, since global and local context modeling usually have different distance preferences, we propose to learn a different parameter in different attention heads to weight the token distances,"
2021.naacl-main.166,2020.emnlp-main.574,0,0.0219409,"distance-aware way. DA-Transformer can effectively improve the However, the distance or relative position embedperformance of many tasks and outperform the dings used by these methods usually cannot keep vanilla Transformer and its several variants. the precise information of the real distance, which may not be beneficial for the Transformer to cap1 Introduction ture word orders and the context relations. Transformer (Vaswani et al., 2017) has achieved In this paper, we propose a distance-aware Transhuge success in the NLP field in recent former (DA-Transformer), which can explicitly exyears (Kobayashi et al., 2020). It serves as the ba- ploit real token distance information to enhance sic architecture of various state-of-the-art models context modeling by leveraging the relative dislike BERT (Devlin et al., 2019) and GPT (Rad- tances between different tokens to re-scale the raw ford et al., 2019), and boosts the performance of attention weights before softmax normalization. 2059 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2059–2068 June 6–11, 2021. ©2021 Association for Computational Linguistics Mor"
2021.naacl-main.166,N19-1238,0,0.0437208,"Missing"
2021.naacl-main.166,N18-2074,0,0.360734,"computed by ever, in contrast to recurrent and convolutional neuthe relevance between attention query and key. ral networks, it is difficult for vanilla TransformConcretely, in different self-attention heads ers to be aware of the token distances (Shaw et al., the relative distance between each pair of to2018), which are usually important cues for context kens is weighted by different learnable pamodeling. Thus, several works explored to incorrameters, which control the different preferporate token distance information into Transformer. ences on long- or short-term information of For example, Shaw et al. (2018) proposed to comthese heads. Since the raw weighted real disbine the embeddings of relative positions with attances may not be optimal for adjusting selfattention weights, we propose a learnable sigtention key and value in the self-attention network. moid function to map them into re-scaled coefThey restricted the maximum relative distance to ficients that have proper ranges. We first clip only keep the precise relative position information the raw self-attention weights via the ReLU within a certain distance. Yan et al. (2019) profunction to keep non-negativity and introduce posed a variant o"
2021.naacl-main.166,D13-1170,0,0.0029176,"ers, which are usually ignorable compared with the projection matrices (i) like WQ . Thus, our approach inherits the efficiency of the Transformer architecture. 4 Experiments 4.1 Datasets and Experimental Settings Our experiments are conducted on five benchmark datasets for different tasks. Four of them are benchmark NLP datasets. The first one is AG’s News2 (denoted as AG), which is a news topic classification dataset. The second one is Amazon Electronics (He and McAuley, 2016) (denoted as Amazon), which is a dataset for review rating prediction. The third one is Stanford Sentiment Treebank (Socher et al., 2013) (denoted as SST). We use the binary classification version of this dataset. The fourth one is Stanford Natural Language Inference (Bowman et al., 2015) (SNLI) dataset, which is a widely used natural language inference dataset. The detailed statistics of these datasets are summarized in Table 1. In addition, we also conduct experiments on a benchmark news recommendation dataset named MIND (Wu et al., 2020c), aiming to validate the effectiveness of our approach in both text and user modeling. It contains the news impression logs of 1 million users from Microsoft News3 from October 12 to Novembe"
2021.naacl-main.166,D19-1145,0,0.0199521,"nsformer portant basic neural architecture of various state-of- Instead of directly using the sinusoidal position the-art NLP models like BERT (Devlin et al., 2019) embedding (Vaswani et al., 2017) or the absolute and GPT (Radford et al., 2019). The core compo- position embedding (Devlin et al., 2019), several nent of Transformer is multi-head self-attention. It variants of the Transformer explore to use the relahas h attention heads, where the parameters in each tive positions to better model the distance between head are independent. For the i-th attention head, contexts (Shaw et al., 2018; Wang et al., 2019; Dai it takes a matrix H as the input. It first uses three et al., 2019; Yan et al., 2019). For example, Shaw 2060 et al. (2018) proposed to add the embeddings of relative positions to the attention key and value to capture the relative distance between two tokens. They only kept the precise distance within a certain range by using a threshold to clip the maximum distance to help generalize to long sequences. Dai et al. (2019) proposed Transformer-XL, which uses another form of relative positional encodings that integrate content-dependent positional scores and a global positional score into"
2021.naacl-main.166,D19-1671,1,0.849948,", 2014) embeddings for word embedding initialization.4 The number of attention head is 16, and the output dimension of each attention is 16. We use one Transformer layer in all experiments. On the AG, SST and SNLI datasets, we directly apply Transformer-based methods to the sentences. On the Amazon dataset, since reviews are usually long documents, we use Transformers in a hierarchical way by learning sentence representations from words via a word-level Transformer first and then learning document representations from sentences via a sentence-level Transformer. On the MIND dataset, following (Wu et al., 2019, 2020b) we also use a hierarchical model architecture that first learns representations of historical clicked news and candidate news from their titles with a word-level Transformer, then learns user representations from the representations of clicked news with a news-level Transformer, and final matches user and candidate news representations to compute click scores.5 We use the same model training strategy with negative sampling techniques as NRMS (Wu et al., 2019). On all datasets we use Adam (Kingma and Ba, 2015) as the optimization algorithm and the learning rate is 1e-3. On the AG, Amaz"
2021.naacl-main.166,2020.aacl-main.6,1,0.888833,"g & BNRist, Tsinghua University, Beijing 100084, China ‡ Microsoft Research Asia, Beijing 100080, China {wuchuhan15,wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn Abstract many tasks like text generation (Koncel-Kedziorski et al., 2019), machine translation (Vaswani et al., Transformer has achieved great success in the 2017), and reading comprehension (Xu et al., 2019). NLP field by composing various advanced Thus, the improvement on the Transformer archimodels like BERT and GPT. However, Transtecture would be beneficial for many NLP-related former and its existing variants may not be fields (Wu et al., 2020a). optimal in capturing token distances because A core component of Transformer is multi-head the position or distance embeddings used by these methods usually cannot keep the precise self-attention, which is responsible for modeling information of real distances, which may not the relations between contexts (Yang et al., 2019; be beneficial for modeling the orders and reGuo et al., 2019). However, self-attention is lations of contexts. In this paper, we proposition-agnostic since it does not distinguish the pose DA-Transformer, which is a distanceorders of inputs. Thus, in the vanilla Transf"
2021.naacl-main.166,2020.acl-main.331,1,0.858287,"g & BNRist, Tsinghua University, Beijing 100084, China ‡ Microsoft Research Asia, Beijing 100080, China {wuchuhan15,wufangzhao}@gmail.com, yfhuang@tsinghua.edu.cn Abstract many tasks like text generation (Koncel-Kedziorski et al., 2019), machine translation (Vaswani et al., Transformer has achieved great success in the 2017), and reading comprehension (Xu et al., 2019). NLP field by composing various advanced Thus, the improvement on the Transformer archimodels like BERT and GPT. However, Transtecture would be beneficial for many NLP-related former and its existing variants may not be fields (Wu et al., 2020a). optimal in capturing token distances because A core component of Transformer is multi-head the position or distance embeddings used by these methods usually cannot keep the precise self-attention, which is responsible for modeling information of real distances, which may not the relations between contexts (Yang et al., 2019; be beneficial for modeling the orders and reGuo et al., 2019). However, self-attention is lations of contexts. In this paper, we proposition-agnostic since it does not distinguish the pose DA-Transformer, which is a distanceorders of inputs. Thus, in the vanilla Transf"
2021.naacl-main.166,N19-1242,0,0.0558815,"Missing"
2021.naacl-main.166,D14-1162,0,0.108661,"19. The training set contains the logs in the first five weeks except those on the last day which are used for validation. The rest logs are used for test. The key statistics of this dataset are summarized in Table 2. Dataset AG Amazon SST SNLI # Train 108k 40k 8k 55k # Dev. 12k 5k 1k 10k # Test 7.6k 5k 2k 10k # Classes 4 5 2 2 Avg. len. 44 133 19 22 Table 1: Statistics of AG, Amazon, SST and SNLI datasets. # Users # News # Impressions 1,000,000 161,013 500,000 Avg. title len. # Click samples # Non-click samples 11.52 5,597,979 136,162,621 In our experiments, we use the 300-dimensional Glove (Pennington et al., 2014) embeddings for word embedding initialization.4 The number of attention head is 16, and the output dimension of each attention is 16. We use one Transformer layer in all experiments. On the AG, SST and SNLI datasets, we directly apply Transformer-based methods to the sentences. On the Amazon dataset, since reviews are usually long documents, we use Transformers in a hierarchical way by learning sentence representations from words via a word-level Transformer first and then learning document representations from sentences via a sentence-level Transformer. On the MIND dataset, following (Wu et a"
2021.naacl-main.453,W13-3515,0,0.206677,"many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phrases, but the implicit relation “Live in” 1 Introduction is not expressed explicitly. Unfortunately, existing Relational triple extraction is defined as automat- methods usually ignored implicit triples (Zhu et al., 2019), which will cause serious incompleteness of ically recognizing semantic relations with triple structures (subject, relation, object) among multi- the constructed KGs and performance degradation of downstream tasks (Angeli and Manning, 2013; ple entities in a sentence. It is a critical task for constructing Knowledge Graphs (KGs) from unla- Jia et al., 2020; Jun et al., 2020). beled corpus (Dong et al., 2014). Our work is motivated by several observations. Early work of relational triple extraction ap- First, other relational triples within a sentence proplied pipeline methods (Zelenko et al., 2003; Chan vide supplementary information for discovering and Roth, 2011), which ran entity recognition and entity pairs that may have implicit relational conrelation classification separately. However, such nections. For example, in Figur"
2021.naacl-main.453,P11-1056,0,0.0284052,"lementary evidence for discovering entity pairs with implicit relational connections. Second, the relation types of the implicit connections need to be identified through real-world reasoning patterns. In this paper, we propose a unified framework for the joint extraction of explicit and implicit rela2 Related Work tional triples. We propose a binary pointer network Early work of relational triple extraction addressed to sequentially extract overlapping relational triples this task in a pipelined manner (Zelenko et al., and externally keep the information of predicted 2003; Zhou et al., 2005; Chan and Roth, 2011; triples for exploring implicitly connected entity Gormley et al., 2015). They first ran named entity pairs. We also propose to introduce real-world rearecognition to identify all entities and then classi- soning patterns in our model to help derive the relafied relations between all entity pairs. However, tion type of implicit triples with a relation network. these pipelined methods usually suffered from er- Experimental results on several benchmark datasets ror propagation problem and failed to capture the demonstrate the effectiveness of our method. 5695 Extracted Triples (i=n) Linear Gate"
2021.naacl-main.453,N19-1423,0,0.0197871,"0 for others. We also train the entity tagger with a cross-entrory loss: 1 4 Table 1: Statistics of evaluation datasets. 4.2 Experimental Settings We determine the hyper-parameters on the validation sets. We use the pre-trained GloVe (Pennington et al., 2014) embeddings as w. We adopt a one-layer CNN with dc = 30 channels to learn c from 30-dimensional randomly-initialized character embeddings. We choose the state-of-the-art RoBERTaLARGE (Liu et al., 2019) model3 as the pre-tained LM. For a fair comparison with previous methods, we also conduct experiments and report the scores with BERTBASE (Devlin et al., 2019). We set din (Equation 1) as 300. The hidden dimensions of the encoder dE and the entity tagger dT are both 200. The dimensions of entity tag embeddings de and relation type embeddings dR are set as 50 and 200, respectively. The projection dimension dP of the RN is set as 500. 2 As mentioned in (Wang et al., 2020), this number is miswritten as 246 in (Wei et al., 2020) and (Yu et al., 2019). Here we quote the correct number from (Wang et al., 2020). 3 https://github.com/huggingface/transformers 5698 NYT Method WebNLG Prec. Rec. F1 Prec. Rec. F1 NovelTagging (Zheng et al., 2017) CopyRE (Zeng et"
2021.naacl-main.453,P19-1136,0,0.0492987,"Missing"
2021.naacl-main.453,P17-1017,0,0.0299237,"the binary scores of candidate triples. We denote our Reasoning pattern enhanced BPtrNet model as R-BPtrNet. Note that we use quite simple formulas for fφ and gθ because our contribution focuses on the effectiveness of introducing relational reasoning patterns for this task rather than the model structure. Exploration for more complex structures will be left for future work. 3.3 4.1 Experiments Datasets and Evaluation Metrics We evaluate our method on two benchmark datasets. NYT (Riedel et al., 2010) consists of sentences from the New York Times corpus and contains 24 relation types. WebNLG (Gardent et al., 2017) was created for natural language generation task. It contains 171 relation types2 and was adopted for relational triple extraction by (Zeng et al., 2018). We split the sentences into three categories: Normal, SingleEntityOverlap (SPO) and EntityPairOverlap (EPO) following Zeng et al. (2018). The statistics of the two datasets are shown in Table 1. Following previous work (Zeng et al., 2018; Wei et al., 2020; Wang et al., 2020), an extracted relational triple is regarded as correct only if the relation and the heads of both subject and object are all correct. We report the standard micro preci"
2021.naacl-main.453,D15-1205,0,0.0627378,"Missing"
2021.naacl-main.453,C16-1239,0,0.100715,"on network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method. Locate in Work for … Mark Spencer, a designer of Digium, a company in Huntsville … Live in Person Organization Location Explict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by t"
2021.naacl-main.453,P14-1038,0,0.211649,"iples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method. Locate in Work for … Mark Spencer, a designer of Digium, a company in Huntsville … Live in Person Organization Location Explict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit"
2021.naacl-main.453,2021.ccl-1.108,0,0.0301336,"Missing"
2021.naacl-main.453,P16-1105,0,0.0215395,"xtracted triples. • To enhance the relation type inference of implicitly connected entity pairs, we propose to introduce relational reasoning patterns, captured with a RN, to augment our model. • We conduct experiments on several benchmark datasets and the experimental results demonstrate the validity of our method. interactions between entities and relations. To overcome these drawbacks, recent research focused on jointly extracting entities and relations, including feature-based models (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017) and neural network-based models (Gupta et al., 2016; Miwa and Bansal, 2016; Zheng et al., 2017). For example, Ren et al. (2017) proposed to jointly embed entities, relations, text features and type labels into two low-dimensional spaces. Miwa and Bansal (2016) proposed a joint model containing two long-short term memories (LSTMs) (Gers et al., 2000) with shared parameters. Zheng et al. (2017) proposed to extract relational triples directly by transforming this task into a sequence tagging problem, whose tags contain the information of entities and the relations they hold. However, they only assigned one label for each word, which means that this method failed to ext"
2021.naacl-main.453,D14-1162,0,0.0887317,"000 5019 703 Training and Inference We calculate the triple loss of a sentence as a binary cross entropy over valid candidate triples Tv , whose subject and object are different entities (or the end words of different entities):  X 1 yt log st +(1−yt ) log(1−st ) Lt = − |Tv | t∈Tv (9) where st is the score of the candidate triple t, yt = 1 for gold triples and 0 for others. We also train the entity tagger with a cross-entrory loss: 1 4 Table 1: Statistics of evaluation datasets. 4.2 Experimental Settings We determine the hyper-parameters on the validation sets. We use the pre-trained GloVe (Pennington et al., 2014) embeddings as w. We adopt a one-layer CNN with dc = 30 channels to learn c from 30-dimensional randomly-initialized character embeddings. We choose the state-of-the-art RoBERTaLARGE (Liu et al., 2019) model3 as the pre-tained LM. For a fair comparison with previous methods, we also conduct experiments and report the scores with BERTBASE (Devlin et al., 2019). We set din (Equation 1) as 300. The hidden dimensions of the encoder dE and the entity tagger dT are both 200. The dimensions of entity tag embeddings de and relation type embeddings dR are set as 50 and 200, respectively. The projection"
2021.naacl-main.453,C10-2160,0,0.0656999,"icit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method. Locate in Work for … Mark Spencer, a designer of Digium, a company in Huntsville … Live in Person Organization Location Explict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there a"
2021.naacl-main.453,D19-1035,0,0.0600746,"1) as 300. The hidden dimensions of the encoder dE and the entity tagger dT are both 200. The dimensions of entity tag embeddings de and relation type embeddings dR are set as 50 and 200, respectively. The projection dimension dP of the RN is set as 500. 2 As mentioned in (Wang et al., 2020), this number is miswritten as 246 in (Wei et al., 2020) and (Yu et al., 2019). Here we quote the correct number from (Wang et al., 2020). 3 https://github.com/huggingface/transformers 5698 NYT Method WebNLG Prec. Rec. F1 Prec. Rec. F1 NovelTagging (Zheng et al., 2017) CopyRE (Zeng et al., 2018) CopyRERL (Zeng et al., 2019) GraphRel (Fu et al., 2019) ETL-Span (Yu et al., 2019) CopyMTL (Zeng et al., 2020) WDec (Nayak and Ng, 2020) CGTUniLM (Ye et al., 2020) C AS R ELLSTM (Wei et al., 2020) C AS R ELBERT (Wei et al., 2020) TPLinkerLSTM (Wang et al., 2020) TPLinkerBERT (Wang et al., 2020) 62.4 72.8 77.9 63.9 84.9 75.7 94.5 94.7 84.2 89.7 83.8 91.3 31.7 69.4 67.2 60.0 72.3 68.7 76.2 84.2 83.0 89.5 83.4 92.5 42.0 71.1 72.1 61.9 78.1 72.0 84.4 89.1 83.6 89.6 83.6 91.9 52.5 60.9 63.3 44.7 84.0 58.0 92.9 86.9 93.4 90.8 91.8 19.3 61.1 59.9 41.1 91.5 54.9 75.6 80.6 90.1 90.3 92.0 28.3 61.0 61.6 42.9 87.6 56.4 83.4 83.7 91"
2021.naacl-main.453,P18-1047,0,0.228397,"ict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phrases, but the implicit relation “Live in” 1 Introduction is not expressed explicitly. Unfortunately, existing Relational triple extraction is defined as automat- methods usually ignored implicit triples (Zhu et al., 20"
2021.naacl-main.453,2020.coling-main.138,0,0.230316,"d implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phrases, but the implicit relation “Live in” 1 Introduction is not expressed explicitly. Unfortunately, existing Relational triple extraction is defined as automat- methods usually ignored implicit triples (Zhu et al., 2019), which will cause serious incompleteness of ically recogniz"
2021.naacl-main.453,P17-1113,0,0.348376,"ct experiments on several benchmark datasets, and the results prove the validity of our method. Locate in Work for … Mark Spencer, a designer of Digium, a company in Huntsville … Live in Person Organization Location Explict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phr"
2021.naacl-main.453,2020.acl-main.136,0,0.463045,"er of Digium, a company in Huntsville … Live in Person Organization Location Explict Triple Implicit Triple Figure 1: An example of explicit and implicit relational triples. Italic phrases are key relational expressions corresponding to the explicit relational triples. with feature-based methods (Yu and Lam, 2010; Li and Ji, 2014; Ren et al., 2017). Afterward, neural network-based models were proposed to eliminate hand-crafted features (Gupta et al., 2016; Zheng et al., 2017). More recently, several methods were proposed to extract overlapping triples, such as tagging-based (Dai et al., 2019; Wei et al., 2020), graph-based (Wang et al., 2018; Fu et al., 2019), copy-based (Zeng et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phrases, but the implicit relation “Live in” 1 Introduction is not expressed explicitly. Unfortunately, existing Relational triple extractio"
2021.naacl-main.453,P19-1128,0,0.230987,"g et al., 2018, 2019, 2020) and token pair linking models (Wang et al., 2020). Existing models achieved considerable success on extracting explicit triples which have direct relational expressions in the sentence. However, there are many implicit relational triples that are not explicitly expressed. For example, in Figure 1, the explicit triples are strongly indicated by the key relational phrases, but the implicit relation “Live in” 1 Introduction is not expressed explicitly. Unfortunately, existing Relational triple extraction is defined as automat- methods usually ignored implicit triples (Zhu et al., 2019), which will cause serious incompleteness of ically recognizing semantic relations with triple structures (subject, relation, object) among multi- the constructed KGs and performance degradation of downstream tasks (Angeli and Manning, 2013; ple entities in a sentence. It is a critical task for constructing Knowledge Graphs (KGs) from unla- Jia et al., 2020; Jun et al., 2020). beled corpus (Dong et al., 2014). Our work is motivated by several observations. Early work of relational triple extraction ap- First, other relational triples within a sentence proplied pipeline methods (Zelenko et al.,"
D19-1493,P19-1033,1,0.361303,"features extracted from the news reading, web browsing, and searching behaviors of users. However, these methods usually rely on manual feature engineering, which necessities massive domain knowledge and time to design. In addition, these methods cannot exploit contextual information in the behavior records of users, which may be insufficient to learn accurate user representations. 4875 In recent years, several deep learning based methods are proposed for news recommendation (Okura et al., 2017; Wang et al., 2018; Khattar et al., 2018; Kumar et al., 2017; Zheng et al., 2018; Wu et al., 2019b; An et al., 2019; Wu et al., 2019c,a; Zhu et al., 2019). For example, Okura et al. (2017) proposed to learn news representations from news bodies via denoising autoencoders, and learn user representations from the news browsing sequence via a GRU network. Wang et al. (2018) proposed to learn news representations from news titles via a knowledge-aware CNN network, which can incorporate useful entity information from knowledge graphs. Wu et al. (2019b) proposed to learn news representations from news titles, and apply personalized attention mechanism at both word- and news-level to generate representations of n"
D19-1493,D14-1162,0,0.0847221,"Missing"
D19-1493,D14-1181,0,0.00403986,"sentation, user representation and click predictor. Next, we introduce the details of each module. 3.1 News Representation Learning The news representation module is used to learn representations of news articles from their titles. It contains three layers. The first one is a word embedding layer, which is used to convert a news title from a sequence of words into a sequence of low-dimensional semantic vectors. Denote a news title with M words as [w1 , w2 , ..., wM ]. It is converted into a vector sequence [e1 , e2 , ..., eM ] via a pre-trained embedding matrix. The second one is a CNN layer (Kim, 2014). (1) r = M X αiw ci . (2) i=1 3.2 User Representation Learning The user representation module is used to learn the representations of users from different kinds of user behaviors, e.g., web searching, news click and webpage browsing. Since these user behaviors usually have different characteristics, simply aggregate them together into a long document may not be optimal for news representation. Thus, we propose an attentive multi-view learning framework to learn unified user representations by incorporating these three kinds of user behavior information as different views of news. The first vi"
D19-1493,P19-1110,1,0.320447,"Missing"
D19-1494,N19-1180,1,0.449453,"eview texts to enhance the learning of latent user and item factors from the rating matrix via non-negative matrix factorization (NMF). However, these methods only extract topics from reviews, and cannot effectively utilize the contexts and word orders in reviews, both of which are important for learning accurate user and item representations. In recent years, several deep learning based methods proposed to learn user and item representations from original review texts (Zhang et al., 2016; Zheng et al., 2017; Catherine and Cohen, 2017; Seo et al., 2017b,a; Chen et al., 2018; Tay et al., 2018; Wu et al., 2019). For example, Zheng et al. (2017) proposed a DeepCoNN approach to learn user and item representations from reviews via CNN networks. Catherine and Cohen (2017) proposed a TransNets approach to use CNNs to learn user and item representations. In their approach, these representations are regularized to be close to the representations of reviews from the target user-item pairs. Seo et al. (2017b) proposed to use CNN networks to learn user and item representations, and applied a word-level attention network to select important words. Chen et al. (2018) proposed to learn review representations usi"
D19-1671,P19-1033,1,0.301369,"l., 2010). Learning accurate representations of news and users are two core tasks in news recommendation (Okura et al., 2017). Several deep learning based methods have been proposed for these tasks (?Kumar et al., 2017; Khattar et al., 2018; 2 https://news.google.com/ https://www.msn.com/en-us/news James Harden's incredible heroics lift Rockets over Warriors in overtime Figure 1: Several news browsed by an example user. Orange and green dashed lines represent the interactions between words and news respectively. Introduction 1 Weather forecast This Week Wu et al., 2019b,c,a; Zhu et al., 2019; An et al., 2019). For example, Okura et al. (2017) proposed to learn news representations from news bodies via auto-encoders, and learn representations of users from their browsed news via GRU. However, GRU is quite time-consuming, and their method cannot capture the contexts of words. Wang et al. (2018) proposed to learn news representations from news titles via a knowledge-aware convolutional neural network (CNN), and learn representations of users based on the similarities between candidate news and their browsed news. However, CNN cannot capture the long-distance contexts of words, and their method cannot"
D19-1671,D14-1162,0,0.0862588,"Missing"
D19-1671,P19-1110,1,0.561777,"Missing"
D19-1671,W18-5909,1,0.785141,"6389–6394, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ??1 News Encoder The news encoder module is used to learn news representations from news titles. It contains three layers. The first one is word embedding, which is used to convert a news title from a sequence of words into a sequence of low-dimensional embedding vectors. Denote a news title with M words as [w1 , w2 , ..., wM ]. Through this layer it is converted into a vector sequence [e1 , e2 , ..., eM ]. The second layer is a word-level multi-head self-attention network (Vaswani et al., 2017; Wu et al., 2018). The interactions between words are important for learning news representations. For example, in the news title “Rockets Ends 2018 with a Win”, the interaction between “Rockets” and “Win” is useful for understanding this news, and such long-distance interactions usually cannot be captured by CNN. In addition, a word may interact with multiple words in the same news. For instance, in above example the word “Rockets” has interactions with both “Ends” and “Win”. Thus, we propose to use multi-head self-attention to learn contextual representations of words by capturing their interactions. The rep"
I17-4007,W10-0208,0,0.0351928,"@gmail.com Abstract value, such as dimensional sentiment analysis. Evaluating sentiment in valence-arousal (VA) space was first proposed by Ressel (1980). As shown in Figure 1, the valence dimension represents the degree of positive or negative sentiment, while the arousal dimension indicates the intensity of sentiment. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015; Wang et al., 2016) or texts(Kim et al., 2010; Paltoglou et al., 2013). Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of Chinese are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valencearousal ratings for Chinese affective words and phrases automatically. In this task, we propose an approach using a densely connected LSTM network and word features to identify dimensional sentiment on valence a"
I17-4007,baccianella-etal-2010-sentiwordnet,0,0.0464147,"Missing"
I17-4007,N16-1066,0,0.0388707,"Missing"
I17-4007,P15-2129,0,0.093201,".edu.cn 2 Microsoft Research Asia, wufangzhao@gmail.com Abstract value, such as dimensional sentiment analysis. Evaluating sentiment in valence-arousal (VA) space was first proposed by Ressel (1980). As shown in Figure 1, the valence dimension represents the degree of positive or negative sentiment, while the arousal dimension indicates the intensity of sentiment. Based on this two-dimensional representation, any affective state can be represented as a point in the VA coordinate plane by determining the degrees of valence and arousal of given words (Wei et al., 2011; Malandrakis et al., 2011; Yu et al., 2015; Wang et al., 2016) or texts(Kim et al., 2010; Paltoglou et al., 2013). Predicting valence-arousal ratings for words and phrases is very useful for constructing affective resources for dimensional sentiment analysis. Since the existing valence-arousal resources of Chinese are mainly in word-level and there is a lack of phrase-level ones, the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) task aims to predict the valencearousal ratings for Chinese affective words and phrases automatically. In this task, we propose an approach using a densely connected LSTM network and word features"
P16-1029,P11-1014,0,0.479648,"n explored. Glorot et al. (2011) proposed a sentiment domain adaptation method based on a deep learning technique, i.e., Stacked Denoising Auto-encoders. The core idea of their method is to learn a high-level representation that can capture generic concepts using the unlabeled data from multiple domains. Yoshida et al. (2011) proposed a probabilistic generative model for cross-domain sentiment classification with multiple source and target domains. In their method, each word is assigned three attributes, i.e., the domain label, the domain dependence/independence label, and sentiment polarity. Bollegala et al. (2011) proposed to construct a sentiment sensitive thesaurus for cross-domain sentiment classification using data from multiple source domains. This thesaurus is used to expand the feature vectors for both training and classification. However, the similarities between target domain and different source domains are not considered in these methods. In addition, although unlabeled data is utilized in these methods, the useful word-level sentiment knowledge in the unlabeled target domain data is not exploited. Related work Sentiment classification is widely known as a domain-dependent task, since differ"
P16-1029,P07-1056,0,0.817903,"eir sentiments in Book domain. In addition, the same word may convey different sentiments in different domains. For example, in Electronics domain “easy” is usually used in positive reviews, e.g., “this digital camera is easy to use.” However, it is frequently used as a negative word in Movie domain. For instance, “the ending of this movie is easy to guess.” Thus, the sentiment classifier trained in one domain usually cannot be applied to another domain directly (Pang and Lee, 2008). In order to tackle this problem, sentiment domain adaptation has been widely studied (Liu, 2012). For example, Blitzer et al. (2007) proposed to compute the correspondence among features from different domains using their associations with pivot features based on structural correspondence learning (SCL). Pan et al. (2010) proposed a spectral feature alignment (SFA) algorithm to align the domain-specific words from different domains in order to reduce the gap between source and target domains. However, all of these methods transfer sentiment information from only one source domain. When the source and target domains have significant difference in feature distributions, the adaptation performance will heavily decline. In som"
P16-1029,P11-1013,0,0.0816587,"fferent. For example, “boring” ∗ Corresponding author. 301 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 301–310, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics labels. Then the correspondence among features from source and target domains is computed using their associations with pivot features. In order to reduce the gap between source and target domains, Pan et al. (2010) proposed a spectral feature alignment (SFA) algorithm to align the domainspecific sentiment words from different domains into clusters. He et al. (2011) proposed to extract polarity-bearing topics using joint sentiment-topic (JST) model to expand the feature representations of texts from both source and target domains. Li et al. (2009) proposed to transfer sentiment knowledge from source domain to target domain using nonnegative matrix factorization. A common shortcoming of above methods is that if the source and target domains have significantly different distributions of sentiment expressions, then the domain adaptation performance will heavily decline (Li et al., 2013). adaptation. At the training stage, we extract two kinds of sentiment m"
P17-1156,P07-1056,0,0.203329,"the help of a small number of labeled samples which are selected and annotated in an active learning mode, as well as the domain-specific sentiment similarities among words mined from unlabeled samples of target domain. A unified model is proposed to fuse different types of sentiment information and train sentiment classifier for target domain. Extensive experiments on benchmark datasets show that our approach can train accurate sentiment classifier with less labeled samples. 1 Introduction Sentiment classification is widely known as a domain-dependent problem (Liu, 2012; Pang and Lee, 2008; Blitzer et al., 2007; Pan et al., 2010). This is because different domains usually have many different sentiment expressions. For example, “lengthy” and “boring” are popularly used in Book domain to express negative sentiment. However, they are rare in Kitchen appliance domain. Moreover, the same word or phrase may convey ∗ Corresponding author. different sentiments in different domains. For instance, “unpredictable” is frequently used to express positive sentiment in Movie domain (e.g., “The plot of this movie is fun and unpredictable”). However, it tends to be used as a negative word in Kitchen appliance domain"
P17-1156,P11-1014,0,0.0438223,"while t &lt; N do 7: t = t + 1. 8: Compute the uncertainty score of each sample xi in U (Eq. (4)). 9: Compute the informativeness score of each sample xi in U (Eq. (6)). 10: Select x∗ from U which has the highest informativeness score. 11: Annotate x∗ and obtain its sentiment label y. 12: L = L + {x∗ , y}, U = U − x∗ . 13: Update sentiment classifier w according to Eq. (7). 14: end while Datasets The dataset used in our experiments is the Amazon product review dataset1 collected by Blitzer et al. (2007), which is widely used in sentiment analysis and domain adaptation research (Pan et al., 2010; Bollegala et al., 2011). This dataset contains product reviews in four domains, i.e., Book, DVD, Electronics, and Kitchen appliances. In each domain, 1,000 positive and 1,000 negative reviews as well as a large number of unlabeled samples are included. The detailed statistics of this dataset are summarized in Table 1. positive negative unlabeled Book 1,000 1,000 973,194 DVD 1,000 1,000 122,438 Electronics 1,000 1,000 21,009 Kitchen 1,000 1,000 17,856 Table 1: The statistics of the Amazon dataset. Following many previous works (Blitzer et al., 2007; Bollegala et al., 2011), unigrams and bigrams were used to build fea"
P17-1156,D09-1062,0,0.777311,"ormation in sentiment lexicons to target domain with the help of a small number of labeled samples which are selected and annotated in an active learning mode. Since the sentiment words in generalpurpose sentiment lexicons usually convey consistent sentiment polarities in different domains, and the actively selected labeled samples contain rich domain-specific sentiment information of target domain, our approach can effectively reduce the risk of negative transfer. The usefulness of labeled samples from target domain in sentiment domain adaptation has been observed by previous research works (Choi and Cardie, 2009; Chen et al., 2011; Li et al., 2013; Wu et al., 2016). For example, Choi and Cardie (2009) proposed to adapt a sentiment lexicon to a specific domain by exploiting both the relations among words which co-occur in the same sentiment expressions and the relations between words and labeled sentiment expressions. However, the 1702 labeled samples used in these methods are randomly selected, while in our approach we actively select informative samples from target domain to annotate. Thus, our approach has the potential to reduce the manual annotation effort. 2.2 Active Learning Active learning is"
P17-1156,D16-1057,0,0.0459369,"nt similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high sentiment similarity score according to Eq. (2). The c is also [0, 1]. Denote Sc ∈ RD×D range of Si"
P17-1156,P97-1023,0,0.0948276,"ntiment label. In this paper we focus on binary sentiment classification and yi ∈ {+1, −1}. In addition, we select log loss for f . Thus, f (xi , yi , w) = log(1 + exp(−yi wT xi )). Besides, we use S ∈ RD×D to represent the sentiment similarities among words extracted from unlabeled samples of target domain. 3.2 Domain-Specific Sentiment Similarities Next we introduce the extraction of domainspecific sentiment similarities among words from unlabeled samples of target domain. Two types of similarities are extracted in this paper. The first one is based on syntactic rules, which is inspired by (Hatzivassiloglou and McKeown, 1997; Huang et al., 2014; Wu and Huang, 2016). If two words have the same POS-tag such as adjective, verb, and adverb, and they are connected by coordinating conjunction “and” in the same sentence, then we regard they convey the same sentiment polarity. In 1703 addition, if two words are connected by adversative conjunction “but” and have the same POS-tag, then they are assumed to have opposite sentiment polarities. Denote Sr ∈ RD×D as the sentiment similarities extracted from unlabeled samples according to syntactic rules, and the similarity score between words i and j is defined as: r Si,j = s o"
P17-1156,P11-1013,0,0.082091,"erformance of directly applying a general sentiment classifier or a sentiment classifier trained in other domains to target domain is usually suboptimal. Since there are a large number of domains in user-generated content, it is impractical to manually annotate enough samples for each domain to train an accurate domain-specific sentiment classifier. Thus, sentiment domain adaptation, which transfers the sentiment classifier trained in a source domain with sufficient labeled data to a target domain with no or scarce labeled data, has been widely studied (Blitzer et al., 2007; Pan et al., 2010; He et al., 2011; Glorot et al., 2011). Existing sentiment domain adaptation methods are mainly based on transfer learning techniques. Many of them try to learn a new feature representation to augment or replace the original feature space in order to reduce the gap of sentiment feature distributions between source and target domains (Pan et al., 2010; Glorot et al., 2011). For example, Blitzer et al. (2007) proposed to learn a latent representation for domain-specific words from both source and target domains by using pivot features as bridge. The advantage of these methods is that no labeled data in target d"
P17-1156,W02-1011,0,0.03428,"nt lexicons to target domain is suboptimal. This is because there are many domain-specific sentiment expressions that are not covered by these general-purpose sentiment lexicons (Choi and Cardie, 2009). In addition, the performance of supervised sentiment classification methods such as SVM, LS, and LR is also 0.85 ASDA SVM 0.8 Accuracy i.e., MPQA (Wilson et al., 2005) and Bing Liu’s lexicon (Hu and Liu, 2004) for sentiment classification following the suggestions in (Hu and Liu, 2004); 2) SVM, LS, and LR, three popular supervised sentiment classification methods, i.e., support vector machine (Pang et al., 2002), least squares (Hu et al., 2013) and logistic regression (Wu et al., 2015); 3) ZIAL, the zero initialized active learning method (Cesa-Bianchi et al., 2006); 4) LIAL, the active learning method initialized by randomly selected labeled data (Settles, 2010); 5) SCL and SFA, two famous sentiment domain adaptation methods proposed in (Blitzer et al., 2007) and (Pan et al., 2010) respectively; 6) ILP, adapting sentiment lexicons to target domain via integer linear programming (Choi and Cardie, 2009); 7) AODA, the active online domain adaptation method (Rai et al., 2010); 8) ALCD, the active learni"
P17-1156,W10-0104,0,0.0338299,"Missing"
P17-1156,P02-1053,0,0.0375828,"me sentiment than opposite sentiments, then they will have a larger positive sentiment similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high senti"
P17-1156,N10-1119,0,0.0201386,"han opposite sentiments, then they will have a larger positive sentiment similarity score. Note r can be negative according to Eq. (1). Here that Si,j we focus on sentiment similarity rather than dissimilarity, and set all the negative values in Sr to r is [0, 1]. zero. The range of Si,j The second type of sentiment similarities are extracted according to the co-occurrence patterns among words. It is inspired by the observation that words frequently co-occurring with each other not only have a high probability to have similar semantics, but also tend to share similar sentiments (Turney, 2002; Velikovich et al., 2010; Yogatama and Smith, 2014; Tang et al., 2015; Hamilton et al., 2016). In this paper, we compute the co-occurrence between words in the context of document. Denote D as the set of all documents, and Ndi as the frequency of word i appearing in document d. Then, the sentiment similarity score between words i and j based on their co-occurrence patterns is defined as: c Si,j = P P d∈D d∈D min{Ndi , Ndj } max{Ndi , Ndj } + α2 , (2) where α2 is a positive smoothing parameter. If two words frequently co-occur with each other in many documents, then they will have a high sentiment similarity score acc"
P17-1156,H05-1044,0,0.144761,"0.7970 0.8329 0.8328 Table 2: Sentiment classification performance of different methods in different domains. Acc and Fscore represent accuracy and macro-averaged Fscore respectively. According to Table 2, the performance of directly applying sentiment lexicons to target domain is suboptimal. This is because there are many domain-specific sentiment expressions that are not covered by these general-purpose sentiment lexicons (Choi and Cardie, 2009). In addition, the performance of supervised sentiment classification methods such as SVM, LS, and LR is also 0.85 ASDA SVM 0.8 Accuracy i.e., MPQA (Wilson et al., 2005) and Bing Liu’s lexicon (Hu and Liu, 2004) for sentiment classification following the suggestions in (Hu and Liu, 2004); 2) SVM, LS, and LR, three popular supervised sentiment classification methods, i.e., support vector machine (Pang et al., 2002), least squares (Hu et al., 2013) and logistic regression (Wu et al., 2015); 3) ZIAL, the zero initialized active learning method (Cesa-Bianchi et al., 2006); 4) LIAL, the active learning method initialized by randomly selected labeled data (Settles, 2010); 5) SCL and SFA, two famous sentiment domain adaptation methods proposed in (Blitzer et al., 20"
P17-1156,P16-1029,1,0.845411,"t classification and yi ∈ {+1, −1}. In addition, we select log loss for f . Thus, f (xi , yi , w) = log(1 + exp(−yi wT xi )). Besides, we use S ∈ RD×D to represent the sentiment similarities among words extracted from unlabeled samples of target domain. 3.2 Domain-Specific Sentiment Similarities Next we introduce the extraction of domainspecific sentiment similarities among words from unlabeled samples of target domain. Two types of similarities are extracted in this paper. The first one is based on syntactic rules, which is inspired by (Hatzivassiloglou and McKeown, 1997; Huang et al., 2014; Wu and Huang, 2016). If two words have the same POS-tag such as adjective, verb, and adverb, and they are connected by coordinating conjunction “and” in the same sentence, then we regard they convey the same sentiment polarity. In 1703 addition, if two words are connected by adversative conjunction “but” and have the same POS-tag, then they are assumed to have opposite sentiment polarities. Denote Sr ∈ RD×D as the sentiment similarities extracted from unlabeled samples according to syntactic rules, and the similarity score between words i and j is defined as: r Si,j = s o Ni,j − Ni,j , o + Ni,j + α1 (1) s Ni,j s"
P19-1110,D14-1181,0,0.0248008,"The architecture of our basic neural news recommendation model is shown in Fig. 2. It consists of three major modules, i.e., news encoder, user encoder and click predictor. News Encoder. The news encoder module is used to learn representations of news from their titles. It contains three layers. The first one is word embedding, which converts a news title from a word sequence into a vector sequence. Denote a news title as [w1 , w2 , ..., wM ], where M is title length. It is converted into word vector sequence [e1 , e2 , ..., eM ] via a word embedding matrix. The second layer is a CNN network (Kim, 2014). Local contexts are important for understanding news titles. For example, in the news title “90th Birthday of Mickey mouse”, the local contexts of “mouse” such as “Mickey” is useful for inferring it is a comic character name. Thus, we use CNN to learn contextual word representations by capturing local contexts. The CNN layer takes the word vectors as input, and outputs the contextual word representations [c1 , c2 , ..., cM ]. Click Probability ?? � Click Predictor Dot ?? ???? ?? ???? ??2?? ???? ???? CNN ???? News Encoder ???? Word Embedding ??1 ???? ???? ???? Candidate News ??1?? ??1 News Enc"
P19-1110,D14-1162,0,0.0905198,"ANR-basic TANR* Table 1: Statistics of our dataset. AUC 0.5660 0.5689 0.6009 0.5735 0.5774 0.5860 0.5869 0.6102 0.6221 0.6289 MRR 0.2924 0.2956 0.3099 0.2989 0.3031 0.3034 0.3044 0.2811 0.3246 0.3315 nDCG@5 0.3015 0.3043 0.3261 0.3094 0.3122 0.3175 0.3184 0.3035 0.3487 0.3544 nDCG@10 0.3932 0.3955 0.4185 0.3996 0.4019 0.4067 0.4071 0.3952 0.4329 0.4392 Table 2: The results of different methods. *The improvement is significant at p < 0.01. Figure 4: Topic distributions in our dataset. In our experiments, word embeddings are 300dimensional and were initialized by the pretrained Glove embedding (Pennington et al., 2014). The CNN network has 400 filters, and their window sizes are 3. The negative sampling ratio K is 4 and the coefficient λ is 0.2. Adam (Kingma and Ba, 2014) is used as the optimization algorithm, and the batch size is 64. These hyperparameters were selected according to the validation set. The metrics used for result evaluation in our experiments include AUC, MRR, nDCG@5 and nDCG@10. We repeated each experiment 10 times and reported the average results. 3.2 Performance Evaluation We evaluate the performance of our TANR approach by comparing it with several baseline methods, including: (1) LibF"
S18-1006,W14-2609,0,0.0318165,"@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, existing methods are aimed to detect irony in tweets with explicit irony related hashtags. For example, tweets with #irony or #sarcasm hashtags are very likely to be ironic. Therefore, models may focus on these hashtags rather than the contextual information. To fill this gap, the SemEval-2018 task"
S18-1006,W16-0425,0,0.0609525,"epresentations of texts, and they will be used in three classification task. In addition, we incorporate several types of features to improve the model performance. Our model achieved an F-score of 70.54 (ranked 2/43) in the subtask A and 49.47 (ranked 3/29) in the subtask B. The experimental results validate the effectiveness of our system. 1 Introduction Figurative languages such as irony are widely used in web messages such as tweets to convey different sentiment. Identifying the ironic texts can help to understand the social web better and has many applications such as sentiment analysis (Ghosh and Veale, 2016). Irony detecting techniques are important to improve the performance of sentiment analysis. For example, the tweet “Monday mornings are my fave:)# not” is an irony with negative sentiment, but it will be probably classified as a positive one by a standard sentiment analysis model (Van Hee et al., 2016b). Thus, capturing the ironic information in texts is useful to predict sentiment more accurately (Van Hee et al., 2016a). However, determining whether a text is ironic is a challenging task since the the differences between ironic and non-ironic texts are usually subtle. For example, the tweet"
S18-1006,W15-4322,0,0.0715824,"Missing"
S18-1006,W15-2905,0,0.0127207,"n Liu1 , Zhigang Yuan1 and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,ljx16,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, exist"
S18-1006,maynard-greenwood-2014-cares,0,0.0543699,"and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,ljx16,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract weather #not happy” is non-ironic. Different approaches are proposed to recognize the complex irony in texts. Existing methods to detect irony are mainly based on rules or machine learning techniques (Joshi et al., 2017). Rules based methods usually depend on lexicons to identify irony (Khattri et al., 2015; Maynard and Greenwood, 2014). However, these methods cannot utilize the contextual information from texts. Traditional machine learning based methods such as SVM (Desai and Dave, 2016) are also effective in this task, but they usually need manually feature engineering (Barbieri et al., 2014). Recently, deep learning techniques are successfully applied to this task. For example, Ghosh et al. (2016) propose to use a CNN-LSTM model to classify the ironic and non-ironic tweets. Their method can significantly improve the classification performance without heavy feature engineering. However, existing methods are aimed to detec"
S18-1006,W17-5205,0,0.0499006,"Missing"
S18-1006,N13-1039,0,0.0670625,"Missing"
S18-1006,L16-1283,0,0.0734504,"Missing"
S18-1006,C16-1257,0,0.118572,"Missing"
S18-1006,S18-1005,0,0.170493,"Missing"
S18-1006,I17-4007,1,0.7889,"er will output the hidden representation H of texts. It will be concatenated with the sentiment features and the sentence embedding features. The sentiment features can provide additional sentiment information to detect irony, such as the sentiment polarity assigned by lexicons. The sentiment features are generated via the AffectiveTweets5 package in weka provided by Mohammad et al. (Mohammad and Bravo-Marquez, 2017). We use the TweetToLexiconFeatureVector (Bravo-Marquez et al., 2014) and TweetToSenIn order to address this problem, we propose a system2 based on a densely connected LSTM model (Wu et al., 2017) with multitask learning techniques. In our model, each LSTM layer will take all outputs of previous LSTM layers as input. Then different levels of contextual information can be learned at the same time. Our model is required to predict in three tasks simultaneously: 1) identifying the missing irony related hashtags; 2) classify ironic or non-ironic; 3) irony type classification. By using multitask learning strategy, the model can combine the information in the different tasks to improve the performance. The experimental results in both subtasks validate the effectiveness of our method. 2 Dens"
S18-1028,L16-1624,0,0.027715,"rporated into our model. The evaluation results show that our system outperform several baseline neural networks and can be further extended. reg). Our model outperforms several baseline neural networks, which proves that our model can identify the intensity of emotions and sentiment effectively. 2 Related Work Sentiment analysis in social media such as Twitter is an important task for opinion mining (Severyn and Moschitti, 2015). Traditional Twitter sentiment analysis methods mainly focus on identifying the polarities (Da Silva et al., 2014; dos Santos and Gatti, 2014) or emotion categories (Dini and Bittar, 2016) of tweets. However, it’s a difficult task to analysis the noisy tweets. They usually contain various nonstandard languages including emoticons, emojis, creatively spelled words and hash tags. In addition, these languages usually contain rich sentiment information. In order to capture such information, several lexicon-based methods are proposed. Nielsen et al. (2011) proposed to use a dictionary to incorporate emoticon information into tweet analysis models. Mohammad et al. proposed to use hash tags to identify emotion categories of tweets (2015). These lexicon-based methods are free from manu"
S18-1028,W15-4322,0,0.0688825,"Missing"
S18-1028,W17-5205,0,0.078575,"Missing"
S18-1028,W17-5207,0,0.076746,"orical and real-valued intensity of emotions or sentiment for English, Arabic, and Spanish (Mohammad et al., 2018). Existing approaches to analysis the intensity of emotions or sentiment are mainly based on lexicons and supervised learning. Lexicon-based methods usually rely on lexicons to assign the intensity scores of affective words in texts (Mohammad and Bravo-Marquez, 2017). However, these method can’t utilize the contextual information from texts. Supervised methods are mainly based on SVR (Madisetty and Desarkar, 2017), linear regression (John and Vechtomova, 2017) and neural networks (Goel et al., 2017; K¨oper et al., 2017). Usually neural network-based methods outperform SVR and linear regression-based methods siginificantly. Motivated by the successful applications of neural models in this task, we propose a system using a CNN-LSTM model with attention mechanism. Firstly, a tweet will be converted into a sequence of dense vectors by an embedding layer. Next, we use a Bi-LSTM layer to extract contextual information from them. The sequential features will be selected by an attention layer. Then we apply a CNN with different kernel sizes to extracting different local information. Thus, our m"
S18-1028,S18-1001,0,0.112862,"Missing"
S18-1028,W17-5235,0,0.0188507,"mEval-2018 Task 1 is aimed to identify the categorical and real-valued intensity of emotions or sentiment for English, Arabic, and Spanish (Mohammad et al., 2018). Existing approaches to analysis the intensity of emotions or sentiment are mainly based on lexicons and supervised learning. Lexicon-based methods usually rely on lexicons to assign the intensity scores of affective words in texts (Mohammad and Bravo-Marquez, 2017). However, these method can’t utilize the contextual information from texts. Supervised methods are mainly based on SVR (Madisetty and Desarkar, 2017), linear regression (John and Vechtomova, 2017) and neural networks (Goel et al., 2017; K¨oper et al., 2017). Usually neural network-based methods outperform SVR and linear regression-based methods siginificantly. Motivated by the successful applications of neural models in this task, we propose a system using a CNN-LSTM model with attention mechanism. Firstly, a tweet will be converted into a sequence of dense vectors by an embedding layer. Next, we use a Bi-LSTM layer to extract contextual information from them. The sequential features will be selected by an attention layer. Then we apply a CNN with different kernel sizes to extracting d"
S18-1028,N13-1039,0,0.0243735,"Missing"
S18-1028,S16-1004,0,0.0307022,"contextual information from texts. We apply attention techniques to selecting this information. A CNN layer with different kernel sizes is used to extract local features. The dense layers take the pooled CNN feature maps and predict the intensity scores. Our system achieves an average Pearson correlation score of 0.722 (ranked 12/48) in the emotion intensity regression task, and 0.810 in the valence regression task (ranked 15/38). It indicates that our system can be further extended. 1 Introduction Detecting the intensity of sentiment is an important task for fine-grained sentiment analysis (Kiritchenko et al., 2016; Mohammad and BravoMarquez, 2017). Intensity refers to the degree or amount of an emotion or degree of sentiment. For example, we can express our emotion by “very happy” or “a little angry”. The intensity can be analysis in multiple categories (i.e. low, moderate and high) or real-valued. Identifying the intensity information of sentiment has potential to applications such as electronic business, social computing and public health (Wilson, 2008). Twitter is a social platform which contains rich textual content. There have been many approaches to twitter sentiment analysis (Khan et al., 2015;"
S18-1028,C14-1008,0,0.0276401,"scores. In addition, several features are incorporated into our model. The evaluation results show that our system outperform several baseline neural networks and can be further extended. reg). Our model outperforms several baseline neural networks, which proves that our model can identify the intensity of emotions and sentiment effectively. 2 Related Work Sentiment analysis in social media such as Twitter is an important task for opinion mining (Severyn and Moschitti, 2015). Traditional Twitter sentiment analysis methods mainly focus on identifying the polarities (Da Silva et al., 2014; dos Santos and Gatti, 2014) or emotion categories (Dini and Bittar, 2016) of tweets. However, it’s a difficult task to analysis the noisy tweets. They usually contain various nonstandard languages including emoticons, emojis, creatively spelled words and hash tags. In addition, these languages usually contain rich sentiment information. In order to capture such information, several lexicon-based methods are proposed. Nielsen et al. (2011) proposed to use a dictionary to incorporate emoticon information into tweet analysis models. Mohammad et al. proposed to use hash tags to identify emotion categories of tweets (2015)."
S18-1028,W17-5206,0,0.0512007,"Missing"
S18-1063,W17-5205,0,0.0593053,"Missing"
S18-1063,E17-2017,0,0.180918,"Missing"
S18-1063,I17-4007,1,0.853953,"Missing"
S18-1063,S18-1003,0,0.0394778,"Missing"
S18-1063,N16-1174,0,0.13369,"Missing"
S18-1063,L16-1626,0,0.370853,"2018 Task 2: Residual CNN-LSTM Network with Attention for English Emoji Prediction Chuhan Wu1 , Fangzhao Wu2 , Sixing Wu1 , Zhigang Yuan1 , Junxin Liu1 and Yongfeng Huang1 1 Tsinghua National Laboratory for Information Science and Technology, Department of Electronic Engineering, Tsinghua University Beijing 100084, China 2 Microsoft Research Asia {wuch15,wu-sx15,yuanzg14,ljx16,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract the semantics, usage or sentiment of emojis (Aoki and Uchida, 2011; Barbieri et al., 2016a,b,c; Ljubeˇsi´c and Fiˇser, 2016; Novak et al., 2015). For example, Barbieri et al. (2016b) explored the meaning and usage of emojis across different languages. Wijeratneet al. (2017) proposed to utilize the emoji sense definitions to improve the performance of emoji embedding model. However, these approaches cannot reveal the interplay between plain texts and emojis. In order to fill this gap, Barbieri et al. (2017) proposed a novel task to predict which emojis are evoked by text-based tweets. For example, given a tweet message “Love my coworkers ! @user”, a system is required to predict that emoji is associated with this tweet. Emojis are widely used by social media and social n"
S18-1063,W16-2610,0,0.0699456,"Missing"
S18-1157,O97-1002,0,0.252731,"tive. The CNNs are used to extract low-level features from the inputs. The MLP takes both the flatten CNN maps and inputs to predict the labels. The evaluation F-score of our system on the test set is 0.629 (ranked 15th), which indicates that our system still needs to be improved. However, the behaviours of our system in our experiments provide useful information, which can help to improve the collective understanding of this novel task. 1 Introduction Evaluating the similarity of words is an important task in semantic modeling. There have been different approaches based on corpus statistics (Jiang and Conrath, 1997; Mihalcea et al., 2006) and ontology (Seco et al., 2004; S´anchez et al., 2012). After an effective word representation proposed by mikolov et al (2013), word similarity can be evaluated based on word embedding weights (Levy and Goldberg, 2014). Usually higher cosine similarity of word embedding vectors indicates higher semantic similarity. However, existing semantic methods are not capable of discriminating similar words between each other without additional information. For example, it is easy for these models to tell “dog” and “puppy” is similar, but they can’t tell the differences between"
S18-1157,D14-1162,0,0.0942581,"or it will be set to 0. In this way, a 2-dim synset feature of each word can be obtained. We use the nltk tool(Bird et al., 2009) to generate the WordNet features. The features above are concatenated with word embedding as the input of MLP-CNN model. Word Embedding Since there are several out-of-vocabulary words in the dataset when using single pre-trained word embedding, we use three different embedding models to cover them. The three embedding models include pre-trained word2vec embedding1 provided by Mikolov et al. (Mikolov et al., 2013), the Glove embedding2 provided by Pennington et al. (Pennington et al., 2014) and the fastText embedding3 released by bojanowski et al. (Bojanowski et al., 2016). These embedding weights are all 300-dim. They are concatenated together as the representation of input words. 2.3 sigmoid 2.4 Word Feature Model Training and Ensemble Since the train set is unbalanced, we randomly select same numbers of positive (the attribute is discriminative) and negative (the attribute is not discriminative) samples from the train set every time. Thus, the training data we used in our experiments In our model, we use one-hot encoded POS tags and two binary features obtained by Word1 https"
S18-1157,I17-4007,1,0.844009,"Missing"
S19-2059,S17-2126,0,0.0185512,"full usage of both chronological information and local contextual information. Finally, max pooling is applied to the concatenated vectors to build conversation representations. Figure 1: The architecture of our attentional LSTMCNN model, the output is generated by soft voting ensemble after the softmax layer. 2.1 Word Embeddings The first layer is a word embedding layer, which aims to convert the sequence of words in conversations into a low-dimensional vector sequence. We harness three types of pre-trained word embeddings, i.e., word2vec-twitter (Godin et al., 2015), pre-trained ekphrasis (Baziotis et al., 2017) vectors and GloVe (Pennington et al., 2014), to initialize the word embedding matrix. 1 CNN Layer 2.5 Emotion Classification To make the final emotion prediction, we use a dense layer with softmax activation function to classify emotions. Considering the unbalanced data in both training set and testing set, we github.com/gesy17/Semeval2019-Task3-Emocontext 341 Precision Recall F1 Happy 0.7452 0.6796 0.7109 Sad 0.8117 0.7760 0.7935 Angry 0.7329 0.7919 0.7613 Average 0.7598 0.7488 0.7542 With word2vec-twitter embedding and GloVe embedding, we send raw texts to NLTK TweetTokenizer and randomly g"
S19-2059,D14-1162,0,0.0820434,"ion and local contextual information. Finally, max pooling is applied to the concatenated vectors to build conversation representations. Figure 1: The architecture of our attentional LSTMCNN model, the output is generated by soft voting ensemble after the softmax layer. 2.1 Word Embeddings The first layer is a word embedding layer, which aims to convert the sequence of words in conversations into a low-dimensional vector sequence. We harness three types of pre-trained word embeddings, i.e., word2vec-twitter (Godin et al., 2015), pre-trained ekphrasis (Baziotis et al., 2017) vectors and GloVe (Pennington et al., 2014), to initialize the word embedding matrix. 1 CNN Layer 2.5 Emotion Classification To make the final emotion prediction, we use a dense layer with softmax activation function to classify emotions. Considering the unbalanced data in both training set and testing set, we github.com/gesy17/Semeval2019-Task3-Emocontext 341 Precision Recall F1 Happy 0.7452 0.6796 0.7109 Sad 0.8117 0.7760 0.7935 Angry 0.7329 0.7919 0.7613 Average 0.7598 0.7488 0.7542 With word2vec-twitter embedding and GloVe embedding, we send raw texts to NLTK TweetTokenizer and randomly generate word vectors for all emojis and thos"
S19-2059,S19-2005,0,0.0238113,"achieved 0.7542 micro-F1 score in the final test data, ranking 15th out of 165 teams. 1 Introduction The analysis of emotions in dialog systems where limited number of words appear with strong semantic relations between them deserves special attention in domain of natural language processing (NLP) due to both interesting language novelties and wide future prospects (Gupta et al., 2017). By analyzing the emotions through conversations, service providers can design better chatting strategies according to users’ emotion patterns, which can improve user experience. Therefore, SemEval-2019 task 3 (Chatterjee et al., 2019) aims to call for research in this field. Given a textual dialogue, i.e., a user utterance along with two turns of context, systems need to classify the emotion of user utterance into four emotion classes: happy, sad, angry or others. 340 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 340–344 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2.2 Besides, we use ensemble strategies by using a combination of the variants of our model with different pre-trained word embeddings. Our model achieved 0.7542 micro-"
S19-2059,D16-1058,0,0.0296381,"tional sentiment analysis requires a lot of feature engineering, such as n-grams and features extracted from sentiment lexicons (Mohammad and Turney, 2013; Kiritchenko et al., 2014a), and then feed them into a classifier such as Support Vector Machines (SVM) (Bollen et al., 2011; Kiritchenko et al., 2014b). However, manual feature engineering usually needs a large amount of domain knowledge. With the rapid development and ambiguity of social dialogues, these feature engineering strategies fade gradually and begin to be supplanted by neural networks (Tang et al., 2015; ˙Irsoy and Cardie, 2014; Wang et al., 2016), which usually take word embeddings as inputs to incorporate rich semantic and syntactic information (Collobert and Weston, 2008). However, dialog emotion analysis is still very challenging, since dialog conversations can be very noisy and informal. In addition, the emotions evoked by conversations are usually highly context-dependent. In this work, we propose an end-to-end attentional LSTM-CNN network as a unified model without hand-crafted features. In our approach, we use a combination of LSTM and CNN to capture both local and long-distance information. We use attention mechanism to select"
S19-2059,W15-4322,0,0.0762342,"Missing"
S19-2059,D14-1080,0,0.0608801,"Missing"
S19-2059,S14-2076,0,0.0350059,"tudy on relevance between tweet texts and emojis. However, understanding textual conversations is challenging in absence of voice modulations and facial expressions, which participants at this task are asked to deal with. Apart from diminishing the negative impact caused by class size imbalance, ambiguity, misspellings and slang, their systems should mainly focus on capturing the intricate interplay between two turns of conversations. Traditional sentiment analysis requires a lot of feature engineering, such as n-grams and features extracted from sentiment lexicons (Mohammad and Turney, 2013; Kiritchenko et al., 2014a), and then feed them into a classifier such as Support Vector Machines (SVM) (Bollen et al., 2011; Kiritchenko et al., 2014b). However, manual feature engineering usually needs a large amount of domain knowledge. With the rapid development and ambiguity of social dialogues, these feature engineering strategies fade gradually and begin to be supplanted by neural networks (Tang et al., 2015; ˙Irsoy and Cardie, 2014; Wang et al., 2016), which usually take word embeddings as inputs to incorporate rich semantic and syntactic information (Collobert and Weston, 2008). However, dialog emotion analys"
S19-2229,Q17-1010,0,0.0215224,"Missing"
S19-2229,S19-2155,0,0.0632924,"Missing"
S19-2229,P18-2012,0,0.0275008,"toponym detection. Usually the performance of these methods heavily relies on the quality of hand-crafted features. However, manually selected features may be sub-optimal. Also, these methods cannot effectively exploit contextual information due to the dependency on bag-of-word features. In recent years, many neural network based methods have been proposed for NER. For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for NER. They use CNN layer to learn character features of each word, LSTM layer to learn the contextual word representations and CRF layer to predict the label jointly. Gregoric et al. (2018) proposed Parallel RNN architecture. They split a single LSTM into multiple equally-size ones with a penalty to promote diversity. However, these methods cannot utilize external knowledge to recognize entities, which is usually important to toponym detection. Usually, linguistic knowledge such as part-of-speech and dictionary knowledge may be useful for toponym detection, and they are easy to obtain. Therefore, in this paper, we aim to incorporate these external knowledge sources to enhance our neural model for toponym detection. Similarly, there are many works on toponym disambiguation. Most"
S19-2229,P16-1101,0,0.0175191,"ER is a widely explored task and most NER methods can be applied to toponym detection. For example, Ratinov and Roth (2009) used n-grams, history predictions as the input features of conditional random fields (CRF) for toponym detection. Usually the performance of these methods heavily relies on the quality of hand-crafted features. However, manually selected features may be sub-optimal. Also, these methods cannot effectively exploit contextual information due to the dependency on bag-of-word features. In recent years, many neural network based methods have been proposed for NER. For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for NER. They use CNN layer to learn character features of each word, LSTM layer to learn the contextual word representations and CRF layer to predict the label jointly. Gregoric et al. (2018) proposed Parallel RNN architecture. They split a single LSTM into multiple equally-size ones with a penalty to promote diversity. However, these methods cannot utilize external knowledge to recognize entities, which is usually important to toponym detection. Usually, linguistic knowledge such as part-of-speech and dictionary knowledge may be useful for toponym detection, an"
S19-2229,D14-1162,0,0.0808159,"Missing"
S19-2229,P17-1161,0,0.0127184,"larly, there are many works on toponym disambiguation. Most of them are rule-based methods. They use some heuristics to rank the candidates and choose the highest one(Gritta et al., 2018). For example, Karimzadeh et al. (2013) used the geographical level(e.g. country, province and city), the Levenshtein Distance and the population of potential candidates to rank the candidate toponym and choose the highest one. However, the result of toponym disambiguation relied on corpus domain and the rule should be reconsidered when applied to different corpus. For the toponym detection task, we use TagLM(Peters et al., 2017) as the basic model. 1302 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1302–1307 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics In our model, we first learn word representations from original characters, then learn contextual word representations by a stacked Bi-LSTM network, and finally use a CRF layer to jointly decode the label sequence. To enrich the representations of words, we incorporate various features such as pre-trained word embeddings, POS tags and lexicon features. For the toponym disambig"
S19-2229,W09-1119,0,0.0319894,"ym detection subtask. 1 Introduction Toponym resolution is an important task in the natural language processing field and has many applications such as emergency response and social media geographical event analysis(Gritta et al., 2018). Toponym resolution is usually modelled as a two-step task. The first step is toponym detection, which is a typical named entity recognition (NER) task. The second step is toponym disambiguation, which aims to map locations to its coordinates in the real world. NER is a widely explored task and most NER methods can be applied to toponym detection. For example, Ratinov and Roth (2009) used n-grams, history predictions as the input features of conditional random fields (CRF) for toponym detection. Usually the performance of these methods heavily relies on the quality of hand-crafted features. However, manually selected features may be sub-optimal. Also, these methods cannot effectively exploit contextual information due to the dependency on bag-of-word features. In recent years, many neural network based methods have been proposed for NER. For example, Ma and Hovy (2016) proposed a CNN-LSTM-CRF model for NER. They use CNN layer to learn character features of each word, LSTM"
S19-2229,D17-1035,0,0.0125367,"set, we will select the highest frequency id as the output. Otherwise, we will select the toponym with the most population as output. 3 Experiment 3.1 3.2 Experimental Settings We conduct experiments on science reports provided the SemEval-2019 task 12. The data set is composed of 72 full-text journal articles in open access. There are four different metrics to evaluate the prediction performance, i.e., strict macro F1, strict micro F1, overlap macro F1 and overlap micro F1. In the toponym detection task, we used NLTK1 for sentence segmentation, word tokenization and POS tagging. We used ELMo(Reimers and Gurevych, 2017) and BERT(Devlin et al., 2018) model to generate 1024-dimensional contextualized word embeddings. We used GeoNames2 1 2 https://www.nltk.org http://www.geonames.org to construct lexical feature. The BIO tagging scheme(Sang and Veenstra, 1999) was used in the toponym detection task. In the toponym disambiguation task, we use GeoNames database to retrieve candidate toponyms. In our approach, the three word embedding vectors we used (Glove(Pennington et al., 2014), word2vec(Mikolov et al., 2013), fasttext(Bojanowski et al., 2017)) were all 300dimensional. The dimension of the character embedding"
S19-2229,E99-1023,0,0.0721783,"Missing"
S19-2229,I17-4007,1,0.891925,"Missing"
W18-0913,P16-2017,0,0.745683,"805 110 Proceedings of the Workshop on Figurative Language Processing, pages 110–114 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics M M - - - CRF or softmax Inference Bi-LSTM CNN zeros POS tag Word Cluster zeros Word embedding Embedding That take him by surprise Lemmatizing That takes him by surprise Figure 1: The architecture of our method. The final metaphor labels will be predicted by a CRF or softmax inference layer. is presented in Figure 1. We will introduce the details of modules in our model from bottom to top. We follow the approach proposed by Klebanov et al. (2016) to use the lemmatizing strategy. The first module in our model is a lemmatizer. This module is used to lemmatize the verbs in texts via a dictionary. The input is a text with a sequence of word, and output is the text with lemmatized words. Since verbs with different forms can share the same lemmas, using the lemmatized verbs in texts can simplify the semantic information and reduce the number of out-of-vocabulary words. We use the NLTK package (Bird et al., 2009) to transform the verbs into their lemmas. nated them with the word embeddings. We use the Stanford parser2 tool to obtain the POS"
W18-0913,W13-0904,0,0.0693244,"Missing"
W18-0913,P14-1045,0,0.015891,"nd they will be fine-tuned during model training. POS tags are useful in metaphor detecting task (Klebanov et al., 2014). Therefore, we also incorporate the one-hot encoded POS tags as additional features into our neural model, and concates∈S where S is the training set, and hs and ys are the hidden states and label sequence of sentence s. 2 111 https://nlp.stanford.edu/software/lex-parser.shtml Softmax: We use a dense layer with softmax activation function to predict the metaphor label sequences. Motivated by the cost-sensitive crossentropy (Santos-Rodr´ıguez et al., 2009; Yang et al., 2014; Muller et al., 2014), the loss function of our model is formulated as follows: LSof tmax = − N XX wyi yi log(yˆi ), 2.0 and 1.0 respectively. The class number of word cluster is set 50. The batch size is 50, and the max training epoch is set to 15. The optimizer we use is RMSProp in our experiment. The performance of both all POS testing and verbs testing subtasks is evaluated by precision, recall and F-score as a standard binary classification task. 3.2 (4) s∈S i=1 We compare the performance of the variants of our model and several baseline methods. The methods to be compared include: 1) CNN+CRF, using CNN to ex"
W18-0913,P14-1024,0,0.386334,"Missing"
W18-0913,W15-1405,0,0.373885,"Missing"
W18-0913,D11-1063,0,0.66098,"Missing"
W18-0913,W13-0908,0,0.0756097,"Missing"
W18-0913,I17-4007,1,0.880841,"Missing"
W18-0913,W14-2302,0,0.674251,"gineering, Tsinghua University, Beijing 100084 2 Microsoft Research Asia {wuch15,ybch14,wu-sx15,yuanzg14,yfhuang}@mails.tsinghua.edu.cn wufangzhao@gmail.com Abstract Existing computational approaches to detect metaphors are mainly based on lexicons (Mohler et al., 2013; Dodge et al., 2015) and supervised methods (Turney et al., 2011; Heintz et al., 2013; Klebanov et al., 2014, 2015, 2016). Lexiconbased methods are free from data annotation, but they are unable to detect novel metaphorical usages and capture the contextual information. Supervised methods such as logistic regression classifier (Klebanov et al., 2014) can capture richer metaphor information. However, they need sophisticated hand-crafted features. To improve the collective techniques on detecting metaphors, the metaphor shared task1 aims to detect both metaphorical verbs and metaphors with other POS. Given a sentence and their words with specific POS tags, systems are required to determine whether each word is a metaphor. We propose a CNN-LSTM model with CRF or weighted softmax classifier to address this task. Our model can take advantage of both long-range and local information by utilizing both LSTM and CNN layers. We propose to use a wei"
W18-0913,W15-1402,0,0.144713,"Missing"
W18-5909,C16-1084,0,0.197519,"Missing"
W18-5909,W18-5904,0,0.111538,"Missing"
W19-3214,W19-3203,0,0.1048,"Missing"
W19-3214,W15-4322,0,0.0731182,"Missing"
W19-3214,W18-5909,1,0.904955,"h which pharmacovigilance (Sarker and Gonzalez, 2015) can be performed at a broader and more automatic level. Recent research focus their attention on online public sources such as tweets due to their availability and authenticity (Onishi et al., 2018; Adrover et al., 2015; Salath´e and Khandelwal, 2011). The SMM4H shared task is proposed (Weissenbacher et al., 2019) to enhance ADR recognition. Task 1 is a binary classification task between ADR mentioned tweets and drug name only tweets, followed by task 2 to extract the particular position of ADR entities. Based on the work we did last year (Wu et al., 2018), we extend our previous model with hierarchical tweet representation and multi-head self-attention (HTR-MSA) to a model using both hierarchical tweet representation and attention (HTA) to jointly participate both tasks. Moreover, additional features and a language model are incorporated to enhance the semantic representations. In task 1, transfer learning † Our Approach hi,j = ReLU (Uc × ei,(j−w):(j+w) + bc ) (1) To remove unnecessary information, we apply the max pooling to pertain only the most salient feature of the ith word. Other features are added at a word level, such as word2vec-twitt"
W19-3214,W18-5908,0,0.0587939,"Missing"
W19-3214,N18-1202,0,0.0141796,"level, such as word2vec-twitter (Godin et al., 2015) word embedding, pos-tag from NLTK library (Bird et al., 2009) and sentiment lexicon1 . To strengthen the 1 Equal contribution. http://sentiwordnet.isti.cnr.it/ 96 th Proceedings of the 4 Social Media Mining for Health Applications (#SMM4H) Workshop & Shared Task, pages 96–98 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 medical meaning of word representation, word appearance in SIDER 4.1 medical lexicon2 is transformed to one-hot vector as additional feature. Besides, the language model ELMo embedding (Peters et al., 2018) is incorporated to overcome the shortage of limited data and get better semantic meaning. Since ELMo contains character level information in their model, it fits better to our task goal than other language model that utilizes a fixed word look-up dictionary. The final output of our hierarchical word representation is the concatenation of character representation, word embedding, pos-tag, sentiment lexicon, medical lexicon feature and language model output. 2.2 3.1 We first send word representation obtained in the previous module to a Bi-LSTM layer to encode long-distance information. The Bi-L"
W19-3214,D14-1181,0,\N,Missing
W19-3214,D16-1024,0,\N,Missing
W19-3214,C16-1084,0,\N,Missing
W19-3214,W18-5904,0,\N,Missing
