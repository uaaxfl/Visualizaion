2020.acl-main.372,I17-1099,0,0.0307699,"were found by Bostan and Klinger (2018) to be noisy in comparison with other emotion datasets. Other datasets are automatically weakly-labeled, based on emotion-related hashtags on Twitter (Wang et al., 2012; AbdulMageed and Ungar, 2017). We build our dataset manually, making it the largest human annotated dataset, with multiple annotations per example for quality assurance. Several existing datasets come from the domain of Twitter, given its informal language and expressive content, such as emojis and hashtags. Other datasets annotate news headlines (Strapparava and Mihalcea, 2007), dialogs (Li et al., 2017), fairy¨ tales (Alm et al., 2005), movie subtitles (Ohman et al., 2018), sentences based on FrameNet (Ghazi et al., 2015), or self-reported experiences (Scherer and Wallbott, 1994) among other domains. We are the first to build on Reddit comments for emotion prediction. 2.2 Emotion Taxonomy One of the main aspects distinguishing our dataset is its emotion taxonomy. The vast majority of existing datasets contain annotations for minor variations of the 6 basic emotion categories (joy, anger, fear, sadness, disgust, and surprise) proposed by Ekman (1992a) and/or along affective dimensions (valenc"
2020.acl-main.372,D19-1656,0,0.107591,"Missing"
2020.acl-main.372,P18-1017,0,0.0311346,"(Cowen et al., in press), 28 by facial expression (Cowen and Keltner, 2019), 12 by speech prosody (Cowen et al., 2019b), and 24 by nonverbal vocalization (Cowen et al., 2018). In this work, we build on these methods and findings to devise our granular taxonomy for text-based emotion recognition and study the dimensionality of language-based emotion space. 2.3 Emotion Classification Models Both feature-based and neural models have been used to build automatic emotion classification models. Feature-based models often make use of handbuilt lexicons, such as the Valence Arousal Dominance Lexicon (Mohammad, 2018). Using representations from BERT (Devlin et al., 2019), a 4041 transformer-based model with language model pretraining, has recently shown to reach state-of-theart performance on several NLP tasks, also including emotion prediction: the top-performing models in the EmotionX Challenge (Hsu and Ku, 2018) all employed a pre-trained BERT model. We also use the BERT model in our experiments and we find that it outperforms our biLSTM model. 3 GoEmotions Our dataset is composed of 58K Reddit comments, labeled for one or more of 27 emotion(s) or Neutral. 3.1 Selecting & Curating Reddit comments We us"
2020.acl-main.372,S18-1001,0,0.271559,"Ekman, 1992b) or Plutchik (Plutchik, 1980) emotions. Emotion expression and detection are central to the human experience and social interaction. With as many as a handful of words we are able to express a wide variety of subtle and complex emotions, and it has thus been a long-term goal to enable machines to understand affect and emotion (Picard, 1997). In the past decade, NLP researchers made available several datasets for language-based emotion classification for a variety of domains and applications, including for news headlines (Strapparava and Mihalcea, 2007), tweets (CrowdFlower, 2016; Mohammad et al., 2018), and narrative sequences (Liu et al., 2019), to name just a few. However, existing available datasets are (1) mostly small, containing up to several thousand instances, and (2) cover a limited emotion taxonomy, with coarse clas∗ Label(s) Table 1: Example annotations from our dataset. Introduction Work done while at Google Research. Data and code available at https://github.com/ google-research/google-research/tree/ master/goemotions. 1 Sample Text Recently, Bostan and Klinger (2018) have aggregated 14 popular emotion classification corpora under a unified framework that allows direct comparis"
2020.acl-main.372,S12-1033,0,0.359229,"Missing"
2020.acl-main.372,D19-1374,0,0.031486,"Missing"
2020.acl-main.372,N18-1202,0,0.152105,"Missing"
2020.acl-main.372,H05-1073,0,\N,Missing
2020.acl-main.372,W17-5203,0,\N,Missing
2020.acl-main.372,C18-1179,0,\N,Missing
2020.acl-main.372,W18-3505,0,\N,Missing
2020.acl-main.372,W18-6205,0,\N,Missing
2020.acl-main.372,P19-1452,0,\N,Missing
2020.acl-main.372,W18-5903,0,\N,Missing
2020.acl-main.372,N19-1423,0,\N,Missing
2020.acl-main.372,D19-1176,0,\N,Missing
2020.acl-main.372,E17-2092,0,\N,Missing
2020.acl-main.372,P17-1067,0,\N,Missing
2020.acl-main.617,P15-1067,0,0.151764,"in KGs. 2 Related Work Previous methods for KG embeddings also rely on geometric properties. Improvements have been obtained by exploiting either more sophisticated spaces (e.g., going from Euclidean to complex or hyperbolic space) or more sophisticated operations (e.g., from translations to isometries, or to learning graph neural networks). In contrast, our approach takes a step forward in both directions. Euclidean embeddings In the past decade, there has been a rich literature on Euclidean embeddings for KG representation learning. These include translation approaches (Bordes et al., 2013; Ji et al., 2015; Wang et al., 2014; Lin et al., 2015) or tensor factorization methods such as RESCAL (Nickel et al., 2011) or DistMult (Yang et al., 2015). While these methods are fairly simple and have few parameters, they fail to encode important logical properties (e.g., translations can’t encode symmetry). Complex embeddings Recently, there has been interest in learning embeddings in complex space, as in the ComplEx (Trouillon et al., 2016) and RotatE (Sun et al., 2019) models. RotatE learns rotations in complex space, which are very effective in capturing logical properties such as symmetry, anti-symmet"
2020.acl-main.617,P19-1466,0,0.0938151,"Missing"
2020.acl-main.617,N18-2053,0,0.0783045,"are very effective in capturing logical properties such as symmetry, anti-symmetry, composition or inversion. The recent QuatE model (Zhang et al., 2019) learns KG embeddings using quaternions. However, a downside is that these embeddings require very highdimensional spaces, leading to high memory costs. Deep neural networks Another family of methods uses neural networks to produce KG embeddings. For instance, R-GCN (Schlichtkrull et al., 2018) extends graph neural networks to the multirelational setting by adding a relation-specific aggregation step. ConvE and ConvKB (Dettmers et al., 2018; Nguyen et al., 2018) leverage the expressiveness of convolutional neural networks to learn entity embeddings and relation embeddings. More recently, the KBGAT (Nathani et al., 2019) and A2N (Bansal et al., 2019) models use graph attention networks for knowledge graph embeddings. A downside of these methods is that they are computationally expensive as they usually require pre-trained KG embeddings as input for the neural network. Hyperbolic embeddings To the best of our knowledge, MuRP (Balaˇzevi´c et al., 2019) is the 6902 only method that learns KG embeddings in hyperbolic space in order to target hierarchical"
2020.acl-main.617,W15-4007,0,0.24049,"based on solely reflections or rotations (Section 5.4). Finally, in high dimensions, we expect hyperbolic models with trainable curvature to learn the best geometry, and perform similarly to their Euclidean analogues (Section 5.5). 5.1 ATT H then combines the two representations using hyperbolic attention (Equation 7) and applies a hyperbolic translation: #entities 41k 15k 123k Experimental setup Datasets We evaluate our approach on the link prediction task using three standard competition benchmarks, namely WN18RR (Bordes et al., 2013; Dettmers et al., 2018), FB15k-237 (Bordes et al., 2013; Toutanova and Chen, 2015) and YAGO3-10 (Mahdisoltani et al., 2013). WN18RR is a subset of WordNet containing 11 lexical relationships between 40,943 word senses, and has a natural hierarchical structure, e.g., (car, hypernym of, sedan). FB15k-237 is a subset of Freebase, a collaborative KB of general world knowledge. FB15k-237 has 14,541 entities and 237 relationships, some of which are non-hierarchical, such as born-in or nationality, while others have natural hierarchies, such as part-of (for organizations). YAGO3-10 is a subset of YAGO3, containing 123,182 entities and 37 relations, where most relations provide des"
2020.acl-main.617,P19-1026,0,0.045694,"Missing"
2020.acl-main.617,2020.emnlp-main.595,0,0.206741,"Missing"
2021.eacl-main.246,N18-2118,0,0.177052,"Missing"
2021.eacl-main.246,C16-1189,0,0.0603429,"Missing"
2021.eacl-main.246,D19-1402,1,0.598697,"s like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long text due to significant information loss in the projection operation. On another side, recurrent architectures represent long sentences well, but the sequential nature of the computations increases latency requirements and makes it difficult to launch on-device. Recently, self-attention based architectures like BERT (Devlin et al., 2018) have demonstrated remarkable success in capturing long term dependencies in the input text via purely attention mechanisms. BERT’s model architecture is a multi-layer bidirectional Transformer encoder based on the or"
2021.eacl-main.246,N16-1062,0,0.0488543,"Missing"
2021.eacl-main.246,P19-1441,0,0.0209122,"approaches like BERT and even outperforms small-sized BERT variants with significant resource savings – reduces the embedding memory footprint from 92.16 MB to 1.7 KB and requires 16× less computation overhead, which is very impressive making it the fastest and smallest on-device model. 1 Introduction Transformers (Vaswani et al., 2017) based architectures like BERT (Devlin et al., 2018), XL-net ∗ Work done during internship at Google Work done while at Google AI † Zornitsa Kozareva Google Mountain View, CA, USA zornitsa@kozareva.com (Yang et al., 2019), GPT-2 (Radford et al., 2019), MT-DNN (Liu et al., 2019a), RoBERTA (Liu et al., 2019b) reached state-of-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva"
2021.eacl-main.246,2021.ccl-1.108,0,0.0599243,"Missing"
2021.eacl-main.246,W17-5530,0,0.041673,"Missing"
2021.eacl-main.246,D18-1092,1,0.912621,"Missing"
2021.eacl-main.246,P19-1368,1,0.790728,"et al., 2019b) reached state-of-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long tex"
2021.eacl-main.246,N19-1339,1,0.743191,"f-the-art performance on tasks like machine translation (Arivazhagan et al., 2019), language modelling (Radford et al., 2019), text classification benchmarks like GLUE (Wang et al., 2018). However, these models require huge amount of memory and need high computational requirements making it hard to deploy to small memory constraint devices such as mobile phones, watches and IoT. Recently, there have been interests in making BERT lighter and faster (Sanh et al., 2019; McCarley, 2019). In parallel, recent on-device works like SGNN (Ravi and Kozareva, 2018), SGNN++ (Ravi and Kozareva, 2019) and (Sankar et al., 2019) produce lightweight models with extremely low memory footprint. They employ a modified form of LSH projection to dynamically generate a fixed binary projection representation, P(x) ∈ [0, 1]T for the input text x using word or character n-grams and skip-grams features, and a 2-layer MLP + softmax layer for classification. As shown in (Ravi and Kozareva, 2018) these models are suitable for short sentence lengths as they compute T bit LSH projection vector to represent the entire sentence. However, (Kozareva and Ravi, 2019) showed that such models cannot handle long text due to significant infor"
2021.eacl-main.246,W04-2319,0,0.166867,"Missing"
2021.eacl-main.246,W18-5446,0,0.0742515,"Missing"
2021.eacl-main.250,P16-1046,0,0.0493217,"Missing"
2021.eacl-main.250,N19-1326,0,0.0354635,"Missing"
2021.eacl-main.250,D19-1402,1,0.811554,"to store any embedding matrices, since the projections are dynamically computed. This further enables user privacy by performing inference directly on device without sending user data (e.g., personal information) to the server. The embedding memory size is reduced from O(V ) to O(K), where V is the token vocabulary size and K &lt;&lt; V , is the binary LSH projection size. The projection representations can operate on either word or character level, and can be used to represent a sentence or a word depending on the NLP application. For instance, recently the Projection Sequence Networks (ProSeqo) (Kozareva and Ravi, 2019) used BiLSTMs over word-level projection representations to represent long sentences and achieved close to state-ofthe-art results in both short and long text classification tasks with varying amounts of supervision and vocabulary sizes. 2871 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2871–2876 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Figure 2: Memory for V look-up vectors for each token vs storing K(&lt;&lt; V ) vectors and linearly combining them for token representation. We consider K = 1120 follow"
2021.eacl-main.250,P18-1157,0,0.0232683,"Missing"
2021.eacl-main.250,D14-1162,0,0.0940837,"requently colliding with the projection representations of other valid words. Overall, our studies showcase the robustness of LSH projection representations and resistance to misspellings. Due to their effectiveness, we believe that in the future, text representations using LSH projections can go beyond memory constrained settings and even be exploited in large scale models like Transformers (Vaswani et al., 2017). One way to remove the dependency on the vocabulary size is to learn a smaller matrix, WK ∈ Rd×K (K &lt;&lt; V ), as shown in Figure 2. For instance, 300-dimensional Glove embeddings, WV (Pennington et al., 2014b) with 400k vocabulary size occupies > 1 GB while the WK occupies only ≈ 1.2 MB for K = 1000 yielding a 1000× reduction in size. Instead of learning a unique vector for each token in the vocabulary, we can think of the columns of this WK matrix as a set of basis vectors and each token can be represented as a linear combination of basis vectors in WK as in Figure 1. We select the basis vectors from WK for each token with a fixed K-bit binary vector instead of a V -bit one-hot vector. The LSH Projection function, P (Figure 3)(Ravi, 2017, 2019) used in SGNN (Ravi and Kozareva, 2018) and ProSeqo"
2021.eacl-main.250,N18-1202,0,0.132859,"Missing"
2021.eacl-main.250,P19-1561,0,0.0217079,"et al., 2018) and train two-layer BiLSTMs (with both word-only and word-piece tokenization) for comparable accuracies with respect to the projection based models for a fair comparison. By word-only tokenization, we mean that models encode input words using a lookup table for each word. In our setup, we test the robustness of the neural classifiers by subjecting the corresponding test sets to common misspellings and omissions. We consider the following perturbation operations: randomly dropping, inserting, and swapping internal characters within words of the input sentences (Gao et al., 2018; Pruthi et al., 2019) 1 . We decide to perturb each word in a sentence with a fixed probability, Pperturb . Following (Ravi and Kozareva, 2018), we fix the projection dimension to K = 1120. 3.1 Datasets For evaluation purposes, we use the following text classification datasets for dialog act classification MRDA (Shriberg et al., 2004) and SWDA (Godfrey et al., 1992; Jurafsky et al., 1997), for intent prediction ATIS (T¨ur et al., 2010) and long text classification Amazon Reviews (Zhang et al., 2015) and Yahoo! Answers (Zhang et al., 2015). Table 1 shows the characteristics of each dataset. Tasks ATIS (Dialog act)"
2021.eacl-main.250,D18-1092,1,0.769169,"Missing"
2021.eacl-main.250,P19-1368,1,0.737077,"for text representations The dependency on vocabulary size V , is one of the primary reasons for the huge memory footprint of embedding matrices. It is common to represent a token, x by one-hot representation, Y(x) ∈ [0, 1]V and a distributed representation of the token is obtained by multiplying the one-hot representation with the embedding matrix, WV ∈ Rd×V as in UV (x) = WV ∗ Y(x)> ∈ Rd 1. Classification with perturbed inputs, where we show that Projection based networks 1) Projection Sequence Networks (ProSeqo) (Kozareva and Ravi, 2019) and 2) SelfGoverning Neural Networks (SGNN) models (Ravi and Kozareva, 2019) evaluated with perturbed LSH projections are robust to misspellings and transformation attacks, while we observe significant drop in performance for BiLSTMs and fine-tuned BERT classifiers. 2. Perturbation Analysis, where we test the robustness of the projection approach by directly analyzing the changes in representations when the input words are subject to the char misspellings. The purpose of this study is to examine if the words or sentences with misspelling are nearby in the projection space instead of frequently colliding with the projection representations of other valid words. Overall"
2021.eacl-main.250,N19-1339,1,0.845721,"re pre-trained word embeddings like Word2Vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014a) and ELMo (Peters et al., 2018). They help initialize the neural models, lead to faster convergence and have improved performance for numerous application such as Question Answering (Liu et al., 2018), Summarization (Cheng and Lapata, 2016), Sentiment Analysis (Yu et al., 2017). While word embeddings are powerful in unlimited constraints such as computation power ∗ Work done during internship at Google Work done while at Google AI † This led to interesting research by (Ravi and Kozareva, 2018; Sankar et al., 2019), who showed that word embeddings can be replaced with lightweight binary Locality-Sensitive Hashing (LSH) based projections learned on-the-fly. The projection approach surmounts the need to store any embedding matrices, since the projections are dynamically computed. This further enables user privacy by performing inference directly on device without sending user data (e.g., personal information) to the server. The embedding memory size is reduced from O(V ) to O(K), where V is the token vocabulary size and K &lt;&lt; V , is the binary LSH projection size. The projection representations can operate"
2021.eacl-main.250,W04-2319,0,0.0258796,"Missing"
2021.eacl-main.250,D13-1170,0,0.00703734,"Missing"
2021.eacl-main.250,D17-1056,0,0.0585607,"Missing"
2021.naacl-main.380,P13-1100,1,0.854454,"Missing"
2021.naacl-main.380,P19-1102,0,0.243296,"multi-document summarization using a pre-trained encoder-decoder 1 All our code publicly available at: https://github. Transformer model (Lewis et al., 2019), depicted com/amazon-research/BartGraphSumm. in Fig. 1, along with an efficient encoding mech4768 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4768–4779 June 6–11, 2021. ©2021 Association for Computational Linguistics anism to encode longer input texts. To this end, we first provide a strong baseline for MDS on the Multi-News dataset (Fabbri et al., 2019) using a pre-trained encoder-decoder model, called BART (Lewis et al., 2019). Next, we incorporate a Longformer-based approach (Beltagy et al., 2020) into the pre-trained BART model, replacing the quadratic memory growth of the full self-attention mechanism with an efficient context window-based attention mechanism that scales the memory linearly w.r.t. the input length. This enables us to encode longer documents than previous work. This efficient encoding mechanism comprises local and global attention mechanisms that address the challenge of modeling inter-document context. approaches on the"
2021.naacl-main.380,D19-1428,0,0.0836103,"g scores based on relevancy and redundancy. Li et al. (2020) further showed the usefulness of pre-trained language models to improve the performance on MDS. However, this approach lacks a pre-trained decoder, and it also limits the document length that can be encoded by the pre-trained language models. In contrast, our work utilizes the pre-trained seq2seq BART (Lewis et al., 2019) model to improve the performance on MDS. We have also incorporated the Longformerbased attention mechanism (Beltagy et al., 2020) into BART model to encode long documents. To encode graphs into an MDS neural model, Fan et al. (2019) constructed a semantic graph representing key phrases and entities from the documents, as well as their expressed relationships; they used linearized forms of these graphs as inputs to their Transformer model. In contrast, we use dual encoders for encoding both documents text and linearized graph text information. Recently, Li et al. (2020) constructed a similarity graph, topic graph, and discourse graph between input documents and encoded this information directly, rather than in linearized form, into a Transformer. In our work, we build semantic graphs at the sentence level and create a con"
2021.naacl-main.380,D18-1443,0,0.151809,"Missing"
2021.naacl-main.380,N18-1065,0,0.14692,"Missing"
2021.naacl-main.380,D18-1446,0,0.0451338,"Missing"
2021.naacl-main.380,C16-1023,0,0.060189,"Missing"
2021.naacl-main.380,2020.acl-main.555,0,0.348161,"ory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.1 Decoder Figure 1: Illustration of our dual-encoder approach to summarizing multi-document clusters with graph encodings. The truncated concatenated text contains the beginnings of each cluster document; the graphs contain information from the full documents. for external deep context representations. Liu and Lapata (2019) and Li et al. ("
2021.naacl-main.380,C18-1101,0,0.0200875,"add auxiliary graph encodings) leads sentence relation graphs. Baralis et al. (2013) built to significant improvements on the Multi-News graphs over sets of terms, rather than sentences. Li dataset (achieving state-of-the-art), overall leading et al. (2016) built a graph over event mentions and to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). Based on vari- their relationships, in order to summarize news events using sentence extraction techniques. Liu ous automatic evaluation metrics, we show that adding graph encodings can help the model ab- et al. (2015) and Liao et al. (2018) leveraged AMR formalism to convert source text into AMR graphs stract away from the specific lexical content of the input and generate summaries that are more ab- and then generate a summary using these graphs. stractive. Further human evaluation shows that More recently, the introduction of larger datasets they are also more informative and factually more for MDS has enabled researchers to train neural consistent with their input documents. We also test models for multi-document summarization. Liu our model with auxiliary graph encodings on the et al. (2018) introduced a large-scale dataset"
2021.naacl-main.380,N15-1114,0,0.075047,"Missing"
2021.naacl-main.380,P19-1500,0,0.234783,"over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents.1 Decoder Figure 1: Illustration of our dual-encoder approach to summarizing multi-document clusters with graph encodings. The truncated concatenated text contains the beginnings of each cluster document; the graphs contain information from the full documents. for external deep context representations. Liu and Lapata (2019) and Li et al. (2020) have addressed the inter-document context modeling to some extent with local and global attention, and document-level similarity graphs. Further, Li et al. (2020) have addressed the later part of using external contextual information (large pre-trained language models, e.g., RoBERTa (Liu et al., 2019)) to improve the performance of MDS models. However, these pre-trained language models are (1) not scalable for long documents because of their encoding length limit and quadratic memory growth; and (2) they do not jointly explore alternate auxiliary information, e.g., semant"
2021.naacl-main.380,2021.ccl-1.108,0,0.0654593,"Missing"
2021.naacl-main.380,W04-3252,0,0.429601,"Missing"
2021.naacl-main.380,2020.emnlp-main.748,0,0.021931,"g graphs increases abstractiveness. Next, we concatenate the documents’ text with linearized graph text and give it has input to the BART model (‘BL-Graph-Concat’) which achieves slightly better results over the baseline. However, when we add the linearized graph text as a separate graph encoder (‘BL-Separate-Graph’; same as our ‘BART-Long-Graph’ model in Table 1), we achieve the best results. How abstractive are the summaries? Abstractive summarizers generate surprisingly extractive summaries, copying large fragments unmodified from the input documents into the summaries (Weber et al., 2018; Pilault et al., 2020). We hypothesize that providing graph representations of the input can help the model abstract away from the specific lexical content of the input and generate summaries that are more abstractive. Table 8 shows Different approaches of graph encodings. Tathe lexical overlap between the summaries and their ble 6 presents the results on various graph encoding inputs when truncating the input documents to difmethods. First, we replace the original input with ferent numbers of words, and when adding a graph linearized graph text and we observe a significant representation of the input (truncated to"
2021.naacl-main.380,J98-3005,0,0.387542,"acing the quadratic memory growth of the full self-attention mechanism with an efficient context window-based attention mechanism that scales the memory linearly w.r.t. the input length. This enables us to encode longer documents than previous work. This efficient encoding mechanism comprises local and global attention mechanisms that address the challenge of modeling inter-document context. approaches on the performance of the MDS system. 2 Related Work Researchers have been interested in automatically summarizing multiple documents since the late 1990s. First works (Mani and Bloedorn, 1997; Radev and McKeown, 1998) cited the gaining popularity of the World Wide Web (WWW) as a motivation for the task. They modeled multi-document collections as graph structures – perhaps influenced by the link structure of the WWW itself. Mani and Bloedorn (1997) summarized pairs of documents by building a graph representation of each and performing graph matching to find salient regions across both documents. Radev and McKeFurther, we build consolidated semantic graph own (1998) summarized multiple documents by representations of the multiple input documents mapping them to abstract template representations, and explore"
2021.naacl-main.380,N18-1081,0,0.0263445,"ns across both documents. Radev and McKeFurther, we build consolidated semantic graph own (1998) summarized multiple documents by representations of the multiple input documents mapping them to abstract template representations, and explore ways to incorporate them into the then generating text from the templates. encoder-decoder model. The semantic graph In the early 2000s, datasets from the Document for a given multi-document cluster is a compact Understanding Conference (DUC), which included representation of subject-predicate-object triplets human-written summaries for multi-document clus(Stanovsky et al., 2018) extracted from the text of ters, sparked increased research interest. In the documents; see Fig. 3 for an example. We pro- LexRank, Erkan and Radev (2004) extracted the pose a dual encoding mechanism that separately en- most salient sentences from a multi-document cluscodes the regular text of a multi-document cluster ter by constructing a graph representing pairwise and a text representation of its graph. The regular sentence similarities and running a PageRank altext is encoded by the pre-trained BART encoder, gorithm on the graph. Subsequent approaches folwhile the graph text is encoded by"
2021.naacl-main.380,D08-1079,0,0.0435598,"ti-document cluster ter by constructing a graph representing pairwise and a text representation of its graph. The regular sentence similarities and running a PageRank altext is encoded by the pre-trained BART encoder, gorithm on the graph. Subsequent approaches folwhile the graph text is encoded by a transformer lowed the same paradigm while improving diverencoder that is not pre-trained. sity of the extracted sentences (Wan and Yang, Empirically, we show that our approach (includ- 2006) or adding document-level information into ing the ability to use longer parts of the input doc- the graph (Wan, 2008). Dasgupta et al. (2013) incorporated dependency graph features into their uments and add auxiliary graph encodings) leads sentence relation graphs. Baralis et al. (2013) built to significant improvements on the Multi-News graphs over sets of terms, rather than sentences. Li dataset (achieving state-of-the-art), overall leading et al. (2016) built a graph over event mentions and to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). Based on vari- their relationships, in order to summarize news events using sentence extraction techniques. Liu ous automatic evaluation m"
2021.sigdial-1.7,2020.nlp4convai-1.6,0,0.453246,"ucted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watches while maintaining high performance. 2 Figure 1: Model architecture for SoDA On-device Sequence Labeling Neural Network. SoDa: On-device Sequence Labeling In this section, we describe the components of our SoDA architecture as shown in Figure 1. 2.1 O(V · d) which is infeasible for on-device applications where storage is limited. Here"
2021.sigdial-1.7,D16-1053,0,0.025745,"Missing"
2021.sigdial-1.7,Q16-1026,0,0.0288392,"rojections: We could use the dynamically constructed projection vector P(x) directly instead of embeddings to build the rest of our model. But to prevent the models from depending on static projection representations too strongly, we further condition or fine-tune the projections on specific sequence tagging task during training to learn better task-specific representations E(x). As noted, both projection conditioning operators result in a tiny number of additional model parameters M  V · d that are tuned during training. 2.1.2 Extending Character-level Representation using CNN Earlier work (Chiu and Nichols, 2016; Ma and Hovy, 2016) showed that CNNs can be effective to model morphological information within words and encode it within neural networks using character-level embeddings. However, these approaches typically compute both word-level (from pre-trained tables) and character-level embeddings (to model long sequence contexts) and combine them to construct word vector representations in their neural network architectures. However as we noted, word embedding lookup tables incur significant memory that are not suitable for on-device usecases. Previous results on sequence labeling (Ma and Hovy, 2016)"
2021.sigdial-1.7,D19-1402,1,0.894181,"Missing"
2021.sigdial-1.7,N19-1423,0,0.0637434,"Missing"
2021.sigdial-1.7,P15-1033,0,0.0239163,"pass through to the next time step. We use the following implementation in SoDA For an input sentence X = (x1 , x2 , ..., xn ) and corresponding sequence of projected embeddings E(X), where each et = [ePt · eCN Nt ] is a ddimensional vector, the LSTM layer in SoDA uses input, forget and output gates to compute a new state ht at time step t. For sequence tagging tasks, both left and right contexts are useful to represent information at any time step. Standard LSTM as well as other sequence models only account for previous history and know nothing about the future. We use a bi-directional LSTM (Dyer et al., 2015) to efficiently model both past and future information in our SoDA model. The only change required is 2.4 CRF Tagging Model For structured prediction tasks like sequence tagging, it is useful to model the dependencies between neighboring labels (Ling et al., 2015) and perform joint decoding of the label sequence for a given input sentence. For example, in sequence labeling tasks with BIO tagging scheme I-LOC label cannot follow B-PER. So, instead of decoding labels at every position separately, similarly to prior work, we perform joint decoding in our model using a condition random field (CRF)"
2021.sigdial-1.7,P19-1544,0,0.0384954,"Missing"
2021.sigdial-1.7,D15-1176,0,0.0872156,"Missing"
2021.sigdial-1.7,N18-2118,0,0.398401,"ngs of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watches while maintaining high performance. 2 Figure 1: Model architecture for SoDA On-device Sequence Labeling Neural Network. SoDa: On-device Sequence Labeling In t"
2021.sigdial-1.7,P16-1101,0,0.0277261,"the dynamically constructed projection vector P(x) directly instead of embeddings to build the rest of our model. But to prevent the models from depending on static projection representations too strongly, we further condition or fine-tune the projections on specific sequence tagging task during training to learn better task-specific representations E(x). As noted, both projection conditioning operators result in a tiny number of additional model parameters M  V · d that are tuned during training. 2.1.2 Extending Character-level Representation using CNN Earlier work (Chiu and Nichols, 2016; Ma and Hovy, 2016) showed that CNNs can be effective to model morphological information within words and encode it within neural networks using character-level embeddings. However, these approaches typically compute both word-level (from pre-trained tables) and character-level embeddings (to model long sequence contexts) and combine them to construct word vector representations in their neural network architectures. However as we noted, word embedding lookup tables incur significant memory that are not suitable for on-device usecases. Previous results on sequence labeling (Ma and Hovy, 2016) show that 58 charac"
2021.sigdial-1.7,D14-1162,0,0.0858935,"ng a sequence of words (x1 , x2 , ..., xn ), where xi refers to i-th word in the sentence, we first construct a sequence of vectors E(X) = (e1 , e2 , ..., en ) where ei denotes a vector representation for word xi . 2.1.1 Word Embedding via Projection Learning good representations for word types from the limited training data (as in slot extraction) is challenging since there are many parameters to estimate. Most neural network approaches for NLP tasks rely on word embedding matrices to overcome this issue. Almost every recent neural network model uses pre-trained word embeddings (e.g., Glove (Pennington et al., 2014), word2vec (Mikolov et al., 2013)) learned from a large corpus that are then plugged into the model and looked up to construct vector representations of individual words and optionally fine-tuned for the specific task. However, these embedding matrices are often huge and require lot of memory F(x) = {hf1 , w1 i, ..., hfK , wK i} 57 (1) where, fk represents each feature id (Fingerprint of the raw character skipgram) and wk its corresponding weight (observed count in the specific input x). We use locality-sensitive projections (Ravi, 2017) to dynamically transform the intermediate feature vector"
2021.sigdial-1.7,P19-1519,0,0.245982,"lf-attention and CRF layer. The resulting network is compact, does not require storing any pre-trained word embedding tables or huge parameters, and is suitable for on-device applications. 56 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devic"
2021.sigdial-1.7,D19-1214,0,0.200425,"ng network is compact, does not require storing any pre-trained word embedding tables or huge parameters, and is suitable for on-device applications. 56 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 56–65 July 29–31, 2021. ©2021 Association for Computational Linguistics • Conducted exhaustive evaluation on multiple conversational slot extraction tasks and demonstrate that our on-device model SoDA reaches state-of-the-art performance and even outperforms larger, non-on-device models like Capsule-NLU (Zhang et al., 2019), StackPropagation (Qin et al., 2019), Interrelated SFFirst with CRF (E et al., 2019), joint BiLSTM (Hakkani-Tur et al., 2016), attention RNN (Liu and Lane, 2016), gated attention (Goo et al., 2018) and even BERT models (Sanh et al., 2019). • Our on-device SoDA model also significantly outperforms state-of-the-art on-device slot extraction models of (Ahuja and Desai, 2020), which are based on convolution and are further compressed with structured pruning and distillation. • Finally, we conduct a series of ablation studies that show SoDA ’s compact size needed for conversational assistant devices like Google and Alexa, smart watch"
2021.sigdial-1.7,D18-1092,1,0.867006,"Missing"
2021.sigdial-1.7,P19-1368,1,0.839552,"mation needed for language understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embeddi"
2021.sigdial-1.7,2021.eacl-main.250,1,0.765897,"e understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embedding-free projections a"
2021.sigdial-1.7,2021.eacl-main.246,1,0.806502,"e understanding. The model will operate entirely on the device chip and will not send or request any external information. Such ondevice models should have low latency, small memory and model sizes to fit on memory-constrained devices like mobile phones, watches and IoT. Recently, there has been a lot of interest and novel research in developing on-device models. Large body of work focuses on wake word detection (Lin et al., 2018; He et al., 2017), text classification like intent recognition (Ravi and Kozareva, 2018), news and product reviews (Kozareva and Ravi, 2019; Ravi and Kozareva, 2019; Sankar et al., 2021b,a). In this paper, we propose a novel on-device neural sequence tagging model called SoDA . Our novel approach uses embedding-free projections and character-level information to construct compact word representations and learns a sequence model on top of the projected representations using a combination of bidirectional LSTM with selfattention and CRF model. We conduct exhaustive evaluation on different conversational slot extraction datasets. The main contributions of our work are as follows: We propose a novel on-device neural sequence labeling model which uses embedding-free projections a"
C10-1106,W09-1804,1,0.887675,"Missing"
C10-1106,P08-1085,0,0.277853,"Missing"
C10-1106,P07-1094,0,0.0945569,"Missing"
C10-1106,P09-1039,0,0.0627814,"Missing"
C10-1106,J94-2001,0,0.826927,"solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech (POS) tagging with a dictionary as formulated by Merialdo (1994) is: given a raw word sequence and a dictionary of legal POS tags for each word type, tag each word token in the text. A common approach to modeling such sequence labeling problems is to build a bigram Hidden Markov Model (HMM) parameterized by tag-bigram transition probabilities P (ti |ti−1 ) and word-tag emission probabilities P (wi |ti ). Given a word sequence w and a tag sequence t, of length N , the joint probability P (w, t) is given by: P (w, t) = N Y i=1 P (wi |ti ) · P (ti |ti−1 ) (1) (2) Ravi and Knight (2009) attack the Merialdo task in two stages. In the first stage, they search fo"
C10-1106,C04-1197,0,0.0356354,"Missing"
C10-1106,D08-1085,1,0.894014,"Missing"
C10-1106,P09-1057,1,0.934644,"rvised Tagging Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang University of Southern California Information Sciences Institute {sravi,avaswani,knight,chiang}@isi.edu Abstract We can train this model using the Expectation Maximization (EM) algorithm (Dempster and Rubin, 1977) which learns P (wi |ti ) and P (ti |ti−1 ) that maximize the likelihood of the observed data. Once the parameters are learnt, we can find the best tagging using the Viterbi algorithm. Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech"
C10-1106,P05-1044,0,0.0539854,"Missing"
D08-1085,D07-1090,0,0.0388751,"a bit too rosy, according to our empirical message equivocation curves. Our experience confirms this as well, as 1-gram frequency counts over a 173-letter cipher are generally insufficient to pin down a solution. 6 Conclusion We provide a method for deciphering letter substitution ciphers with low-order models of English. This method, based on integer programming, requires very little coding and can perform an optimal search over the key space. We conclude by noting that English language models currently used in speech recognition (Chelba and Jelinek, 1999) and automated language translation (Brants et al., 2007) are much more powerful, employing, for example, 7-gram word models (not letter models) trained on trillions of words. Obtaining optimal keys according to such models will permit the automatic decipherment of shorter ciphers, but this requires more specialized search than what is provided by general integer programming solvers. Methods such as these should also be useful for natural language decipherment problems such as character code conversion, phonetic decipherment, and word substitution ciphers with applications in machine translation (Knight et al., 2006). 819 7 Acknowledgements The auth"
D08-1085,H94-1005,0,0.00815647,"nguage model, whose job is to assign some probability to any sequence of letters. According to a 1gram model of English, the probability of a plaintext p1 ...pn is given by: P (p1 ...pn ) = P (p1 ) · P (p2 ) · ... · P (pn ) That is, we obtain the probability of a sequence by multiplying together the probabilities of the individual letters that make it up. This model assigns a probability to any letter sequence, and the probabilities of all letter sequences sum to one. We collect letter probabilities (including space) from 50 million words of text available from the Linguistic Data Consortium (Graff and Finch, 1994). We also estimate 2- and 3-gram models using the same resources: P (p1 ...pn ) = each letter in the phrase, estimates the same probability for both the phrases “het oxf” and “the fox”, since the same letters occur in both phrases. On the other hand, the 2-gram and 3-gram models, which take context into account, are able to distinguish between the English and non-English phrases better, and hence assign a higher probability to the English phrase “the fox”. Model 1-gram 2-gram 3-gram = 1-gram: 4.19 2-gram: 3.51 3-gram: 2.93 P (p1 |ST ART ) · P (p2 |p1 ) · P (p3 |p2 ) · P (p1 |ST ART ) · P (p2 |"
D08-1085,P06-2065,1,0.397315,"erment accuracy varies as a function of cipher length and n-gram order. We also make an empirical investigation of Shannon’s (1949) theory of uncertainty in decipherment. 1 Introduction A number of papers have explored algorithms for automatically solving letter-substitution ciphers. Some use heuristic methods to search for the best deterministic key (Peleg and Rosenfeld, 1979; Ganesan and Sherman, 1993; Jakobsen, 1995; Olson, 2007), often using word dictionaries to guide that search. Others use expectation-maximization (EM) to search for the best probabilistic key using letter n-gram models (Knight et al., 2006). In this paper, we introduce an exact decipherment method based on integer programming. We carry out extensive decipherment experiments using letter n-gram models, and we find that our accuracy rates far exceed those of EM-based methods. We also empirically explore the concepts in Shannon’s (1949) paper on information theory as applied to cipher systems. We provide quantitative plots for uncertainty in decipherment, including the famous unicity distance, which estimates how long a cipher must be to virtually eliminate such uncertainty. • We outline an exact letter-substitution decipherment me"
D08-1093,P07-1038,0,0.020617,"slations can be produced and measured. Although a real estimate of the impact of a parser design decision in this scenario can only be gauged from the quality of the translations produced, it is impractical to create such estimates for each design decision. On the other hand, estimates using the solution proposed in this paper can be obtained fast, before submitting the parser output to a costly training procedure. 2 Related Work and Experimental Framework There have been previous studies which explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is neces"
D08-1093,P08-1087,0,0.0121278,"notation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et"
D08-1093,H91-1060,0,0.0220532,"hn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee reg"
D08-1093,P05-1022,0,0.765075,"mains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are consistent with these studies. For the Brown corpus, the test set was formed from every tenth sentence in the corpus (Gildea, 2001). Training Set Test Set WSJ sec. 02-21 (39,832 sent.) WSJ sec. 24 WSJ sec. 23 Brown-test Sent. count 1308 2343 2186 Charniak accuracy 90.48 91.13 86.34 Here we investigate algorithms for predicting the accuracy of a parser P on sentences, chunks of sentences, and whole corpora. We also investigate and contrast several scenarios for prediction: (1) the predictor looks at the input text only, (2) the predic"
D08-1093,2003.mtsummit-papers.6,1,0.676395,"accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse"
D08-1093,P98-1035,0,0.00969915,"es. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees pr"
D08-1093,P05-1066,0,0.0104477,"ta from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of"
D08-1093,J03-4003,0,0.0828782,"rees for the Brown test set helps us decide which prediction is better. Our predictions are much closer to the actual Charniak parser performance on the Brown-test set, with the adjusted prediction at 86.96 compared to the actual F-score of 86.34. 6 Ranking Parser Performance One of the main goals for computing F-score figures (either by traditional PARSEVAL evaluation against gold standards or by methods such as the one proposed in this paper) is to compare parsing accuracy when confronted with a choice between various parser deployments. Not only are there many parsing techniques available (Collins, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008), but recent annotation efforts in providing training material for statistical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experi"
D08-1093,N04-1035,1,0.557497,"ser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample s"
D08-1093,W01-0521,0,0.112482,"h explored the problem of automatically predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-m"
D08-1093,W97-0302,0,0.049047,"tical parsing (LDC, 2005; LDC, 2006a; LDC, 2006b; LDC, 2006c; LDC, 2007) have compounded the difficulty of the choices (“Do I parse using parser X?”, “Do I train parser X using the treebank Y or Z?”). In this section, we show how our predictor can provide guidance when dealing with some of these choices, namely the choice of the training material to use with a statistical parser, prior to its application in an NLP task. For the experiments reported in this paper, we use as parser P, our in-house implementation of the Collins parser (Collins, 2003), to which various speed-related enhancements (Goodman, 1997) have been applied. This choice has been made to better reflect a scenario in which parser P would be used in a data-intensive application such as syntax-driven machine translation, in which the parser must be able to run through hundreds of millions of training words in a timely manner. We use the more accurate, but slower Charniak parser (Charniak and Johnson, 2005) as the reference parser Pref in our predictor (see Section 3.3). In order to predict the Collinsstyle parser behavior on the ranking task, we use the same predictor model (including feature weights and adjustment parameters) that"
D08-1093,W01-1203,0,0.0212184,"the absence of gold-standard parse trees. In this paper, we propose a technique that automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain o"
D08-1093,2006.amta-papers.8,1,0.772156,"have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to c"
D08-1093,P08-1067,0,0.0354199,"r prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lexBool100 rootSYN UNK LM-PPL puncSYN de"
D08-1093,P03-1054,0,0.011043,"e of parser P against Pref . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool100"
D08-1093,N07-1006,0,0.0122313,"olean-based counterparts. Since these features measure different but overlapping pieces of the information available, it is to be expected that some of the feature combinations would provide better correlation that the individual features, but the gains are not strictly additive. By taking the individual features that provide the best discriminative power, we are able to get a correlation score of 0.42 on the test set. 3.5 dev (r) 0.55 test (r) 0.42 Optimizing for Maximum Correlation If our goal is to obtain the highest correlations with the F-score measure, is SVM regression the best method? Liu and Gildea (2007) recently introduced Maximum Correlation Training (MCT), a search procedure that follows the gradient of the formula for correlation coefficient (r). We implemented MCT, but obtained no better results. Moreover, it required many random re-starts just to obtain results comparable to SVM regression (Table 1). 4 WSJ-test (r) 0.42 0.61 0.62 0.69 0.79 WSJ-test (rms error) 0.098 0.026 0.019 0.015 0.011 Table 2: Performance of predictor on n-sentence chunks from WSJ-test (Correlation and rms error between actual/predicted accuracies). Table 1: Comparison of correlation (r) obtained using MCT versus S"
D08-1093,W06-1606,1,0.742809,"ins. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is implicitly given in the large literature on parsing to date is to have human annotators build parse trees for a sample set from the domain of interest, and conseq"
D08-1093,J93-2004,0,0.0309262,"e is to have human annotators build parse trees for a sample set from the domain of interest, and consequently use them to compute a PARSEVAL (Black et al., 1991) score that is indicative of the intrinsic performance of the parser. Given the wide range of domains and genres for which NLP applications are of interest, combined with the high expertise required from human annotators to produce parse tree annotations, this recipe is, albeit precise, too expensive. The other recipe that is currently used on a large scale is to measure the performance of a parser on existing treebanks, such as WSJ (Marcus et al., 1993), and assume that the accuracy measure will carry over to the domains of interest. This recipe, albeit cheap, cannot provide any guarantee regarding the performance of a parser on a new domain, and, as experiments in this paper show, can give wrong indications regarding important decisions for the design of NLP systems that use a syntactic parser as an important component. This paper proposes another method for measuring the performance of a parser on a given domain that is both cheap and effective. It is a fully automated procedure (no expensive annotation involved) that uses properties of bo"
D08-1093,P06-1043,0,0.604855,"y predicting the task difficulty for various NLP applications. (Albrecht and Hwa, 2007) presented a regression based method for developing automatic evaluation metrics for machine translation systems without directly relying on human reference translations. (Hoshino and Nakagawa, 2007) built a computer-adaptive system for generating questions to teach English grammar and vocabulary to students, by predicting the difficulty level of a question using various features. There have been a few studies of English parser accuracy in domains/genres other than WSJ (Gildea, 2001; Bacchiani et al., 2006; McClosky et al., 2006), but in order to make measurements for such studies, it is necessary to have gold-standard parses in the non888 WSJ domain of interest. Gildea (2001) studied how well WSJ-trained parsers do on the Brown corpus, for which a gold standard exists. He looked at sentences with 40 words or less. (Bacchiani et al., 2006) carried out a similar experiment on sentences of all lengths, and (McClosky et al., 2006) report additional results. The table below shows results from our own measurements of Charniak parser1 (Charniak and Johnson, 2005) accuracy (F-measure on sentences of all lengths), which are c"
D08-1093,P03-1021,0,0.00309409,"Missing"
D08-1093,N07-1051,0,0.0277637,"f . We use this F-measure score as a feature for prediction. For the experiments presented in this section we use as Pref , the parser from (Bikel, 2002). Intuitively, the requirement for choosing parser Pref in conjunction with parser P seems to be that they are different enough to produce non-identical trees when presented with the same input, and at the same time to be accurate enough to produce reliable parse trees. The choice of P as (Charniak and Johnson, 2005) and Pref as (Bikel, 2002) fits this bill, but many other choices can be made regarding Pref , such as (Klein and Manning, 2003; Petrov and Klein, 2007; McClosky et al., 2006; Huang, 2008). We leave the task of creating features based on the consensus of multiple parsers as future work. The correlation given by the reference parser– based feature Pref on the test set is the highest among all the features we explored. Feature set Pref 3.4 dev (r) 0.40 test (r) 0.36 The Aggregated Power of Features The table below lists all the individual features we have described in this section, sorted according to the correlation value obtained on the test set. Feature set Pref labelSYN lexCount500 lexBool500 lexCount1000 lexBool1000 Length lexCount100 lex"
D08-1093,J01-2004,0,0.0635174,"automatically takes into account certain characteristics of the domains of interest, and accurately predicts parser performance on data from these new domains. As a result, we have a cheap (no annotation involved) and effective recipe for measuring the performance of a statistical parser on any given domain. 1 Introduction Statistical natural language parsers have recently become more accurate and more widely available. As a result, they are being used in a variety of applications, such as question answering (Hermjakob, 2001), speech recognition (Chelba and Jelinek, 1998), language modeling (Roark, 2001), language generation (Soricut, 2006) and, most notably, machine translation (Charniak et al., 2003; Galley et al., 2004; Collins et al., 2005; Marcu et al., 2006; Huang et al., 2006; Avramidis and Koehn, 2008). These applications are employed on a wide range of domains and genres, and therefore the question of how accurate a parser is on the domain and genre of interest becomes acute. Ideally, one would want to have available a recipe for precisely answering this question: “given a parser and a particular domain of interest, how accurate are the parse trees produced?” The only recipe that is"
D08-1093,C98-1035,0,\N,Missing
D08-1093,P06-1121,1,\N,Missing
D12-1136,P05-1018,0,0.086765,"Missing"
D12-1136,J92-1002,0,0.561627,"Missing"
D12-1136,N03-2003,0,0.21134,"arrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Shannon (1951). An extensive bibliography covering early related work (e.g., insights into the structure of language via information theory, entropy estimates via other techniques and/or for different languages, as well as a broad range of applications of such estimates) can be found in (Cover and King, 1978). More recently, Brown et. al (1992) computed a"
D12-1136,D11-1054,0,0.262442,"al typing efforts using mobile device keypads, examine the speed and cognitive load of different input methods, and evaluate with emprical user studies in lab settings (James and Reischel, 2001; How and Kan, 2005), where the underlying technique for language prediction can be as simple as unigram frequency (James and Reischel, 2001), or restricted to narrow domains such as grocery shopping lists (Nurmi et al., 2009). In addition, to the best of our knowledge, no previous work in predictive text input addressed the conversationl setting. As discussed in Section 1, the response generation task (Ritter et al., 2011) also considered the converstational setting, but the MT-based technique was not well-suited to produce responses as intended by the user. There has been extensive previous research in language modeling (Rosenfeld, 2000). While previous work has explored Web text sources that are “better matched to a conversational speaking style” (Bulyko et al., 2003), we are not aware of much previous work that has taken advantage of information in the stimulus for word predictions in responses. Previous work on entropy of language stems from the field of information theory (Shannon, 1948), starting with Sha"
D18-1092,C16-1189,0,0.563028,"of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has becom"
D18-1092,N16-1062,0,0.555306,"ut dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep ne"
D18-1092,I17-4004,0,0.10651,"Missing"
D18-1092,W17-5530,0,0.605857,"-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the biggest obstacles to deploy"
D18-1092,N06-1036,0,0.0153702,"lex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017). We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. 4.3 Acc. 33.7 47.3 71.0 77.0 74.0 69.9 73.1 74.2 73.8 80.1 83.1 Table 2: SwDA Dataset Results Method Majority Class (baseline)(Ortega and Vu, 2017) Naive Bayes (baseline) (Khanpour et al., 2016) Graphical Model (Ji and Bilmes, 2006) CNN (Lee and Dernoncourt, 2016) RNN+Attention(Ortega and Vu, 2017) RNN (Khanpour et al., 2016) SGNN: Self-Governing Neural Network (ours) Acc. 59.1 74.6 81.3 84.6 84.3 86.8 86.7 Table 3: MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). 5 Conclusion We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and"
D18-1092,D12-1136,1,0.724513,"cation models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and wo"
D18-1092,P13-1036,1,0.784609,"nt over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy. 1 There are multiple strategies to build lightweight text classification models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of u"
D18-1092,W04-2319,0,0.498594,"in standard neural networks for feasible training. The binary representation is significant since this results in a significantly compact representation for the Computing Projections. We employ an efficient randomized projection method for the projection step. We use locality sensitive hashing (LSH) (Charikar, 2002) to model the underlying projection operations in SGNN. LSH is typically used as a dimensionality reduction technique for clustering (Manning et al., 2008). LSH allows us to project similar inputs ~xi or interme806 • MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. projection network parameters that in turn considerably reduces the model size. SGNN Parameters. In practice, we employ T different projection functions Pj=1...T , each resulting in d-bit vector that is concatenated to form the projected vector ip in Equation 5. T and d vary depending on the projection network parameter configuration specified for P and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a single projection matrix of size T · d or T separate matrices o"
D18-1092,J00-3003,0,0.228023,"Missing"
D18-1092,P17-2083,0,0.0405934,"s to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the bigges"
D18-1105,C16-1189,0,0.226412,"of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has becom"
D18-1105,N16-1062,0,0.163316,"ut dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep ne"
D18-1105,I17-4004,0,0.0477525,"Missing"
D18-1105,W17-5530,0,0.143652,"-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the biggest obstacles to deploy"
D18-1105,N06-1036,0,0.0172243,"lex neural network architectures such as deep CNN (Lee and Dernoncourt, 2016) and RNN variants (Khanpour et al., 2016; Ortega and Vu, 2017). We believe that the compression techniques like locality sensitive projections jointly coupled with non-linear functions are effective at capturing lowdimensional semantic text representations that are useful for text classification applications. 4.3 Acc. 33.7 47.3 71.0 77.0 74.0 69.9 73.1 74.2 73.8 80.1 83.1 Table 2: SwDA Dataset Results Method Majority Class (baseline)(Ortega and Vu, 2017) Naive Bayes (baseline) (Khanpour et al., 2016) Graphical Model (Ji and Bilmes, 2006) CNN (Lee and Dernoncourt, 2016) RNN+Attention(Ortega and Vu, 2017) RNN (Khanpour et al., 2016) SGNN: Self-Governing Neural Network (ours) Acc. 59.1 74.6 81.3 84.6 84.3 86.8 86.7 Table 3: MRDA Dataset Results in further speed up for high-dimensional feature spaces. This amounts to a huge savings in storage and computation cost wrt FLOPs (floating point operations per second). 5 Conclusion We proposed Self-Governing Neural Networks for on-device short text classification. Experiments on multiple dialog act datasets showed that our model outperforms state-of-the-art deep leaning methods (Lee and"
D18-1105,D12-1136,1,0.723046,"cation models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of unique words/features to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and wo"
D18-1105,P13-1036,1,0.777188,"nt over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy. 1 There are multiple strategies to build lightweight text classification models for ondevice. One can create a small dictionary of common input → category mapping on the device and use a naive look-up at inference time. However, such an approach does not scale to complex natural language tasks involving rich vocabularies and wide language variability. Another strategy is to employ fast sampling techniques (Ahmed et al., 2012; Ravi, 2013) or incorporate deep learning models with graph learning like (Bui et al., 2017, 2018), which result in large models but have proven to be extremely powerful for complex language understanding tasks like response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). In this paper, we propose Self-Governing Neural Networks (SGNNs) inspired by projection networks (Ravi, 2017). SGNNs are on-device deep learning models learned via embedding-free projection operations. We employ a modified version of the locality sensitive hashing (LSH) to reduce input dimension from millions of u"
D18-1105,W04-2319,0,0.05681,"in standard neural networks for feasible training. The binary representation is significant since this results in a significantly compact representation for the Computing Projections. We employ an efficient randomized projection method for the projection step. We use locality sensitive hashing (LSH) (Charikar, 2002) to model the underlying projection operations in SGNN. LSH is typically used as a dimensionality reduction technique for clustering (Manning et al., 2008). LSH allows us to project similar inputs ~xi or interme889 • MRDA: ICSI Meeting Recorder Dialog Act Corpus (Adam et al., 2003; Shriberg et al., 2004) is a dialog corpus of multiparty meetings with 5 tags of dialog acts. projection network parameters that in turn considerably reduces the model size. SGNN Parameters. In practice, we employ T different projection functions Pj=1...T , each resulting in d-bit vector that is concatenated to form the projected vector ip in Equation 5. T and d vary depending on the projection network parameter configuration specified for P and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a single projection matrix of size T · d or T separate matrices o"
D18-1105,J00-3003,0,0.0908289,"Missing"
D18-1105,P17-2083,0,0.0138891,"s to a short, fixed-length sequence of bits. This allows us to compute a projection for an incoming text very fast, on-the-fly, with a small memory footprint on the device since we do not need to store the incoming text and word embeddings. We evaluate the performance of our SGNNs on Dialogue Act classification, because (1) it is an important step towards dialog interpretation and conversational analysis aiming to understand the intent of the speaker at every utterance of the conversation and (2) deep learning methods reached state-of-the-art (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Tran et al., 2017; Ortega and Vu, 2017). Introduction Deep neural networks are one of the most successful machine learning methods outperforming many state-of-the-art machine learning methods in natural language processing (Sutskever et al., 2014), speech (Hinton et al., 2012) and visual recognition tasks (Krizhevsky et al., 2012). The availability of high performance computing has enabled research in deep learning to focus largely on the development of deeper and more complex network architectures for improved accuracy. However, the increased complexity of the deep neural networks has become one of the bigges"
D19-1402,D14-1179,0,0.0181301,"Missing"
D19-1402,N18-2118,0,0.283504,"in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent repository combined with the larger vocabulary. • AG: AG News corpus is a collection of news articles on the web, where each document has a title and a description field. We used the dataset from (Zhang et al., 2015). • Y!A: Yahoo! Answers is a text classification task with 10 diverse clas"
D19-1402,C16-1189,0,0.0739429,"thods, we do not apply any vocabulary pruning or pre-processing at all to the input sentences and documents, except for splitting tokens by space. We use a 2-layer ProSeqo neural network with recurrent projections of size T = 60, d = 14 and 256 hidden dimensions to represent state in each bidirectional LSTM-projection layer. We use character 7-grams (with 1-skip) and context size of 1 to model the projector described in Section 2.2. ProSeqo network is trained with SGD and Adam 3898 SWDA 88.3 83.1 80.1 73.8 73.1 - Model ProSeqo (our on-device model) SGNN(Ravi and Kozareva, 2018)(on-device) RNN(Khanpour et al., 2016) RNN+Attention(Ortega and Vu, 2017) CNN(Lee and Dernoncourt, 2016) GatedIntentAtten.(Goo et al., 2018) GatedFullAtten.(Goo et al., 2018) JointBiLSTM(Hakkani-Tur et al., 2016) Atten.RNN(Liu and Lane, 2016) MRDA 90.1 86.7 86.8 84.3 84.6 - ATIS 97.8 88.9 94.1 93.6 92.6 91.1 SNIPS 97.9 93.4 96.8 97.0 96.9 96.7 Table 2: Short Text Classification On-device Results & Comparisons to Prior Work optimizer (Kingma and Ba, 2014) over shuffled mini-batches of size 100. We did not do any additional dataset-specific tuning or processing. 4 STC: Short Text Classification Results This section focuses on the mu"
D19-1402,N15-1147,1,0.900338,"Missing"
D19-1402,W04-2319,0,0.225551,"919,336 Avg. Length 7 8 11 10 38 108 92 Train 193,000 78,000 4,478 13,084 120,000 1,400,000 3,000,000 Test 5,000 15,000 893 700 7,600 60,000 650,000 Table 1: Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al."
D19-1402,N16-1062,0,0.270212,"600 60,000 650,000 Table 1: Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent reposito"
D19-1402,W17-5530,0,0.348994,"Text Classification Tasks and Dataset Characteristics 3.1 Dataset Description • SWDA: Switchboard Dialog Act Corpus is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). The dataset is used in the on-device work of (Ravi and Kozareva, 2018). • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). The dataset was also used by on-device work of (Ravi and Kozareva, 2018). We used the same data split as (Lee and Dernoncourt, 2016; Ortega and Vu, 2017; Ravi and Kozareva, 2018). • ATIS: The Airline Travel Information Systems dataset (T¨ur et al., 2010) is widely used in dialog and speech research. The dataset contains audio recordings of people making flight reservations (T¨ur et al., 2010; Goo et al., 2018). • SNIPS: To test the generalizability of our model, we use another NLU dataset with custom intentengines collected by Snips personal voice assistant. We used the data from (Goo et al., 2018). Compared to the single-domain ATIS dataset, Snips is more complicated mostly because of the rich and diverse intent repository combined with the"
D19-1402,N16-1174,0,0.367912,"he larger vocabulary. • AG: AG News corpus is a collection of news articles on the web, where each document has a title and a description field. We used the dataset from (Zhang et al., 2015). • Y!A: Yahoo! Answers is a text classification task with 10 diverse classes: Society & Culture, Science & Mathematics, Health, Education & Reference, Computers & Internet, Sports, Business & Finance, Entertainment & Music, Family & Relationships and Politics & Government. Each document contains a question title, question context and best answers. We obtained the data from (Zhang et al., 2015). Note that (Yang et al., 2016) used a smaller test sample for evaluation. To present fair results, we will not compare to (Yang et al., 2016). • AMZN: Amazon review dataset is obtained from (Zhang et al., 2015). Resolving this task can be very helpful for product categorization (Kozareva, 2015). The corpora has reviews with ratings ranging from 1 to 5. Following prior work, we use 3,000,000 reviews for training and 650,000 reviews for testing. 3.2 Dataset Characteristics Table 1 shows the datasets characteristics such as the type of the task, number of classes, vocabulary size, average text length, train and test sizes. As"
D19-1402,D18-1092,1,0.753826,"Missing"
D19-1402,P19-1368,1,0.484541,"lop and deploy text and speech models that run inference entirely on-device and return accurate predictions in real-time. To make this work, the on-device models have to be very small in size (few kilobytes or megabytes) to fit on-devices like mobile phone, watch and IoT; to have low latency and be as accurate as server side models. These on-device challenges opened up new active area of research, which recently has shown promising results for speech (Lin et al., 2018), wake word detection (He et al., 2017), dialog act (Ravi and Kozareva, 2018) and intent prediction short text classification (Ravi and Kozareva, 2019). Previous attempts for on-device text classification used hashed &lt;input text, output class> pairs. Unfortunately, such models cannot handle examples that are not part of the lookup table and cannot generalize to more complex tasks like the ones we solve in this paper – long text classification, conversational intent prediction. (Bui et al., 2018) combined graphs with neural networks to improve the robustness of the model, but this resulted in large model sizes that cannot fit on-device. The most successful work is the self-governing neural network (SGNN) from (Ravi and Kozareva, 2018) and (SG"
D19-1402,N19-1339,1,0.547028,"e character-level, and (c) model longer context across words and sentences that is suited for both short and long-text classification tasks. The key differences to prior work (e.g., RNNs, CNNs) are that we use context information to dynamically compute text representations and learn compact neural networks for text classification that are suited for on-device applications. Our work departs significantly from recent on-device work (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) which leverage projection operations to learn efficient networks that can even be transferred to other tasks (Sankar et al., 2019b). However, they are limited to short-text classification tasks. In contrast, our model effectively uses contextual information and combines recurrent and projection operations to achieve efficiency and enable learning more powerful neural networks that generalize well and can solve more complex language classification tasks. As a result, our projection sequence network is able to dynamically project and learn efficient neural classifier models that are competitive with state-of-the-art RNNs and CNNs without the need to store or lookup any pre-trained embeddings. 2.1 The overall architecture"
D19-1506,N15-1011,0,0.0304001,"oves over prior CNN and LSTM models. • Applicability of PRADO for transfer learning, showed its robustness and ability to further improve performance in limited data scenarios. 2 Related Work Early work on text classification relied on sparse lexical features such as n-grams and linear classifiers (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). But with the raise of deep learning, various CNN and LSTM approaches lead to significant improvement in performance and reaching state-of-the-art results. (Kim, 2014) used CNN architecture from computer vision for text classification. (Johnson and Zhang, 2015), used high-dimensional one hot vector and later introduced character-level CNN that achieved even more competitive results. (Tai et al., 2015) used tree structured LSTM for classification, while (Tang et al., 2015) use CNN or LSTM to capture sentence vector followed by bidirectional gated recurrent network which composes the vectors to get a document vector. Recently, (Yang et al., 2016) introduced hierarchical attention neural networks, which captures document representation by incorporating knowledge of the document structure into the model. This approach reaches the best performance on lar"
D19-1506,C16-1189,0,0.110749,"reserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classification tasks such as dialog act and user intent understanding and outperformed prior RNN work (Khanpour et al., 2016; Ortega and Vu, 2017). In this work, we take one step further by proposing a novel projection attention neural network called PRADO . Unlike SGNN which has static projections, PRADO combines trainable projections with attention and convolutions allowing it to capture long range dependencies and making it a powerful and flexible approach for long text classification. We study the impact of different hyperparameters on accuracy vs model size. We also address the problem of producing compact architectures by develop a quantized version of PRADO . In a series of experimental evaluations on multip"
D19-1506,D14-1181,0,0.0193047,"as it has wide applications in spam detection (Jindal and Liu, 2007), product categorization (Kozareva, 2015), sentiment classification (Pang and Lee, 2008) and it also plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 20"
D19-1506,N15-1147,1,0.845654,"Missing"
D19-1506,W17-5530,0,0.0674511,"nable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classification tasks such as dialog act and user intent understanding and outperformed prior RNN work (Khanpour et al., 2016; Ortega and Vu, 2017). In this work, we take one step further by proposing a novel projection attention neural network called PRADO . Unlike SGNN which has static projections, PRADO combines trainable projections with attention and convolutions allowing it to capture long range dependencies and making it a powerful and flexible approach for long text classification. We study the impact of different hyperparameters on accuracy vs model size. We also address the problem of producing compact architectures by develop a quantized version of PRADO . In a series of experimental evaluations on multiple long text classific"
D19-1506,D15-1167,0,0.373154,"tion relied on sparse lexical features such as n-grams and linear classifiers (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). But with the raise of deep learning, various CNN and LSTM approaches lead to significant improvement in performance and reaching state-of-the-art results. (Kim, 2014) used CNN architecture from computer vision for text classification. (Johnson and Zhang, 2015), used high-dimensional one hot vector and later introduced character-level CNN that achieved even more competitive results. (Tai et al., 2015) used tree structured LSTM for classification, while (Tang et al., 2015) use CNN or LSTM to capture sentence vector followed by bidirectional gated recurrent network which composes the vectors to get a document vector. Recently, (Yang et al., 2016) introduced hierarchical attention neural networks, which captures document representation by incorporating knowledge of the document structure into the model. This approach reaches the best performance on large set of text classification tasks. The aforementioned prior work mostly focuses on building the best neural network model independent of any model size or memory constrains. However, recent work by (Ravi and Kozar"
D19-1506,D12-1136,1,0.702567,"ependent of any model size or memory constrains. However, recent work by (Ravi and Kozareva, 2018, 2019) show the importance of building ondevice text classification models that can preserve user privacy, provide consistent user experience and most importantly are compact in size, while yet achieving state-of-art results. Previously, to build lightweight text classification approaches (Ravi, 2013) proposed fast sampling techniques, while (Bui et al., 2018) incorporated deep neural networks with graph learning. While successful, such approaches resulted in large models for response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). To address the challenge of fitting huge deep neural network on-device, (Ravi and Kozareva, 2018) developed a novel self-governing neural networks (SGNNs) that learns projections on the fly leading to small models. SGNN was applied on short text classification tasks such as dialog act and user intent understanding and showed significant improvement over state-of-the-art RNN (Khanpour et al., 2016) and RNN with attention (Ortega and Vu, 2017) approaches. In this work, we take one step further by developing trainable projection network with attention mecha"
D19-1506,N16-1174,0,0.287816,"uct categorization (Kozareva, 2015), sentiment classification (Pang and Lee, 2008) and it also plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on"
D19-1506,P13-1036,1,0.820707,"document structure into the model. This approach reaches the best performance on large set of text classification tasks. The aforementioned prior work mostly focuses on building the best neural network model independent of any model size or memory constrains. However, recent work by (Ravi and Kozareva, 2018, 2019) show the importance of building ondevice text classification models that can preserve user privacy, provide consistent user experience and most importantly are compact in size, while yet achieving state-of-art results. Previously, to build lightweight text classification approaches (Ravi, 2013) proposed fast sampling techniques, while (Bui et al., 2018) incorporated deep neural networks with graph learning. While successful, such approaches resulted in large models for response completion (Pang and Ravi, 2012) and Smart Reply (Kannan et al., 2016). To address the challenge of fitting huge deep neural network on-device, (Ravi and Kozareva, 2018) developed a novel self-governing neural networks (SGNNs) that learns projections on the fly leading to small models. SGNN was applied on short text classification tasks such as dialog act and user intent understanding and showed significant i"
D19-1506,D18-1092,1,0.88315,"Missing"
D19-1506,P19-1368,1,0.847919,"plays an important role for improving document retrieval and ranking (Deerwester et al., 1990). For a long time, the most successful text classification approaches relied on sparse lexical features such as n-grams, which are later used by linear or kernel models (Joachims, 1998; McCallum and Nigam, 1998; Joulin et al., 2016). However, with the recent advancements in deep learning, various neural network architectures like CNN (Kim, 2014), LSTM (Zhang et al., 2015), hierarchical attention mechanisms (Yang et al., 2016) showed improvement in performance. Recently, (Ravi and Kozareva, 2018) and (Ravi and Kozareva, 2019) showed the importance of building on-device neural models for short text classification, which preserve user privacy, enable consistent user experience and most importantly perform inference on the device. One of the biggest challenges is how to fit these large and complex neural networks on devices with limited memory and computation capacity while still maintaining high performance. (Ravi and Kozareva, 2018, 2019) developed ondevice self-governing neural networks (SGNN) and (SGNN++) based on locality-sensitive projections (Ravi, 2017, 2019). Those methods were evaluated on short text classi"
D19-1506,N19-1339,1,0.820034,"lication, in reality it is a look-up of the corresponding row in the embedding matrix as δi is modeled using the Dirac delta function. Embeddings via Trainable Projections: Our approach PRADO replaces this embedding with a projection approach to build the word encoder. Instead of mapping wi to δi , we map it to fi using a projection operator P. Recent work (Ravi, 2017; Ravi and Kozareva, 2018; Ravi, 2019) has shown that projection-based neural approaches can help train compact neural networks that achieve good performance on certain language tasks. These networks learn robust representations (Sankar et al., 2019a) that can be also transferred to other tasks (Sankar et al., 2019b). We follow a similar strategy but unlike the static projections used in these works, we propose a new type of projection that decomposes the operation and makes the projection trainable, leading to more powerful encoders capable of capturing contextual information for long-text classification while maintaining a very low memory footprint. Our method does not rely on a fixed vocabulary. The projection operator we use in this work first fingerprints the words and extracts B bit features from the fingerprint. The word vectors e"
D19-1506,P15-1150,0,0.0950526,"Missing"
I11-1022,D10-1072,1,0.760614,"ermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the dependency is formed when the two heads combine, it is available to be used as a local feature by the semantic role labeler. This property of CCG and its impact on packed-chart SRL is described extensively in Boxwell et al. (2010). This ability to predict dependencies (and semantic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in"
I11-1022,C04-1041,0,0.204923,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,P04-1014,0,0.203309,"tic derivations. We will call this two-part SRL model the C HART model. We compare this model to the more traditional G OLD model, which uses the same features but is generated from gold standard trees. We test the system using both gold-standard parse trees and single-best autoIn order to test the performance of our semantic role labeler, we will need automatically generated parses to run the SRL models over. Even though we are able to train SRL models in the absence of syntactic training data, we still need test parses on which to predict roles. So why not use the fast, accurate CCG parser (Clark and Curran, 2004b) used with previous CCG-based SRL systems? It makes sense to use the highest quality parses available. But recall that the reason for this roundabout way of training the semantic role labeler is to enable us to generate SRL models without syntactic training data. If we use an off-the-shelf syntactic parser that was trained on gold-standard training data, we introduce a source of additional training 194 SAID : LOVE : SAID : LOVE : Robin said John loves Mary np (s[dcl]
p)/s[dcl] np (s[dcl]
p)/np np > s[dcl]
p < s[dcl] > s[dcl]
p < s[dcl] Figure 3: In the first stage of the semantic role la"
I11-1022,E09-1026,0,0.0598503,"Missing"
I11-1022,W03-1008,0,0.195301,"entence if it could combine with a noun phrase to the right and another noun phrase to the left”. The process of automatically assigning CCG categories to words is called “supertagging”, and CCG categories are sometimes informally referred to as “supertags”. An example of how categories combine to make sentences is shown in Figure 1. CCG has many capabilities that go beyond that of a typical context-free grammar. First, it has a sophisticated internal system of managing syntactic heads and dependencies1 . These dependencies are used to great effect in CCG-based semantic role labeling systems (Gildea and Hockenmaier, 2003; Boxwell et al., 2009), as they do not suffer the same data-sparsity effects encountered with treepath features in CFG-based SRL systems. Secondly, CCG permits these dependencies to be passed through intermediary categories in grammatical structures like relative clauses. In Figure 2, the steak is still in the object relation to devoured, even though the verb is inside a relative clause. Finally and most importantly, these dependencies are represented directly on the CCG categories themselves. This is crucial for the prediction of semantic roles inside a packed parse chart – because the depen"
I11-1022,J07-3004,0,0.0683133,"esource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conferen"
I11-1022,N10-1137,0,0.0213533,"like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential. For example, determiners belong to the category NP/N, or “the category of words that become noun 192 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 192–200, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP The phrases when combined with a noun to the right”. The rightmost category indicates the argument that the category is seeking, the leftmost category indicates the res"
I11-1022,C08-1008,1,0.899679,"Missing"
I11-1022,J93-2004,0,0.0366651,"cting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combinatory Categorial Grammar Combinatory Categorial Grammar (Steedman, 2000) is a grammar formalism that describes words in terms of their combinatory potential."
I11-1022,boxwell-white-2008-projecting,1,0.896164,"ntic roles) at parse time figures heavily into the process described here. 3 man devoured the steak np/n n (s
p)/np np/n n np > np s
p > > < s Figure 1: A simple CCG derivation. steak that the man devoured np (np
p)/(s/np) np (s
p)/np >T s/(s
p) s/np np
p np >B > < Figure 2: An example of CCG’s treatment of relative clauses. The syntactic dependency between devoured and steak is the same as it was in figure 1. It is trained using CCGbank and a version of Propbank that has been aligned to the CCGbank in order to account for discrepancies in terminal indexation (Honnibal and Curran, 2007; Boxwell and White, 2008). The system is organized in a twostage pipeline of maximum entropy models3 , following the organization of a previous CFG-style approach (Punyakanok et al., 2008). The first stage is the identification stage, where, for each predicate in the sentence, each word is tagged as either a role or a nonrole (figure 3). The second stage is the classification stage, where the roles are sorted into A RG 0, A RG 1, and so on (figure 4). The identification model and the classification model share the same features, but they are trained and run separately. For the results presented here, we use a version"
I11-1022,J05-1004,0,0.0128898,"ics relationship, and then predicting roles on novel syntactic analyses. The gold standard syntactic training data can be eliminated from the process by extracting training instances from semantic roles projected onto a packed parse chart. This process can be used to rapidly develop NLP tools for resource-poor languages of interest. 1 Introduction Semantic role labeling is the process of generating sets of semantic roles from syntactic analyses. The process of training a semantic role labeler, however, is costly in resources. First, it requires gold-standard semantic role data, like Propbank (Palmer et al., 2005). Secondly, it requires a detailed syntactic annotation of the same resource. We are fortunate to have the reasonablysized Penn Treebank (Marcus et al., 1993) and adaptations for formalisms like Tree Adjoining Grammar (Chen and Shanker, 2004) and Combinatory Categorial Grammar (Hockenmaier and Steedman, 2007) alongside the Propbank data, but for other languages, such resources are unlikely to be available. There has been work in generating semantic role labelers using gold-standard trees in the absence of semantic training data (F¨urstenau and Lapata, 2009; Lang and Lapata, 2010). But 2 Combin"
I11-1022,P10-1051,1,0.895534,"Missing"
I11-1022,J08-2005,0,\N,Missing
I11-1022,P09-1005,1,\N,Missing
J10-3001,J93-2003,0,0.134265,"stical machine translation (SMT). Brown et al. (1993) have provided the most popular word alignment algorithm to date, one that has been implemented in the GIZA (Al-Onaizan et al. 1999) and GIZA++ (Och and Ney 2003) software and adopted by nearly every SMT project. In this article, we investigate whether this algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns alignments that are sub-optimal according to a trained model. 1. Background Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Brown et al. (1993) align an English/French sentence pair by positing a probabilistic model by which an English sentence is translated into French.1 The model provides a set of non-deterministic choices. When a particular sequence of choices is applied to an English input sentence e1 ...el , the result is a particular French output sentence f1 ...fm . In the Brown et al. models, a decision sequence also implies a speciﬁc word alignment vector a1 ...am . We say aj = i when French word fj was produced by English word ei during the translation. Here is a sample sentence pair (e, f) and word alignment a: e: NULL0 Ma"
J10-3001,P01-1030,1,0.849898,"r and GIZA++ makes more errors. Finally, Figure 5 plots the time taken for ILP alignment at different sentence lengths showing a positive correlation as well. 5. Discussion We have determined that GIZA++ makes few search errors, despite the heuristic nature of the algorithm. These search errors do not materially affect overall alignment accuracy. In practice, this means that researchers should not spend time optimizing this particular aspect of SMT systems. Search errors can occur in many areas of SMT. The area that has received the most attention is runtime decoding/translation. For example, Germann et al. (2001) devise an optimal ILP decoder to identify types of search errors made by other decoders. A second area (this article) is ﬁnding Viterbi alignments, given a set of alignment parameter values. A third area is actually learning those parameter values. Brown et al.’s (1993) EM learning algorithm aims to optimize the probability of the French side of the parallel corpus given the English side. For Model 3 and above, Brown et al. collect parameter counts over subsets of alignments, instead of over all alignments. These subsets, like Viterbi alignments, are generated heuristically, and it may be tha"
J10-3001,J03-1002,0,0.0268533,"Missing"
J10-3001,E06-1004,0,0.228718,"φ0 ...φl are only shorthand, as their values are completely determined by the alignment a1 ...am . 296 Ravi and Knight Does GIZA++ Make Search Errors? 3. Finding the Viterbi Alignment We assume that probability tables n, t, d, and p have already been learned from data, using the EM method described by Brown et al. (1993). We are concerned solely with the problem of then ﬁnding the best alignment for a given sentence pair (e, f) as described in Equation 1. Brown et al. were unable to discover a polynomial time algorithm for this problem, which was in fact subsequently shown to be NP-complete (Udupa and Maji 2006). Brown et al. therefore devise a hill-climbing algorithm. This algorithm starts with a reasonably good alignment (Viterbi IBM Model 2, computable in quadratic time), after which it greedily executes small changes to the alignment structure, gradually increasing P(a, f|e). The small changes consist of moves, in which the value of some aj is changed, and swaps, in which a pair aj and ak exchange values. At each step in the greedy search, all possible moves and swaps are considered, and the one which increases P(a, f|e) the most is executed. When P(a, f|e) can no longer be improved, the search h"
N09-1005,P02-1051,1,0.825471,"Missing"
N09-1005,P08-2014,0,0.0227015,"We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must output “Jon Kyl”, not “John K"
N09-1005,D08-1037,0,0.0315634,"We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must output “Jon Kyl”, not “John K"
N09-1005,P07-1094,0,0.0559287,"Missing"
N09-1005,P04-1021,0,0.0389268,"here we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct ta"
N09-1005,P08-1045,1,0.864813,"thern California Information Sciences Institute Marina del Rey, California 90292 {sravi,knight}@isi.edu Abstract We explore the third dimension, where we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtra"
N09-1005,N04-1036,0,0.0272817,"e third dimension, where we see several techniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is oft"
N09-1005,P07-1082,0,0.021348,"transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transport"
N09-1005,W99-0906,1,0.87206,"w EM to run until the P(j) likelihood ratio between subsequent training iterations reaches 0.9999, and we terminate early if 200 iterations are reached. Finally, we decode our test set of U.S. Senator names. Following Knight et al (2006), we stretch out the P(j|e) model probabilities after decipherment training and prior to decoding our test set, by cubing their values. Decipherment under the conditions of transliteration is substantially more difficult than solving letter-substitution ciphers (Knight et al., 2006; Ravi and Knight, 2008; Ravi and Knight, 2009) or phoneme-substitution ciphers (Knight and Yamada, 1999). This is because the target table contains significant non-determinism, and because each symbol has multiple possible fertilities, which introduces uncertainty about the length of the target string. 4.1 Baseline P(e) Model Clearly, we can design P(e) in a number of ways. We might expect that the more the system knows about English, the better it will be able to decipher the Japanese. Our baseline P(e) is a 2-gram phoneme model trained on phoneme sequences from the CMU dictionary. The second row (2a) in Figure 4 shows results when we decipher with this fixed P(e). This approach performs poorly"
N09-1005,P06-2065,1,0.901678,"apply it to the name transliteration task—decoding 100 U.S. Senator names from Japanese to English using the automata shown in Figure 1. For all experiments, we keep the rest of the models in the cascade (WFSA A, WFST B, and WFST D) unchanged. We evaluate on whole-name error-rate (maximum of 100/100) as well as normalized word edit distance, which gives partial credit for getting the first or last name correct. 4 Acquiring Phoneme Mappings from Non-Parallel Data Our main data consists of 9350 unique Japanese phoneme sequences, which we can consider as a single long sequence j. As suggested by Knight et al (2006), we explain the existence of j as the result of someone initially producing a long English phoneme sequence e, according to P(e), then transforming it into j, according to P(j|e). The probability of our observed data P(j) can be written as: P (j) = X e P (e) · P (j|e) We take P(e) to be some fixed model of monolingual English phoneme production, represented as a weighted finite-state acceptor (WFSA). P(j|e) is implemented as the initial, uniformly-weighted WFST C described in Section 2, with 15320 phonemic connections. We next maximize P(j) by manipulating the substitution table P(j|e), aimin"
N09-1005,P06-1142,0,0.0251455,"itution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English"
N09-1005,P07-1016,0,0.0259859,"rforming machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U"
N09-1005,W01-1413,0,0.0263405,"ong a number of dimensions: • phoneme substitution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequen"
N09-1005,D08-1085,1,0.791913,"h an algorithm for EM training of weighted finite-state transducers. 40 We allow EM to run until the P(j) likelihood ratio between subsequent training iterations reaches 0.9999, and we terminate early if 200 iterations are reached. Finally, we decode our test set of U.S. Senator names. Following Knight et al (2006), we stretch out the P(j|e) model probabilities after decipherment training and prior to decoding our test set, by cubing their values. Decipherment under the conditions of transliteration is substantially more difficult than solving letter-substitution ciphers (Knight et al., 2006; Ravi and Knight, 2008; Ravi and Knight, 2009) or phoneme-substitution ciphers (Knight and Yamada, 1999). This is because the target table contains significant non-determinism, and because each symbol has multiple possible fertilities, which introduces uncertainty about the length of the target string. 4.1 Baseline P(e) Model Clearly, we can design P(e) in a number of ways. We might expect that the more the system knows about English, the better it will be able to decipher the Japanese. Our baseline P(e) is a 2-gram phoneme model trained on phoneme sequences from the CMU dictionary. The second row (2a) in Figure 4"
N09-1005,P07-1109,0,0.0192972,"ut any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must o"
N09-1005,P07-1119,0,0.0361491,"ut any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu (the name of a U.S. Senator transported to Japanese), we must o"
N09-1005,P06-1010,0,0.121479,"Jon Kyl”, not “John Kyre” or any other variation. There are many techniques for transliteration and back-transliteration, and they vary along a number of dimensions: • phoneme substitution vs. character substitution • heuristic vs. generative vs. discriminative models • manual vs. automatic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Ch"
N09-1005,W06-1630,0,0.0870334,"ic knowledge acquisition 37 • Extraction of parallel examples from bilingual corpora, using bootstrap dictionaries e.g., (Sherif and Kondrak, 2007a; Goldwasser and Roth, 2008a). • Extraction of parallel examples from comparable corpora, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English sound sequence Japanese sound sequence WFST - C ( S P EH N S ER EY B R AH HH AE M ) WFST - D ( S U P E N S A"
N09-1005,D07-1106,0,0.0386338,"Missing"
N09-1005,P95-1026,0,0.165751,"a, using bootstrap dictionaries, and temporal and word co-occurrence, e.g., (Sproat et al., 2006; Klementiev and Roth, 2008). • Extraction of parallel examples from web queries, using bootstrap dictionaries, e.g., (Nagata et al., 2001; Oh and Isahara, 2006; Kuo et al., 2006; Wu and Chang, 2007). • Comparing terms from different languages in phonetic space, e.g., (Tao et al., 2006; Goldberg and Elhadad, 2008). In this paper, we investigate methods to acquire transliteration mappings from non-parallel sources. We are inspired by previous work in unsupervised learning for natural language, e.g. (Yarowsky, 1995; Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37–45, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics WFSA - A English word sequence WFST - B ( SPENCER ABRAHAM ) English sound sequence Japanese sound sequence WFST - C ( S P EH N S ER EY B R AH HH AE M ) WFST - D ( S U P E N S A A E E B U R A H A M U ) Japanese katakana sequence (スペンサー・エーブラハム) Figure 1: Model used for back-transliteration of Japanese katakana names and terms into English. The model employs a four-stage cascade of weighted finite-state tran"
N09-1005,P07-1015,0,0.041991,"ent a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for example, given jyon.kairu"
N09-1005,W06-1672,0,0.0193428,"chniques in use: We present a method for performing machine transliteration without any parallel resources. We frame the transliteration task as a decipherment problem and show that it is possible to learn cross-language phoneme mapping tables using only monolingual resources. We compare various methods and evaluate their accuracies on a standard name transliteration task. 1 • Manually-constructed transliteration models, e.g., (Hermjakob et al., 2008). • Models constructed from bilingual dictionaries of terms and names, e.g., (Knight and Graehl, 1998; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Yoon et al., 2007; Li et al., 2007; Karimi et al., 2007; Sherif and Kondrak, 2007b; Goldwasser and Roth, 2008b). Introduction Transliteration refers to the transport of names and terms between languages with different writing systems and phoneme inventories. Recently there has been a large amount of interesting work in this area, and the literature has outgrown being citable in its entirety. Much of this work focuses on backtransliteration, which tries to restore a name or term that has been transported into a foreign language. Here, there is often only one correct target spelling—for exampl"
N09-1005,J98-4003,1,\N,Missing
N10-1068,P09-1088,0,0.0973336,"trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the initial FST cascade. We return to this issu"
N10-1068,P02-1065,0,0.0314681,"multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tre"
N10-1068,D08-1033,0,0.0722013,"Missing"
N10-1068,P05-1045,0,0.0169169,"transformer of strings. Such a cascade is trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the"
N10-1068,D08-1036,0,0.0242846,"ation after the sidetracking transition is selected? Should the path attempt to re-join the old derivation as soon as possible, and if so, how is this efficiently done? Then, how can we compute new derivation scores for all possible sidetracks, so that we can choose a new sample by an appropriate weighted coin flip? Finally, would such a sampler be reversible? In order to satisfy theoretical conditions for Gibbs sampling, if we move from sample A to sample B, we must be able to immediately get back to sample A. We take a different tack here, moving from pointwise sampling to blocked sampling. Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. We again start with a random derivation for each example in the corpus. We then choose a training example and exchange its entire derivation lattice to the end of the corpus. We create a weighted version of this lattice, called the proposal lattice, such that we can approximately sample whole paths by stochastic generation. The probabilities are based on the event counts from the rest of the sample (the cache), and on the base distribution, 451 αP0 (r |q) + c(q, r) α + c(q) (3) where q"
N10-1068,P07-1094,0,0.598979,"revious decisions inside the cache. We use Gibbs sampling to estimate the distribution of tags given words. The key to efficient sampling is to define a sampling operator that makes some small change to the overall corpus derivation. With such an operator, we derive an incremental formula for re-scoring the probability of an entire new derivation based on the probability of the old derivation. Exchangeability makes this efficient— we pretend like the area around the small change occurs at the end of the corpus, so that both old and new derivations share the same cache. Goldwater and Griffiths (2007) choose the re-sampling operator “change the tag of a single word,” and they derive the corresponding incremental scoring formula for unsupervised tagging. For other problems, designers develop different sampling operators and derive different incremental scoring formulas. 3.2 Generic Case In order to develop a generic algorithm, we need to abstract away from these problem-specific design choices. In general, hidden derivations correspond to paths through derivation lattices, so we first and are computed in this way: P(r |q) = Figure 3: Changing a decision in the derivation lattice. All paths"
N10-1068,J08-3004,1,0.847533,"ribed general training algorithms for FST cascades and their implementation, and examined the problem of run selection for both EM and Bayesian training. This work raises several interesting points for future study. First, is there an efficient method for performing pointwise sampling on general FSTs, and would pointwise sampling deliver better empirical results than blocked sampling across a range of tasks? Second, can generic methods similar to the ones described here be developed for cascades of tree transducers? It is straightforward to adapt our methods to train a single tree transducer (Graehl et al., 2008), but as most types of tree transducers are not closed under composition (G´ecseg and Steinby, 1984), the compose/de-compose method cannot be directly applied to train cascades. Third, what is the best way to extend the FST formalism to represent non-parametric Bayesian models? Consider the English re-spacing application. We currently take observed (un-spaced) data and build a giant unigram FSA that models every letter sequence seen in the data of up to 10 letters, both words and non-words. This FSA has 207,253 transitions. We also define P0 for each individual transition, which allows a prefe"
N10-1068,knight-al-onaizan-1998-translation,1,0.78918,"atically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022."
N10-1068,P06-2065,1,0.854535,"data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and"
N10-1068,N03-1018,0,0.0182333,"ining runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tree transducers play t"
N10-1068,J94-2001,0,0.720149,"s are: • We describe a Bayesian inference algorithm that can be used to train any cascade of WFSTs on end-to-end data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform"
N10-1068,H94-1050,0,0.0137433,"a. We also investigate the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF gra"
N10-1068,D08-1085,1,0.85282,"method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with"
N10-1068,N09-1005,1,0.84273,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,P09-1057,1,0.812237,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,J96-3004,0,0.091326,"the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DA"
N10-1068,J98-4003,1,\N,Missing
N16-1017,D14-1124,0,0.0922637,"h emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing"
N16-1017,J10-3004,0,0.0200898,"positive coefficients). The relative importance of these features, which focus on the interaction between teams, suggests that audiences tend to favor debating strategies which emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al.,"
N16-1017,P11-1118,0,0.0314604,"relative importance of these features, which focus on the interaction between teams, suggests that audiences tend to favor debating strategies which emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). I"
N16-1017,P11-1099,0,0.198961,"gical stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. (2016) has examined the effectiveness of arguments in the context of a forum where people invite others to challenge their opinions. We complement this line of work by looking at the relative persuasiveness of participants in extended conversations as they exchange arguments over multiple turns. Previous studies of influence in extended conversations have largely dealt with the political domain, examining moderated but relatively unstructured settings such as talk shows or presidential debates, and suggesting features like topic control (Nguy"
N16-1017,E12-1079,0,0.0183164,"in usage of own talking points from introduction to discussion (positive coefficients). The relative importance of these features, which focus on the interaction between teams, suggests that audiences tend to favor debating strategies which emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and r"
N16-1017,D14-1083,0,0.0682815,"er and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. (2016) has examined the effectiveness of arguments in the context of a forum where people invite others to challenge their opinions. We complement this line of work by looking at the relative persuasiveness of parti"
N16-1017,N16-1070,1,0.821407,"Missing"
N16-1017,P15-1159,1,0.85004,"of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of"
N16-1017,I13-1042,0,0.0705434,"Missing"
N16-1017,N10-1020,0,0.094338,"cients), and the drop in usage of own talking points from introduction to discussion (positive coefficients). The relative importance of these features, which focus on the interaction between teams, suggests that audiences tend to favor debating strategies which emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), o"
N16-1017,W15-4625,0,0.0803761,"ous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. ("
N16-1017,P98-2188,0,0.061077,"ound (negative coefficients), and the drop in usage of own talking points from introduction to discussion (positive coefficients). The relative importance of these features, which focus on the interaction between teams, suggests that audiences tend to favor debating strategies which emphasize the discussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sri"
N16-1017,W10-0214,0,0.113827,". 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 20"
N16-1017,P15-1012,0,0.0582938,"998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. (2016) has examined the effectiveness of arguments in the context of a forum where people invit"
N16-1017,D14-1006,0,0.0715093,"daran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. (2016) has examined the effectiveness of arguments in the context of a forum where people invite others to challenge their opinions. We complement this line of work by looking at the relative persuasiveness of participants in extended conversations as they exchange arguments over multiple turns. Previous studies of influence in extended conversations have largely dealt with the political domain, examining moderated but relatively unstructured settings such as talk shows or presidential debates, and suggesting features like topic control (Nguyen et al., 2014), linguist"
N16-1017,W06-1639,0,0.21374,"cts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentation strategies has largely focused on exploiting the structure of monologic arguments (Mochales and Moens, 2011), like those of persuasive essays (Feng and Hirst, 2011; Stab and Gurevych, 2014). In addition, Tan et al. (2016) has examined the effectiveness of arguments in the context of a f"
N16-1017,P14-2113,0,0.0725641,"ussion. 13 We optimize the regularizer (`1 or `2 ), and the value of the regularization parameter C (between 10−5 and 105 ). For Flow* we also optimize the number of features selected. 5 Further Related Work Previous work on conversational structure has proposed approaches to model dialogue acts (Samuel et al., 1998; Ritter et al., 2010; Ferschke et al., 2012) or disentangle interleaved conversations (Elsner and Charniak, 2010; Elsner and Charniak, 2011). Other research has considered the problem of detecting conversation-level traits such as the presence of disagreements (Allen et al., 2014; Wang and Cardie, 2014) or the likelihood of relation dissolution (Niculae et al., 2015). At the participant level, several studies present approaches to identify ideological stances (Somasundaran and Wiebe, 2010; Rosenthal and McKeown, 2015), using features based on participant interactions (Thomas et al., 2006; Sridhar et al., 2015), or extracting words and reasons characterizing a stance (Monroe et al., 2008; Nguyen et al., 2010; Hasan and Ng, 2014). In our setting, both the stances and the turn structure of a debate are known, allowing us to instead focus on the debate’s outcome. Existing research on argumentati"
N19-1339,N09-1003,0,0.0691329,"ification tasks. Similarity tests the ability to capture words, while language modeling and classification warrant the ability to transfer the neural projections. 4.2.1 Similarity Task We evaluate our NP-SG word representations on 4 different widely used benchmark datasets for measuring similarities. Dataset: MTurk-287 (Radinsky et al., 2011) has 287 pairs of words and was constructed by crowdsourcing the human similarity ratings using Amazon Mechanical Turk. WS353 (Finkelstein et al., 2001) has 353 pairs of similar English words rated by humans and is further split into WS353-SIM. WS353-REL (Agirre et al., 2009) captures different types of similarities and relatedness. RWSTANFORD (Luong et al., 2013) has 2034 rare word pairs sampled from different frequency bins. Evaluation: For all the datasets, we compute the Spearmans rank correlation coefficient between the rankings computed by skip-gram models (baseline SG and NP-SG) and the human rankings. We use cosine similarity metric to measure word similarity. Results: Table 1 shows that NP-SG, with significantly smaller number of parameters comes close to the skip-gram model (SG) and even outperforms it with 2.5x-10x compression. NP-SG gets better represe"
N19-1339,W13-3512,0,0.135344,"Missing"
N19-1339,D14-1162,0,0.0823304,"ferred to other NLP tasks. For qualitative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work. 1 Introduction Pre-trained word representations are at the core of many neural language understanding models. Among the most popular and widely used word embeddings are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018). The biggest challenge with word embedding is that they require lookup and a large memory footprint, as we have to store one entry (d-dim vector) per word and it blows up. In parallel, the tremendous success of deep learning models and the explosion of mobile, IoT de∗ Work done during internship at Google. vices coupled together with the growing user privacy concerns have led to the need for deploying deep learning models on-device for inference. This has led to new research in compressing large and complex deep learning models for low power ondevice deployment."
N19-1339,N18-1202,0,0.0489516,"itative evaluation, we analyze the nearest neighbors of the word representations and discover semantically similar words even with misspellings. For quantitative evaluation, we plug our transferable projections into a simple LSTM and run it on multiple NLP tasks and show how our transferable projections achieve better performance compared to prior work. 1 Introduction Pre-trained word representations are at the core of many neural language understanding models. Among the most popular and widely used word embeddings are word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018). The biggest challenge with word embedding is that they require lookup and a large memory footprint, as we have to store one entry (d-dim vector) per word and it blows up. In parallel, the tremendous success of deep learning models and the explosion of mobile, IoT de∗ Work done during internship at Google. vices coupled together with the growing user privacy concerns have led to the need for deploying deep learning models on-device for inference. This has led to new research in compressing large and complex deep learning models for low power ondevice deployment. Recently, (Ravi and Kozareva,"
N19-1339,D18-1092,1,0.377902,"Missing"
P09-1057,C04-1080,0,0.533736,"Missing"
P09-1057,W02-0603,0,0.0430739,"Missing"
P09-1057,P08-1085,0,0.518174,"nknown Words The literature also includes results reported in a different setting for the tagging problem. In some scenarios, a complete dictionary with entries for all word types may not be readily available to us and instead, we might be provided with an incomplete dictionary that contains entries for only frequent word types. In such cases, any word not appearing in the dictionary will be treated as an unknown word, and can be labeled with any of the tags from given tagset (i.e., for every unknown word, there are 17 tag possibilities). Some previous approaches (Toutanova and Johnson, 2008; Goldberg et al., 2008) handle unknown words explicitly using ambiguity class components conditioned on various morphological features, and this has shown to produce good tagging results, especially when dealing with incomplete dictionaries. We follow a simple approach using just one of the features used in (Toutanova and Johnson, 2008) for assigning tag possibilities to every unknown word. We first identify the top-100 suffixes (up to 3 characters) for words in the dictionary. Using the word/tag pairs from the dictionary, we train a simple probabilistic model that predicts the 7 Discussion The method proposed in th"
P09-1057,J01-2001,0,0.0542802,"Missing"
P09-1057,P07-1094,0,0.736423,"Missing"
P09-1057,D07-1031,0,0.51444,"in the EM tagging output. We also look at things more globally. We investigate the Viterbi tag sequence generated by EM training and count how many distinct tag bigrams there are in that sequence. We call this the observed grammar size, and it is 915. That is, in tagging the 24,115 test tokens, EM uses 915 of the available 45 × 45 = 2025 tag bigrams.2 The advantage of the observed grammar size is that we What goes wrong with EM? We analyze the tag sequence output produced by EM and try to see where EM goes wrong. The overall POS tag distribution learnt by EM is relatively uniform, as noted by Johnson (2007), and it tends to assign equal number of tokens to each 2 We contrast observed size with the model size for the grammar, which we define as the number of P(t2 |t1 ) entries in EM’s trained tag model that exceed 0.0001 probability. 505 training text IP formulation they START g1 PRO-AUX g2 PRO-V g3 AUX-N g4 AUX-V g5 V-N g6 V-V g7 N-PUNC g8 V-PUNC g9 PUNC-PRO g10 PRO-N fish . fish L1 L10 L2 AUX L3 L9 L4 dictionary variables I L0 PRO d1 PRO-they d2 AUX-can d3 V-can d4 N-fish d5 V-fish d6 PUNC-. d7 PRO-I can V L5 L7 L11 link variables L6 N L8 PUNC Integer Program Minimize: ∑i=1…10 gi Constraints: g"
P09-1057,J94-2001,0,0.86645,"ctionary entries are relevant to the 5,878 word types in the test set. Per-token ambiguity is about 1.5 tags/token, yielding approximately 106425 possible ways to tag the data. There are 45 distinct grammatical tags. In this set-up, there are no unknown words. Figure 1 shows prior results for this problem. While the methods are quite different, they all make use of two common model elements. One is a probabilistic n-gram tag model P(ti |ti−n+1 ...ti−1 ), which we call the grammar. The other is a probabilistic word-given-tag model P(wi |ti ), which we call the dictionary. The classic approach (Merialdo, 1994) is expectation-maximization (EM), where we estimate grammar and dictionary probabilities in order to maximize the probability of the observed word sequence: We describe a novel method for the task of unsupervised POS tagging with a dictionary, one that uses integer programming to explicitly search for the smallest model that explains the data, and then uses EM to set parameter values. We evaluate our method on a standard test corpus using different standard tagsets (a 45-tagset as well as a smaller 17-tagset), and show that our approach performs better than existing state-of-the-art systems i"
P09-1057,P05-1044,0,0.72365,"Missing"
P10-1051,W02-0603,0,0.0336295,"smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains almost 3.5 million word/ta"
P10-1051,D09-1123,0,0.0345392,"Missing"
P10-1051,P08-1085,0,0.0980955,"ctical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July"
P10-1051,J01-2001,0,0.0134756,"tion 1) finds the smallest grammar Gmin1 ⊂ G that explains the set of word bigram types observed in the data rather than the word sequence itself, and the second (Minimization 2) finds the smallest augmentation of Gmin1 that explains the full word sequence. Minimized models for supertagging The idea of searching for minimized models is related to classic Minimum Description Length (MDL) (Barron et al., 1998), which seeks to select a small model that captures the most regularity in the observed data. This modeling strategy has been shown to produce good results for many natural language tasks (Goldsmith, 2001; Creutz and Lagus, 2002; Ravi and Knight, 2009). For tagging, the idea has been implemented using Bayesian models with priors that indirectly induce sparsity in the learned models (Goldwater and Griffiths, 2007); however, Ravi and Knight (2009) show a better approach is to directly minimize the model using an integer programming (IP) formulation. Here, we build on this idea for supertagging. There are many challenges involved in using IP minimization for supertagging. The 1241 distinct supertags in the tagset result in 1.5 million tag bigram entries in the model and the dictionary contains al"
P10-1051,P07-1094,0,0.338937,"ictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Lingu"
P10-1051,J07-3004,0,0.125826,"e integer programming formulation that identifies minimal grammars efficiently and effectively. 2 NPAPER+CIVIL NPAPER CIVIL Max 126 Type ambig 1.69 Tok ambig 18.71 849 644 486 64 48 39 1.48 1.42 1.52 11.76 12.17 11.33 Table 1: Statistics for the training data used to extract lexicons for CCGbank and CCG-TUT. Distinct: # of distinct lexical categories; Max: # of categories for the most ambiguous word; Type ambig: per word type category ambiguity; Tok ambig: per word token category ambiguity. Data CCGbank. CCGbank was created by semiautomatically converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions fo"
P10-1051,J93-2004,0,0.0402447,"ng accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the"
P10-1051,C08-1008,1,0.801717,"lly converting the Penn Treebank to CCG derivations (Hockenmaier and Steedman, 2007). We use the standard splits of the data used in semi-supervised tagging experiments (e.g. Banko and Moore (2004)): sections 0-18 for training, 19-21 for development, and 22-24 for test. trast, supertags are detailed, structured labels; a universal set of grammatical rules defines how categories may combine with one another to project syntactic structure.2 Because of this, properties of the CCG formalism itself can be used to constrain learning—prior to considering any particular language, grammar or data set. Baldridge (2008) uses this observation to create grammar-informed tag transitions for a bitag HMM supertagger based on two main properties. First, categories differ in their complexity and less complex categories tend to be used more frequently. For example, two categories for buy in CCGbank are (S[dcl]NP)/NP and ((((S[b]NP)/PP)/PP)/(S[adj]NP))/NP; the former occurs 33 times, the latter once. Second, categories indicate the form of categories found adjacent to them; for example, the category for sentential complement verbs ((SNP)/S) expects an NP to its left and an S to its right. Categories combine via r"
P10-1051,J94-2001,0,0.4331,"basic expectation-maximization training with a bitag Hidden Markov Model, which we show on the CCGbank and CCG-TUT corpora. The strategies provide further error reductions when combined. We describe a new two-stage integer programming strategy that efficiently deals with the high degree of ambiguity on these datasets while obtaining the full effect of model minimization. 1 Kevin Knight1 Introduction Creating accurate part-of-speech (POS) taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the sta"
P10-1051,C04-1080,0,0.134526,") taggers using a tag dictionary and unlabeled data is an interesting task with practical applications. It has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Asso"
P10-1051,P09-1057,1,0.867693,"has been explored at length in the literature since Merialdo (1994), though the task setting as usually defined in such experiments is somewhat artificial since the tag dictionaries are derived from tagged corpora. Nonetheless, the methods proposed apply to realistic scenarios in which one has an electronic part-of-speech tag dictionary or a hand-crafted grammar with limited coverage. Most work has focused on POS-tagging for English using the Penn Treebank (Marcus et al., 1993), such as (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Goldberg et al., 2008; Ravi and Knight, 2009). This generally involves working with the standard set of 45 POS-tags employed in the Penn Treebank. The most ambiguous word has 7 different POS tags associated with it. Most methods have employed some variant of Expectation Maximization (EM) to learn parameters for a bigram 1 See Banko and Moore (2004) for a description of how many early POS-tagging papers in fact used a number of heuristic cutoffs that greatly simplify the problem. 495 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495–503, c Uppsala, Sweden, 11-16 July 2010. 2010 Association"
P10-1051,N06-1019,0,0.249753,"Missing"
P10-1051,J07-4004,0,0.0952989,"Missing"
P11-1002,P09-1088,0,0.0105762,"ntext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are"
P11-1002,J93-2003,0,0.110375,"@isi.edu Abstract In this work, we tackle the task of machine translation (MT) without parallel training data. We frame the MT problem as a decipherment task, treating the foreign text as a cipher for English and present novel methods for training translation models from nonparallel text. 1 Introduction Bilingual corpora are a staple of statistical machine translation (SMT) research. From these corpora, we estimate translation model parameters: wordto-word translation tables, fertilities, distortion parameters, phrase tables, syntactic transformations, etc. Starting with the classic IBM work (Brown et al., 1993), training has been viewed as a maximization problem involving hidden word alignments (a) that are assumed to underlie observed sentence pairs (e, f ): learned model to translate new foreign strings. As successful work develops along this line, we expect more domains and language pairs to be conquered by SMT. How can we learn a translation model from nonparallel data? Intuitively, we try to construct translation model tables which, when applied to observed foreign text, consistently yield sensible English. This is essentially the same approach taken by cryptanalysts and epigraphers when they d"
P11-1002,N10-1068,1,0.884511,"re cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are no memory bottlenecks"
P11-1002,P05-1045,0,0.0019567,"ociated with the plaintext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gr"
P11-1002,W97-0119,0,0.0183403,"s decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (p"
P11-1002,P01-1030,1,0.22055,"Missing"
P11-1002,P07-1094,0,0.00959101,"all other parameters e → cj associated with the plaintext word type e (where cj 6= c). This yields a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we"
P11-1002,P08-1088,0,0.46365,"tion/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (plaintext) sequence is substituted by a cipher to"
P11-1002,knight-al-onaizan-1998-translation,1,0.158118,"1128 unique), 62k word tokens, 411 word types. TEST (Spanish): 13181 sentences (1127 unique), 39k word tokens, 562 word types. Both Spanish/English sides of TRAIN are used for parallel MT training, whereas decipherment uses only monolingual English data for training LMs. MT Systems: We build and compare different MT systems under two training scenarios: 1. Parallel training using: (a) MOSES, a phrase translation system (Koehn et al., 2007) widely used in MT literature, and (b) a simpler version of IBM Model 3 (without distortion parameters) which can be trained tractably using the strategy of Knight and Al-Onaizan (1998). 2. Decipherment without parallel data using: (a) EM method (from Section 3.1), and (b) Bayesian method (from Section 3.2). Evaluation: All the MT systems are run on the Spanish test data and the quality of the resulting English translations are evaluated using two different measures—(1) Normalized edit distance score (Navarro, 2001),6 and (2) BLEU (Papineni et 6 When computing edit distance, we account for substitutions, insertions, deletions as well as local-swap edit operations required to convert a given English string into the (gold) reference translation. 19 al., 2002), a standard MT ev"
P11-1002,P06-2065,1,0.770651,"ages 12–21, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics same full translation model. We note that for each f , not only is the alignment a still hidden, but now the English translation e is hidden as well. A language model P (e) is typically used in SMT decoding (Koehn, 2009), but here P (e) actually plays a central role in training translation model parameters. To distinguish the two, we refer to (5) as decipherment, rather than decoding. We can now draw on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training"
P11-1002,P09-5002,1,0.730568,"nts, we get: arg max θ YX f e P (e) · X Pθ (f, a|e) (5) a Note that this formula has the same free Pθ (f, a|e) parameters as expression (2). We seek to manipulate these parameters in order to learn the 12 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 12–21, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics same full translation model. We note that for each f , not only is the alignment a still hidden, but now the English translation e is hidden as well. A language model P (e) is typically used in SMT decoding (Koehn, 2009), but here P (e) actually plays a central role in training translation model parameters. To distinguish the two, we refer to (5) as decipherment, rather than decoding. We can now draw on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text,"
P11-1002,N06-1020,0,0.0293911,"We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in the natural language (plaintext) sequence is substituted by a cipher token, according to a substitution key. The key is deterministic"
P11-1002,P02-1040,0,0.0843365,"Missing"
P11-1002,P95-1050,0,0.62072,"w on previous decipherment work for solving simpler substitution/transposition ciphers (Bauer, 2006; Knight et al., 2006). We must keep in mind, however, that foreign language is a much more demanding code, involving highly nondeterministic mappings and very large substitution tables. The contributions of this paper are therefore: • We give first results for training a full translation model from non-parallel text, and we apply the model to translate previously-unseen text. This work is thus distinguished from prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). It also contrasts with self-training (McClosky et al., 2006), which requires a parallel seed and often does not engage in iterative maximization. • We develop novel methods to deal with largescale vocabularies inherent in MT problems. 2 Word Substitution Decipherment Before we tackle machine translation without parallel data, we first solve a simpler problem—word substitution decipherment. Here, we do not have to worry about hidden alignments since there is only one alignment. In a word substitution cipher, every word in"
P11-1002,P10-1107,1,0.522377,"lds a much smaller channel with size &lt; (2K + 1)2 . Retrain the new channel using EM algorithm. 4. Goto Step 2 and repeat the procedure, extending the channel size iteratively in each stage. Finally, we decode the given ciphertext c by using the Viterbi algorithm to choose the plaintext decoding e that maximizes P (e) · Pθtrained (c|e)3 , stretching the channel probabilities (Knight et al., 2006). 2.2 Bayesian Decipherment Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010; Snyder et al., 2010). These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. Here, we propose a novel decipherment approach using Bayesian learning. Our method holds several other advantages over the EM approach—(1) inference using smart sampling strategies permits efficient training, allowing us to scale to large data/vocabulary sizes, (2) incremental scoring of derivations during sampling allows efficient inference even when we use higher-order n-gram LMs, (3) there are no memory bottlenecks since the full channe"
P11-1002,1983.tc-1.13,0,0.830965,"Missing"
P11-1002,P07-2045,0,\N,Missing
P11-1025,P09-1088,0,0.0324409,"each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easil"
P11-1025,N10-1068,1,0.811065,"pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easily specify priors that"
P11-1025,P10-1106,0,0.543437,"Missing"
P11-1025,P05-1045,0,0.0135964,"P (p). 2. Substitute each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods lik"
P11-1025,P07-1094,0,0.0864767,"p = p1 ...pn , with probability P (p). 2. Substitute each plaintext letter pi with a ciphertext token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using con"
P11-1025,P06-2065,1,0.826387,"Missing"
P11-1025,D08-1085,1,0.676072,"Missing"
P11-1025,P10-1107,1,0.73913,"token ci , with probability P (ci |pi ) in order to generate the ciphertext sequence c = c1 ...cn . We build a statistical English language model (LM) for the plaintext source model P (p), which assigns a probability to any English letter sequence. Our goal is to estimate the channel model parameters θ in order to maximize the probability of the observed ciphertext c: X arg max P (c) = arg max Pθ (p, c) (1) θ θ = arg max Bayesian inference methods have become popular in natural language processing (Goldwater and Griffiths, 2007; Finkel et al., 2005; Blunsom et al., 2009; Chiang et al., 2010). Snyder et al. (2010) proposed a Bayesian approach in an archaeological decipherment scenario. These methods are attractive for their ability to manage uncertainty about model parameters and allow one to incorporate prior knowledge during inference. A common phenomenon observed while modeling natural language problems is sparsity. For simple letter substitution ciphers, the original substitution key exhibits a 1-to-1 correspondence between the plaintext letters and cipher types. It is not easy to model such information using conventional methods like EM. But we can easily specify priors that favor sparse distribut"
P11-1025,N10-1082,0,\N,Missing
P13-1036,D12-1025,0,0.605312,"stimate translation model parameters involving word-to-word translation tables, fertilities, distortion, phrase translations, syntactic transformations, etc. But obtaining parallel data is an expensive process and not available for all language 362 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 362–371, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Translation Model: Machine translation is a much more complex task than solving other decipherment tasks such as word substitution ciphers (Ravi and Knight, 2011b; Dou and Knight, 2012). The mappings between languages involve non-determinism (i.e., words can have multiple translations), re-ordering of words can occur as grammar and syntax varies with language, and in addition word insertion and deletion operations are also involved. Ideally, for the translation model P (f |e) we would like to use well-known statistical models such as IBM Model 3 and estimate its parameters θ using the EM algorithm (Dempster et al., 1977). But training becomes intractable with complex translation models and scalability is also an issue when large corpora sizes are involved and the translation"
P13-1036,W97-0119,0,0.0220711,"ic has been receiving increasing attention from researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language. The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data. The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; R"
P13-1036,P07-1094,0,0.0534422,"ingual corpora to induce phrase tables, etc. These when combined with standard MT systems such as Moses (Koehn et al., 2007) trained on parallel corpora, have been shown to yield some BLEU score improvements. Nuhn et al. (2012) show some sample English/French lexicon entries learnt using EM algorithm with a pruned translation candidate set on a portion of the Gigaword corpus11 but do not report any actual MT results. In addition, as we showed earlier our method can use Bayesian inference (which has a lot of nice properties compared to EM for unsupervised natural language tasks (Johnson, 2007; Goldwater and Griffiths, 2007)) and still scale easily to large vocabulary, data sizes while allowing the models to grow in complexity. Most importantly, our method produces better translation results (as demonstrated on the OPUS MT task). And to our knowledge, this is the first time that anyone has reported MT results under truly non-parallel settings on such a large-scale task (EMEA). Our method is also easily extensible to outof-domain translation scenarios similar to (Dou and Knight, 2012). While their work also uses Bayesian inference with a slice sampling scheme, our new approach uses a novel hash sampling scheme for"
P13-1036,P08-1088,0,0.702204,"researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language. The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data. The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to deco"
P13-1036,D07-1031,0,0.0444532,"that uses monolingual corpora to induce phrase tables, etc. These when combined with standard MT systems such as Moses (Koehn et al., 2007) trained on parallel corpora, have been shown to yield some BLEU score improvements. Nuhn et al. (2012) show some sample English/French lexicon entries learnt using EM algorithm with a pruned translation candidate set on a portion of the Gigaword corpus11 but do not report any actual MT results. In addition, as we showed earlier our method can use Bayesian inference (which has a lot of nice properties compared to EM for unsupervised natural language tasks (Johnson, 2007; Goldwater and Griffiths, 2007)) and still scale easily to large vocabulary, data sizes while allowing the models to grow in complexity. Most importantly, our method produces better translation results (as demonstrated on the OPUS MT task). And to our knowledge, this is the first time that anyone has reported MT results under truly non-parallel settings on such a large-scale task (EMEA). Our method is also easily extensible to outof-domain translation scenarios similar to (Dou and Knight, 2012). While their work also uses Bayesian inference with a slice sampling scheme, our new approach uses"
P13-1036,P10-1107,0,0.0360362,"Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers. The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual corIn this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides si"
P13-1036,E12-1014,0,0.317941,"source and target words in a vector space similar to how documents are represented in typical information retrieval settings. But unlike documents, here each word w is associated with a feature vector w1 ...wd (where wi represents the weight for the feature indexed by i) which is constructed from monolingual corpora. For instance, context features for word w may include other words (or phrases) that appear in the immediate context (n-gram window) surrounding w in the monolingual corpus. Similarly, we can add other features based on topic models, orthography (Haghighi et al., 2008), temporal (Klementiev et al., 2012), etc. to our representation all of which can be extracted from monolingual corpora. Next, given two high dimensional vectors u and v it is possible to calculate the similarity between the two words denoted by s(u, v). The feature construction process is described in more detail below: Target Language: We represent each word (or phrase) ei with the following contextual features along with their counts: (a) f−context : every (word n-gram, position) pair immediately preceding ei in the monolingual corpus (n=1, position=−1), (b) similar features f+context to model the context following ei , and ("
P13-1036,W99-0906,0,0.177377,"lel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers. The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual corIn this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of"
P13-1036,P12-1017,0,0.0896903,"ge, and in addition word insertion and deletion operations are also involved. Ideally, for the translation model P (f |e) we would like to use well-known statistical models such as IBM Model 3 and estimate its parameters θ using the EM algorithm (Dempster et al., 1977). But training becomes intractable with complex translation models and scalability is also an issue when large corpora sizes are involved and the translation tables become huge to fit in memory. So, instead we use a simplified generative process for the translation model as proposed by Ravi and Knight (2011b) and used by others (Nuhn et al., 2012) for this task: pora. Their best performing method uses an EM algorithm to train a word translation model and they show results on a Spanish/English task. Nuhn et al. (2012) extend the former approach and improve training efficiency by pruning translation candidates prior to EM training with the help of context similarities computed from monolingual corpora. In this work we propose a new Bayesian inference method for estimating translation models from scratch using only monolingual corpora. Secondly, we introduce a new feature-based representation for sampling translation candidates that allow"
P13-1036,P02-1040,0,0.0916967,"elds much better results than the Bayesian inference method presented in (Ravi and Knight, 2011b). This is due to the accelerated sampling scheme introduced earlier which helps it converge to better solutions faster. Table 2 (last column) also compares the efficiency of different methods in terms of CPU time required for training. Both our 2-gram and 3-gram based methods are significantly faster than those previously reported for EM based training methods presented in (Ravi and Knight, 2011b; Nuhn We test our method on two different corpora. To evaluate translation quality, we use BLEU score (Papineni et al., 2002), a standard evaluation measure used in machine translation. First, we present MT results on non-parallel Spanish/English data from the OPUS corpus (Tiedemann, 2009) which was used by Ravi and Knight (2011b) and Nuhn et al. (2012). We show that our method achieves the best performance (BLEU scores) on this task while being significantly faster than both the previous approaches. We then apply our method to a much larger non-parallel French/Spanish corpus constructed from the EMEA corpus (Tiedemann, 2009). Here the vocabulary sizes are much larger and we show how our new Bayesian decipherment me"
P13-1036,P95-1050,0,0.799511,"ly, this topic has been receiving increasing attention from researchers and new methods have been proposed to train statistical machine translation models using only monolingual data in the source and target language. The underlying motivation behind most of these methods is that statistical properties for linguistic elements are shared across different languages and some of these similarities (mappings) could be automatically identified from large amounts of monolingual data. The MT literature does cover some prior work on extracting or augmenting partial lexicons using non-parallel corpora (Rapp, 1995; Fung and McKeown, 1997; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999"
P13-1036,P11-1025,1,0.633714,"7; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers. The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual corIn this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as s"
P13-1036,P11-1002,1,0.734203,"7; Koehn and Knight, 2000; Haghighi et al., 2008). However, none of these methods attempt to train end-to-end MT models, instead they focus on mining bilingual lexicons from monolingual corpora and often they require parallel seed lexicons as a starting point. Some of them (Haghighi et al., 2008) also rely on additional linguistic knowledge such as orthography, etc. to mine word translation pairs across related languages (e.g., Spanish/English). Unsupervised training methods have also been proposed in the past for related problems in decipherment (Knight and Yamada, 1999; Snyder et al., 2010; Ravi and Knight, 2011a) where the goal is to decode unknown scripts or ciphers. The body of work that is more closely related to ours include that of Ravi and Knight (2011b) who introduced a decipherment approach for training translation models using only monolingual corIn this paper, we propose a new Bayesian inference method to train statistical machine translation systems using only nonparallel corpora. Following a probabilistic decipherment approach, we first introduce a new framework for decipherment training that is flexible enough to incorporate any number/type of features (besides simple bag-of-words) as s"
P13-1036,P05-1077,0,0.0264583,"context of a more generic and widely known family of distributions—mixtures of exponential families. Then we derive a novel proposal distribution for sampling translation candidates and introduce a new sampler for decipherment training that Bayesian MT Decipherment via Hash Sampling The next step is to use the feature representations described earlier and iteratively sample a target word (or phrase) translation candidate ei for every 365 is based on locality sensitive hashing (LSH). Hashing methods such as LSH have been widely used in the past in several scenarios including NLP applications (Ravichandran et al., 2005). Most of these approaches employ LSH within heuristic methods for speeding up nearestneighbor look up and similarity computation techniques. However, we use LSH hashing within a probabilistic framework which is very different from the typical use of LSH. Our work is inspired by some recent work by Ahmed et al. (2012) on speeding up Bayesian inference for unsupervised clustering. We use a similar technique as theirs but a different approximate distribution for the proposal, one that is bettersuited for machine translation models and without some of the additional overhead required for computin"
P13-1036,P07-2045,0,\N,Missing
P13-1100,P06-1039,0,0.0237056,"Missing"
P13-1100,de-marneffe-etal-2006-generating,0,0.0253105,"Missing"
P13-1100,W04-1017,0,0.0215606,"dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective"
P13-1100,C10-1039,0,0.0342316,"well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage function and a non-submodular dispersion function. We then describe a simple greedy algorithm for optimizi"
P13-1100,C10-1101,0,0.0204477,"also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several summarization scenarios. This objective function is the sum of a monotone submodular coverage functio"
P13-1100,W03-0509,0,0.0307907,"ed to select a subset of comments (at most 20 comments) that best represented a summary capturing the most popular as well as diverse set of views and opinions expressed by different users that are relevant to the given news article. We then compare the automatically generated comment summaries against the human-generated summaries and compute the ROUGE-1 and ROUGE-2 scores.6 This summarization task is particularly hard for even human annotators since user-generated comments are typically noisy and there are several hundreds of comments per article. Similar to existing work in the literature (Sekine and Nobata, 2003), we computed inter-annotator agreement for the humans by comparing their summaries against each other on a small held-out set of articles. The average ROUGE-1 F-scores observed for humans was much higher (59.7) than that of automatic systems measured against the human-generated summaries (our best system achieved a score of 28.9 ROUGE-1 on the same dataset). This shows that even though this is a new type of summarization task, humans tend to generate more consistent summaries and hence their annotations are reliable for evaluation purposes as in multi-document summarization. 5.3 Results Multi"
P13-1100,N10-1100,0,0.00977809,"rm their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et al., 2011; Nenkova and McKeown, 2012). 3 Framework In this section we present the summarization framework. We start by describing a generic objective function that can be widely applied to several su"
P13-1100,C10-1111,0,0.0201768,"we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related topics, see (Kim et"
P13-1100,E09-1089,0,0.0458228,"news articles. On DUC 2004, we obtain performance that matches (Lin and Bilmes, 2011), without any serious parameter tuning; note that their framework does not have the dispersion function. On the comment corpus, we outperform their method, demonstrating that value of dispersion functions. As part of our methodology, we also use a new structured representation for summaries. 2 Related Work Automatic summarization is a well-studied problem in the literature. Several methods have been proposed for single- and multi-document summarization (Carbonell and Goldstein, 1998; Conroy and O’Leary, 2001; Takamura and Okumura, 2009; Shen and Li, 2010). Related concepts have also been used in several other scenarios such as query-focused summarization in information retrieval (Daum´e and Marcu, 2006), microblog summarization (Sharifi et al., 2010), event summarization (Filatova, 2004), and others (Riedhammer et al., 2010; Qazvinian et al., 2010; Yatani et al., 2011). Graph-based methods have been used for summarization (Ganesan et al., 2010), but in a different context—using paths in graphs to produce very short abstractive summaries. For a detailed survey on existing automatic summarization techniques and other related"
P13-1100,P11-1052,0,0.256696,"one is faced with a different set of problems: volume (popular articles generate more than 10,000 comments), noise (most comments are vacuous, linguistically deficient, and tangential to the article), and redundancy (similar views are expressed by multiple commenters). In both cases, there is a delicate balance between choosing the salient, relevant, popular, and diverse points (e.g., sentences) versus minimizing syntactic and semantic redundancy. While there have been many approaches to automatic summarization (see Section 2), our work is directly inspired by the recent elegant framework of (Lin and Bilmes, 2011). They employed the powerful theory of submodular functions for summarization: submodularity embodies the “diminishing returns” property and hence is a natural vocabulary to express the summarization desiderata. In this framework, each of the constraints (relevance, redundancy, etc.) is captured as a submodular function and the objective is to maximize their sum. A simple greedy algorithm is guaranteed to produce an approximately optimal summary. They used this framework to obtain the best results on the DUC 2004 corpus. Even though the submodularity framework is quite general, it has limitati"
P13-1100,W04-1013,0,0.0228914,"s to produce a summary of all the documents for a given cluster. Comments summarization. We extracted a set of news articles and corresponding user comments from Yahoo! News website. Our corpus contains a set of 34 articles and each article is associated with anywhere from 100–500 comments. Each comment contains more than three sentences and 36 words per sentence on average. 5.2 Evaluation For each summarization task, we compare the system output (i.e., summaries automatically produced by the algorithm) against the humangenerated summaries and evaluate the performance in terms of ROUGE score (Lin, 2004), a standard recall-based evaluation measure used in summarization. A system that produces higher ROUGE scores generates better quality summary and vice versa. We use the following evaluation settings in our experiments for each summarization task: (1) For multi-document summarization, we compute the ROUGE-15 scores that was the main evaluation criterion for DUC 2004 evaluations. 3 The choice of the value 0.25 in the cover component is inspired by the observations made by (Lin and Bilmes, 2011) for the α value used in their cover function. 4 http://duc.nist.gov/duc2004/tasks.html 5 ROUGE v1.5."
P13-1100,W06-2501,0,0.00856417,"or each pair of nodes (u, v) in the graph, we compute the semantic similarity score (using WordNet) between every pair of dependency relation (rel: a, b) in u and v as: X s(u, v) = WN(ai , aj ) × WN(bi , bj ), reli ∈u,relj ∈v reli =relj where rel is a relation type (e.g., nsubj) and a, b are the two arguments present in the dependency relation (b does not exist for some relations). WN(wi , wj ) is defined as the WordNet similarity score between words wi and wj .2 The edge weights are then normalized across all edges in the 2 There exists various semantic relatedness measures based on WordNet (Patwardhan and Pedersen, 2006). In our experiments, for WN we pick one that is based on the path length between the two words in the WordNet graph. graph. This allows us to perform approximate matching of syntactic treelets obtained from the dependency parses using semantic (WordNet) similarity. For example, the sentences “I adore tennis” and “Everyone likes tennis” convey the same view and should be assigned a higher similarity score as opposed to “I hate tennis”. Using the syntactic structure along with semantic similarity helps us identify useful (valid) nuggets of information within comments (or documents), avoid redun"
P19-1368,I17-4010,0,0.0266403,"Missing"
P19-1368,I17-4009,0,0.0215441,"Missing"
P19-1368,N18-2118,0,0.116232,"Missing"
P19-1368,C16-1189,0,0.38796,"Missing"
P19-1368,N16-1062,0,0.170753,"Missing"
P19-1368,P17-2083,0,0.0495047,"Missing"
P19-1368,I17-4004,0,0.0164021,"the language agnostic capabilities of SGNN++ • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). • SwDA: Switchboard Dialog Act is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). • ATIS: Intent Understanding is a widely used corpus in the speech and dialog community (T¨ur et al., 2010) for understanding different intents during flight reservation. • CF: Customer Feedback is a multilingual customer feedback analysis task (Liu et al., 2017) that aims at categorizing customer feedback as “comment, “request, “bug, “complaint, “meaningless, or “undetermined. The data is in English (EN), Japanese (JP), French (FR) and Spanish (SP) languages. Table 1 shows the characteristics of each task: language, number of classes, training and test data. MRDA SwDA ATIS CF-EN CF-JP CF-FR CF-SP Experimental Setup & Parameter Tuning We setup our experiments as given a classification task and a dataset, generate an on-device model. For each task, we report Accuracy on the test set. Lang. EN EN EN EN JP FR SP #Classes 6 42 21 5 5 5 5 Train 78K 193K 4,"
P19-1368,W17-5530,0,0.234682,"Missing"
P19-1368,D18-1092,1,0.809259,"Missing"
P19-1368,N19-1339,1,0.428301,"roject sequence xi to word projection vectors. We use word-level context features (e.g., phrases and word-level skip-grams) extracted from the raw text to compute the intermediate feature vector ~xw = Fw and compute projections. ej (xi ) = projection(x~iw , P ej ) P w w e1...` (xiw ) eipw = P w e1 (xi ), ..., P e` (xiw ) ] = [P w (7) (8) w We reserve ` bits to capture the word projection space computed using a series of ` functions e1 , ..., P e` . The functions project the sentence P w w structure into low-dimensional representation that 3786 captures similarity in the word-projection space (Sankar et al., 2019). 2.3.2 Character Projections Given the input text xi , we can capture morphology (character-level) information in a similar way. We use character-level context features (e.g., character-level skip-grams) again extracted directly from the raw text to compute ~xc = Fc and compute character projections eipc . ej ) ej (xi ) = projection(x~ic , P (9) P c c `+1...T e eipc = P (10) (xic ) c e`+1 (xi ), ..., P eT (xiw ) ] = [P c c The character feature space and hence projections eipc are reserved and computed separately. Note that even though we compute separate projections for character-level conte"
P19-1368,W04-2319,0,0.245116,"ep is O(T · d · C), where C is the number of hidden units in e hp in the multi-layer projection network and typically smaller than T · d. 3 NLP Datasets and Experimental Setup 3.1 Datasets & Tasks We evaluate our on-device SGNN++ model on four NLP tasks and languages such as English, Japanese, Spanish and French. The datasets were selected so we can compare against prior ondevice work (Ravi and Kozareva, 2018) and also test the language agnostic capabilities of SGNN++ • MRDA: Meeting Recorder Dialog Act is a dialog corpus of multiparty meetings annotated with 6 dialog acts (Adam et al., 2003; Shriberg et al., 2004). • SwDA: Switchboard Dialog Act is a popular open domain dialog corpus between two speakers with 42 dialog acts (Godfrey et al., 1992; Jurafsky et al., 1997). • ATIS: Intent Understanding is a widely used corpus in the speech and dialog community (T¨ur et al., 2010) for understanding different intents during flight reservation. • CF: Customer Feedback is a multilingual customer feedback analysis task (Liu et al., 2017) that aims at categorizing customer feedback as “comment, “request, “bug, “complaint, “meaningless, or “undetermined. The data is in English (EN), Japanese (JP), French (FR) and"
P19-1431,D15-1174,0,0.221567,"Missing"
P19-1431,D17-1060,0,0.0276202,"n Fig. 2, refer to the Appendix B for more qualitative analysis. 4 Related Work KG completion is an important research area, with several embedding-based models proposed, such as TransE which scores translations of entities in embedding space (Bordes et al., 2013), DistMult (Toutanova et al., 2015), ComplEx which is an extension to complex space (Trouillon et al., 2016), ConvE which uses 2D convolution layers (Dettmers et al., 2017) as well as recent tensor decomposition methods (Lacroix et al., 2018). Refer to Nickel et al. (2016) for a more comprehensive review. Recently, Das et al. (2017); Xiong et al. (2017) proposed reinforcement learning methods which find paths in KG. We compared with MINERVA (Das et al., 2017), a recent method, and found A2N to perform favorably. Graph Convolution Networks (Kipf and Welling, 2016; Schlichtkrull et al., 2017) and Graph attention networks (Veliˇckovi´c et al., 2017) also learn neighborhood based representations of nodes. However, they do not learn a query-dependent composition of the neighborhood which is sub-optimal as also seen our in experiments and noted previously (Dettmers et al., 2017). They are also computationally expensive. Nguyen et al. (2016) propos"
Q14-1009,C04-1080,0,0.101209,"Missing"
Q14-1009,bosco-etal-2000-building,0,0.0117266,"sible tags). As in their setup, we then use the first 47,996 tokens of section 16 as raw data and perform final evaluation on the sections 22-24. We use the raw corpus along with the unlabeled test data to perform model minimization and EM training. Unknown words are allowed to have all possible tags in both these procedures. Italian Data: The minimization strategy presented here is a general-purpose method that does not require any specific tuning and works for other languages as well. To demonstrate this, we also perform evaluation on a different language (Italian) us112 ing the TUT corpus (Bosco et al., 2000). Following (Garrette and Baldridge, 2012), we use the same data splits as their setting. We take the first half of each of the five sections to build the word-tag dictionary, the next quarter as raw data and the last quarter as test data. The dictionary was constructed from 41,000 tokens comprised of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of 18,574 tokens and the test contained 18,763 tokens. We use the unlabeled corpus from the raw and test data to perform model minimization followed by unsu"
Q14-1009,W06-2920,0,0.0444332,"comprised of 7,814 word types, 8,370 word/tag pairs, per-type ambiguity of 1.07 and a per-token ambiguity of 1.41 on the raw data. The raw data consisted of 18,574 tokens and the test contained 18,763 tokens. We use the unlabeled corpus from the raw and test data to perform model minimization followed by unsupervised EM training. Other Languages: In order to test the effectiveness of our method in other non-English settings, we also report the performance of our method on several other Indo-European languages using treebank data from CoNLL-X and CoNLL-2007 shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). The corpus statistics for the five languages (Danish, Greek, Italian, Portuguese and Spanish) are listed below. For each language, we construct a dictionary from the raw training data. The unlabeled corpus from the raw training and test data is used to perform model minimization followed by unsupervised EM training. As before, unknown words are allowed to have all possible tags. We report the final tagging performance on the test data and compare it to baseline EM. Garrette and Baldridge (2012) treat unknown words (words that appear in the raw text but are missing from t"
Q14-1009,D10-1056,0,0.0144409,"e grammar. • We demonstrate the power of the new method by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited a"
Q14-1009,P11-1061,0,0.0228149,"s (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only enco"
Q14-1009,D12-1075,0,0.254825,"from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encounter new words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words using some heuristics (Toutanova and Johnson, 2008) or using other methods such as type-supervision (Garrette and Baldridge, 2012). We extend the method and show how to efficiently parallelize the algorithm on modern parallel computing platforms while preserving approximation guarantees. The new method easily scales to large data and grammar sizes, overcoming the memory bottleneck in previous approaches. We demonstrate the power of the new algorithm by evaluating on various sequence labeling tasks: Part-of-Speech tagging for multiple languages (including lowresource languages), with complete and incomplete dictionaries, and supertagging, a complex sequence labeling task, where the grammar size alone can grow to millions"
Q14-1009,N13-1014,0,0.143541,"0b) introduced a two-step greedy approximation to the original objective function (called the MIN-GREEDY algorithm) that runs much faster while maintaining the high tagging performance. Garrette and Baldridge (2012) showed how to use several heuristics to further improve this algorithm (for instance, better choice of tag bigrams when breaking ties) and stack other techniques on top, such as careful initialization of HMM emission models which results in further performance gains. Their method also works under incomplete dictionary scenarios and can be applied to certain low-resource scenarios (Garrette and Baldridge, 2013) by combining model minimization with supervised training. In this work, we propose a new scalable algorithm for performing model minimization for this task. By making an assumption on the structure of the solution, we prove that a variant of the greedy set cover algorithm always finds an approximately optimal label set. This is in contrast to previous methods that employ heuristic approaches with no guarantee on the quality of the solution. In addition, we do not have to rely on ad hoc tie-breaking procedures or careful initializations for unknown words. Finally, not only is the proposed meth"
Q14-1009,P11-2008,0,0.0399487,"Missing"
Q14-1009,P08-1085,0,0.0219689,"rk Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative me"
Q14-1009,P07-1094,0,0.0298242,"tstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. Formally, we are given a set of sequences, S = {S1 , S2 , . . . , Sn } where each Si is a sequence of words, Si = wi1 , wi2 , . . . , wi,|Si |. W"
Q14-1009,E09-1042,0,0.0167952,"hod achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling"
Q14-1009,J07-3004,0,0.0143698,"inimization combined with additional heuristics. We observe that switching to the new model minimization procedure alone yields significant improvement in tagging accuracy under both dictionary scenarios. It is encouraging that a better minimization procedure also leads to higher tagging quality on the unknown word tokens (column 4 in the table), even when the input dictionary is tiny. 6.2 Supertagging Compared to POS tagging, a more challenging task is learning supertaggers for lexicalized grammar formalisms such as Combinatory Categorial Grammar (CCG) (Steedman, 2000). For example, CCGbank (Hockenmaier and Steedman, 2007) contains 1241 distinct supertags (lexical categories) and the most ambiguous word has 126 supertags. This provides a much more challenging starting point for the semi-supervised methods typically applied to the task. Yet, this is an important task since creating grammars and resources for CCG parsers for new domains and languages is highly labor- and knowledge-intensive. As described earlier, our approach scales easily to large datasets as well as label sizes. To evaluate it on the supertagging task, we use the same dataset from (Ravi et al., 2010a) and compare against their baseline method t"
Q14-1009,D07-1031,0,0.0594848,"Missing"
Q14-1009,2005.mtsummit-papers.11,0,0.00660923,"m the test corpus that were labeled correctly by the system. 6.1 Part-of-Speech Tagging Task 6.1.1 Tagging Using a Complete Dictionary Data: We use a standard test set (consisting of 24,115 word tokens from the Penn Treebank) for the POS tagging task. The tagset consists of 45 distinct tag labels and the dictionary contains 57,388 word/tag pairs derived from the entire Penn Treebank. Per-token ambiguity for the test data is about 1.5 tags/token. In addition to the standard 24k dataset, we also train and test on larger data sets— 973k tokens from the Penn Treebank, 3M tokens from PTB+Europarl (Koehn, 2005) data. Methods: We evaluate and compare performance for POS tagging using four different methods that employ the model minimization idea combined with EM training: • EM: Training a bigram HMM model using EM algorithm (Merialdo, 1994). • ILP + EM: Minimizing grammar size using integer linear programming, followed by EM training (Ravi and Knight, 2009). Speedups: We also observe a significant speedup when using the parallelized version of the DMLC algorithm. Performing model minimization on the 24k tokens dataset takes 55 seconds on a single machine, whereas parallelization permits model minimiz"
Q14-1009,J93-2004,0,0.0450147,"sets used in previous work (Garrette and Baldridge, 2012) and compare against the previous best reported performance for the same task. In all the experiments (described here and in subsequent sections), we use the following terminology—raw data refers to unlabeled text used by different methods (for model minimization or other unsupervised training procedures such as EM), dictionary consists of word/tag entries that are legal, and test refers to data over which tagging evaluation is performed. English Data: For English POS tagging with incomplete dictionary, we evaluate on the Penn Treebank (Marcus et al., 1993) data. Following (Garrette and Baldridge, 2012), we extracted a word-tag dictionary from sections 00-15 (751,059 tokens) consisting of 39,087 word types, 45,331 word/tag entries, a per-type ambiguity of 1.16 yielding a pertoken ambiguity of 2.21 on the raw corpus (treating unknown words as having all 45 possible tags). As in their setup, we then use the first 47,996 tokens of section 16 as raw data and perform final evaluation on the sections 22-24. We use the raw corpus along with the unlabeled test data to perform model minimization and EM training. Unknown words are allowed to have all poss"
Q14-1009,J94-2001,0,0.676583,"(Banko and Moore, 2004). We propose a new method for unsupervised tagging that finds minimal models which are then further improved by Expectation Maximization training. In contrast to previous approaches that rely on manually specified and multi-step heuristics for model minimization, our approach is a simple greedy approximation algorithm DMLC (D ISTRIBUTED M INIMUM -L ABEL -C OVER) that solves this objective in a single step. Recently, there has been an increasing amount of research tackling this problem using unsupervised methods. A popular approach is to learn from POS-tag dictionaries (Merialdo, 1994), where we are given a raw word sequence and a dictionary of legal tags for each word type. Learning from POStag dictionaries is still challenging. Complete wordtag dictionaries may not always be available for use and in every setting. When they are available, the dictionaries are often noisy, resulting in high tagging ambiguity. Furthermore, when applying taggers in new domains or different datasets, we may encounter new words that are missing from the dictionary. There have been some efforts to learn POS taggers from incomplete dictionaries by extending the dictionary to include these words"
Q14-1009,D10-1020,0,0.0135847,"by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge"
Q14-1009,P09-1057,1,0.744425,"results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for buildin"
Q14-1009,P10-1051,1,0.893268,"12) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. For"
Q14-1009,C10-1106,1,0.923866,"12) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Garrette and Baldridge, 2013). Additional work focused on building supervised taggers for noisy domains such as Twitter (Gimpel et al., 2011). While most of the relevant work in this area centers on POS tagging, there has been some work done for building taggers for more complex sequence labeling tasks such as supertagging (Ravi et al., 2010a). Other related work include alternative methods for learning sparse models via priors in Bayesian inference (Goldwater and Griffiths, 2007) and posterior regularization (Ganchev et al., 2010). But these methods only encourage sparsity and do not explicitly seek to minimize the model size, which is the objective function used in this work. Moreover, taggers learned using model minimization have been shown to produce state-of-the-art results for the problems discussed here. 3 Model Following Ravi and Knight (2009), we formulate the problem as that of label selection on the sentence graph. For"
Q14-1009,W10-2909,0,0.0146768,"ower of the new method by evaluating under several different scenarios—POS tagging for multiple languages (including low-resource languages), with complete and incomplete dictionaries, as well as a complex sequence labeling task of supertagging. Our results show that for all these settings, our method achieves state-of-the-art performance yielding high quality taggings. 2 Related Work Recently, there has been an increasing amount of research tackling this problem from multiple directions. Some efforts have focused on inducing POS tag clusters without any tags (Christodoulopoulos et al., 2010; Reichart et al., 2010; Moon et al., 2010), but evaluating such systems proves difficult since it is not straightforward to map the cluster labels onto gold standard tags. A more popular approach is to learn from POS-tag dictionaries (Merialdo, 1994; Ravi and Knight, 2009), incomplete dictionaries (Hasan and Ng, 2009; Garrette and Baldridge, 2012) and human-constructed dictionaries (Goldberg et al., 2008). Another direction that has been explored in the past includes bootstrapping taggers for a new language based on information acquired from other languages (Das and Petrov, 2011) or limited annotation resources (Ga"
Q14-1009,D07-1096,0,\N,Missing
W09-1804,J93-2003,0,0.194849,"or example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proc"
W09-1804,J07-2003,0,0.0397522,"rat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat"
W09-1804,P08-2007,0,0.0555579,"2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. There, integer programming is used only for alignment, not for learning parameter values. 5 Conclusions and Future Work Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 34 We have presented a novel objective function for alignment, and we have applied it"
W09-1804,P06-1097,0,0.0170631,"re 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are n"
W09-1804,D07-1006,0,0.0284779,"d alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural L"
W09-1804,N04-1035,1,0.765488,"ubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat"
W09-1804,J01-2001,0,0.0231395,"s the multi-morpheme structure inside Turkish words. Consider the tiny Turkish-English corpus in Figure 5. Even a non-Turkish speaker might plausibly align yurur to walk, um to I, and ler to they. However, none of the popular machine aligners is able to do this, since they align at the wholeword level. Designers of translation systems sometimes employ language-specific word breakers before alignment, though these are hard to build and maintain, and they are usually not only languagespecific, but also language-pair-specific. Good unsupervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, in"
W09-1804,N06-1015,0,0.0680526,"tantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNer"
W09-1804,P97-1063,0,0.0295588,"ies, including garcia/garcia, are/son, are/estan, not/no, has/tiene, etc. shows the gold alignment for the corpus in Figure 1 (displayed here as English-Spanish), which results in 28 distinct bilingual dictionary entries. By contrast, a monotone alignment induces 39 distinct entries, due to less re-use. Next we look at how to automatically rifle through all legal alignments to find the one with the best score. What is a legal alignment? For now, we consider it to be one where: • Every foreign word is aligned exactly once (Brown et al., 1993). • Every English word has either 0 or 1 alignments (Melamed, 1997). We formulate our integer program (IP) as follows. We set up two types of binary variables: • Alignment link variables. If link-i-j-k = 1, that means in sentence pair i, the foreign word at position j aligns to the English words at position k. • Bilingual dictionary variables. If dict-f-e = 1, that means word pair (f, e) is “in” the dictionary. We constrain the values of link variables to satisfy the two alignment conditions listed earlier. We also require that if link-i-j-k = 1 (i.e., we’ve decided on an alignment link), then dict-fij -eik should also equal 1 (the linked words are recorded a"
W09-1804,J04-4002,0,0.0728219,"ok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok"
W09-1804,N04-1021,0,0.0207175,"ame meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zan"
W09-1804,P05-1034,0,0.0277886,"n anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok"
W09-1804,E06-1004,0,0.0748656,"r training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. There, integer programming is used only for alignment, not for learning parameter values. 5 Conclusions and Future Work Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 34 We have presented a novel objective function for alignment, and we have applied it to whole-word and sub-word alignment problems. Preliminary results look good, especially given that new objective function is simpler than those previously proposed. The integer programming framework makes the model easy to implement, and its optimal behavior frees us from worrying about search errors. We believe there are good future possibilities for this work: • Extend legal alignments to cover n-to-m and discontinuous cases. While morphemeto-morpheme alignment is more frequently a • •"
W09-1804,C69-0101,0,0.681853,"pervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it does not explicitly enumerate the Turkish vocabulary, this transducer can output a very large number of distinct Turkish words (i.e., character sequences preceded and followed by space). This transducer has 177 rules, 18 states, and 23 terminals (Turkish characters). RTG generation produces English trees that the transducer converts to Turkish, both via the tree automata toolkit Tiburon (May and Knight, 2006). From this, we obtain a parallel Turkish-English corpus. A fragment of the corpus is shown in Figu"
W09-1804,P08-1084,0,0.101809,"stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are not explicitly programmed to do so. We also address another weakness of current aligners: they only align full words. With few exceptions, e.g. (Zhang et al., 2003; Snyder and Barzilay, 2008), aligners do not operate at the sub-word level, making them much less useful for agglutinative languages such as Turkish. Our present contributions are as follows: • We offer a simple new objective function that scores a corpus alignment based on how many distinct bilingual word pairs it contains. • We use an integer programming solver to carry out optimization and corpus alignment. • We extend the system to perform subword alignment, which we demonstrate on a Turkish-English corpus. The results in this paper constitute a proof of concept of these ideas, executed on small corpora. We conclude"
W09-1804,H05-1010,0,0.497013,"lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dicti"
W09-1804,C96-2141,0,0.570617,"responds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT W"
W09-1804,J97-3002,0,0.615325,"st sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f |e) or P(e, f), with lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integ"
W11-2213,D09-1056,0,0.161412,"Missing"
W11-2213,P98-1012,0,0.393346,"Missing"
W11-2213,E06-1002,0,0.398963,"Missing"
W11-2213,S07-1024,0,0.0526643,"Missing"
W11-2213,D07-1074,0,0.3922,"Missing"
W11-2213,P06-1039,0,0.0806836,"Missing"
W11-2213,N10-1061,0,0.0420086,"disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a document corpus D associated with a certain ambiguous name, our task is to group the documents into K sets such that each do"
W11-2213,W03-0405,0,0.332327,"pproaches (Artiles et al., 2009b; Chen et al., 2009; Lan et al., 2009) focus on person name disambiguation, while others (Pedersen et al., 2006) also explore ambiguity in organization and location names. In the medical domain, Hatzivassiloglou et al. (2001) and Ginter et al. (2004) tackle the problem of gene and protein name disambiguation. Due to the high interest in this task, researchers have explored a wide range of approaches and features. Among the most common and efficient ones are those based on clustering and bag-of-words representation (Pedersen et al., 2005; Artiles et al., 2009b). Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. Others (Bunescu and Pasca, 2006; Cucerzan, 2007; Nguyen and Cao, 2008) work on Wikipedia articles, using infobox and link information. Pedersen et al. (2006) rely on second order co-occurrence vectors. A few others (Matthias, 2005; Wan et al., 2005; Popescu and Magnini, 2007) identify names of people, locations and organizations and use them as a source of evidence to measure the similarity between documents containing the ambiguous names. The most similar wor"
W11-2213,S07-1041,0,0.293535,"wide range of approaches and features. Among the most common and efficient ones are those based on clustering and bag-of-words representation (Pedersen et al., 2005; Artiles et al., 2009b). Mann and Yarowsky (2003) extract biographic facts such as date or place of birth, occupation, relatives among others to help resolve ambiguous names of people. Others (Bunescu and Pasca, 2006; Cucerzan, 2007; Nguyen and Cao, 2008) work on Wikipedia articles, using infobox and link information. Pedersen et al. (2006) rely on second order co-occurrence vectors. A few others (Matthias, 2005; Wan et al., 2005; Popescu and Magnini, 2007) identify names of people, locations and organizations and use them as a source of evidence to measure the similarity between documents containing the ambiguous names. The most similar work to ours is that of Song et al. (2007) who use a topic-based modeling approach for name disambiguation. However, their method explicitly tries to model the distribution of latent topics with regard to person names and words appearing within documents whereas in our method, the latent topics represent the underlying entities (name senses) for an ambiguous name. Unlike the previous approaches which were specif"
W11-2213,P09-1070,0,0.0297868,"or a corpus such as Wikipedia or the Web, in this paper we show a novel unsupervised topic modeling approach for name disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a documen"
W11-2213,P10-1044,0,0.0153272,"er we show a novel unsupervised topic modeling approach for name disambiguation for any corpora (i.e. Web, news, Wikipedia), languages (i.e. English, Spanish, Romanian and Bulgarian) and semantic categories (i.e. people, location and organization). The obtained results show substantial improvements over the existing approaches. 3 Name Disambiguation with LDA Recently, topic modeling methods have found widespread applications in NLP for various tasks such as summarization (Daum´e III and Marcu, 2006), inferring concept-attribute attachments (Reisinger and Pasca, 2009), selectional preferences (Ritter et al., 2010) and cross-document co-reference resolution (Haghighi and Klein, 2010). Topic models such as LDA are generative models for documents and represent hidden or latent topics (where a topic is a probability distribution over words) underlying the semantic structure of documents. An important use for methods such as LDA is to infer the set of topics associated with a given document (or a collection of documents). Next, we present a novel approach for the task of name disambiguation using unsupervised topic models. 3.1 Method Description Given a document corpus D associated with a certain ambiguous"
W11-2213,C98-1012,0,\N,Missing
W11-2213,S07-1012,0,\N,Missing
W19-5901,C16-1189,0,0.132756,"ting responses for the same dialog context (Wei et al., 2017). When trained with a maximum-likelihood (MLE) objective, generative models usually tend to place more probability mass around the most commonly observed responses for a given context. So, we end up observing little variance in the generated responses in such cases. While these two imbalances are problematic for training a dialog model, they are also inherent characteristics of a dialog dataset which cannot be removed. Several approaches have been proposed in the literature to address the generic response generation issue. Li et al. (2016) propose to modify the loss function to increase the diversity in the generated responses. Multi-resolution RNN (Serban et al., 2017) addresses the above issue by additionally Open domain dialog systems face the challenge of being repetitive and producing generic responses. In this paper, we demonstrate that by conditioning the response generation on interpretable discrete dialog attributes and composed attributes, it helps improve the model perplexity and results in diverse and interesting non-redundant responses. We propose to formulate the dialog attribute prediction as a reinforcement lear"
W19-5901,D17-1321,0,0.0168306,"ce of the utterances on the dialog flow causing coherency issues. This calls for a Reinforcement Learning (RL) based framework which has the ability to optimize policies for maximizing long term rewards. At the core, the MLE objective tries to increase the conditional utterance probabilities and influences the model to place higher probabilities over the commonly occurring utterances. On the other hand, RL based methods circumvent this issue by shifting the optimization problem to maximizing long term rewards which could promote diversity, coherency, etc. Previous approaches Li et al. (2016); Kottur et al. (2017); Lewis et al. (2017) propose to model the token prediction of the next utterance as a reinforcement learning problem and optimize the models to maximize hand-crafted rewards for improving diversity, coherency, and ease of answering. Their approaches involves pre-training the encoder-decoder models with supervised training and then refining the utterance generation further with RL using the hand-engineered rewards. Their state space consists of the dialog context representation (encoder hidden states). Their action space at a given time step includes all possible words that the decoder can gen"
W19-5901,D14-1179,0,0.0160319,"Missing"
W19-5901,W02-0214,0,0.124753,"kever et al., 2014; Cho et al., 2014), there has been a growing interest in adapting the encoder-decoder models to model open-domain conversations (Sordoni et al., 2015; Serban et al., 2016a,b; Vinyals and Le, 2015).This is done by framing the next utterance generation as a machine translation problem by treating the dialog history as the source sequence and the next utterance as the target sequence. Then the models are trained end-to-end with Maximum Likelihood (MLE) objective without any hand crafted structures like slot-value pairs, dialog manager, etc used in conventional dialog modeling (Lagus and Kuusisto, 2002). Such data driven approaches are worth pursuing in the context of open-domain conversations since the next utterance distribution in open-domain conversations ∗ Work done during internship at Google 1 Proceedings of the SIGDial 2019 Conference, pages 1–10 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics conditioning with entity information in the previous utterances. Alternatively, Song et al. (2016) uses external knowledge from a retrieval model to condition the response generation. Latent variable models inspired by Conditional Variational Autoencode"
W19-5901,D17-1259,0,0.0243486,"n the dialog flow causing coherency issues. This calls for a Reinforcement Learning (RL) based framework which has the ability to optimize policies for maximizing long term rewards. At the core, the MLE objective tries to increase the conditional utterance probabilities and influences the model to place higher probabilities over the commonly occurring utterances. On the other hand, RL based methods circumvent this issue by shifting the optimization problem to maximizing long term rewards which could promote diversity, coherency, etc. Previous approaches Li et al. (2016); Kottur et al. (2017); Lewis et al. (2017) propose to model the token prediction of the next utterance as a reinforcement learning problem and optimize the models to maximize hand-crafted rewards for improving diversity, coherency, and ease of answering. Their approaches involves pre-training the encoder-decoder models with supervised training and then refining the utterance generation further with RL using the hand-engineered rewards. Their state space consists of the dialog context representation (encoder hidden states). Their action space at a given time step includes all possible words that the decoder can generate (which is very"
W19-5901,W09-3951,0,0.0942489,"Missing"
W19-5901,N16-1014,0,0.616397,"le interesting responses for the same dialog context (Wei et al., 2017). When trained with a maximum-likelihood (MLE) objective, generative models usually tend to place more probability mass around the most commonly observed responses for a given context. So, we end up observing little variance in the generated responses in such cases. While these two imbalances are problematic for training a dialog model, they are also inherent characteristics of a dialog dataset which cannot be removed. Several approaches have been proposed in the literature to address the generic response generation issue. Li et al. (2016) propose to modify the loss function to increase the diversity in the generated responses. Multi-resolution RNN (Serban et al., 2017) addresses the above issue by additionally Open domain dialog systems face the challenge of being repetitive and producing generic responses. In this paper, we demonstrate that by conditioning the response generation on interpretable discrete dialog attributes and composed attributes, it helps improve the model perplexity and results in diverse and interesting non-redundant responses. We propose to formulate the dialog attribute prediction as a reinforcement lear"
W19-5901,D16-1127,0,0.685386,"le interesting responses for the same dialog context (Wei et al., 2017). When trained with a maximum-likelihood (MLE) objective, generative models usually tend to place more probability mass around the most commonly observed responses for a given context. So, we end up observing little variance in the generated responses in such cases. While these two imbalances are problematic for training a dialog model, they are also inherent characteristics of a dialog dataset which cannot be removed. Several approaches have been proposed in the literature to address the generic response generation issue. Li et al. (2016) propose to modify the loss function to increase the diversity in the generated responses. Multi-resolution RNN (Serban et al., 2017) addresses the above issue by additionally Open domain dialog systems face the challenge of being repetitive and producing generic responses. In this paper, we demonstrate that by conditioning the response generation on interpretable discrete dialog attributes and composed attributes, it helps improve the model perplexity and results in diverse and interesting non-redundant responses. We propose to formulate the dialog attribute prediction as a reinforcement lear"
W19-5901,P14-5010,0,0.0094343,"t. It comprises conversations from around 9000 randomly sampled Reddit threads with over 100000 comments and an average of 12 turns per thread. Open-Subtitles: Additionally, we show results with the unannotated Open-Subtitles dataset (Tiedemann, 2009) (we randomly sample up to 2 million dialogs for training and validation). We tag the dataset with dialog attributes using pre-trained classifiers. We experiment with two types of dialog attributes in this paper - sentiment and dialog-acts. We annotate the utterances with sentiment tags positive, negative, neutral using the Stanford CoreNLP tool (Manning et al., 2014). We adopt the dialog-acts from two annotated dialog corpus Switchboard (Godfrey et al., 1992) and Frames (Schulz et al., 2017). Switchboard: Switchboard corpus(Godfrey et al., 1992) is a collection of 1155 chit-chat style telephonic conversations based on 70 topics. Jurafsky et al. (1997) revised the original tags to 42 dialogacts. In our experiments, we restrict dialog-acts to the top-10 most frequently annotated tags in the corpus - Statement-non-opinion, Acknowledge , Statement-opinion, Agree/Accept, Abandoned or Turn-Exit, Appreciation, Yes-No-Question, Nonverbal, Yes answers, Conventiona"
W19-5901,P08-1028,0,0.0801985,"ty measures). The diversity scores, distinct-1 and distinct-2 are computed as the fraction of uni-grams and bi-grams in the generated responses as described in (Li et al., 2016). Improvements on Dialog datasets demonstrated through quantitative & qualitative Evaluations: Additionally, we annotate an existing open domain dialog dataset using dialog attribute classifiers trained with tagged datasets like Switchboard (Godfrey et al., 1992; Jurafsky et al., 1997), Frames (Schulz et al., 2017) and demonstrate both quantitative (in terms of token perplexity/embedding metrics (Rus and Lintean, 2012; Mitchell and Lapata, 2008)) and qualitative improvements (based on human evaluations) in generating interesting responses. In this work, we show results with two types of dialog attributes - sentiment and dialogacts. It is worth investigating this approach as we need not invest much in training classifiers for very high accuracy and we show empirically that annotations from classifiers with low accuracy are able to boost token perplexity. We conjecture that the irregularities in the auto-annotated dialog attributes induce a regularization effect while training deep neural networks analogous to the dropout mechanism. Al"
W19-5901,W12-2018,0,0.0330872,"respect to the diversity measures). The diversity scores, distinct-1 and distinct-2 are computed as the fraction of uni-grams and bi-grams in the generated responses as described in (Li et al., 2016). Improvements on Dialog datasets demonstrated through quantitative & qualitative Evaluations: Additionally, we annotate an existing open domain dialog dataset using dialog attribute classifiers trained with tagged datasets like Switchboard (Godfrey et al., 1992; Jurafsky et al., 1997), Frames (Schulz et al., 2017) and demonstrate both quantitative (in terms of token perplexity/embedding metrics (Rus and Lintean, 2012; Mitchell and Lapata, 2008)) and qualitative improvements (based on human evaluations) in generating interesting responses. In this work, we show results with two types of dialog attributes - sentiment and dialogacts. It is worth investigating this approach as we need not invest much in training classifiers for very high accuracy and we show empirically that annotations from classifiers with low accuracy are able to boost token perplexity. We conjecture that the irregularities in the auto-annotated dialog attributes induce a regularization effect while training deep neural networks analogous"
W19-5901,W17-2626,0,0.0680918,"Missing"
W19-5901,P17-1061,0,0.0367558,"g in the context of open-domain conversations since the next utterance distribution in open-domain conversations ∗ Work done during internship at Google 1 Proceedings of the SIGDial 2019 Conference, pages 1–10 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics conditioning with entity information in the previous utterances. Alternatively, Song et al. (2016) uses external knowledge from a retrieval model to condition the response generation. Latent variable models inspired by Conditional Variational Autoencoders (CVAEs) are explored in (Shen et al., 2017; Zhao et al., 2017). While models with continuous latent variables tend to be uninterpretable, discrete latent variable models exhibit high variance during inference. Shen et al. (2017) append discrete attributes such as sentiment to the latent representation to generate next utterance. 1.1 By using REINFORCE (Williams, 1992) to further optimize the dialog attribute selection process, We then show improvements in specificity of the generated responses both qualitatively (based on human evaluations) and quantitatively (with respect to the diversity measures). The diversity scores, distinct-1 and distinct-2 are co"
W19-5901,P17-2080,0,0.0586851,"s are worth pursuing in the context of open-domain conversations since the next utterance distribution in open-domain conversations ∗ Work done during internship at Google 1 Proceedings of the SIGDial 2019 Conference, pages 1–10 c Stockholm, Sweden, 11-13 September 2019. 2019 Association for Computational Linguistics conditioning with entity information in the previous utterances. Alternatively, Song et al. (2016) uses external knowledge from a retrieval model to condition the response generation. Latent variable models inspired by Conditional Variational Autoencoders (CVAEs) are explored in (Shen et al., 2017; Zhao et al., 2017). While models with continuous latent variables tend to be uninterpretable, discrete latent variable models exhibit high variance during inference. Shen et al. (2017) append discrete attributes such as sentiment to the latent representation to generate next utterance. 1.1 By using REINFORCE (Williams, 1992) to further optimize the dialog attribute selection process, We then show improvements in specificity of the generated responses both qualitatively (based on human evaluations) and quantitatively (with respect to the diversity measures). The diversity scores, distinct-1 a"
W19-5901,N15-1020,0,0.152495,"Missing"
