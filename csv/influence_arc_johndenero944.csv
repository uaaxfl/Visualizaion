2020.acl-main.146,W18-6318,0,0.0443468,"onditioned on the position of the previous alignment link. While simpler and faster tools exist such as FastAlign (Dyer et al., 2013), which is based on a reparametrization of IBM Model 2, the GIZA++ implementation of Model 4 is still used today in applications where alignment quality is important. In contrast to GIZA++, our neural approach is easy to integrate on top of an attention-based translation network, has a training pipeline with fewer steps, and leads to superior alignment quality. 2.2 Neural Models Most neural alignment approaches in the literature, such as Tamura et al. (2014) and Alkhouli et al. (2018), rely on alignments generated by statistical systems that are used as supervision for training the neural systems. These approaches tend to learn to copy the alignment errors from the supervising statistical models. Zenkel et al. (2019) use attention to extract alignments from a dedicated alignment layer of a neural model without using any output from a statistical aligner, but fail to match the quality of GIZA++. Garg et al. (2019) represents the current state of the art in word alignment, outperforming GIZA++ by training a single model that is able to both translate and align. This model is"
2020.acl-main.146,D16-1162,0,0.0335281,"on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typica"
2020.acl-main.146,J93-2003,0,0.137104,"sferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is"
2020.acl-main.146,P11-1043,1,0.765311,"approach that shares most parameters with a neural translation model can potentially take advantage of improvements to the underlying translation model, for example from domain adaptation via fine-tuning. Figure 1: Word alignment generated by a human annotator. tions, we infer a symmetrized attention matrix that jointly optimizes the likelihood of the correct output words under both models in both languages. Ablation experiments highlight the effectiveness of this novel extension, which is reminiscent of agreement-based methods for statistical models (Liang et al., 2006; Grac¸a et al., 2008; DeNero and Macherey, 2011). End-to-end experiments show that our system is the first to consistently yield higher alignment quality than GIZA++ using a fully unsupervised neural model that does not use the output of a statistical alignment model in any way. 2 2.1 Related Work Statistical Models Statistical alignment models directly build on the lexical translation models of Brown et al. (1993), known as the IBM models. The most popular statistical alignment tool is GIZA++ (Och and Ney, 2000b, 2003; Gao and Vogel, 2008). For optimal performance, the training pipeline of GIZA++ relies on multiple iterations of IBM Model"
2020.acl-main.146,N13-1073,0,0.834775,"annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias"
2020.acl-main.146,2015.mtsummit-papers.11,0,0.0250753,"not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several"
2020.acl-main.146,D19-1453,0,0.765707,"cult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage cont"
2020.acl-main.146,2013.mtsummit-wptp.9,0,0.0506155,"y integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has mult"
2020.acl-main.146,2005.iwslt-1.8,0,0.626432,"IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural translation model can poten"
2020.acl-main.146,P19-1124,0,0.0615851,"n are both salient in the input to the attention component. 3.4 A state-of-the-art Transformer includes multiple attention heads whose context vectors are stacked to form the context activation for a layer, and the encoder and decoder have multiple layers. For all experiments, we use a downscaled Transformer model trained for translation with a 6-layer encoder, a 3-layer decoder, and 256-dimensional hidden states and embedding vectors. Alignment Layer Attention Optimization Extracting alignments with attention-based models works well when used in combination with greedy translation inference (Li et al., 2019). However, the alignment task involves predicting an alignment between a sentence and an observed translation, which requires forced decoding. When a token in the target sentence is unexpected given the preceding target prefix, attention activations computed 1607 Attention Optimization Word Alignment Layer Alignment Layer Word Softmax A Guided Loss Softmax Linear Linear K ApplyAtt A V Linear Q Self Attention Linear Softmax AL Linear Softmax AL CalcAttLogits CalcAttLogits Q K Linear Linear Encoder Decoder Input Emb. Output Emb. E Encoder Decoder Input Emb. Output Emb. Figure 3: Alignment layer"
2020.acl-main.146,N06-1014,0,0.166445,"al Linguistics Moreover, our fully neural approach that shares most parameters with a neural translation model can potentially take advantage of improvements to the underlying translation model, for example from domain adaptation via fine-tuning. Figure 1: Word alignment generated by a human annotator. tions, we infer a symmetrized attention matrix that jointly optimizes the likelihood of the correct output words under both models in both languages. Ablation experiments highlight the effectiveness of this novel extension, which is reminiscent of agreement-based methods for statistical models (Liang et al., 2006; Grac¸a et al., 2008; DeNero and Macherey, 2011). End-to-end experiments show that our system is the first to consistently yield higher alignment quality than GIZA++ using a fully unsupervised neural model that does not use the output of a statistical alignment model in any way. 2 2.1 Related Work Statistical Models Statistical alignment models directly build on the lexical translation models of Brown et al. (1993), known as the IBM models. The most popular statistical alignment tool is GIZA++ (Och and Ney, 2000b, 2003; Gao and Vogel, 2008). For optimal performance, the training pipeline of G"
2020.acl-main.146,W03-0301,0,0.455216,"Missing"
2020.acl-main.146,W17-4804,0,0.522781,"Missing"
2020.acl-main.146,C00-2163,0,0.624383,"her underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural trans"
2020.acl-main.146,P00-1056,0,0.900563,"her underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Moreover, our fully neural approach that shares most parameters with a neural trans"
2020.acl-main.146,J03-1002,0,0.151023,"for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017). The decoder has multiple layers, each with several attention heads, which makes it difficult to interpret attention activations as word alignments. As a result, the most widely used tools to infer word alignments, namely GIZA++ (Och and Ney, 2003) and FastAlign (Dyer et al., 2013), are still based on the statistical IBM word alignment models developed nearly thirty years ago (Brown et al., 1993). No previous unsupervised neural approach has matched their performance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment"
2020.acl-main.146,P16-1162,0,0.192525,"Missing"
2020.acl-main.146,D19-1084,0,0.12189,"Missing"
2020.acl-main.146,P14-1138,0,0.46193,"each alignment link is conditioned on the position of the previous alignment link. While simpler and faster tools exist such as FastAlign (Dyer et al., 2013), which is based on a reparametrization of IBM Model 2, the GIZA++ implementation of Model 4 is still used today in applications where alignment quality is important. In contrast to GIZA++, our neural approach is easy to integrate on top of an attention-based translation network, has a training pipeline with fewer steps, and leads to superior alignment quality. 2.2 Neural Models Most neural alignment approaches in the literature, such as Tamura et al. (2014) and Alkhouli et al. (2018), rely on alignments generated by statistical systems that are used as supervision for training the neural systems. These approaches tend to learn to copy the alignment errors from the supervising statistical models. Zenkel et al. (2019) use attention to extract alignments from a dedicated alignment layer of a neural model without using any output from a statistical aligner, but fail to match the quality of GIZA++. Garg et al. (2019) represents the current state of the art in word alignment, outperforming GIZA++ by training a single model that is able to both transla"
2020.acl-main.146,2011.eamt-1.10,0,0.0337482,"odel in a manner that is tightly integrated and does not affect translation quality. 1 Introduction Although word alignments are no longer necessary to train machine translation (MT) systems, they still play an important role in applications of neural MT. For example, they enable injection of an external lexicon into the inference process to enforce the use of domain-specific terminology or improve the translations of low-frequency content words (Arthur et al., 2016). The most important application today for word alignments is to transfer text annotations from source to target (M¨uller, 2017; Tezcan and Vandeghinste, 2011; Joanis et al., 2013; Escartın and Arcedillo, 2015). For example, if part of a source sentence is underlined, the corresponding part of its translation should be underlined as well. HTML tags and other markup must be transferred for published documents. Although annotations could in principle be generated directly as part of the output sequence, they are instead typically transferred via word alignments because example annotations typically do not exist in MT training data. The Transformer architecture provides state-ofthe-art performance for neural machine translation (Vaswani et al., 2017)."
2020.acl-main.146,C96-2141,0,0.944322,"ance. Recent work on alignment components that are integrated into neural translation models either underperform the IBM models or must use the output of IBM models during training to outperform them (Zenkel et al., 2019; Garg et al., 2019). This work combines key components from Zenkel et al. (2019) and Garg et al. (2019) and presents two novel extensions. Statistical alignment methods contain an explicit bias towards contiguous word alignments in which adjacent source words are aligned to adjacent target words. This bias is expressed in statistical systems using a hidden Markov model (HMM) (Vogel et al., 1996), as well as symmetrization heuristics such as the growdiag-final algorithm (Och and Ney, 2000b; Koehn et al., 2005). We design an auxiliary loss function that can be added to any attention-based network to encourage contiguous attention matrices. The second extension replaces heuristic symmetrization of word alignments with an activation optimization technique. After training two alignment models that translate in opposite direc1605 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1605–1617 c July 5 - 10, 2020. 2020 Association for Computational L"
2020.acl-main.146,W08-0509,0,\N,Missing
2021.findings-emnlp.299,J93-2003,0,0.136384,"Missing"
2021.findings-emnlp.299,2020.emnlp-main.42,0,0.011746,"slation system in the forward and backward direction. We then extract alignments using bidirectional attention optimization. We follow the hyperparameter settings of Zenkel et al. (2020): 6 encoder and 3 decoder layers with a layer dimension of 256. Finally, we train a guided alignment layer on top of the existing translation model in the forward direction. In contrast to Zenkel et al. (2020), we additionally shift the attention by one 5 Scripts to reproduce this setup are available at https://github.com/lilt/markup-transfer-scripts. unit to the right using the “SHIFT-ATT” method described by Chen et al. (2020), which resulted in higher quality alignments. We finally generate attention distributions from the guided alignment layer and extract alignments based on the attention. To extract alignments, for each target token we select the source token with the highest attention value as its alignment link. This method, which is commonly used across neural alignment systems (Garg et al., 2019; Zenkel et al., 2019), does not produce any unaligned target tokens and produces more alignment links than FastAlign. 6.4 Supervised Markup Transfer: Sequence-to-Sequence Model The sequence-to-sequence markup transf"
2021.findings-emnlp.299,N13-1073,0,0.0340796,"expresses the token-level correspondence between a source sentence and its target translation. Tokens can be words, individual characters or subwords. Our experiments align subwords to minimize alignment error rate (Zenkel et al., 2020). Let si and tj represent the ith token in the source sentence and the jth token in its translation, respectively. The number of tokens of the source sentence and its translation are I and J. Additionally, let A(si ) ⊆ {1, . . . , J} define the alignments of the ith source token to a set of target tokens. In this work, we compare the popular FastAlign toolkit (Dyer et al., 2013), a statistical aligner, to a state-of-the-art neural alignment approach described by Zenkel et al. (2020) based on the Transformer architecture. 4.2 Min-Max Tag Pair Projection As a baseline markup transfer algorithm we implement the approach described by Hanneman and https://github.com/lilt/markup-tag-evaluation. 2 Dinu (2020), which we call the Min-Max algorithm. In case of ambiguity due to multiple tags with the same label, each reference tag is matched with a unique hypothesis Each tag pair in the source sentence spans multiple tag in a way that maximizes accuracy. contiguous source token"
2021.findings-emnlp.299,D19-1453,0,0.0991263,"sfer markup into this text. This paper describes approaches to automating the sec- 2 Related Work ond step in this workflow by automatically transferring source markup into a fixed reference trans- Recent work in neural word alignment has indilation. This fixed reference may not be preferred cated that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation mod"
2021.findings-emnlp.299,2020.wmt-1.138,0,0.0260092,"o approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as input a source sentence with XML markup and directly generates a translation that includes XML tags. A translation training set that includes markup can either be created by human translators (Hashimoto et al., 2019) or synthesized by adding markup to an existing unformatted bitext (Hanneman and Dinu, 2020). A translation model that generates both text and markup may prefer an output sequence for which the XML markup is invalid (e.g. there might be an opening tag that is not closed). This problem can be addressed through XML-constrained beam search (Hashimoto et al., 2019). This approach requires training data that contains XML markup. The second approach is to train the translation model without markup, separately train a word aligner, and then transfer format using an inference pipeline. After the translation model has generated a text translation, the alignment model aligns the tokens of the"
2021.findings-emnlp.299,W19-5212,0,0.323417,"ted that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as input a source sentence with XML markup and directly generates a translation that includes XML tags. A translation training set that includes markup can either be created by human translators (Hashimoto et al., 2019) or synthesized by adding markup to an e"
2021.findings-emnlp.299,2020.findings-emnlp.147,0,0.0168766,"s approaches to automating the sec- 2 Related Work ond step in this workflow by automatically transferring source markup into a fixed reference trans- Recent work in neural word alignment has indilation. This fixed reference may not be preferred cated that markup transfer is a downstream task, 3524 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3524–3533 November 7–11, 2021. ©2021 Association for Computational Linguistics though evaluations of word aligners have not included an explicit evaluation of markup transfer (Garg et al., 2019; Nagata et al., 2020; Jalili Sabet et al., 2020). Experiments in this paper are the first to quantify the amount by which the improved alignment quality of a neural aligner compared to FastAlign also improves markup transfer accuracy. Markup can be represented using XML tags (Hashimoto et al., 2019). Previous work describes two approaches to markup transfer for fully automated machine translation, where the goal is to place each XML tag from the source into the target translation in a way that produces well-formed XML. The first approach is to include markup while training the translation model, such that the translation model takes as inpu"
2021.findings-emnlp.299,P18-1007,0,0.0132837,"onsisting of approximately 100k segments, a validation set of 2k segments and an unreleased test set. One fourth of the segments in both the training and validation set contain at least one markup tag. We hold out 1k segments of the training set for early stopping, use the remaining segments for training and the validation set for testing. Only a fixed set of 14 different opening and closing markup tags appear in the dataset, each of these tag pairs spanning one or more characters. 6.2 Tokenization We use byte pair encoding (BPE) (Sennrich et al., 2016) computed via the SentencePiece toolkit (Kudo, 2018), and follow the setup described by Hashimoto et al. (2019) for subword tokenization. We add all tags and the separator token used for the input of the sequence-to-sequence model as user-defined symbols. In contrast to Hashimoto et al. (2019), we also add all punctuation marks to this set. These symbols will not be split or merged by the SentencePiece toolkit and are always represented as a single token. We learn a joint subword vocabulary of 10k tokens for each language pair and use this tokenization for both the supervised sequence-to-sequence model and the unsupervised alignment systems. Ze"
2021.findings-emnlp.299,W17-4804,0,0.0172484,"generated a text translation, the alignment model aligns the tokens of the source segment to the generated translation. Finally, a deterministic algorithm (labeled Min-Max in Section 4.2) transfers the markup from the source segment into the translation via the word alignments (Hanneman and Dinu, 2020). This approach does not require training data that contains XML markup. Figure 1: Two nested tag pairs that have similar tag positions. Tag pair 1 is the parent of tag pair 2. segments (Hashimoto et al., 2019). Past work has also included manual evaluation of the transferred markup information (Müller, 2017; Hanneman and Dinu, 2020), since transfer accuracy could not be assessed directly. In contrast, our goal is to transfer markup directly into the reference translation. Evaluation of markup accuracy is therefore straightforward: a tag is placed correctly if it appears at the correct character position within the reference translation. 3 Bilingual Markup Transfer In this section we introduce tag pairs, the data structure with which we represent markup information, and define two evaluation metrics. 3.1 Definition We represent all markup information as tag pairs. A tag pair contains an opening a"
2021.findings-emnlp.299,2020.emnlp-main.41,0,0.0223022,"Missing"
2021.findings-emnlp.299,P02-1040,0,0.110985,"ers from the reference by more than just markup. characters that appear before the tag in the sentence, Instead, automatic metrics such as XML accuracy not including any other tags. In contrast to the check that all source tags appear in the target and opening tag, the label of a closing tag contains are properly nested. XML-based BLEU splits the a forward slash (e.g. &lt;/b&gt;). There are no selftranslation at every formatting tag both for the ref- closing tags in this representation. A TagPair erence and the translation and calculates the BLEU has a parent if there is another TagPair that score (Papineni et al., 2002) on the resulting sub- encloses it. 3525 3.2 Metrics The following two metrics1 score a proposed set of tags that are well-formed XML (properly nested with each opening tag closed) in which every source tag pair appears exactly once in the target. Let L be the character length of the reference translation. In the following we denote the character position of a tag as p ∈ {0, ..., L}. We start by matching the reference and hypothesis tags by their label. Therefore, let T = {(pr , ph )} ∈ {0, ..., L} × {0, ..., L} be the set of tuples of all reference and hypothesis character-level positions, an"
2021.findings-emnlp.299,P16-1162,0,0.0195699,"s aligned segments. The data set is split into a training set consisting of approximately 100k segments, a validation set of 2k segments and an unreleased test set. One fourth of the segments in both the training and validation set contain at least one markup tag. We hold out 1k segments of the training set for early stopping, use the remaining segments for training and the validation set for testing. Only a fixed set of 14 different opening and closing markup tags appear in the dataset, each of these tag pairs spanning one or more characters. 6.2 Tokenization We use byte pair encoding (BPE) (Sennrich et al., 2016) computed via the SentencePiece toolkit (Kudo, 2018), and follow the setup described by Hashimoto et al. (2019) for subword tokenization. We add all tags and the separator token used for the input of the sequence-to-sequence model as user-defined symbols. In contrast to Hashimoto et al. (2019), we also add all punctuation marks to this set. These symbols will not be split or merged by the SentencePiece toolkit and are always represented as a single token. We learn a joint subword vocabulary of 10k tokens for each language pair and use this tokenization for both the supervised sequence-to-seque"
2021.findings-emnlp.299,2020.acl-main.146,1,0.916657,"e apply a twostep process. First we use an unsupervised aligner to infer the alignments between source and target subwords. The second step uses a deterministic algorithm to place tag pairs based on these alignments. Two advantages of this unsupervised approach are that it does not require training data with markup, and it can leverage any word aligner. 4.1 Alignments An alignment expresses the token-level correspondence between a source sentence and its target translation. Tokens can be words, individual characters or subwords. Our experiments align subwords to minimize alignment error rate (Zenkel et al., 2020). Let si and tj represent the ith token in the source sentence and the jth token in its translation, respectively. The number of tokens of the source sentence and its translation are I and J. Additionally, let A(si ) ⊆ {1, . . . , J} define the alignments of the ith source token to a set of target tokens. In this work, we compare the popular FastAlign toolkit (Dyer et al., 2013), a statistical aligner, to a state-of-the-art neural alignment approach described by Zenkel et al. (2020) based on the Transformer architecture. 4.2 Min-Max Tag Pair Projection As a baseline markup transfer algorithm w"
D08-1033,W06-3123,0,0.481906,"Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the larger structures win o"
D08-1033,W07-0403,0,0.376506,"hat is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentation"
D08-1033,P08-2007,1,0.27816,"r in training bitexts. Since bitexts do not come segmented and aligned into phrase pairs, these counts are typically gathered by fixing a word alignment and applying phrase extraction heuristics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values"
D08-1033,W06-3105,1,0.836239,"ned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and small structures, the la"
D08-1033,P07-1035,0,0.03404,"Missing"
D08-1033,P06-1085,0,0.581666,"from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts d"
D08-1033,P07-1107,1,0.305536,"now has the general form P (x, z, θJ ); all other model parameters have been fixed. Instead of searching for a suitable θJ ,9 we sample from the posterior distribution P (z|x) with θJ marginalized out. To this end, we convert our Gibbs sampler into a collapsed Gibbs sampler10 using the Chinese Restaurant Process (CRP) representation of the DP (Aldous, 1985). With the CRP, we avoid the problem of explicitely representing samples from the DP. CRP-based samplers have served the community well in related language tasks, such as word segmentation and coreference resolution (Goldwater et al., 2006; Haghighi and Klein, 2007). Under this representation, the probability of each sampling outcome is a simple expression in terms of the state of the rest of the training corpus (the Markov blanket), rather than explicitly using θJ . Let zm be the set of aligned phrase pair tokens observed in the rest of the corpus. Then, when he, f i is aligned (that is, neither e nor f are null), the conditional probability for a pair he, f i takes the form: τ (he, f i|zm ) = counthe,f i (zm ) + α · M0 (he, f i) , |zm |+ α where counthe,f i (zm ) is the number of times that he, f i appears in zm . We can write this expression thanks to"
D08-1033,N07-1018,0,0.0201427,", state z0 , which sets all latent variables to some initial configuration. We then produce a sequence of sample states zi , each of which differs from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been a"
D08-1033,P07-2045,0,0.0111405,"tally share the same lexical content. Details of these fringe conditions have been omitted for space, but were included in our implementation. 12 The largest phrase pair found was 13 English words by 7 Spanish words. 321 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 4.5 ∼ DP (Pe , α0 ) . Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parame"
D08-1033,W07-0734,0,0.0139473,"DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 100 iterations through the"
D08-1033,W02-1018,0,0.725912,"tics to this word-aligned training corpus. Alternatively, phrase pair frequencies can be learned via a probabilistic model of phrase alignment, but this approach has presented several practical challenges. In this paper, we address the two most significant challenges in phrase alignment modeling. The first challenge is with inference: computing alignment expectations under general phrase models is #P-hard (DeNero and Klein, 2008). Previous phrase alignment work has sacrificed consistency for efficiency, employing greedy hill-climbing algorithms and constraining inference with word alignments (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). We describe a Gibbs sampler that consistently and efficiently approximates expectations, using only polynomial-time computable operators. Despite the combinatorial complexity of the phrase alignment space, our sampled phrase pair expectations are guaranteed to converge to the true posterior distributions under the model (in theory) and do converge to effective values (in practice). The second challenge in learning phrase alignments is avoiding a degenerate behavior of the general model class: as with many models which can choose between large and sma"
D08-1033,D07-1038,0,0.125653,"positional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6 For experiments, we ran the sampler for 100 iterations. 319 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over θJ , the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be an appropriate prior. A draw from a Kdimensional Dirichlet distribution is a list of K real numbers in [0, 1] that sum to one, which can be in"
D08-1033,W07-0715,0,0.0825004,"e procedure, for example with word alignments and linguistic features (Birch et al., 2006), or by disallowing large phrase pairs using a noncompositional constraint (Cherry and Lin, 2007; Zhang et al., 2008). However, the problem lies with the model, and therefore should be corrected in the model, rather than the inference procedure. Model-based solutions appear in the literature as well, though typically combined with word alignment constraints on inference. A sparse Dirichlet prior coupled with variational EM was explored by Zhang et al. (2008), but it did not avoid the degenerate solution. Moore and Quirk (2007) proposed a new conditional model structure that does not cause large and small phrases to compete for probability mass. May and Knight (2007) added additional model terms to balance the cost of long and short derivations in a syntactic alignment model. 6 For experiments, we ran the sampler for 100 iterations. 319 4.2 A Dirichlet Process Prior We control this degenerate behavior by placing a Dirichlet process (DP) prior over θJ , the distribution over aligned phrase pairs (Ferguson, 1973). If we were to assume a maximum number K of phrase pair types, a (finite) Dirichlet distribution would be"
D08-1033,J03-1002,0,0.0360836,"lexical content. Details of these fringe conditions have been omitted for space, but were included in our implementation. 12 The largest phrase pair found was 13 English words by 7 Spanish words. 321 Translation Results We evaluate our new estimates using the baseline translation pipeline from the 2007 Statistical Machine Translation Workshop shared task. 5.1 4.5 ∼ DP (Pe , α0 ) . Baseline System We trained Moses on all Spanish-English Europarl sentences up to length 20 (177k sentences) using GIZA++ Model 4 word alignments and the growdiag-final-and combination heuristic (Koehn et al., 2007; Och and Ney, 2003; Koehn, 2002), which performed better than any alternative combination heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phras"
D08-1033,P03-1021,0,0.0251313,"nation heuristic.13 The baseline estimates (Heuristic) come from extracting phrases up to length 7 from the word alignment. We used a bidirectional lexicalized distortion model that conditions on both foreign and English phrases, along with their orientations. Our 5-gram language model was trained on 38.3 million words of Europarl using Kneser-Ney smoothing. We report results with and without lexical weighting, denoted lex. We tuned and tested on development corpora for the 2006 translation workshop. The parameters for each phrase table were tuned separately using minimum error rate training (Och, 2003). Results are 13 Sampling iteration time scales quadratically with sentence length. Short sentences were chosen to speed up our experiment cycle. Estimate Heuristic DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5."
D08-1033,P02-1040,0,0.103231,"cle. Estimate Heuristic DP HDP DP-composed HDP-composed DP-smooth HDP-smooth Heuristic + lex DP-smooth + lex HDP-smooth + lex Phrase Pair Count 4.4M 0.6M 0.3M 3.7M 3.1M 4.8M 4.6M 4.4M 4.8M 4.6M NIST BLEU 29.8 28.8 29.1 30.1 30.1 30.1 30.2 30.5 30.4 30.7 Exact Match 5.3 METEOR 52.4 51.7 52.0 52.7 52.6 52.5 52.7 52.9 53.0 53.2 Table 1: BLEU results for learned distributions improve over a heuristic baseline. Estimate labels are described fully in section 5.3. The label lex indicates the addition of a lexical weighting feature. scored with lowercased, tokenized NIST BLEU, and exact match METEOR (Papineni et al., 2002; Lavie and Agarwal, 2007). The baseline system gives a BLEU score of 29.8, which increases to 30.5 with lex, as shown in Table 1. For reference, training on all sentences of length less than 40 (the shared task baseline default) gives 32.4 BLEU with lex. 5.2 Learned Distribution Performance We initialized the sampler with a configuration derived from the word alignments generated by the baseline. We greedily constructed a phrase alignment from the word alignment by identifying minimal phrase pairs consistent with the word alignment in each region of the sentence. We then ran the sampler for 1"
D08-1033,P08-1084,0,0.00762606,"m pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for a sequence at once – requires a #P-hard computation. While this asymptotic complexity was apparently not prohibitive in the case of morphological alignment, where the sequences are short, it is prohibitive in phrase alignment, where the sentences are often very long. 3.2 Sampling with the S WAP Operator Our Gibbs sampler repeatedly applies each of five operators to each position in each training sentence pa"
D08-1033,P06-1124,0,0.110307,"d alignment, state z0 , which sets all latent variables to some initial configuration. We then produce a sequence of sample states zi , each of which differs from the last by some small local change. The samples zi are guaranteed (in the limit) to consistently approximate the conditional distribution P (z|x, θ) (or P (z|x) later). Therefore, the average counts of phrase pairs in the samples converge to expected counts under the model. Normalizing these expected counts yields estimates for the features φ(e|f ) and φ(f |e). Gibbs sampling is not new to the natural language processing community (Teh, 2006; Johnson et al., 2007). However, it is usually used as a search procedure akin to simulated annealing, rather than for approximating expectations (Goldwater et al., 2006; Finkel et al., 2007). Our application is also atypical 316 for an NLP application in that we use an approximate sampler not only to include Bayesian prior information (section 4), but also because computing phrase alignment expectations exactly is a #P-hard problem (DeNero and Klein, 2008). That is, we could not run EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under"
D08-1033,P08-1012,0,0.552863,"un EM exactly, even if we wanted maximum likelihood estimates. 3.1 Related Work Expected phrase pair counts under P (z|x, θ) have been approximated before in order to run EM. Marcu and Wong (2002) employed local search from a heuristic initialization and collected alignment counts during a hill climb through the alignment space. DeNero et al. (2006) instead proposed an exponential-time dynamic program pruned using word alignments. Subsequent work has relied heavily on word alignments to constrain inference, even under reordering models that admit polynomial-time E-steps (Cherry and Lin, 2007; Zhang et al., 2008). None of these approximations are consistent, and they offer no method of measuring their biases. Gibbs sampling is not only consistent in the limit, but also allows us to add Bayesian priors conveniently (section 4). Of course, sampling has liabilities as well: we do not know in advance how long we need to run the sampler to approximate the desired expectations “closely enough.” Snyder and Barzilay (2008) describe a Gibbs sampler for a bilingual morphology model very similar in structure to ours. However, the basic sampling step they propose – resampling all segmentations and alignments for"
D08-1033,J93-2003,0,\N,Missing
D08-1033,2006.amta-papers.2,0,\N,Missing
D09-1147,D08-1023,0,0.100647,"Missing"
D09-1147,W07-0403,0,0.0138536,"n the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extr"
D09-1147,D08-1024,0,0.0743063,"computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance reliably using consensus decoding and occasionally"
D09-1147,P09-1064,1,0.928192,"decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To main"
D09-1147,P02-1001,0,0.101743,"n integrated language model decoder. These contexts are required both to correctly compute the model score of derivations and to compute clipped n-gram counts. To speed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some"
D09-1147,P07-1019,0,0.0988077,"the rule, while head(h) refers to the “head” or “parent”. A forest of translations is built by combining the nodes vi using h to form a new node u = head(h). Each forest node consists of a grammar symbol and target language boundary words used to track n-grams. In the above, we keep one boundary word for each node, which allows us to track bigrams. In this section, we develop an analytical expression for the gradient of CoBLEU, then discuss how to efficiently compute the value of the objective function and gradient. 3.1 Translation Model Form We first assume the general hypergraph setting of Huang and Chiang (2007), namely, that derivations under our translation model form a hypergraph. This framework allows us to speak about both phrase-based and syntax-based translation in a unified framework. We define a probability distribution over derivations d via θ as: Z(fi ) = X Eθ [c(φk , d)|fi ] (2) Eθ [`n (d)|fi ] (3) Eθ [c(φk , d) · `n (d)|fi ] (4) P where `n (d) = gn c(gn , d) is the sum of all ngrams on derivation d (its “length”). The first expectation is an expected count of the kth feature φk over all derivations of fi . The second is an expected length, the total expected count of all ngrams in deriva"
D09-1147,P07-2045,0,0.0256167,"e replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5"
D09-1147,P09-1019,0,0.162126,"ond benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translatio"
D09-1147,D09-1005,0,0.103725,"ed our computations, we use the cube pruning method of Huang and Chiang (2007) with a fixed beam size. For regularization, we added an L2 penalty on the size of θ to the CoBLEU objective, a simple addition for gradient ascent. We did not find that our performance varied very much for moderate levels of regularization. 3.6 Related Work The calculation of expected counts can be formulated using the expectation semiring framework of Eisner (2002), though that work does not show how to compute expected products of counts which are needed for our gradient calculations. Concurrently with this work, Li and Eisner (2009) have generalized Eisner (2002) to compute expected products of counts on translation forests. The training algorithm of Kakade et al. (2002) makes use of a dynamic program similar to ours, though specialized to the case of sequence models. 4 Consensus Decoding Once model parameters θ are learned, we must select an appropriate decoding objective. Several new decoding approaches have been proposed recently that leverage some notion of consensus over the many weighted derivations in a translation forest. In this paper, we adopt the fast consensus decoding procedure of DeNero et al. (2009), which"
D09-1147,P09-1067,0,0.362973,"An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency"
D09-1147,D08-1076,0,0.254574,"at this function and its gradient are efficiently computable over packed forests of translations generated by machine translation systems. The gradient includes expectations of products of features and n-gram counts, a quantity that has not appeared in previous work. We present a new dynamic program which allows the efficient computation of these quantities over translation forests. The resulting gradient ascent procedure does not require any k-best approximations. Optimizing over translation forests gives similar stability benefits to recent work on lattice-based minimum error rate training (Macherey et al., 2008) and large-margin training (Chiang et al., 2008). We developed CoBLEU primarily to complement consensus decoding, which it does; it produces higher BLEU scores than coupling MERT with consensus decoding. However, we found an additional empirical benefit: CoBLEU is less prone to overfitting than MERT, even when using Viterbi decoding. In experiments, models trained to maximize tuning set BLEU using MERT consistently degraded in performance from tuning to test set, while CoBLEU-trained models generalized more robustly. As a result, we found that optimizing CoBLEU improved test set performance re"
D09-1147,J03-1002,0,0.00628979,"te training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a feature that counts the number of inverted ITG rewrites. 5.2 Data We extracted phrase tables from the SpanishEnglish and French-English sections of the Europarl corpus, which include approximately 8.5 million words of bitext for each of the language pairs (Koehn, 2002). We used a trigram language model trained on the entire corpus of English parliamentary proceedings provided with"
D09-1147,P03-1021,0,0.135668,"predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference translations. To maintain consistency across the translation pipeline, we formulate CoBLEU to share the functional form of BLEU used for evaluation. As a result, CoBLEU optimizes exactly the quantities that drive efficient consensus decoding techniques and precisely mirrors the objective used for fast consensus decoding in DeNero et al. (2009). CoBLEU is a continuous and (mostly) different"
D09-1147,P02-1040,0,0.0861394,"unts us3 -12 ing CoBP(R, F, θ): g min{Eθ [c(g, d)|f ], c(g, r)} &quot; g Eθ [c(g, d)|f ] Figure 1: (a) A simple hypothesis space of translations for a single sentence containing three alternatives, each with two features. The hypotheses are scored under a log-linear model with parameters θ equal to the identity vector. (b) The expected counts of all bigrams that appear in the computation of consensus bigram precision. 2 Model: TM + !LM • LM g = Pm P i=1 where Eθ [c(g2 , d)|fi ] = Our proposed objective function maximizes ngram precision by adapting the BLEU evaluation metric as a tuning objective (Papineni et al., 2002). To simplify exposition, we begin by adapting a simpler metric: bigram precision. Bigram Precision Tuning Let the tuning corpus consist of source sentences F = f1 . . . fm and human-generated references R = r1 . . . rm , one reference for each source sentence. Let ei be a translation of fi , and let E = e1 . . . em be a corpus of translations, one for each source sentence. A simple evaluation score for E is its bigram precision BP(R, E): Pm P BP(R, E) = i=1 min{E c(g2 , ri )}2 2 , d)|fi ],!LM θ [c(g 0 Parameter: Pm P (1) E [c(g , d)|f ] 2 i θ i=1 g 2 (b) Objectives as functions of ! LM Consen"
D09-1147,D08-1012,1,0.873057,"Missing"
D09-1147,P06-2101,0,0.165268,"in Table 1. In Spanish-English, CoBLEU slightly outperformed MERT under the same initialization, while the opposite pattern appears for French-English. The best test set performance in both language pairs was the third condition, in which CoBLEU training was initialized with MERT. This condition also gave the highest CoBLEU objective value. This pattern indicates that CoBLEU is a useful objective for translation with consensus decoding, but that the gradient ascent optimization is getting stuck in local maxima during tuning. This issue can likely be addressed with annealing, as described in (Smith and Eisner, 2006). Interestingly, the brevity penatly results in French indicate that, even though CoBLEU did Spanish Test 30.2 30.9 French Tune Test 32.0 31.0 31.7 30.9 Tune 32.5 30.5 ∆ -2.3 +0.4 ∆ -1.0 -0.8 Table 2: Performance measured by BLEU using Viterbi decoding indicates that CoBLEU is less prone to overfitting than MERT. not outperform MERT in a statistically significant way, CoBLEU tends to find shorter sentences with higher n-gram precision than MERT. Table 1 displays a second benefit of CoBLEU training: compared to MERT training, CoBLEU performance degrades less from tuning to test set. In Spanish,"
D09-1147,D08-1065,0,0.119162,"downstream consensus decoding procedures. An unexpected second benefit is that it reduces overfitting, which can improve test set BLEU scores when using standard Viterbi decoding. 1 Introduction Increasing evidence suggests that machine translation decoders should not search for a single top scoring Viterbi derivation, but should instead choose a translation that is sensitive to the model’s entire predictive distribution. Several recent consensus decoding methods leverage compact representations of this distribution by choosing translations according to n-gram posteriors and expected counts (Tromble et al., 2008; DeNero et al., 2009; Li et al., 2009; Kumar et al., 2009). This change in decoding objective suggests a complementary change in tuning objective, to one that optimizes expected n-gram counts directly. The ubiquitous minimum error rate training (MERT) approach optimizes Viterbi predictions, but does not explicitly boost the aggregated posterior probability of desirable n-grams (Och, 2003). We therefore propose an alternative objective function for parameter tuning, which we call consensus BLEU or CoBLEU, that is designed to maximize the expected counts of the n-grams that appear in reference"
D09-1147,J97-3002,0,0.0373861,"mpute the similarity of a sentence e to a reference r based on the n-gram counts of each. When training with CoBLEU, we replace e with expected counts and maximize θ. In consensus decoding, we replace r with expected counts and maximize e. Several other efficient consensus decoding pro1423 5 Experiments We compared CoBLEU training with an implementation of minimum error rate training on two language pairs. 5.1 Model Our optimization procedure is in principle tractable for any syntactic translation system. For simplicity, we evaluate the objective using an Inversion Transduction Grammar (ITG) (Wu, 1997) that emits phrases as terminal productions, as in (Cherry and Lin, 2007). Phrasal ITG models have been shown to perform comparably to the state-ofthe art phrase-based system Moses (Koehn et al., 2007) when using the same phrase table (Petrov et al., 2008). We extract a phrase table using the Moses pipeline, based on Model 4 word alignments generated from GIZA++ (Och and Ney, 2003). Our final ITG grammar includes the five standard Moses features, an n-gram language model, a length feature that counts the number of target words, a feature that counts the number of monotonic ITG rewrites, and a"
D09-1147,J07-2003,0,\N,Missing
D11-1018,P11-1103,0,0.0429393,"left bound of their projection ψ(i). Null-aligned words are placed to the left of the next aligned word to their right in the original order. The reordering-specific loss functions defined in Equation 1 has been shown to correlate with human judgements of translation quality, especially for language pairs with substantial reordering like EnglishJapanese (Talbot et al., 2011). Other reorderingspecific loss functions also correlate with human judgements (Birch et al., 2010). Future research could experiment with alternative reordering-based loss functions, such as Kendall’s Tau, as suggested by Birch and Osborne (2011). 3.3.2 of the parallel parsing model is best measured on B, given fully trained parsing and reordering models. ! ! X R σ arg max [w · φ(t)] , σ ∗ (2) 4. An indicator feature of whether parentheses and brackets are balanced in each span. The model weights of features 3 and 4 above are fixed to large negative constants to prefer terminal spans of length up to k and spans with balanced punctuation. The weight of feature 1 is fixed to 1, and weight 2 was set via line search to 0.3. Ties among trees were broken randomly. Of course, the problem of selecting training trees need not be directly tied"
D11-1018,P09-1088,0,0.0139005,"e order Inverted order Syntactic pre-ordering (Genzel, 2010) Prec 82.0 81.3 81.2 75.4 72.5 Parsing Rec 87.8 87.7 87.9 82.0 61.0 F1 84.8 84.4 84.4 78.5 66.3 Tree Reordering accN accT RO 97.3 93.6 87.7 97.0 92.6 86.6 95.9 93.2 83.8 89.2 89.7 66.8 91.6 83.3 72.0 Pipeline R 80.5 79.4 77.8 49.7 49.5 34.9 30.8 66.0 Table 1: Accuracy of individual monolingual parsing and reordering models, as well as complete pipelines trained on annotated and learned word alignments. 4.4 Bilingual Grammar Induction Also related to STIR is previous work on bilingual grammar induction from parallel corpora using ITG (Blunsom et al., 2009). These models have focused on learning phrasal translations — which are the terminal productions of a synchronous ITG — rather than reordering patterns that occur higher in the tree. Hence, while this paper shares formal machinery and data sources with that line of work, the models themselves target orthogonal aspects of the translation problem. 5 Experimental Results As training data for our models we used 14,000 English sentences that were sampled from the web, translated into Japanese, and manually annotated with word alignments. The annotation was carried out by the original translators t"
D11-1018,J93-2003,0,0.0332401,"ct extension of these approaches, but one which induces structure from parallel corpora rather than relying on a treebank. Tromble (2009) show that some pre-ordering benefits can be realized without a parsing step at all, by instead casting pre-ordering as a permutation modeling problem. While not splitting the task of preordering into parsing and tree rordering, that work shows that pre-ordering models can be learned directly from parallel corpora. 4.2 Integrated Reordering Models Distortion models have been primary components in machine translation models since the advent of statistical MT (Brown et al., 1993). In modern systems, reordering models are integrated into decoders as additional features in a discriminative loglinear model, which also includes a language model, translation features, etc. In these cases, reordering models interact with the strong signal of a targetside language model. Because ordering prediciton is conflated with target-side generation, evaluations are conducted on the entire generated output, which cannot isolate reordering errors from other sorts of errors, like lexical selection. Despite these differences, certain integrated reordering models are similar in character t"
D11-1018,N09-1009,0,0.0103045,"plied during training (for word alignment and rule extraction) and translation time without adding complexity to the extraction and decoding algorithms. Of course, integrating our model into translation inference represents a potentially fruitful avenue of future research. 4.3 Grammar Induction The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text (Pereira and Schabes, 1992). Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from wellchosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al., 2009). In principle, these models must discover the syntactic patterns that govern a language from the sequences of word tokens alone. These models are often evaluated relative to reference treebank annotations. Grammar induction in the context of machine translation reordering offers different properties. The alignment patterns in a parallel corpus provide an additional signal to models that is strongly tied to syntactic properties of the aligned languages. Also, the evaluation is straightforward—any syntactic structure that supports the prediction o"
D11-1018,P05-1066,0,0.882805,"th integrated reordering, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction. 1 Introduction Recent work in statistical machine translation (MT) has demonstrated the effectiveness of syntactic preordering: an approach that permutes source sentences into a target-like order as a pre-processing step, using features of a source-side syntactic parse (Collins et al., 2005; Xu et al., 2009). Syntactic pre-ordering is particularly effective at applying structural transformations, such as the ordering change from a subject-verb-object (SVO) language like English to a subject-object-verb (SOV) language like Japanese. However, state-of-the-art 193 Jakob Uszkoreit Google Research uszkoreit@google.com pre-ordering methods require a supervised syntactic parser to provide structural information about each sentence. We propose a method that learns both a parsing model and a reordering model directly from a word-aligned parallel corpus. Our approach, which we call Struct"
D11-1018,N10-1128,0,0.0270452,"entire generated output, which cannot isolate reordering errors from other sorts of errors, like lexical selection. Despite these differences, certain integrated reordering models are similar in character to syntactic pre-ordering models. In particular, the tree rotation model of Yamada and Knight (2001) posited that reordering decisions involve rotations of a source-side syntax tree. The parameters of such a model can be trained by treating tree rotations as latent variables in a factored translation model, which parameterizes reordering and transfer separately but performs joint inference (Dyer and Resnik, 2010). Syntactic reordering and transfer can also be modeled jointly, for instance in a tree-to-string translation system parameterized by a transducer grammar. While the success of integrated reordering models certainly highlights the importance of reordering in machine translation systems, we see several advantages to a pipelined, pre-ordering approach. First, 200 the pre-ordering model can be trained and evaluated directly. Second, pre-ordering models need not factor according to the same dynamic program as the translation model. Third, the same reordering can be applied during training (for wor"
D11-1018,P08-1109,0,0.0290132,"Missing"
D11-1018,P09-1042,0,0.0206658,"e extraction) and translation time without adding complexity to the extraction and decoding algorithms. Of course, integrating our model into translation inference represents a potentially fruitful avenue of future research. 4.3 Grammar Induction The language processing community actively works on the problem of automatically inducing grammatical structure from a corpus of text (Pereira and Schabes, 1992). Some success in this area has been demonstrated via generative models (Klein and Manning, 2002), which often benefit from wellchosen priors (Cohen and Smith, 2009) or posterior constraints (Ganchev et al., 2009). In principle, these models must discover the syntactic patterns that govern a language from the sequences of word tokens alone. These models are often evaluated relative to reference treebank annotations. Grammar induction in the context of machine translation reordering offers different properties. The alignment patterns in a parallel corpus provide an additional signal to models that is strongly tied to syntactic properties of the aligned languages. Also, the evaluation is straightforward—any syntactic structure that supports the prediction of reordering is rewarded. Kuhn (2004) applied al"
D11-1018,C10-1043,0,0.688471,"onal Linguistics Order perspective on the canonical NLP task of grammar induction—one which marries the wide-spread scientific interest in unsupervised parsing models with a clear application and extrinsic evaluation methodology. To support this perspective, we highlight several avenues of future research throughout the paper. We evaluate STIR in a large-scale EnglishJapanese machine translation system. We measure how closely our predicted reorderings match those implied by hand-annotated word alignments. STIR approaches the performance of the state-of-the-art pre-ordering method described in Genzel (2010), which learns reordering rules for supervised treebank parses. STIR gives a translation improvement of 3.84 BLEU over a standard phrase-based system with an integrated reordering model. 2 Parsing and Reordering Models STIR consists of two pipelined log-linear models for parsing and reordering, as well as a third model for inducing trees from parallel corpora, trees that serve to train the first two models. This section describes the domain and structure of each model, while Section 3 describes features and learning objectives. Figure 1 depicts the relationship between the three models. For ea"
D11-1018,P02-1017,0,0.419418,"pproach resembles work with binary synchronous grammars (Wu, 1997), but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model. The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus. This objective differs from those of previous grammar induction models, which typically focus on succinctly explaining the observed source language corpus via latent hierarchical structure (Pereira and Schabes, 1992; Klein and Manning, 2002). Our convex objective allows us to train a feature-rich log-linear parsing model, even without supervised treebank data. Focusing on pre-ordering for MT leads to a new Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193–203, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Order perspective on the canonical NLP task of grammar induction—one which marries the wide-spread scientific interest in unsupervised parsing models with a clear application and extrinsic evaluation methodology. To support this perspec"
D11-1018,W05-0820,0,0.0419517,"Missing"
D11-1018,N03-1017,0,0.059908,"Missing"
D11-1018,P04-1060,0,0.0440078,"(Ganchev et al., 2009). In principle, these models must discover the syntactic patterns that govern a language from the sequences of word tokens alone. These models are often evaluated relative to reference treebank annotations. Grammar induction in the context of machine translation reordering offers different properties. The alignment patterns in a parallel corpus provide an additional signal to models that is strongly tied to syntactic properties of the aligned languages. Also, the evaluation is straightforward—any syntactic structure that supports the prediction of reordering is rewarded. Kuhn (2004) applied alignment-based constraints to the problem of inducing probabilistic context-free grammars, and showed an improvement with respect to Penn Treebank annotations over monolingual induction. Their work is distinct from ours because it focused on projecting distituents across languages, but mirrors ours in demonstrating that there is a role for aligned parallel corpora in grammar induction. Snyder et al. (2009) also demonstrated that parallel corpora can play a role in improving the quality of grammar induction models. Their work differs from ours in that it focuses on multilingual lexica"
D11-1018,W07-0734,0,0.0118572,"an be reordered perfectly. However, some of those trees will be easier to reproduce and reorder than others. 1 An astute reviewer pointed out that no binary tree over an S-V-O sentence can license both S-O-V and V-S-O orderings. Hence, parse trees that are induced for multilingual reordering will need n-ary branches. 3.3.1 Reordering Loss Function In order to measure the effectiveness of a reordering pipeline, we would like a metric over permutations. Fortunately, permutation loss for machine translation is already an established component of the METEOR metric, called a fragmentation penalty (Lavie and Agarwal, 2007). We define a slight variant of METEOR’s fragmentation penalty that ranges from 0 to 1. Given a sentence e, a reference permutation σ ∗ of (0, · · · , |e |− 1), and a hypothesized permutation σ ˆ , let chunks(ˆ σ , σ ∗ ) be the minimum number of “chunks” in σ ˆ : the number of elements in a partition of σ ˆ such that each contiguous subsequence is also contiguous in σ ∗ . We can define the reordering score between two permutations in terms of chunks. |σ ∗ |− chunks(ˆ σ, σ∗) R(ˆ σ, σ ) = ∗ |σ |− 1 ∗ Parallel Parsing Objective (e,σ ∗ )∈B t∈B(e) Evaluating this objective involves training the oth"
D11-1018,C10-1071,0,0.0530067,"reordering models more generally, and models for the unsupervised induction of syntactic structure. We can train our reordering pipeline by dividing an aligned parallel corpus into two halves, A and B, where the monolingual parsing and tree reordering 4.1 Pre-Ordering Models models are trained on A, and their effectiveness is Our reordering pipeline is intentionally similar to evaluated on held-out set B. Then, the effectiveness approaches that use a treebank-trained supervised 199 parser to reorder source sentences at training and translation time (Xia and McCord, 2004; Collins et al., 2005; Lee et al., 2010). Given a supervised parser, a rule-based pre-ordering procedure can either be specified by hand (Xu et al., 2009) or learned automatically (Genzel, 2010). We consider our approach to be a direct extension of these approaches, but one which induces structure from parallel corpora rather than relying on a treebank. Tromble (2009) show that some pre-ordering benefits can be realized without a parsing step at all, by instead casting pre-ordering as a permutation modeling problem. While not splitting the task of preordering into parsing and tree rordering, that work shows that pre-ordering models"
D11-1018,D08-1076,1,0.738335,"Missing"
D11-1018,J04-4002,0,0.105955,"Missing"
D11-1018,W99-0604,0,0.266269,"Missing"
D11-1018,P02-1040,0,0.104921,"Missing"
D11-1018,P92-1017,0,0.514556,"y permutes this tree. Our approach resembles work with binary synchronous grammars (Wu, 1997), but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model. The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus. This objective differs from those of previous grammar induction models, which typically focus on succinctly explaining the observed source language corpus via latent hierarchical structure (Pereira and Schabes, 1992; Klein and Manning, 2002). Our convex objective allows us to train a feature-rich log-linear parsing model, even without supervised treebank data. Focusing on pre-ordering for MT leads to a new Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193–203, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics Order perspective on the canonical NLP task of grammar induction—one which marries the wide-spread scientific interest in unsupervised parsing models with a clear application and extrinsic evaluation methodolog"
D11-1018,D08-1091,0,0.0146951,"all contribute structural information. All features except POS are computed directly from aligned parallel corpora. The Cluster and POS features play a similar role of expressing reordering patterns over collections of similar words. The ablation study in Section 5 compares these two feature sets directly. 3.2 Monolingual Parsing Features The monolingual parsing model is also trained discriminatively, but involves structured prediction, as in a conditional random field (Lafferty et al., 2001). Conditional likelihood objectives have proven effective for supervised parsers (Finkel et al., 2008; Petrov and Klein, 2008). Recall that the score of a tree t = (T , N ) factors over spans. s(t) = X [k,`)∈T wT φT (k, `) + X wN φN (k, m, `) [k,`)∈N exp [s(t)] 0 (t0 )∈B(e) exp [s(t )] P(t|e) = P where B(e) is the set of well-formed trees over e. The parallel parsing model (Section 2.2) produces a tree over the source sentence of each aligned sentence pair; these trees serve as our training examples. We can maximize their conditional likelihood according to this model via gradient methods. Each tree t over sentence e has a cumulative feature vector of dimension |w |= |wT |+|wN |, formed by stacking the terminal and n"
D11-1018,P09-1009,0,0.0304282,"at is strongly tied to syntactic properties of the aligned languages. Also, the evaluation is straightforward—any syntactic structure that supports the prediction of reordering is rewarded. Kuhn (2004) applied alignment-based constraints to the problem of inducing probabilistic context-free grammars, and showed an improvement with respect to Penn Treebank annotations over monolingual induction. Their work is distinct from ours because it focused on projecting distituents across languages, but mirrors ours in demonstrating that there is a role for aligned parallel corpora in grammar induction. Snyder et al. (2009) also demonstrated that parallel corpora can play a role in improving the quality of grammar induction models. Their work differs from ours in that it focuses on multilingual lexical statistics and dependency relationships, rather than reordering patterns. Annotated word alignments All features All but POS All but Cluster All but POS & Cluster All features Learned alignments Monotone order Inverted order Syntactic pre-ordering (Genzel, 2010) Prec 82.0 81.3 81.2 75.4 72.5 Parsing Rec 87.8 87.7 87.9 82.0 61.0 F1 84.8 84.4 84.4 78.5 66.3 Tree Reordering accN accT RO 97.3 93.6 87.7 97.0 92.6 86.6"
D11-1018,W11-2102,0,0.344309,"σ ∗ , then ∗ chunks(ˆ σ , σ ) = |σ|. Hence, the metric defined by Equation 1 ranges from 0 to 1. The reference permutation σ ∗ of a source sentence e can be defined from an aligned sentence pair (e, f , A) by sorting the words ei of e by the left bound of their projection ψ(i). Null-aligned words are placed to the left of the next aligned word to their right in the original order. The reordering-specific loss functions defined in Equation 1 has been shown to correlate with human judgements of translation quality, especially for language pairs with substantial reordering like EnglishJapanese (Talbot et al., 2011). Other reorderingspecific loss functions also correlate with human judgements (Birch et al., 2010). Future research could experiment with alternative reordering-based loss functions, such as Kendall’s Tau, as suggested by Birch and Osborne (2011). 3.3.2 of the parallel parsing model is best measured on B, given fully trained parsing and reordering models. ! ! X R σ arg max [w · φ(t)] , σ ∗ (2) 4. An indicator feature of whether parentheses and brackets are balanced in each span. The model weights of features 3 and 4 above are fixed to large negative constants to prefer terminal spans of lengt"
D11-1018,D09-1105,0,0.521997,"eordering pipeline is intentionally similar to evaluated on held-out set B. Then, the effectiveness approaches that use a treebank-trained supervised 199 parser to reorder source sentences at training and translation time (Xia and McCord, 2004; Collins et al., 2005; Lee et al., 2010). Given a supervised parser, a rule-based pre-ordering procedure can either be specified by hand (Xu et al., 2009) or learned automatically (Genzel, 2010). We consider our approach to be a direct extension of these approaches, but one which induces structure from parallel corpora rather than relying on a treebank. Tromble (2009) show that some pre-ordering benefits can be realized without a parsing step at all, by instead casting pre-ordering as a permutation modeling problem. While not splitting the task of preordering into parsing and tree rordering, that work shows that pre-ordering models can be learned directly from parallel corpora. 4.2 Integrated Reordering Models Distortion models have been primary components in machine translation models since the advent of statistical MT (Brown et al., 1993). In modern systems, reordering models are integrated into decoders as additional features in a discriminative logline"
D11-1018,P08-1086,1,0.0500165,"uld stay contiguous after reordering. Features based on this statistic apply to both terminal and short non-terminal spans. The second statistic indicates when a possibly discontiguous pair of words should be adjacent after reordering. This statistic is applied to pairs of words that would end up adjacent after an inversion: ek and e`−1 for span [k, `). For instance, PC (added to) = 0.68 and PD (lexicon, to) = 0.19. Cluster. All source word types are clustered into word classes, which together maximize likelihood of the source side of a large parallel corpus under a hidden Markov model, as in Uszkoreit and Brants (2008). Indicator features based on clusterings over c classes are defined over words ek , em−1 , em and e`−1 , as well as word sequences for spans up to length 4. Features are included for a variety of clusterings with sizes c ∈ {23 , 24 , . . . , 211 }. Lexical. For a list of very common words in the source language, we include lexical indicator features for the boundary words ek and e`−1 . For instance, the word “to” triggers a reordering, as do prepositions in general. Length. Length computed as ` − k, length as a fraction of sentence length, and quantized length features all contribute structur"
D11-1018,C10-1124,1,0.876882,"Missing"
D11-1018,C96-2141,0,0.344986,"Missing"
D11-1018,J97-3002,0,0.878056,"r Reordering (STIR), requires no syntactic annotations to train, but approaches the performance of a recent syntactic pre-ordering method in a large-scale English-Japanese MT system. STIR predicts a pre-ordering via two pipelined models: (1) parsing and (2) tree reordering. The first model induces a binary parse, which defines the space of possible reorderings. In particular, only trees that properly separate verbs from their object noun phrases will license an SVO to SOV transformation. The second model locally permutes this tree. Our approach resembles work with binary synchronous grammars (Wu, 1997), but is distinct in its emphasis on monolingual parsing as a first phase, and in selecting reorderings without the aid of a target-side language model. The parsing model is trained to maximize the conditional likelihood of trees that license the reorderings implied by observed word alignments in a parallel corpus. This objective differs from those of previous grammar induction models, which typically focus on succinctly explaining the observed source language corpus via latent hierarchical structure (Pereira and Schabes, 1992; Klein and Manning, 2002). Our convex objective allows us to train"
D11-1018,C04-1073,0,0.613217,"work, including other pre-ordering methods, reordering models more generally, and models for the unsupervised induction of syntactic structure. We can train our reordering pipeline by dividing an aligned parallel corpus into two halves, A and B, where the monolingual parsing and tree reordering 4.1 Pre-Ordering Models models are trained on A, and their effectiveness is Our reordering pipeline is intentionally similar to evaluated on held-out set B. Then, the effectiveness approaches that use a treebank-trained supervised 199 parser to reorder source sentences at training and translation time (Xia and McCord, 2004; Collins et al., 2005; Lee et al., 2010). Given a supervised parser, a rule-based pre-ordering procedure can either be specified by hand (Xu et al., 2009) or learned automatically (Genzel, 2010). We consider our approach to be a direct extension of these approaches, but one which induces structure from parallel corpora rather than relying on a treebank. Tromble (2009) show that some pre-ordering benefits can be realized without a parsing step at all, by instead casting pre-ordering as a permutation modeling problem. While not splitting the task of preordering into parsing and tree rordering,"
D11-1018,N09-1028,0,0.485797,"ng, but also approaches the performance of a recent preordering method based on a supervised parser. These results show that the syntactic structure which is relevant to MT pre-ordering can be learned automatically from parallel text, thus establishing a new application for unsupervised grammar induction. 1 Introduction Recent work in statistical machine translation (MT) has demonstrated the effectiveness of syntactic preordering: an approach that permutes source sentences into a target-like order as a pre-processing step, using features of a source-side syntactic parse (Collins et al., 2005; Xu et al., 2009). Syntactic pre-ordering is particularly effective at applying structural transformations, such as the ordering change from a subject-verb-object (SVO) language like English to a subject-object-verb (SOV) language like Japanese. However, state-of-the-art 193 Jakob Uszkoreit Google Research uszkoreit@google.com pre-ordering methods require a supervised syntactic parser to provide structural information about each sentence. We propose a method that learns both a parsing model and a reordering model directly from a word-aligned parallel corpus. Our approach, which we call Structure Induction for"
D11-1018,P01-1067,0,0.349428,"tional features in a discriminative loglinear model, which also includes a language model, translation features, etc. In these cases, reordering models interact with the strong signal of a targetside language model. Because ordering prediciton is conflated with target-side generation, evaluations are conducted on the entire generated output, which cannot isolate reordering errors from other sorts of errors, like lexical selection. Despite these differences, certain integrated reordering models are similar in character to syntactic pre-ordering models. In particular, the tree rotation model of Yamada and Knight (2001) posited that reordering decisions involve rotations of a source-side syntax tree. The parameters of such a model can be trained by treating tree rotations as latent variables in a factored translation model, which parameterizes reordering and transfer separately but performs joint inference (Dyer and Resnik, 2010). Syntactic reordering and transfer can also be modeled jointly, for instance in a tree-to-string translation system parameterized by a transducer grammar. While the success of integrated reordering models certainly highlights the importance of reordering in machine translation syste"
D11-1018,P11-1084,0,0.130778,"Missing"
D13-1060,W11-0815,0,0.0564787,"f words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining t"
D13-1060,W02-2001,0,0.0510673,"areness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In contrast, there are verbs followed by particles that are not phrasal verbs, because their meaning is compositional, such as walk towards, sit behind, or paint on. We identify phrasal verbs by using frequency statistics calculated from parallel corpora, consisting of bilingual pairs of documents such that one is a translation of the other, with one document in English. We leverage the observation"
D13-1060,W03-1812,0,0.143078,"ent work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking"
D13-1060,W03-1809,0,0.104528,"in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. In contrast, there are verbs followed by particles that are not phrasal verbs, because their meaning is compositional, such as walk towards, sit behind, or paint on. We identify phrasal verbs by using frequency statistics calculated from parallel corpora, consisting of bilingual pairs of documents such that one is a translation of the other, with one document in English. We leverage the observation that a verb will translate in an aty"
D13-1060,E06-1042,0,0.0339219,"identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates multiple weak ranking sig"
D13-1060,D07-1090,0,0.0300103,"Missing"
D13-1060,J93-2003,0,0.0198052,"and a large number of rare phrases. The remaining phrases were split randomly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, }, with  the empty string. The set of candidate phrasal verbs we consider during evaluation i"
D13-1060,J90-1003,0,0.0981388,"s statistic to other types of multiword expressions, one could compute a similar distribution for other content words in the phrase. 0 5 10 15 20 25 30 35 40 45 50 Number of languages (k) N (e, f ) Aligned Phrase Pair looking forward to deseando 1 deseando tion distributions for any inflected form ei ∈ E:   1 X 0 0 kP ϕL (e) = DKL Pv(e e ) i i |E| ⇡1 (e, f ) ei ∈E looking forward to 3 3.2 mirando mirando adelante a looking 5 mirando 3 buscando We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003). The monolingual statistics are designed to identify frequent collocations in a language. This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual feaegin{tabular}{rrrr} tures in this paper. &	extit{mirando} &	extit{deseando} &	ex As above, define E to be &$frac{5}{8}=0.625 the set of morpholog$P_{v(e)}(x)$ $ &$0$ &$fr \ [-1ex] ically inflectedhline variants of a candidate e, and let $P'_{v(e)}(x)$&$0.610 $ &$0.02$ &$0.3 V be the set ofhline inflected variants of the head verb \ [-1ex] v(e) of e. We d"
D13-1060,W11-0823,0,0.0257205,"ented boosting algorithm produces a comprehensive set of English phrasal verbs, achieving performance comparable to a human-curated set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during"
D13-1060,W09-2903,0,0.0206477,"body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates multiple weak ranking signals, and demonstrated ex"
D13-1060,E06-1043,0,0.0508776,"and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Concl"
D13-1060,W11-0805,0,0.0187498,"in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs inc"
D13-1060,W06-1203,0,0.020413,"d, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-o"
D13-1060,N03-1017,0,0.00803806,"d set. 1 Introduction A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followe"
D13-1060,P10-1116,0,0.012447,"ting many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning"
D13-1060,N06-1014,0,0.0303489,"d-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, }, with  the empty string. The set of candidate phrasal verbs we consider during evaluation is the product V × P × T , which contains 96,880 items. 642 Results We optimize a ranker using the boosting algorithm describ"
D13-1060,P99-1041,0,0.357094,"es of multiword expressions, one could compute a similar distribution for other content words in the phrase. 0 5 10 15 20 25 30 35 40 45 50 Number of languages (k) N (e, f ) Aligned Phrase Pair looking forward to deseando 1 deseando tion distributions for any inflected form ei ∈ E:   1 X 0 0 kP ϕL (e) = DKL Pv(e e ) i i |E| ⇡1 (e, f ) ei ∈E looking forward to 3 3.2 mirando mirando adelante a looking 5 mirando 3 buscando We also collect a number of monolingual statistics for each phrasal verb candidate, motivated by the considerable body of previous work on the topic (Church and Hanks, 1990; Lin, 1999; McCarthy et al., 2003). The monolingual statistics are designed to identify frequent collocations in a language. This set of monolingual features is not comprehensive, as we focus our attention primarily on bilingual feaegin{tabular}{rrrr} tures in this paper. &	extit{mirando} &	extit{deseando} &	ex As above, define E to be &$frac{5}{8}=0.625 the set of morpholog$P_{v(e)}(x)$ $ &$0$ &$fr \ [-1ex] ically inflectedhline variants of a candidate e, and let $P'_{v(e)}(x)$&$0.610 $ &$0.02$ &$0.3 V be the set ofhline inflected variants of the head verb \ [-1ex] v(e) of e. We define three"
D13-1060,W03-1810,0,0.683527,"omatic “look down on him for his politics” with compositional “look down on him from the balcony.” In this paper, we focus on the task of determining whether a phrase type is a phrasal verb, meaning that it frequently expresses an idiomatic meaning across its many token usages in a corpus. We do not attempt to distinguish which individual phrase tokens in the corpus have idiomatic senses. Ranking vs. Classification. Identifying phrasal verbs involves relative, rather than categorical, judgments: some phrasal verbs are more compositional than others, but retain a degree of noncompositionality (McCarthy et al., 2003). Moreover, a polysemous phrasal verb may express an idiosyncratic sense more or less often than a compositional sense in a particular corpus. Therefore, we should expect a corpus-driven system not to classify phrases as strictly idiomatic or compositional, but instead assign a ranking or relative scoring to a set of candidates. Candidate Phrases. We distinguish between the task of identifying candidate multiword expressions 2 http://en.wiktionary.org 637 Feature ϕL (×50) µ1 µ2 µ3 Description KL Divergence for each language L frequency of phrase given verb PMI of verb and particles µ1 with int"
D13-1060,W97-0311,0,0.77223,"ressions 2 http://en.wiktionary.org 637 Feature ϕL (×50) µ1 µ2 µ3 Description KL Divergence for each language L frequency of phrase given verb PMI of verb and particles µ1 with interposed pronouns Table 1: Features used by the polyglot ranking system. and the task of ranking those candidates by their semantic idiosyncracy. With English phrasal verbs, it is straightforward to enumerate all desired verbs followed by one or more particles, and rank the entire set. Using Parallel Corpora. There have been a number of approaches proposed for the use of multilingual resources for MWE identification (Melamed, 1997; Villada Moir´on and Tiedemann, 2006; Caseli et al., 2010; Tsvetkov and Wintner, 2012; Salehi and Cook, 2013). Our approach differs from previous work in that we identify MWEs using translation distributions of verbs, as opposed to 1–1, 1–m, or m–n word alignments, most-likely translations, bilingual dictionaries, or distributional entropy. To the best of our knowledge, ours is the first approach to use translational distributions to leverage the observation that a verb typically translates differently when it heads a phrasal verb. 3 The Polyglot Ranking Approach Our approach uses bilingual a"
D13-1060,W10-3707,0,0.0132436,"ession (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that"
D13-1060,W09-2907,0,0.0209812,"n A multiword expression (MWE), or noncompositional compound, is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more p"
D13-1060,S13-1039,0,0.511053,"language L frequency of phrase given verb PMI of verb and particles µ1 with interposed pronouns Table 1: Features used by the polyglot ranking system. and the task of ranking those candidates by their semantic idiosyncracy. With English phrasal verbs, it is straightforward to enumerate all desired verbs followed by one or more particles, and rank the entire set. Using Parallel Corpora. There have been a number of approaches proposed for the use of multilingual resources for MWE identification (Melamed, 1997; Villada Moir´on and Tiedemann, 2006; Caseli et al., 2010; Tsvetkov and Wintner, 2012; Salehi and Cook, 2013). Our approach differs from previous work in that we identify MWEs using translation distributions of verbs, as opposed to 1–1, 1–m, or m–n word alignments, most-likely translations, bilingual dictionaries, or distributional entropy. To the best of our knowledge, ours is the first approach to use translational distributions to leverage the observation that a verb typically translates differently when it heads a phrasal verb. 3 The Polyglot Ranking Approach Our approach uses bilingual and monolingual statistics as features, computed over unlabeled corpora. Each statistic characterizes the degre"
D13-1060,W01-0513,0,0.0416541,"tics. The system of Salehi and Cook (2013) is perhaps the closest to the current work, judging noncompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identificati"
D13-1060,E09-1086,0,0.0143736,"es is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal verb identification, using parallel corpora from many languages to identify phrasal verbs. We proposed an evaluation metric that acknowledges the inherent incompleteness of reference sets, but distinguishes among competing systems in a manner aligned to the goals of the task. We developed a recall-oriented learning method that integrates m"
D13-1060,C10-1124,0,0.0175151,"es (bottom) compared to three baselines (top) gives comparable performance to the humancurated upper bound. removed a few very frequent phrases and a large number of rare phrases. The remaining phrases were split randomly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small"
D13-1060,W06-1204,0,0.0207598,"ds, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be determined by combining the simplex meanings of its constituent words (Baldwin and Villavicencio, 2002; Dixon, 1982; Bannard et al., 2003).1 Examples of phrasal verbs include count on [rely], look after [tend], or take off [remove], the meanings of which do not involve counting, looking, or taking. I"
D13-1060,W06-2405,0,0.184326,"Missing"
D13-1060,W03-1808,0,0.0229704,"ompositionality using string edit distance between a candidate phrase’s automatic translation and its components’ individual translations. Unlike the current work, their method does not use distributions over translations or combine individual bilingual values with boosting; however, they find, as we do, that incorporating many languages is beneficial to MWE identification. A large body of work has investigated the identification of noncompositional compounds from monolingual sources (Lin, 1999; Schone and Jurafsky, 2001; Fazly and Stevenson, 2006; McCarthy et al., 2003; Baldwin et al., 2003; Villavicencio, 2003). Many of these monolingual statistics could be viewed as weak rankers and fruitfully incorporated into our framework. There has also been a substantial amount of work addressing the problem of differentiating between literal and idiomatic instances of phrases in context (Katz and Giesbrecht, 2006; Li et al., 2010; Sporleder and Li, 2009; Birke and Sarkar, 2006; Diab and Bhutada, 2009). We do not attempt this task; however, techniques for token identification could be used to improve type identification (Baldwin, 2005). 6 Conclusion We have presented the polyglot ranking approach to phrasal ve"
D13-1060,C96-2141,0,0.0159884,"domly into a development set of 694 items and a held-out test set of 695 items. 4.3 Corpora Our monolingual English corpus consists of news articles and documents collected from the web. Our parallel corpora from English to each of 50 languages also consist of documents collected from the web via distributed data mining of parallel documents based on the text content of web pages (Uszkoreit et al., 2010). The parallel corpora were segmented into aligned sentence pairs and word-aligned using two iterations of IBM Model 1 (Brown et al., 1993) and two iterations of the HMM-based alignment model (Vogel et al., 1996) with posterior symmetrization (Liang et al., 2006). This training recipe is common in largescale machine translation systems. 4.4 Generating Candidates To generate the set of candidate phrasal verbs considered during evaluation, we exhaustively enumerated the Cartesian product of all verbs present in the previously described Wiktionary set (V), all particles in Table 2 (P) and a small set of second particles T = {with, to, on, }, with  the empty string. The set of candidate phrasal verbs we consider during evaluation is the product V × P × T , which contains 96,880 items. 642 Results We opt"
D13-1060,W10-3708,0,0.0201989,", is a sequence of words whose meaning cannot be composed directly from the meanings of its constituent words. These idiosyncratic phrases are prevalent in the lexicon of a language; Jackendoff (1993) estimates that their number is on the same order of magnitude as that of single words, and Sag et al. (2002) suggest that they are much more common, though quantifying them is challenging (Church, 2011). The task of identifying MWEs is relevant not only to lexical semantics applications, but also machine translation (Koehn et al., 2003; Ren et al., 2009; Pal et al., 2010), information retrieval (Xu et al., 2010; Acosta et al., 2011), and syntactic parsing (Sag et al., 2002). Awareness of MWEs has empirically proven useful in a number of domains: Finlayson and Kulkarni (2011), for example, use MWEs to attain a significant performance improvement in word sense disambiguation; Venkatapathy and Joshi (2006) use features associated with MWEs to improve word alignment. ∗ Research conducted during an internship at Google. We focus on a particular subset of MWEs, English phrasal verbs. A phrasal verb consists of a head verb followed by one or more particles, such that the meaning of the phrase cannot be det"
D15-1123,2010.amta-papers.16,0,0.0507827,"Missing"
D15-1123,2013.mtsummit-papers.5,0,0.089191,"177.6 58.5 + genre TM + doc. weights + sparse features 14.1 9.8 5.8 Table 2: Decoding speed on the lecture data. 2 0 -2 2 4 6 8 10 12 14 i-th sentence in document 16 18 20 Figure 2: Bleu difference between baseline + genre weights and our incremental adaptation approach, computed on a single segment from each document according to their order, i.e. the first segment from each document, then the second segment from each document, etc. large improvement in this genre would only be expected to arise in similarly structured domains. This property is quantified by the repetition rate measure (RR) (Bertoldi et al., 2013) reported in Table 1, which confirms the finding by Cettolo et al. (2014) that RR correlates with the effectiveness of adaptation. Analysis. Figure 2 shows Bleu score differences to the baseline + genre weights system for different subsets of the news and patent test sets. Each point is computed by document slicing, i.e. on a single segment from each document. The rightmost data point is the Bleu score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus. Note that this group does not correspond to any number in Table 1, which reports Bleu on the entire te"
D15-1123,2011.eamt-1.5,0,0.0230876,"lue phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For"
D15-1123,2014.amta-researchers.13,0,0.066112,"2: Decoding speed on the lecture data. 2 0 -2 2 4 6 8 10 12 14 i-th sentence in document 16 18 20 Figure 2: Bleu difference between baseline + genre weights and our incremental adaptation approach, computed on a single segment from each document according to their order, i.e. the first segment from each document, then the second segment from each document, etc. large improvement in this genre would only be expected to arise in similarly structured domains. This property is quantified by the repetition rate measure (RR) (Bertoldi et al., 2013) reported in Table 1, which confirms the finding by Cettolo et al. (2014) that RR correlates with the effectiveness of adaptation. Analysis. Figure 2 shows Bleu score differences to the baseline + genre weights system for different subsets of the news and patent test sets. Each point is computed by document slicing, i.e. on a single segment from each document. The rightmost data point is the Bleu score we obtain by evaluating on the 20th segment of each document, grouped into a pseudo-corpus. Note that this group does not correspond to any number in Table 1, which reports Bleu on the entire test sets. Thus, we evaluate on all sentences that have learned from exactl"
D15-1123,D13-1107,0,0.0179591,"cators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre,"
D15-1123,P07-1033,0,0.305837,"Missing"
D15-1123,E14-1042,0,0.352159,"o levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example, our extension adjusts off"
D15-1123,W10-1757,0,0.137447,"lation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and den"
D15-1123,N09-1068,0,0.083889,"Missing"
D15-1123,P13-1031,1,0.904175,"ible hierarchical domain structure within a single consistent model. Both weights and rules are updated incrementally on a stream of post-edits. Our multi-level domain hierarchy allows the system to adapt simultaneously towards local context at different levels of granularity, including genres and individual documents. Our experiments show consistent improvements in translation quality from all components of our approach. 1 Introduction Suggestions from a machine translation system can increase the speed and quality of professional human translators (Guerberof, 2009; Plitt and Masselot, 2010; Green et al., 2013a, inter alia). However, querying a single fixed model for all different documents fails to incorporate contextual information that can potentially improve suggestion quality. We describe a model architecture that adapts simultaneously to multiple genres and individual documents, so that translation suggestions are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of"
D15-1123,W14-3311,1,0.910621,"Missing"
D15-1123,W14-3360,1,0.877624,"m rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example, our extension adjusts offsets to a fixed baseline system. In this way, the system can adapt to multiple genres while preventing cross-genre contamination. In large-scale experiments, we adapt a multigenre baseline system to patents, lectures, and news articles. Our experiments show that sparse models, hierarchical updates, and rule adaptation all contribute consistent improvements. We observe quality gains in all genres, validating our hypothesis that document and genre context are important additional inputs to a machine translation"
D15-1123,2010.amta-papers.21,0,0.55906,"ection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and dense feature estimation.1 To translate the ith sentence fi , the system uses weights wi−1 and corpus ci−1 . The new corpus ci results from adding (fi , e∗i ) to ci−1"
D15-1123,P13-2121,0,0.0566368,"Missing"
D15-1123,N03-1017,0,0.00683929,". where f ∈ F is a string in the set of all source language strings F, e ∈ E is a string in the set of all target language strings E, r is a phrasal derivation with source and target projections src(r) and tgt(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropriate normalizing constant. During search, the maximum approximation is applied rather than summing over the derivations r. Model. We extend a phrase-based system for which φ(r; c) includes 16 dense features: • Two phrasal channel models and two lexical channel models (Koehn et al., 2003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Lian"
D15-1123,P07-2045,0,0.00955131,"t(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropriate normalizing constant. During search, the maximum approximation is applied rather than summing over the derivations r. Model. We extend a phrase-based system for which φ(r; c) includes 16 dense features: • Two phrasal channel models and two lexical channel models (Koehn et al., 2003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The"
D15-1123,W04-3250,0,0.0871522,"Missing"
D15-1123,N10-1062,0,0.064019,"ific corpus that includes all sentence pairs from the tuning corpus as well as from the adaptation corpus (fj , e∗j ) with j < i sharing fi ’s genre. We refer to this combined corpus as ci . The tuning corpus is the same that is used for parameter tuning in the baseline system. The adaptation corpus is our test set. Note that in our evaluation, each sentence is translated before it is used for adaptation, so that there is no contamination of results. In order to extend the model efficiently within a streaming data environment, we make use of a suffix-array implementation for our phrase table (Levenberg et al., 2010). Rather than combining corpus counts across these different sources, separate rules extracted from the baseline corpus and the genre-specific corpus exist independently in the derivation space, and features of each are computed only with one corpus. In this configuration, a large amount of outof-domain evidence from the baseline model will not dampen the feature value adaptation effects of adding new sentence pairs from the adaptation corpus. The genre-specific phrases are distinguished by an additional binary provenance feature. In order to extract features from the genrespecific corpus, a w"
D15-1123,P06-1096,0,0.0385464,"003), the (log) count of the rule in the training corpus c, and an indicator for singleton rules in c. • Six orientation models that score ordering configurations in r by their frequency in c (Koehn et al., 2007). • A linear distortion penalty that promotes monotonic translation. • An n-gram language model score, p(e), which scores the target language projection of r using statistics from a monolingual corpus. • Fixed-value phrase and word penalties. The elements of φ(r; c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also"
D15-1123,W13-2237,0,0.501967,"ns are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et al., 2013b), a technique based on AdaGrad (Duchi et al., 2011) and forward-backward splitting (Duchi and Singer, 2009) that can accurately set weights for both dense and sparse features (Green et al., 2014b). Rather than adjusting all weights based on each example,"
D15-1123,J03-1002,0,0.00999119,"t ideal for observing the effects of incremental adaptation techniques. To train the language and translation model we additionally leveraged all available bilingual and monolingual data provided for the EMNLP 2015 Tenth Workshop on Machine Translation3 . The total size of the bitext used for rule extraction and feature estimation was 6.4M sentence pairs. We trained a standard 5-gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) using the KenLM toolkit (Heafield et al., 2013) on 4 billion running words. The bitext was word-aligned with mgiza (Och and Ney, 2003), and we used the phrasal decoder (Green et al., 2014a) with standard GermanEnglish settings for experimentation. Our second set of experiments was performed on a mixed-genre corpus containing lectures, patents, and news articles. The standard dev and test sets of the IWSLT 2014 shared task4 were used for the lecture genre. Each document corresponded to an entire lecture. For the news genre, we used newstest2012 for tuning, newstest2013 for metaparameter optimization, and newstest2014 for testing. The tune set for the patent genre is identical to the first set of experiments, while the test se"
D15-1123,J04-4002,0,0.107292,"cross-genre contamination. In large-scale experiments, we adapt a multigenre baseline system to patents, lectures, and news articles. Our experiments show that sparse models, hierarchical updates, and rule adaptation all contribute consistent improvements. We observe quality gains in all genres, validating our hypothesis that document and genre context are important additional inputs to a machine translation system used for post-editing. 2 Background The log-linear appoach to statistical machine translation models the predictive translation distribution p(e|f ; w) directly in log-linear form (Och and Ney, 2004): h i X 1 p(e|f ; w) = exp w&gt; φ(r; c) (1) Z(f ) r: src(r)=f tgt(r)=e 1059 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1059–1065, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. where f ∈ F is a string in the set of all source language strings F, e ∈ E is a string in the set of all target language strings E, r is a phrasal derivation with source and target projections src(r) and tgt(r), w ∈ Rd is the vector of model parameters, φ(·) ∈ Rd is a feature map computed using corpus c, and Z(f ) is an appropria"
D15-1123,N10-1079,0,0.318148,"single fixed model for all different documents fails to incorporate contextual information that can potentially improve suggestion quality. We describe a model architecture that adapts simultaneously to multiple genres and individual documents, so that translation suggestions are informed by two levels of contextual information. Our primary technical contribution is a hierarchical adaptation technique for a post-editing scenario with incremental adaptation, in which users request translations of sentences in corpus order and provide corrected translations of each sentence back to the system (Ortiz-Martínez et al., 2010). Our learning approach resembles Hierarchical Bayesian Domain Adaptation (Finkel and Manning, 2009), but updates both the model weights and translation John DeNero Lilt Inc. john@lilt.com rules in real time based on these corrected translations (Mathur et al., 2013; Denkowski et al., 2014). Our adapted system can provide on-demand translations for any genre and document to which it has ever been exposed, using weights and rules for domains associated with each translation request. Our weight adaptation is performed using a hierarchical extension to fast and adaptive online training (Green et"
D15-1123,P13-1082,0,0.0188518,"rse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains:"
D15-1123,W13-2236,0,0.155725,"tive lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure"
D15-1123,P12-1002,0,0.0376166,"c) may also include sparse features that have non-zero values for only a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothes"
D15-1123,2011.mtsummit-papers.33,0,0.0284313,"a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root Domain Patents Genre News Genre Lectures Genre Each document has 3 domains: root, its genre, & the document itself Figure 1: The weights used to translate a document in the patent genre include three domains. model weights w and corpus c used for rule extraction and dense feature estimatio"
D15-1123,2012.amta-papers.18,0,0.0182524,"depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the machine. Observing e∗ can affect both the Root"
D15-1123,2013.mtsummit-papers.2,0,0.0257576,"ntion in the experiment cycle, simulated post-editing (Hardt and Elming, 2010; Denkowski et al., 2014) replaces each e∗ with a reference that is not a corrected variant of e. Thus, a standard test corpus can be used as an adaptation corpus. Prior work on online learning from post-edits has demonstrated the benefit of adjusting only c (Ortiz-Martínez et al., 2010; Hardt and Elming, 2010) and further benefit from adjusting both c and w (Mathur et al., 2013; Denkowski et al., 2014). Incremental adaptation of both c and the weights w for sparse features is reported to yield large quality gains by Wäschle et al. (2013).2 3 Hierarchical Incremental Adaptation Our hierachical approach to incremental adaptation uses document and genre information to adapt appropriately to multiple contexts. We assume that each sentence fi has a known set Di of domains, which identify the genre and individual document origin of the sentence. This set could be extended to include topics, individual translators, etc. Figure 1 shows the domains that we apply in experiments. All sentences in the baseline training corpus, the tuning corpus, and the adaptation corpus share a root domain. 1 For the purpose of our description, the corp"
D15-1123,2007.mtsummit-papers.68,0,0.0533315,"a subset of rules, but typically do not depend on c (Liang et al., 2006). In this paper, we use four types of sparse features: rule indicators, discriminative lexicalized reordering indicators, rule shape indicators and alignment features (Green et al., 2014b). The model parameters w are chosen to maximize translation quality on a tuning set. Adaptation. Domain adaptation for machine translation has improved quality using a variety of approaches, including data selection (Ceauşfu et al., 2011), regularized online learning (Simianer et al., 2012; Green et al., 2013b), and input classification (Xu et al., 2007; Banerjee et al., 2010; Wang et al., 2012) and has also been investigated for multidomain tasks (Sennrich et al., 2013; Cui et al., 2013; Simianer and Riezler, 2013). Even without domain labels at either training or test time, multi-task learning can boost translation quality in a batch setting (Duh et al., 2010; Song et al., 2011). Post-editing with incremental adaptation describes a particular mixed-initiative setting (OrtizMartínez et al., 2010; Hardt and Elming, 2010). For each f in a corpus, the machine generates a hypothesis e, then a human provides a corrected translation e∗ to the mac"
D15-1249,P12-1092,0,0.0507467,"011b) describe an architecture in which output word types are grouped into classes by frequency: the network first predicts a class, then a word in that class. Mikolov et al. (2013) describe an encoding of the output vocabulary as a binary tree. To our knowledge, hierarchical encodings have not been applied to the input vocabulary of a machine translation system. Other methods have also been developed to work around large-vocabulary issues in language modeling. Morin and Bengio (2005), Mnih and Hinton (2009), and Mikolov et al. (2011a) develop hierarchical versions of the softmax computation; Huang et al. (2012) and Collobert and Weston (2008) remove the need for normalization, thus avoiding computation of the summation term over the entire vocabulary. 2.3 Huffman Codes An encoding can be used to represent a sequence of tokens from a large vocabulary V using a small vocabulary W. In the case of translation, let V be the original corpus vocabulary, which can number in the millions of word types in a typical corpus. Let W be the vocabulary size of a neural translation model, typically set to a much smaller number such as 30,000. A deterministically invertible, variable-length encoding maps each v ∈ V t"
D15-1249,D13-1176,0,0.0973062,"2014). In this paper, we propose a novel approach to circumvent the large-vocabulary challenge by preprocessing the source and target word sequences, encoding them as a longer token sequence drawn from a small vocabulary that does not discard any information. Common words are unaffected, but rare words are encoded as a sequence of two pseudo-words. The exact same learning and inferBackground Neural Machine Translation Neural machine translation describes approaches to machine translation that learn from corpora in a single integrated model that embeds words and sentences into a vector space (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). We focus on one recent approach to neural machine translation, proposed by Bahdanau et al. (2014), that predicts both a translation and its alignment to the source sentence, though our technique is relevant to related approaches as well. The architecture consists of an encoder and a decoder. The encoder receives a source sentence x and encodes each prefix using a recurrent neural network that recursively combines embeddings xj for each word position j: → − → − h j = f (xj , h j−1 ) (1) where f is a non-linear function. Reverse encod← − ings h j are"
D15-1249,P02-1040,0,0.0949382,"in the training corpus tended to have encodings with the same first symbol. Similarly, the standard Huffman construction algorithm groups together rare words with similar frequencies within subtrees. More intelligent heuristics for constructing trees, such as using translation statistics instead of training corpus frequency, would be an interesting area of future work. 4.1 Results We used the RNNsearch-50 architecture from Bahdanau et al. (2014) as our machine translation system. We report results for this system alone, as well as for each of our three encoding schemes, using the BLEU metric (Papineni et al., 2002). Table 1 summarizes our results after training each variant for 5 days, corresponding to roughly 2 passes through the 180K-sentence training corpus. Alternative techniques that leverage bilingual resources have been shown to provide larger improvements. Jean et al. (2014) demonstrate an improvement of 3.1 BLEU by using bilingual word co-occurrence statistics in an aligned corpus to replace unknown word tokens. Luong et al. (2014) demonstrate an improvement of up to 2.8 BLEU over a series of stronger baselines using an unknown word model that also makes predictions using a bilingual dictionary"
D15-1249,D08-1027,0,\N,Missing
D15-1249,P15-1001,0,\N,Missing
D15-1249,D07-1090,0,\N,Missing
D15-1249,N10-1080,0,\N,Missing
D15-1249,P15-1002,0,\N,Missing
D15-1249,N09-2019,0,\N,Missing
D15-1249,D09-1030,0,\N,Missing
D15-1249,P05-1033,0,\N,Missing
D15-1249,W13-3512,0,\N,Missing
D15-1249,P16-1160,0,\N,Missing
D15-1249,P16-1162,0,\N,Missing
D15-1249,N15-1173,0,\N,Missing
D15-1249,D13-1140,0,\N,Missing
D15-1249,P03-1021,0,\N,Missing
D18-1104,2014.amta-researchers.13,0,0.036152,"Missing"
D18-1104,E14-1042,0,0.165274,"Missing"
D18-1104,W14-0311,0,0.0605659,"Missing"
D18-1104,W10-1757,0,0.0280409,"ova et al. (2017) apply vanilla fine-tuning algorithms. In addition to fine-tuning towards user corrections, the former applies a priori adaptation to retrieved data that is similar to the incoming source sentences. Peris et al. (2017) propose a variant of fine-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for pers"
D18-1104,D16-1139,0,0.0285815,"et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are applied to each sub-layer. A series of stacked decoder layers produces a sequence of target word vectors y1 , . . . , yn . Each decoder layer has three sub-layers: self-attention, encoder-attention, and a filter. For target position j, the self-attention layer can attend to any previous target position j 0 ∈ [1, j],"
D18-1104,2015.iwslt-evaluation.11,0,0.111581,"ers generate intermediate representations z1 , . . . , zm . Each layer of the encoder consists of two sub-layers: a multi-head selfattention layer that uses scaled dot-product attenCompact Adaptation 4.1 Fine Tuning Personalized machine translation requires batch adaptation to a domain-relevant bitext (such as a user provided translation memory) as well as incremental adaptation to the test set. We apply finetuning, which involves continuing to train model parameters with a gradient-based method on domainrelevant data, as a simple and effective method for neural translation domain adaptation (Luong and Manning, 2015). The fine-tuned model without regularization and clipping is denoted as the Full Model. Confirming previous work, we found that 882 1 https://www.tensorflow.org/ stochastic gradient descent (SGD) is the most effective optimizer for fine tuning (Turchi et al., 2017). In our experiments, batch adaptation uses a batch size of 7000 words for 10 Epochs and a fixed learning rate of 0.1, dropout of 0.1, and label smoothing with ls = 0.1 (Szegedy et al., 2016). Incremental adaptation uses a batch size of one and a learning rate of 0.01. To ensure a strong adaptation effect within a single document,"
D18-1104,P18-2050,0,0.0522982,"arning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are applied to each sub-layer. A series of stacked decoder layers produces a sequence of target word vectors y1 , . ."
D18-1104,P02-1040,0,0.103085,"ting 4 http://workshop2017.iwslt.org/ 3 and the dev2010 set (888 sentences) for testing. The overall best performing compact adaptation technique, group lasso regularization, is further evaluated on six other language pairs trained using production data sets collected from Lilt’s user base: English↔French, English↔Russian and English↔Chinese. Adaptation is performed on user data from various domains (technical manuals, finance, legal), each with 8k-10k segments for batch adaptation and 2000 segments for testing and incremental adaptation. Translation quality is evaluated using the cased Bleu (Papineni et al., 2002) measure. 5.2 Results Table 1 shows English→German results. Full model adaptation, where all offsets are stored, improves over the baseline in all cases to various degrees. This full model contains only 25.8M parameters, as offsets for both embedding matrices are stored as sparse collections of columns for the vocabulary present in the adaptation data. Next, we evaluate the impact of storing offsets only for one region at a time. We observe that among the three vocabulary matrices, the output projection Yo has the strongest impact on quality, which is not dimin884 Baseline Full Model Batch+Inc"
D18-1104,P16-1162,0,0.281616,"Missing"
D18-1104,W13-2236,1,0.845457,"essive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowl"
D18-1104,P12-1002,1,0.839561,"ne-tuning with passive-aggressive learning algorithms. In contrast to these papers, where all model parameters are possibly altered during training, this work focuses on space efficiency of the adapted models. Regularization methods that promote or enforce sparsity have been previously used in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush,"
D18-1104,P09-1054,0,0.0419851,"d in the context of sparse feature models for SMT: Duh et al. (2010) presented an application of multi-task learning via `1 /`2 regularization for feature selection in an N best reranking task. A similar approach, employing `1 /`2 regularization for feature selection and multi-task learning, was developed by Simianer et al. (2012) and Simianer and Riezler (2013) for tuning of SMT systems. Both works report improvements from regularization. Techniques for enforcing sparse models using `1 regularization during stochastic gradient descent optimization were previously developed for linear models (Tsuruoka et al., 2009). An extremely space efficient method for personalized model adaptation is presented by Michel and Neubig (2018). Here, adaptation is performed solely on the output vocabulary bias vector. Another notable approach for creating compact models is student-teacher-training or knowledge distillation (Kim and Rush, 2016). To the best of our knowledge, this has not been applied in a domain adaptation setting. tion over all source positions, followed by a feedforward filter layer. Layer normalization (Ba et al., 2016), dropout (Srivastava et al., 2014), and residual connections (He et al., 2016) are a"
D18-1104,D15-1123,1,0.865103,"Missing"
N07-1052,N04-1035,0,0.0372704,"onolingual outside score αs (X [i, j]) is the minimal costs for any completion of the edge. Hence, αs (X [i, j]) ≤ cs (os ) and αt (X [k, l]) ≤ ct (ot ). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A∗ expands substantially fewer states while searching for the optimal parse with our op7 The bilingual corpus consists of translation pairs with fixed English parses and word alignments. Rules were scored by their relative frequencies. 8 Rare words were replaced with their parts of speech to limit the memory consumption o"
N07-1052,N04-1014,0,0.0271972,"synchronous tree. o projects monolingual completions os and ot which have well-defined costs cs (os ) and ct (ot ) under Gs and Gt respectively. Their sum cs (os ) + ct (ot ) will underestimate αG (e) by pointwise admissibility. Furthermore, the heuristic we compute underestimates this sum. Recall that the monolingual outside score αs (X [i, j]) is the minimal costs for any completion of the edge. Hence, αs (X [i, j]) ≤ cs (os ) and αt (X [k, l]) ≤ ct (ot ). Admissibility follows. 3.2 Experiments We demonstrate our technique using the synchronous grammar formalism of tree-to-tree transducers (Knight and Graehl, 2004). In each weighted rule, an aligned pair of nonterminals generates two ordered lists of children. The non-terminals in each list must align one-to-one to the non-terminals in the other, while the terminals are placed freely on either side. Figure 3(a) shows an example rule. Following Galley et al. (2004), we learn a grammar by projecting English syntax onto a foreign language via word-level alignments, as in figure 3(b).7 We parsed 1200 English-Spanish sentences using a grammar learned from 40,000 sentence pairs of the English-Spanish Europarl corpus.8 Figure 4(a) shows that A∗ expands substan"
N07-1052,P04-1084,0,0.0128318,"ompletion costs, we more heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γa− , we can bound our search er− ror. Suppose γmax = maxa∈A γa− and that L∗ is the length of the longest optimal solution for the − , original problem. Then, h(s) ≤ h∗ (s) + L∗ γmax ∀s ∈ S. This -admissible heuristic (Ghallab and − .3 Allard, 1982) bounds our search error by L∗ γmax 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6 ) in the length of the sente"
N07-1052,P00-1056,0,0.0597597,"is slow or even intractable. One general technique to increase efficiency while preserving optimality is A∗ search (Hart et al., 1968); however, successfully using A∗ search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A∗ estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A∗ estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use"
N07-1052,J97-3002,0,0.107366,"re heavily penalize overestimating the cost by the constant C. 2.2 Bounding Search Error In the case where we allow pointwise inadmissibility, i.e. variables γa− , we can bound our search er− ror. Suppose γmax = maxa∈A γa− and that L∗ is the length of the longest optimal solution for the − , original problem. Then, h(s) ≤ h∗ (s) + L∗ γmax ∀s ∈ S. This -admissible heuristic (Ghallab and − .3 Allard, 1982) bounds our search error by L∗ γmax 3 Bitext Parsing In bitext parsing, one jointly infers a synchronous phrase structure tree over a sentence ws and its translation wt (Melamed et al., 2004; Wu, 1997). Bitext parsing is a natural candidate task for our approximate factoring technique. A synchronous tree projects monolingual phrase structure trees onto each sentence. However, the costs assigned by a weighted synchronous grammar (WSG) G do not typically factor into independent monolingual WCFGs. We can, however, produce a useful surrogate: a pair of monolingual WCFGs with structures projected by G and weights that, when combined, underestimate the costs of G. Parsing optimally relative to a synchronous grammar using a dynamic program requires time O(n6 ) in the length of the sentence (Wu, 19"
N07-1052,W06-1627,0,0.0770748,"tractable. One general technique to increase efficiency while preserving optimality is A∗ search (Hart et al., 1968); however, successfully using A∗ search is challenging in practice. The design of admissible (or nearly admissible) heuristics which are both effective (close to actual completion costs) and also efficient to compute is a difficult, open problem in most domains. As a result, most work on search has focused on non-optimal methods, such as beam search or pruning based on approximate models (Collins, 1999), though in certain cases admissible heuristics are known (Och and Ney, 2000; Zhang and Gildea, 2006). For example, Klein and Manning (2003) show a class of projection-based A∗ estimates, but their application is limited to models which have a very restrictive kind of score decomposition. In this work, we broaden their projectionbased technique to give A∗ estimates for models which do not factor in this restricted way. Like Klein and Manning (2003), we focus on search problems where there are multiple projections or “views” of the structure, for example lexical parsing, in which trees can be projected onto either their CFG backbone or their lexical attachments. We use general optimization tec"
N07-1052,J03-4003,0,\N,Missing
N09-1026,J98-2004,0,0.0963594,"Missing"
N09-1026,P05-1033,0,0.212091,"Missing"
N09-1026,D07-1079,0,0.0420296,"Missing"
N09-1026,N04-1035,0,0.470513,"Missing"
N09-1026,P06-1121,0,0.278456,"Missing"
N09-1026,P07-1019,0,0.0526295,"nts. These parameters were tuned on a development set. 233 Language Model Integration Large n-gram language models (LMs) are critical to the performance of machine translation systems. Recent innovations have managed the complexity of LM integration using multi-pass architectures. Zhang and Gildea (2008) describes a coarse-to-fine approach that iteratively increases the order of the LM. Petrov et al. (2008) describes an additional coarse-to-fine hierarchy over language projections. Both of these approaches integrate LMs via bottomup dynamic programs that employ beam search. As an alternative, Huang and Chiang (2007) describes a forest-based reranking algorithm called cube growing, which also employs beam search, but focuses computation only where necessary in a top-down pass through a parse forest. In this section, we show that the coarse-to-fine idea of constraining each pass using marginal predictions of the previous pass also applies effectively to cube growing. Max marginal predictions from the parse can substantially reduce LM integration time. 6.1 Language Model Forest Reranking Parsing produces a forest of derivations, where each edge in the forest holds its Viterbi (or one-best) derivation under"
N09-1026,P03-1054,1,0.0267897,"quires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly (Zhang et al., did not binarized slap the green witch 2006) orMary implicitly using Early-style intermediate symbols (Zollmann et al., 2006). Moreover, no daba una bofetada a la bruja verde theMaria resulting binary rules cannot be Markovized to merge symbols, as in Klein and Manning (2003), because each rule is associated with a target-side tree that cannot be abstracted. We also do not restrict the form of rules in the Right-branching 8,095 grammar, a common technique in syntactic machine Left-branching translation. For instance, Zollmann 5,871 et al. (2006) follow Chiang (2005) in disallowing adjacent nonGreedy 1,101 terminals. Optimal (ILP) Watanabe 443 et al. (2006) limit grammars to Griebach-Normal form. However, general tree 0 3,000 excellent 6,000 translation 9,000 transducer grammars provide performance (Galley et al., 2006), and so we focus on parsing with all availabl"
N09-1026,D07-1104,0,0.0305308,"mic program is organized into spans Sij and computes the Viterbi score w(i, j, X) for each edge Sij [X], the weight of the maximum parse over words i+1 to j, rooted at symbol X. For each Sij , computation proceeds in three phases: binary, lexical, and unary. 4.1 w(i, j, X) = max(wl (i, j, X), wb (i, j, X)). To efficiently compute mappings, we store lexical rules in a trie (or suffix array) – a searchable graph that indexes rules according to their sequence of lexical items and non-terminals. This data structure has been used similarly to index whole training sentences for efficient retrieval (Lopez, 2007). To find all rules that map onto a span, we traverse the trie using depth-first search. 4.3 max r=X→X1 X2 w(i, j, X) = j−1 ωr max w(i, k, X1 ) · w(k, j, X2 ). k=i+1 The quantities w(i, k, X1 ) and w(k, j, X2 ) will have already been computed by the dynamic program. The work in this phase is cubic in sentence length. Applying Lexical Rules On the other hand, lexical rules in LNF can be applied without binarization, because they only apply to particular spans that contain the appropriate lexical items. For a given Sij , we first compute all the legal mappings of each rule onto the span. A mappi"
N09-1026,W06-1606,0,0.0341851,"Missing"
N09-1026,N07-1051,1,0.838206,"Missing"
N09-1026,P06-1055,1,0.723838,"ogram of the size of rules applicable to a typical 30-word sentence appears in Figure 2. The grammar includes 149 grammatical symbols, an augmentation of the Penn Treebank symbol set. To evaluate, we decoded 300 sentences of up to 40 words in length from the NIST05 Arabic-English test set. 3 Efficient Grammar Encodings Monolingual parsing with a source-projected transducer grammar is a natural first pass in multi-pass decoding. These grammars are qualitatively different from syntactic analysis grammars, such as the lexicalized grammars of Charniak (1997) or the heavily state-split grammars of Petrov et al. (2006). 70,000 52,500 35,000 17,500 0 NNP1 did not slap DT2 green NN3 (a) InSthis ! section, we develop an appropriate grammar encoding that NNP daba unaefficient bofetadaparsing. a DT2 NN3 verde 1 noenables 7+ It is problematic to convert these grammars into which aCKY requires. (b) Chomsky S ! NNPnormal no dabaform, una bofetada DT NN verde Because transducer rules are very flat and contain speS cific lexical items, binarization introduces a large number of intermediate grammar symbols. Rule size affectDT parsing complexity whether (c) and lexicalization NNP NN the grammar is binarized explicitly"
N09-1026,D08-1012,1,0.9397,"be a coarse-to-fine pruning scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, an"
N09-1026,D08-1018,0,0.183564,"o ensure that the sequences X1 . . . Xk and Xk+1 . . . Xn can be constructed. As baselines, we used left-branching (where k = 1 always) and right-branching (where k = n − 1) binarizations. We also tested a greedy binarization approach, choosing k to minimize the number of grammar symbols introduced. We first try to select k such that both X1:k and Xk+1:n are already in the grammar. If no such k exists, we select k such that one of the intermediate types generated is already  used. If no such k exists again, we choose k = 21 n . This policy only creates new intermediate types when necessary. Song et al. (2008) propose a similar greedy approach to binarization that uses corpus statistics to select common types rather than explicitly reusing types that have already been introduced. Finally, we computed an optimal binarization that explicitly minimizes the number of symbols in the resulting grammar. We cast the minimization as an integer linear program (ILP). Let V be the set of all base non-terminal symbols in the grammar. We introduce an indicator variable TY for each symbol Y ∈ V + to indicate that Y is used in the grammar. Y can be either a base non-terminal symbol Xi or an intermediate symbol X1:"
N09-1026,N07-1063,0,0.0332326,"Missing"
N09-1026,P06-1098,0,0.0486143,"Missing"
N09-1026,J97-3002,0,0.0777932,"me. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands the minimal domain of translation rules, so rules are large and flat. Finally, we see most rules very few times,"
N09-1026,P08-1025,0,0.122277,"runing scheme for forest-based language model reranking that allows a 100-fold increase in beam size while reducing decoding time. The resulting translations improve by 1.3 BLEU. 1 Introduction Current approaches to syntactic machine translation typically include two statistical models: a syntactic transfer model and an n-gram language model. Recent innovations have greatly improved the efficiency of language model integration through multipass techniques, such as forest reranking (Huang and Chiang, 2007), local search (Venugopal et al., 2007), and coarse-to-fine pruning (Petrov et al., 2008; Zhang and Gildea, 2008). Meanwhile, translation grammars have grown in complexity from simple inversion transduction grammars (Wu, 1997) to general tree-to-string transducers (Galley et al., 227 Given that parsing is well-studied in the monolingual case, it is worth asking why MT grammars are not simply like those used for syntactic analysis. There are several good reasons. The most important is that MT grammars must do both analysis and generation. To generate, it is natural to memorize larger lexical chunks, and so rules are highly lexicalized. Second, syntax diverges between languages, and each divergence expands"
N09-1026,N06-1033,0,0.278249,"Missing"
N09-1026,W06-3119,0,0.0979856,"Missing"
N09-1026,J99-4004,0,\N,Missing
N10-1083,H05-1009,0,0.0250886,": D ICTIONARY: S TEM: P REFIX: C HARACTER: (e = ·, y = ·) (dist(y, e) = ·) ((y, e) ∈ D) for dictionary D. (stem(e) = ·, y = ·) for Porter stemmer. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, whil"
N10-1083,N09-1009,0,0.61572,"noulli STOP probabilities θd,c,STOP capture the valence of a particular head. For this type, the decision d is whether or not to stop generating arguments, and the context c contains the current head h, direction δ and adjacency adj. If a head’s stop probability is high, it will be encouraged to accept few arguments. The ATTACH multinomial probability distributions θd,c,ATTACH capture attachment preferences of heads. For this type, a decision d is an argument token a, and the context c consists of a head h and a direction δ. We take the same approach as previous work (Klein and Manning, 2004; Cohen and Smith, 2009) and use gold POS tags in place of words. 3 Haghighi and Klein (2006) achieve higher accuracies by making use of labeled prototypes. We do not use any external information. 586 5.2 Grammar Induction Features One way to inject knowledge into a dependency model is to encode the similarity between the various morphological variants of nouns and verbs. We encode this similarity by incorporating features into both the STOP and the ATTACH probabilities. The attachment features appear below; the stop feature templates are similar and are therefore omitted. (a = ·, h = ·, δ = ·) Generalize the morphol"
N10-1083,P07-1003,1,0.716969,"ages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the baseline and the feature-enhanced models, are computed by training the generative models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying"
N10-1083,A94-1009,0,0.149142,"h task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z,"
N10-1083,P07-1094,0,0.456947,"stically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P"
N10-1083,P06-1085,0,0.0182129,"ution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word segmentation models for 300 iterations with 10 random initializations and report the mean and standard deviation of F1 in Table 1. 7.3 Segmentation Features The SEGMENT multinomial is the important distribution in this model. We use the following features: BASIC: L ENGTH: N UMBER -VOWELS: P HONO -C LASS -P REF: P HONO -C LASS -P REF: (z = ·) (length(z) = ·) (numVowels(z) = ·) (prefix(coarsePhone"
N10-1083,P09-1104,1,0.664262,". For this type, there is no context and the decision is the particular string generated. In order to avoid the degenerate MLE that assigns mass only to single segment sentences it is helpful to independently generate a length for each segment from a fixed distribution. Liang and Klein (2009) constrain individual segments to have maximum length 10 and generate lengths from the following distribution: θl,LENGTH = exp(−l1.6 ) when 1 ≤ l ≤ 10. Their model is deficient since it is possible to generate 6 The best published results for this dataset are supervised, and trained on 17 times more data (Haghighi et al., 2009). lengths that are inconsistent with the actual lengths of the generated segments. The likelihood equation is given by: P (Y = y, Z = z) = θSTOP |z| Y   (1 − θSTOP ) θzi ,SEGMENT exp(−|zi |1.6 ) i=1 7.2 Segmentation Data and Evaluation We train and test on the phonetic version of the Bernstein-Ratner corpus (1987). This is the same set-up used by Liang and Klein (2009), Goldwater et al. (2006), and Johnson and Goldwater (2009). This corpus consists of 9790 child-directed utterances transcribed using a phonetic representation. We measure segment F1 score on the entire corpus. We run all word"
N10-1083,N09-1036,0,0.361345,"lish sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Basic-Model 1 EM Feature-Model 1 EM Basic-HMM EM Feature-HMM EM Word Segmentation Basic-Unigram EM Feature-Unigram EM LBFGS (Johnson and Goldwater, 2009) BR NIST ChEn 5.4 Grammar Induction Results We are able to outperform Cohen and Smith’s (2009) best system, which requires a more complicated variational inference method, on both English and Chinese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional in"
N10-1083,D07-1031,0,0.900802,"lates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions: exp hw, f (y, z, EMIT)i ′ y ′ exp hw, f (y , z, EMIT )i θy,z,EMIT (w) = P This feature-ba"
N10-1083,P04-1061,1,0.924351,"margin of 12.4. These results show that the direct gradient approach can offer additional boosts in performance when used with a feature-enhanced model. We also outperform the globally normalized MRF, which uses the same set of features and which we train using a direct gradient approach. To the best of our knowledge, our system achieves the best performance to date on the WSJ corpus for totally unsupervised POS tagging.3 5 Grammar Induction We next apply our technique to a grammar induction task: the unsupervised learning of dependency parse trees via the dependency model with valence (DMV) (Klein and Manning, 2004). A dependency parse is a directed tree over tokens in a sentence. Each edge of the tree specifies a directed dependency from a head token to a dependent, or argument token. Thus, the number of dependencies in a parse is exactly the number of tokens in the sentence, not counting the artificial root token. 5.1 Dependency Model with Valence The DMV defines a probability distribution over dependency parse trees. In this head-outward attachment model, a parse and the word tokens are derived together through a recursive generative process. For each token generated so far, starting with the root, a"
N10-1083,N09-1069,1,0.872403,"models in both directions, combining the result with hard union competitive thresholding (DeNero and Klein, 2007), and us588 For both IBM Model 1 and the HMM alignment model, EM training with feature-enhanced models outperforms the standard multinomial models, by 2.4 and 3.8 AER respectively.6 As expected, large positive weights are assigned to both the dictionary and edit distance features. Stem and character features also contribute to the performance gain. 7 Word Segmentation Finally, we show that it is possible to improve upon the simple and effective word segmentation model presented in Liang and Klein (2009) by adding phonological features. Unsupervised word segmentation is the task of identifying word boundaries in sentences where spaces have been removed. For a sequence of characters y = (y1 , ..., yn ), a segmentation is a sequence of segments z = (z1 , ..., z|z |) such that z is a partition of y and each zi is a contiguous subsequence of y. Unsupervised models for this task infer word boundaries from corpora of sentences of characters without ever seeing examples of well-formed words. 7.1 Unigram Double-Exponential Model Liang and Klein’s (2009) unigram doubleexponential model corresponds to"
N10-1083,N06-1014,1,0.244933,"each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselines for all four tasks. LBFGS outperformed EM in all cases where the algorithm was sufficiently fast to run. Details of each experiment appear in the main text. The distortion term p(zj = i|zj−1 ) is uniform in Model 1, and Markovian in the HMM. See Liang et al. (2006) for details on the specific variant of the distortion model of the HMM that we used. We use these standard distortion models in both the baseline and feature-enhanced word alignment systems. The bilexical emission model θy,e,ALIGN differentiates our feature-enhanced system from the baseline system. In the former, the emission model is a standard conditional multinomial that represents the probability that decision word y is generated from context word e, while in our system, the emission model is re-parameterized as a logistic regression model and feature-enhanced. Many supervised feature-bas"
N10-1083,J93-2004,0,0.0463736,"ate for words with certain orthographic properties. We use only the BASIC features for transitions. For an emission with word y and tag z, we use the following feature templates: (y = ·, z = ·) Check if y contains digit and conjoin with z: (containsDigit(y) = ·, z = ·) C ONTAINS -H YPHEN: (containsHyphen(x) = ·, z = ·) I NITIAL -C AP: Check if the first letter of y is capitalized: (isCap(y) = ·, z = ·) N-G RAM: Indicator functions for character ngrams of up to length 3 present in y. BASIC: C ONTAINS -D IGIT: 4.2 POS Induction Data and Evaluation We train and test on the entire WSJ tag corpus (Marcus et al., 1993). We attempt the most difficult version of this task where the only information our system can make use of is the unlabeled text itself. In particular, we do not make use of a tagging dictionary. We use 45 tag clusters, the number of POS tags that appear in the WSJ corpus. There is an identifiability issue when evaluating inferred tags. In order to measure accuracy on the hand-labeled corpus, we map each cluster to the tag that gives the highest accuracy, the many-1 evaluation approach (Johnson, 2007). We run all POS induction models for 1000 iterations, with 10 random initializations. The mea"
N10-1083,J94-2001,0,0.262389,"entation. In each task, we show that declaring a few linguistically motivated feature templates yields state-of-the-art results. 2 Models We start by explaining our feature-enhanced model for part-of-speech (POS) induction. This particular example illustrates our approach to adding features to unsupervised models in a well-known NLP task. We then explain how the technique applies more generally. 2.1 Example: Part-of-Speech Induction POS induction consists of labeling words in text with POS tags. A hidden Markov model (HMM) is a standard model for this task, used in both a frequentist setting (Merialdo, 1994; Elworthy, 1994) and in a Bayesian setting (Goldwater and Griffiths, 2007; Johnson, 2007). A POS HMM generates a sequence of words in order. In each generation step, an observed word emission yi and a hidden successor POS tag zi+1 are generated independently, conditioned on the current POS tag zi . This process continues until an absorbing stop state is generated by the transition model. There are two types of conditional distributions in the model—emission and transition probabilities— that are both multinomial probability distributions. The joint likelihood factors into these distributions:"
N10-1083,C96-2141,0,0.109148,"or our Chinese experiments, we use the same corpus and training/test split as Cohen and Smith 6 Word Alignment Word alignment is a core machine learning component of statistical machine translation systems, and one of the few NLP tasks that is dominantly solved using unsupervised techniques. The purpose of word alignment models is to induce a correspondence between the words of a sentence and the words of its translation. 6.1 Word Alignment Models We consider two classic generative alignment models that are both used heavily today, IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Ney and Vogel, 1996). These models generate a hidden alignment vector z and an observed foreign sentence y, all conditioned on an observed English sentence e. The likelihood of both models takes the form: Y p(zj = i|zj−1 ) · θyj ,ei ,ALIGN P (y, z|e) = j 4 Using additional bilingual data, Cohen and Smith (2009) achieve an accuracy of 62.0 for English, and an accuracy of 52.0 for Chinese, still below our results. 587 WSJ WSJ10 – 1.0 5.0 Grammar Induction Basic-DMV Feature-DMV Inference EM LBFGS EM LBFGS EM EM LBFGS (Cohen and Smith, 2009) Basic-DMV EM Feature-DMV EM LBFGS (Cohen and Smith, 2009) Word Alignment Bas"
N10-1083,P00-1056,0,0.0467888,"er. (prefix(e) = ·, y = ·) for prefixes of length 4. (e = ·, charAt(y, i) = ·) for index i in the Chinese word. These features correspond to several common augmentations of word alignment models, such as adding dictionary priors and truncating long words, but here we integrate them all coherently into a single model. 6.3 Word Alignment Data and Evaluation We evaluate on the standard hand-aligned portion of the NIST 2002 Chinese-English development set (Ayan et al., 2005). The set is annotated with sure S and possible P alignments. We measure alignment quality using alignment error rate (AER) (Och and Ney, 2000). We train the models on 10,000 sentences of FBIS Chinese-English newswire. This is not a large-scale experiment, but large enough to be relevant for lowresource languages. LBFGS experiments are not provided because computing expectations in these models is too computationally intensive to run for many iterations. Hence, EM training is a more appropriate optimization approach: computing the Mstep gradient requires only summing over word type pairs, while the marginal likelihood gradient needed for LBFGS requires summing over training sentence alignments. The final alignments, in both the basel"
N10-1083,P05-1044,0,0.794494,"in the form of conditional independence structure, which means that injecting it is both tricky (because the connection between independence and knowledge is subtle) and timeconsuming (because new structure often necessitates new inference algorithms). In this paper, we present a range of experiments wherein we improve existing unsupervised models by declaratively adding richer features. In particular, we parameterize the local multinomials of existThe idea of using features in unsupervised learning is neither new nor even controversial. Many top unsupervised results use feature-based models (Smith and Eisner, 2005; Haghighi and Klein, 2006). However, such approaches have presented their own barriers, from challenging normalization problems, to neighborhood design, to the need for complex optimization procedures. As a result, most work still focuses on the stable and intuitive approach of using the EM algorithm to optimize data likelihood in locally normalized, generative models. The primary contribution of this paper is to demonstrate the clear empirical success of a simple and accessible approach to unsupervised learning with features, which can be optimized by using standard NLP building blocks. We c"
N10-1083,J07-4003,0,0.0056588,"malized models. For models parameterized by standard multinomials, EM optimizes L(θ) = log Pθ (Y = y) (Dempster et al., 1977). The E-step computes expected counts for each tuple of decision d, context c, and multinomial type t: ed,c,t ← Eθ 2 &quot; X i∈I # (Xi = d, Xπ(i) = c, t) Y = y (2) The locally normalized model class is actually equivalent to its globally normalized counterpart when the former meets the following three conditions: (1) The graphical model is a directed tree. (2) The BASIC features are included in f . (3) We do not include regularization in the model (κ = 0). This follows from Smith and Johnson (2007). 584 ed,c,t d′ ed′ ,c,t θd,c,t ← P Normalizing expected counts in this way maximizes the expected complete log likelihood with respect to the current model parameters. EM can likewise optimize L(w) for our locally normalized models with logistic parameterizations. The E-step first precomputes multinomial parameters from w for each decision, context, and type: exphw, f (d, c, t)i ′ d′ exphw, f (d , c, t)i θd,c,t (w) ← P Then, expected counts e are computed according to Equation 2. In the case of POS induction, expected counts are computed with the forwardbackward algorithm in both the standard"
N10-1083,C02-1145,0,0.0134758,"inese data sets. Their system achieves an accuracy of 61.3 for English and an accuracy of 51.9 for Chinese.4 Our feature-enhanced model, trained using the direct gradient approach, achieves an accuracy of 63.0 for English, and an accuracy of 53.6 for Chinese. To our knowledge, our method for featurebased dependency parse induction outperforms all existing methods that make the same set of conditional independence assumptions as the DMV. Reg κ – 0.1 1.0 1.0 κ – 0.05 10.0 CTB10 Model POS Induction Basic-HMM Feature-MRF Feature-HMM (2009). We train on sections 1-270 of the Penn Chinese Treebank (Xue et al., 2002), similarly reduced (CTB10). We test on sections 271-300 of CTB10, and use sections 400-454 as a development set. The DMV is known to be sensitive to initialization. We use the deterministic harmonic initializer from Klein and Manning (2004). We ran each optimization procedure for 100 iterations. The results are reported in Table 1. κ – – – – κ – 0.2 0.2 Eval Many-1 63.1 (1.3) 59.6 (6.9) 68.1 (1.7) 75.5 (1.1) Dir 47.8 48.3 63.0 61.3 42.5 49.9 53.6 51.9 AER 38.0 35.6 33.8 30.0 F1 76.9 (0.1) 84.5 (0.5) 88.0 (0.1) 87 Table 1: Locally normalized feature-based models outperform all proposed baselin"
N10-1083,J93-2003,0,\N,Missing
N10-1083,N06-1041,1,\N,Missing
N10-1083,P01-1027,0,\N,Missing
N10-1083,P06-1111,1,\N,Missing
N10-1141,N09-2019,0,0.0608563,"Missing"
N10-1141,P09-1064,1,0.923825,"mprovements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a lin"
N10-1141,A94-1016,0,0.199907,"ted. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By r"
N10-1141,D09-1125,0,0.38116,"t does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consensus decoding on each of the component systems. In addition, it performs better than standard sentence-based or word-based system combination techniques applied to either max-derivation or MBR ou"
N10-1141,D08-1011,0,0.243258,"(or a lattice) of translations. This flexibility allows the technique to be applied quite broadly. For instance, de Gispert et al. (2009) describe combining systems based on multiple source representations using minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via w"
N10-1141,N04-1022,1,0.62066,"ystem combination to their output. We demonstrate BLEU improvements across data sets and language pairs in large-scale experiments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivatio"
N10-1141,P09-1019,1,0.566957,"forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical, and syntax-augmented"
N10-1141,D09-1005,0,0.0131418,"7: 8: for ` ∈ Leaves(r) do 9: b ← b × β(`) 10: for g ∈ Ngrams(n)  do  ¯b(g) ← ¯b(g) × β(`) − β(`, ˆ g) 11: 12: 13: 14: 15: 16: 17: 18: 19: β(n) ← β(n) + b for g ∈ Ngrams(n) do if g ∈ Ngrams(r) then ˆ g) ← β(n, ˆ g)+b β(n, else ˆ g) ← β(n, ˆ g)+b − ¯b(g) β(n, for g ∈ Ngrams(root) (all g in the HG) do ˆ ,g) P (g|f ) ← β(β(root root) This algorithm can in principle compute the posterior probability of any indicator function on local features of a derivation. More generally, this algorithm demonstrates how vector-backed inside passes can compute quantities beyond expectations of local features (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4 Indicator functions on derivations are not locally additive 978 3.2 Ensuring N -gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). I"
N10-1141,W08-0402,0,0.00573615,"ures (Li and Eisner, 2009).4 Chelba and Mahajan (2009) developed a similar algorithm for lattices. 4 Indicator functions on derivations are not locally additive 978 3.2 Ensuring N -gram Locality DeNero et al. (2009) describes an efficient algorithm for computing n-gram expected counts from a translation forest. This method assumes n-gram locality of the forest, the property that any n-gram introduced by a hyperedge appears in all derivations that include the hyperedge. However, decoders may recombine forest nodes whenever the language model does not distinguish between n-grams due to backoff (Li and Khudanpur, 2008). In this case, a forest encoding of a posterior distribution may not exhibit n-gram locality in all regions of the search space. Figure 3 shows a hypergraph which contains nonlocal trigrams, along with its local expansion. Algorithm 2 expands a forest to ensure n-gram locality while preserving the encoded distribution over derivations. Let a forest (N, R) consist of nodes N and hyperedges R, which correspond to rule applications. Let Rules(n) be the subset of R rooted by n, and Leaves(r) be the leaf nodes of rule application r. The expanded forest (Ne , Re ) is constructed by a function Reapp"
N10-1141,P09-1066,0,0.628281,"iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component."
N10-1141,P09-1067,0,0.177181,"iments. 1 Introduction Once statistical translation models are trained, a decoding approach determines what translations are finally selected. Two parallel lines of research have shown consistent improvements over the standard max-derivation decoding objective, which selects the highest probability derivation. Consensus decoding procedures select translations for a single system by optimizing for model predictions about n-grams, motivated either as minimizing Bayes risk (Kumar and Byrne, 2004), maximizing sentence similarity (DeNero et al., 2009), or approximating a max-translation objective (Li et al., 2009b). System combination procedures, on the other hand, generate translations from the output of multiple component systems (Frederking and Nirenburg, 1994). In this paper, we present model combination, a technique that unifies these two approaches by learning a consensus model over the n-gram features of multiple underlying component models. Model combination operates over the component models’ posterior distributions over translation derivations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component."
N10-1141,P09-1065,0,0.208006,"pio cedure could be applied to statistical systems that mar et al. (2009).5 only generate k-best lists. However, we would not ! 2 "" expect the same strong performance from model 4.1v2 =System Combination “saw the”: vpb = 0.9, 0.7 h combination in these constrained settings. System combination techniques in machine translation take as input the outputs {e1 , · · · , ek } of k dd Features for the Combination Model 4.2 Joint Decoding and Collaborative Decoding translation systems, where ei is a structured translaModel Training and Inference tion object (or k-best lists thereof), typically viewed Liu et al. (2009) describes two techniques for com!"" # $ as a sequence of words. The dominant approach in bining multiple synchronous grammars, which the arg max BLEU arg max sw (d) ; e the field chooses a primary translation ep as a back- Rauthors characterize as joint decoding. Joint dew d∈D(f ) Rpb Rh not involve a consensus or minimumbone, then finds an alignment ai to the backbone for coding does arg max sw (d) d∈D each ei . A new search space is constructed from Bayes-risk decoding objective; indeed, their best these backbone-aligned outputs, and then a voting results come from standard max-derivation de"
N10-1141,D07-1105,1,0.929139,"Missing"
N10-1141,J04-4002,1,0.241364,"Missing"
N10-1141,P02-1040,0,0.108937,"ing minimum Bayes risk decoding—likewise, they could be combined via model combination. Model combination has two significant advantages over current approaches to system combination. First, it does not rely on hypothesis alignment between outputs of individual systems. Aligning translation hypotheses accurately can be challenging, and has a substantial effect on combination performance (He et al., 2008). Instead of aligning hypotheses, we compute expectations of local features of n-grams. This is analogous to how BLEU score is computed, which also views sentences as vectors of n-gram counts (Papineni et al., 2002) . Second, we do not need to pick a backbone system for combination. Choosing a backbone system can also be challenging, and also affects system combination performance (He and Toutanova, 2009). Model combination sidesteps this issue by working with the conjoined forest produced by the union of the component forests, and allows the consensus model to express system preferences via weights on system indicator features. Despite its simplicity, model combination provides strong performance by leveraging existing consensus, search, and training techniques. The technique outperforms MBR and consens"
N10-1141,N07-1029,0,0.164137,"Missing"
N10-1141,D08-1065,1,0.887618,"vations, encoded as a forest of derivations.1 We combine these components by constructing a linear consensus model that includes features from each component. We then optimize this consensus model over the space of all translation derivations in the support of all component models’ posterior distributions. By reusing the components’ search spaces, we entirely avoid the hypothesis alignment problem that is central to standard system combination approaches (Rosti et al., 2007). Forest-based consensus decoding techniques differ in whether they capture model predictions through n-gram posteriors (Tromble et al., 2008; Kumar et al., 2009) or expected n-gram counts (DeNero et al., 2009; Li et al., 2009b). We evaluate both in controlled experiments, demonstrating their empirical similarity. We also describe algorithms for expanding translation forests to ensure that n-grams are local to a forest’s hyperedges, and for exactly computing n-gram posteriors efficiently. Model combination assumes only that each translation model can produce expectations of n-gram features; the latent derivation structures of component systems can differ arbitrarily. This flexibility allows us to combine phrase-based, hierarchical,"
N10-1141,N09-2052,0,0.247129,"Missing"
N10-1141,W06-3119,0,0.0204,"Missing"
N10-1141,J07-2003,0,\N,Missing
N12-1095,E09-1010,0,0.0205909,"xplored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference s"
N12-1095,P98-1012,0,0.0486869,"of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we briefly describe its relevance to the translation sense clustering ta"
N12-1095,J92-4003,0,0.0504551,"anslations of the Spanish source word colocar that appear in our input dataset. Clustering with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the featu"
N12-1095,J93-2003,0,0.0638348,"with usage examples. 1 Dekang Lin Google lindek@google.com Figure 1: This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vel´azquez de la Cadena et al., 1965). Introduction The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing. Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (Brown et al., 1993). However, word-to-word correspondences do not capture the full structure of a bilingual lexicon. Consider the example bilingual dictionary entry in Figure 1; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters. Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers. ∗ Author was a summer intern with Google Research while conducting this research project. This paper formalizes the task of clustering a set of translation"
N12-1095,P02-1033,0,0.0598201,"k To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised so"
N12-1095,N03-1015,0,0.0600517,"translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means c"
N12-1095,N03-1017,0,0.00542878,"700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Ro"
N12-1095,P09-1116,1,0.590238,"statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional con"
N12-1095,P03-1058,0,0.199405,"sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method"
N12-1095,D07-1043,0,0.115773,"3). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. Aft"
N12-1095,D11-1095,0,0.0160154,"ons of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and"
N12-1095,C04-1192,0,0.0406943,"task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively reco"
N12-1095,P08-1086,0,0.114418,"with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Li"
N12-1095,C10-1124,0,0.0119987,"of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs. The J→E dataset has 369 sourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001)"
N12-1095,W09-0210,0,0.0168583,"d to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The unde"
N12-1095,C96-2141,0,0.30935,"ourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). F"
N12-1095,C98-1012,0,\N,Missing
N13-1138,N07-1020,0,0.0212847,"Missing"
N13-1138,E12-1066,0,0.0743844,"Missing"
N13-1138,E12-1053,0,0.109772,"Missing"
N13-1138,D11-1057,0,0.654445,"nd attribute vector, we selected the first in linear order; applying the same principle, we also kept only the first inflection table when more than one was listed for a given base form. Furthermore, base forms and inflected forms separated by spaces, hyphens, or colons were discarded. As a result, we discarded German verbpreposition compounds such as ablehnen4 and Spanish reflexives such as lavarse. 6 Experiments We evaluate our model under two experimental conditions. First, we use the German verb lexicon in the CELEX lexical database (Baayen et al., 1995) with the same train/test splits as Dreyer and Eisner (2011). Second, we train on our Wiktionary data described in Section 5 and evaluate on held-out forms from this same dataset. In each case, we evaluate two variants of our model in order to examine the importance of jointly modeling the production of the entire inflection table. Our J OINT model is exactly as defined in Section 4. For our FACTORED model, the dictionary of rules is extracted separately for each setting of the attributes a; i.e., we run the entire procedure in Section 3 with only one inflected form at a time and forego the U NION S PANS step. A separate prediction model is trained for"
N13-1138,P02-1001,0,0.0111212,"rb denken aligning to its past participle gedacht, the initial d and g will be aligned and the following e’s will be aligned, preventing the algorithm from recognizing the addition of the prefix ge-. To solve this problem, we use a dynamic edit distance cost scheme in which I, D, and unmatched substitutions all have a cost of 0. Matched substitutions have a negative cost −ci , where i is the index in the base form and ci is the number of other inflected forms for which i is aligned to a matching character. The inflected forms are iteratively realigned with the base form until the ci converge (Eisner, 2002; Oncina and Sebban, 2006). This cost scheme encourages a single consistent analysis of the base form as it aligns to all of its inflected forms. Span Merging. From each aligned pair of words, the P ROJECT S PANS procedure identifies sequences of character edit operations with contiguous spans of the base form. We construct a set of changed spans Ca of b as follows: include the span (i, j) if and only if no characters between i and j were aligned to matching characters in Ta (b) and no smaller span captures the same set of changes. Projected spans for the inflected forms of schleichen are show"
N13-1138,J01-2001,0,0.357111,"se language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mini"
N13-1138,P08-1103,0,0.00742441,"odels to compute the gradient of pw , and at test time, the Viterbi algorithm can exactly find the best rule subset under the model: Aˆ = arg maxA pw (A|b). Features. The feature function φ captures contextual information in the base form surrounding the site of the anchored rule application. It is well understood that different morphological rules may require examining different amounts of context to apply correctly (Kohonen, 1986; Torkkola, 1993; Shalonova and Gol´enia, 2010); to this end, we will use local character n-gram features, which have been successfully applied to related problems (Jiampojamarn et al., 2008; Dinu et al., 2012). A sketch of our feature computation scheme is shown in Figure 2b. Our basic feature template is an indicator on a character n-gram with some offset from the rule application site, conjoined with the identity of the rule R being applied. Our features look at variable amounts of context: we include features on unigrams through 4-grams, starting up to five letters behind the anchored rule span and ending up to five letters past the anchored rule span. These features can model most hand-coded morphological rules, but are in many cases more numerous than necessary. However, we"
N13-1138,D12-1127,0,0.061659,"Missing"
N13-1138,W09-4615,0,0.0411817,"Missing"
N13-1138,N09-1024,0,0.0971816,"up, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mining lemma-base form pairs from a corpus (Schone and Jurafs"
N13-1138,N01-1024,0,0.0323965,"Missing"
N13-1138,C10-1110,0,0.0312875,"Missing"
N13-1138,P08-1084,0,0.183299,"they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2001; Forsberg et al., 2006; D´etrez and Ranta, 2012) for resources such as Grammatical Framework (Ranta, 2011). Related Work Much of the past work on morphology has focused on concatenative morphology using unsupervised methods (Goldsmith, 2001; Creutz and Lagus, 2007; Monson, 2008; Poon et al., 2009; Goldwater et al., 2009) or weak forms of supervision (Snyder and Barzilay, 2008). These methods can handle aspects of derivational morphology that we cannot, such as compounding, but we can handle a much larger subset of inflectional morphology, including more complex prefix and suffix rules, stem changes, and irregular forms. Some unsupervised work has specifically targeted these sorts of phenomena by, for example, learning spelling rules for mildly nonconcatenative cases (Dasgupta and Ng, 2007; Naradowsky and Goldwater, 2009) or mining lemma-base form pairs from a corpus (Schone and Jurafsky, 2001), but it is extremely difficult to make unsupervised methods perform as w"
N13-1138,P09-1055,0,0.060924,"ly beyond the capacity of a model based purely on orthography. Words ending in -e are commonly feminine, and none of our other training examples end in -we, so guessing that L¨owe follows a common feminine inflection pattern is reasonable (though L¨owe is, in fact, masculine). Disambiguating this case requires either features on observed genders, a more complex model of the German language, or observing the word in a large corpus. Generally, when the model fails, as in this case, it is because of a fundamental linguistic information source that it does not have access to. 7 2008) or decoding (Toutanova and Cherry, 2009) steps similar to those of our model, but none attempt to jointly predict a complete inflection table based on automatically extracted rules. Some previous work has addressed the joint analysis (Zajac, 2001; Monson, 2008) or prediction (Lind´en and Tuovila, 2009; Dinu et al., 2012) of whole inflection tables, as we do, but rarely are both aspects addressed simultaneously and most approaches are tuned to one particular language or use language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large am"
N13-1138,W04-0109,0,0.0681234,"Missing"
N13-1138,P00-1027,0,0.707675,"Missing"
N13-1138,W01-0711,0,0.0107563,"rn is reasonable (though L¨owe is, in fact, masculine). Disambiguating this case requires either features on observed genders, a more complex model of the German language, or observing the word in a large corpus. Generally, when the model fails, as in this case, it is because of a fundamental linguistic information source that it does not have access to. 7 2008) or decoding (Toutanova and Cherry, 2009) steps similar to those of our model, but none attempt to jointly predict a complete inflection table based on automatically extracted rules. Some previous work has addressed the joint analysis (Zajac, 2001; Monson, 2008) or prediction (Lind´en and Tuovila, 2009; Dinu et al., 2012) of whole inflection tables, as we do, but rarely are both aspects addressed simultaneously and most approaches are tuned to one particular language or use language-specific, curated resources. In overall setup, our work most closely resembles that of Dreyer and Eisner (2011), but they focus on incorporating large amounts of raw text data rather than using large training sets effectively. Broadly similar techniques are also employed in systems to filter candidate rules and aid in human annotation of paradigms (Zajac, 2"
N13-1138,zesch-etal-2008-extracting,0,0.0155438,"Missing"
P07-1003,H05-1011,0,0.0267717,"yntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments."
P07-1003,P06-1121,0,0.360131,"word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative met"
P07-1003,N06-1014,1,0.924763,"ppealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-"
P07-1003,W05-0812,0,0.228491,". Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger transducer rules. Thus, if the"
P07-1003,2004.tmi-1.5,0,0.0160442,"ntactic correspondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as"
P07-1003,C96-2141,0,0.880353,"uents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be incorporated into larger transducer rules. Thus, if the word alignment of a sentence pair does not respect the constituent structure of the target sentence, then the minimal translation units must span large tree fragments, which do not generalize well. We present and evaluate an unsupervised word alignment model similar in character and computation to the HMM model (Ney and Vogel, 1996), but which incorporates a novel, syntax-aware distortion component which conditions on target language parse trees. These trees, while automatically generated and therefore imperfect, are nonetheless (1) a useful source of structural bias and (2) the same trees which constrain future stages of processing anyway. In our model, the trees do not rule out any alignments, but rather softly influence the probability of transitioning between alignment positions. In particular, transition probabilities condition upon paths through the target parse tree, allowing the model to prefer distortions which"
P07-1003,J03-1002,0,0.0137066,"es. 5. Null alignments are treated just as in the HMM model, incurring a fixed cost from any position. This model can be simplified by removing all conditioning on node types. However, we found this variant to slightly underperform the full model described above. Intuitively, types carry information about cross-linguistic ordering preferences. 3.2 Training Approach Because our model largely mirrors the generative process and structure of the original HMM model, we apply a nearly identical training procedure to fit the parameters to the training data via the Expectation-Maximization algorithm. Och and Ney (2003) gives a detailed exposition of the technique. In the E-step, we employ the forward-backward algorithm and current parameters to find expected counts for each potential pair of links in each training pair. In this familiar dynamic programming approach, we must compute the distortion probabilities for each pair of English positions. The minimal path between two leaves in a tree can be computed efficiently by first finding the path from the root to each leaf, then comparing those paths to find the nearest common ancestor and a path through it – requiring time linear in the height of the tree. Co"
P07-1003,P06-2014,0,0.46099,"(2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and word alignments. Rules are extracted from target side constituents that can be projected onto contiguous spans of the source sentence via the word alignment. Constituents that project onto non-contiguous spans of the source sentence do not yield transducer rules themselves, and can only be inc"
P07-1003,H05-1010,1,0.42338,"rogress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host of discriminative methods have been introduced (Taskar et al., 2005; Moore, 2005; Ayan 17 Dan Klein Computer Science Division University of California, Berkeley klein@cs.berkeley.edu and Dorr, 2006). However, few of these methods have explicitly addressed the tension between word alignments and the syntactic processes that employ them (Cherry and Lin, 2006; Daum´e III and Marcu, 2005; Lopez and Resnik, 2005). We are particularly motivated by systems like the one described in Galley et al. (2006), which constructs translations using tree-to-string transducer rules. These rules are extracted from a bitext annotated with both English (target side) parses and wor"
P07-1003,P05-1033,0,0.0490835,"unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by the now much-maligned AER metric. A host"
P07-1003,J05-4004,0,0.0137143,"Missing"
P07-1003,J97-3002,0,0.115131,"ondences. We propose a novel model for unsupervised word alignment which explicitly takes into account target language constituent structure, while retaining the robustness and efficiency of the HMM alignment model. Our model’s predictions improve the yield of a tree transducer extraction system, without sacrificing alignment quality. We also discuss the impact of various posteriorbased methods of reconciling bidirectional alignments. 1 Introduction Syntactic methods are an increasingly promising approach to statistical machine translation, being both algorithmically appealing (Melamed, 2004; Wu, 1997) and empirically successful (Chiang, 2005; Galley et al., 2006). However, despite recent progress, almost all syntactic MT systems, indeed statistical MT systems in general, build upon crude legacy models of word alignment. This dependence runs deep; for example, Galley et al. (2006) requires word alignments to project trees from the target language to the source, while Chiang (2005) requires alignments to induce grammar rules. Word alignment models have not stood still in recent years. Unsupervised methods have seen substantial reductions in alignment error (Liang et al., 2006) as measured by"
P07-1003,J93-2003,0,\N,Missing
P07-1003,D07-1111,0,\N,Missing
P07-1003,P06-1002,0,\N,Missing
P08-2007,W07-0403,0,0.0660862,"finding an optimal alignment can be cast as an integer linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi infe"
P08-2007,W06-3105,1,0.812183,"hrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While ILP is of course also NP-ha"
P08-2007,N03-1017,0,0.024153,"ctive phrase alignments:     G G A= a: eij = e ; fkl = f   (eij ,fkl )∈a (eij ,fkl )∈a 1 As in parsing, the position between each word is assigned an index, where 0 is to the left of the first word. In this paper, we assume all phrases have length at least one: j &gt; i and l &gt; k. 25 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25–28, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Both the conditional model of DeNero et al. (2006) and the joint model of Marcu and Wong (2002) operate in A, as does the phrase-based decoding framework of Koehn et al. (2003). 2.3 Problem Definitions For a weighted sentence pair (e, f, φ), let the score of an alignment be the product of its link scores: Y φ(eij , fkl ). φ(a) = (eij ,fkl )∈a Four related problems involving scored alignments arise when training phrase alignment models. O PTIMIZATION, O: Given (e, f, φ), find the highest scoring alignment a. D ECISION, D: Given (e, f, φ), decide if there is an alignment a with φ(a) ≥ 1. O arises in the popular Viterbi approximation to EM (Hard EM) that assumes probability mass is concentrated at the mode of the posterior distribution over alignments. D is the corresp"
P08-2007,P07-2045,0,0.00901026,"Missing"
P08-2007,P06-1096,1,0.224556,"Missing"
P08-2007,W02-1018,0,0.459285,"lly quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using this formulation, exact solutions to the Viterbi search problem can be found by highly optimized, general purpose ILP solvers. While IL"
P08-2007,C04-1030,0,0.0539567,"teger linear program, which provides a simple, declarative approach to Viterbi inference for phrase alignment models that is empirically quite efficient. 1 2 Phrase Alignment Problems Rather than focus on a particular model, we describe four problems that arise in training phrase alignment models. 2.1 Introduction Learning in phrase alignment models generally requires computing either Viterbi phrase alignments or expectations of alignment links. For some restricted combinatorial spaces of alignments—those that arise in ITG-based phrase models (Cherry and Lin, 2007) or local distortion models (Zens et al., 2004)—inference can be accomplished using polynomial time dynamic programs. However, for more permissive models such as Marcu and Wong (2002) and DeNero et al. (2006), which operate over the full space of bijective phrase alignments (see below), no polynomial time algorithms for exact inference have been exhibited. Indeed, Marcu and Wong (2002) conjectures that none exist. In this paper, we show that Viterbi inference in this full space is NP-hard, while computing expectations is #P-hard. On the other hand, we give a compact formulation of Viterbi inference as an integer linear program (ILP). Using"
P09-1064,N04-1022,0,0.861309,"n statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny fraction of a model’s full posterior distribution. In this paper, we introduce a variant of the MBR decoding procedure that applies efficiently to translation forests. Instead of maximizing expected similarity, we express similarity in terms of features of sentences, and choose translations that are similar to expected feature values. 567 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567"
P09-1064,W07-0734,0,0.0159683,"s original intent. One could imagine different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the"
P09-1064,P09-1067,0,0.302909,"ttice or forest, whereas fast consensus decoding restricts this search to a k-best list. However, Tromble et al. (2008) showed that most of the improvement from lattice-based consensus decoding comes from lattice-based expectations, not search: searching over lattices instead of k-best lists did not change results for two language pairs, and improved a third language pair by 0.3 BLEU. Thus, we do not consider our use of k-best lists to be a substantial liability of our approach. Fast consensus decoding is also similar in character to the concurrently developed variational decoding approach of Li et al. (2009). Using BLEU, both approaches choose outputs that match expected n-gram counts from forests, though differ in the details. It is possible to define a similarity measure under which the two approaches are equivalent.5 Experimental Results We evaluate these consensus decoding techniques on two different full-scale state-of-the-art hierarchical machine translation systems. Both systems were trained for 2008 GALE evaluations, in which they outperformed a phrase-based system trained on identical data. 4.1 Hiero: a Hierarchical MT Pipeline Hiero is a hierarchical system that expresses its translatio"
P09-1064,D08-1024,1,0.377021,"r word-aligned sentence pairs provides rule frequency counts, which are normalized to estimate features on rules. The grammar rules of Hiero all share a single non-terminal symbol X, and have at most two non-terminals and six total items (non-terminals and lexical items), for example: my X2 ’s X1 → X1 de mi X2 We extracted the grammar from training data using standard parameters. Rules were allowed to span at most 15 words in the training data. The log-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: ("
P09-1064,W06-1606,1,0.591075,"yntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Co"
P09-1064,P08-1023,0,0.0827768,"r this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperfo"
P09-1064,P03-1021,0,0.0977732,"syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Consensus (Alg 3) 4m 52s Speed Ratio 128 Arabic-English Similarity Hiero 52.0 BLEU"
P09-1064,D07-1079,1,0.132411,"(2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (A"
P09-1064,P02-1040,0,0.12146,"lists. Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists. We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems. Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU. 1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny frac"
P09-1064,P07-2026,0,0.185314,"ind e∗ = arg maxe λ · θ(f, e). For MBR decoding, we instead leverage a similarity measure S(e; e0 ) to choose a translation using the model’s probability distribution P(e|f ), which has support over a set of possible translations E. The Viterbi derivation e∗ is the mode of this distribution. MBR is meant to choose a translation that will be similar, on expectation, to any possible reference translation. To this end, MBR chooses e˜ that maximizes expected similarity to the sentences in E under P(e|f ):1 We can sometimes exit the inner for loop early, whenever Ae can never become larger than A (Ehling et al., 2007). Even with this shortcut, the running time of Algorithm 1 is O(k 2 · n), where n is the maximum sentence length, assuming that S(e; e0 ) can be computed in O(n) time. 2.2 Minimum Bayes Risk over Features We now consider the case when S(e; e0 ) is a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be i"
P09-1064,D07-1014,0,0.0079072,"(e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feature expectations, and li"
P09-1064,N04-1035,1,0.103795,"g-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decod"
P09-1064,2006.amta-papers.25,0,0.0235326,"ne different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the bigrams produced by each rule"
P09-1064,P06-1121,1,0.537803,"BR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using"
P09-1064,W06-1666,0,0.0163604,"function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feat"
P09-1064,D08-1065,0,0.317373,"University of California, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distribution"
P09-1064,P96-1024,0,0.036875,"a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once"
P09-1064,N07-1063,0,0.0177076,"propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperforms using k-best lists co"
P09-1064,2006.amta-papers.8,1,0.218007,"tside algorithm, while the denominator is the inside score of the root node. Note that many possible derivations of f are pruned from the forest during decoding, and so this posterior is approximate. The expected n-gram count vector for a hyperedge is E[φ(h)] = P(h|f ) · φ(h). Hence, after computing P (h|f ) for every h, we need only sum P(h|f ) · φ(h) for all h to compute E[φ(e)]. This entire procedure is a linear-time computation in the number of hyper-edges in the forest. To complete forest-based fast consensus decoding, we then extract a k-best list of unique translations from the forest (Huang et al., 2006) and continue Algorithm 3 from line 5, which chooses the e˜ from the k-best list that maximizes BLEU(e; E[φ(e0 )]). 4 The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al. (2008) replace the clipped n-gram count with the product of an ngram count and an n-gram indicator function. 571 4 generated already by the decoder of a syntactic translation system. Second, rather than use BLEU as a sentencelevel similarity measure directly, Tromble et al. (2008) approximate corpus BLEU with G above. The parameters θ of the approximation must be estimated on"
P09-1064,P08-1025,0,0.390763,"ia, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets o"
P09-1064,P08-1067,0,0.0158513,"a, 2008; Tromble et al., 2008). In this section, we consider BLEU in particular, for which the relevant features φ(e) are n-gram counts up to length n = 4. We show how to compute expectations of these counts efficiently from translation forests. 3.1 Translation Forests Translation forests compactly encode an exponential number of output translations for an input sentence, along with their model scores. Forests arise naturally in chart-based decoding procedures for many hierarchical translation systems (Chiang, 2007). Exploiting forests has proven a fruitful avenue of research in both parsing (Huang, 2008) and machine translation (Mi et al., 2008). Formally, translation forests are weighted acyclic hyper-graphs. The nodes are states in the decoding process that include the span (i, j) of the sentence to be translated, the grammar symbol s over that span, and the left and right context words of the translation relevant for computing n-gram language model scores.3 Each hyper-edge h represents the application of a synchronous rule r that combines nodes corresponding to non-terminals in Computing Feature Expectations We now turn our focus to efficiently computing feature expectations, in service of"
P09-1064,W02-1019,0,0.0552059,"res. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this i"
P09-1064,D08-1022,0,\N,Missing
P09-1064,J07-2003,1,\N,Missing
P09-1104,P06-1002,0,0.100247,"d or d d, which are rendered as English indefinite determiners. The right-hand three columns in Table 2 present supervised results on our Chinese English data set using block features. We note that almost all of our performance gains (relative to both the HMM and 1-1 matchings) come from BITG and block features. The maximum likelihood-trained normal form ITG model outperforms the HMM, even without including any features derived from the unlabeled data. Once we include the posteriors of the HMM as a feature, the AER decreases to 14.4. The previous best AER result on this data set is 15.9 from Ayan and Dorr (2006), who trained stacked neural networks based on GIZA++ alignments. Our results are not directly comparable (they used more labeled data, but did not have the HMM posteriors as an input feature). 6.3 Rec 84 77 80 83 Translations Rules BLEU 1.9M 23.22 4.0M 23.05 3.8M 24.28 4.2M 24.32 Table 3: Results on the NIST MT05 Chinese-English test set show that our ITG alignments yield improvements in translation quality. thresholding (DeNero and Klein, 2007). The ITG Viterbi alignments are the Viterbi output of the ITG model with all features, trained to maximize log likelihood. The ITG Posterior alignmen"
P09-1104,P06-2014,0,0.677855,"97) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), which general matchings cannot efficiently do. The need for block alignments is es"
P09-1104,W07-0403,0,0.759433,"tences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–9"
P09-1104,D08-1024,0,0.00973797,"as follows, log Pw (a|x) = w·φ(x,a)−log a∈A max a∗ In our data sets, many are not in A1-1 (and thus not in AIT G ), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1 , the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a∗ , we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a∗p . Second, we perform lossˆ. This yields the augmented inference to obtain a modified QP, s.t. w · ˆ) + ≥ w · φ(x, a exp(w·φ(x,a0 )) where the log-denominator represents a sum over the alignment family A. This alignment probability only places mass on members of A. The likelihood objective is given by, (2) ˆ = arg max wt · φ(x, a) where a φ(x, a∗p ) X a0 ∈A ˆ) + L(ˆ s.t. w · φ(x, a∗ ) ≥ w · φ(x, a a, a∗ ) wt+1"
P09-1104,W02-1001,0,0.106614,"mmer et al., 2006). MIRA is an online procedure, where at each time step t + 1, we update our weights as follows: wt+1 = argminw ||w − wt ||22 4 An alternative to margin-based training is a likelihood objective, which learns a conditional alignment distribution Pw (a|x) parametrized as follows, log Pw (a|x) = w·φ(x,a)−log a∈A max a∗ In our data sets, many are not in A1-1 (and thus not in AIT G ), implying the minimum infamily loss must exceed 0. Since MIRA operates in an online fashion, this can cause severe stability problems. On the Hansards data, the simple averaging technique described by Collins (2002) yields a reasonable model. On the Chinese NIST data, however, where almost no alignment is in A1-1 , the update rule from Equation (2) is completely unstable, and even the averaged model does not yield high-quality results. We instead use a variant of MIRA similar to Chiang et al. (2008). First, rather than update towards the hand-labeled alignment a∗ , we update towards an alignment which achieves minimal loss within the family.4 We call this bestin-class alignment a∗p . Second, we perform lossˆ. This yields the augmented inference to obtain a modified QP, s.t. w · ˆ) + ≥ w · φ(x, a exp(w·φ("
P09-1104,P07-1003,1,0.743967,"s our best model without external alignment models and the second row includes features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N uses the normal form grammar (Section 4.1). 6.2 (2005); we compute external features from the same unlabeled data, 1.1 million sentence pairs. Our second is the Chinese-English hand-aligned portion of the 2002 NIST MT evaluation set. This dataset has 491 sentences, which we split into a training set of 150 and a test set of 191. When we trained external Chinese models, we used the same unlabeled data set as DeNero and Klein (2007), including the bilingual dictionary. For likelihood based models, we set the L2 regularization parameter, σ 2 , to 100 and the threshold for posterior decoding to 0.33. We report results using the simple ITG grammar (ITG-S, Section 2.2) where summing over derivations double counts alignments, as well as the normal form ITG grammar (ITG-N,Section 4.1) which does not double count. We ran our annealed lossaugmented MIRA for 15 iterations, beginning with λ at 0 and increasing it linearly to 0.5. We compute Viterbi alignments using the averaged weight vector from this procedure. 6.1 Chinese NIST R"
P09-1104,P08-2007,1,0.0414573,"ng the same heuristic Dice potentials on the Hansards test set, the maximal scoring alignment from AIT G yields 28.4 AER—2.4 better than A1-1 —indicating that ITG can be beneficial as a constraint on heuristic alignments. 2.3 Block ITG An important alignment pattern disallowed by A1-1 is the many-to-one alignment block. While not prevalent in our hand-aligned French Hansards dataset, blocks occur frequently in our handaligned Chinese-English NIST data. Figure 1 contains an example. Extending A1-1 to include blocks is problematic, because finding a maximal 1-1 matching over phrases is NP-hard (DeNero and Klein, 2008). With ITG, it is relatively easy to allow contiguous many-to-one alignment blocks without added complexity.3 This is accomplished by adding additional unary terminal productions aligning a foreign phrase to a single English terminal or vice versa. We will use BITG to refer to this block ITG variant and ABIT G to refer to the alignment family, which is neither contained in nor contains A1-1 . For this alignment family, we expand the alignment potential decomposition in Equation (1) to incorporate block potentials sef and sef which represent English and foreign many-to-one alignment blocks, res"
P09-1104,N09-1026,1,0.810211,"ate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly attempting to build each cell in the dynamic program. Instead, we track bounds on the spans for which we have successfully built ITG cells, and we only iterate over larger spans that fall within those bounds. The details of a similar bounding approach appear in DeNero et al. (2009). In all, pruning reduces MIRA iteration time from 175 to 5 minutes on the NIST ChineseEnglish dataset with negligible performance loss. Likelihood training time is reduced by nearly two orders of magnitude. Se ne est pas suﬃsant d gh ou en o go t no is at Th Figure 3: Often, the gold alignment a∗ isn’t in our alignment family, here ABIT G . For the likelihood objective (Section 4.2), we maximize the probability of the set M(a∗ ) consisting of alignments ABIT G which achieve minimal loss relative to a∗ . In this example, the minimal loss is 1, and we have a choice of removing either of the sur"
P09-1104,P08-1112,0,0.0283754,"ord side of a block is. The final block feature type consists of phrase shape features. These are designed as follows: For each word in a potential many-to-one block alignment, we map an individual word to X if it is not one of the 25 most frequent words. Some example features of this type are, French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model significantly improves over the HMM alone, which is consistent with the results of Taskar et al. (2005). 929 Alignments Model Prec GIZA++ 62 Join"
P09-1104,N06-1015,1,0.455258,"l as pointwise mutual information statistics for the multi-word parts of many-to-one blocks. These features capture roughly how “coherent” the multi-word side of a block is. The final block feature type consists of phrase shape features. These are designed as follows: For each word in a potential many-to-one block alignment, we map an individual word to X if it is not one of the 25 most frequent words. Some example features of this type are, French Hansards Results The French Hansards data are well-studied data sets for discriminative word alignment (Taskar et al., 2005; Cherry and Lin, 2006; Lacoste-Julien et al., 2006). For this data set, it is not clear that improving alignment error rate beyond that of GIZA++ is useful for translation (Ganchev et al., 2008). Table 1 illustrates results for the Hansards data set. The first row uses dice and the same distance features as Taskar et al. (2005). The first two rows repeat the experiments of Taskar et al. (2005) and Cherry and Lin (2006), but adding ITG models that are trained to maximize conditional likelihood. The last row includes the posterior of the jointly-trained HMM of Liang et al. (2006) as a feature. This model alone achieves an AER of 5.4. No model si"
P09-1104,W08-0402,0,0.0139488,"aligners under simple and normal form grammars. We showed that through the combination of relaxed learning objectives, many-to-one block alignment potential, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four s"
P09-1104,N06-1014,1,0.70755,"grammar (Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar. ∗ Target Alignments M(a ) Gold Alignment a∗ lihood uses the inside-outside algorithm for computing cell posteriors. Exhaustive computation of these quantities requires an O(n6 ) dynamic program that is prohibitively slow even on small supervised training sets. However, most of the search space can safely be pruned using posterior predictions from a simpler alignment models. We use posteriors from two jointly estimated HMM models to make pruning decisions during ITG inference (Liang et al., 2006). Our first pruning technique is broadly similar to Cherry and Lin (2007a). We select high-precision alignment links from the HMM models: those word pairs that have a posterior greater than 0.9 in either model. Then, we prune all bitext cells that would invalidate more than 8 of these high-precision alignments. Our second pruning technique is to prune all one-by-one (word-to-word) bitext cells that have a posterior below 10−4 in both HMM models. Pruning a one-by-one cell also indirectly prunes larger cells containing it. To take maximal advantage of this indirect pruning, we avoid explicitly a"
P09-1104,D07-1104,0,0.0187893,"al, and efficient pruning, ITG models can yield state-of-the art word alignments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The"
P09-1104,W03-0301,0,0.0226162,"d-based objective is that we can obtain posteriors over individual alignment cells, Pw ((i, j)|x) = X Pw (a|x) a∈A:(i,j)∈a We obtain posterior ITG alignments by including all alignment cells (i, j) such that Pw ((i, j)|x) exceeds a fixed threshold t. Posterior thresholding allows us to easily trade-off precision and recall in our alignments by raising or lowering t. 5 6 Dynamic Program Pruning Alignment Quality Experiments We present results which measure the quality of our models on two hand-aligned data sets. Our first is the English-French Hansards data set from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). Here we use the same 337/100 train/test split of the labeled data as Taskar et al. Both discriminative methods require repeated model inference: MIRA depends upon lossaugmented Viterbi parsing, while conditional like6 Note that alignments that achieve the minimal loss would not introduce any alignments not either sure or possible, so it suffices to keep track only of the number of sure recall errors. 928 Features Dice, dist, blcks, dict, lex +HMM P 1-1 R AER P 85.7 90.5 63.7 69.4 26.8 21.2 86.2 91.2 MIRA ITG R AER 65.8 70.1 25.2 20.3 BITG R AER P 85.0 90.2 73.3 80.1 21.1 15.0 P 85.7 87.3 Lik"
P09-1104,P06-1065,0,0.452034,"Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), which general matchings"
P09-1104,P03-1021,0,0.0204384,"ments, even when the underlying gold alignments are highly nonITG. Our models yielded the lowest published error for Chinese-English alignment and an increase in downstream translation performance. End-To-End MT Experiments We further evaluated our alignments in an end-toend Chinese to English translation task using the publicly available hierarchical pipeline JosHUa (Li and Khudanpur, 2008). The pipeline extracts a Hiero-style synchronous context-free grammar (Chiang, 2007), employs suffix-array based rule extraction (Lopez, 2007), and tunes model parameters with minimum error rate training (Och, 2003). We trained on the FBIS corpus using sentences up to length 40, which includes 2.7 million English words. We used a 5-gram language model trained on 126 million words of the Xinhua section of the English Gigaword corpus, estimated with SRILM (Stolcke, 2002). We tuned on 300 sentences of the NIST MT04 test set. Results on the NIST MT05 test set appear in Table 3. We compared four sets of alignments. The GIZA++ alignments7 are combined across directions with the grow-diag-final heuristic, which outperformed the union. The joint HMM alignments are generated from competitive posterior References"
P09-1104,D08-1012,1,0.831177,"ne block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matchi"
P09-1104,H05-1010,1,0.548785,"ments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP in O(n3 ) time using a bipartite matching algorithm (Kuhn, 1955).1 On the other hand, summing over A1-1 is #P -hard (Valiant, 1979). Initially, we consider heuristic alignment potentials given by Dice coefficients structured alignments (i.e. phrases), wh"
P09-1104,J97-3002,0,0.871532,"lihood models. A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockThis work investigates supervised word alignment m"
P09-1104,P03-1019,0,0.717345,"A major challenge in both cases is that our training alignments are often not one-to-one ITG alignments. Under such conditions, directly training to maximize margin is unstable, and training to maximize likelihood is ill-defined, since the target alignment derivations don’t exist in our hypothesis class. We show how to adapt both margin and likelihood objectives to learn good ITG aligners. In the case of likelihood training, two innovations are presented. The simple, two-rule ITG grammar exponentially over-counts certain alignment structures relative to others. Because of this, Wu (1997) and Zens and Ney (2003) introduced a normal form ITG which avoids this over-counting. We extend this normal form to null productions and give the first extensive empirical comparison of simple and normal form ITGs, for posterior decoding under our likelihood models. Additionally, we show how to deal with training instances where the gold alignments are outside of the hypothesis class by instead optimizing the likelihood of a set of minimum-loss alignments. Perhaps the greatest advantage of ITG models is that they straightforwardly permit blockThis work investigates supervised word alignment methods that exploit inve"
P09-1104,P05-1059,0,0.319588,"er allow us to align sentences two orders of magnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of"
P09-1104,P08-1012,0,0.381786,"gnitude faster than naive bitext CKY parsing. Finally, we introduce many-to-one block alignment features, which significantly improve our ITG models. Altogether, our method results in the best reported AER numbers for Chinese-English and a performance improvement of 1.1 BLEU over GIZA++ alignments. 1 Introduction Inversion transduction grammar (ITG) constraints (Wu, 1997) provide coherent structural constraints on the relationship between a sentence and its translation. ITG has been extensively explored in unsupervised statistical word alignment (Zhang and Gildea, 2005; Cherry and Lin, 2007a; Zhang et al., 2008) and machine translation decoding (Cherry and Lin, 2007b; Petrov et al., 2008). In this work, we investigate large-scale, discriminative ITG word alignment. Past work on discriminative word alignment has focused on the family of at-most-one-to-one matchings (Melamed, 2000; Taskar et al., 2005; Moore et al., 2006). An exception to this is the work of Cherry and Lin (2006), who discriminatively trained one-to-one ITG models, albeit with limited feature sets. As they found, ITG 923 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923–931, c Suntec, Singapor"
P09-1104,J00-2004,0,\N,Missing
P09-1104,J07-2003,0,\N,Missing
P09-2036,N09-1026,1,0.882842,"inarization, we transform the translation forest into n-ary form. In the n-ary forest, each hyperedge corresponds to an original ! correspond to original grammar rule, and all nodes grammar symbols, rather than those introduced during binarizaiton. Transforming the entire forest to n-ary form is intractable, however, because the number of hyperedges would be exponential in n. Instead, we include only the top k n-ary backtraces for each forest node. These backtraces can be enumerated efficiently from the binary forest. Figure 2(b) illustrates the result. For efficiency, we follow DeNero et al. (2009) in pruning low-scoring nodes in the n-ary forest under the weighted translation grammar. We use a max-marginal threshold to prune unlikely nodes, which can be computed through a maxsum semiring variant of inside-outside (Goodman, 1996; Petrov and Klein, 2007). Forest reranking with a language model can be performed over this n-ary forest using the cube growing algorithm of Huang and Chiang (2007). Cube growing lazily builds k-best lists of derivations at each node in the forest by filling a nodespecific priority queue upon request from the parent. N -ary forest reranking serves as our baselin"
P09-2036,N06-1033,0,0.123236,"al. (2009) describe normal forms particularly suited to transducer grammars, demonstrating that well-chosen binarizations admit cubic-time parsing algorithms while introducing very few intermediate grammar symbols. Binarization choice can also improve monolingual parsing efficiency (Song et al., 2008). The parsing stage of our decoder proceeds by first converting the source-side projection of the translation grammar into lexical normal form (DeNero et al., 2009), which allows each rule to be applied to any span in linear time, then build2.3 Reranking with Target-Side Binarization Zhang et al. (2006) demonstrate that reranking over binarized derivations improves search accuracy by better exploring the space of translations within the strict confines of beam search. Binarizing the forest during reranking permits pairs of adjacent non-terminals in the target-side projection of rules to be rescored at intermediate forest nodes. This target-side binarization can be performed onthe-fly: when a node Pij is queried for its k-best list, we binarize its n-ary backtraces. Suppose Pij can be constructed from a rule r with target-side projection P → `0 C1 `1 C2 `2 . . . Cn `n where C1 , . . . , Cn ar"
P09-2036,N07-1051,1,\N,Missing
P09-2036,D08-1018,0,\N,Missing
P09-2036,J09-4009,0,\N,Missing
P09-2036,W07-0405,0,\N,Missing
P09-2036,J97-3002,0,\N,Missing
P09-2036,P07-1019,0,\N,Missing
P09-2036,P06-1121,0,\N,Missing
P09-2036,P96-1024,0,\N,Missing
P10-1147,P06-1002,0,0.192022,"ze is a closed form function of the loss and feature vectors: τ =   L(Am ; Ag ) − θ · (φ(Ag ) − φ(Am )) min C, ||φ(Ag ) − φ(Am )||22 We train the model for 30 iterations over the training set, shuffling the order each time, and we average the weight vectors observed after each iteration to estimate our final model. 3.1 Extraction Set Loss Function In order to focus learning on predicting the right bispans, we use an extraction-level loss L(Am ; Ag ): an F-measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule"
P10-1147,H05-1009,0,0.0856986,"Missing"
P10-1147,W06-3123,0,0.0684454,"l errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed"
P10-1147,P09-1088,0,0.111641,"t j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et"
P10-1147,J98-2004,0,0.021304,"small to large, where we 后spe- [after] appears in each edge column or row, but the define the size of a bispan as the total number of cific location of edge links is not required. words contained within it. For each size, we main[I] tain a separate agenda. Only when the agenda for 我 l =4 size k is exhausted does the parser proceed to proIn order to gcompute I(AhL=3 , AR ), we need 睡 cer- [sleep] =1 cess the agenda for size k + 1. tain information about the alignment configuraWe also employ coarse-to-fine search to speed tions of AL and AR where they adjoin at a corner. 了 (past) up inference (Charniak and Caraballo, 1998). In The state must represent (a) the specific alignment the coarse pass, we search over the space of ITG links in the nAfter − 1 deep cornerI of each dinner sleptA, and (b) alignments, but score only features on alignment whether any sure alignments appear in the rows or links and bispans that are local to terminal blocks. columns extending from those corners.6 With this This simplification eliminates the need to augment information, we can infer the bispans licensed by grammar symbols, and so we can exhaustively exadjoining AL and AR , as in Figure 6. plore the (pruned) space. We then comput"
P10-1147,P06-2014,0,0.0201374,"00; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation model estimation, while we focus on alignment and translate using standard relative frequency estimators. Deng and Zhou (2009) present an alignment combination technique that uses phrasal features. O"
P10-1147,W07-0403,0,0.706688,"ut loss of information, although we emphasize that A is a set of sure and possible alignments, and φ(A) does not decompose as a sum of vectors on individual word-level alignment links. Our model is parameterized by a weight vector θ, which scores an extraction set Rn (A) as θ · φ(A). To further limit the space of extraction sets we are willing to consider, we restrict A to block inverse transduction grammar (ITG) alignments, a space that allows many-to-many alignments through phrasal terminal productions, but otherwise enforces at-most-one-to-one phrase matchings with ITG reordering patterns (Cherry and Lin, 2007; Zhang et al., 2008). The ITG constraint 1455 On February n] 饭 15 2010 被 [passive marker] 发现 [discover] was discovered model class. Hence, we update toward the extraction set for a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence an"
P10-1147,D08-1024,0,0.00554395,"summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer and Singer (2003)—a large-margin, perceptron-style, online learning algorithm. MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al., 2009) and translation models (Chiang et al., 2008). For each training example, MIRA requires that we find the alignment Am corresponding to the highest scoring extraction set Rn (Am ) under the current model, Am = arg maxA∈ITG(e,f) θ · φ(A) (2) Section 4 describes our approach to solving this search problem for model inference. MIRA updates away from Rn (Am ) and toward a gold extraction set Rn (Ag ). Some handannotated alignments are outside of the block ITG Inference details appear in Section 4.3. σ(f 2) Given Ag and Am , we update the model parameters away from Am and toward Ag . θ ← θ + τ · (φ(Ag ) − φ(Am )) where τ is the minimal size th"
P10-1147,P07-1003,1,0.949486,"rs included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-ofthe-art unsupervised baseline. Supervised Baseline: Block ITG. We discriminatively trained a block ITG aligner with only sure links, using block terminal productions up to 3 words by 3 words in size. This supervised baseline is a reimplementation of the MIRA-trained model of Haghighi et al. (2009). We use the same features and parser implementation for this model as we do for our extraction set model to ensure a clean comparison. To remain within the alignment class, MIRA updates this model toward a pseudogold alignment with only sure links. This model does not score any o"
P10-1147,D08-1033,1,0.665858,"ch that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2"
P10-1147,P09-2058,0,0.0625,"models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation model estimation, while we focus on alignment and translate using standard relative frequency estimators. Deng and Zhou (2009) present an alignment combination technique that uses phrasal features. Our approach differs in two ways. First, their approach is tightly coupled to the input alignments, while we perform a full search over the space of ITG alignments. Also, their approach uses greedy search, while our search is optimal aside from pruning and beaming. Despite these differences, their strong results reinforce our claim that phraselevel information is useful for alignment. 6 Experiments We evaluate our extraction set model by the bispans it predicts, the word alignments it generates, and the translations genera"
P10-1147,P06-1097,0,0.183362,"extraction-level loss L(Am ; Ag ): an F-measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule extraction consequences. That is, adding a word link that prevents the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features co"
P10-1147,D07-1006,0,0.0207186,"hey stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) tra"
P10-1147,P06-1121,0,0.0354551,"odel is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length (Koehn et al., 2003), number of gaps (Chiang, 2007), count of internal tree nodes (Galley et al., 2006), etc. In this paper, we focus on phrasal rule extraction (i.e., phrase pair extraction), upon which most other extraction procedures are based. Given a sentence pair (e, f), phrasal rule extraction defines a mapping from a set of word-to-word and likewise each word fj projects to a span of e. Then, Rn (A) includes a bispan [g, h) ⇔ [k, `) iff σ(ei ) ⊆ [k, `) ∀i ∈ [g, h) σ(fj ) ⊆ [g, h) ∀j ∈ [k, `) That is, every word in one of the phrasal spans must project within the other. This mapping is deterministic, and so we can interpret a word-level alignment A as also specifying the phrasal rules th"
P10-1147,P96-1024,0,0.0148137,"rI of each dinner sleptA, and (b) alignments, but score only features on alignment whether any sure alignments appear in the rows or links and bispans that are local to terminal blocks. columns extending from those corners.6 With this This simplification eliminates the need to augment information, we can infer the bispans licensed by grammar symbols, and so we can exhaustively exadjoining AL and AR , as in Figure 6. plore the (pruned) space. We then compute outApplying our score recurrence yields a side scores for bispans under a max-sum semirpolynomial-time dynamic program. This dynamic ing (Goodman, 1996). In the fine pass with the program is an instance of ITG bitext parsing, full extraction set model, we impose a maximum where the grammar uses symbols to encode size of 10,000 for each agenda. We order states on the alignment contexts described above. This agendas by the sum of their inside score under the context-as-symbol augmentation of the grammar full model and the outside score computed in the is similar in character to augmenting symbols with coarse pass, pruning all states not within the fixed lexical items to score language models during agenda beam size. hierarchical decoding (Chian"
P10-1147,P09-1104,1,0.816047,"e show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture of sure (filled) and possible (striped) word-level alignment links. [I] is more computationally convenient than arbitrarily ordered phrase matchings (Wu, 1997; DeNero and Klein, 2008). However, the space of block [sleep] ITG alignments is expressive enough to include [past tense] the vast majority of patterns observed in handannotated parallel corpora (Haghighi et al., 2009). In summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer and Singer (2003)—a large-margin, perceptron-style, online learning algorithm. MIRA has been used successfully in MT to estimate both alignment models (Haghighi et al., 2009) and translation mod"
P10-1147,P07-1019,0,0.00436211,"res O(k 6 ) ner and size combination, built states are maintime in sentence length k, and is prohibitively slow tained in sorted order according to their inside when there is no sparsity in the grammar. Mainscore. This ordering allows us to stop combintaining the context necessary to score non-local ing states early when the results are falling off the bispans further increases running time. That is, agenda beams. Similar search and beaming strateITG inference is organized around search states gies appear in many decoders for machine transassociated with a grammar symbol and a bispan; lation (Huang and Chiang, 2007; Koehn and Hadaugmenting grammar symbols also augments this dow, 2009; Moore and Quirk, 2007). state space. To parse quickly, we prune away search states 4.3 Finding Pseudo-Gold ITG Alignments using predictions from the more efficient HMM Equation 3 asks for the block ITG alignment 6 The number of configuration states does not depend on Ag that is closest to a reference alignment At , the size of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 d"
P10-1147,D09-1107,0,0.177678,"Missing"
P10-1147,N03-1016,1,0.375845,"of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 dinner I [after] [dinner] 后 [after] 我 [I] 睡 [sleep] 了 [past tense] slept Figure 7: A* search for pseudo-gold ITG alignments uses an admissible heuristic for bispans that counts the number of gold links outside of [k, `) but within [g, h). Above, the heuristic is 1, which is also the minimal number of alignment errors that an ITG alignment will incur using this bispan. Ag using A* bitext parsing (Klein and Manning, 2003). Search states, which correspond to bispans [g, h) ⇔ [k, `), are scored by the number of errors within the bispan plus the number of (i, j) ∈ At such that j ∈ [k, `) but i ∈ / [g, h) (recall errors). As an admissible heuristic for the future cost of a bispan [g, h) ⇔ [k, `), we count the number of (i, j) ∈ At such that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly n"
P10-1147,P08-2007,1,0.304937,"a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture of sure (filled) and possible (striped) word-level alignment links. [I] is more computationally convenient than arbitrarily ordered phrase matchings (Wu, 1997; DeNero and Klein, 2008). However, the space of block [sleep] ITG alignments is expressive enough to include [past tense] the vast majority of patterns observed in handannotated parallel corpora (Haghighi et al., 2009). In summary, our model scores all Rn (A) for A ∈ ITG(e, f) where A can include block terminals of size up to n. In our experiments, n = 3. Unlike previous work, we allow possible alignment links to appear in the block terminals, as depicted in Figure 4. 3 Model Estimation We estimate the weights θ of our extraction set model discriminatively using the margin-infused relaxed algorithm (MIRA) of Crammer"
P10-1147,W09-0429,0,0.0367767,"Missing"
P10-1147,W06-3105,1,0.181167,"nks will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation"
P10-1147,N03-1017,0,0.353293,"that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments. 1 Introduction In the last decade, the field of statistical machine translation has shifted from generating sentences word by word to systems that recycle whole fragments of training examples, expressed as translation rules. This general paradigm was first pursued using contiguous phrases (Och et al., 1999; Koehn et al., 2003), and has since been generalized to a wide variety of hierarchical and syntactic formalisms. The training stage of statistical systems focuses primarily on discovering translation rules in parallel corpora. Most systems discover translation rules via a two-stage pipeline: a parallel corpus is aligned at the word level, and then a second procedure extracts fragment-level rules from word-aligned sentence pairs. This paper offers a model-based alternative to phrasal rule extraction, which merges this two-stage pipeline into a single step. We present a discriminative model that directly predicts w"
P10-1147,P07-2045,0,0.0628644,"m phrase length n ensures that also demonstrate that extraction sets are useful for max(h − g, ` − k) ≤ n. end-to-end machine translation. Our model imFebruarythis 15 mapping 2010 PDT We canOndescribe via word-toproves translation quality relative to state-of-thephrase projections, as illustrated in Figure 1. Let art Chinese-to-English baselines across two pubword ei project to the phrasal span σ(ei ), where licly available systems, providing total BLEU im  provements of 1.2 in Moses, a phrase-based sysσ(ei ) = min j , max j + 1 (1) tem, and 1.4 in a Joshua, a hierarchical system j∈Ji j∈Ji (Koehn et al., 2007; Li et al., 2009) Ji = {j : (i, j) ∈ A} 2 Extraction Set Models The input to our model is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length"
P10-1147,P02-1040,0,0.10707,"h the HMM caused parse failures, which in turn caused training sentences to be skipped. To account for these issues, we added counts of phrasal rules extracted from the baseline HMM to the counts produced by supervised aligners. In Moses, our extraction set model predicts the set of phrases extracted by the system, and so the estimation techniques for the alignment model and translation model both share a common underlying representation: extraction sets. Empirically, we observe a BLEU score improvement of 1.2 over the best unsupervised baseline and 0.8 over the block ITG supervised baseline (Papineni et al., 2002). In Joshua, hierarchical rule extraction is based upon phrasal rule extraction, but abstracts away sub-phrases to create a grammar. Hence, the extraction sets we predict are closely linked to the representation that this system uses to translate. The extraction model again outperformed both unsupervised and supervised baselines, by 1.4 BLEU and 1.2 BLEU respectively. 7 Conclusion Our extraction set model serves to coordinate the alignment and translation model components of a statistical translation system by unifying their representations. Moreover, our model provides an effective alternativ"
P10-1147,N06-1015,1,0.93101,"measure of the overlap between bispans in Rn (Am ) and Rn (Ag ). This measure has been proposed previously to evaluate alignment systems (Ayan and Dorr, 2006). Based on preliminary translation results during development, we chose bispan F5 as our loss: Pr(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Am )| Rc(Am ) = |Rn (Am ) ∩ Rn (Ag )|/|Rn (Ag )| (1 + 52 ) · Pr(Am ) · Rc(Am ) 52 · Pr(Am ) + Rc(Am ) L(Am ; Ag ) = 1 − F5 (Am ; Ag ) F5 (Am ; Ag ) = F5 favors recall over precision. Previous alignment work has shown improvements from adjusting the F-measure parameter (Fraser and Marcu, 2006). In particular, Lacoste-Julien et al. (2006) also chose a recall-biased objective. Optimizing for a bispan F-measure penalizes alignment mistakes in proportion to their rule extraction consequences. That is, adding a word link that prevents the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features come from the predictions of unsupervised model"
P10-1147,H05-1010,1,0.85326,"h priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) a"
P10-1147,W09-0424,0,0.0785326,"sures that also demonstrate that extraction sets are useful for max(h − g, ` − k) ≤ n. end-to-end machine translation. Our model imFebruarythis 15 mapping 2010 PDT We canOndescribe via word-toproves translation quality relative to state-of-thephrase projections, as illustrated in Figure 1. Let art Chinese-to-English baselines across two pubword ei project to the phrasal span σ(ei ), where licly available systems, providing total BLEU im  provements of 1.2 in Moses, a phrase-based sysσ(ei ) = min j , max j + 1 (1) tem, and 1.4 in a Joshua, a hierarchical system j∈Ji j∈Ji (Koehn et al., 2007; Li et al., 2009) Ji = {j : (i, j) ∈ A} 2 Extraction Set Models The input to our model is an unaligned sentence pair, and the output is an extraction set of phrasal translation rules. Word-level alignments are generated as a byproduct of inference. We first specify the relationship between word alignments and extraction sets, then define our model. 2.1 Extraction Sets from Word Alignments Rule extraction is a standard concept in machine translation: word alignment constellations license particular sets of overlapping rules, from which subsets are selected according to limits on phrase length (Koehn et al., 200"
P10-1147,J97-3002,0,0.853276,"function during training affects model performance. We optimize for a phrase-level F-measure in order to focus learning on the task of predicting phrasal rules rather than word alignment links. Third, our discriminative approach requires that we perform inference in the space of extraction sets. Our model does not factor over disjoint wordto-word links or minimal phrase pairs, and so existing inference procedures do not directly apply. However, we show that the dynamic program for a block ITG aligner can be augmented to score extraction sets that are indexed by underlying ITG word alignments (Wu, 1997). We also describe a 1453 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453–1463, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics σ(ei ) 2010年2010年 过去 [past] ast] σ(fj ) wo] σ(fj ) 2月 Distribution [go over] Distribution over over 过 [go过over] possible link types possible link types 地球 [Earth] 地球 [Earth] 2月 [two] over Earth the Earth over the 2: Role-equivalent word pairs Type 2:Type Role-equivalent pairs that are equivalents not lexical equivalents are not that lexical 15日 15日 年 ear] [year] n]中 [in] 饭 (a) 1:Typ"
P10-1147,P05-1059,0,0.0752464,"lt from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block models (Haghighi et al., 2009). Our model directly extends this research agenda with first-class possible links, overlapping phrasal rule features, and an extraction-level loss function. K¨aa¨ ri¨ainen (2009) trains a translation model discriminatively using features on overlapping phrase pairs. That work differs from ours in that it uses fixed word alignments and focuses on translation"
P10-1147,N06-1014,1,0.872769,"s the extraction of many correct phrasal rules, or which licenses many incorrect rules, is strongly discouraged by this loss. 1456 2010年 2月 15日 3.2 σ(ei ) Features on Extraction Sets The discriminative power of our model is driven by the features on sure word alignment links φa (i, j) and bispans φb (g, h, k, `). In both cases, the most important features come from the predictions of unsupervised models trained on large parallel corpora, which provide frequency and cooccurrence information. To score word-to-word links, we use the posterior predictions of a jointly trained HMM alignment model (Liang et al., 2006). The remaining features include a dictionary feature, an identical word feature, an absolute position distortion feature, and features for numbers and punctuation. To score phrasal translation rules in an extraction set, we use a mixture of feature types. Extraction set models allow us to incorporate the same phrasal relative frequency statistics that drive phrase-based translation performance (Koehn et al., 2003). To implement these frequency features, we extract a phrase table from the alignment predictions of a jointly trained unsupervised HMM model using Moses (Koehn et al., 2007), and sc"
P10-1147,W06-1627,0,0.0510636,"Missing"
P10-1147,D07-1104,0,0.00646831,"er methods did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate for parse failures. 6.3 Translation Experiments We evaluate the alignments predicted by our model using two publicly available, open-source, state-of-the-art translation systems. Moses is a phrase-based system with lexicalized reordering (Koehn et al., 2007). Joshua (Li et al., 2009) is an implementation of Hiero (Chiang, 2007) using a suffix-array-based grammar extraction approach (Lopez, 2007). Both of these systems take word alignments as input, and neither of these systems accepts possible links in the alignments they consume. To interface with our extraction set models, we produced three sets of sure-only alignments from our model predictions: one that omitted possible links, one that converted all possible links to sure links, and one that includes each possible link with 0.5 probability. These three sets were aggregated and rules were extracted from all three. The training set we used for MT experiments is quite heterogenous and noisy compared to our alignment test sets, and t"
P10-1147,P08-1012,0,0.558386,", although we emphasize that A is a set of sure and possible alignments, and φ(A) does not decompose as a sum of vectors on individual word-level alignment links. Our model is parameterized by a weight vector θ, which scores an extraction set Rn (A) as θ · φ(A). To further limit the space of extraction sets we are willing to consider, we restrict A to block inverse transduction grammar (ITG) alignments, a space that allows many-to-many alignments through phrasal terminal productions, but otherwise enforces at-most-one-to-one phrase matchings with ITG reordering patterns (Cherry and Lin, 2007; Zhang et al., 2008). The ITG constraint 1455 On February n] 饭 15 2010 被 [passive marker] 发现 [discover] was discovered model class. Hence, we update toward the extraction set for a pseudo-gold alignment Ag ∈ σ(e ) ITG (e, f) with minimal1distance from the true reference alignment At . Ag = arg minA∈ITG(e,f) |A ∪ At − A ∩ At |(3) [after] [dinner] [after] Figure 4: Above, we show a representative subset of the block alignment patterns that serve as terminal productions of the ITG that restricts the output space of our model. These terminal productions cover up to n = 3 words in each sentence and include a mixture o"
P10-1147,W02-1018,0,0.188876,"euristic for the future cost of a bispan [g, h) ⇔ [k, `), we count the number of (i, j) ∈ At such that i ∈ [g, h) but j ∈ / [k, `), as depicted in Figure 7. These links will become recall errors eventually. A* search with this heuristic makes no errors, and the time required to compute pseudo-gold alignments is negligible. 5 Relationship to Previous Work Our model is certainly not the first alignment approach to include structures larger than words. Model-based phrase-to-phrase alignment was proposed early in the history of phrase-based translation as a method for training translation models (Marcu and Wong, 2002). A variety of unsupervised models refined this initial work with priors (DeNero et al., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word align"
P10-1147,2007.mtsummit-papers.43,0,0.0084729,"rohibitively slow tained in sorted order according to their inside when there is no sparsity in the grammar. Mainscore. This ordering allows us to stop combintaining the context necessary to score non-local ing states early when the results are falling off the bispans further increases running time. That is, agenda beams. Similar search and beaming strateITG inference is organized around search states gies appear in many decoders for machine transassociated with a grammar symbol and a bispan; lation (Huang and Chiang, 2007; Koehn and Hadaugmenting grammar symbols also augments this dow, 2009; Moore and Quirk, 2007). state space. To parse quickly, we prune away search states 4.3 Finding Pseudo-Gold ITG Alignments using predictions from the more efficient HMM Equation 3 asks for the block ITG alignment 6 The number of configuration states does not depend on Ag that is closest to a reference alignment At , the size of A because corners have fixed size, and because the position of links within rows or columns is not needed. which may not lie in ITG(e,f). We search for 1458 σ l σ(f2 ) 在 k =1 饭 l =4 g =0 After h =3 dinner I [after] [dinner] 后 [after] 我 [I] 睡 [sleep] 了 [past tense] slept Figure 7: A* search fo"
P10-1147,H05-1011,0,0.0650786,"l., 2008; Blunsom et al., 2009) and inference constraints (DeNero et al., 2006; Birch et al., 2006; Cherry and Lin, 2007; Zhang et al., 2008). These models fundamentally differ from ours in that they stipulate a segmentation of the sentence pair into phrases, and only align the minimal phrases in that segmentation. Our model scores the larger overlapping phrases that result from composing these minimal phrases. Discriminative alignment is also a wellexplored area. Most work has focused on predicting word alignments via partial matching inference algorithms (Melamed, 2000; Taskar et al., 2005; Moore, 2005; Lacoste-Julien et al., 2006). Work in semi-supervised estimation has also contributed evidence that hand-annotations are useful for training alignment models (Fraser and Marcu, 2006; Fraser and Marcu, 2007). The ITG grammar formalism, the corresponding word alignment class, and inference procedures for the class have also been explored extensively (Wu, 1997; Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008). At the intersection of these lines of work, discriminative ITG models have also been proposed, including one-to-one alignment models (Cherry and Lin, 2006) and block mode"
P10-1147,C96-2141,0,0.354347,"ions generated by two end-to-end systems. Table 1 compares the five systems described below, including three baselines. All supervised aligners were optimized for bispan F5 . Unsupervised Baseline: GIZA++. We trained GIZA++ (Och and Ney, 2003) using the default parameters included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive thresholding heuristic of DeNero and Klein (2007), yielding a state-ofthe-art unsupervised baseline. Supervised Baseline: Block ITG. We discriminatively trained a block ITG aligner with only sure links, using block terminal productions up to 3 words by 3 words in size. This supervised baseline is a reimplementation of the MIRA-trained model of Haghighi et al. (2009). We use"
P10-1147,J03-1002,0,0.00697973,"ace of ITG alignments. Also, their approach uses greedy search, while our search is optimal aside from pruning and beaming. Despite these differences, their strong results reinforce our claim that phraselevel information is useful for alignment. 6 Experiments We evaluate our extraction set model by the bispans it predicts, the word alignments it generates, and the translations generated by two end-to-end systems. Table 1 compares the five systems described below, including three baselines. All supervised aligners were optimized for bispan F5 . Unsupervised Baseline: GIZA++. We trained GIZA++ (Och and Ney, 2003) using the default parameters included with the Moses training script (Koehn et al., 2007). The designated regimen concludes by Viterbi aligning under Model 4 in both directions. We combined these alignments with 1459 On Febru the grow-diag heuristic (Koehn et al., 2003). Unsupervised Baseline: Joint HMM. We trained and combined two HMM alignment models (Ney and Vogel, 1996) using the Berkeley Aligner.7 We initialized the HMM model parameters with jointly trained Model 1 parameters (Liang et al., 2006), combined word-toword posteriors by averaging (soft union), and decoded with the competitive"
P10-1147,W99-0604,0,0.178065,"ased loss function that relates directly to the end task of generating translations. Our model gives improvements in alignment quality relative to state-of-the-art unsupervised and supervised baselines, as well as providing up to a 1.4 improvement in BLEU score in Chinese-to-English translation experiments. 1 Introduction In the last decade, the field of statistical machine translation has shifted from generating sentences word by word to systems that recycle whole fragments of training examples, expressed as translation rules. This general paradigm was first pursued using contiguous phrases (Och et al., 1999; Koehn et al., 2003), and has since been generalized to a wide variety of hierarchical and syntactic formalisms. The training stage of statistical systems focuses primarily on discovering translation rules in parallel corpora. Most systems discover translation rules via a two-stage pipeline: a parallel corpus is aligned at the word level, and then a second procedure extracts fragment-level rules from word-aligned sentence pairs. This paper offers a model-based alternative to phrasal rule extraction, which merges this two-stage pipeline into a single step. We present a discriminative model tha"
P10-1147,2009.eamt-smart.4,0,\N,Missing
P10-1147,J00-2004,0,\N,Missing
P10-1147,2006.amta-papers.2,0,\N,Missing
P10-1147,J07-2003,0,\N,Missing
P11-1043,P06-1002,0,0.348745,"Missing"
P11-1043,P09-1088,0,0.0166344,"predictions of two directional models, while taking into account each model’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010). Inference in our combined model is not tractable because of numerous edge cycles in the model graph. However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual bound on the model objective. In cases where our algorithm converges, we have a certificate of optimality under the full model. Early stopping before convergence still yields u"
P11-1043,J93-2003,0,0.0443278,"composition, which reuses the efficient inference algorithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations. 1 Introduction Word alignment is the task of identifying corresponding words in sentence pairs. The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e, such as IBM Model 4 (Brown et al., 1993) or the HMM-based alignment model (Vogel et al., 1996). Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f (Och et al., 1999). Combination can reduce errors and relax the one-to-many structural restriction of directional models. Common combination methods include the union or intersection of directional alignments, as 420 Klaus Macherey Google Research kmach@google.com well as heuristic interpolations between the union and intersection like grow-diag-final (Koehn et al., 2003). This paper presents a model-based"
P11-1043,N09-1013,0,0.0453736,"Missing"
P11-1043,N10-1015,0,0.0448185,"Missing"
P11-1043,E09-1020,0,0.185454,"Missing"
P11-1043,P11-1061,0,0.0450019,"Missing"
P11-1043,P08-2007,1,0.873397,"raph Ga includes all of the vertices corresponding to variables a and c. The other subgraph Gb includes vertices for variables b and c. Every edge in 423 ∀ (i, j) ∈ I . The Lagrangian relaxation of this optimization problem is L(a, b, c(a) , c(b) , u) = X (a) (b) f (a, c(a) ) + g(b, c(b) ) + u(i, j)(ci,j − ci,j ) . (i,j)∈I Hence, we can rewrite the original problem as max ci0 j 0 → bi0 → ci0 j → aj → cij Additional cycles also exist in the graph through the edges between aj−1 and aj and between bi−1 and bi . The general phrase alignment problem under an arbitrary model is known to be NP-hard (DeNero and Klein, 2008). (b) such that: cij = cij (3) a,b,c(a) ,c(b) min L(a, b, c(a) , c(b) , u) . u We can form a dual problem that is an upper bound on the original optimization problem by swapping the order of min and max. In this case, the dual problem decomposes into two terms that are each local to an acyclic subgraph.    X (a) min  max f (a, c(a) ) + u(i, j)c  u ij a,c(a) i,j  + max g(b, c(b) ) − b,c(b)  X i,j (b) u(i, j)cij  (4) c11(b) c11(a) c12(b) c12(a) c21(b) b1 你 c11(a) c12(a) c13(a) c23(b) b2 好 c21(a) c22(a) c23(a) a1 a2 a3 How are you c13(a) c22(b) c22(a) c21(a) c13(b) c23(a) a1 a2 a3 How"
P11-1043,P10-1147,1,0.931235,"el’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010). Inference in our combined model is not tractable because of numerous edge cycles in the model graph. However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual bound on the model objective. In cases where our algorithm converges, we have a certificate of optimality under the full model. Early stopping before convergence still yields useful outputs. Our model-based approach to aligner combination yields impro"
P11-1043,D08-1033,1,0.906699,"Missing"
P11-1043,P09-2058,0,0.0587895,"f the directional sets. A∪ = Aa ∪ A b A∩ = Aa ∩ Ab . More complex combiners, such as the grow-diagfinal heuristic (Koehn et al., 2003), produce alignment link sets that include all of A∩ and some subset of A∪ based on the relationship of multiple links (Och et al., 1999). In addition, supervised word alignment models often use the output of directional unsupervised aligners as features or pruning signals. In the case that a supervised model is restricted to proposing alignment links that appear in the output of a directional aligner, these models can be interpreted as a combination technique (Deng and Zhou, 2009). Such a model-based approach differs from ours in that it requires a supervised dataset and treats the directional aligners’ output as fixed. Combination is also related to agreement-based learning (Liang et al., 2006). This approach to jointly learning two directional alignment models yields state-of-the-art unsupervised performance. Our method is complementary to agreement-based learning, as it applies to Viterbi inference under the model rather than computing expectations. In fact, we employ agreement-based training to estimate the parameters of the directional aligners in our experiments."
P11-1043,P08-1112,0,0.114826,"Missing"
P11-1043,P09-1104,1,0.937915,"g into account each model’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010). Inference in our combined model is not tractable because of numerous edge cycles in the model graph. However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual bound on the model objective. In cases where our algorithm converges, we have a certificate of optimality under the full model. Early stopping before convergence still yields useful outputs. Our model-based approach to aligner"
P11-1043,W07-0711,0,0.0210363,"g a and b. In this way, we relax the one-to-many constraints of the directional models. However, all of the information about how words align is expressed by the vertex and edge potentials on a and b. The coherence edges and the link matrix c only serve to resolve conflicts between the directional models and communicate information between them. Because directional alignments are preserved intact as components of our model, extensions or refinements to the underlying directional Markov alignment model could be integrated cleanly into our model as well, including lexicalized transition models (He, 2007), extended conditioning contexts (Brunning et al., 2009), and external information (Shindo et al., 2010). For any assignment to (a, b, c) with non-zero probability, c must encode a one-to-one phrase alignment with a maximum phrase length of 3. That is, any word in either sentence can align to at most three words in the opposite sentence, and those words must be contiguous. This restriction is directly enforced by the edge potential in Equation 2. the graph belongs to exactly one of these two subgraphs. The dual decomposition inference approach allows us to exploit this sub-graph structure (Rus"
P11-1043,N03-1017,0,0.591064,"translation e, such as IBM Model 4 (Brown et al., 1993) or the HMM-based alignment model (Vogel et al., 1996). Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f (Och et al., 1999). Combination can reduce errors and relax the one-to-many structural restriction of directional models. Common combination methods include the union or intersection of directional alignments, as 420 Klaus Macherey Google Research kmach@google.com well as heuristic interpolations between the union and intersection like grow-diag-final (Koehn et al., 2003). This paper presents a model-based alternative to aligner combination. Inference in a probabilistic model resolves the conflicting predictions of two directional models, while taking into account each model’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the outpu"
P11-1043,D10-1125,0,0.0818553,"t most three words in the opposite sentence, and those words must be contiguous. This restriction is directly enforced by the edge potential in Equation 2. the graph belongs to exactly one of these two subgraphs. The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). In particular, we can iteratively apply exact inference to the subgraph problems, adjusting their potentials to reflect the constraints of the full problem. The technique of dual decomposition has recently been shown to yield state-of-the-art performance in dependency parsing (Koo et al., 2010). 3.2 Dual Problem Formulation To describe a dual decomposition inference procedure for our model, we first restate the inference problem under our graphical model in terms of the two overlapping subgraphs that admit tractable inference. Let c(a) be a copy of c associated with Ga , and c(b) with Gb . Also, let f (a, c(a) ) be the unnormalized log-probability of an assignment to Ga and g(b, c(b) ) be the unnormalized log-probability of an assignment to Gb . Finally, let I be the index set of all (i, j) for c. Then, the maximum likelihood assignment to our original model can be found by optimizi"
P11-1043,P09-1019,0,0.0516115,"Missing"
P11-1043,N06-1014,0,0.378617,"e relationship of multiple links (Och et al., 1999). In addition, supervised word alignment models often use the output of directional unsupervised aligners as features or pruning signals. In the case that a supervised model is restricted to proposing alignment links that appear in the output of a directional aligner, these models can be interpreted as a combination technique (Deng and Zhou, 2009). Such a model-based approach differs from ours in that it requires a supervised dataset and treats the directional aligners’ output as fixed. Combination is also related to agreement-based learning (Liang et al., 2006). This approach to jointly learning two directional alignment models yields state-of-the-art unsupervised performance. Our method is complementary to agreement-based learning, as it applies to Viterbi inference under the model rather than computing expectations. In fact, we employ agreement-based training to estimate the parameters of the directional aligners in our experiments. A parallel idea that closely relates to our bidirectional model is posterior regularization, which has also been applied to the word alignment problem (Grac¸a et al., 2008). One form of posterior regularization stipula"
P11-1043,W02-1018,0,0.0187801,"based alternative to aligner combination. Inference in a probabilistic model resolves the conflicting predictions of two directional models, while taking into account each model’s uncertainty over its output. This result is achieved by embedding two directional HMM-based alignment models into a larger bidirectional graphical model. The full model structure and potentials allow the two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010). Inference in our combined model is not tractable because of numerous edge cycles in the model graph. However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual bound on the model objective. In cases where our algorithm converges, we"
P11-1043,J04-4002,0,0.289023,"Missing"
P11-1043,W99-0604,0,0.613696,"ectional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations. 1 Introduction Word alignment is the task of identifying corresponding words in sentence pairs. The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e, such as IBM Model 4 (Brown et al., 1993) or the HMM-based alignment model (Vogel et al., 1996). Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f (Och et al., 1999). Combination can reduce errors and relax the one-to-many structural restriction of directional models. Common combination methods include the union or intersection of directional alignments, as 420 Klaus Macherey Google Research kmach@google.com well as heuristic interpolations between the union and intersection like grow-diag-final (Koehn et al., 2003). This paper presents a model-based alternative to aligner combination. Inference in a probabilistic model resolves the conflicting predictions of two directional models, while taking into account each model’s uncertainty over its output. This"
P11-1043,P02-1040,0,0.0820709,"Missing"
P11-1043,D10-1001,0,0.269478,"e two embedded directional models to disagree to some extent, but reward agreement. Moreover, the bidirectional model enforces a one-to-one phrase alignment structure, similar to the output of phrase alignment models (Marcu and Wong, 2002; DeNero et al., 2008), unsupervised inversion transduction grammar (ITG) models (Blunsom et al., 2009), and supervised ITG models (Haghighi et al., 2009; DeNero and Klein, 2010). Inference in our combined model is not tractable because of numerous edge cycles in the model graph. However, we can employ dual decomposition as an approximate inference technique (Rush et al., 2010). In this approach, we iteratively apply the same efficient sequence algorithms for the underlying directional models, and thereby optimize a dual bound on the model objective. In cases where our algorithm converges, we have a certificate of optimality under the full model. Early stopping before convergence still yields useful outputs. Our model-based approach to aligner combination yields improvements in alignment quality and phrase extraction quality in Chinese-English experiments, relative to typical heuristic combinations methods applied to the predictions of independent directional models"
P11-1043,P10-2025,0,0.023097,"er, all of the information about how words align is expressed by the vertex and edge potentials on a and b. The coherence edges and the link matrix c only serve to resolve conflicts between the directional models and communicate information between them. Because directional alignments are preserved intact as components of our model, extensions or refinements to the underlying directional Markov alignment model could be integrated cleanly into our model as well, including lexicalized transition models (He, 2007), extended conditioning contexts (Brunning et al., 2009), and external information (Shindo et al., 2010). For any assignment to (a, b, c) with non-zero probability, c must encode a one-to-one phrase alignment with a maximum phrase length of 3. That is, any word in either sentence can align to at most three words in the opposite sentence, and those words must be contiguous. This restriction is directly enforced by the edge potential in Equation 2. the graph belongs to exactly one of these two subgraphs. The dual decomposition inference approach allows us to exploit this sub-graph structure (Rush et al., 2010). In particular, we can iteratively apply exact inference to the subgraph problems, adjus"
P11-1043,D09-1086,0,0.0467935,"Missing"
P11-1043,C96-2141,0,0.931762,"rithms of the directional models. Our bidirectional model enforces a one-to-one phrase constraint while accounting for the uncertainty in the underlying directional models. The resulting alignments improve upon baseline combination heuristics in word-level and phrase-level evaluations. 1 Introduction Word alignment is the task of identifying corresponding words in sentence pairs. The standard approach to word alignment employs directional Markov models that align the words of a sentence f to those of its translation e, such as IBM Model 4 (Brown et al., 1993) or the HMM-based alignment model (Vogel et al., 1996). Machine translation systems typically combine the predictions of two directional models, one which aligns f to e and the other e to f (Och et al., 1999). Combination can reduce errors and relax the one-to-many structural restriction of directional models. Common combination methods include the union or intersection of directional alignments, as 420 Klaus Macherey Google Research kmach@google.com well as heuristic interpolations between the union and intersection like grow-diag-final (Koehn et al., 2003). This paper presents a model-based alternative to aligner combination. Inference in a pro"
P11-1043,2002.tmi-tutorials.1,0,\N,Missing
P11-1043,2002.amta-tutorials.1,0,\N,Missing
P11-1043,2007.mtsummit-tutorials.1,0,\N,Missing
P11-1043,P01-1067,0,\N,Missing
P11-1043,D09-1125,0,\N,Missing
P11-1043,W05-0801,0,\N,Missing
P11-1043,D11-1006,0,\N,Missing
P11-1043,2010.amta-papers.31,0,\N,Missing
P12-1016,P11-2062,0,0.0582865,"Missing"
P12-1016,P08-1087,0,0.0287878,"nce in Fig. 1. The correct translation is a monotone mapping of the input. However, in Arabic, SVO word order requires both gender and number agreement between  the subject èPAJ Ë@ ‘the car’ and verb I . ë YK ‘go’. The MT system selects the correct verb stem, but with masculine inflection. Although the translation has the correct semantics, it is ultimately ungrammatical. This paper addresses the problem of generating text that conforms to morpho-syntactic agreement rules. Agreement relations that cross statistical phrase boundaries are not explicitly modeled in most phrasebased MT systems (Avramidis and Koehn, 2008). We address this shortcoming with an agreement model that scores sequences of fine-grained morphosyntactic classes. First, bound morphemes in translation hypotheses are segmented. Next, the segments are labeled with classes that encode both syntactic category information (i.e., parts of speech) and grammatical features such as number and gender. Finally, agreement is promoted by scoring the predicted class sequences with a generative Markov model. Our model scores hypotheses during decoding. Unlike previous models for scoring syntactic relations, our model does not require bitext annotations,"
P12-1016,W07-0702,0,0.00875583,"translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. Words are represented as a vector of features such as lemma and POS. The bitext is annotated with separate models, and the annotations are saved during phrase extraction. Hassan et al. (2007) noticed that the targetside POS sequences could be scored, much as we do in this work. They used a target-side LM over Combinatorial Categorial Grammar (CCG) supertags, along with a penalty for the number of operator violations, and also modified the phrase probabilities based on the tags. However, Birch et al. (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. Birch et al. (2007) then investigated source-side CCG supertag features, but did not show an improvement for Dutch-English. Subotin (2011) recently extended factored translation models to hierarchical phrase-based translation and developed a discriminative model for predicting target-side morphology in English-Czech. His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext. In contrast to these methods, our mo"
P12-1016,D07-1090,0,0.0103763,"esults, we initialized MERT with uniform feature weights. We trained the translation model on 502 million words of parallel text collected from a variety of sources, including the Web. Word alignments were induced using a hidden Markov model based alignment model (Vogel et al., 1996) initialized with bilexical parameters from IBM Model 1 (Brown et al., 1993). Both alignment models were trained using two iterations of the expectation maximization algorithm. Our distributed 4-gram language model was trained on 600 million words of Arabic text, also collected from many sources including the Web (Brants et al., 2007). For development and evaluation, we used the NIST Arabic-English data sets, each of which contains one set of Arabic sentences and multiple English references. To reverse the translation direction for each data set, we chose the first English reference as the source and the Arabic as the reference. The NIST sets come in two varieties: newswire (MT02-05) and mixed genre (MT06,08). Newswire contains primarily Modern Standard Arabic (MSA), while the mixed genre data sets also contain transcribed speech and web text. Since the ATB contains MSA, and significant lexical and syntactic differences 15"
P12-1016,J92-4003,0,0.241927,"uded in the baseline. Birch et al. (2007) then investigated source-side CCG supertag features, but did not show an improvement for Dutch-English. Subotin (2011) recently extended factored translation models to hierarchical phrase-based translation and developed a discriminative model for predicting target-side morphology in English-Czech. His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext. In contrast to these methods, our model does not affect phrase extraction and does not require annotated translation rules. Class-based LMs Class-based LMs (Brown et al., 1992) reduce lexical sparsity by placing words in equivalence classes. They have been widely used for speech recognition, but not for MT. Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. They used a classical exchange algorithm for clustering, and learned 512 classes from a large monolingual corpus. Then they mixed the classes into a word-based LM. However, both Och"
P12-1016,J93-2003,0,0.0223254,"Missing"
P12-1016,N10-2003,0,0.0270795,"xt, the segments are labeled with classes that encode both syntactic category information (i.e., parts of speech) and grammatical features such as number and gender. Finally, agreement is promoted by scoring the predicted class sequences with a generative Markov model. Our model scores hypotheses during decoding. Unlike previous models for scoring syntactic relations, our model does not require bitext annotations, phrase table features, or decoder modifications. The model can be implemented using the feature APIs of popular phrase-based decoders such as Moses (Koehn et al., 2007) and Phrasal (Cer et al., 2010). Intuition might suggest that the standard n-gram language model (LM) is sufficient to handle agreement phenomena. However, LM statistics are sparse, and they are made sparser by morphological variation. For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM. 146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generati"
P12-1016,P11-2031,0,0.0122615,"Missing"
P12-1016,N09-1046,0,0.00828444,"s as a unit during phrase extraction, but to score segmented morphemes separately for agreement. We treat segmentation as a character-level sequence modeling problem and train a linear-chain conditional random field (CRF) model (Lafferty et al., 2001). As a pre-processing step, we group contiguous non-native characters (e.g., Latin characters in Arabic text). The model assigns four labels: • • • • I: Continuation of a morpheme O: Outside morpheme (whitespace) B: Beginning of a morpheme F: Non-native character(s) 2 Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al., 2011). e f a Π h λ H Translation Model Target sequence of I words Source sequence of J words Sequence of K phrase alignments for he, f i Permutation of the alignments for target word order e Sequence of M feature functions Sequence of learned weights for the M features A priority queue of hypotheses t∈T s∈S θseg θtag φo , φt τ π sˆ σ Class-based Agreement Model Set of morpho-syntactic classes Set of all word segments Learned weights for the CRF-based segmenter Learned weights for the CRF-based tagger CRF potential functions (emission and transition) Sequence of I"
P12-1016,W12-5611,0,0.0407969,"Missing"
P12-1016,E12-1068,0,0.0341836,"enomena. However, LM statistics are sparse, and they are made sparser by morphological variation. For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM. 146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generation because the bitext may not contain all inflected variants (Minkov et al., 2007; Toutanova et al., 2008; Fraser et al., 2012). However, using lexical coverage experiments, we show that there is ample room for translation quality improvements through better selection of forms that already exist in the translation model. 2 A Class-based Model of Agreement 2.1 Morpho-syntactic Agreement Morpho-syntactic agreement refers to a relationship between two sentence elements a and b that must have at least one matching grammatical feature.1 Agreement relations tend to be defined for particular syntactic configurations such as verb-subject, noun-adjective, and pronoun-antecedent. In some languages, agreement affects the surface"
P12-1016,P09-1087,0,0.100353,"nder agreement). 4 Case is also relevant to agreement in Arabic, but it is mostly indicated by diacritics, which are absent in unvocalized text. 2.4 Word Class Sequence Scoring The CRF tagger model defines a conditional distribution p(τ |e; θtag ) for a class sequence τ given a sentence e and model parameters θtag . That is, the sample space is over class—not word—sequences. However, in MT, we seek a measure of sentence quality q(e) that is comparable across different hypotheses on the beam (much like the n-gram language model score). Discriminative model scores have been used as MT features (Galley and Manning, 2009), but we obtained better results by scoring the 1-best class sequences with a generative model. We trained a simple add-1 smoothed bigram language model over gold class sequences in the same treebank training data: q(e) = p(τ ) = I Y p(τi |τi−1 ) i=1 We chose a bigram model due to the aggressive recombination strategy in our phrase-based decoder. For contexts in which the LM is guaranteed to back off (for instance, after an unseen bigram), our decoder maintains only the minimal state needed (perhaps only a single word). In less restrictive decoders, higher order scoring models could be used to"
P12-1016,C10-1045,1,0.30532,"Missing"
P12-1016,P05-1071,0,0.0214754,"agging After segmentation, we tag each segment with a finegrained morpho-syntactic class. For this task we also train a standard CRF model on full sentences with gold classes and segmentation. We use the same QN procedure as before to obtain θtag . A translation derivation is a tuple he, f, ai where e is the target, f is the source, and a is an alignment between the two. The CRF tagging model predicts a target-side class sequence τ ∗ τ ∗ = arg max τ I X θtag · {φo (τi , i, e) + φt (τi , τi−1 )} i=1 where further notation is defined in Fig. 3. 3 Mada, the standard tool for Arabic segmentation (Habash and Rambow, 2005), relies on a manually compiled lexicon. 148 Set of Classes The tagger assigns morpho-syntactic classes, which are coarse POS categories refined with grammatical features such as gender and definiteness. The coarse categories are the universal POS tag set described by Petrov et al. (2012). More than 25 treebanks (in 22 languages) can be automatically mapped to this tag set, which includes “Noun” (nominals), “Verb” (verbs), “Adj” (adjectives), and “ADP” (preand post-positions). Many of these treebanks also contain per-token morphological annotations. It is easy to combine the coarse categories"
P12-1016,N06-2013,0,0.0901393,"verb. Our model segments the raw token, tags each segment with a morpho-syntactic class (e.g., “Pron+Fem+Sg”), and then scores the class sequences. the single raw token in Fig. 2 contains at least four grammatically independent morphemes. Because the morphemes bear conflicting grammatical features and basic parts of speech (POS), we need to segment the token before we can evaluate agreement relations.2 Segmentation is typically applied as a bitext preprocessing step, and there is a rich literature on the effect of different segmentation schemata on translation quality (Koehn and Knight, 2003; Habash and Sadat, 2006; El Kholy and Habash, 2012). Unlike previous work, we segment each translation hypothesis as it is generated (i.e., during decoding). This permits greater modeling flexibility. For example, it may be useful to count tokens with bound morphemes as a unit during phrase extraction, but to score segmented morphemes separately for agreement. We treat segmentation as a character-level sequence modeling problem and train a linear-chain conditional random field (CRF) model (Lafferty et al., 2001). As a pre-processing step, we group contiguous non-native characters (e.g., Latin characters in Arabic te"
P12-1016,P07-1037,0,0.0352461,"Missing"
P12-1016,D07-1091,0,0.0834596,"nd it is unclear how to learn such knowledgerich representations from unlabeled data. One partial 150 solution is to manually extract unification rules from phrase-structure trees. Williams and Koehn (2011) annotated German trees, and extracted translation rules from them. They then specified manual unification rules, and applied a penalty according to the number of unification failures in a hypothesis. In contrast, our class-based model does not require any manual rules and scores similar agreement phenomena as probabilistic sequences. Factored Translation Models Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. Words are represented as a vector of features such as lemma and POS. The bitext is annotated with separate models, and the annotations are saved during phrase extraction. Hassan et al. (2007) noticed that the targetside POS sequences could be scored, much as we do in this work. They used a target-side LM over Combinatorial Categorial Grammar (CCG) supertags, along with a penalty for the number of operator violations, and also modified the phrase probabilities based on the tags. However, Birch et al. (2007) showed that this approa"
P12-1016,E03-1076,0,0.00974634,"ject and plural for the verb. Our model segments the raw token, tags each segment with a morpho-syntactic class (e.g., “Pron+Fem+Sg”), and then scores the class sequences. the single raw token in Fig. 2 contains at least four grammatically independent morphemes. Because the morphemes bear conflicting grammatical features and basic parts of speech (POS), we need to segment the token before we can evaluate agreement relations.2 Segmentation is typically applied as a bitext preprocessing step, and there is a rich literature on the effect of different segmentation schemata on translation quality (Koehn and Knight, 2003; Habash and Sadat, 2006; El Kholy and Habash, 2012). Unlike previous work, we segment each translation hypothesis as it is generated (i.e., during decoding). This permits greater modeling flexibility. For example, it may be useful to count tokens with bound morphemes as a unit during phrase extraction, but to score segmented morphemes separately for agreement. We treat segmentation as a character-level sequence modeling problem and train a linear-chain conditional random field (CRF) model (Lafferty et al., 2001). As a pre-processing step, we group contiguous non-native characters (e.g., Latin"
P12-1016,P07-2045,0,0.00500078,"tion hypotheses are segmented. Next, the segments are labeled with classes that encode both syntactic category information (i.e., parts of speech) and grammatical features such as number and gender. Finally, agreement is promoted by scoring the predicted class sequences with a generative Markov model. Our model scores hypotheses during decoding. Unlike previous models for scoring syntactic relations, our model does not require bitext annotations, phrase table features, or decoder modifications. The model can be implemented using the feature APIs of popular phrase-based decoders such as Moses (Koehn et al., 2007) and Phrasal (Cer et al., 2010). Intuition might suggest that the standard n-gram language model (LM) is sufficient to handle agreement phenomena. However, LM statistics are sparse, and they are made sparser by morphological variation. For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM. 146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics It has also been suggested that this setting"
P12-1016,D08-1076,0,0.1198,"Missing"
P12-1016,P11-1140,0,0.00851081,"extraction, but to score segmented morphemes separately for agreement. We treat segmentation as a character-level sequence modeling problem and train a linear-chain conditional random field (CRF) model (Lafferty et al., 2001). As a pre-processing step, we group contiguous non-native characters (e.g., Latin characters in Arabic text). The model assigns four labels: • • • • I: Continuation of a morpheme O: Outside morpheme (whitespace) B: Beginning of a morpheme F: Non-native character(s) 2 Segmentation also improves translation of compounding languages such as German (Dyer, 2009) and Finnish (Macherey et al., 2011). e f a Π h λ H Translation Model Target sequence of I words Source sequence of J words Sequence of K phrase alignments for he, f i Permutation of the alignments for target word order e Sequence of M feature functions Sequence of learned weights for the M features A priority queue of hypotheses t∈T s∈S θseg θtag φo , φt τ π sˆ σ Class-based Agreement Model Set of morpho-syntactic classes Set of all word segments Learned weights for the CRF-based segmenter Learned weights for the CRF-based tagger CRF potential functions (emission and transition) Sequence of I target-side predicted classes T dim"
P12-1016,P07-1017,0,0.178847,"del (LM) is sufficient to handle agreement phenomena. However, LM statistics are sparse, and they are made sparser by morphological variation. For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM. 146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generation because the bitext may not contain all inflected variants (Minkov et al., 2007; Toutanova et al., 2008; Fraser et al., 2012). However, using lexical coverage experiments, we show that there is ample room for translation quality improvements through better selection of forms that already exist in the translation model. 2 A Class-based Model of Agreement 2.1 Morpho-syntactic Agreement Morpho-syntactic agreement refers to a relationship between two sentence elements a and b that must have at least one matching grammatical feature.1 Agreement relations tend to be defined for particular syntactic configurations such as verb-subject, noun-adjective, and pronoun-antecedent. In"
P12-1016,D11-1080,0,0.0278454,"showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. They used a classical exchange algorithm for clustering, and learned 512 classes from a large monolingual corpus. Then they mixed the classes into a word-based LM. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. It is unclear if their classes captured agreement information. Monz (2011) recently investigated parameter estimation for POS-based language models, but his classes did not include inflectional features. Target-Side Syntactic LMs Our agreement model is a form of syntactic LM, of which there is a long history of research, especially in speech processing.5 Syntactic LMs have traditionally been too slow for scoring during MT decoding. One exception was the quadratic-time dependency language model presented by Galley and Manning (2009). They applied a quadratic time dependency parser to every hypothesis during decoding. However, to achieve quadratic running time, they p"
P12-1016,J04-4002,0,0.0833914,"coding Scoring the agreement model as part of translation decoding requires a novel inference procedure. Crucially, the inference procedure does not measurably affect total MT decoding time. 3.1 Input: implicitly defined search space generate initial hypotheses and add to H set Hf inal to ∅ while H is not empty: set Hext to ∅ for each hypothesis η in H: if η is a goal hypothesis: add η to Hf inal else Extend η and add to Hext IScore agreement Recombine and Prune Hext set H to Hext Output: argmax of Hf inal Phrase-based Translation Decoding We consider the standard phrase-based approach to MT (Och and Ney, 2004). The distribution p(e|f ) is modeled directly using a log-linear model, yielding the following decision rule: ( e∗ = arg max e,a,Π M X ) λm hm (e, f, a, Π) (1) m=1 This decoding problem is NP-hard, thus a beam search is often used (Fig. 4). The beam search relies on three operations, two of which affect the agreement model: 149 We assume familiarity with these operations, which are described in detail in (Och and Ney, 2004). 3.2 Agreement Model Inference The class-based agreement model is implemented as a feature function hm in Eq. (1). Specifically, when Extend generates a new hypothesis, we"
P12-1016,N04-1021,0,0.0659821,"Missing"
P12-1016,E99-1010,0,0.098952,"n (2011) recently extended factored translation models to hierarchical phrase-based translation and developed a discriminative model for predicting target-side morphology in English-Czech. His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext. In contrast to these methods, our model does not affect phrase extraction and does not require annotated translation rules. Class-based LMs Class-based LMs (Brown et al., 1992) reduce lexical sparsity by placing words in equivalence classes. They have been widely used for speech recognition, but not for MT. Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. They used a classical exchange algorithm for clustering, and learned 512 classes from a large monolingual corpus. Then they mixed the classes into a word-based LM. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. It is unclear if their classes captured agreement information."
P12-1016,P02-1040,0,0.106659,"o reverse the translation direction for each data set, we chose the first English reference as the source and the Arabic as the reference. The NIST sets come in two varieties: newswire (MT02-05) and mixed genre (MT06,08). Newswire contains primarily Modern Standard Arabic (MSA), while the mixed genre data sets also contain transcribed speech and web text. Since the ATB contains MSA, and significant lexical and syntactic differences 152 may exist between MSA and the mixed genres, we achieved best results by tuning on MT04, the largest newswire set. We evaluated translation quality with BLEU-4 (Papineni et al., 2002) and computed statistical significance with the approximate randomization method of Riezler and Maxwell (2005).9 6 Discussion of Translation Results Tbl. 2 shows translation quality results on newswire, while Tbl. 3 contains results for mixed genres. The baseline is our standard system feature set. For comparison, +POS indicates our class-based model trained on the 11 coarse POS tags only (e.g., “Noun”). Finally, +POS+Agr shows the class-based model with the fine-grained classes (e.g., “Noun+Fem+Sg”). The best result—a +1.04 BLEU average gain— was achieved when the class-based model training d"
P12-1016,C04-1081,0,0.0111318,"al functions (emission and transition) Sequence of I target-side predicted classes T dimensional (log) prior distribution over classes Sequence of l word segments Model state: a tagged segment hs, ti Figure 3: Notation used in this paper. The convention eIi indicates a subsequence of a length I sequence. The features are indicators for (character, position, label) triples for a five character window and bigram label transition indicators. This formulation is inspired by the classic “IOB” text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al., 2004). It can be learned from gold-segmented data, generally applies to languages with bound morphemes, and does not require a handcompiled lexicon.3 Moreover, it has only four labels, so Viterbi decoding is very fast. We learn the parameters θseg using a quasi-Newton (QN) procedure with l1 (lasso) regularization (Andrew and Gao, 2007). 2.3 Morpho-syntactic Tagging After segmentation, we tag each segment with a finegrained morpho-syntactic class. For this task we also train a standard CRF model on full sentences with gold classes and segmentation. We use the same QN procedure as before to obtain θt"
P12-1016,petrov-etal-2012-universal,0,0.0061743,"i where e is the target, f is the source, and a is an alignment between the two. The CRF tagging model predicts a target-side class sequence τ ∗ τ ∗ = arg max τ I X θtag · {φo (τi , i, e) + φt (τi , τi−1 )} i=1 where further notation is defined in Fig. 3. 3 Mada, the standard tool for Arabic segmentation (Habash and Rambow, 2005), relies on a manually compiled lexicon. 148 Set of Classes The tagger assigns morpho-syntactic classes, which are coarse POS categories refined with grammatical features such as gender and definiteness. The coarse categories are the universal POS tag set described by Petrov et al. (2012). More than 25 treebanks (in 22 languages) can be automatically mapped to this tag set, which includes “Noun” (nominals), “Verb” (verbs), “Adj” (adjectives), and “ADP” (preand post-positions). Many of these treebanks also contain per-token morphological annotations. It is easy to combine the coarse categories with selected grammatical annotations. For Arabic, we used the coarse POS tags plus definiteness and the so-called phi features (gender,  number, and person).4 For example, èPAJ Ë@ ‘the car’ would be tagged “Noun+Def+Sg+Fem”. We restricted the set of classes to observed combinations in"
P12-1016,W95-0107,0,0.0233906,"ights for the CRF-based segmenter Learned weights for the CRF-based tagger CRF potential functions (emission and transition) Sequence of I target-side predicted classes T dimensional (log) prior distribution over classes Sequence of l word segments Model state: a tagged segment hs, ti Figure 3: Notation used in this paper. The convention eIi indicates a subsequence of a length I sequence. The features are indicators for (character, position, label) triples for a five character window and bigram label transition indicators. This formulation is inspired by the classic “IOB” text chunking model (Ramshaw and Marcus, 1995), which has been previously applied to Chinese segmentation (Peng et al., 2004). It can be learned from gold-segmented data, generally applies to languages with bound morphemes, and does not require a handcompiled lexicon.3 Moreover, it has only four labels, so Viterbi decoding is very fast. We learn the parameters θseg using a quasi-Newton (QN) procedure with l1 (lasso) regularization (Andrew and Gao, 2007). 2.3 Morpho-syntactic Tagging After segmentation, we tag each segment with a finegrained morpho-syntactic class. For this task we also train a standard CRF model on full sentences with gol"
P12-1016,W05-0908,0,0.0585949,"and the Arabic as the reference. The NIST sets come in two varieties: newswire (MT02-05) and mixed genre (MT06,08). Newswire contains primarily Modern Standard Arabic (MSA), while the mixed genre data sets also contain transcribed speech and web text. Since the ATB contains MSA, and significant lexical and syntactic differences 152 may exist between MSA and the mixed genres, we achieved best results by tuning on MT04, the largest newswire set. We evaluated translation quality with BLEU-4 (Papineni et al., 2002) and computed statistical significance with the approximate randomization method of Riezler and Maxwell (2005).9 6 Discussion of Translation Results Tbl. 2 shows translation quality results on newswire, while Tbl. 3 contains results for mixed genres. The baseline is our standard system feature set. For comparison, +POS indicates our class-based model trained on the 11 coarse POS tags only (e.g., “Noun”). Finally, +POS+Agr shows the class-based model with the fine-grained classes (e.g., “Noun+Fem+Sg”). The best result—a +1.04 BLEU average gain— was achieved when the class-based model training data, MT tuning set, and MT evaluation set contained the same genre. We realized smaller, yet statistically sig"
P12-1016,P11-1063,0,0.0236993,"t his classes did not include inflectional features. Target-Side Syntactic LMs Our agreement model is a form of syntactic LM, of which there is a long history of research, especially in speech processing.5 Syntactic LMs have traditionally been too slow for scoring during MT decoding. One exception was the quadratic-time dependency language model presented by Galley and Manning (2009). They applied a quadratic time dependency parser to every hypothesis during decoding. However, to achieve quadratic running time, they permitted ill-formed trees (e.g., parses with multiple roots). More recently, Schwartz et al. (2011) integrated a right-corner, incremental parser into Moses. They showed a large improvement for Urdu-English, but decoding slowed by three orders of magnitude.6 In contrast, our class-based model encodes shallow syntactic information without a noticeable effect on decoding time. Our model can be viewed as a way to score local syntactic relations without extensive decoder modifications. For long-distance relations, Shen et al. (2010) proposed a new decoder that generates target-side dependency trees. The target-side structure enables scoring hypotheses with a trigram dependency LM. 5 Experiments"
P12-1016,J10-4005,0,0.00617658,"every hypothesis during decoding. However, to achieve quadratic running time, they permitted ill-formed trees (e.g., parses with multiple roots). More recently, Schwartz et al. (2011) integrated a right-corner, incremental parser into Moses. They showed a large improvement for Urdu-English, but decoding slowed by three orders of magnitude.6 In contrast, our class-based model encodes shallow syntactic information without a noticeable effect on decoding time. Our model can be viewed as a way to score local syntactic relations without extensive decoder modifications. For long-distance relations, Shen et al. (2010) proposed a new decoder that generates target-side dependency trees. The target-side structure enables scoring hypotheses with a trigram dependency LM. 5 Experiments We first evaluate the Arabic segmenter and tagger components independently, then provide EnglishArabic translation quality results. 5.1 Intrinsic Evaluation of Components Experimental Setup All experiments use the Penn Arabic Treebank (ATB) (Maamouri et al., 2004) parts 1–3 divided into training/dev/test sections according to the canonical split (Rambow et al., 2005).7 5 See (Zhang, 2009) for a comprehensive survey. In principle,"
P12-1016,P11-1024,0,0.0237393,"et al. (2007) noticed that the targetside POS sequences could be scored, much as we do in this work. They used a target-side LM over Combinatorial Categorial Grammar (CCG) supertags, along with a penalty for the number of operator violations, and also modified the phrase probabilities based on the tags. However, Birch et al. (2007) showed that this approach captures the same re-ordering phenomena as lexicalized re-ordering models, which were not included in the baseline. Birch et al. (2007) then investigated source-side CCG supertag features, but did not show an improvement for Dutch-English. Subotin (2011) recently extended factored translation models to hierarchical phrase-based translation and developed a discriminative model for predicting target-side morphology in English-Czech. His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext. In contrast to these methods, our model does not affect phrase extraction and does not require annotated translation rules. Class-based LMs Class-based LMs (Brown et al., 1992) reduce lexical sparsity by placing words in equivalence classes. They have been widely used for speech recognition, but not for MT. Och (199"
P12-1016,N04-4026,0,0.0291669,"Missing"
P12-1016,P08-1059,0,0.226584,"t to handle agreement phenomena. However, LM statistics are sparse, and they are made sparser by morphological variation. For English-to-Arabic translation, we achieve a +1.04 BLEU average improvement by tiling our model on top of a large LM. 146 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146–155, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics It has also been suggested that this setting requires morphological generation because the bitext may not contain all inflected variants (Minkov et al., 2007; Toutanova et al., 2008; Fraser et al., 2012). However, using lexical coverage experiments, we show that there is ample room for translation quality improvements through better selection of forms that already exist in the translation model. 2 A Class-based Model of Agreement 2.1 Morpho-syntactic Agreement Morpho-syntactic agreement refers to a relationship between two sentence elements a and b that must have at least one matching grammatical feature.1 Agreement relations tend to be defined for particular syntactic configurations such as verb-subject, noun-adjective, and pronoun-antecedent. In some languages, agreeme"
P12-1016,P08-1086,0,0.0800468,"et-side morphology in English-Czech. His model benefited from gold morphological annotations on the target-side of the 8M sentence bitext. In contrast to these methods, our model does not affect phrase extraction and does not require annotated translation rules. Class-based LMs Class-based LMs (Brown et al., 1992) reduce lexical sparsity by placing words in equivalence classes. They have been widely used for speech recognition, but not for MT. Och (1999) showed a method for inducing bilingual word classes that placed each phrase pair into a two-dimensional equivalence class. To our knowledge, Uszkoreit and Brants (2008) are the only recent authors to show an improvement in a state-of-the-art MT system using class-based LMs. They used a classical exchange algorithm for clustering, and learned 512 classes from a large monolingual corpus. Then they mixed the classes into a word-based LM. However, both Och (1999) and Uszkoreit and Brants (2008) relied on automatically induced classes. It is unclear if their classes captured agreement information. Monz (2011) recently investigated parameter estimation for POS-based language models, but his classes did not include inflectional features. Target-Side Syntactic LMs O"
P12-1016,C96-2141,0,0.144068,"Missing"
P12-1016,W11-2126,0,0.0387008,"previous approaches to scoring syntactic relations in MT. Unification-based Formalisms Agreement rules impose syntactic and semantic constraints on the structure of sentences. A principled way to model these constraints is with a unification-based grammar (UBG). Johnson (2003) presented algorithms for learning and parsing with stochastic UBGs. However, training data for these formalisms remains extremely limited, and it is unclear how to learn such knowledgerich representations from unlabeled data. One partial 150 solution is to manually extract unification rules from phrase-structure trees. Williams and Koehn (2011) annotated German trees, and extracted translation rules from them. They then specified manual unification rules, and applied a penalty according to the number of unification failures in a hypothesis. In contrast, our class-based model does not require any manual rules and scores similar agreement phenomena as probabilistic sequences. Factored Translation Models Factored translation models (Koehn and Hoang, 2007) facilitate a more data-oriented approach to agreement modeling. Words are represented as a vector of features such as lemma and POS. The bitext is annotated with separate models, and"
P12-1016,E06-1047,0,\N,Missing
P12-2004,P10-1131,0,0.0244978,"Missing"
P12-2004,N10-1083,1,0.923688,"Missing"
P12-2004,W06-2912,0,0.0500733,"Missing"
P12-2004,P05-1033,0,0.19461,"Missing"
P12-2004,N09-1009,0,0.388242,"Missing"
P12-2004,D11-1005,0,0.0230984,"Missing"
P12-2004,D11-1018,1,0.909557,"Missing"
P12-2004,W11-2139,0,0.0473365,"Missing"
P12-2004,N09-1012,0,0.392275,"Missing"
P12-2004,P02-1017,0,0.491187,") The prior over unobserved bracketings PT (B) is fixed to be the uniform distribution over all legal bracketings. The other distributions, PS (·) and PC (·), are multinomials whose isolated parameters are estimated to maximize the likelihood of a set of observed sentences {sn } using EM (Dempster et al., 1977).1 2.1 The Log-Linear CCM A fundamental limitation of the CCM is that it contains a single isolated parameter for every span. The number of different possible span types increases exponentially in span length, leading to data sparsity as the sentence length increases. 1 As mentioned in (Klein and Manning, 2002), the CCM model is deficient because it assigns probability mass to yields and spans that cannot consistently combine to form a valid sentence. Our model does not address this issue, and hence it is similarly deficient. 18 The Log-Linear CCM (LLCCM) reparameterizes the distributions in the CCM using intuitive features to address the limitations of CCM while retaining its predictive power. The set of proposed features includes a BASIC feature for each parameter of the original CCM, enabling the LLCCM to retain the full expressive power of the CCM. In addition, LLCCM contains a set of coarse fea"
P12-2004,P04-1061,0,0.533274,"Missing"
P12-2004,J93-2004,0,0.0436475,"Missing"
P12-2004,D10-1120,0,0.0697336,"Missing"
P12-2004,P92-1017,0,0.608271,"Missing"
P12-2004,P11-1108,0,0.368003,"Missing"
P12-2004,D10-1067,0,0.0249645,"Missing"
P12-2004,P07-1049,0,0.81752,"Missing"
P14-1139,P06-1002,0,0.0974445,"e bidirectional models with soft-penalties to explicitly permit these violations. A Proof of NP-Hardness We can show that the bidirectional alignment problem is NP-hard by reduction from the trav50 0 50 1000 50 100 150 200 250 iteration 300 350 400 (a) The best dual and the best primal score, relative to the optimal score, averaged over all sentence pairs. The best primal curve uses a feasible greedy algorithm, whereas the intersection curve is calculated by taking the intersection of x and y. 1.0 relative search space size ble 2 also compares the models in terms of phraseextraction accuracy (Ayan and Dorr, 2006). We use the phrase extraction algorithm described by DeNero and Klein (2010), accounting for possible links and  alignments. CONS performs better than each of the directional models, but worse than the best D&M model. Finally we consider the impact of constraint addition, pruning, and use of a lower bound. Table 3 gives the average number of constraints added for sentence pairs for which Lagrangian relaxation alone does not produce a certificate. Figure 7(a) shows the average over all sentence pairs of the best dual and best primal scores. The graph compares the use of the greedy algorithm f"
P14-1139,J93-2003,0,0.121881,"Missing"
P14-1139,E09-1020,0,0.0164391,"e heuristic based on the intersection of x and y at the current round of Lagrangian relaxation. Experiments show that running this algorithm significantly improves the lower bound compared to just taking the intersection, and consequently helps pruning significantly. 7 Related Work The most common techniques for bidirectional alignment are post-hoc combinations, such as union or intersection, of directional models, (Och et al., 1999), or more complex heuristic combiners such as grow-diag-final (Koehn et al., 2003). Several authors have explored explicit bidirectional models in the literature. Cromieres and Kurohashi (2009) use belief propagation on a factor graph to train and decode a one-to-one word alignment problem. Qualitatively this method is similar to ours, although the model and decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries"
P14-1139,P10-1147,1,0.888202,"Missing"
P14-1139,P11-1043,1,0.881064,"for building statistical machine translation systems. In order to ensure accurate word alignments, most systems employ a post-hoc symmetrization step to combine directional word aligners, such as IBM Model 4 (Brown et al., 1993) or hidden Markov model (HMM) based aligners (Vogel et al., 1996). Several authors have proposed bidirectional models that incorporate this step directly, but decoding under many bidirectional models is NP-Hard and finding exact solutions has proven difficult. In this paper, we describe a novel Lagrangianrelaxation based decoder for the bidirectional model proposed by DeNero and Macherey (2011), with the goal of improving search accuracy. In that work, the authors implement a dual decomposition-based decoder for the problem, but We begin in Section 2 by formally describing the directional word alignment problem. Section 3 describes a preliminary bidirectional model using full agreement constraints and a Lagrangian relaxation-based solver. Section 4 modifies this model to include adjacency constraints. Section 5 describes an extension to the relaxed algorithm to explicitly enforce constraints, and Section 6 gives a pruning method for improving the efficiency of the algorithm. Experim"
P14-1139,P08-1112,0,0.187295,"Missing"
P14-1139,N06-1015,0,0.118406,"I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in that it does not track the last"
P14-1139,N06-1014,0,0.294672,"d decoding algorithm are different, and their method is not able to provide certificates of optimality. A series of papers by Ganchev et al. (2010), Graca et al. (2008), and Ganchev et al. (2008) use posterior regularization to constrain the posterior probability of the word alignment problem to be symmetric and bijective. This work acheives stateof-the-art performance for alignment. Instead of utilizing posteriors our model tries to decode a single best one-to-one word alignment. A different approach is to use constraints at training time to obtain models that favor bidirectional properties. Liang et al. (2006) propose agreement-based learning, which jointly learns probabilities by maximizing a combination of likelihood and agreement between two directional models. General linear programming approaches have also been applied to word alignment problems. Lacoste-Julien et al. (2006) formulate the word alignment problem as quadratic assignment problem and solve it using an integer linear programming solver. Our work is most similar to DeNero and Macherey (2011), which uses dual decomposition to encourage agreement between two directional HMM aligners during decoding time. 8 Experiments Our experimental"
P14-1139,H05-1011,0,0.0783886,"MM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and weights w (Moore, 2005; Lacoste-Julien et al., 2006). Now reverse the direction of the model and consider the f →e alignment problem. An f →e alignment is a binary vector y ∈ Y where for each j ∈ [J], y(i, j) = 1 for exactly one i ∈ [I]0 . Define the set of HMM alignments Y ⊂ {0, 1}([I]0 ×[J]0 )∪([I]0 ×[I]0 ×[J]) as Y=  y : y(0, 0) = 1,    I  X    y(i, j) = y(i0 , i, j)        y(i, j) = i0 =0 I X ∀i ∈ [I]0 , j ∈ [J], y(i, i0 , j + 1) Similarly define the objective function g(y; ω) = J X I X I X ω(i0 , i, j)y(i0 , i, j) j=1 i=0 i0 =0 1 Our definition differs slightly from other HMM-based aligners in t"
P14-1139,W99-0604,0,0.548881,"Note that unlike an alignment from Y multiple words may be aligned in a column and words may transition from nonaligned positions. Note that for both of these models we can solve the optimization problem exactly using the standard Viterbi algorithm for HMM decoding. The first can be solved in O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this m"
P14-1139,C96-2141,0,0.757889,"ng problem. Given a sentence e of length |e |= I and a sentence f of length |f |= J, our goal is to find the best bidirectional alignment between the two sentences under a given objective function. Before turning to the model of interest, we first introduce directional word alignment. 2.1 Word Alignment In the e→f word alignment problem, each word in e is aligned to a word in f or to the null word . This alignment is a mapping from each index i ∈ [I] to an index j ∈ [J]0 (where j = 0 represents alignment to ). We refer to a single word alignment as a link. A first-order HMM alignment model (Vogel et al., 1996) is an HMM of length I + 1 where the hidden state at position i ∈ [I]0 is the aligned index j ∈ [J]0 , and the transition score takes into account the previously aligned index j 0 ∈ [J]0 .1 Formally, define the set of possible HMM alignments as X ⊂ {0, 1}([I]0 ×[J]0 )∪([I]×[J]0 ×[J]0 ) with where the vector θ ∈ R[I]×[J]0 ×[J]0 includes the transition and alignment scores. For a generative model of alignment, we might define θ(j 0 , i, j) = log(p(ei |fj )p(j|j 0 )). For a discriminative model of alignment, we might define θ(j 0 , i, j) = w · φ(i, j 0 , j, f , e) for a feature function φ and wei"
P14-1139,P09-1104,1,0.884523,"Missing"
P14-1139,N03-1017,0,0.289841,"O(IJ 2 ) time and the second in O(I 2 J) time. 3 Bidirectional Alignment The directional bias of the e→f and f →e alignment models may cause them to produce differing alignments. To obtain the best single alignment, it is common practice to use a post-hoc algorithm to merge these directional alignments (Och et al., 1999). First, a directional alignment is found from each word in e to a word f . Next an alignment is produced in the reverse direction from f to e. Finally, these alignments are merged, either through intersection, union, or with an interpolation algorithm such as grow-diag-final (Koehn et al., 2003). In this work, we instead consider a bidirectional alignment model that jointly considers both directional models. We begin in this section by introducing a simple bidirectional model that enforces full agreement between directional models and giving a relaxation for decoding. Section 4 loosens this model to adjacent agreement. 3.1 Y0 = ∗ x ,y = arg max f (x) + g(y) s.t. x∈X ,y∈Y x(i, j) = y(i, j) ∀i ∈ [I], j ∈ [J]  y : y(0, 0) = 1, P y(i, j) = Ii0 =0 y(i0 , i, j) ∀i ∈ [I]0 , j ∈ [J] Figure 2(b) shows a possible y ∈ Y 0 and a valid unchained structure. To form the Lagrangian dual with relaxe"
P14-2132,W95-0101,0,0.599273,"Missing"
P14-2132,N13-1014,0,0.103188,"ith the part-of-speech of each token in context. Type-supervised tagging (Merialdo, 1994) explores this scenario; a model is provided with type-level information, such as the fact that “only” can be an adjective, adverb, or conjunction, but not any token-level information about which instances of “only” in a corpus are adjectives. Recent research has focused on using type-level supervision to infer token-level tags. For instance, Li et al. (2012) derive type-level supervision from Wiktionary, Das and Petrov (2011) and T¨ackstr¨om et al. (2013) project type-level tag sets across languages, and Garrette and Baldridge (2013) solicit type-level annotations directly from speakers. In all of these efforts, a probabilistic sequence model is trained to disambiguate token-level tags that are 2 Type-Supervised Tagging A first-order Markov model for part-of-speech tagging defines a distribution over sentences for which a single tag is given to each word token. Let wi ∈ W refer to the ith word in a sentence w, drawn from language vocabulary W . Likewise, ∗ Research conducted during an internship at Google. 816 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816"
P14-2132,P08-1085,0,0.0189242,"ing to select a minimal set of parameters that can generate the test sentences, followed by EM to set parameter values. This technique requires the additional information of which sentences will be used for evaluation, and its scalability is limited. In addition, this work used a subset of the WSJ Penn Treebank for training and selecting a tag dictionary. This restriction actually tends to improve performance, because a smaller tag dictionary further constrains model optimization. We compare directly to their training set, kindly provided to us by the authors. The linguistic initialization of Goldberg et al. (2008) is most similar to the current work, in that it estimates maximum likelihood parameters of an HMM using EM, but starting with a wellchosen initialization with language specific linguistic knowledge. That work estimates emission distributions using a combination of suffix morphology rules and corpus context counts. Table 2 compares our results to these related techniques. Each column represents a variant of the experimental setting used in prior work. Smith and Eisner (2005) introduced a mapping from the full 45 tag set of the Penn Treebank to 17 coarse tags. We report results on this coarse s"
P14-2132,P07-1094,0,0.661102,"i |ti ) i=1 3 Above, t0 is a fixed start-of-sentence tag. For a set of sentences S, the EM algorithm can be used to iteratively find a local maximum of the corpus log-likelihood: "" # X X `(φ, θ; S) = ln Pφ,θ (w, t) The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. U NIFORM initializes the model with uniform distributions over allowed outcomes: t w∈S Initializing HMM Taggers The parameters φ and θ can then be used to predict the most"
P14-2132,D07-1031,0,0.0232703,"HMM (Toutanova and Johnson, 2008) Linguistic initialization (Goldberg et al., 2008) Minimal models (Ravi and Knight, 2009) 17 tag set All train 973k train 93.9 94.8 88.7 – 87.3 – 93.4 – 93.8 – – 96.8 Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009). ence.1 Using the full tag set or the full training data, our method offers the best published performance without language-specific assumptions or approximate inference. been motivated empirically for this task (Johnson, 2007). The Bayesian HMM model predicts tag sequences via Gibbs sampling, integrating out model parameters. The Bayesian LDA-based model of Toutanova and Johnson (2008) models ambiguity classes of words, which allows information sharing among words in the tag dictionary. In addition, it incorporates morphology features and a sparse prior of tags for a word. Inference approximations are required to predict tags, integrating out model parameters. Ravi and Knight (2009) employs integer linear programming to select a minimal set of parameters that can generate the test sentences, followed by EM to set p"
P14-2132,D12-1127,0,0.231502,"Missing"
P14-2132,J93-2004,0,0.0454131,"previous tag and θ defines the probability of a word given its tag. Pφ,θ (w, t) = |w| Y Pφ (ti |ti−1 ) · Pθ (wi |ti ) i=1 3 Above, t0 is a fixed start-of-sentence tag. For a set of sentences S, the EM algorithm can be used to iteratively find a local maximum of the corpus log-likelihood: "" # X X `(φ, θ; S) = ln Pφ,θ (w, t) The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. U NIFORM initializes the model with uniform distrib"
P14-2132,J94-2001,0,0.61476,"Missing"
P14-2132,P09-1057,0,0.0214707,"et of sentences S, the EM algorithm can be used to iteratively find a local maximum of the corpus log-likelihood: "" # X X `(φ, θ; S) = ln Pφ,θ (w, t) The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. U NIFORM initializes the model with uniform distributions over allowed outcomes: t w∈S Initializing HMM Taggers The parameters φ and θ can then be used to predict the most likely sequence of tags for each sentence under the model"
P14-2132,P05-1044,0,0.745076,"w, t) = |w| Y Pφ (ti |ti−1 ) · Pθ (wi |ti ) i=1 3 Above, t0 is a fixed start-of-sentence tag. For a set of sentences S, the EM algorithm can be used to iteratively find a local maximum of the corpus log-likelihood: "" # X X `(φ, θ; S) = ln Pφ,θ (w, t) The EM algorithm is sensitive to initialization. In a latent variable model, different parameter values may yield similar data likelihoods but very different predictions. We explore this issue via experiments on the Wall Street Journal section of the English Penn Treebank (Marcus et al., 1993). We adopt the transductive data setting introduced by Smith and Eisner (2005) and used by Goldwater and Griffiths (2007), Toutanova and Johnson (2008) and Ravi and Knight (2009); models are trained on all sections 00-24, the tag dictionary D is constructed by allowing all word-tag pairs appearing in the entire labeled corpus, and the tagging accuracy is evaluated on a 1005 sentence subset sampled from the corpus. The degree of variation in tagging accuracy due to initialization can be observed most clearly by two contrasting initializations. U NIFORM initializes the model with uniform distributions over allowed outcomes: t w∈S Initializing HMM Taggers The parameters φ"
P14-2132,P11-2081,0,0.0217059,"e useful for part-of-speech tagging is well understood (Brill, 1995), but the approach of estimating an initial transition model only from unambiguous word pairs is novel. Our experiments show that for EM-trained HMM taggers in a type-supervised transductive data setting, observational initialization is an effective technique for guiding training toward highaccuracy solutions, approaching the oracle accuracy of S UPERVISED T RANSITIONS initialization. The fact that models with similar data likelihood can vary dramatically in accuracy has been observed in other learning problems. For instance, Toutanova and Galley (2011) show that optimal Related Work We now compare with several previous published results on type-supervised part-of-speech tagging trained using the same data setting on the English WSJ Penn Treebank, introduced by Smith and Eisner (2005). Contrastive estimation (Smith and Eisner, 2005) is a learning technique that approximates the partition function of the EM objective in a log-linear model by considering a neighborhood around observed training examples. The Bayesian HMM of Goldwater and Griffiths (2007) is a secondorder HMM (i.e., likelihood factors over triples of tags) that is estimated usin"
P14-2132,P11-1061,0,\N,Missing
P14-2132,Q13-1001,0,\N,Missing
P16-1007,2005.eamt-1.6,1,0.820554,"ase-based engine, like , die in Ex. 6. Table 7 summarizes how many times each of the systems produced a better output than the other, broken down by category. 8 within a word graph, which is generated by the translation decoder without constraints at the beginning of the workflow. A given prefix is then matched to the paths within the word graph. This approach was recently refined with more permissive matching criteria by Koehn et al. (2014), who report strong improvements in prediction accuracy. Instead of using a word graph, it is also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several"
P16-1007,bernth-mccord-2000-effect,0,0.0631123,"haracters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-Trilles et al., 2008), or source difficulty (Bernth and McCord, 2000) would certainly be relevant for evaluating a full interactive system, but are beyond the scope of this work. 3 Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation candidates are bundled and sorted with respect to each target phrase rather than each source phrase. Crucially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned, each hypothesis from the final target beam is c"
P16-1007,J93-2003,0,0.105246,"Missing"
P16-1007,N10-1062,0,0.168941,"o avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics. We first find the intersection of two directional word alignments. The directional alignments are obtained similar to IBM Model 2 (Brown et al., 1993) by aligning the most likely source word to each target word. Given a source sequence f = f1 . . . f|f | and a target sequence e = e1 . . . e|e |, we define the alignment a = a1 . . . a|e |, where ai = j means th"
P16-1007,buck-etal-2014-n,0,0.0502999,"Missing"
P16-1007,D15-1166,0,0.0345425,"Missing"
P16-1007,D08-1076,0,0.0481463,"se replicas can be interpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix ep and suffix es , then we can generate diverse alternatives in precisely the right location in the target. Let G = (V, E) be the search lattice created by decodin"
P16-1007,P07-1033,0,0.0839266,"Missing"
P16-1007,W15-5003,0,0.0374497,"Missing"
P16-1007,J04-4002,0,0.161353,"d correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned, each hypothesis from the final target beam is copied to an appropriate source beam. Search starts with the lowest-count source beam that contains at least one hypothesis. Here, we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical"
P16-1007,P04-3001,0,0.032026,"also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several different approaches were evaluated. Barrachina et al. (2008) reports experimental results that show the superiority of phrase-based models over stochastic finite state transducers and alignment templates, which were extended for the interactive translation paradigm by Och et al. (2003). Ortiz-Martínez et al. (2009) confirm this observation, and find that their own suggested method using partial statistical phrase-based alignments performs on a similar level on most tasks. The approach using phrase-based models is used as the baseline"
P16-1007,W99-0604,0,0.136312,"arget is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3) (4) (5) (6) 2 We choose cube pruning (Huang and Chiang, 2007) as the beam-filling strategy. 68 Here, cnt(ei , fj ) is the count of all word alignments between ei and fj in the training bitext, and cnt(fj ) the monolingual occurrence count of fj . We perform standard phrase extraction (Och et al., 1999; Koehn et al., 2003) to obtain our synthetic phrases, whose translation probabilities are again estimated based on the single-word probabilities p(ei |fj ) from our translation model. Given a synthetic phrase pair (e, f ), the phrase translation probability is computed as Y p(e|f ) = max p(ei |fj ) (7) 1≤i≤|e| f ∈ φ, one for each d ∈ D. ( f, d ∈ D fd = 0, otherwise. The weights of the replicated feature space are initialized with 0 except for the root domain, where we copy the baseline weights w. ( w, d is root wd = (9) 0, otherwise. All our phrase-based systems are first tuned without prefix"
P16-1007,1997.mtsummit-papers.1,0,0.895889,"n et al. (2014), who report strong improvements in prediction accuracy. Instead of using a word graph, it is also possible to perform a new search for every interaction (Bender et al., 2005; Ortiz-Martínez et al., 2009), which is the approach we have adopted. Ortiz-Martínez et al. (2009) perform the most similar study to our work in the literature. The authors also define prefix decoding as a two-stage process, but focus on investigating different smoothing techniques, while our work includes new metrics, models, and inference. Related Work Target-mediated interactive MT was first proposed by Foster et al. (1997) and then further developed within the TransType (Langlais et al., 2000) and TransType2 (Esteban et al., 2004; Barrachina et al., 2008) projects. In TransType2, several different approaches were evaluated. Barrachina et al. (2008) reports experimental results that show the superiority of phrase-based models over stochastic finite state transducers and alignment templates, which were extended for the interactive translation paradigm by Och et al. (2003). Ortiz-Martínez et al. (2009) confirm this observation, and find that their own suggested method using partial statistical phrase-based alignme"
P16-1007,E03-1032,0,0.449493,"ric is 0 if the first word of es is not also the first word of e∗s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full reference suffix, most reference information is unused in practice. 2.2 Prefix-Bleu (pxBleu): Bleu (Papineni et al., 2002) is computed from the geometric mean of clipped n-gram precisions precn (·, ·) and a brevity Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-T"
P16-1007,W08-0509,0,0.0398275,"ength(v) ≤ P and length(w) > P then 5: Add w to M . Mark node 6: end if 7: v.child = v.child ⊕ w . Child pointer update 8: end for 9: N = [] . n-best target strings 10: for m ∈ M do 11: Add target(m) to N 12: end for 13: return N 7 We evaluate our models and methods for EnglishFrench and English-German on two domains: software and news. The phrase-based systems are built with Phrasal (Green et al., 2014), an open source toolkit. We use a dynamic phrase table (Levenberg et al., 2010) and tune parameters with AdaGrad. All systems have 42 dense baseline features. We align the bitexts with mgiza (Gao and Vogel, 2008) and estimate 5-gram language models (LMs) with KenLM (Heafield et al., 2013). The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 (Bojar et al., 2015). The LM was estimated from the target side of the bitext. For English-German we run large-scale experiments. The bitext contains 19.9M parallel segments collected from WMT 2015 and the OPUS collection (Skadin¸š et al., 2014). The LM was estimated from the target side of the bitext and the monolingual Common Crawl corpus (Buck et al., 2014), altogether 37.2B running"
P16-1007,R09-1060,0,0.122015,"∈ Rd is a feature map, and Z(f ) is an appropriate normalizing constant. For the same model, the distribution over suffixes es ∈ E must also condition on a prefix ep ∈ E: h i X 1 p(es |ep , f ; w) = exp w> φ(r) (2) Z(f ) r: src(r)=f tgt(r)=ep es In phrase-based decoding, the best scoring derivation r given a source sentence f and weights w is found efficiently by beam search, with one beam for every count of source words covered by a partial derivation (known as the source coverage cardinality). To predict a suffix conditioned on a prefix by constrained decoding, Barrachina et al. (2008) and Ortiz-Martínez et al. (2009) modify the beam search by discarding hypotheses (partial derivations) that do not match the prefix ep . We propose target beam search, a two-step inference procedure. The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3)"
P16-1007,D13-1111,0,0.039975,"rpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can find all edges in the decoding lattice that span the prefix ep and suffix es , then we can generate diverse alternatives in precisely the right location in the target. Let G = (V, E) be the search lattice created by decoding, where V are nodes a"
P16-1007,N10-1079,0,0.0838326,"y of the suffix. Each metric takes example triples (f, ep , e∗ ) produced during an interactive MT session in which ep was generated in the process of constructing e∗ . A simulated corpus of examples can be produced from a parallel corpus of (f, e∗ ) pairs by selecting prefixes of each e∗ . An exhaustive simulation selects all possible prefixes, while a sampled simulation selects only k prefixes uniformly at random for each e∗ . Computing metrics for exhaustive simulations is expensive because it requires performing suffix prediction inference for every prefix: |e∗ | times for each reference. Ortiz-Martínez et al. (2010) use BLEU directly for training an interactive system, but we propose a variant that only scores the predicted suffix and not the input prefix. The pxBleu metric comˆ E ˆ ∗ ) for the following constructed putes Bleu(E, ˆ ˆ ∗: sequences E and E • For each (f, ep , e∗ ) and suffix prediction es , ˆ includes the full sentence e = ep es . E ˆ ∗ is a masked copy of • For each (f, ep , e∗ ), E e∗ in which all prefix words that do not match any word in e are replaced by null tokens. This construction maintains the original computation of the brevity penalty, but does not include the prefix in the pre"
P16-1007,P13-1031,1,0.856739,"baseline translation quality and only update the weights corresponding to the prefix, overlap and suffix domains. 1≤j≤|f | Additionally, we introduce three indicator features that count the number of synthetic phrase pairs, source words and target words, respectively. 4 (8) Tuning 5 In order to tune the model for suffix prediction, we optimize the weights w in Equation 2 to maximize the metrics introduced in Section 2. Model tuning is performed with AdaGrad (Duchi et al., 2011), an online subgradient method. It features an adaptive learning rate and comes with good theoretical guarantees. See Green et al. (2013) for the details of applying AdaGrad to phrase-based translation. The same model scores both alignment of the prefix and translation of the suffix. However, different feature weights may be appropriate for scoring each step of the inference process. In order to learn different weights for alignment and translation within a unified joint model, we apply the hierarchical adaptation method of Wuebker et al. (2015), which is based on frustratingly easy domain adaptation (FEDA) (Daumé III, 2007). We define three sub-segment domains: prefix, overlap and suffix. The prefix domain contains all phrases"
P16-1007,P02-1040,0,0.0971621,"ic, learning can focus on these words while using all available information in the references. Number of Predicted Words (#prd) is the maximum number of contiguous words at the start of the predicted suffix that match the reference. Like WPA, this metric is 0 if the first word of es is not also the first word of e∗s . In a sampled simulation, all reference words that follow the first mis-predicted word in the sampled suffix are ignored. While it is possible that the metric will require the full reference suffix, most reference information is unused in practice. 2.2 Prefix-Bleu (pxBleu): Bleu (Papineni et al., 2002) is computed from the geometric mean of clipped n-gram precisions precn (·, ·) and a brevity Keystroke Ratio (KSR) In addition to these metrics, suffix prediction can be evaluated by the widely used keystroke ratio (KSR) metric (Och et al., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quan"
P16-1007,W14-3311,1,0.844197,"e, the most probable word ei is passed to the next time step. Require: Lattice G = (V, E), prefix length P 1: M = [] . Marked nodes 2: for w ∈ V in reverse topological order do 3: v = parent(w) . v, w ∈ E 4: if length(v) ≤ P and length(w) > P then 5: Add w to M . Mark node 6: end if 7: v.child = v.child ⊕ w . Child pointer update 8: end for 9: N = [] . n-best target strings 10: for m ∈ M do 11: Add target(m) to N 12: end for 13: return N 7 We evaluate our models and methods for EnglishFrench and English-German on two domains: software and news. The phrase-based systems are built with Phrasal (Green et al., 2014), an open source toolkit. We use a dynamic phrase table (Levenberg et al., 2010) and tune parameters with AdaGrad. All systems have 42 dense baseline features. We align the bitexts with mgiza (Gao and Vogel, 2008) and estimate 5-gram language models (LMs) with KenLM (Heafield et al., 2013). The English-French bilingual training data consists of 4.9M sentence pairs from the Common Crawl and Europarl corpora from WMT 2015 (Bojar et al., 2015). The LM was estimated from the target side of the bitext. For English-German we run large-scale experiments. The bitext contains 19.9M parallel segments co"
P16-1007,D08-1051,0,0.0885272,"l., 2003). This ratio assumes that 67 any number of characters from the beginning of the suggested suffix can be appended to the user prefix using a single keystroke. It computes the ratio of key strokes required to enter the reference interactively to the character count of the reference. Our MT architecture does not permit tuning to KSR. Other methods of quantifying effort in an interactive MT system are more appropriate for user studies than for direct evaluation of MT predictions. For example, measuring pupil dilation, pause duration and frequency (Schilperoord, 1996), mouse-action ratio (Sanchis-Trilles et al., 2008), or source difficulty (Bernth and McCord, 2000) would certainly be relevant for evaluating a full interactive system, but are beyond the scope of this work. 3 Therefore, each beam contains hypotheses for a fixed prefix of target words. Phrasal translation candidates are bundled and sorted with respect to each target phrase rather than each source phrase. Crucially, the source distortion limit is not enforced during alignment, so that long-range reorderings can be analyzed correctly. The second step generates the suffix using standard beam search.2 Once the target prefix is completely aligned,"
P16-1007,P13-2121,0,0.0528998,"Missing"
P16-1007,P07-1019,0,0.0476989,"ypotheses (partial derivations) that do not match the prefix ep . We propose target beam search, a two-step inference procedure. The first step is to produce a phrase-based alignment between the target prefix and a subset of the source words. The target is aligned left-to-right by appending aligned phrase pairs. However, each beam is associated with a target word count, rather than a source word count. ai = arg max p(ai = j|f, e) j∈{1,...,|f |} p(ai = j|f, e) = p(ei |fj ) · p(ai |j) cnt(ei , fj ) p(ei |fj ) = cnt(fj ) p(ai |j) = Poisson(|ai − j|, 1.0) (3) (4) (5) (6) 2 We choose cube pruning (Huang and Chiang, 2007) as the beam-filling strategy. 68 Here, cnt(ei , fj ) is the count of all word alignments between ei and fj in the training bitext, and cnt(fj ) the monolingual occurrence count of fj . We perform standard phrase extraction (Och et al., 1999; Koehn et al., 2003) to obtain our synthetic phrases, whose translation probabilities are again estimated based on the single-word probabilities p(ei |fj ) from our translation model. Given a synthetic phrase pair (e, f ), the phrase translation probability is computed as Y p(e|f ) = max p(ei |fj ) (7) 1≤i≤|e| f ∈ φ, one for each d ∈ D. ( f, d ∈ D fd = 0,"
P16-1007,skadins-etal-2014-billions,0,0.0423715,"Missing"
P16-1007,N03-1017,0,0.391558,"we re-instate the distortion limit with the following modification to avoid search failures: The decoder can always translate any source position before the last source position that was covered in the alignment phase. Phrase-Based Inference In the log-linear approach to phrase-based translation (Och and Ney, 2004), the distribution of translations e ∈ E given a source sentence f ∈ F is: h i X 1 p(e|f ; w) = exp w> φ(r) (1) Z(f ) r: 3.1 Synthetic Phrase Pairs The phrase pairs available during decoding may not be sufficient to align the target prefix to the source. Pre-compiled phrase tables (Koehn et al., 2003) are typically pruned, and dynamic phrase tables (Levenberg et al., 2010) require sampling for efficient lookup. To improve alignment coverage, we include additional synthetic phrases extracted from word-level alignments between the source sentence and target prefix inferred using unpruned lexical statistics. We first find the intersection of two directional word alignments. The directional alignments are obtained similar to IBM Model 2 (Brown et al., 1993) by aligning the most likely source word to each target word. Given a source sequence f = f1 . . . f|f | and a target sequence e = e1 . . ."
P16-1007,P14-2094,0,0.541798,"r boundary conditions, the reference e∗ is masked by the prefix ep as follows: we replace each of the first |ep − 3 |words with a null token enull , unless the word also appears in the suffix e∗s . Masking retains the last three words of the prefix so that the first words after the prefix can contribute to the precision of all n-grams that overlap with the prefix, up to n = 4. Words that also appear in the suffix are retained so that their correct prediction in the suffix can contribute to those precisions, which would otherwise be clipped. Word Prediction Accuracy (WPA) or nextword accuracy (Koehn et al., 2014) is 1 if the first word of the predicted suffix es is also the first word of reference suffix e∗s , and 0 otherwise. Averaging over examples gives the frequency that the word following the prefix was predicted correctly. In a sampled simulation, all reference words that follow the first word of a sampled suffix are ignored by the metric, so most reference information is unused. 2.1 Loss Functions for Learning All of these metrics can be used as the tuning objective of a phrase-based machine translation system. Tuning toward a sampled simulation that includes one or two prefixes per reference i"
P16-1007,W00-0507,0,0.507119,"sed to translate the remainder of the sentence. The root domain spans the entire phrasal derivation. Formally, given a set of domains D = {root, prefix, overlap, suffix}, each feature is replicated for each domain d ∈ D. These replicas can be interpreted as domain-specific “offsets” to the baseline weights. For an original feature vector φ with a set of domains D ⊆ D, the replicated feature vector contains |D |copies fd of each feature Diverse n-best Extraction Consider the interactive MT application setting in which the user is presented with an autocomplete list of alternative translations (Langlais et al., 2000). The user query may be satisfied if the machine predicts the correct completion in its top-n output. However, it is well-known that n-best lists are poor approximations of MT structured output spaces (Macherey et al., 2008; Gimpel et al., 2013). Even very large values of n can fail to produce alternatives that differ in the first words of the suffix, which limits n-best KSR and WPA improvements at test time. For tuning, WPA is often zero for every item on the n-best list, which prevents learning. Fortunately, the prefix can help efficiently enumerate diverse next-word alternatives. If we can"
P16-1007,D15-1123,1,0.82902,"Section 2. Model tuning is performed with AdaGrad (Duchi et al., 2011), an online subgradient method. It features an adaptive learning rate and comes with good theoretical guarantees. See Green et al. (2013) for the details of applying AdaGrad to phrase-based translation. The same model scores both alignment of the prefix and translation of the suffix. However, different feature weights may be appropriate for scoring each step of the inference process. In order to learn different weights for alignment and translation within a unified joint model, we apply the hierarchical adaptation method of Wuebker et al. (2015), which is based on frustratingly easy domain adaptation (FEDA) (Daumé III, 2007). We define three sub-segment domains: prefix, overlap and suffix. The prefix domain contains all phrases that are used for aligning the prefix with the source sentence. Phrases that span both prefix and suffix additionally belong to the overlap domain. Finally, once the prefix has been completely covered, the suffix domain applies to all phrases that are used to translate the remainder of the sentence. The root domain spans the entire phrasal derivation. Formally, given a set of domains D = {root, prefix, overlap"
W06-3105,N03-1017,0,0.0553999,"al difference is that distinct word alignments cannot all be correct, while distinct segmentations can. Alternate segmentations rather than alternate alignments compete, resulting in increased determinization of the phrase table, decreased generalization, and decreased final BLEU score. We also show that interpolation of the two methods can result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IB"
W06-3105,N03-2036,0,0.0294915,"Missing"
W06-3105,J03-1002,0,0.0185038,"Missing"
W06-3105,W99-0604,0,0.573162,"Missing"
W06-3105,2002.tmi-tutorials.2,0,0.0640769,"result in a modest increase in BLEU score. 1 Introduction At the core of a phrase-based statistical machine translation system is a phrase table containing pairs of source and target language phrases, each weighted by a conditional translation probability. Koehn et al. (2003a) showed that translation quality is very sensitive to how this table is extracted from the training data. One particularly surprising result is that a simple heuristic extraction algorithm based on surface statistics of a word-aligned training set outperformed the phrase-based generative model proposed by Marcu and Wong (2002). This result is surprising in light of the reverse situation for word-based statistical translation. Specifically, in the task of word alignment, heuristic approaches such as the Dice coefficient consistently underperform their re-estimated counterparts, such as the IBM word alignment models (Brown et al., 1993). This well-known result is unsurprising: reestimation introduces an element of competition into the learning process. The key virtue of competition in word alignment is that, to a first approximation, only one source word should generate each target word. If a good alignment for a wor"
W06-3105,koen-2004-pharaoh,0,\N,Missing
W06-3105,J93-2003,0,\N,Missing
W16-6637,J93-2004,0,0.0530169,"layer size, and backpropagation through time (BPTT) steps. Classes are used to factor the vocabulary mappings to improve performance, by predicting a distribution over classes of words and then over words in a class (Mikolov et al., 2011). BPTT steps determine how many times the recurrent layer of the network is unwrapped for training. Unless otherwise mentioned all neural models have class of 100 and use four BPTT steps. We use the Penn Tree Bank (PTB), constructed from articles from the Wall Street Journal, as our primary training corpus, with the standard training split of 42068 sentences (Marcus et al., 1993). Correspondingly, our generated language corpora also contain 42068 sentences. Novel sentences are easily sampled from trained language models by prompting with a start of sentence token, 227 Proceedings of The 9th International Natural Language Generation conference, pages 227–231, c Edinburgh, UK, September 5-8 2016. 2016 Association for Computational Linguistics Corpus Trigram 5-gram Neural Hidden 100 Neural Hidden 500 Sum of Error 27736 29694 19237 14132 Table 1: Sum of errors for sentence lengths, including normalized over total sentences. Figure 1: Sentence Length Distributions sampling"
