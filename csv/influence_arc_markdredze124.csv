2020.acl-main.720,P19-1299,0,0.0203621,"target language. Rahimi et al. (2019) do this on a massive scale for NER, leveraging over 40 languages for crosslingual transfer. Xie et al. (2018) employed selfattention to combat word-order differences when transferring parameters from high-resource languages to low-resource. Much work in this space has looked at how to leverage a mixture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019). These works show, similar to our work, that it is possible to leverage multilingual data to increase performance across languages. 3 Models We evaluate three polyglot NER neural models.1 3.1 Word Level CRF The Neural (BiLSTM) CRF is a standard model for sequence labeling tasks (Ma and Hovy, 2016; Durrett and Klein, 2015). Our implementation 1 We release the code for these models at https:// github.com/davidandym/multilingual-NER 8094 Model Eng Deu Nld Spa Avg Amh Ara Fas Hin Hun Ind Som Swa Tgl Vie Avg 84.91 83.38 86.49 71.39 70.86 72.95 78.96 79.38 80.91 82.60 81.64 82."
2020.acl-main.720,P19-4007,0,0.0663393,"Missing"
2020.acl-main.720,I17-2016,0,0.016977,"ounterparts. To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer parameters than multiple mono"
2020.acl-main.720,W99-0612,0,0.272072,". In particular, transfer can occur when there is high lexical overlap or closely related languages in the polyglot training set. • §5.3 Languages share a large number of important parameters between each other in polyglot models, and fine-tuning may utilize those parameters to strengthen it’s performance. To our knowledge, ours is the first systematic study of polyglot NER models. 2 Related Work There is a long history of multilingual learning for NER (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012). This work has is driven by an interest in learning NER models for many languages (Cucerzan and Yarowsky, 1999; Pan et al., 2017a) and the relative lack of data for many languages of interest (Das et al., 2017). Polyglot Models Johnson et al. (2017) and Lee et al. (2017) showed that a single neural MT model could benefit from being trained in a multilingual setting. Gillick et al. (2016) showed similar results for NER, presenting a model that benefited from learning to perform NER on 4 languages at once. We find that other polyglot NER models are rarely better than monolingual models in terms of absolute performance. Mulcaire et al. (2019) showed that polyglot language model pretraining can help impro"
2020.acl-main.720,P07-1033,0,0.202194,"Missing"
2020.acl-main.720,P15-1030,0,0.0228283,"ture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019). These works show, similar to our work, that it is possible to leverage multilingual data to increase performance across languages. 3 Models We evaluate three polyglot NER neural models.1 3.1 Word Level CRF The Neural (BiLSTM) CRF is a standard model for sequence labeling tasks (Ma and Hovy, 2016; Durrett and Klein, 2015). Our implementation 1 We release the code for these models at https:// github.com/davidandym/multilingual-NER 8094 Model Eng Deu Nld Spa Avg Amh Ara Fas Hin Hun Ind Som Swa Tgl Vie Avg 84.91 83.38 86.49 71.39 70.86 72.95 78.96 79.38 80.91 82.60 81.64 82.72 79.45 77.85 80.82 60.62 59.39 59.86 43.22 43.25 44.69 45.11 43.20 46.85 62.12 62.88 68.30 60.47 60.86 65.21 62.14 64.59 67.15 61.75 65.45 66.11 68.04 68.32 70.07 84.13 84.80 87.03 47.31 49.71 51.80 59.49 59.87 62.71 85.75 83.79 86.68 71.42 71.54 73.02 78.36 79.43 80.09 81.19 80.25 82.95 79.18 78.75 80.69 59.13 57.03 59.37 44.95 42.88 42.69"
2020.acl-main.720,R11-1017,0,0.0745819,"Missing"
2020.acl-main.720,N16-1155,0,0.191588,"al model. In the context of named entity recognition, we may expect aspects of the task to transfer across languages. For example, since entity names tend to be transliterated or directly used across languages, even distant languages may see benefit from training a single model, e.g. “Apple” (company) is rendered as such in French rather than as “Pomme.” Intuitively, the more similar and the larger the set of languages, the more we should expect to see a benefit from considering them jointly. These polyglot models can take advantage of different sets of labeled corpora in different languages (Gillick et al., 2016; Mulcaire et al., 2019). Nevertheless, progress towards this goal remains mixed; polyglot models often do not improve results in each language (Mulcaire et al., 2019; Kondratyuk and Straka, 2019; Upadhyay et al., 2018; Conneau et al., 2019). Models trained across all languages come close but typically fail to outperform monolingual models. Thus, while multilingual learning can benefit low resource languages through transfer and simplify models by sharing one across all languages, it fails to realize a key goal: improving results in each language. Our experiments in §4 confirm this negative re"
2020.acl-main.720,P84-1044,0,0.297604,"Missing"
2020.acl-main.720,N19-1383,0,0.0166035,"himi et al. (2019) do this on a massive scale for NER, leveraging over 40 languages for crosslingual transfer. Xie et al. (2018) employed selfattention to combat word-order differences when transferring parameters from high-resource languages to low-resource. Much work in this space has looked at how to leverage a mixture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019). These works show, similar to our work, that it is possible to leverage multilingual data to increase performance across languages. 3 Models We evaluate three polyglot NER neural models.1 3.1 Word Level CRF The Neural (BiLSTM) CRF is a standard model for sequence labeling tasks (Ma and Hovy, 2016; Durrett and Klein, 2015). Our implementation 1 We release the code for these models at https:// github.com/davidandym/multilingual-NER 8094 Model Eng Deu Nld Spa Avg Amh Ara Fas Hin Hun Ind Som Swa Tgl Vie Avg 84.91 83.38 86.49 71.39 70.86 72.95 78.96 79.38 80.91 82.60 81.64 82.72 79.45 77.85 80.82"
2020.acl-main.720,W18-2705,0,0.0164019,"language. Our experiments in §4 confirm this negative result in two different multilingual settings for 4 different neural NER models. Our first contribution is a technique in which a polyglot NER model can be adapted to a target language by fine-tuning on monolingual data. 8093 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8093–8104 c July 5 - 10, 2020. 2020 Association for Computational Linguistics A similar continued training approach to transfer has been explored for domain adaptation in neural machine translation (Luong and Manning, 2015; Khayrallah et al., 2018); we show that it works with polyglot models for NER, improving performance by up to 3 F1 over monolingual baselines. Our second contribution is an explanation of the surprising effectiveness of this technique through an extensive empirical study of polyglot models for NER. We compare several types of neural NER models, including three character (or byte) level architectures, and evaluate transfer across a small (4) and large (10) set of languages. In particular, we find that: • §4 Other than Byte-to-Span (BTS; Gillick et al., 2016), most NER architectures do not benefit from polyglot training"
2020.acl-main.720,D17-1302,0,0.0228301,"n how we can leverage polyglot learning to improve performance across all languages. Cross-lingual Models Cross-lingual transfer leverages labeled data from different source languages to augment data for a target language. Rahimi et al. (2019) do this on a massive scale for NER, leveraging over 40 languages for crosslingual transfer. Xie et al. (2018) employed selfattention to combat word-order differences when transferring parameters from high-resource languages to low-resource. Much work in this space has looked at how to leverage a mixture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019). These works show, similar to our work, that it is possible to leverage multilingual data to increase performance across languages. 3 Models We evaluate three polyglot NER neural models.1 3.1 Word Level CRF The Neural (BiLSTM) CRF is a standard model for sequence labeling tasks (Ma and Hovy, 2016; Durrett and Klein, 2015). Our implementation 1 We release the code for th"
2020.acl-main.720,P12-1073,0,0.201332,"ructure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer parameters than multiple monolingual models. Additionally, they can enable multilingual transfer (Devlin, 2018; Wu and Dredze, 2019; Pires et al., 2019). Howe"
2020.acl-main.720,D19-1279,0,0.0998311,"used across languages, even distant languages may see benefit from training a single model, e.g. “Apple” (company) is rendered as such in French rather than as “Pomme.” Intuitively, the more similar and the larger the set of languages, the more we should expect to see a benefit from considering them jointly. These polyglot models can take advantage of different sets of labeled corpora in different languages (Gillick et al., 2016; Mulcaire et al., 2019). Nevertheless, progress towards this goal remains mixed; polyglot models often do not improve results in each language (Mulcaire et al., 2019; Kondratyuk and Straka, 2019; Upadhyay et al., 2018; Conneau et al., 2019). Models trained across all languages come close but typically fail to outperform monolingual models. Thus, while multilingual learning can benefit low resource languages through transfer and simplify models by sharing one across all languages, it fails to realize a key goal: improving results in each language. Our experiments in §4 confirm this negative result in two different multilingual settings for 4 different neural NER models. Our first contribution is a technique in which a polyglot NER model can be adapted to a target language by fine-tuni"
2020.acl-main.720,C16-1087,0,0.126525,"s similar to the one described above, with the key difference being that word-level representation are obtained using a pretrained subword-level BERT model, as opposed to being built from raw characters/bytes. As is done in the original BERT paper, 2 Early experiments found these models suffered much less from multilingual training than subword/word models. we treat the representation of the first subword of each word as a representation for that word, and take the concatenation of the outputs of the last 4 layers at that subword position as our final word representation. 3.2 CharNER CharNER (Kuru et al., 2016) is a deep neural sequence labeling architecture which operates strictly at the character level during training, but uses word-level boundaries during inference. The model runs a 5-layer BiLSTM over sequences of characters, and is trained to predict the NER tag for each character of the sequence (without BIO labels). During inference a Viterbi decoder with untrained transition parameters enforces consistent character level tags across each word; no heuristics and little post-processing is necessary to obtain word-level BIO labels. To compare with the other architectures, we apply this model to"
2020.acl-main.720,N16-1030,0,0.0127649,"ingual, multilingual, and finetuned models trained on either CoNLL (left) or LORELEI (right) data sets. The results are taken from the best model out of 5 random seeds, as measured by dev performance. Almost every model achieves the best performance in the finetuned setting, indicating that multilingual pretraining is learning transferable parameters, but multilingual models are not able to use them effectively across all languages simultaneously. Note that we do not evaluate Amharic with mBERT, because the Amharic script is not a part of mBERT’s vocabulary. broadly follows the description in Lample et al. (2016), and we consider three different variants of this model. The first two are character- and byte-level models.2 We consider these since Gillick et al. (2016) showed that multilingual transfer could occur across byte-level representations and we were interested in whether characters produced similar results when more diverse languages were involved. Each word passes through a multi-layer BiLSTM as a sequence of characters or bytes to produce word-level representations. Word-level representations feed into a sentence-level BiLSTM, which outputs, for each time step, logits for all possible labels."
2020.acl-main.720,Q17-1026,0,0.0311195,"important parameters between each other in polyglot models, and fine-tuning may utilize those parameters to strengthen it’s performance. To our knowledge, ours is the first systematic study of polyglot NER models. 2 Related Work There is a long history of multilingual learning for NER (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012). This work has is driven by an interest in learning NER models for many languages (Cucerzan and Yarowsky, 1999; Pan et al., 2017a) and the relative lack of data for many languages of interest (Das et al., 2017). Polyglot Models Johnson et al. (2017) and Lee et al. (2017) showed that a single neural MT model could benefit from being trained in a multilingual setting. Gillick et al. (2016) showed similar results for NER, presenting a model that benefited from learning to perform NER on 4 languages at once. We find that other polyglot NER models are rarely better than monolingual models in terms of absolute performance. Mulcaire et al. (2019) showed that polyglot language model pretraining can help improve performance on NER tasks, although polyglot NER training hurts. However, multilingual BERT (Devlin et al., 2018b), when compared to monolingual BERT performan"
2020.acl-main.720,2015.iwslt-evaluation.11,0,0.0254267,"improving results in each language. Our experiments in §4 confirm this negative result in two different multilingual settings for 4 different neural NER models. Our first contribution is a technique in which a polyglot NER model can be adapted to a target language by fine-tuning on monolingual data. 8093 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8093–8104 c July 5 - 10, 2020. 2020 Association for Computational Linguistics A similar continued training approach to transfer has been explored for domain adaptation in neural machine translation (Luong and Manning, 2015; Khayrallah et al., 2018); we show that it works with polyglot models for NER, improving performance by up to 3 F1 over monolingual baselines. Our second contribution is an explanation of the surprising effectiveness of this technique through an extensive empirical study of polyglot models for NER. We compare several types of neural NER models, including three character (or byte) level architectures, and evaluate transfer across a small (4) and large (10) set of languages. In particular, we find that: • §4 Other than Byte-to-Span (BTS; Gillick et al., 2016), most NER architectures do not bene"
2020.acl-main.720,P16-1101,0,0.0359604,"w to leverage a mixture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019). These works show, similar to our work, that it is possible to leverage multilingual data to increase performance across languages. 3 Models We evaluate three polyglot NER neural models.1 3.1 Word Level CRF The Neural (BiLSTM) CRF is a standard model for sequence labeling tasks (Ma and Hovy, 2016; Durrett and Klein, 2015). Our implementation 1 We release the code for these models at https:// github.com/davidandym/multilingual-NER 8094 Model Eng Deu Nld Spa Avg Amh Ara Fas Hin Hun Ind Som Swa Tgl Vie Avg 84.91 83.38 86.49 71.39 70.86 72.95 78.96 79.38 80.91 82.60 81.64 82.72 79.45 77.85 80.82 60.62 59.39 59.86 43.22 43.25 44.69 45.11 43.20 46.85 62.12 62.88 68.30 60.47 60.86 65.21 62.14 64.59 67.15 61.75 65.45 66.11 68.04 68.32 70.07 84.13 84.80 87.03 47.31 49.71 51.80 59.49 59.87 62.71 85.75 83.79 86.68 71.42 71.54 73.02 78.36 79.43 80.09 81.19 80.25 82.95 79.18 78.75 80.69 59.13 57.0"
2020.acl-main.720,W03-1504,0,0.191081,"Missing"
2020.acl-main.720,D17-1269,0,0.0218136,"m their monolingual counterparts. To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer para"
2020.acl-main.720,N19-1392,0,0.129554,"xt of named entity recognition, we may expect aspects of the task to transfer across languages. For example, since entity names tend to be transliterated or directly used across languages, even distant languages may see benefit from training a single model, e.g. “Apple” (company) is rendered as such in French rather than as “Pomme.” Intuitively, the more similar and the larger the set of languages, the more we should expect to see a benefit from considering them jointly. These polyglot models can take advantage of different sets of labeled corpora in different languages (Gillick et al., 2016; Mulcaire et al., 2019). Nevertheless, progress towards this goal remains mixed; polyglot models often do not improve results in each language (Mulcaire et al., 2019; Kondratyuk and Straka, 2019; Upadhyay et al., 2018; Conneau et al., 2019). Models trained across all languages come close but typically fail to outperform monolingual models. Thus, while multilingual learning can benefit low resource languages through transfer and simplify models by sharing one across all languages, it fails to realize a key goal: improving results in each language. Our experiments in §4 confirm this negative result in two different mu"
2020.acl-main.720,P18-2064,0,0.0203523,"s with 320 hidden units, byte dropout of 3.0 and layer dropout of 5.0. 8096 we do not correct for minor size differences.6 All models were trained over 5 random seeds, with the best model selected by development performance. For polyglot models, we select the best model using the average development performance across all languages. Results Table 1 reports test performance. With few exceptions, polyglot training does worse than monolingual. In some cases, the two settings do nearly the same (such as Character and mBERT CRFs on LORELEI) but we do not see improved results from a polyglot model. Murthy et al. (2018) found that languages with different label distributions do worse for transfer. We find large label distribution changes in CoNLL, but not LORELEI. To determine if this could explain polyglot NER failures in CoNLL, we allow our CRF models to learn languagespecific label distributions via language-specific CRF transition parameters. However, we saw little difference in the results for either CoNLL or LORELEI (no more than 0.5 F1 on any language). This suggests that other factors are preventing more language transfer. The exception to these observations is the BTS model, which showed significant"
2020.acl-main.720,P17-1135,0,0.0541652,"Missing"
2020.acl-main.720,P17-1178,0,0.382022,"occur when there is high lexical overlap or closely related languages in the polyglot training set. • §5.3 Languages share a large number of important parameters between each other in polyglot models, and fine-tuning may utilize those parameters to strengthen it’s performance. To our knowledge, ours is the first systematic study of polyglot NER models. 2 Related Work There is a long history of multilingual learning for NER (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012). This work has is driven by an interest in learning NER models for many languages (Cucerzan and Yarowsky, 1999; Pan et al., 2017a) and the relative lack of data for many languages of interest (Das et al., 2017). Polyglot Models Johnson et al. (2017) and Lee et al. (2017) showed that a single neural MT model could benefit from being trained in a multilingual setting. Gillick et al. (2016) showed similar results for NER, presenting a model that benefited from learning to perform NER on 4 languages at once. We find that other polyglot NER models are rarely better than monolingual models in terms of absolute performance. Mulcaire et al. (2019) showed that polyglot language model pretraining can help improve performance on"
2020.acl-main.720,P19-1493,0,0.023905,"atasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer parameters than multiple monolingual models. Additionally, they can enable multilingual transfer (Devlin, 2018; Wu and Dredze, 2019; Pires et al., 2019). However, a key goal of polyglot learning concerns producing a single model that does better on each language than a monolingual model. In the context of named entity recognition, we may expect aspects of the task to transfer across languages. For example, since entity names tend to be transliterated or directly used across languages, even distant languages may see benefit from training a single model, e.g. “Apple” (company) is rendered as such in French rather than as “Pomme.” Intuitively, the more similar and the larger the set of languages, the more we should expect to see a benefit from c"
2020.acl-main.720,D19-1077,1,0.873535,"his phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer parameters than multiple monolingual models. Addit"
2020.acl-main.720,D18-1034,0,0.086803,"stently and significantly outperform their monolingual counterparts. To explain this phenomena, we explore the sources of multilingual transfer in polyglot NER models and examine the weight structure of polyglot models compared to their monolingual counterparts. We find that polyglot models efficiently share many parameters across languages and that fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They c"
2020.acl-main.720,P19-1015,0,0.0387602,"t polyglot pretraining is not always beneficial for downstream tasks. Finally, most recently, Kondratyuk and Straka (2019) showed how to train a single model on 75 languages for dependency parsing while retaining competitive performance or improving performance, mostly on low-resource languages. This work is closely related to ours, although we are predominantly interested in how we can leverage polyglot learning to improve performance across all languages. Cross-lingual Models Cross-lingual transfer leverages labeled data from different source languages to augment data for a target language. Rahimi et al. (2019) do this on a massive scale for NER, leveraging over 40 languages for crosslingual transfer. Xie et al. (2018) employed selfattention to combat word-order differences when transferring parameters from high-resource languages to low-resource. Much work in this space has looked at how to leverage a mixture of shared features and language-specific features (Kim et al., 2017), similar to domain adaptation techniques Daum´e III (2007). Recently, a lot of this work has focused on using adversarial models to force models to learn language-agnostic feature spaces (Chen et al., 2019; Huang et al., 2019"
2020.acl-main.720,W12-1908,0,0.0312508,"Missing"
2020.acl-main.720,N19-1209,0,0.0329887,"Missing"
2020.acl-main.720,N16-1161,0,0.0307464,"fine-tuning may utilize a large number of those parameters. 1 Introduction Multilingual learning—using data from multiple languages to train a single model—can take many forms, such as adapting a model from a highresource to low-resource language (Xie et al., 2018; Ni et al., 2017; Mayhew et al., 2017; Cotterell and Duh, 2017; Wu and Dredze, 2019; M`arquez et al., 2003), taking advantage of beneficial multilingual features or datasets (Kim et al., 2012; Ehrmann et al., 2011; T¨ackstr¨om, 2012), and unsupervised representation learning (Devlin et al., 2018a). We adopt the term “Polyglot” from Tsvetkov et al. (2016) to refer to models that are trained on and applied to multiple languages. There are several advantages to training a single polyglot model across languages. Single models ease production requirements; only one model need be maintained. They can be more efficient, using fewer parameters than multiple monolingual models. Additionally, they can enable multilingual transfer (Devlin, 2018; Wu and Dredze, 2019; Pires et al., 2019). However, a key goal of polyglot learning concerns producing a single model that does better on each language than a monolingual model. In the context of named entity rec"
2020.acl-main.720,D18-1270,0,0.102866,"Missing"
2020.acl-main.720,N19-1423,0,\N,Missing
2020.acl-main.760,C10-1032,1,0.864199,"ge System (UMLS) ontology (Bodenreider, 2004). However, this may be absent from metadata as it is not part of the current diagnosis. Concept mentions can use non-standard ∗ Contribution performed during an internship at Johns Hopkins University. terms (e.g. epilepsy), thus concept linking requires non-lexical methods. Additionally, some terms (cancer) are ambiguous and could refer to multiple concepts (breast cancer, colon cancer, etc.) The related task of Entity Linking – linking named entities (people, places, and organizations) to a knowledge base – has been explored in nonmedical domains (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). Entity linking systems consider three sources of information: 1) similarity between mention strings and names for the KB entity; 2) comparison of the document context to information about the KB entity (e.g. entity description); 3) information contained in the KB, such as entity popularity or inter-entity relations. In contrast to the dense KBs in entity linking, concept linking uses sparse ontologies, which contain a unique identifier (CUI), title, and links to synonyms and related concepts, but rarely longform text. For example, while the conce"
2020.acl-main.760,P15-2049,0,0.263098,"Missing"
2020.acl-main.760,Q14-1037,0,0.209391,"logy (Bodenreider, 2004). However, this may be absent from metadata as it is not part of the current diagnosis. Concept mentions can use non-standard ∗ Contribution performed during an internship at Johns Hopkins University. terms (e.g. epilepsy), thus concept linking requires non-lexical methods. Additionally, some terms (cancer) are ambiguous and could refer to multiple concepts (breast cancer, colon cancer, etc.) The related task of Entity Linking – linking named entities (people, places, and organizations) to a knowledge base – has been explored in nonmedical domains (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). Entity linking systems consider three sources of information: 1) similarity between mention strings and names for the KB entity; 2) comparison of the document context to information about the KB entity (e.g. entity description); 3) information contained in the KB, such as entity popularity or inter-entity relations. In contrast to the dense KBs in entity linking, concept linking uses sparse ontologies, which contain a unique identifier (CUI), title, and links to synonyms and related concepts, but rarely longform text. For example, while the concept epilepsy has many syno"
2020.acl-main.760,D17-1284,0,0.0823648,"However, this may be absent from metadata as it is not part of the current diagnosis. Concept mentions can use non-standard ∗ Contribution performed during an internship at Johns Hopkins University. terms (e.g. epilepsy), thus concept linking requires non-lexical methods. Additionally, some terms (cancer) are ambiguous and could refer to multiple concepts (breast cancer, colon cancer, etc.) The related task of Entity Linking – linking named entities (people, places, and organizations) to a knowledge base – has been explored in nonmedical domains (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). Entity linking systems consider three sources of information: 1) similarity between mention strings and names for the KB entity; 2) comparison of the document context to information about the KB entity (e.g. entity description); 3) information contained in the KB, such as entity popularity or inter-entity relations. In contrast to the dense KBs in entity linking, concept linking uses sparse ontologies, which contain a unique identifier (CUI), title, and links to synonyms and related concepts, but rarely longform text. For example, while the concept epilepsy has many synonyms in UMLS, it has"
2020.acl-main.760,D18-1126,0,0.0175478,"variations to increase matches (Metamap) (Aronson, 2001; Aronson and Lang, 2010), dictionary matching algorithms (Kipper-Schuler et al., 2008; Savova et al., 2010), rule based systems (D’Souza and Ng, 2015), and mention/ontology context overlap (Aggarwal and Barker, 2015). Learned ensembles can also be effective (Rajani et al., 2017). Concept linking has also been applied to bio-medical literature (Do˘gan et al., 2014; Zheng et al., 2015; Tsai and Roth, 2016; Zhao et al., 2019) and is most similar to the task of entity linking (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017; Mueller and Durrett, 2018). Similar to our approach, Choi et al. (2016) learn representations of concepts in UMLS. While we cannot make a direct comparison since they do not cover all of our KB (SNOMEDCT), initial experiments with their embeddings performed worse than our method. While some jointly consider the task of mention finding and linking (Durrett and Klein, 2014), we follow the more common convention of separating the two and assuming gold mention spans (Leaman et al., 2013; D’Souza and Ng, 2015). Formally, we are given a mention m in a document and must select the best CUI (concept) c from an ontology/KB, or"
2020.acl-main.760,N18-1202,0,0.375514,"s are based on three sources of information – the similarity of the mention string to an entity’s name, the similarity of the context of the document to the entity, and broader information about the knowledge base (KB). In some domains, there is little contextual information present in the KB and thus we rely more heavily on mention string similarity. We consider one example of this, concept linking, which seeks to link mentions of medical concepts to a medical concept ontology. We propose an approach to concept linking that leverages recent work in contextualized neural models, such as ELMo (Peters et al., 2018), which create a token representation that integrates the surrounding context of the mention and concept name. We find a neural ranking approach paired with contextualized embeddings provides gains over a competitive baseline (Leaman et al., 2013). Additionally, we find that a pre-training step using synonyms from the ontology offers a useful initialization for the ranker. 1 Introduction Medical concept linking produces structured topical content from clinical free text (Aronson and Lang, 2010). Healthcare providers often refer to medical concepts in clinical text notes that are absent from as"
2020.acl-main.760,W17-2305,0,0.0183797,"tem (Leaman et al., 2013) in most metrics. 2 Concept Linking Concept linking (alternatively: named entity recognition, entity normalization), has a long history (Pradhan et al., 2013; Luo et al., 2019) in the clinical NLP community, with common approaches including generating lexical variations to increase matches (Metamap) (Aronson, 2001; Aronson and Lang, 2010), dictionary matching algorithms (Kipper-Schuler et al., 2008; Savova et al., 2010), rule based systems (D’Souza and Ng, 2015), and mention/ontology context overlap (Aggarwal and Barker, 2015). Learned ensembles can also be effective (Rajani et al., 2017). Concept linking has also been applied to bio-medical literature (Do˘gan et al., 2014; Zheng et al., 2015; Tsai and Roth, 2016; Zhao et al., 2019) and is most similar to the task of entity linking (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017; Mueller and Durrett, 2018). Similar to our approach, Choi et al. (2016) learn representations of concepts in UMLS. While we cannot make a direct comparison since they do not cover all of our KB (SNOMEDCT), initial experiments with their embeddings performed worse than our method. While some jointly consider the task of mention findin"
2020.acl-main.760,Q16-1011,0,0.0147152,"rmalization), has a long history (Pradhan et al., 2013; Luo et al., 2019) in the clinical NLP community, with common approaches including generating lexical variations to increase matches (Metamap) (Aronson, 2001; Aronson and Lang, 2010), dictionary matching algorithms (Kipper-Schuler et al., 2008; Savova et al., 2010), rule based systems (D’Souza and Ng, 2015), and mention/ontology context overlap (Aggarwal and Barker, 2015). Learned ensembles can also be effective (Rajani et al., 2017). Concept linking has also been applied to bio-medical literature (Do˘gan et al., 2014; Zheng et al., 2015; Tsai and Roth, 2016; Zhao et al., 2019) and is most similar to the task of entity linking (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017; Mueller and Durrett, 2018). Similar to our approach, Choi et al. (2016) learn representations of concepts in UMLS. While we cannot make a direct comparison since they do not cover all of our KB (SNOMEDCT), initial experiments with their embeddings performed worse than our method. While some jointly consider the task of mention finding and linking (Durrett and Klein, 2014), we follow the more common convention of separating the two and assuming gold mention s"
2020.acl-main.760,D18-1270,0,0.0472993,"Missing"
2020.emnlp-main.362,D19-1252,0,0.0255873,"Missing"
2020.emnlp-main.362,2005.mtsummit-papers.11,0,0.0793478,"Wu et al., 2019; Liu et al., 2019) or fine-tuning (Cao et al., 2020). However, as word-level alignments from an unsupervised aligner are often suboptimal, we develop a new cross-lingual alignment objective for training our model. We base on our objective on contrastive learning, in which two similar inputs – such as from a bitext – are directly optimized to be similar, relative to a negative set. These methods have been effective in computer vision tasks (He et al., 2019; Chen et al., 2020a). Additionally, most previous work on contextual alignments consider high-quality bitext like Europarl (Koehn, 2005) or MultiUN (Eisele and Chen, 2010). While helpful, these resources are unavailable for most languages for which we seek a zero-shot transfer. To better reflect the quality of bitext available for most languages, we additionally use OPUS-100 (Zhang et al., 2020), a randomly sampled 1 million subset (per language pair) of the OPUS collection (Tiedemann, 2012). We show that our new contrastive learning alignment objectives outperform previous work (Cao et al., 2020) when applied to bitext from previous works or the OPUS-100 bitext. However, our experiments also produce a negative result. While p"
2020.emnlp-main.362,P07-2045,0,0.00625623,"(1) For bitext used in previous works, we use MultiUN for Arabic, Spanish, French, Russian or Chinese, EUBookshop (Skadin¸sˇ et al., 2014) for German, IIT Bombay corpus (Kunchukuttan et al., 2018) for Hindi and OpenSubtitles (Lison et al., 2018) for Vietnamese. We sample 1M bitext for each target language. (2) The OPUS-100 covering 100 languages with English as the center, and sampled from the OPUS collection randomly, which better reflects the average quality of bitext for most languages. It contains 1M bitext for each target language, except Hindi (0.5M). We tokenize the bitext with Moses (Koehn et al., 2007) and segment Chinese with Chang et al. (2008). We use fast align (Dyer et al., 2013) to produce unsupervised word alignments in both direction and symmetrize with the grow-diag-finaland heuristic. We only keep one-to-one alignment and discard any trivial alignment where the source and target words are identical. We train the L2, weak, and strong alignment objectives in a multilingual fashion. Each batch contains examples from all target languages. Following Devlin et al. (2019), we optimize with Adam (Kingma and Ba, 2014), learning rate 1e-4, 128 batch size, 100k total steps (≈ 2 epochs), 4k s"
2020.emnlp-main.362,L18-1548,0,0.0314081,"B }. For both weak and strong alignment objectives, we add a regularization term Eq. (5) with λ = 1. 3 Experiments Multilingual Alignment We consider alignment and transfer from English to 8 target languages: Arabic, German, English, Spanish, French, Hindi, Russian, Vietnamese, and Chinese. We use two sets of bitexts: (1) bitext used in previous works (Conneau and Lample, 2019) and (2) the OPUS100 bitext (Zhang et al., 2020). (1) For bitext used in previous works, we use MultiUN for Arabic, Spanish, French, Russian or Chinese, EUBookshop (Skadin¸sˇ et al., 2014) for German, IIT Bombay corpus (Kunchukuttan et al., 2018) for Hindi and OpenSubtitles (Lison et al., 2018) for Vietnamese. We sample 1M bitext for each target language. (2) The OPUS-100 covering 100 languages with English as the center, and sampled from the OPUS collection randomly, which better reflects the average quality of bitext for most languages. It contains 1M bitext for each target language, except Hindi (0.5M). We tokenize the bitext with Moses (Koehn et al., 2007) and segment Chinese with Chang et al. (2008). We use fast align (Dyer et al., 2013) to produce unsupervised word alignments in both direction and symmetrize with the grow-diag-f"
2020.emnlp-main.362,K19-1004,0,0.0352637,"Missing"
2020.emnlp-main.362,tiedemann-2012-parallel,0,0.0512104,"to be similar, relative to a negative set. These methods have been effective in computer vision tasks (He et al., 2019; Chen et al., 2020a). Additionally, most previous work on contextual alignments consider high-quality bitext like Europarl (Koehn, 2005) or MultiUN (Eisele and Chen, 2010). While helpful, these resources are unavailable for most languages for which we seek a zero-shot transfer. To better reflect the quality of bitext available for most languages, we additionally use OPUS-100 (Zhang et al., 2020), a randomly sampled 1 million subset (per language pair) of the OPUS collection (Tiedemann, 2012). We show that our new contrastive learning alignment objectives outperform previous work (Cao et al., 2020) when applied to bitext from previous works or the OPUS-100 bitext. However, our experiments also produce a negative result. While previous work showed improvements from alignmentbased objectives on zero-shot cross-lingual transfer for a single task (XNLI) with a single random seed, our more extensive analysis tells a different story. We report the mean and standard derivation of multiple runs with the same hyperparam4471 Proceedings of the 2020 Conference on Empirical Methods in Natural"
2020.emnlp-main.362,P17-1178,0,0.052826,"contains examples from all target languages. Following Devlin et al. (2019), we optimize with Adam (Kingma and Ba, 2014), learning rate 1e-4, 128 batch size, 100k total steps (≈ 2 epochs), 4k steps linear warmup and linear decay. We use 16-bit precision and train each model on a single RTX TITAN for around 18 hours. We set the maximum sequence length to 96. For linear mapping, we use a linear decay learning rate from 1e-4 to 0 in 20k steps (≈ 3 epochs), and train for 3 hours for each language pairs. Evaluation We consider zero-shot cross-lingual transfer with XNLI (Conneau et al., 2018), NER (Pan et al., 2017), POS tagging and dependency 4473 XNLI NER POS Parsing XNLI NER POS Parsing mBERT + Linear Mapping + L2 Align + Weak Align (Our) + Strong Align (Our) 70.1±0.8 70.0±0.6 69.7±0.4 70.5±0.7 70.4±0.7 67.7±1.3 63.7±1.5 67.1±1.0 68.0±1.3 67.7±1.1 78.3±0.5 79.5±0.5 78.0±1.3 78.8±0.7 79.0±0.7 52.6±0.4 53.6±0.3 52.2±0.7 53.1±0.6 53.0±0.6 mBERT + Linear Mapping + L2 Align + Weak Align (Our) + Strong Align (Our) 70.1±0.8 70.2±0.6 70.3±0.5 70.8±0.7 70.4±0.7 67.7±1.3 63.8±1.3 67.8±1.4 67.3±0.9 67.2±1.1 78.3±0.5 80.1±0.4 78.2±1.2 78.8±0.6 79.0±0.7 52.6±0.4 53.6±0.3 52.8±0.7 52.9±0.6 53.3±0.6 XLMRbase + Linea"
2020.emnlp-main.362,D19-1575,0,0.0704541,"le almost all encoders are pretrained without explicit crosslingual objective, i.e. enforcing similar words from Code is available at https://github.com/ shijie-wu/crosslingual-nlp. different languages have similar representation, improvements can be attained through the use of explicit cross-lingually linked data during pretraining, such as bitexts (Conneau and Lample, 2019; Huang et al., 2019; Ji et al., 2019) and dictionaries (Wu et al., 2019). As with cross-lingual embeddings (Ruder et al., 2019), these data can be used to support explicit alignment objectives with either linear mappings (Wang et al., 2019, 2020; Wu et al., 2019; Liu et al., 2019) or fine-tuning (Cao et al., 2020). However, as word-level alignments from an unsupervised aligner are often suboptimal, we develop a new cross-lingual alignment objective for training our model. We base on our objective on contrastive learning, in which two similar inputs – such as from a bitext – are directly optimized to be similar, relative to a negative set. These methods have been effective in computer vision tasks (He et al., 2019; Chen et al., 2020a). Additionally, most previous work on contextual alignments consider high-quality bitext like Eu"
2020.emnlp-main.362,N18-1202,0,0.0271021,", we use an unsupervised word aligner. Let S and T be the contextual hidden state matrix of corresponding words from a pretrained multilingual encoder. We assume S is English while T is a combination of different target languages. As both mBERT and XLMR operate at the subword level, we use the representation of the first subword, which is consistent with the evaluation stage. Each si and ti are a corresponding row of S and T, respectively. S and T come from the final layer of the encoder while Sl and Tl come from the lth -layer. Linear Mapping If S and T are static feature (such as from ELMo (Peters et al., 2018)) then T can be aligned so that it is close to S via a linear mapping (Wang et al., 2019, 2020; Wu et al., 2019; Liu et al., 2019), similar to aligning monolingual embeddings to produce cross-lingual embeddings. For feature Sl and Tl from layer l, we can learn a mapping Wl . Wl∗ = arg min kSl − Tl Wl k22 Wl Wl (1) following update rule W ← (1 + β)W − β(WWT )W with β = 0.01. Note we learn different Wl for each target language. This approach has yielded improvements in several studies. Wang et al. (2019) used mBERT and 10k parallel sentences from Europarl to improve dependency parsing. Wang et a"
2020.emnlp-main.362,2020.emnlp-demos.6,0,0.0595201,"Missing"
2020.emnlp-main.362,2020.emnlp-main.617,0,0.0736462,"Missing"
2020.emnlp-main.362,D19-1077,1,0.821852,"tive outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives. 1 Introduction Unsupervised massively multilingual encoders including multilingual BERT (Devlin et al., 2019, mBERT) and XLM-RoBERTa (Conneau et al., 2019, XLMR) are now standard tools for zeroshot cross-lingual transfer for NLP tasks (Wu and Dredze, 2019; Xia et al., 2020). While almost all encoders are pretrained without explicit crosslingual objective, i.e. enforcing similar words from Code is available at https://github.com/ shijie-wu/crosslingual-nlp. different languages have similar representation, improvements can be attained through the use of explicit cross-lingually linked data during pretraining, such as bitexts (Conneau and Lample, 2019; Huang et al., 2019; Ji et al., 2019) and dictionaries (Wu et al., 2019). As with cross-lingual embeddings (Ruder et al., 2019), these data can be used to support explicit alignment objectives with"
2020.emnlp-main.362,2020.emnlp-main.608,1,0.745431,"ious work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives. 1 Introduction Unsupervised massively multilingual encoders including multilingual BERT (Devlin et al., 2019, mBERT) and XLM-RoBERTa (Conneau et al., 2019, XLMR) are now standard tools for zeroshot cross-lingual transfer for NLP tasks (Wu and Dredze, 2019; Xia et al., 2020). While almost all encoders are pretrained without explicit crosslingual objective, i.e. enforcing similar words from Code is available at https://github.com/ shijie-wu/crosslingual-nlp. different languages have similar representation, improvements can be attained through the use of explicit cross-lingually linked data during pretraining, such as bitexts (Conneau and Lample, 2019; Huang et al., 2019; Ji et al., 2019) and dictionaries (Wu et al., 2019). As with cross-lingual embeddings (Ruder et al., 2019), these data can be used to support explicit alignment objectives with either linear mappi"
2020.findings-emnlp.337,W18-0601,0,0.214767,"nd adversarial training (Blitzer et al., 2006; Bach et al., 2016; Tzeng et al., 2017). Domain adaptation is of particular interest in the mental health space, where there exist numerous complexities in obtaining a sufficient sample of training data. For instance, the sensitive nature of mental health data necessitates extra care when creating and supporting new datasets (Benton et al., 2017a). Additionally, behavioral disorders are known to display variable clinical presentations amongst different populations, which can make identification of ground truth difficult (De Choudhury et al., 2017; Arseniev-Koehler et al., 2018). The latter point highlights the presence of label noise inherent in mental health data (Mitchell et al., 2009; Shing et al., 2018). This facet serves as one of two primary issues unique to this research space that may hinder attempts at domain transfer. Indeed, prior work found that diverse and sometimes conflicting views humans have regarding suicidal ideation can make obtaining reliable gold-standard labels fundamentally challenging and lead to degradation in model performance (Liu et al., 2017). Sampling-related biases present the other main area of concern for successful domain transfer"
2020.findings-emnlp.337,W06-1615,0,0.198739,"l and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring models between domains include variance in source and target class distributions (Japkowicz and Stephen, 2002), semantic misalignment (Wu and Huang, 2016), and sparse vocabulary overlap (Stojanov et al., 2019). Fortunately, once these issues are identified, it is typically possible to decrease the transfer performance gap via methods such as structural correspondence learning, feature subspace mapping, and adversarial training (Blitzer et al., 2006; Bach et al., 2016; Tzeng et al., 2017). Domain adaptation is of particular interest in the mental health space, where there exist numerous complexities in obtaining a sufficient sample of training data. For instance, the sensitive nature of mental health data necessitates extra care when creating and supporting new datasets (Benton et al., 2017a). Additionally, behavioral disorders are known to display variable clinical presentations amongst different populations, which can make identification of ground truth difficult (De Choudhury et al., 2017; Arseniev-Koehler et al., 2018). The latter po"
2020.findings-emnlp.337,W15-1204,1,0.885558,"Missing"
2020.findings-emnlp.337,N16-1122,1,0.827914,"r domains in an equitable manner, trading improvements with each other across transfer settings. These results indicate that sample size and class balance are not solely responsible for generalization loss. 5.2 Temporal Transfer Typical sources of transfer loss concern differences in features between domains (Blitzer et al., 2007; Ben-David et al., 2010). However, other factors may govern model degradation for depression classification. One such cause of loss is temporal misalignment between the datasets (Table 1). Prior work has shown that language dynamics may hinder models upon deployment (Dredze et al., 2016; Huang and Paul, 2018). In social media, where users adopt new linguistic norms rapidly, performance may be more volatile (Brigadir et al., 2015). 5.2.1 Class Misalignment As an exercise to understand whether temporal artifacts are present in the datasets, we first consider training and evaluating single-domain models with a temporal misalignment between the control and depression groups. By training on mutuallyexclusive time periods for each class, we hypothesize the classifier will not only able to learn how to distinguish between groups, but also to distinguish between time periods. If thi"
2020.findings-emnlp.337,D19-6208,0,0.0321283,"Missing"
2020.findings-emnlp.337,P18-2110,0,0.0233329,"able manner, trading improvements with each other across transfer settings. These results indicate that sample size and class balance are not solely responsible for generalization loss. 5.2 Temporal Transfer Typical sources of transfer loss concern differences in features between domains (Blitzer et al., 2007; Ben-David et al., 2010). However, other factors may govern model degradation for depression classification. One such cause of loss is temporal misalignment between the datasets (Table 1). Prior work has shown that language dynamics may hinder models upon deployment (Dredze et al., 2016; Huang and Paul, 2018). In social media, where users adopt new linguistic norms rapidly, performance may be more volatile (Brigadir et al., 2015). 5.2.1 Class Misalignment As an exercise to understand whether temporal artifacts are present in the datasets, we first consider training and evaluating single-domain models with a temporal misalignment between the control and depression groups. By training on mutuallyexclusive time periods for each class, we hypothesize the classifier will not only able to learn how to distinguish between groups, but also to distinguish between time periods. If this hypothesis holds true"
2020.findings-emnlp.337,P19-1403,0,0.0221862,"he attributes used to construct our Twitter datasets originally were inferred via now-outdated text-based models. Accordingly, demographic inference errors may be propagated to and correlated with depression classification errors. Moreover, these attributes were not considered within the construction of any of the Reddit datasets we explored. The effect of demographics on generalization remains a valuable insight for future exploration. Finally, our attempts at domain transfer are constrained. Namely, we do not invoke explicit domain adaptation methods (Peng and Dredze, 2017; Li et al., 2018; Huang and Paul, 2019). Moving forward, we plan to explore algorithmic strategies to mitigate the biases discovered in this study. 3782 References Omar Abdel-Rahman. 2019. Socioeconomic predictors of suicide risk among cancer patients in the united states: A population-based study. Cancer epidemiology, 63:101601. Silvio Amir, Mark Dredze, and John W Ayers. 2019. Mental health surveillance over social media with digital cohorts. In Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 114–120. American Psychiatric Association APA. 2013. Diagnostic and statistical manual of men"
2020.findings-emnlp.337,W18-0620,0,0.0550508,"er, the proxy-based annotation mechanisms used to label large social media data sets with mental health status invite the introduction of selfdisclosure bias into the modeling task (Amir et al., 2019). Specifically, labels sourced from populations of individuals who self-disclose certain attributes may contain activity-level and thematic biases that cause poor generalization in larger populations (Lippincott and Carrell, 2018). Research leveraging text data for mental health status classification has primarily only considered a constrained form of domain transfer. In a withinsubject analysis, Ireland and Iserman (2018) examined differences in language usage by Reddit users who had posted in an anxiety support forum within and outside mental health forums. Similarly, Wolohan et al. (2018) explored the predictive power of models trained to detect depression within Reddit users as a function of access to text from explicit mental health related subreddits. Both studies highlighted a mitigation of overt mental health discussion outside of the support forums, but still detected linguistic nuances in individuals with an affiliation to the mental health forums. 3775 Dataset Platform Years CLPsych Twitter 2011-2014"
2020.findings-emnlp.337,W18-0615,0,0.0224417,"ferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction. 1 Introduction In the last decade, there has been substantial growth in the area of digital psychiatry. Automated methods using natural language processing have been able to detect mental health disorders based on a person’s language in a variety of data types, such as social media (Mowery et al., 2016; Morales et al., 2017), speech (Iter et al., 2018) and other writings (Kayi et al., 2017; Just et al., 2019). As in-person clinical visits are made increasingly difficult by socioeconomic barriers and public-health crises, such as COVID-19, tools for measuring mental wellness using implicit signal become more important than ever (Abdel-Rahman, 2019; Bojdani et al., 2020). Early work in this area leveraged traditional human subject studies in which individuals with clinically validated psychiatric diagnoses volunteered their language data to train classifiers and perform quantitative analyses (Rude et al., 2004; Jarrold et al., 2010). In an ef"
2020.findings-emnlp.337,W19-3015,0,0.039503,"Missing"
2020.findings-emnlp.337,S17-1028,0,0.135671,"stematically differs from the rest of Reddit (De Choudhury and De, 2014; Ireland and Iserman, 2018). To encourage our mental health classifiers to learn subtle linguistic nuances that cannot be easily captured using straightforward logic, we make efforts to exclude unambiguous mental health content from all training and evaluation procedures. In line 3776 with prior work, we discard posts that include mentions of clinically-defined psychiatric conditions, adopting the list of mental health terms enumerated by Cohan et al. (2018) as a reference. This list (N=458) extends work from Yates et al. (2017) by including disorders tangential to depression, common misspellings, and colloquial references. As is standard for mental health modeling, we also discard posts made in subreddits dedicated to providing mental health support (Yates et al., 2017; Cohan et al., 2018; Wolohan et al., 2018). Since new subreddits are created daily and our version of the Topic-Restricted Text dataset contains posts made after collection of RSDD and SMHD, we create an updated list of mental health support subreddits. To do so, we examine the empirical distribution of posts amongst subreddits within the TopicRestric"
2020.findings-emnlp.337,W18-1107,0,0.0107012,"nder, age, and disorder co-morbidity have been found to significantly affect the presentation of mental health disorders in language data (Cummins et al., 2015; Preot¸iuc-Pietro et al., 2015). Moreover, the proxy-based annotation mechanisms used to label large social media data sets with mental health status invite the introduction of selfdisclosure bias into the modeling task (Amir et al., 2019). Specifically, labels sourced from populations of individuals who self-disclose certain attributes may contain activity-level and thematic biases that cause poor generalization in larger populations (Lippincott and Carrell, 2018). Research leveraging text data for mental health status classification has primarily only considered a constrained form of domain transfer. In a withinsubject analysis, Ireland and Iserman (2018) examined differences in language usage by Reddit users who had posted in an anxiety support forum within and outside mental health forums. Similarly, Wolohan et al. (2018) explored the predictive power of models trained to detect depression within Reddit users as a function of access to text from explicit mental health related subreddits. Both studies highlighted a mitigation of overt mental health d"
2020.findings-emnlp.337,W17-3101,0,0.100777,"stantial loss occurs when transferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction. 1 Introduction In the last decade, there has been substantial growth in the area of digital psychiatry. Automated methods using natural language processing have been able to detect mental health disorders based on a person’s language in a variety of data types, such as social media (Mowery et al., 2016; Morales et al., 2017), speech (Iter et al., 2018) and other writings (Kayi et al., 2017; Just et al., 2019). As in-person clinical visits are made increasingly difficult by socioeconomic barriers and public-health crises, such as COVID-19, tools for measuring mental wellness using implicit signal become more important than ever (Abdel-Rahman, 2019; Bojdani et al., 2020). Early work in this area leveraged traditional human subject studies in which individuals with clinically validated psychiatric diagnoses volunteered their language data to train classifiers and perform quantitative analyses (Rude et al., 2004; Jar"
2020.findings-emnlp.337,W16-4320,0,0.106271,"only reveal that substantial loss occurs when transferring between platforms, but also that there exist several unreliable confounding factors that may enable researchers to overestimate classification performance. Based on these results, we enumerate recommendations for future mental health dataset construction. 1 Introduction In the last decade, there has been substantial growth in the area of digital psychiatry. Automated methods using natural language processing have been able to detect mental health disorders based on a person’s language in a variety of data types, such as social media (Mowery et al., 2016; Morales et al., 2017), speech (Iter et al., 2018) and other writings (Kayi et al., 2017; Just et al., 2019). As in-person clinical visits are made increasingly difficult by socioeconomic barriers and public-health crises, such as COVID-19, tools for measuring mental wellness using implicit signal become more important than ever (Abdel-Rahman, 2019; Bojdani et al., 2020). Early work in this area leveraged traditional human subject studies in which individuals with clinically validated psychiatric diagnoses volunteered their language data to train classifiers and perform quantitative analyses"
2020.findings-emnlp.337,W19-5365,0,0.0145772,"several areas of natural language processing (Jiang, 2008; Peng and Dredze, 2017). It is particularly useful in situations where acquiring ample training data for a target application is intractable (e.g. monetary, time constraints) or impossible (e.g. privacy constraints) (Rieman et al., 2017). For example, in the sub-field of machine translation, significant effort is devoted to finding ways to effectively use large corpora of formal parallel text to train models for application in domains with informal and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring models between domains include variance in source and target class distributions (Japkowicz and Stephen, 2002), semantic misalignment (Wu and Huang, 2016), and sparse vocabulary overlap (Stojanov et al., 2019). Fortunately, once these issues are identified, it is typically possible to decrease the transfer performance gap via methods such as structural correspondence learning, feature subspace mapping, and adversarial training (Blitzer et al., 2006; Bach et al., 2016; Tzeng et al., 2017). Domain adaptation is of particular interest in the"
2020.findings-emnlp.337,W18-0609,0,0.0286503,"s leveraging proxy-based annotations have supported their design by demonstrating alignment with existing psychological theory regarding language usage by individuals living with a mental health disorder (Cavazos-Rehg et al., 2016; Vedula and Parthasarathy, 2017). For example, feature analyses have highlighted higher amounts of negative affect and increased personal pronoun prevalence amongst depressed individuals (Park et al., 2012; De Choudhury et al., 2013). Given these consistencies, the field has largely turned its attention toward optimizing predictive power via state of the art models (Orabi et al., 2018; Song et al., 2018). The ultimate goal of these efforts has been threefold—to better personalize psychiatric care, to enable early intervention, and to monitor population-level health outcomes in real time. Nonetheless, research has largely trudged forward without stopping to ask one critical question: do models of mental health conditions trained on automatically annotated social media data actually generalize to new data platforms and populations? Typically, the answer is no—or at least not without modification. Performance loss is to be expected in a variety of scenarios due to underlying"
2020.findings-emnlp.337,W17-2612,1,0.931648,"status of individuals, additional precautions based on guidance from Benton et al. (2017a) were taken during all data collection and analysis procedures. Data sourced from external research groups was retrieved according to each dataset’s respective data usage policy. The research was deemed exempt from review by our Institutional Review Board (IRB) under 45 CFR § 46.104. 2 Domain Adaptation in Mental Health Domain adaptation (or “transfer”) of statistical classifiers is a well-studied computational problem with high relevance across several areas of natural language processing (Jiang, 2008; Peng and Dredze, 2017). It is particularly useful in situations where acquiring ample training data for a target application is intractable (e.g. monetary, time constraints) or impossible (e.g. privacy constraints) (Rieman et al., 2017). For example, in the sub-field of machine translation, significant effort is devoted to finding ways to effectively use large corpora of formal parallel text to train models for application in domains with informal and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring mode"
2020.findings-emnlp.337,D14-1162,0,0.0850085,"cross-domain transfer experiments discussed in §5.1. Results. We find the minimum similarity occurs between the CLPsych and RSDD datasets (JS = 0.10) while the maximum occurs between the Topic-Restricted Text and SMHD datasets (JS = 0.65).3 Only a weak correlation between similarity and performance exists (Pearson ρ < 0.18), suggesting poor generalization is not solely due to differences in vocabulary. 6.2 Topical Alignment Our classification models leverage reduced feature representations in the form of LDA topicdistributions (Blei et al., 2003) and mean-pooled pre-trained GloVe embeddings (Pennington et al., 2014). Designed to capture and reflect semantics, we hypothesized these low-dimensional features would mitigate transfer loss due to poor vocabulary alignment. Lacking support from our cross-domain transfer results, we look closer at the themes present within each dataset. Methods. We identify the unigrams that are most unique to each dataset and group. For each dataset, we use scores assigned by our KLdivergence-based feature selection method (see Appendix D) to rank the most informative features per class (Chang et al., 2012). We jointly examine the top-500 most informative unigrams per class, no"
2020.findings-emnlp.337,W15-1203,0,0.0419455,"Missing"
2020.findings-emnlp.337,I17-1077,0,0.0222189,"rding to each dataset’s respective data usage policy. The research was deemed exempt from review by our Institutional Review Board (IRB) under 45 CFR § 46.104. 2 Domain Adaptation in Mental Health Domain adaptation (or “transfer”) of statistical classifiers is a well-studied computational problem with high relevance across several areas of natural language processing (Jiang, 2008; Peng and Dredze, 2017). It is particularly useful in situations where acquiring ample training data for a target application is intractable (e.g. monetary, time constraints) or impossible (e.g. privacy constraints) (Rieman et al., 2017). For example, in the sub-field of machine translation, significant effort is devoted to finding ways to effectively use large corpora of formal parallel text to train models for application in domains with informal and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring models between domains include variance in source and target class distributions (Japkowicz and Stephen, 2002), semantic misalignment (Wu and Huang, 2016), and sparse vocabulary overlap (Stojanov et al., 2019). Fortuna"
2020.findings-emnlp.337,D11-1141,0,0.0596639,"Missing"
2020.findings-emnlp.337,W17-3005,0,0.0155639,"older samples within all Reddit datasets. While models trained on RSDD perform better on Topic-Restricted Text as latency is reduced, models trained on SMHD do not exhibit the same trend. 6 Post-hoc Analysis In the previous section, we identified the degree to which loss occurs under a variety of domain transfer settings. However, these settings do not account for all performance disparities. In this section, we measure differences between the datasets to understand the source of loss. 3779 6.1 Vocabulary Overlap Traditionally, different feature vocabularies account for domain transfer loss (Serra et al., 2017; Chen and Gomes, 2019; Stojanov et al., 2019). Therefore, we hypothesize that limited feature overlap and poor vocabulary alignment across datasets could hinder cross-domain generalization. Methods. We explore this phenomenon by computing the Jaccard Similarity (JS) of vocabularies between each dataset. We examine correlations between JS and F1 scores from the cross-domain transfer experiments discussed in §5.1. Results. We find the minimum similarity occurs between the CLPsych and RSDD datasets (JS = 0.10) while the maximum occurs between the Topic-Restricted Text and SMHD datasets (JS = 0.6"
2020.findings-emnlp.337,W18-0603,0,0.0469195,"Missing"
2020.findings-emnlp.337,Y18-1070,0,0.0132802,"ased annotations have supported their design by demonstrating alignment with existing psychological theory regarding language usage by individuals living with a mental health disorder (Cavazos-Rehg et al., 2016; Vedula and Parthasarathy, 2017). For example, feature analyses have highlighted higher amounts of negative affect and increased personal pronoun prevalence amongst depressed individuals (Park et al., 2012; De Choudhury et al., 2013). Given these consistencies, the field has largely turned its attention toward optimizing predictive power via state of the art models (Orabi et al., 2018; Song et al., 2018). The ultimate goal of these efforts has been threefold—to better personalize psychiatric care, to enable early intervention, and to monitor population-level health outcomes in real time. Nonetheless, research has largely trudged forward without stopping to ask one critical question: do models of mental health conditions trained on automatically annotated social media data actually generalize to new data platforms and populations? Typically, the answer is no—or at least not without modification. Performance loss is to be expected in a variety of scenarios due to underlying distributional shift"
2020.findings-emnlp.337,D19-6213,0,0.0191319,"e subcultures (Blanco and Barnett, 2014; Bowes et al., 2015). Additionally, we find several temporallyisolated references within the Twitter datasets (e.g. ‘#RIPRobinWilliams’, ‘#SDCC’). In the Multi-task Learning dataset, we also see several terms using non-American English (e.g. ‘colour’, ‘favourite’) which may represent a geographic imbalance amongst the sampled individuals. 6.3 Stability of LIWC The Linguistic Inquiry and Word Count (LIWC) dictionary has been an effective tool for measuring linguistic-nuances of mental health disorders regardless of textual formality (Mowery et al., 2016; Turcan and McKeown, 2019). Our version of the dictionary (2007) maps approximately 12k words to 64 dimensions (e.g. negative emotion, leisure) that have been empirically validated to capture an individual’s social and psychological states (Tausczik and Pennebaker, 2010).4 A single LIWC feature value represents the proportion of words used across an individual’s post history that match the given LIWC dimension. In the same way that we expect semantic distributions (§6.2) to ameliorate transfer loss, we hypothesize that models trained on this representation will be more robust when vocabulary overlap is sparse. Methods."
2020.findings-emnlp.337,D17-1155,0,0.0144923,"h relevance across several areas of natural language processing (Jiang, 2008; Peng and Dredze, 2017). It is particularly useful in situations where acquiring ample training data for a target application is intractable (e.g. monetary, time constraints) or impossible (e.g. privacy constraints) (Rieman et al., 2017). For example, in the sub-field of machine translation, significant effort is devoted to finding ways to effectively use large corpora of formal parallel text to train models for application in domains with informal and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring models between domains include variance in source and target class distributions (Japkowicz and Stephen, 2002), semantic misalignment (Wu and Huang, 2016), and sparse vocabulary overlap (Stojanov et al., 2019). Fortunately, once these issues are identified, it is typically possible to decrease the transfer performance gap via methods such as structural correspondence learning, feature subspace mapping, and adversarial training (Blitzer et al., 2006; Bach et al., 2016; Tzeng et al., 2017). Domain adaptation is of par"
2020.findings-emnlp.337,W18-4102,0,0.496605,"ask (Amir et al., 2019). Specifically, labels sourced from populations of individuals who self-disclose certain attributes may contain activity-level and thematic biases that cause poor generalization in larger populations (Lippincott and Carrell, 2018). Research leveraging text data for mental health status classification has primarily only considered a constrained form of domain transfer. In a withinsubject analysis, Ireland and Iserman (2018) examined differences in language usage by Reddit users who had posted in an anxiety support forum within and outside mental health forums. Similarly, Wolohan et al. (2018) explored the predictive power of models trained to detect depression within Reddit users as a function of access to text from explicit mental health related subreddits. Both studies highlighted a mitigation of overt mental health discussion outside of the support forums, but still detected linguistic nuances in individuals with an affiliation to the mental health forums. 3775 Dataset Platform Years CLPsych Twitter 2011-2014 Multi-Task Learning Twitter 2013-2016 RSDD Reddit 2008-2017 SMHD Reddit 2010-2018 Topic-Restricted Text Reddit 2014-2020 Size (Individuals) Control: 477 Depression: 477 Co"
2020.findings-emnlp.337,P16-1029,0,0.0216208,"y, time constraints) or impossible (e.g. privacy constraints) (Rieman et al., 2017). For example, in the sub-field of machine translation, significant effort is devoted to finding ways to effectively use large corpora of formal parallel text to train models for application in domains with informal and dynamic language, such as social media and conversational speech (Wang et al., 2017; Murakami et al., 2019). Traditional challenges encountered when transferring models between domains include variance in source and target class distributions (Japkowicz and Stephen, 2002), semantic misalignment (Wu and Huang, 2016), and sparse vocabulary overlap (Stojanov et al., 2019). Fortunately, once these issues are identified, it is typically possible to decrease the transfer performance gap via methods such as structural correspondence learning, feature subspace mapping, and adversarial training (Blitzer et al., 2006; Bach et al., 2016; Tzeng et al., 2017). Domain adaptation is of particular interest in the mental health space, where there exist numerous complexities in obtaining a sufficient sample of training data. For instance, the sensitive nature of mental health data necessitates extra care when creating an"
2020.findings-emnlp.337,D17-1322,0,0.0885247,"iterature, depression classification is a critical first target for evaluating generalization of mental health models in social media (Chancellor and De Choudhury, 2020). To quantify the nature of domain transfer loss, we consider five datasets. Datasets were selected based on their common adoption in the literature (Preot¸iuc-Pietro et al., 2015; Gamaarachchige and Inkpen, 2019) and their use of proxy-based annotations (Coppersmith et al., 2014). We use two Twitter—CLPsych 2015 Shared Task (Coppersmith et al., 2015b), Multi-Task Learning (Benton et al., 2017b)—and three Reddit datasets—RSDD (Yates et al., 2017), SMHD (Cohan et al., 2018), and TopicRestricted Text (Wolohan et al., 2018). Table 1 presents summary statistics. Construction details are in Appendix A as a courtesy to the reader. 3.1 Mitigating Bias Each dataset was curated in part by a system of simple rules (e.g. matches to “I was diagnosed with depression,” participation in a depression support forum). While these heuristics are useful for identifying candidates to include within each dataset, they also risk introducing bias that may render the modeling task trivial. For example, individuals who disclose a depression diagnosis are likel"
2020.repl4nlp-1.16,P17-1042,0,0.0330709,"e, 2019; Huang et al., 2019) and word translation pairs from a dictionary (Wu et al., 2019) or induced from a bitext (Ji et al., 2019). Several factors need to be considered in understanding mBERT. First, the 104 most common Wikipedia languages vary considerably in size (Table 1). Therefore, mBERT training attempted to equalize languages by up-sampling words from low resource languages and down-sampling words from high resource languages. Previous work has found that shared strings across languages provide sufficient signal for inducing cross-lingual word representations (Lample et al., 2018; Artetxe et al., 2017). While Wu and Dredze (2019) finds the number of shared subwords across languages correlates with cross-lingual performance, multilingual BERT can still learn cross-lingual representation without any vocabulary overlap across languages (Wu et al., 2019; K et al., 2020). Additionally, Wu et al. (2019) find bilingual BERT can still achieve decent crosslingual transfer by sharing only the transformer layer across languages. Artetxe et al. (2019) shows learning the embedding layer alone while using a fixed transformer encoder from English monolingual BERT can also produce decent cross-lingual tran"
2020.repl4nlp-1.16,Q17-1010,0,0.0578675,"representation without any vocabulary overlap across languages (Wu et al., 2019; K et al., 2020). Additionally, Wu et al. (2019) find bilingual BERT can still achieve decent crosslingual transfer by sharing only the transformer layer across languages. Artetxe et al. (2019) shows learning the embedding layer alone while using a fixed transformer encoder from English monolingual BERT can also produce decent cross-lingual transfer performance. Second, while each language Representations for Low Resource Languages Embeddings with subword information, a noncontextual representation, like fastText (Bojanowski et al., 2017) and BPEmb (Heinzerling and Strube, 2018) are more data-efficient compared to contextual representation like ELMo and BERT when a limited amount of text is available. For low resource languages, there are usually limits on monolingual corpora and task specific supervision. When task-specific supervision is limited, e.g. sequence labeling in low resource languages, mBERT performs better than fastText while underperforming a single BPEmb trained on all languages (Heinzerling and Strube, 2019). Contrary to this work, we focus on mBERT from the perspective of representation learning for each langu"
2020.repl4nlp-1.16,K18-2005,0,0.0591919,"Missing"
2020.repl4nlp-1.16,P19-4007,0,0.059462,"Missing"
2020.repl4nlp-1.16,D18-1269,0,0.0320655,"Emb trained on all languages (Heinzerling and Strube, 2019). Contrary to this work, we focus on mBERT from the perspective of representation learning for each language in terms of monolingual corpora resources and analyze how to improve BERT for low resource languages. We also consider parsing in addition to sequence labeling tasks. Concurrently, Conneau et al. (2019) train a multilingual masked language model (Devlin et al., 2019) on 2.5TB of CommonCrawl filtered data covering 100 languages and show it outperforms a Wikipedia-based model on low resource languages (Urdu and Swahili) for XNLI (Conneau et al., 2018). Using CommonCrawl greatly increases monolingual resource especially for low resource languages, and makes low resource languages in terms of Wikipedia size high resource. For example, Mongolian has 6 million and 248 million tokens in Wikipedia and CommonCrawl, respectively. Indeed, a 40-fold data increase of Mongolian (mn) increases its WikiSize, a measure of monolingual corpus size introduced in §3.1, from 5 to roughly 10, as shown in Tab. 1, making it 121 relatively high resource with respect to mBERT. 3 Experimental Setup We begin by defining high and low resource languages in mBERT, a de"
2020.repl4nlp-1.16,N19-1423,0,0.468305,"ing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data. 1 Introduction Pretrained contextual representation models trained with language modeling (Peters et al., 2018; Yang et al., 2019) or the cloze task objectives (Devlin et al., 2019; Liu et al., 2019) have quickly set a new standard for NLP tasks. These models have also been trained in multilingual settings. As the authors of BERT say “[...] (they) do not plan to release more single-language models”, they instead train a single BERT model with Wikipedia to serve 104 languages, without any explicit cross-lingual links, yielding a multilingual BERT (mBERT) (Devlin, 2018). Surprisingly, mBERT learn high-quality cross-lingual representation and show strong zeroshot cross-lingual transfer performance (Wu and Dredze, 2019; Pires et al., 2019). However, evaluations have focused"
2020.repl4nlp-1.16,D19-1539,0,0.0257776,"uages, we suggest either collect more data to make low resource language high resource (Conneau et al., 2019), or consider more data-efficient pretraining techniques like Clark et al. (2020). We leave exploring more data-efficient pretraining techniques as future work. 2 may be similarly represented in the training data, subwords are not evenly distributed among the languages. Many languages share common characters and cognates, biasing subword learning to some languages over others. Both of these factors may influence how well mBERT learns representations for low resource languages. Finally, Baevski et al. (2019) show that in general larger pretraining data for English leads to better downstream performance, yet increasing the size of pretraining data exponentially only increases downstream performance linearly. For a low resource language with limited pretraining data, it is unclear whether contextual representations outperform previous methods. Related Work Multilingual Contextual Representations Deep contextualized representation models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have set a new standard for NLP systems. Their application to multilingual settings, pretraining o"
2020.repl4nlp-1.16,L18-1473,0,0.0912589,"Missing"
2020.repl4nlp-1.16,P19-1027,0,0.017964,"or Low Resource Languages Embeddings with subword information, a noncontextual representation, like fastText (Bojanowski et al., 2017) and BPEmb (Heinzerling and Strube, 2018) are more data-efficient compared to contextual representation like ELMo and BERT when a limited amount of text is available. For low resource languages, there are usually limits on monolingual corpora and task specific supervision. When task-specific supervision is limited, e.g. sequence labeling in low resource languages, mBERT performs better than fastText while underperforming a single BPEmb trained on all languages (Heinzerling and Strube, 2019). Contrary to this work, we focus on mBERT from the perspective of representation learning for each language in terms of monolingual corpora resources and analyze how to improve BERT for low resource languages. We also consider parsing in addition to sequence labeling tasks. Concurrently, Conneau et al. (2019) train a multilingual masked language model (Devlin et al., 2019) on 2.5TB of CommonCrawl filtered data covering 100 languages and show it outperforms a Wikipedia-based model on low resource languages (Urdu and Swahili) for XNLI (Conneau et al., 2018). Using CommonCrawl greatly increases"
2020.repl4nlp-1.16,P18-1031,0,0.0294611,"gly, Tab. 2 shows training size still has a positive but slightly smaller slope, but the slope of WikiSize change sign, which suggests WikiSize might correlate with training size. We confirm this by fitting a linear model with training size as x and WikiSize as y and the slope is over 0.5 with p &lt; 0.001. This finding is unsurprising as the NER dataset is built from Wikipedia so larger Wikipedia size means larger training size. In conclusion, the larger the task-specific supervised dataset, the better the downstream performance on NER. Unsurprisingly, while pretraining improve data-efficiency (Howard and Ruder, 2018), it still cannot solve a task with limited supervision. Training vocabulary and Wikipedia size correlate with training size, and increasing either one factor leads to better performance. A similar conclusion could be found when we try to predict the performance ratio of mBERT and the baseline instead. Statistical analysis shows a correlation between resource and mBERT performance but can not give a causal answer on why low resource languages within mBERT perform poorly. 5.2 mBERT vs monolingual BERT We have established that mBERT does not perform well in low-resource languages. Is this becaus"
2020.repl4nlp-1.16,D19-1252,0,0.0467462,"Missing"
2020.repl4nlp-1.16,P18-1007,0,0.0557244,"Missing"
2020.repl4nlp-1.16,D18-2012,0,0.0567436,"Missing"
2020.repl4nlp-1.16,2021.ccl-1.108,0,0.16669,"Missing"
2020.repl4nlp-1.16,N19-1392,0,0.0366786,"wnstream performance linearly. For a low resource language with limited pretraining data, it is unclear whether contextual representations outperform previous methods. Related Work Multilingual Contextual Representations Deep contextualized representation models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have set a new standard for NLP systems. Their application to multilingual settings, pretraining one model on text from multiple languages with a single vocabulary, has driven forward work in cross-language learning and transfer (Wu and Dredze, 2019; Pires et al., 2019; Mulcaire et al., 2019). BERT-based pretraining also benefits language generation tasks like machine translation (Conneau and Lample, 2019). BERT can be further improve with explicit cross-language signals including: bitext (Conneau and Lample, 2019; Huang et al., 2019) and word translation pairs from a dictionary (Wu et al., 2019) or induced from a bitext (Ji et al., 2019). Several factors need to be considered in understanding mBERT. First, the 104 most common Wikipedia languages vary considerably in size (Table 1). Therefore, mBERT training attempted to equalize languages by up-sampling words from low resource la"
2020.repl4nlp-1.16,P17-1178,0,0.0501741,"e (15.5GB) and Yoruba the lowest (10MB).2 Tab. 1 shows languages and their relative resources. 3.2 Downstream Tasks mBERT supports 104 languages, and we seek to evaluate the learned representations for as many of these as possible. We consider three NLP tasks for which annotated task data exists in a large number of languages: named entity recognition (NER), universal part-of-speech (POS) tagging and universal dependency parsing. For each task, we train a taskspecific model using within-language supervised data on top of the mBERT representation with finetuning. For NER we use data created by Pan et al. (2017) automatically built from Wikipedia, which covers 99 of the 104 languages supported by mBERT. We evaluate NER with entity-level F1. This data is in-domain as mBERT is pretrained on Wikipedia. For POS tagging and dependency parsing, we use Universal Dependencies (UD) v2.3 (Nivre et al., 2018), which covers 54 languages (101 treebanks) supported by mBERT. We evaluate POS with accuracy (ACC) and Parsing with label attachment score (LAS) and unlabeled attachment score (UAS). For POS, we consider UPOS within the treebank. For parsing, we only consider universal dependency labels. The domain is tree"
2020.repl4nlp-1.16,N18-1202,0,0.205497,"ecognition (99 languages), Part-of-speech Tagging, and Dependency Parsing (54 languages each). mBERT does better than or comparable to baselines on high resource languages but does much worse for low resource languages. Furthermore, monolingual BERT models for these languages do even worse. Paired with similar languages, the performance gap between monolingual BERT and mBERT can be narrowed. We find that better models for low resource languages require more efficient pretraining techniques or more data. 1 Introduction Pretrained contextual representation models trained with language modeling (Peters et al., 2018; Yang et al., 2019) or the cloze task objectives (Devlin et al., 2019; Liu et al., 2019) have quickly set a new standard for NLP tasks. These models have also been trained in multilingual settings. As the authors of BERT say “[...] (they) do not plan to release more single-language models”, they instead train a single BERT model with Wikipedia to serve 104 languages, without any explicit cross-lingual links, yielding a multilingual BERT (mBERT) (Devlin, 2018). Surprisingly, mBERT learn high-quality cross-lingual representation and show strong zeroshot cross-lingual transfer performance (Wu an"
2020.repl4nlp-1.16,P19-1493,0,0.100573,"019) or the cloze task objectives (Devlin et al., 2019; Liu et al., 2019) have quickly set a new standard for NLP tasks. These models have also been trained in multilingual settings. As the authors of BERT say “[...] (they) do not plan to release more single-language models”, they instead train a single BERT model with Wikipedia to serve 104 languages, without any explicit cross-lingual links, yielding a multilingual BERT (mBERT) (Devlin, 2018). Surprisingly, mBERT learn high-quality cross-lingual representation and show strong zeroshot cross-lingual transfer performance (Wu and Dredze, 2019; Pires et al., 2019). However, evaluations have focused on high resource languages, with cross-lingual transfer using English as a source language or within language performance. As Wu and Dredze (2019) evaluated mBERT on 39 languages, this leaves the majority of mBERT’s 104 languages, most of which are low resource languages, untested. Does mBERT learn equally high-quality representation for its 104 languages? If not, which languages are hurt by its massively multilingual style pretraining? While it has been observed that for high resource languages like English, mBERT performs worse than monolingual BERT on Eng"
2020.repl4nlp-1.16,E17-2025,0,0.0862051,"Missing"
2020.repl4nlp-1.16,D19-1077,1,0.911009,"2018; Yang et al., 2019) or the cloze task objectives (Devlin et al., 2019; Liu et al., 2019) have quickly set a new standard for NLP tasks. These models have also been trained in multilingual settings. As the authors of BERT say “[...] (they) do not plan to release more single-language models”, they instead train a single BERT model with Wikipedia to serve 104 languages, without any explicit cross-lingual links, yielding a multilingual BERT (mBERT) (Devlin, 2018). Surprisingly, mBERT learn high-quality cross-lingual representation and show strong zeroshot cross-lingual transfer performance (Wu and Dredze, 2019; Pires et al., 2019). However, evaluations have focused on high resource languages, with cross-lingual transfer using English as a source language or within language performance. As Wu and Dredze (2019) evaluated mBERT on 39 languages, this leaves the majority of mBERT’s 104 languages, most of which are low resource languages, untested. Does mBERT learn equally high-quality representation for its 104 languages? If not, which languages are hurt by its massively multilingual style pretraining? While it has been observed that for high resource languages like English, mBERT performs worse than mo"
2020.wnut-1.28,W17-1612,1,0.82514,"degrade into weaponizing against dissenting opinion. Additionally, non-government actors could use predictions of unrest to squash disapproving voices. Moreover, frequently, marginalized voices have found solace and organization using social media (Xiong et al., 2019; Ince et al., 2017) and predicting civil unrest could unintentionally lead to actions such as further policing of overpoliced communities. With this in mind, we should consider Twitter data not just as text data, but as people. Several proposals for protecting people including avoiding reverse identification (Ayers et al., 2018; Benton et al., 2017)) and data anonymization tools (Nguyen-Son et al., 2012). We believe that the numerous studies of civil unrest that further our understanding of complex societal issues are convincing evidence that there is much to be gained Conclusion We have presented the Civil Unrest on Twitter (CUT) dataset and a baseline classifier trained on the data for identifying tweets related to a civil unrest event. Future work can build on our multifaceted annotations to expand the study of communities and how they express concern about complex societal issues through civil unrest. Acknowledgments The authors than"
2020.wnut-1.28,W14-2714,0,0.450867,"Missing"
2020.wnut-1.28,P12-3005,0,0.061163,"14 to 2019). 3 Dataset Creation We present a dataset to support the study of civil unrest on Twitter. Our data set contains English tweets with annotations related to civil unrest produced through annotations from Amazon Mechani216 cal Turk. Twitter Data We selected a sample of tweets from 2014 to 2019 that were collected from the Twitter streaming API based on filters to collect geolocated data. Our geolocation filters included African, Middle Eastern, and Southeast Asian countries (see Table 4 in the appendix.) We filtered the data set to include English only tweets as identified by langid (Lui and Baldwin, 2012).4 We excluded retweets. Geolocation is present in every tweet due to the method of collection. We select tweets based on their inclusion of an English language keyword related to civil unrest. Using an approach similar to that of Muthiah et al. (2015) and Ramakrishnan et al. (2014), we used a combination of manual and automated methods to create a large set of 709 keywords, which include terms such as “unemployment,” “police,” and “extremist.” The full list of keywords appears with the released dataset. In total, we include 4,415 tweets in the dataset for annotation.5 34 Tweets are removed fr"
2020.wnut-1.28,S16-1003,0,0.0637938,"Missing"
2021.adaptnlp-1.18,K16-1017,0,0.017608,"representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as do"
2021.adaptnlp-1.18,P16-2003,1,0.855504,"om online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaura"
2021.adaptnlp-1.18,E17-1015,0,0.123495,"ectronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting th"
2021.adaptnlp-1.18,P04-3031,0,0.270174,"sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the IMDb) and neutral. Table 1 shows a summary of the datasets. 2.1 Privacy Considerati"
2021.adaptnlp-1.18,D16-1171,0,0.109652,"ress sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentim"
2021.adaptnlp-1.18,W14-4012,0,0.0651627,"Missing"
2021.adaptnlp-1.18,D17-1241,0,0.0587459,"Missing"
2021.adaptnlp-1.18,2020.acl-main.700,0,0.0209386,"et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and generalize classification models across users (Wu and Huang, 2016; Yang and Eisenstein, 2017). For example, (Wu and Huang, 2016) optimizes document Amazon-Health Precision Recall F1 .834 .768 .793 .841 .777 .801 .838 .771 .796 .813 .844 .812 .836 .811 .821 .821 .832 .825 .866 .822 .840 .863 .812 .831 .873 .838 ."
2021.adaptnlp-1.18,S19-1015,1,0.873981,"19; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors conditioned on user in"
2021.adaptnlp-1.18,C18-1079,0,0.0517273,"Missing"
2021.adaptnlp-1.18,P12-3005,0,0.0502504,"glish reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included English movies produced in the US from 1960 to 2019. Each review associates with its author and the rated item, which refers to a movie in the IMDb data, a business unit in the Yelp data and a product in the Amazon data. To keep consistency in each dataset, we retain top 4 frequent genres of rated items and the review documents with no less than 10 tokens.1 We dropped non-English review documents by the language detector (Lui and Baldwin, 2012), lowercased all tokens and tokenized the corpora using NLTK (Bird and Loper, 2004). The review datasets have different score scales. We 1 The top 4 rated categories of Amazon-Health, IMDb and Yelp are [sports nutrition, sexual wellness, shaving & hair removal, vitamins & dietary supplements], [comedy, thriller, drama, action] and [restaurants, health & medical, home services, beauty & spas] respectively. normalize the scales and encode each review score into three discrete categories: positive (&gt; 3 for the Yelp and Amazon, &gt; 6 for the IMDb), negative (&lt; 3 for the Yelp and Amazon, &lt; 5 for the"
2021.adaptnlp-1.18,2020.acl-main.472,0,0.217767,"line users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while"
2021.adaptnlp-1.18,W19-2103,0,0.0193578,"processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classification generally improves the performance of document classifiers (Flek, 2020). The multitask learning framework has been applied for personalizing document classifiers by optimizing the classifiers on multiple document levels (Benton et al., 2017) or general and individual levels (Wu and Huang, 2016). The social relation can bridge connections between users and g"
2021.adaptnlp-1.18,D17-1119,0,0.0919775,"018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 20, 2021. ©2021 Association for Computational Linguistics review datasets from Amazon, IMDb, and Yelp containing diverse behaviors"
2021.adaptnlp-1.18,P17-1116,0,0.02697,"tion models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services domains) and propose a multitask framework to model language variations and incorporate the user factor into user embeddings. We focus on three online 172 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 172–182 April 2"
2021.adaptnlp-1.18,N19-1215,0,0.115669,"cross user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representat"
2021.adaptnlp-1.18,P11-1077,0,0.0151672,"es of rated items. To map the 300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Per"
2021.adaptnlp-1.18,D15-1036,0,0.247783,"ion 3. We then propose our user embedding model that adapts the user interests using a multitask learning framework in Section 4. Research (Pan and Ding, 2019) generally evaluates the user embedding via downstream tasks, but user annotations sometimes are hard to obtain and those evaluations are extrinsic instead of intrinsic tasks. For example, the MyPersonality (Kosinski et al., 2015) that was used in previous work (Ding et al., 2017; Farnadi et al., 2018; Pan and Ding, 2019) is no longer available, and an extrinsic task is to evaluate if user embeddings can help text classifiers. Research (Schnabel et al., 2015) suggests that the intrinsic evaluation including clustering is better than the extrinsic evaluation for controlling less hyperparameters. We propose an intrinsic evaluation for user embedding, which can provide a new perspective for testing future experiments. We show that our user-factor-adapted user embedding can generally outperform the existing methods on both intrinsic and extrinsic tasks. 2 Data We collected English reviews of Amazon (health product), IMDb and Yelp from the publicly available sources (He and McAuley, 2016; Yelp, 2018; IMDb, 2020). For the IMDb dataset, we included Engli"
2021.adaptnlp-1.18,P15-1098,0,0.0239691,"AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al."
2021.adaptnlp-1.18,D13-1187,0,0.210638,"on. While existing work mainly evaluated the user embedding by extrinsic tasks, we propose an intrinsic evaluation via clustering and evaluate user embeddings by an extrinsic task, text classification. The experiments on the three Englishlanguage social media datasets show that our proposed approach can generally outperform baselines via adapting the user factor. 1 Introduction Language varies across user factors including user interests, demographic attributes, personalities, and latent factors from user history. Research shows that language usage diversifies according to online user groups (Volkova et al., 2013), which women were more likely to use the word weakness in a positive way while men were the opposite. In social media, the user interests can include topics of user reviews (e.g., home vs. health services in Yelp) and categories of reviewed items (electronic vs kitchen products in Amazon). The ways that users express themselves depend on current contexts of user interests (Oba et al., 2019) that users may use the same words for opposite meanings and different words for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or"
2021.adaptnlp-1.18,C18-1119,0,0.020667,"rds for the same meaning. For example, online users can use the word “fast” to criticize battery quality of the electronic domain or praise medicine effectiveness of the medical products; users can also use the words “cool” to describe a property of AC products or express sentiments. User embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extra"
2021.adaptnlp-1.18,W17-2912,1,0.698095,"Missing"
2021.adaptnlp-1.18,W17-4406,1,0.722932,"predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Huang and Paul (2019) treated the language variations as a domain adaptation problem and referred to this idea as user factor adaptation. In this study, we treat the user interest as domains (e.g., restaurants vs. home services"
2021.adaptnlp-1.18,Q17-1021,0,0.374267,"r embedding, which is to learn a fixed-length representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisen"
2021.adaptnlp-1.18,P19-1270,0,0.0235045,"h representation based on multiple user reviews of each user, can infer the user latent information into a unified vector space (Benton, 2018; Pan and Ding, 2019). The inferred latent representations from online content can predict user profile (Volkova et al., 2015; Wang et al., 2018; Farnadi et al., 2018; Lynn et al., 2020) and behaviors (Zhang et al., 2015; Amir et al., 2017; Benton et al., 2017; Ding et al., 2017). User embeddings can personalize classification models, and further improve model performance (Tang et al., 2015; Chen et al., 2016a; Yang and Eisenstein, 2017; Wu et al., 2018; Zeng et al., 2019; Huang et al., 2019). The representations of user language can help models better understand documents as global contexts. However, existing user embedding methods (Amir et al., 2016; Benton et al., 2016; Xing and Paul, 2017; Pan and Ding, 2019) mainly focus on extracting features from language itself while ignoring user interests. Recent research has demonstrated that adapting the user factors can further improve user geolocation prediction (Miura et al., 2017), demographic attribute prediction (Farnadi et al., 2018), and sentiment analysis (Yang and Eisenstein, 2017). Lynn et al. (2017); Hu"
2021.adaptnlp-1.18,L16-1478,0,0.0153144,"300d user embeddings, we use the TSNE algorithm from scikit-learn (Pedregosa et al., 2011) to compress the dimension into 2-d vectors. We set the n component as 2 and leave the other parameters as their defaults in the TSNE. We can observe that the MTL user embedding model shows more cluster178 6 Related Work User Profiling is a common task in natural language processing. Online generated user texts show demographic variations in the linguistic styles, and the linguistic style variability could be used for predicting user’s personality and demographic attributes (Rosenthal and McKeown, 2011; Zhang et al., 2016; Hovy and Fornaciari, 2018; WoodDoughty et al., 2020; Gjurkovi´c et al., 2020; Lynn et al., 2020). The demographic user factors influence how online users express their opinions (Volkova et al., 2013; Hovy, 2015; WoodDoughty et al., 2017) and show promising improvements in the text classification task (Lynn et al., 2017; Huang and Paul, 2019; Lynn et al., 2019). However, in this work, the goal of modeling user factor is to train robust user embeddings via domain adaptation, rather than the end goal being demographic factor prediction and document classification itself. Personalized classifica"
2021.clpsych-1.2,W14-3207,1,0.779614,"ditions manifest, but also introduce difficulty contextualizing results between different studies. Moreover, many of these definitions may still fall short of capturing the nuances of mental health disorders (Arseniev-Koehler et al., 2018). As researchers look to transition computational models into the clinical setting, it is imperative they have access to standardized benchmarks that inform interpretation of predictive results in a consistent manner (Norgeot et al., 2020). More recently, Aguirre et al. (2021) found evidence of demographic (gender and racial/ethnic) bias within datasets from Coppersmith et al. (2014a, 2015c) that can create fairness issues in downstream tasks. They found poor representation and strong group imbalance in these datasets; however, simple changes in dataset size and balance alone could not fully account for performance disparities between groups. Indeed, common signs of depression recognized in prior linguistic analyses (e.g. differences in distributions for some categories of LIWC) were found not to be equally informative for all demographics. Thus, while performance disparities between demographic groups may certainly arise due to poor representation at training time, disp"
2021.clpsych-1.2,W15-1201,1,0.92801,"e (e.g. Twitter, SMS) 125 Se Initia arc l h U Da niq Ontase ue ly ts We develop a high-level schema to code properties of each dataset. In addition to standard reference information (i.e. Title, Year Published, Authors), we note the following characteristics: # Articles 2.3 Analysis Our literature search yielded 139 articles referencing 111 nominally-unique datasets. Application of exclusion criteria left us with 102 datasets. A majority of the datasets were released after 2012, with an average of 12.75 per year, a minimum of 1 (2012), and a maximum of 23 (2017). The 2015 CLPsych Shared Task (Coppersmith et al., 2015b), Reddit Self-reported Depression Diagnosis (Yates et al., 2017), and “Language of Mental Health” (Gkotsis et al., 2016) datasets were the most reused resources, serving as the basis of 7, 17 with higher-level psychiatric conditions (e.g. symptoms of depression, stress events and stressor subjects) (Mowery et al., 2015; Lin et al., 2016). The dearth of anxiety-specific datasets was somewhat surprising given the condition’s prevalence and the abundance of pyschometric batteries for assessing anxiety (Cougle et al., 2009; Antony and Barlow, 2020). That said, generalized anxiety disorder (GAD)"
2021.clpsych-1.2,W15-1204,1,0.859718,"Missing"
2021.clpsych-1.2,W17-1612,1,0.883434,"essions to identify self-reported diagnoses or grouping individuals based on activity patterns, have provided opportunities to construct datasets aware of this heterogeneity (Coppersmith et al., 2015b; Kumar et al., 2015). However, they typically rely on oversimplifications that lack the same clinical validation and robustness as something like a mental health battery (Zhang et al., 2014; Ernala et al., 2019). Ethical considerations further complicate data acquisition, with the sensitive nature of mental health data requiring tremendous care when constructing, analyzing, and sharing datasets (Benton et al., 2017). Privacy-preserving measures, such as de-identifying individuals and requiring IRB approval to access data, have made it possible to share some data across research groups. However, these mechanisms can be technically cumbersome to implement and are subject to strict governance policies when clinical information is involved due to HIPAA (Price and Cohen, 2019). Moreover, many privacy-preserving practices require that signal relevant to modeling mental health, such as an individual’s demographics or their social network, are discarded (Bakken et al., 2004). This missingness has the potential t"
2021.clpsych-1.2,2020.acl-main.485,0,0.0701449,"Missing"
2021.clpsych-1.23,2021.eacl-main.256,1,0.778723,"ion for Computational Linguistics 2 Related Work Several existing papers have considered the role of demographics in mental health prediction. Elazar and Goldberg (2018) demonstrated that demographics are implicitly encoded in text data. Wood-Doughty et al. (2017) and Loveys et al. (2018) both studied differing language use across cultures. The former used a Twitter data set with inferred demographic labels, while the latter used a carefully-curated proprietary data set from 7 Cups of Tea. Amir et al. (2019) explored the role of cohort selection in assessing mental health disorder prevalence. Aguirre et al. (2021) is the closest to the present work. The authors characterized the biases present in depression prediction models by showing there are differences in performance for different demographic subgroups. This work studied biases that arise due to the specific data set used for training,focusing on the popular, publicly available data sets CLPsych (Coppersmith et al., 2015) and MULTITASK (Benton et al., 2017). The present work differs from those cited in that we seek to quantify demographic bias in depression prediction using self-disclosures in a publicly available data set. This approach improves"
2021.clpsych-1.23,W19-3013,1,0.909785,"orkshop on Computational Linguistics and Clinical Psychology, pages 217–223 June 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work Several existing papers have considered the role of demographics in mental health prediction. Elazar and Goldberg (2018) demonstrated that demographics are implicitly encoded in text data. Wood-Doughty et al. (2017) and Loveys et al. (2018) both studied differing language use across cultures. The former used a Twitter data set with inferred demographic labels, while the latter used a carefully-curated proprietary data set from 7 Cups of Tea. Amir et al. (2019) explored the role of cohort selection in assessing mental health disorder prevalence. Aguirre et al. (2021) is the closest to the present work. The authors characterized the biases present in depression prediction models by showing there are differences in performance for different demographic subgroups. This work studied biases that arise due to the specific data set used for training,focusing on the popular, publicly available data sets CLPsych (Coppersmith et al., 2015) and MULTITASK (Benton et al., 2017). The present work differs from those cited in that we seek to quantify demographic bi"
2021.clpsych-1.23,C18-1126,0,0.0392408,"Missing"
2021.clpsych-1.23,W14-3207,1,0.85902,"Missing"
2021.clpsych-1.23,D18-1002,0,0.0264863,"scuss the use of causal methodologies to assess our stronger hypothesis that gender confounds depression prediction. We highlight the types of methods that could be used and the data that is necessary to test the causal hypothesis. We conclude with a discussion of limitations and the ethical implications of this work. 217 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 217–223 June 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work Several existing papers have considered the role of demographics in mental health prediction. Elazar and Goldberg (2018) demonstrated that demographics are implicitly encoded in text data. Wood-Doughty et al. (2017) and Loveys et al. (2018) both studied differing language use across cultures. The former used a Twitter data set with inferred demographic labels, while the latter used a carefully-curated proprietary data set from 7 Cups of Tea. Amir et al. (2019) explored the role of cohort selection in assessing mental health disorder prevalence. Aguirre et al. (2021) is the closest to the present work. The authors characterized the biases present in depression prediction models by showing there are differences i"
2021.clpsych-1.23,W17-1601,0,0.0279651,"rs whose gender-related flair use was inconsistent (i.e. at least one post each with a male- and female-indicating flair). While people who identify as non-binary are known to have higher rates of depression (Budge et al., 2013; Wolohan et al., 2018) and thus could benefit from the studies like this one, we did not have a reliable method for identifying non-binary users beyond the list of inconsistent users and the sub-population in our cohort was too small to yield meaningful analysis. For the remainder of the paper we restrict attention to binary genders under the folk conception of gender (Larson, 2017). For each of the 26,381 gender-binary users, we collected the user’s entire Reddit posting and commenting history from January 1, 2019 to December 31, 2019, totaling 1,035,782 original submissions and 19,029,981 comments across 64,162 subreddits. Following the literature on social media-driven mental health surveillance (De Choudhury et al., 2013; Yates et al., 2017), we defined a user as true-depressed if they authored an original submission or comment in r/depression during the study period and true-control otherwise. The breakdown of gender and depression classes is 721 and 713 depressed m"
2021.clpsych-1.23,W18-0608,0,0.0440268,"ght the types of methods that could be used and the data that is necessary to test the causal hypothesis. We conclude with a discussion of limitations and the ethical implications of this work. 217 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 217–223 June 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work Several existing papers have considered the role of demographics in mental health prediction. Elazar and Goldberg (2018) demonstrated that demographics are implicitly encoded in text data. Wood-Doughty et al. (2017) and Loveys et al. (2018) both studied differing language use across cultures. The former used a Twitter data set with inferred demographic labels, while the latter used a carefully-curated proprietary data set from 7 Cups of Tea. Amir et al. (2019) explored the role of cohort selection in assessing mental health disorder prevalence. Aguirre et al. (2021) is the closest to the present work. The authors characterized the biases present in depression prediction models by showing there are differences in performance for different demographic subgroups. This work studied biases that arise due to the specific data set used"
2021.clpsych-1.23,D18-1004,0,0.0215267,"o difference in model performance when we do or don’t condition on gender. 6.2 Limitations Aside from the limitations described above, i) all users in our cohort posted in r/AskMen or r/AskWomen (which we used to derive ground truth) and ii) we rebalanced our data sets due to insufficient numbers of depressed users in the ‘representative’ population. These decisions could reduce the generalizability of our results. One way to address this would be to collect data on more users by expanding the study period and by consulting other subreddits with gender self-disclosure such as r/relationships (Wang and Jurgens, 2018). Additionally, while our use of self-disclosed genders increases scalability, this could induce bias in two ways. Users could be dishonest in their disclosure and, even if they aren’t, users who choose to self-disclose could be fundamentally different from the general population. It’s likely that the only solution is to collect data external to Reddit about Reddit users’ genders as a more reliable supplement to our data. Finally, our depression labels were not obtained via self-disclosures. Rather, they were defined based on 220 Figure 2: Features in common between the male- and female- train"
2021.clpsych-1.23,W18-4102,0,0.0246391,"post to be true-male if they used one of ‘Male’, ‘male’, ‘Dude’, or ♂ for their flair, and truefemale if they used one of ‘Female’, ‘female’, ♀, or ♀♥. Of the mined posts, 1,002,079 had some sort of flair, while 660,684 had one of the male or female indicator flairs. This process yielded a data set of 15,140 unique male and 11,241 unique female users, as well as 59 users whose gender-related flair use was inconsistent (i.e. at least one post each with a male- and female-indicating flair). While people who identify as non-binary are known to have higher rates of depression (Budge et al., 2013; Wolohan et al., 2018) and thus could benefit from the studies like this one, we did not have a reliable method for identifying non-binary users beyond the list of inconsistent users and the sub-population in our cohort was too small to yield meaningful analysis. For the remainder of the paper we restrict attention to binary genders under the folk conception of gender (Larson, 2017). For each of the 26,381 gender-binary users, we collected the user’s entire Reddit posting and commenting history from January 1, 2019 to December 31, 2019, totaling 1,035,782 original submissions and 19,029,981 comments across 64,162 s"
2021.clpsych-1.23,W17-2912,1,0.745515,"epression prediction. We highlight the types of methods that could be used and the data that is necessary to test the causal hypothesis. We conclude with a discussion of limitations and the ethical implications of this work. 217 Proceedings of the Seventh Workshop on Computational Linguistics and Clinical Psychology, pages 217–223 June 11, 2021. ©2021 Association for Computational Linguistics 2 Related Work Several existing papers have considered the role of demographics in mental health prediction. Elazar and Goldberg (2018) demonstrated that demographics are implicitly encoded in text data. Wood-Doughty et al. (2017) and Loveys et al. (2018) both studied differing language use across cultures. The former used a Twitter data set with inferred demographic labels, while the latter used a carefully-curated proprietary data set from 7 Cups of Tea. Amir et al. (2019) explored the role of cohort selection in assessing mental health disorder prevalence. Aguirre et al. (2021) is the closest to the present work. The authors characterized the biases present in depression prediction models by showing there are differences in performance for different demographic subgroups. This work studied biases that arise due to t"
2021.clpsych-1.23,D17-1322,0,0.0564045,"Missing"
2021.eacl-main.256,W19-3013,1,0.431586,"or depressive disorder was found to be more prevalent in females and White adults. Yet, it remains unclear whether these supposed differences in depression prevalence between gender and racial/ethnic demographic groups are the result of measurement error or other confounders. Various psychological studies have found mental health disorders, including depression, may manifest differently depending on cultural background and thus make uniform diagnosis a difficult proposition (Blanchard et al., 2020; Henrich et al., 2010). These ambiguities were highlighted by recent computational research from Amir et al. (2019), which found predictive rates of depression inferred using classifiers for social media data to not match previous US depression estimates. Indeed, the authors actually find that Black and Hispanic/Latinx individuals are more likely to be affected by depression than White individuals. Additionally, NLP and other data-driven algorithms have been shown to suffer from content biases; that is, undesirable group-wise differences with respect to protected groups, such as race/eth2932 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2"
2021.eacl-main.256,W17-1612,1,0.895031,"Missing"
2021.eacl-main.256,D14-1162,0,0.0839101,"Missing"
2021.eacl-main.256,W15-1203,0,0.0548762,"Missing"
2021.eacl-main.256,P14-1018,0,0.0792095,"Missing"
2021.eacl-main.256,W18-4102,0,0.506624,"Missing"
2021.eacl-main.256,W18-1114,1,0.865944,"Missing"
2021.emnlp-main.149,K19-1035,0,0.0237412,"projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evaluation and study how the encoders themselves—and components containing them—impact performance on a range of downstream tasks and languages. With a test bed of English–Arabic IE tasks, we find that adding projected silver training data overall yields i"
2021.emnlp-main.149,P15-1039,0,0.374766,"size and the number of supported languages. 3 Data Projection We create silver versions of the data by automatically projecting annotations from source English gold data to their corresponding machine translations in the target language.3 Data projection transfers word-level annotations in a source language to a target language via word-to-word alignments (Yarowsky et al., 2001). The technique has been used to create cross-lingual datasets for a variety of structured natural language processing tasks, including named entity recognition (Stengel-Eskin et al., 2019) and semantic role labeling (Akbik et al., 2015; Aminian et al., 2017; Fei et al., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zer"
2021.emnlp-main.149,I17-2003,0,0.3029,"of supported languages. 3 Data Projection We create silver versions of the data by automatically projecting annotations from source English gold data to their corresponding machine translations in the target language.3 Data projection transfers word-level annotations in a source language to a target language via word-to-word alignments (Yarowsky et al., 2001). The technique has been used to create cross-lingual datasets for a variety of structured natural language processing tasks, including named entity recognition (Stengel-Eskin et al., 2019) and semantic role labeling (Akbik et al., 2015; Aminian et al., 2017; Fei et al., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual pe"
2021.emnlp-main.149,W19-0417,0,0.0206532,"separately for each desired language pair instead of in joint multilingual training. We also examine self-training with translated text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect t"
2021.emnlp-main.149,J93-2003,0,0.0838433,"Missing"
2021.emnlp-main.149,C18-1233,0,0.0598207,"Missing"
2021.emnlp-main.149,W11-2103,0,0.0721601,"Missing"
2021.emnlp-main.149,Q18-1039,0,0.0224463,"with the exception of parsing. As data projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evaluation and study how the encoders themselves—and components containing them—impact performance on a range of downstream tasks and languages. With a test bed of English–Arabic IE tasks, we find that adding projec"
2021.emnlp-main.149,2020.acl-main.747,0,0.284752,"tilingual contextualized encoders 1 Introduction on each data projection component, and the use We consider zero-shot cross-lingual information of different encoders in task-specific models. We extraction (IE), in which training data exists in also offer suggestions for practitioners operating a source language but not in a target language. under different computation budgets on four tasks: Massively multilingual encoders like Multilingual event extraction, named entity recognition, part-ofBERT (mBERT; Devlin et al., 2019) and XLM- speech tagging, and dependency parsing, followRoBERTa (XLM-R; Conneau et al., 2020a) allow ing recent work that uses English-to-Arabic tasks for a strategy of training only on the source lan- as a test bed (Lan et al., 2020). We then apply guage data, trusting entirely in a shared underly- data projection and self-training to three structured ing feature representation across languages (Wu prediction tasks—named entity recognition, partand Dredze, 2019; Conneau et al., 2020b). How- of-speech tagging, and dependency parsing—in ever, in meta-benchmarks like XTREME (Hu et al., multiple target languages. Additionally, we use 2020), such cross-lingual performance on struc- self-"
2021.emnlp-main.149,2020.emnlp-main.382,0,0.285569,"Missing"
2021.emnlp-main.149,2020.findings-emnlp.147,0,0.0184073,". Our transducer uses the same architecture and hyperparameters as our baseline MT system, but with 1k BPE operations instead of 32k. On an internal held-out test set, we get a BLEU score of 96.9 with a unigram score of 98.6, implying few errors will propagate due to the denormalization process.13 Encoder Word Alignment Until recently, alignments have typically been obtained using unsupervised statistical models such as GIZA++ (Och and Ney, 2003) and fast-align (Dyer et al., 2013). Recent work has focused on using the similarities between contextualized embeddings to obtain alignments (Jalili Sabet et al., 2020; Daza and Frank, 2020; Dou and Neubig, 2021), achieving state-of-the-art performance. We use two automatic word alignment tools: fast-align, a widely used statistical alignment tool based on IBM models (Brown et al., 1993); and Awesome-align (Dou and Neubig, 2021), a contextualized embedding-based word aligner that extracts word alignments based on similarities of the tokens’ contextualized embeddings. Awesomealign achieves state-of-the-art performance on five language pairs. Optionally, Awesome-align can be fine-tuned on parallel text with objectives suitable for word alignment and on gold a"
2021.emnlp-main.149,li-etal-2012-parallel,0,0.0142817,"tool based on IBM models (Brown et al., 1993); and Awesome-align (Dou and Neubig, 2021), a contextualized embedding-based word aligner that extracts word alignments based on similarities of the tokens’ contextualized embeddings. Awesomealign achieves state-of-the-art performance on five language pairs. Optionally, Awesome-align can be fine-tuned on parallel text with objectives suitable for word alignment and on gold alignment data. We benchmark the word aligners on the gold standard alignments in the GALE Arabic–English Intrinsic Evaluation Table 2 shows the denor- Parallel Aligned Treebank (Li et al., 2012). We use malized and detokenized BLEU scores for English– the same data splits as Stengel-Eskin et al. (2019), Arabic MT systems with different encoders on the containing 1687, 299, and 315 sentence pairs in the 13 Denormalization code available at https://github. train, dev, and test splits, respectively. To obtain com/KentonMurray/ArabicDetokenizer alignments using fast-align, we append the test 1954 Model fast-align* Layer† AER P R F n/a 47.4 53.9 51.4 52.6 Awesome-align w/o FT mBERT GBv4 8 8 35.6 32.7 78.5 85.6 54.5 55.4 64.4 67.3 XLM-R L64K L128K 16 17 17 40.1 34.0 35.1 78.6 81.5 80.0 48."
2021.emnlp-main.149,2020.acl-main.156,0,0.350417,"Missing"
2021.emnlp-main.149,N19-4009,0,0.0382122,"Missing"
2021.emnlp-main.149,2021.eacl-main.221,0,0.0351332,"text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect to the source corpus. Fei et al. (2020) and Daza and Frank (2020) find that a non-bootstrapped approach works well for cross-lingua"
2021.emnlp-main.149,P17-1178,0,0.0258177,"arguments, aligned to the source span in the middle. Three and argument roles. For English, we use the same of the IE tasks we consider—ACE, named entity recognition, and BETTER—use span-based projec- English document splits as (Lin et al., 2020). That work does not consider Arabic, so for Arabic we tion, and we filter out projected target spans that are five times longer than the source spans. Two syntac- use the document splits from (Lan et al., 2020). tic tasks—POS tagging and dependency parsing— use token-based projection. For dependency pars- 4.2 Named Entity Recognition We use WikiAnn (Pan et al., 2017) for English– ing, following Tiedemann et al. (2014), we adapt the disambiguation of many-to-one mappings by Arabic and multilingual experiments. The labeling scheme is BIO with 3 types of named entities: PER, choosing as the head the node that is highest up in LOC, and ORG. On top of the encoder, we use a the dependency tree. In the case of a non-aligned linear classification layer with softmax to obtain dependency head, we choose the closest aligned word-level predictions. The labeling is word-level ancestor as the head. while the encoders operate at subword-level, thus, To address issues li"
2021.emnlp-main.149,Y18-1061,0,0.0278057,"d on an alignment of predicted and reference event structures. To find all events in a sentence and their corresponding arguments, we model the structure of the events as a tree, where event triggers are children of the “virtual root” of the sentence and arguments are children of event triggers (Cai et al., 2018). Each node is associated with a span in the text and is labeled with an event or argument type label. We use a model for event structure prediction that has three major components: a contextualized encoder, tagger, and typer (Xia et al., 2021).8 The tagger is a BiLSTM-CRF BIO tagger (Panchendrarajan and Amaresan, 2018) trained to predict child spans conditioned on parent spans and labels. 6 We use the following treebanks: Arabic-PADT, GermanGSD, English-EWT, Spanish-GSD, French-GSD, HindiHDTB, Russian-GSD, Vietnamese-VTB, and Chinese-GSD. 7 https://www.iarpa.gov/index.php/ research-programs/better 8 Code available at https://github.com/ hiaoxui/span-finder The typer is a feedforward network whose inputs are a parent span representation, parent label embedding, and child span representation. The tree is produced level-wise at inference time, first predicting event triggers, typing them, and then predicting a"
2021.emnlp-main.149,D18-1039,0,0.0271163,"roj). This could be the result of noise from alignments of various quality mutually interfering. In fact, selftraining with the same translated text (+Self) outperforms data projection and zero-shot scenarios, again with the exception of parsing. As data projection and self-training use the same translated text and differ only by label source, the results indicate that the external knowledge from frozen mBERTbased alignment is worse than what the model learns from source language data alone. Thus, further performance improvement could be achieved with an improved aligner. 7 Related Work tors (Platanios et al., 2018), or language-specific semantic spaces (Luo et al., 2021). Conversely, adversarial training (Ganin et al., 2016) has been used to discourage models from learning language-specific information (Chen et al., 2018; Keung et al., 2019; Ahmad et al., 2019). 8 Conclusion In this paper, we explore data projection and the use of silver data in zero-shot cross-lingual IE, facilitated by neural machine translation and word alignment. Recent advances in pretrained encoders have improved machine translation systems and word aligners in terms of intrinsic evaluation. We conduct an extensive extrinsic evalu"
2021.emnlp-main.149,W18-6319,0,0.0122623,"gradient updates are applied to them during MT training, whereas the randomly initialized baselines are updated. A preliminary experiment in Zhu et al. (2020) uses a related system that leverages the last layer of BERT. However, that experiment was monolingual, and our hypothesis is that the shared embedding space of a multilingual encoder will aid in training a translation system. BLEU Public 12.7 None 14.9 mBERT GBv4 15.7 15.7 XLM-R L64K L128K 16.0 16.2 15.8 Table 2: BLEU scores of MT systems with different pre-trained encoders on English–Arabic IWSLT’17. IWLST’17 test set using sacreBLEU (Post, 2018). The use of contextualized embeddings from pretrained encoders results in better performance than using a standard randomly initialized MT model regardless of which encoder is used. The best performing system uses our bilingual L64K encoder, but all pretrained encoder-based systems perform well and within 0.5 BLEU points of each other. We hypothesize that the MT systems are able to leverage the shared embedding spaces of the pretrained language models in order to assist with translation. 5.3 Denormalization System Generating text in Arabic is a notoriously difficult problem due to data sparsi"
2021.emnlp-main.149,L16-1144,0,0.0288817,"train with 9.2B words of Arabic text and 26.8B words of English text, more than either XLM-R (2.9B words/23.6B words) or GBv4 (4.3B words/6.1B words).10 We build two English–Arabic joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language model"
2021.emnlp-main.149,C02-1070,0,0.280509,"ead of in joint multilingual training. We also examine self-training with translated text to assess when data projection helps crosslingual transfer, and find it to be another viable option for obtaining labels for some tasks. In future work, we will explore how to improve alignment quality and how to combine data projection and self-training techniques. Although projected data may be of lower quality than the original source data due to errors in translation or alignment, it is useful for tasks such as semantic role labeling (Akbik et al., 2015; Aminian et al., 2019), information extraction (Riloff et al., 2002), POS tagging (Yarowsky and Ngai, 2001), and dependency parsing (Ozaki et al., 2021). The intuition is that although the projected data may be noisy, training on it gives a model useful information about the statistics of the target language. Akbik et al. (2015) and Aminian et al. (2017) use bootstrapping algorithms to iteratively construct projected datasets for semantic role labeling. Akbik et al. (2015) additionally use manually defined filters to maintain high data quality, which results in a projected dataset that has low recall with respect to the source corpus. Fei et al. (2020) and Daz"
2021.emnlp-main.149,2021.emnlp-main.802,0,0.0698842,"Missing"
2021.emnlp-main.149,2013.iwslt-evaluation.8,1,0.754834,"gual L64K encoder, but all pretrained encoder-based systems perform well and within 0.5 BLEU points of each other. We hypothesize that the MT systems are able to leverage the shared embedding spaces of the pretrained language models in order to assist with translation. 5.3 Denormalization System Generating text in Arabic is a notoriously difficult problem due to data sparsity problems arising from the morphological richness of the language, frequently necessitating destructive normalization schemes during training that must be heuristically undone in postprocessing to ensure well-formed text (Sajjad et al., 2013). All of the most common multilingual pretrained encoders use a form of destructive normalization which removes diacritics, which causes MT systems to translate into normalized Arabic text. To generate valid Arabic text, we train a sequenceto-sequence model that transduces normalized text into unnormalized text using the Arabic side of our bitext, before and after normalization. Our transducer uses the same architecture and hyperparameters as our baseline MT system, but with 1k BPE operations instead of 32k. On an internal held-out test set, we get a BLEU score of 96.9 with a unigram score of"
2021.emnlp-main.149,2021.eacl-main.115,0,0.037193,"Arabic text and 26.8B words of English text, more than either XLM-R (2.9B words/23.6B words) or GBv4 (4.3B words/6.1B words).10 We build two English–Arabic joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretrai"
2021.emnlp-main.149,W17-2619,0,0.0166884,"joint vocabularies using SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available ht"
2021.emnlp-main.149,P16-1162,0,0.00515715,"slation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsinki-NLP/ opus-mt-en-ar 1953 10 at as inputs to the encoder. For the decoder vocabulary, these systems all use the GBv4 vocabulary regardless of which pretrained language model was used to augment"
2021.emnlp-main.149,D19-1084,1,0.845728,"Missing"
2021.emnlp-main.149,2020.emnlp-main.8,0,0.0191688,"SentencePiece (Kudo and Richardson, 2018), resulting in two encoders: L64K and L128K. For the latter, we additionally enforce coverage of all Arabic characters after normalization. 5.2 Machine Translation For all of our MT experiments, we use a dataset of 2M sentences from publicly available data including the UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsi"
2021.emnlp-main.149,2020.wmt-1.139,0,0.407731,"UN corpus, Global Voices, wikimatrix, and newscommentary11 (Ziemski et al., 2016; Prokopidis et al., 2016; Schwenk et al., 2021; CallisonBurch et al., 2011). We pre-filtered the data using LASER scores to ensure high quality translations are used for our bitext (Schwenk and Douze, 2017; Thompson and Post, 2020). All of our systems are based on the Transformer architecture (Vaswani et al., 2017).11 Our baseline system uses a joint English–Arabic vocabulary with 32k BPE operations (Sennrich et al., 2016). The public system is a publicly released model that has been demonstrated to perform well (Tiedemann, 2020).12 The other systems use contextualized embeddings from frozen pretrained language models 9 Details of pretraining can be found in Appendix B. We measure word count with wc -w. 11 See Appendix C for a full list of hyperparameters. 12 The public MT model is available https://huggingface.co/Helsinki-NLP/ opus-mt-en-ar 1953 10 at as inputs to the encoder. For the decoder vocabulary, these systems all use the GBv4 vocabulary regardless of which pretrained language model was used to augment the encoder. Incorporating Pretrained LMs In order to make use of the pretrained language models, we use the"
2021.emnlp-main.149,W14-1614,0,0.0771516,"Missing"
2021.emnlp-main.149,D19-1077,1,0.786563,"., 2020). To create silver data, as shown in Figure 1, we: (1) translate the source text to the target language using the MT system described in Section 5.2, (2) obtain word alignments between the original and translated parallel text using a word alignment tool, and (3) project the annotations along the word alignments. We then combine silver target data with gold source data to augment the training set for the structured prediction task. For step (1), we rely on a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the support"
2021.emnlp-main.149,2020.repl4nlp-1.16,1,0.925623,"a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/cro"
2021.emnlp-main.149,2020.emnlp-main.362,1,0.895717,"a variety of source-toWhile massively multilingual encoders like mBERT and XLM-R enable strong zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/cro"
2021.emnlp-main.149,2021.eacl-demos.19,1,0.828272,"Missing"
2021.emnlp-main.149,2021.adaptnlp-1.22,1,0.328699,"Missing"
2021.emnlp-main.149,2021.naacl-main.41,0,0.0220796,"ng zero-shot crosslingual performance (Wu and Dredze, 2019; Conneau et al., 2020a), they suffer from the curse of multilinguality (Conneau et al., 2020a): crosslingual effectiveness suffers as the number of supported languages increases for a fixed model size. We would therefore expect that when restricted to only the source and target languages, a bilingual model should perform better than (or at least on par with) a multilingual model of the same size, assuming both languages have sufficient corpora (Wu and Dredze, 2020a). If a practitioner is interested 1 We do not include multilingual T5 (Xue et al., 2021) as in only a small subset of the supported languages, it is still an open question on how to best utilize text-to-text models for structured prediction tasks (Ruder et al., 2021). is the multilingual model still the best option? 2 L128K available at https://huggingface.co/ To answer this question, we use English and jhu-clsp/roberta-large-eng-ara-128k 3 Arabic as a test bed. In Table 1, we summarize exCode available at https://github.com/ isting publicly available encoders that support both shijie-wu/crosslingual-nlp 1951 Multilingual Bilingual Base Large mBERT (Devlin et al.) GBv4 (Lan et al"
2021.emnlp-main.149,P95-1026,0,0.837635,"ord alignment. If bilingual encoders exist, using them in aligners requires little additional computation. options should be explored if one’s budget allows. In terms of computation budget, using pretrained encoders in a custom MT system requires medium additional computation. Impact of Label Source To assess the quality of Impact of Encoder on MT By comparing the projected annotations in the silver data, we congroups C and E, we observe the performance differ- sider a different way to automatically label transence between the bilingual encoder based MT and lated sentences: self-training (ST; Yarowsky, 1995). the public MT depends on the task and encoder, and For self-training, we translate the source data to the neither MT system clearly outperforms the other in target language, label the translated data using a all settings, despite the bilingual encoder having a zero-shot model trained on source data, and combetter BLEU score. The results suggest that both bine the labeled translations with the source data to 1956 Encoder Data ar de en es fr hi ru vi zh Average NER (F1) mBERT Zero-shot + Self + Proj + Proj (Bi) 41.6 +7.7 -5.8 +0.3 78.8 -0.5 -0.6 -0.7 83.9 +0.4 +0.3 +0.1 73.1 +4.8 +3.6 +5.2 79."
2021.findings-acl.52,Q17-1010,0,0.0295704,"y linker with Wikipedia as the KB. This approach typically uses English Wikipedia as the KB, though it could use a KB in other languages. Tsai and Roth (2016) use a two-step linking approach, first using an IR-based triage system (which we also use). Second, they use a candidate ranking step based on a linear ranking SVM model with several features, including contextual, document, and coreference. The most closely related work to our own is that of Upadhyay et al. (2018), who use multilingual embeddings as the basis for their representations, and Wikipedia as training data. They use FastText (Bojanowski et al., 2017; Smith et al., 2017) 584 ... el jefe de la Oficina de la Presidencia (m.01p1k, ORG), Aurelio Nu˜no y ... name President of Mexico (m.01p1k) desc. The President of the United ... type government office Figure 1: Example Spanish mention Oficina de la Presidencia, which is a link to entity President of Mexico, and the architecture for our neural ranker, using that example and a negatively-sampled entity The Office. to align embeddings across languages, and a small dictionary to identify alignments. They pass these representations through a convolutional neural network to create a mention represe"
2021.findings-acl.52,P19-4007,0,0.0245528,"Missing"
2021.findings-acl.52,N19-1423,0,0.195477,"English entity name and entity description, as well as compare the mention and entity type. Previous work has focused on transliteration or translation approaches for name and context (McNamee et al., 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to"
2021.findings-acl.52,C10-1032,1,0.541258,"he remaining challenges in zeroshot entity linking should focus on topic adaptation, instead of improvements in cross-lingual representations. In summary, this paper uses a simple ranker to explore effective cross-language entity linking with multiple languages. We demonstrate its effectiveness at zero-shot linking, evaluate a pre-training objective to improve zero-shot transfer, and lay out guidelines to inform future research on zero-shot linking. 2 Cross-Language Entity Linking A long line of work on entity linking has developed standard models to link textual mentions to entities in a KB (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). The models in this area have served as the basis for developing multilingual and cross-language entity linking systems, and they inform our own model development. We define multilingual to mean a model that can operate on mentions from more than one language at the same time (link both English and Chinese mentions to an ontology) and cross-language to refer to linking mentions in one language (e.g., Spanish) to an ontology in another (e.g., English). A common approach to cross-language entity linking is to use transliteration data to transform no"
2021.findings-acl.52,Q14-1037,0,0.0272262,"es in zeroshot entity linking should focus on topic adaptation, instead of improvements in cross-lingual representations. In summary, this paper uses a simple ranker to explore effective cross-language entity linking with multiple languages. We demonstrate its effectiveness at zero-shot linking, evaluate a pre-training objective to improve zero-shot transfer, and lay out guidelines to inform future research on zero-shot linking. 2 Cross-Language Entity Linking A long line of work on entity linking has developed standard models to link textual mentions to entities in a KB (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). The models in this area have served as the basis for developing multilingual and cross-language entity linking systems, and they inform our own model development. We define multilingual to mean a model that can operate on mentions from more than one language at the same time (link both English and Chinese mentions to an ontology) and cross-language to refer to linking mentions in one language (e.g., Spanish) to an ontology in another (e.g., English). A common approach to cross-language entity linking is to use transliteration data to transform non-English mentions into E"
2021.findings-acl.52,D17-1284,0,0.0232647,"king should focus on topic adaptation, instead of improvements in cross-lingual representations. In summary, this paper uses a simple ranker to explore effective cross-language entity linking with multiple languages. We demonstrate its effectiveness at zero-shot linking, evaluate a pre-training objective to improve zero-shot transfer, and lay out guidelines to inform future research on zero-shot linking. 2 Cross-Language Entity Linking A long line of work on entity linking has developed standard models to link textual mentions to entities in a KB (Dredze et al., 2010; Durrett and Klein, 2014; Gupta et al., 2017). The models in this area have served as the basis for developing multilingual and cross-language entity linking systems, and they inform our own model development. We define multilingual to mean a model that can operate on mentions from more than one language at the same time (link both English and Chinese mentions to an ontology) and cross-language to refer to linking mentions in one language (e.g., Spanish) to an ontology in another (e.g., English). A common approach to cross-language entity linking is to use transliteration data to transform non-English mentions into English strings. Early"
2021.findings-acl.52,2010.amta-papers.12,0,0.0774398,"anguage – e.g., English BERT for Spanish, Arabic BERT for Farsi – would produce acceptable results. Again, as shown in Table 4, the performance is most often worse, illustrating that mBERT is an important aspect of the linker’s performance. 7 7.1 Improving Zero-shot Transfer Name Matching Objective Given the importance of matching the mention string with the entity name, will improving this component enhance zero-shot transfer? While obtaining within-language entity linking data isn’t possible in a zero-shot setting, we can use pairs of translated names, which are often more easily available (Irvine et al., 2010; Peng et al., 2015). Since Chinese performance suffers the most zero-shot performance reduction when compared to the multilingual setting, we use Chinese English name pair data (Huang, 2005) to support an auxiliary training objective. An example name pair: “巴尔的摩－俄 亥俄铁路公司” and Baltimore & Ohio Railroad. We augment model training as follows. For each update in a mini-batch, we first calculate the loss of the subset of the model that scores the mention string and entity name on a randomly selected pair k = 25, 000 of the Chinese/English name pair corpus. We score the Chinese name z and the corre"
2021.findings-acl.52,D19-1279,0,0.0246304,"mounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to combine these representations, along with a one-hot embedding for the entity and mention type, to produce a cross-language linker. We use this ranking architecture to highlight the ability of mBERT to perform on this task without a more complex architectu"
2021.findings-acl.52,K19-1020,0,0.018957,"6) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to combine these representations, along with a one-hot embedding for the entity and mention type, to produce a cross-language linker. We use this ranking architecture to highlight the ability of mBERT to perform on this task without a more complex architecture. Although previous work tends to use multil"
2021.findings-acl.52,I11-1029,1,0.507761,"formance. We conduct several analyses to identify the sources of performance degradation in the zero-shot setting. Results indicate that while multilingual transformer models transfer well between languages, issues remain in disambiguating similar entities unseen in training. 1 Introduction Entity linking grounds named entities mentioned in text, such as Chancellor, to a reference knowledge base (KB) or ontology entry, such as Angela Merkel. Historically, entity linking work focused on English documents and knowledge bases, but subsequent work expanded the task to consider multiple languages (McNamee et al., 2011). In cross-language entity linking, entities in a set of multilingual documents is linked to a KB in a single language. The TAC KBP shared task (Ji et al., 2015), for example, links mentions in Chinese and Spanish documents with an English KB. Success in building cross-language linking systems can be helpful in tasks such as discovering all documents relevant to an entity, regardless of language. Successfully linking a mention across languages requires adapting several common entity linking components to the cross-language setting. Consider the example in Figure 1, which contains the Spanish m"
2021.findings-acl.52,2020.acl-main.720,1,0.84268,"(2018), which is largely based on work in Tsai and Roth (2016). This allows us to score a smaller set of entities for each mention as opposed to the entire KB. For a give mention m, a triage system will provide a set of k candidate entities e1 . . . ek . The system uses Wikipedia cross5 Model Evaluation We consider several different training and evaluation settings to explore the multilingual ability of transformers on this task. Recent studies suggest that multilingual models can achieve similar or even better performance on cross-language entity linking (Upadhyay et al., 2018). Other work (Mueller et al., 2020) has shown that this is not always the case. Therefore, we begin by asking: does our linker do better when trained on all languages (multilingual cross-language) or trained separately on each individual language (monolingual cross-language)? We train our model on each of the 7 individual languages in the two datasets (noted as Mono). Next, we train a single model for each dataset (3 languages in TAC, 4 in Wiki, each noted as Multi). Mono and Multi share the exact same architecture there are no multilingual adjustments made, and the model contains no language-specific features. As Multi uses da"
2021.findings-acl.52,N15-1119,0,0.0308833,"ss languages requires adapting several common entity linking components to the cross-language setting. Consider the example in Figure 1, which contains the Spanish mention Oficina de la Presidencia, a reference to the entity President of Mexico in an English KB. To link the mention to the relevant entity we must compare the mention text and its surrounding textual context in Spanish to the English entity name and entity description, as well as compare the mention and entity type. Previous work has focused on transliteration or translation approaches for name and context (McNamee et al., 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsi"
2021.findings-acl.52,P17-1178,0,0.148407,"same time (link both English and Chinese mentions to an ontology) and cross-language to refer to linking mentions in one language (e.g., Spanish) to an ontology in another (e.g., English). A common approach to cross-language entity linking is to use transliteration data to transform non-English mentions into English strings. Early transliteration work (McNamee et al., 2011) uses a transliteration corpus to train a support vector machine ranker, which uses common entity linking features such as name and context matching, co-occurring entities, and an indicator for NIL (no matching candidate.) Pan et al. (2017) uses transliteration data for a set of 282 languages to generate all possible combinations of mentions. A related approach is to use machine translation to translate a document into English, and then use an English entity linker. However, an MT system may not be available, and it further needs a specialized name module to properly translate entity names. Several systems from the TAC 2015 KBP Entity Discovery and Linking task (Ji et al., 2015) translate nonEnglish documents into English, then use standard Entity Linking systems. Cross-language Wikification is a closely related task, which uses"
2021.findings-acl.52,P15-2062,1,0.83198,"sh BERT for Spanish, Arabic BERT for Farsi – would produce acceptable results. Again, as shown in Table 4, the performance is most often worse, illustrating that mBERT is an important aspect of the linker’s performance. 7 7.1 Improving Zero-shot Transfer Name Matching Objective Given the importance of matching the mention string with the entity name, will improving this component enhance zero-shot transfer? While obtaining within-language entity linking data isn’t possible in a zero-shot setting, we can use pairs of translated names, which are often more easily available (Irvine et al., 2010; Peng et al., 2015). Since Chinese performance suffers the most zero-shot performance reduction when compared to the multilingual setting, we use Chinese English name pair data (Huang, 2005) to support an auxiliary training objective. An example name pair: “巴尔的摩－俄 亥俄铁路公司” and Baltimore & Ohio Railroad. We augment model training as follows. For each update in a mini-batch, we first calculate the loss of the subset of the model that scores the mention string and entity name on a randomly selected pair k = 25, 000 of the Chinese/English name pair corpus. We score the Chinese name z and the correctly matched English"
2021.findings-acl.52,P19-1493,0,0.0247283,"n English KB. To link the mention to the relevant entity we must compare the mention text and its surrounding textual context in Spanish to the English entity name and entity description, as well as compare the mention and entity type. Previous work has focused on transliteration or translation approaches for name and context (McNamee et al., 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking t"
2021.findings-acl.52,D19-1410,0,0.0181293,"mbeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to combine these representations, along with a one-hot embedding for the entity and mention type, to produce a cross-language linker. We use this ranking architecture to highlight the ability of mBERT to perform on this task without a more complex architecture. Although previous work tends to use multilingual encoders for one langu"
2021.findings-acl.52,2020.semeval-1.271,0,0.0362033,"ving a marginal effect. This highlights the importance of the multilingual encoder, since both name and context rely on effective multilingual representations. Separately, how does using a multilingual transformer model, such as mBERT, affect the performance of our ranker? First, it is possible that using a monolingual linker with a BERT model trained only on the target language would improve performance, since such a model does not need to represent several languages as the same time. As shown in Table 4, model performance for these settings is largely worse for English-only and Arabic-only (Safaya et al., 2020) models when compared to using mBERT, with the exception that precision increases significantly for English. Second, perhaps a monolingual linker with a BERT model trained only on a related language – e.g., English BERT for Spanish, Arabic BERT for Farsi – would produce acceptable results. Again, as shown in Table 4, the performance is most often worse, illustrating that mBERT is an important aspect of the linker’s performance. 7 7.1 Improving Zero-shot Transfer Name Matching Objective Given the importance of matching the mention string with the entity name, will improving this component enhan"
2021.findings-acl.52,N16-1072,0,0.329425,"e cross-language setting. Consider the example in Figure 1, which contains the Spanish mention Oficina de la Presidencia, a reference to the entity President of Mexico in an English KB. To link the mention to the relevant entity we must compare the mention text and its surrounding textual context in Spanish to the English entity name and entity description, as well as compare the mention and entity type. Previous work has focused on transliteration or translation approaches for name and context (McNamee et al., 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo"
2021.findings-acl.52,D19-1374,0,0.0112173,", 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to combine these representations, along with a one-hot embedding for the entity and mention type, to produce a cross-language linker. We use this ranking architecture to highlight the ability of mBERT"
2021.findings-acl.52,D18-1270,0,0.0321801,"Missing"
2021.findings-acl.52,D19-1575,0,0.0183033,"leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslanguage entity linking to handle the mention text, entity name, mention context and entity description1 . We use a neural ranking objective and a deep learning model to combine these representations, along with a one-hot embedding for the entity and mention type, to produce a cross-language linker. We use this ranking architecture to highlight the ability of mBERT to perform on this task wit"
2021.findings-acl.52,D19-1077,1,0.928724,"sident of Mexico in an English KB. To link the mention to the relevant entity we must compare the mention text and its surrounding textual context in Spanish to the English entity name and entity description, as well as compare the mention and entity type. Previous work has focused on transliteration or translation approaches for name and context (McNamee et al., 2011; Pan et al., 2015), or leveraging large amounts of crosslanguage information (Tsai and Roth, 2016) and multilingual embeddings (Upadhyay et al., 2018). Since this work emerged, there have been major advances in multilingual NLP (Wu and Dredze, 2019; Pires et al., 2019). Mainstream approaches to multilingual learning now use multilingual encoders, trained on raw text from multiple languages (Devlin et al., 2019). These models, such as multilingual BERT or XMLR (Conneau et al., 2019), have achieved impressive results on a range of multilingual NLP tasks, including part of speech tagging (Tsai et al., 2019), parsing (Wang et al., 2019; Kondratyuk and Straka, 2019), and semantic similarity (Lo and Simard, 2019; Reimers and Gurevych, 2019). We propose to leverage text representations with multilingual BERT (Devlin et al., 2019) for crosslang"
2021.naacl-main.243,D15-1075,0,0.0264846,"d use τ = 100 here. We label each document with its most probable topic by counting the number of tokens in the document in the top-10 token list for each topic, then taking the argmax. We perform the same procedure on the out-of-domain COVID dataset to generate out-of-domain topic classification supervision, finding that τ = 80 is best on this dataset with respect to NPMI coherence. For the document classification task, we use MLDoc (Schwenk and Li, 2018), a multilingual news dataset; we fine-tune on the English data. For NLI, we follow Reimers and Gurevych (2020) in using a mixture of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), both of which only contain English data. where LELBO is the negated evidence lower bound objective of the CTM, and LNLL is the negative log- Training Details We consider embeddings produced by both mBERT (Devlin et al., 2018) and likelihood loss over topic classifications. We refer XLM-R (Conneau et al., 2019). For fine-tuning, we to this as the topic classification contextualized append to these models a fully-connected layer foltopic modeling (TCCTM) loss, denoted LTCCTM . 3 TCCTM modifies the topic model, but not the https://linguatools.org/tools/corpo"
2021.naacl-main.243,2020.tacl-1.29,0,0.0424934,"s mBERT (Devlin et al., 2018) or XLM-R (Conneau et al., 2019), then the topic model becomes implicitly polylingual due to the unsupervised alignments induced between languages during pre-training. This is distinct from how polylinguality is induced in approaches based on Latent Dirichlet Allocation (LDA; Blei et al., 2003), which require some form of cross-lingual alignments (Mimno et al., 2009). Using embeddings in topic models is not new (Das et al., 2015; Liu et al., 2015; Li et al., 2016). While a few recent approaches have leveraged word embeddings for topic modeling (Gupta et al., 2019; Dieng et al., 2020; Sia et al., 2020), none of these have investigated cross-lingual topic transfer. Polylingual Topic Models Polylingual topic models require some form of cross-lingual alignNeural Topic Models Neural topic models ments, which can come from comparable docu(NTMs) are defined by their parameterization ments (Mimno et al., 2009), word alignments (Zhao by (deep) neural networks or incorporation of and Xing, 2006), multilingual dictionaries (Jagarlaneural elements. This approach has become mudi and Daumé, 2010), code-switched documents practical largely due to advances in variational (Peng et al., 2"
2021.naacl-main.243,D18-1096,0,0.060737,"Missing"
2021.naacl-main.243,D09-1092,0,0.176106,"Missing"
2021.naacl-main.243,P14-2110,1,0.813393,"g et al., 2020; Sia et al., 2020), none of these have investigated cross-lingual topic transfer. Polylingual Topic Models Polylingual topic models require some form of cross-lingual alignNeural Topic Models Neural topic models ments, which can come from comparable docu(NTMs) are defined by their parameterization ments (Mimno et al., 2009), word alignments (Zhao by (deep) neural networks or incorporation of and Xing, 2006), multilingual dictionaries (Jagarlaneural elements. This approach has become mudi and Daumé, 2010), code-switched documents practical largely due to advances in variational (Peng et al., 2014), or other distant alignments such inference—specifically, variational autoencoders as anchors (Yuan et al., 2018). Work on incompara(VAEs; Kingma and Welling, 2013). The Neural ble documents with soft document links (Hao and Paul, 2018) still relies on dictionaries. 1 https://github.com/aaronmueller/ contextualized-topic-models While these types of alignments have been com3055 2 Background mon in multilingual learning (Ruder et al., 2019b), they no longer represent the state-of-the-art. More recent approaches instead tend to employ large pretrained multilingual models (Wu and Dredze, 2019) th"
2021.naacl-main.243,P19-1493,0,0.0271697,"tilingual BERT (mBERT; Dewhen using representations from multilingual vlin et al., 2018) or XLM-RoBERTa (XLM-R; Conmodels is that they facilitate zero-shot polylinneau et al., 2019) can produce a representation of gual topic modeling. However, while it has been widely observed that pre-trained embedtext in a shared subspace across multiple input landings should be fine-tuned to a given task, it is guages, suitable for both monolingual and multilinnot immediately clear what supervision should gual settings, including zero-shot language transfer look like for an unsupervised task such as topic (Pires et al., 2019). modeling. Thus, we propose several methSimultaneously, topic models have increasingly ods for fine-tuning encoders to improve both incorporated neural components. This has included monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on inference networks which learn representations of auxiliary tasks, constructing a new topic clasthe input document (Miao et al., 2017; Srivastava sification task, integrating the topic classificaand Sutton, 2017) that improve over using bags tion objective directly into topic model trainof words directly, as well as replacing b"
2021.naacl-main.243,D19-1410,0,0.260753,"les which do not need to be transformed. ProdLDA uses an inference network with a VAE to map from an input bag of words to a continuous latent representation. The decoder network samples from this hidden representation to form latent topic representations. Bags of words are reconstructed for each latent space; these constitute the output topics. Others have reported that ProdLDA is the best-performing NTM with respect to topic coherence (Miao et al., 2017). Contextualized topic models (CTMs; Bianchi et al., 2020a,b) extend ProdLDA by replacing the input bag of words with sentence-BERT (SBERT; Reimers and Gurevych, 2019) embeddings. If the SBERT embeddings are based on a multilingual model such as mBERT (Devlin et al., 2018) or XLM-R (Conneau et al., 2019), then the topic model becomes implicitly polylingual due to the unsupervised alignments induced between languages during pre-training. This is distinct from how polylinguality is induced in approaches based on Latent Dirichlet Allocation (LDA; Blei et al., 2003), which require some form of cross-lingual alignments (Mimno et al., 2009). Using embeddings in topic models is not new (Das et al., 2015; Liu et al., 2015; Li et al., 2016). While a few recent appro"
2021.naacl-main.243,2020.emnlp-main.365,0,0.0418,"Missing"
2021.naacl-main.243,N19-5004,0,0.0269667,"ctionaries (Jagarlaneural elements. This approach has become mudi and Daumé, 2010), code-switched documents practical largely due to advances in variational (Peng et al., 2014), or other distant alignments such inference—specifically, variational autoencoders as anchors (Yuan et al., 2018). Work on incompara(VAEs; Kingma and Welling, 2013). The Neural ble documents with soft document links (Hao and Paul, 2018) still relies on dictionaries. 1 https://github.com/aaronmueller/ contextualized-topic-models While these types of alignments have been com3055 2 Background mon in multilingual learning (Ruder et al., 2019b), they no longer represent the state-of-the-art. More recent approaches instead tend to employ large pretrained multilingual models (Wu and Dredze, 2019) that induce unsupervised alignments between languages during pre-training. 3 Fine-Tuning Encoders Fine-tuning is known to improve an encoder’s representations for a specific task when data directly related to the task is present (Howard and Ruder, 2018; Wu and Dredze, 2019). Nonetheless, this requires supervised data, which is absent in unsupervised tasks like ours. We consider several approaches to create fine-tuning supervision for topic"
2021.naacl-main.243,L18-1560,0,0.161521,"the target task. What task can serve as an effective auxiliary task for topic modeling? We turn to document classification, the task of identifying the primary topic present in a document from a fixed set of (typically human-identified and human-labeled) topics. We may not have a document classification dataset from the same domain as the topic modeling corpus, nor a dataset which uses the same topics as those present in the corpus. However, fine-tuning could teach the encoder to produce topic-level document representations, regardless of the specific topics present in the data. We use MLDoc (Schwenk and Li, 2018), a multilingual news document classification dataset and fine-tune on English. 3.2 Fine-tuning on Topic Models The auxiliary tasks use data from a different domain (and task) than the domain of interest for the topic model. Can we bootstrap more direct supervision on our data? We employ an LDA-based topic model to produce a form of topic supervision. We first run LDA on the target corpus to generate topic distributions for each document. Then, we use the inferred topic distributions as supervision by labeling each document with its most probable topic. We fine-tune on this data as we did for"
2021.naacl-main.243,2020.emnlp-main.135,0,0.0230966,"l., 2018) or XLM-R (Conneau et al., 2019), then the topic model becomes implicitly polylingual due to the unsupervised alignments induced between languages during pre-training. This is distinct from how polylinguality is induced in approaches based on Latent Dirichlet Allocation (LDA; Blei et al., 2003), which require some form of cross-lingual alignments (Mimno et al., 2009). Using embeddings in topic models is not new (Das et al., 2015; Liu et al., 2015; Li et al., 2016). While a few recent approaches have leveraged word embeddings for topic modeling (Gupta et al., 2019; Dieng et al., 2020; Sia et al., 2020), none of these have investigated cross-lingual topic transfer. Polylingual Topic Models Polylingual topic models require some form of cross-lingual alignNeural Topic Models Neural topic models ments, which can come from comparable docu(NTMs) are defined by their parameterization ments (Mimno et al., 2009), word alignments (Zhao by (deep) neural networks or incorporation of and Xing, 2006), multilingual dictionaries (Jagarlaneural elements. This approach has become mudi and Daumé, 2010), code-switched documents practical largely due to advances in variational (Peng et al., 2014), or other dist"
2021.naacl-main.243,N18-1101,0,0.022513,"ocument with its most probable topic by counting the number of tokens in the document in the top-10 token list for each topic, then taking the argmax. We perform the same procedure on the out-of-domain COVID dataset to generate out-of-domain topic classification supervision, finding that τ = 80 is best on this dataset with respect to NPMI coherence. For the document classification task, we use MLDoc (Schwenk and Li, 2018), a multilingual news dataset; we fine-tune on the English data. For NLI, we follow Reimers and Gurevych (2020) in using a mixture of SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), both of which only contain English data. where LELBO is the negated evidence lower bound objective of the CTM, and LNLL is the negative log- Training Details We consider embeddings produced by both mBERT (Devlin et al., 2018) and likelihood loss over topic classifications. We refer XLM-R (Conneau et al., 2019). For fine-tuning, we to this as the topic classification contextualized append to these models a fully-connected layer foltopic modeling (TCCTM) loss, denoted LTCCTM . 3 TCCTM modifies the topic model, but not the https://linguatools.org/tools/corpora/ wikipedia-comparable-corpora/ emb"
2021.naacl-main.243,D19-1077,1,0.930651,"sformer-based language models to en- this line of work omits a key step in using con3054 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3054–3068 June 6–11, 2021. ©2021 Association for Computational Linguistics textualized embeddings: fine-tuning. It has been widely observed that task specific fine-tuning of pretrained embeddings, even with a small amount of supervised data, can significantly improve performance on many tasks, including in zero- and few-shot settings (Howard and Ruder, 2018; Wu and Dredze, 2019). However, in the case of unsupervised topic modeling, from where are we to obtain task-specific supervised training data? We propose an investigation of how supervision should be bootstrapped to improve language encoders for monolingual and polylingual topic model learning. We also propose a set of experiments to better understand why certain forms of supervision are effective in this unsupervised task. Our contributions include the following: 1. We fine-tune contextualized sentence embeddings on various established auxiliary tasks, finding that many different tasks can be used to improve dow"
2021.naacl-main.243,2020.emnlp-main.608,0,0.0815429,"Missing"
2021.naacl-main.243,P06-2124,0,0.17873,"Missing"
2021.socialnlp-1.11,W16-0307,0,0.130517,"ies in the same row share a superscript, they are not significantly different at a 0.05 confidence level. We cannot test significance against the random sample. differences between groups. 7 Twitter Behaviors across Groups Our experiments show that our datasets enable better predictive models, but say nothing about how self-reporting users use Twitter. Do different groups in our dataset differ in other behaviors? We explore this using a variety of quantitative analyses of Twitter user behavior, following similarly-motivated public health research (Coppersmith et al., 2014; Homan et al., 2014; Gkotsis et al., 2016). Two interpretations are possible for these group-level differences: either user behavior correlates with demographic categories (Wood-Doughty et al., 2017), or the choice to self-report correlates with these behaviors. These can both be true, and our current methods cannot distinguish between them. While our empirical evaluation shows that our data is still useful for training classifiers to predict gold-standard labels, possible selection bias may influence real-world applications. Lexical features are widely used to study Twitter (Pennacchiotti and Popescu, 2011; Blodgett et al., 2016). Fo"
2021.socialnlp-1.11,W14-3213,0,0.0310202,"However, if two entries in the same row share a superscript, they are not significantly different at a 0.05 confidence level. We cannot test significance against the random sample. differences between groups. 7 Twitter Behaviors across Groups Our experiments show that our datasets enable better predictive models, but say nothing about how self-reporting users use Twitter. Do different groups in our dataset differ in other behaviors? We explore this using a variety of quantitative analyses of Twitter user behavior, following similarly-motivated public health research (Coppersmith et al., 2014; Homan et al., 2014; Gkotsis et al., 2016). Two interpretations are possible for these group-level differences: either user behavior correlates with demographic categories (Wood-Doughty et al., 2017), or the choice to self-report correlates with these behaviors. These can both be true, and our current methods cannot distinguish between them. While our empirical evaluation shows that our data is still useful for training classifiers to predict gold-standard labels, possible selection bias may influence real-world applications. Lexical features are widely used to study Twitter (Pennacchiotti and Popescu, 2011; Blo"
2021.socialnlp-1.11,W18-0608,0,0.0358383,"ck et al., 2004; Vargas the noise of automated supervision, our and Stainback, 2016; Culley, 2006; Andrus self-report datasets enable improvements et al., 2021). Despite this complexity, in classification performance on gold understanding race and ethnicity is crucial standard self-report survey data. The result is a reproducible method for for public health research (Coldman et al., creating large-scale training resources for 1988; Dressler et al., 2005; Fiscella and race and ethnicity. Fremont, 2006; Elliott et al., 2008, 2009). Analyses that explore mental health on 1 Introduction Twitter (Loveys et al., 2018) should consider Contextualization of population studies with racial disparities in healthcare (Satcher, demographics forms a central analysis method 2001; Amir et al., 2019) or online within the social sciences. In domains interactions (Delisle et al., 2019; Burnap such as political science or public health, and Williams, 2016). Despite the importance standard demographic panels in telephone of race and ethnicity in these studies, surveys enable better analyses of opinions and multiple proof-of-concept classification and trends. Demographics such as age, studies, there are no readily-availabl"
2021.socialnlp-1.11,S19-1015,0,0.0322788,"Missing"
2021.socialnlp-1.11,W16-5614,1,0.827553,"haly and Culotta, demographics are unavailable on 2017; Jung et al., 2018; Huang and Paul, many social media platforms (e.g. Twitter), numerous studies have inferred 2019). Demographics factor into social demographics automatically. Despite media studies across domains such as health, many studies presenting proof-of-concept politics, and linguistics (O’Connor et al., 2010; inference of race and ethnicity, training Eisenstein et al., 2014). Off-the-shelf software of practical systems remains elusive packages support the inference of gender and since there are few annotated datasets. location (Knowles et al., 2016; Dredze et al., Existing datasets are small, inaccurate, 2013; Wang et al., 2019). or fail to cover the four most common racial and ethnic groups in the United Unlike age or geolocation, race and States. We present a method to identify ethnicity are sociocultural categories with self-reports of race and ethnicity from competing definitions and measurement Twitter profile descriptions. Despite approaches (Comstock et al., 2004; Vargas the noise of automated supervision, our and Stainback, 2016; Culley, 2006; Andrus self-report datasets enable improvements et al., 2021). Despite this complexity"
2021.socialnlp-1.11,W14-2702,0,0.0518726,"e/demographer 2 behavior, platform moderators could simply http://www.cs.jhu.edu/~mdredze/demographics-training-data/ 124 accurate systems. Third, the dataset should be reproducible; Twitter datasets shrink as users delete or restrict accounts, and models become less useful due to domain drift (Huang and Paul, 2018). We present a method for automatically constructing a large Twitter dataset for race and ethnicity. Keyword-matching produces a large, high-recall corpus of Twitter users who potentially self-identify as a racial or ethnic group, building on past work that considered self-reports (Mohammady and Culotta, 2014; Beller et al., 2014; Coppersmith et al., 2014). We then learn a set of filters to improve precision by removing users who match keywords but do not self-report their demographics. Our approach can be automatically repeated in the future to update the dataset. While our automatic supervision contains noise – self-descriptions are hard to identify and potentially unreliable – our large dataset demonstrates benefits when compared to or combined with previous crowdsourced datasets. We validate this comparison on a gold-standard survey dataset of self-reported labels (Preot¸iuc-Pietro and Ungar,"
2021.socialnlp-1.11,D15-1256,0,0.160099,"nge of basic user behaviors on the Twitter platform. Almost all differences 5 The total number of tokens in a tweet without URLs, user mentions and stopwords divided by the total number of tokens in the tweet. 4 The number of unique tokens in a tweet divided by the total number of tokens in the tweet. 6 7 130 https://github.com/YahooArchive/formality-classifier https://github.com/sudhof/politeness in these behavioral features are significant across groups. Device usage shows the biggest difference; White users are much more likely to have used an iPhone than an Android to tweet. In past work, Pavalanathan and Eisenstein (2015) demonstrated that the use of Twitter geotagging was more prevalent in metropolitan areas and among younger users. Table 7 follows Wood-Doughty et al. (2017) which calculated these features for a sample of 1M Twitter users. Users in our datasets comparatively more often customize their profile image or URL or enable geotagging. More bots or spam in the random sample may partially account for these differences (Morstatter et al., 2013). Table 8 in Appendix D also compares lists of the most common common emojis, emoticons, and part-of-speech tags within each group. These analyses show substantia"
2021.socialnlp-1.11,Q16-1005,0,0.0238861,"While our empirical evaluation shows that our data is still useful for training classifiers to predict gold-standard labels, possible selection bias may influence real-world applications. Lexical features are widely used to study Twitter (Pennacchiotti and Popescu, 2011; Blodgett et al., 2016). For each user in our dataset, we follow §3.1 of Inuwa-Dutse et al. (2018) and calculate Type-Token Ratio4 , Lexical Diversity5 (Tweedie and Baayen, 1998), and the number of hashtags and English contractions they use per tweet. We then use existing trained models for analyzing formality and politeness (Pavlick and Tetreault, 2016; Danescu-Niculescu-Mizil et al., 2013) of online text. The formality score is estimated with a regression model over lexical and syntactic features including n-grams, dependency parse, and word embeddings. The politeness classifier uses unigram features and lexicons for gratitude and sentiment. We use the published implementations.6,7 For both trained models, we macro-average over users’ scores to obtain a value for each demographic group. We also use a SAGE (Eisenstein et al., 2011) lexical variation implementation to find the words that most distinguish each group. The means of the six quan"
2021.socialnlp-1.11,D17-1248,0,0.0384842,"Missing"
2021.socialnlp-1.11,C18-1130,0,0.0307792,"Missing"
2021.socialnlp-1.11,W18-1108,1,0.786448,"ke up the bottom seven rows of our results in Table 3, below. These results show our three models evaluated on the imbalanced and balanced test sets. The balanced and imbalanced dev sets are used for all model and training set combinations in Table 3, which controls for the effect of model hyper-parameter selection. Cross-validation could be used in practical low-resource settings, but we use a single held-out dev set, which we subsample in the balanced case. 5.1 Demographic Prediction Models We consider three demographic inference models which we train on each training set. The first follows Wood-Doughty et al. (2018) and uses a single tweet per user. A character-level CNN maps the user’s name to an embedding which is combined with features from the profile metadata, such as user verification and follower count. These are passed through a two fully-connected layers to produce classifications. This model is referred to as “Names” in Table 3. The second model 5 Experimental Evaluation from Volkova and Bachrach (2015) uses a bag-of-words representation of the words in We now conduct an empirical evaluation the user’s recent tweets as the input to a sparse of our noisy self-report datasets. Showing logistic re"
2021.socialnlp-1.11,W17-2912,1,0.938543,"r HF filters (§ 4) individually and together. Precision is on dev set from Appendix B, after thresholding on self-report score. assumes that racial identity can be accurately perceived by others, an assumption that has serious flaws for gender and age (Flekova et al., 2016; Preot¸iuc-Pietro et al., 2017). Rule-based or statistical systems for data collection can be effective (Burger et al., 2011; Chang et al., 2010), but raise concerns about selection bias: if we only label users who take a certain action, a model trained on those users may not generalize to users who do not take that action (Wood-Doughty et al., 2017). Gold-standard labels for sensitive traits requires individual survey responses, but this yields small or skewed datasets due to the expense (Preot¸iuc-Pietro and Ungar, 2018). Our approach instead relies on automated supervision from racial self-identification and minimal manual annotation to refine our dataset labels. We are not the first to use users’ self-identification to label Twitter users’ demographics, but past work has relied heavily either on restrictive regular expressions or manual annotation (Pennacchiotti and Popescu, 2011; Mohammady and Culotta, 2014). Such work has also been"
2021.wnut-1.44,P12-3005,0,0.0152934,"ble at https://github .com/AADeLucia/civil-unrest-case -study. 2 Data We analyze Twitter since it is a widely used social media platform, with many public posts about major events in real-time. For ground-truth data on civil unrest we use the Armed Conflict Location & Event Data Project (ACLED) (Raleigh et al., 2010), a manually curated database tracking civil unrest events. 2.1 Twitter Collection We collected geotagged tweets from the Twitter streaming API from 2014 to 2019 (inclusive) for 42 countries from Africa, the Middle East, and Southeast Asia. We selected English tweets using langid (Lui and Baldwin, 2012).1 Appendix Table 4 lists the number of tweets per country. To focus on relevant tweets we filter our data using the BERTweet civil unrest tweet classifier from Sech et al. (2020).2 We keep tweets with probabilities above 0.5. Previous work has achieved this filtration using keywords (Muthiah et al., 2015; Ramakrishnan et al., 2014) or focus on post-event 1 Twitter provided language identification was not available for the early years in our dataset. 2 The model has a test F1-score of 0.81 for identifying civil unrest-related tweets. data collection (Alsaedi et al., 2017). The number of tweets"
2021.wnut-1.44,D14-1162,0,0.0867548,"t this method on tweets from the Johannesburg Riots. For the query process we created embedding representations of each tweet from September 3, 2019 in South Africa (the peak of the protests) and of 4 General Indicators of Civil Unrest the aggregated event descriptions from the same day in ACLED, and then used cosine similarity In §3 we determined that for two specific events between event and tweet representations to find in different countries (South Africa and Ethiopia), the relevant tweets. We used the average Twitter there is signal in the country’s Twitter activity that GloVe embedding (Pennington et al., 2014) to rep- a civil unrest event occurred. Those case studies 400 ACLED Event Descriptions Foreign nationals demonstrated and barricaded roads in Rosettenville (City of Johannesburg, Gauteng). Some sources reported demonstrators held weapons, in case they were attacked. The demonstration was in response to nationals attacking foreign nationals. Students, mainly female, demonstrated at the University of Cape Town, disrupting classes. The demonstration was in response to the rape and murder of student, calling for more safety for students. Most Similar Tweets Is the @SAPS investigation of the alleg"
2021.wnut-1.44,N16-3020,0,0.0377867,"ns of other features, essentially a feature “importance"" (Lundberg et al., 2020). A negative SHAP value means the feature pushes the prediction towards the negative class, and a positive value pushes it towards the positive class. Although SHAP operates on a single instance, the aggregation of SHAP values can provide insights on the overall impact of a feature. The magnitude of a feature’s impact is the sum of its SHAP values across many examples, regardless of its positive/negative impact (absolute value). We chose SHAP because it worked better with the sparse count-based features than LIME (Ribeiro et al., 2016). 4.2 Features Indicative of Civil Unrest The magnitude of a feature’s SHAP values across many examples provides insight into features the model deems important to decision making. In this case, a feature is the raw number of times a token appeared in all the tweets for a country for a day. To evaluate country generalizability we check the top features for a specific country, Myanmar, and all 42 countries in the dataset (including Myanmar). The SHAP values are from all Myanmar samples in the test set and 500 samples from all countries. All samples are pulled from the test set, years 2018– 2019"
2021.wnut-1.44,2020.wnut-1.28,1,0.792742,"real-time. For ground-truth data on civil unrest we use the Armed Conflict Location & Event Data Project (ACLED) (Raleigh et al., 2010), a manually curated database tracking civil unrest events. 2.1 Twitter Collection We collected geotagged tweets from the Twitter streaming API from 2014 to 2019 (inclusive) for 42 countries from Africa, the Middle East, and Southeast Asia. We selected English tweets using langid (Lui and Baldwin, 2012).1 Appendix Table 4 lists the number of tweets per country. To focus on relevant tweets we filter our data using the BERTweet civil unrest tweet classifier from Sech et al. (2020).2 We keep tweets with probabilities above 0.5. Previous work has achieved this filtration using keywords (Muthiah et al., 2015; Ramakrishnan et al., 2014) or focus on post-event 1 Twitter provided language identification was not available for the early years in our dataset. 2 The model has a test F1-score of 0.81 for identifying civil unrest-related tweets. data collection (Alsaedi et al., 2017). The number of tweets by country before and after filtering for language and relevance are shown in Figure 2. Only a small number of these countries have a large number of English tweets, and we consi"
C08-1060,P07-1124,0,0.0288534,"designed for this problem. The news system improves market prediction over baseline market systems. 1 Introduction The mass media can affect world events by swaying public opinion, officials and decision makers. Financial investors who evaluate the economic performance of a company can be swayed by positive and negative perceptions about the company in the media, directly impacting its economic position. The same is true of politics, where a candidate’s performance is impacted by media influenced public perception. Computational linguistics can discover such signals in the news. For example, Devitt and Ahmad (2007) gave a computable metric of polarity in financial news text consistent with human judgments. Koppel and Shtrimberg (2004) used a daily news analysis to predict financial market performance, though predictions could not be used for future investment decisions. Recently, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. a study conducted of the 2007 French presidential election showed a correlation between the frequency of a candidate’s name in the news and electoral s"
C08-1060,W06-2932,1,0.807763,"Missing"
C08-1060,N03-1033,0,0.0395658,"Missing"
C08-1060,D07-1113,0,0.221988,"rom syntactic dependency parses of the news and a user-defined set of market entities. Successive news days are compared to determine the novel component of each day’s news resulting in features for a machine learning system. A combination system uses this information as well as predictions from internal market forces to model prediction markets better than several baselines. Results show that news articles can be mined to predict changes in public opinion. Opinion forecasting differs from that of opinion analysis, such as extracting opinions, evaluating sentiment, and extracting predictions (Kim and Hovy, 2007). Contrary to these tasks, our system receives objective news, not subjective opinions, and learns what events will impact public opinion. For example, “oil prices rose” is a fact but will likely shape opinions. This work analyzes news (cause) to predict future opinions (effect). This affects the structure of our task: we consider a timeseries setting since we must use past data to predict future opinions, rather than analyzing opinions in batch across the whole dataset. 473 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473–480 Manchester, A"
C10-1032,P98-1012,0,0.333517,"23rd International Conference on Computational Linguistics (Coling 2010), pages 277–285, Beijing, August 2010 Motivated by ambiguity in personal name search, Mann and Yarowsky (2003) disambiguate person names using biographic facts, like birth year, occupation and affiliation. When present in text, biographic facts extracted using regular expressions help disambiguation. More recently, the Web People Search Task (Artiles et al., 2008) clustered web pages for entity disambiguation. The related task of cross document coreference resolution has been addressed by several researchers starting from Bagga and Baldwin (1998). Poesio et al. (2008) built a cross document coreference system using features from encyclopedic sources like Wikipedia. However, successful coreference resolution is insufficient for correct entity linking, as the coreference chain must still be correctly mapped to the proper KB entry. Previous work by Bunescu and Pasca (2006) and Cucerzan (2007) aims to link entity mentions to their corresponding topic pages in Wikipedia but the authors differ in their approaches. Cucerzan uses heuristic rules and Wikipedia disambiguation markup to derive mappings from surface forms of entities to their Wik"
C10-1032,P08-1004,0,0.00706837,"knowledge bases, like Wikipedia, which contain millions of entries. Furthermore, our system learns when to withhold a link when an entity has no matching KB entry, a task that has largely been neglected in prior research in cross-document entity coreference. Our system produces high quality predictions compared with recent work on this task. 2 Related Work The information extraction oeuvre has a gamut of relation extraction methods for entities like persons, organizations, and locations, which can be classified as open- or closed-domain depending on the restrictions on extractable relations (Banko and Etzioni, 2008). Closed domain systems extract a fixed set of relations while in open-domain systems, the number and type of relations are unbounded. Extracted relations still require processing before they can populate a KB with facts: namely, entity linking and disambiguation. 277 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277–285, Beijing, August 2010 Motivated by ambiguity in personal name search, Mann and Yarowsky (2003) disambiguate person names using biographic facts, like birth year, occupation and affiliation. When present in text, biographic f"
C10-1032,E06-1002,0,0.823432,"rent mentions are the same entity is crucial in updating an entity’s KB record. This task has been variously called entity disambiguation, record linkage, or entity linking. When performed without a KB, entity disambiguation is called coreference resolution: entity mentions either within the same document or across multiple documents are clustered together, where each cluster corresponds to a single real world entity. The emergence of large scale publicly available KBs like Wikipedia and DBPedia has spurred an interest in linking textual entity references to their entries in these public KBs. Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. In this work we introduce an entity disambiguation system for linking entities to corresponding Wikipedia pages designed for open domains, where a large percentage of entities will not be linkable. Further, our method and some of our features readily generalize to other curated KB. We adopt a supervised approach, where each of the possible entities contained within Wikipedia are scored for"
C10-1032,D07-1074,0,0.954062,"tity is crucial in updating an entity’s KB record. This task has been variously called entity disambiguation, record linkage, or entity linking. When performed without a KB, entity disambiguation is called coreference resolution: entity mentions either within the same document or across multiple documents are clustered together, where each cluster corresponds to a single real world entity. The emergence of large scale publicly available KBs like Wikipedia and DBPedia has spurred an interest in linking textual entity references to their entries in these public KBs. Bunescu and Pasca (2006) and Cucerzan (2007) presented important pioneering work in this area, but suffer from several limitations including Wikipedia specific dependencies, scale, and the assumption of a KB entry for each entity. In this work we introduce an entity disambiguation system for linking entities to corresponding Wikipedia pages designed for open domains, where a large percentage of entities will not be linkable. Further, our method and some of our features readily generalize to other curated KB. We adopt a supervised approach, where each of the possible entities contained within Wikipedia are scored for a match to the query"
C10-1032,D08-1113,0,0.0120421,"aries and partial character matches, enable matches between “MIT” and “Madras Institute of Technology” or “Ministry of Industry and Trade.” Aliases. Many aliases or nicknames are nontrivial to guess. For example JAVA is the stock symbol for Sun Microsystems, and “Ginger Spice” is a stage name of Geri Halliwell. A reasonable way to do this is to employ a dictionary and alias lists that are commonly available for many domains5 . FST Name Matching. Another measure of surface similarity between a query and a candidate was computed by training finite-state transducers similar to those described in Dreyer et al. (2008). These transducers assign a score to any string pair by summing over all alignments and scoring all 5 We used multiple lists, including class-specific lists (i.e., for PER, ORG, and GPE) lists extracted from Freebase (Bollacker et al., 2008) and Wikipedia redirects. PER, ORG, and GPE are the commonly used terms for entity types for people, organizations and geo-political regions respectively. contained character n-grams; we used n-grams of length 3 and less. The scores are combined using a global log-linear model. Since different spellings of a name may vary considerably in length (e.g., J Mi"
C10-1032,W03-0405,0,0.0140932,"zations, and locations, which can be classified as open- or closed-domain depending on the restrictions on extractable relations (Banko and Etzioni, 2008). Closed domain systems extract a fixed set of relations while in open-domain systems, the number and type of relations are unbounded. Extracted relations still require processing before they can populate a KB with facts: namely, entity linking and disambiguation. 277 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277–285, Beijing, August 2010 Motivated by ambiguity in personal name search, Mann and Yarowsky (2003) disambiguate person names using biographic facts, like birth year, occupation and affiliation. When present in text, biographic facts extracted using regular expressions help disambiguation. More recently, the Web People Search Task (Artiles et al., 2008) clustered web pages for entity disambiguation. The related task of cross document coreference resolution has been addressed by several researchers starting from Bagga and Baldwin (1998). Poesio et al. (2008) built a cross document coreference system using features from encyclopedic sources like Wikipedia. However, successful coreference reso"
C10-1032,W03-0419,0,0.1715,"Missing"
C10-1032,C98-1012,0,\N,Missing
C10-2121,P98-1012,0,0.911059,"John Phillips the American football player and John Phillips the musician, but it is quite probable that documents discussing each will appear in the same collection. Therefore, while matching entities with the same mention string can work well for within document coreference, more sophisticated approaches are necessary for the cross document scenario where a one-entity-per-name assumption is unreasonable. One of the most common approaches to both within document and cross document coreference resolution has been based on agglomerative clustering, where vectors might be bag-of-word contexts (Bagga and Baldwin, 1998; Mann and 1050 Coling 2010: Poster Volume, pages 1050–1058, Beijing, August 2010 Yarowsky, 2003; Gooi and Allan, 2004; Chen and Martin, 2007). These algorithms creates a O(n2 ) dependence in the number of mentions – for within document – and documents – for cross document. This is a reasonable limitation for within document, since the number of references will certainly be small; we are unlikely to encounter a document with millions of references. In contrast to the small n encountered within a document, we fully expect to run a CDCR system on hundreds of thousands or millions of documents. M"
C10-2121,D08-1029,0,0.678219,"middle initials or names were discarded. 4.3 ACE 2008 corpus The NIST ACE 2008 (ace08) evaluation studied several related technologies for information extraction, including named-entity recognition, relation extraction, and cross-document coreference for person names in both English and Arabic. Approximately 10,000 documents from several genres (predominantly newswire) were given to participants, who were expected to cluster person and organization entities across the entire collection. However, only a selected set of about 400 documents were annotated and used to evaluate system performance. Baron and Freedman (2008) describe their work in this evaluation, which included a separate task for within-document coreference. 4.4 TAC-KBP 2009 corpus The NIST TAC 2009 Knowledge Base Population track (kbp09) (McNamee and Dang, 2009) conducted an evaluation of a system’s ability to link entity mentions to corresponding Wikipediaderived knowledge base nodes. The TAC-KBP task focused on ambiguous person, organization, and geo-political entities mentioned in newswire, and required systems to cope with name variation (e.g., “Osama Bin Laden” / “Usama Bin Laden” or “Mark Twain” / “Samuel Clemens”) as well as name disamb"
C10-2121,D07-1020,0,0.639442,"in the same collection. Therefore, while matching entities with the same mention string can work well for within document coreference, more sophisticated approaches are necessary for the cross document scenario where a one-entity-per-name assumption is unreasonable. One of the most common approaches to both within document and cross document coreference resolution has been based on agglomerative clustering, where vectors might be bag-of-word contexts (Bagga and Baldwin, 1998; Mann and 1050 Coling 2010: Poster Volume, pages 1050–1058, Beijing, August 2010 Yarowsky, 2003; Gooi and Allan, 2004; Chen and Martin, 2007). These algorithms creates a O(n2 ) dependence in the number of mentions – for within document – and documents – for cross document. This is a reasonable limitation for within document, since the number of references will certainly be small; we are unlikely to encounter a document with millions of references. In contrast to the small n encountered within a document, we fully expect to run a CDCR system on hundreds of thousands or millions of documents. Most previous approaches cannot handle collections of this size. In this work, we present a new method for cross document coreference resolutio"
C10-2121,N07-1009,0,0.0166336,"often relying on named entity recognition, and then decide if these references refer to a single individual or multiple entities, creating a coreference chain for each unique entity. Feature representations include surface form similarity, lexical context of mentions, position in the document and distance between references. A variety of statistical learning methods have been applied to this problem, including use of decision trees (Soon et al., 2001; Ng and Cardie, 2002), graph partitioning (Nicolae and Nicolae, 2006), maximum-entropy models (Luo et al., 2004), and conditional random fields (Choi and Cardie, 2007). Given pre-processed documents, in which entities have been identified and entity mentions have been linked into chains, we seek to identify across an entire document collection all chains that refer to the same entity. This task is called cross document coreference resolution (CDCR). Several of the challenges associated with CDCR differ from the within document task. For example, it is unlikely that the same document will discuss John Phillips the American football player and John Phillips the musician, but it is quite probable that documents discussing each will appear in the same collectio"
C10-2121,D07-1074,0,0.00876219,". The TAC-KBP task focused on ambiguous person, organization, and geo-political entities mentioned in newswire, and required systems to cope with name variation (e.g., “Osama Bin Laden” / “Usama Bin Laden” or “Mark Twain” / “Samuel Clemens”) as well as name disambiguation. Furthermore, the task required detection of when no appropriate KB entry exists, which is a departure from the conventional disambiguation problem. The collection contains over 1.2 million documents, primarily newswire. Wikipedia was used as a surrogate knowledge base, and it has been used in several previous studies (e.g., Cucerzan (2007)). This task is closely related to CDCR, as mentions that are aligned to the same knowledge base entry create a coreference cluster. However, there are no actual CDCR annotations for this corpus, though we used it nonetheles as a benchmark corpus to evaluate speed and to demonstrate scalability. 5 5.1 Discussion Accuracy In Table 2 we report cross document coreference resolution performance for a variety of experimental conditions using the B 3 method, which includes precision, recall, and calculated Fβ=1 values. For each of the three evaluation corpora (smith, nytac, and ace08) we report valu"
C10-2121,J01-4004,0,0.122887,"arack Obama”) followed by additional expressions for the same entity (“President Obama.”) An intra-document coreference system must first identify each reference, often relying on named entity recognition, and then decide if these references refer to a single individual or multiple entities, creating a coreference chain for each unique entity. Feature representations include surface form similarity, lexical context of mentions, position in the document and distance between references. A variety of statistical learning methods have been applied to this problem, including use of decision trees (Soon et al., 2001; Ng and Cardie, 2002), graph partitioning (Nicolae and Nicolae, 2006), maximum-entropy models (Luo et al., 2004), and conditional random fields (Choi and Cardie, 2007). Given pre-processed documents, in which entities have been identified and entity mentions have been linked into chains, we seek to identify across an entire document collection all chains that refer to the same entity. This task is called cross document coreference resolution (CDCR). Several of the challenges associated with CDCR differ from the within document task. For example, it is unlikely that the same document will disc"
C10-2121,N04-1002,0,0.836097,"ssing each will appear in the same collection. Therefore, while matching entities with the same mention string can work well for within document coreference, more sophisticated approaches are necessary for the cross document scenario where a one-entity-per-name assumption is unreasonable. One of the most common approaches to both within document and cross document coreference resolution has been based on agglomerative clustering, where vectors might be bag-of-word contexts (Bagga and Baldwin, 1998; Mann and 1050 Coling 2010: Poster Volume, pages 1050–1058, Beijing, August 2010 Yarowsky, 2003; Gooi and Allan, 2004; Chen and Martin, 2007). These algorithms creates a O(n2 ) dependence in the number of mentions – for within document – and documents – for cross document. This is a reasonable limitation for within document, since the number of references will certainly be small; we are unlikely to encounter a document with millions of references. In contrast to the small n encountered within a document, we fully expect to run a CDCR system on hundreds of thousands or millions of documents. Most previous approaches cannot handle collections of this size. In this work, we present a new method for cross docume"
C10-2121,P04-1018,0,0.146063,"erence system must first identify each reference, often relying on named entity recognition, and then decide if these references refer to a single individual or multiple entities, creating a coreference chain for each unique entity. Feature representations include surface form similarity, lexical context of mentions, position in the document and distance between references. A variety of statistical learning methods have been applied to this problem, including use of decision trees (Soon et al., 2001; Ng and Cardie, 2002), graph partitioning (Nicolae and Nicolae, 2006), maximum-entropy models (Luo et al., 2004), and conditional random fields (Choi and Cardie, 2007). Given pre-processed documents, in which entities have been identified and entity mentions have been linked into chains, we seek to identify across an entire document collection all chains that refer to the same entity. This task is called cross document coreference resolution (CDCR). Several of the challenges associated with CDCR differ from the within document task. For example, it is unlikely that the same document will discuss John Phillips the American football player and John Phillips the musician, but it is quite probable that docu"
C10-2121,W03-0405,0,0.926214,"ocuments and then performed some form of clustering on these vectors. This is a simple extension of Firth’s distributional hypothesis applied to entities (Firth, 1957). We describe some of the seminal work in this area. Some of the earliest work in CDCR was by Bagga and Baldwin (1998). Key contributions of their research include: promotion of a settheoretic evaluation measure, B-CUBED; introduction of a data set based on 197 New York Times articles which mention a person named John Smith; and, use of TF/IDF weighted vectors and cosine similarity in single-link greedy agglomerative clustering. Mann and Yarowsky (2003) extended Bagga and Baldwin’s work and contributed several innovations, including: use of biographical attributes (e.g., year of birth, occupation), and evaluation using pseudonames. Pseudonames are sets of artificially conflated names that are used as an efficient method for producing a set of gold-standard disambiguations.1 Mann and Yarowsky used 4 pairs of conflated names in their evaluation. Their system did not perform as well on named entities with little available biographic information. Gooi and Allan (2004) expanded on the use of pseudonames by semi-automatically creating a much large"
C10-2121,P02-1014,0,0.051605,"wed by additional expressions for the same entity (“President Obama.”) An intra-document coreference system must first identify each reference, often relying on named entity recognition, and then decide if these references refer to a single individual or multiple entities, creating a coreference chain for each unique entity. Feature representations include surface form similarity, lexical context of mentions, position in the document and distance between references. A variety of statistical learning methods have been applied to this problem, including use of decision trees (Soon et al., 2001; Ng and Cardie, 2002), graph partitioning (Nicolae and Nicolae, 2006), maximum-entropy models (Luo et al., 2004), and conditional random fields (Choi and Cardie, 2007). Given pre-processed documents, in which entities have been identified and entity mentions have been linked into chains, we seek to identify across an entire document collection all chains that refer to the same entity. This task is called cross document coreference resolution (CDCR). Several of the challenges associated with CDCR differ from the within document task. For example, it is unlikely that the same document will discuss John Phillips the"
C10-2121,W06-1633,0,0.0104399,"entity (“President Obama.”) An intra-document coreference system must first identify each reference, often relying on named entity recognition, and then decide if these references refer to a single individual or multiple entities, creating a coreference chain for each unique entity. Feature representations include surface form similarity, lexical context of mentions, position in the document and distance between references. A variety of statistical learning methods have been applied to this problem, including use of decision trees (Soon et al., 2001; Ng and Cardie, 2002), graph partitioning (Nicolae and Nicolae, 2006), maximum-entropy models (Luo et al., 2004), and conditional random fields (Choi and Cardie, 2007). Given pre-processed documents, in which entities have been identified and entity mentions have been linked into chains, we seek to identify across an entire document collection all chains that refer to the same entity. This task is called cross document coreference resolution (CDCR). Several of the challenges associated with CDCR differ from the within document task. For example, it is unlikely that the same document will discuss John Phillips the American football player and John Phillips the m"
C10-2121,C98-1012,0,\N,Missing
D07-1112,W07-2416,0,0.0334886,"Missing"
D07-1112,W04-3111,0,0.0744338,"Missing"
D07-1112,J93-2004,0,0.0316121,"Missing"
D07-1112,W06-2932,1,0.426727,"Missing"
D07-1112,W97-0309,1,0.621417,"Missing"
D07-1112,D07-1096,0,\N,Missing
D08-1072,P07-1033,0,0.326413,"Missing"
D08-1072,W03-0425,0,0.0252607,"tion and many shared classifiers. Some multi-task work has also considered the grouping of tasks similar to our learning of domain subgroups (Thrun and O’Sullivan, 1998; Bakker and Heskes, 2003). There are many techniques for combining the output of multiple classifiers for ensemble learning or mixture of experts. Kittler et al. (Mar 1998) provide a theoretical framework for combining classifiers. Some empirical work has considered adding versus multiplying classifier output (Tax et al., 2000), using local accuracy estimates for combination (Woods et al., 1997), and applications to NLP tasks (Florian et al., 2003). However, these papers consider combining classifier output for prediction. In contrast, we consider parameter combination for both prediction and learning. 10 Conclusion We have explored several multi-domain learning settings using CW classifiers and a combination method. Our approach creates a better classifier for a new target domain than selecting a random source classifier a prior, reduces learning error on multiple domains compared to baseline approaches, can handle many disparate domains by using many shared classifiers, and scales to a very large number of domains with a small perform"
D08-1072,P07-1034,0,0.820557,"labeled with different classification functions. For example, one user may enjoy some emails that another user considers spam: differing in their classification function. The goal of multi-task learning is to generalize across tasks/domains (Dekel et al., 2006; Evgeniou and Pontil, 2004). Furthermore, as in domain adaptation, some examples are draw from different distributions. For example, one user may receive emails about engineering while another about art, differing in their distribution over features. Domain adaptation deals with these feature distribution changes (Blitzer et al., 2007; Jiang and Zhai, 2007). Our work combines these two areas by learning both across distributions and behaviors or functions. 3 Confidence-Weighted Linear Classifiers Confidence-weighted (CW) linear classification (Dredze et al., 2008), a new online algorithm, maintains a probabilistic measure of parameter confidence, which may be useful in combining parameters from different domain distributions. We summarize CW learning to familiarize the reader. Parameter confidence is formalized by a Gaussian distribution over weight vectors with mean µ ∈ RN and diagonal covariance Σ ∈ RN ×N . The values µj and Σj,j represent kno"
D08-1072,W04-3237,0,\N,Missing
D08-1072,P07-1056,1,\N,Missing
D09-1052,W02-1001,0,0.538674,"s. CW learning explicitly models classifier weight uncertainty using a multivariate Gaussian distribution over weight vectors. The learner makes online updates based on its confidence in the current parameters, making larger 2 Problem Setting In the multi-class setting, instances from an input space X take labels from a finite set Y, |Y |= K. 496 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 496–504, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP for appropriate αi and βi . For prediction, they use the Bayesian rule We use a standard approach (Collins, 2002) for generalizing binary classification and assume a feature function f (x, y) ∈ Rd mapping instances x ∈ X and labels y ∈ Y into a common space. We work in the online framework, where learning is performed in rounds. On each round the learner receives an input xi , makes a prediction yˆi according to its current rule, and then learns the true label yi . The learner uses the new example (xi , yi ) to modify its prediction rule. Its goal is to minimize the total number of rounds with incorrect predictions, |{i : yi 6= yˆi }|. In this work we focus on linear models parameterized by weights w and"
D09-1052,P07-1056,1,\N,Missing
D10-1045,D08-1027,0,\N,Missing
D10-1045,J96-1002,0,\N,Missing
D10-1045,P98-1012,0,\N,Missing
D10-1045,C98-1012,0,\N,Missing
D10-1045,P07-1064,0,\N,Missing
D10-1045,W10-0701,1,\N,Missing
D10-1045,D09-1052,1,\N,Missing
D10-1057,W06-1615,0,0.397221,"main like “efficient” and “noisy compressor” will have never been seen during training and therefore not be in the model. Furthermore, we do not assume labeled instances are available to help detect these harmful changes. Other tasks related to changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguisti"
D10-1057,P07-1056,1,0.728216,"and “noisy compressor” will have never been seen during training and therefore not be in the model. Furthermore, we do not assume labeled instances are available to help detect these harmful changes. Other tasks related to changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics main the shift will"
D10-1057,D08-1070,0,0.019038,"d confidence estimation has been used to estimate the confidence of patterns derived from partially supervised relation extraction (Agichtein, 2006). Confidence estimation has also been used to improve the overall effectiveness of NLP systems. Confidence estimates obtained via neural networks have shown gains for speech recognition, spoken language understanding, and machine translation (Gandrabur et al., 2006). Pipeline models using confidence estimates at one stage as weights for further downstream stages improve over baseline dependency parsing and named entity recognition pipeline models (Bunescu, 2008). An alternative formulation of domain adaptation trains on different corpora from many different domains, then uses linear combinations of models trained on the different corpora(McClosky et al., 2010). Work in novelty detection is relevant to the task of detecting domain shifts (Scholkopf et al., 2000), 300 Domain Classifier Domain Classifier 1200 1000 250 800 200 600 150 400 100 200 00 200 400 600 CWPM 800 1000 1200 50 00 50 100 150 CWPM 200 250 300 Figure 6: A-distance over a stream of 1s and 0s produced by a supervised classifier trained to differentiate between the source and target doma"
D10-1057,N04-4028,0,0.0240748,"vailable. 593 Related Work Early NLP work in the unsupervised setting monitored classification confidence values, setting a confidence threshold based on a break-even heuristic, monitoring the rate of (presumed) irrelevant examples based on this threshold, and signaling a change when this rate increased (Lanquillon, 1999). Confidence estimation has been used for specific NLP components such as information extraction. The correctness of fields extracted via a conditional random field extractor has been shown to correlate well to an estimate obtained by a constrained forward-backward technique (Culotta and McCallum, 2004). EM-based confidence estimation has been used to estimate the confidence of patterns derived from partially supervised relation extraction (Agichtein, 2006). Confidence estimation has also been used to improve the overall effectiveness of NLP systems. Confidence estimates obtained via neural networks have shown gains for speech recognition, spoken language understanding, and machine translation (Gandrabur et al., 2006). Pipeline models using confidence estimates at one stage as weights for further downstream stages improve over baseline dependency parsing and named entity recognition pipeline"
D10-1057,P07-1033,0,0.201336,"Missing"
D10-1057,W01-1007,0,0.0298909,"is more informative in our setting than the mere fact that novel instances are observed. We are also motivated by the problem of detecting genre shift in addition to domain shift, as in the ACE 2005 data set shifts from newswire to transcripts and blogs. Different text genres occur in traditional settings, such as broadcast news transcripts and newswire, and have begun to proliferate with the variety of social media technologies now available including weblogs. Static genre classification has been explored using a variety of techniques, including exploiting punctuation (Kessler et al., 1997; Dewdney et al., 2001), TF-IDF statistics (Lee and Myaeng, 2002), and part-of-speech statistics and histograms (Finn and Kushmerick, 2006; Feldman et al., 2009). Finally, statistical estimation in a streaming context has been considered in data mining applications (Muthukrishnan, 2005). Change detection via sequential hypothesis testing has been effective for streaming applications such as network intrusion detection (Muthukrishnan et al., 2007). Detecting new events in a stream of Twitter posts can be done using constant time and space similarity measures based on a modification of locality sensitive hashing (Petr"
D10-1057,P08-2059,1,0.902085,"changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics main the shift will happen. A discussion forum devoted to science fiction books may change over time to focus more on fantasy and then narrow to discussions of vampire fiction. Maybe this shift is harmless and it is possible to identify the senti"
D10-1057,D08-1072,1,0.858796,"changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics main the shift will happen. A discussion forum devoted to science fiction books may change over time to focus more on fantasy and then narrow to discussions of vampire fiction. Maybe this shift is harmless and it is possible to identify the senti"
D10-1057,N09-1068,0,0.0852211,"assume labeled instances are available to help detect these harmful changes. Other tasks related to changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics main the shift will happen. A discussion forum devoted to science fiction books may change over time to focus more on fantasy and then narrow to d"
D10-1057,P07-1034,0,0.693603,"will have never been seen during training and therefore not be in the model. Furthermore, we do not assume labeled instances are available to help detect these harmful changes. Other tasks related to changes in data distributions, like detecting concept drift in which the labeling function changes, may require labeled instances, but that is not the focus of this paper. There is significant work on the related problem of adapting a classifier for a known domain shift. Versions of this problem include adapting using only unlabeled target domain data (Blitzer et al., 2006; Blitzer et al., 2007; Jiang and Zhai, 2007), adapting using a limited amount of target domain labeled data (Daum´e, 2007; Finkel and Manning, 2009), and learning across multiple domains simultaneously in an online setting (Dredze and Crammer, 2008b). However, in practical settings, we do not know if the data distribution will change, and certainly not when. Additionally, we will not know to what do585 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 585–595, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics main the shift will happen. A discussion f"
D10-1057,P97-1005,0,0.213911,"Missing"
D10-1057,N10-1004,0,0.0220417,"improve the overall effectiveness of NLP systems. Confidence estimates obtained via neural networks have shown gains for speech recognition, spoken language understanding, and machine translation (Gandrabur et al., 2006). Pipeline models using confidence estimates at one stage as weights for further downstream stages improve over baseline dependency parsing and named entity recognition pipeline models (Bunescu, 2008). An alternative formulation of domain adaptation trains on different corpora from many different domains, then uses linear combinations of models trained on the different corpora(McClosky et al., 2010). Work in novelty detection is relevant to the task of detecting domain shifts (Scholkopf et al., 2000), 300 Domain Classifier Domain Classifier 1200 1000 250 800 200 600 150 400 100 200 00 200 400 600 CWPM 800 1000 1200 50 00 50 100 150 CWPM 200 250 300 Figure 6: A-distance over a stream of 1s and 0s produced by a supervised classifier trained to differentiate between the source and target domain. Samples from the unseen target domain is very effective. However, for many shifts, the margin based A-distance detector is still competitive. CWPM had a single false positive while the domain classi"
D10-1057,N10-1021,0,0.0874371,"Missing"
D10-1057,W10-0104,0,0.0299091,"Missing"
D12-1032,N03-1003,0,0.199016,"st name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We assu"
D12-1032,D07-1074,0,0.0461471,"iscourse” in which our authors operate.10 In this case, we would need (expensive) new algorithms to reconstruct the strings w. However, this model could infer a more realistic phylogeny by positing unobserved ancestral or intermediate forms that relate the observed tokens, as in transformation models (Eisner, 2002; Andrews and Eisner, 2011). 7 7.1 Experimental Evaluation Data preparation Scraping Wikipedia. Wikipedia documents many variant names for entities. As a result, it has frequently been used as a source for mining name variations, both within and across languages (Parton et al., 2008; Cucerzan, 2007). We used Wikipedia to create a list of name aliases for different entities. Specifically, we mined English Wikipedia11 for all redirects: page names that lead directly to another page. Redirects are created by Wikipedia users for resolving common name variants to the correct page. For example, the pages titled Barack Obama Junior and Barack Hussein Obama automatically redirect to the page titled Barack Obama. This redirection implies that the first two are name variants of the third. Collecting all such links within English Wikipedia yields a large number of aliases for each page. However, ma"
D12-1032,D11-1057,1,0.91438,"ional EM learning algorithm alternately reestimates this phylogeny and the transducer parameters. The final learned transducer can quickly link any test name into the final phylogeny, thereby locating variants of the test name. We find that our method can effectively find name variants in a corpus of web strings used to refer to persons in Wikipedia, improving over standard untrained distances such as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morp"
D12-1032,D08-1113,1,0.922811,"ch as Jaro-Winkler and Levenshtein distance. 1 Introduction Systematic relationships between pairs of strings are at the core of problems such as transliteration (Knight and Graehl, 1998), morphology (Dreyer and Eisner, 2011), cross-document coreference resolution (Bagga and Baldwin, 1998), canonicalization (Culotta et al., 2007), and paraphrasing (Barzilay and Lee, 2003). Stochastic transducers such as probabilistic finite-state transducers are often used to capture such relationships. They model a conditional distribution p(y |x), and are ordinarily trained on input-output pairs of strings (Dreyer et al., 2008). In this paper, we are interested in learning from an unorganized collection of strings, some of which might have been derived from others by transformative linguistic processes such as abbreviation, morphological derivation, historical sound or spelling change, loanword formation, translation, transliteration, editing, or transcription error. We assume that each string was derived from at most one parent, but may give rise to any number of children. The difficulty is that most or all of these parentchild relationships are unobserved. We must reconstruct this evolutionary phylogeny. At the sa"
D12-1032,W11-2202,0,0.1157,"Missing"
D12-1032,W02-1009,1,0.865922,"ion over sequences of entities e is exchangeable. However, our distribution over sequences of named entities y = (e, w) is non-exchangeable. It assigns different probabilities to different orderings of the same tokens. This is because our model posits that later authors are influenced by earlier authors, copying entity names from them with mutation. So ordering is important. The mutation process is not symmetric—for example, Figure 1 reflects a tendency to shorten rather than lengthen names. Non-exchangeability is one way that our present model differs from (parametric) transformation models (Eisner, 2002) and (non-parametric) transformation processes (Andrews and Eisner, 2011). These too are defined using mutation of strings or other types. From a transformation process, one can draw a distribution over types, from which the tokens are then sampled IID. This results in an exchangeable sequence of tokens, just as in the Dirichlet process. We avoid transformation models here for three reasons. (1) Inference is more expensive. (2) A transformation process seems less realistic as a model of authorship. It constructs a distribution over derivational paths, similar to the paths in Figure 1. It effec"
D12-1032,P08-1088,0,0.0285121,"der. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) org"
D12-1032,P06-1103,0,0.0219438,"ollection, in an unknown order. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreye"
D12-1032,D07-1015,0,0.0523126,"edge from vertex x to vertex y, according to (10), is X cxy = pθ (T ) (11) T ∈T♦ (G):(x→y)∈T The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff’s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L ∈ Rn×n denote the Laplacian of G, namely  P 0 x0 δ(y |x ) if x = y L= (12) −δ(y |x) if x 6= y Tutte’s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 equals the total weight of all directed spanning trees rooted at node 0. This yields the partition function Z(G) (assuming node 0 is ♦). ˆ be the matrix L with the 0th row and 0th Let L column removed. Then the edge marginals of interest are related to the log partition function by cxy = ˆ ∂ log Z(G) ∂ log |L| = ∂ δ(y |x) ∂ δ(y |x) which has the c"
D12-1032,P11-1044,0,0.0237375,"e model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morphological paradigms of"
D12-1032,W02-2026,0,0.0117651,"is could include learning common nicknames; explicitly modeling abbreviation processes such as initials; conditioning on name components such as title and middle name; and transliterating across languages.14 In other domains, one could model bibliographic entry propagation, derivational morphology, or historical sound change (again using language tags). Another future direction would be to incorporate the context of tokens in order to help reconstruct which tokens are coreferent. Combining contextual similarity with string similarity has previously proved very useful for identifying cognates (Schafer and Yarowsky, 2002; Schafer, 2006b; Bergsma and Van Durme, 2011). In our setting it would help to distinguish people with identical names, as well as determining whether two people with similar names are really the same. 14 8 Conclusions and Future Work We have presented a new unsupervised method for learning string-to-string transducers. It learns from a collection of related strings whose relationships are unknown. The key idea is that some strings are mutations of common strings that occurred earlier. We compute a distribution over the unknown phylogeThese last two points suggest that the mutation model shou"
D12-1032,2006.amta-papers.23,0,0.316217,"e names in the collection, in an unknown order. Given learned parameters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ances"
D12-1032,D07-1014,0,0.0537622,"x to vertex y, according to (10), is X cxy = pθ (T ) (11) T ∈T♦ (G):(x→y)∈T The probability cxy is a “pseudocount” for the expected number of mutations from x to y. This is at most 1 under our assumptions. Calculating cxy requires summing over all spanning trees of G, of which there are nn−2 for a fully connected graph with n vertices. Fortunately, Tutte (1984) shows how to compute this sum by the following method, which extends Kirchhoff’s classical matrix-tree theorem to weighted directed graphs. This result has previously been employed in nonprojective dependency parsing (Koo et al., 2007; Smith and Smith, 2007). Let L ∈ Rn×n denote the Laplacian of G, namely  P 0 x0 δ(y |x ) if x = y L= (12) −δ(y |x) if x 6= y Tutte’s theorem relates the determinant of the Laplacian to the spanning trees in graph G. In particular, the cofactor L0,0 equals the total weight of all directed spanning trees rooted at node 0. This yields the partition function Z(G) (assuming node 0 is ♦). ˆ be the matrix L with the 0th row and 0th Let L column removed. Then the edge marginals of interest are related to the log partition function by cxy = ˆ ∂ log Z(G) ∂ log |L| = ∂ δ(y |x) ∂ δ(y |x) which has the closed-form solution ( ˆ"
D12-1032,P10-1107,0,0.0219979,"meters, we can ask the model whether a name Dr. J. J. Jingelheimer in the collection is more likely to have been generated from scratch, or derived from some previous name. 1.1 Related Work Several previous papers have also considered learning transducers or other models of word pairs when the pairing between inputs and outputs is not given. Most commonly, one observes parallel or comparable corpora in two languages, and must reconstruct a matching from one language’s words to the other’s before training on the resulting pairs (Schafer, 2006b; Klementiev and Roth, 2006; Haghighi et al., 2008; Snyder et al., 2010; Sajjad et al., 2011). Hall and Klein (2010) extend this setting to more than two languages, where the phylogenetic tree is known. A given lexeme (abstract word) can be realized in each language by at most one word (string type), derived from the parent language’s realization of the same lexeme. The system must match words that share an underlying lexeme (i.e., cognates), creating a matching of each language’s vocabulary to its parent language’s vocabulary. A further challenge is that the parent words are unobserved ancestral forms. Similarly, Dreyer and Eisner (2011) organize words into morp"
D12-1032,P10-1105,0,\N,Missing
D12-1032,J98-4003,0,\N,Missing
D12-1119,P08-1029,1,0.814281,"ly distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other fa"
D12-1119,W06-1615,0,0.15196,"ecific distribution Ddi , and yi is the label (e.g. yi ∈ {−1, +1} for binary labels). Standard learning ignores di , but MDL uses these to improve learning accuracy. Why should we care about the domain label? Domain differences can introduce errors in a number of ways (Ben-David et al., 2007; Ben-David et al., 2009). First, the domain-specific distributions Ddi can differ such that they favor different features, i.e. p(x) changes between domains. As a result, some features may only appear in one domain. This aspect of domain difference is typically the focus of unsupervised domain adaptation (Blitzer et al., 2006; Blitzer et al., 2007). Second, the features may behave differently with respect to the label in each domain, i.e. p(y|x) changes between domains. As a result, a learning algorithm cannot generalize the behavior of features from one domain to another. The key idea behind many MDL algorithms is to target one or both of these properties of domain difference 1303 to improve performance. Prior approaches to MDL can be broadly categorized into two classes. The first set of approaches (Daum´e III, 2007; Dredze et al., 2008) introduce parameters to capture domain-specific behaviors while preserving"
D12-1119,P07-1056,1,0.862631,"has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product r"
D12-1119,W04-3237,0,0.00951458,"ssues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art. 1 Introduction Research efforts in recent years have demonstrated the importance of domains in statistical natural language processing. A mismatch between training and test domains can negatively impact system accuracy as it violates a core assumption in many machine learning algorithms: that data points are independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One su"
D12-1119,W10-2608,0,0.0251531,"Missing"
D12-1119,P07-1033,0,0.72823,"Missing"
D12-1119,D08-1072,1,0.960015,"independent and identically distributed (i.i.d.). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would"
D12-1119,N09-1068,0,0.0948268,".). As a result, numerous domain adaptation methods (Chelba and Acero, 2004; Daum´e III and Marcu, 2006; Blitzer et al., 2007) target settings with a training set from one domain and a test set from another. Often times the training set itself violates the i.i.d. assumption and contains multiple domains. In this case, training a single model obscures domain distinctions, and separating the dataset by domains reduces training data. Instead, multi-domain learning (MDL) can take advantage of these domain labels to improve learning (Daum´e III, 2007; Dredze and Crammer, 2008; Arnold et al., 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011). One such example is sentiment classification of product reviews. Training data is available from many product categories and while all data should be used to learn a model, there are important differences between the categories (Blitzer et al., 2007)1 . While much prior research has shown improvements using MDL, this paper explores what properties of an MDL setting matter. Are previous improvements from MDL algorithms discovering important distinctions between features in different domains, as we would hope, or are other factors contributing to lear"
D12-1119,D07-1111,0,0.0119993,"ety of tasks. The key idea behind ensemble learning, that of combining a diverse array of models, has been applied to settings in which data preprocessing is used to create many different classifiers. Examples include instance bagging and feature bagging (Dietterich, 2000). The core idea of using diverse inputs in making classification decisions is common in the MDL literature. In fact, the top performing and only successful entry to the 2007 CoNLL shared task on domain adaptation for dependency parsing was a straightforward implementation of ensemble learning by creating variants of parsers (Sagae and Tsujii, 2007). Many MDL algorithms, among them Dredze and Crammer (2008), Daum´e III (2009), Zhang and Yeung (2010) and Saha et al. (2011), all include some notion of learning domain-specific classifiers on the training data, and combining them in the best way possible. To be clear, we do not claim that these approaches can be reduced to an existing ensemble learning algorithm. There are crucial elements in each of these algorithms that separate them from existing ensemble learning algorithms. One example of such a distinction is the learning of domain relationships by both Zhang and Yeung (2010) and Saha"
D12-1119,W06-1639,0,0.0122179,", electronics and kitchen appliances. The original dataset contained 2,000 reviews for each of the four domains, with 1,000 positive and 1,000 negative reviews per domain. Feature extraction follows Blitzer et al. (2007): we use case insensitive unigrams and bigrams, although we remove rare features (those that appear less than five times in the training set). The reduced feature set was selected given the sensitivity to feature size of some of the MDL methods. ConVote (C ONVOTE) Our second dataset is taken from segments of speech from United States Congress floor debates, first introduced by Thomas et al. (2006). The binary classification task on this dataset is that of predicting whether a given speech segment supports or opposes a bill under discussion in the floor debate. We select this dataset because, unlike the A MAZON data, C ONVOTE can be divided into domains in several ways based on different metadata attributes available with the dataset. We consider two types of domain divisions: the bill identifier and the political party of the speaker. Division based on the bill creates domain differences in that each bill has its own topic. Division based on political party implies preference for diffe"
D15-1064,W04-1119,0,0.0465915,"putational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media,"
D15-1064,W14-2907,0,0.0288029,"Missing"
D15-1064,W10-0713,1,0.384968,"Missing"
D15-1064,H05-1091,0,0.213055,"Missing"
D15-1064,P05-1045,0,0.0112183,"as above. where C is a tradeoff parameter. 3.3 Mentions Nominal 0 38 31 636 Table 1: Mention statistics for the Weibo NER corpus. The first objective is notated Ls for “supervised” (trained on labeled NER data), and the second is Lu , “unsupervised” (trained on raw text.) Both objectives share the same variables ew . The overall goal is to maximize their weighted sum: arg max = Ls (λ, ew ) + CLu (ew ) Name 243 88 224 721 5 Experiments We evaluate our methods under two settings: training on only name mentions, and training on both name and nominal mentions. We re-train the Stanford NER system (Finkel et al., 2005) as a baseline; besides, we also evaluate our implementation of the CRF from Mao et al. (2008) as described in §2 as Baseline Features. To this baseline, we add each of our three embedding models: word, character, character+position (as described in §3), and report results on the modified Weibo NER Corpus We constructed a corpus of Weibo messages annotated for NER. We followed the DEFT ERE (Linguistics Data Consortium, 2014) 9 annotation 9 See Aguilar et al. (2014) for a comparison of DEFT ERE with other common standards. 10 551 https://github.com/hltcoe/golden-horse Method Stanford Baseline F"
D15-1064,W10-0701,1,0.359481,"013) to do skip-gram training for language model, and implement our own CRF model to modify the embeddings. We optimize (2) by alternating the optimzation of each of the two objectives. 4 Total 243 126 255 1,357 guidelines for entities, which includes four major semantic types: person, organization, location and geo-political entity. We annotated both name and nominal mentions. Chinese pronoun mentions can be easily recognized with a regular expression. We used Amazon Mechanical Turk, using standard methods of multiple annotators and including gold examples to ensure high quality annotations (Callison-Burch and Dredze, 2010). Our corpus includes 1,890 messages sampled from Weibo between November 2013 and December 2014. Rather than selecting messages at random, which would yield a small number of messages with entities, we selected messages that contained three or more (segmented) words that were not in a fixed vocabulary of common Chinese words. Initial experiments showed this gave messages more likely to contain entities. Table 1 shows statistics of the final corpus. We divided the corpus into 7 folds, each with 127 messages, where each message corresponds to a single instance. We use the first 5 folds for train"
D15-1064,fromreide-etal-2014-crowdsourcing,0,0.0434094,"tion Named entity recognition (NER), and more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to Engli"
D15-1064,N15-1075,0,0.309073,"se Social Media with Jointly Trained Embeddings Nanyun Peng and Mark Dredze Human Language Technology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract the popularity of the service (comparable in size to Twitter and previously used in NLP research (Ling et al., 2013)) and the challenges faced in processing Chinese language data. One approach is to utilize lexical embeddings to improve NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014), including for Twitter (Cherry and Guo, 2015). However, the use of embeddings for Chinese remains a challenge. Unlike most languages, we cannot easily assign an embedding to each Chinese word without automated segmentation, which may be unreliable, especially when we want to model informal text.3 For this reason, state-of-the-art NER systems for Chinese do not tag words; they instead tag characters directly (Mao et al., 2008). While work has explored different embeddings for Chinese (Liu et al., 2014; Sun et al., 2014; Qiu et al., 2014; Chen et al., 2015), their inclusion in downstream tasks, such as NER, remains untested. We explore sev"
D15-1064,W12-6321,0,0.139576,"Missing"
D15-1064,W99-0613,0,0.142277,"Missing"
D15-1064,I08-4010,0,0.0293428,"Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses"
D15-1064,C10-1032,1,0.818044,"Missing"
D15-1064,W12-6307,0,0.0727395,"formal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 1 Since we consider name and nominals, our work is closer to mention detection. For simplicity, we use the term NER. 2 Etter et al. (2013) considered Spanish Twitter, which is quite similar to English from the standpoint of building models and features. 548 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 54"
D15-1064,P13-1018,0,0.0361143,"Missing"
D15-1064,P11-1037,0,0.0174892,"a stateof-the-art baseline. 1 Introduction Named entity recognition (NER), and more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER pe"
D15-1064,P10-1040,0,0.0148339,"reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We"
D15-1064,P12-1055,0,0.015772,"more generally the task of mention detection1 , is an essential component of information extraction technologies: the first step before tasks such as relation extraction (Bunescu and Mooney, 2005) and entity linking (Dredze et al., 2010; Ratinov et al., 2011). A long line of work has focused on NER in both formal and informal domains (Collins and Singer, 1999; McCallum and Li, 2003; Nadeau and Sekine, 2007; Jin and Chen, 2008; He et al., 2012a), with recent efforts turning towards social media (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012). While NER has included work on several languages, work on social media NER has largely focused on English language data.2 We consider NER on Chinese social media from the popular Sina Weibo service, both because of 3 Word segmentation performance is much worse on social media compared to formal text (Duan et al., 2012). 4 Consider the overall F1 scores from Ritter et al. (2011), Cherry and Guo (2015) and Fromreide et al. (2014) compared to our best results in Table 2. This is despite the fact that Chinese NER performance on formal texts is similar to English. 1 Since we consider name and nom"
D15-1064,I08-4013,0,0.681737,"rs, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are worse in social media, which has worse word segmentation. Additionally, typical Chinese corpora use exclusively traditional or simplified characters, whereas social media mixes them. Figure 1 demonstrates some challenges. The baseline system for our task is our own implementation of Mao et al. (2008), which is the current state-of-the-art on the SIGHAN 2008 shared task (Jin and Chen, 2008). They use a CRF tagger with a BIOSE (begin, inside, outside, singleton, end) encoding that tags individual characters, not words, since word segmentation errors are especially problematic for NER (Zhang et al., 2006). Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting"
D15-1064,W03-0430,0,0.0697886,"Missing"
D15-1064,P14-2089,1,0.805775,"okup the embedding that matches the segmented word. Since the NER system tags characters, we add the same word embedding features to each character in the word. This is a standard method that has been previously explored in sequential and structured prediction problem (Collobert et al., 2011; Zheng et al., 2013; Yao et al., 2014; Pei et al., 2014). 3.2 Fine-tuning has a disadvantage: it can arbitrarily deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF"
D15-1064,Q15-1017,1,0.81813,"y deviate from the settings obtained from training on large amounts of raw text. Recent work has instead tuned embeddings for a specific task, while maintaining information learned from raw text. Yu and Dredze (2014) use multi-part objectives that include both standard unlabeled objectives, such as skip-gram models in word2vec, and task specific objectives. Jointly training the embeddings with the multi-part objectives allows the fine-tuned embeddings to further influence other embeddings, even those that do not appear in the labeled training data. This type of training can help improve OOVs (Yu and Dredze, 2015), an important aspect of improving social media NER. We propose to jointly learn embeddings for both language models and the NER task. The modified objective function (log-likelihood) for the CRF is given by: Character Embeddings We learn an embedding for each character in the training corpus (Sun et al., 2014; Liu et al., 2014).This removes the dependency on pre-processing the text, and better fits our intended use case: NER tagging over characters. Since there are many fewer characters than words, we learn many fewer embeddings. On the one hand, this means fewer parameters and less over-fitt"
D15-1064,W06-0126,0,0.744155,"548–554, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundari"
D15-1064,W14-1609,0,0.0310465,"al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embeddings represent words in a continuous low dimensional space, which can capture semantic or syntactic properties of the lexicon: similar words would have similar low dimensional vector representations. Embeddings have been used to gain improvements in a variety of NLP tasks. In NER specifically, several papers have shown improvements by using pre-trained neural embeddings as features in standard NER systems (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014). More recently, these improvements have been demonstrated on Twitter data (Cherry and Guo, 2015). Embeddings are especially helpful when there is little training data, since they can be trained on a large amount of unlabeled data. This is the case for new languages and domains, the task we face in this paper. However, training embeddings for Chinese is not straightforward: Chinese is not word segmented, so embeddings for each word cannot be trained on a raw corpus. Additionally, the stateof-the-art systems for downstream Chinese tasks, such as NER, may not use words. We present three types of"
D15-1064,D13-1031,0,0.0119729,". Features include many common English NER features, e.g. character unigrams and bigrams, with context windows of size 5. See Mao et al. (2008) for complete details on their system. Mao et al. (2008) use a two pass approach, training a CRF first for mention detection and using the resulting predictions as a feature for an NER system. Furthermore, they make extensive use of gazetteer features. For simplicity, we exclude the first pass mention detection and the gazetteer features, which make only small improvements to their overall performance. We note that other implementations of this system (Zhang et al., 2013) have been unable to match the performance reported in Mao et al. (2008). Similarly, our implementation yields results on SIGHAN 2008 similar 看见前女友和她的新欢走在一起的时候，已经无处可躲了，只好 硬着 头皮上去打招呼哎呀，好久不见，你儿子都这么高了。 When saw ex-girl friend and her new partner coming across, nowhere to hide, have to say hello, long time no see, your son grown up. Figure 1: Examples of Weibos messages and translations with named (red) and nominal (blue) mentions. to those reported in Zhang et al. (2013).6 Overall, we take this tagger as representative of state-ofthe-art for Chinese NER. 3 Embeddings for Chinese Text Lexical embe"
D15-1064,P14-1028,0,0.0578429,"Missing"
D15-1064,D13-1061,0,0.0431402,"Missing"
D15-1064,C14-1015,0,0.0255117,"Missing"
D15-1064,W03-1718,0,0.050791,"ssociation for Computational Linguistics. jective that trains embeddings simultaneously for both NER and language modeling. Joint training yields better results than post-hoc fine-tuning. 有好多好多的话想对你说李巾凡想要瘦瘦瘦成李帆我是想切开云 朵的心 Have many many words to say to you Jinfan Li wanna thin thin thin to Fan Li I am a heart that want to cut the cloud 2 美得呀～顾天池苦逼青年杨素晗闵日记肖立伟嘻嘻嘻嘻嘻嘻美啊 Beautiful Tianchi Gu bitter youth Suhan Yang Riji Min Liwei Xiao hahahahahaha beautiful NER for Chinese Social Media Several SIGHAN shared tasks have focused on Chinese NER (Zhang et al., 2006; Jin and Chen, 2008; He et al., 2012b; Zhu et al., 2003; Fang et al., 2004; Zhang et al., 2006), though they have been restricted to formal text, e.g. news. NER for Chinese social media remains unexplored.5 As is the case for other languages, social media informality introduces numerous problems for NLP systems, such as spelling errors, novel words, and ungrammatical constructions. Chinese presents additional challenges, since it uses logograms instead of alphabets, and lacks many of the clues that a word is a name, e.g. capitalization and punctuation marks. The lack of explicit word boundaries further confuses NER systems. These problems are wors"
D15-1064,P11-1138,0,0.0698598,"Missing"
D15-1064,D11-1141,0,0.0509802,"Missing"
D15-1205,Q14-1043,0,0.0267622,"ieve such specialization in a more general fashion: 1. Enhancing Compositional Models with Features. A recent trend enhances compositional models with annotation features. Such an approach has been shown to significantly improve over pure compositional models. For example, Hermann et al. (2014) and Nguyen and Grishman (2014) gave different weights to words with different syntactic context types or to entity head words with different argument IDs. Zeng et al. (2014) use concatenations of embeddings as features in a CNN model, according to their positions relative to the target entity mentions. Belinkov et al. (2014) enrich embeddings with linguistic features before feeding them forward to a RNN model. Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags. 2. Engineering of Embedding Features. A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation ext"
D15-1205,W06-1670,0,0.020732,"Missing"
D15-1205,P15-1061,0,0.0537127,"our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word embeddings. Compared to existing compositional models, FCM can easily handle arbitrary types of input and handle global information for composition, while remaining easy to implement. We have demonstrated that FCM alone attains near state-of-the-art performances on several relation extraction tasks, and in combination with traditional feature based loglinear models it obtains state-of-the-art results"
D15-1205,K15-1027,0,0.267326,"2005 (Walker et al., 2006) and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010). Contributions This paper makes several contributions, including: 1. We introduce the FCM, a new compositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the"
D15-1205,W09-2415,0,0.223175,"Missing"
D15-1205,P13-1088,0,0.0209533,"al models with annotation features. Such an approach has been shown to significantly improve over pure compositional models. For example, Hermann et al. (2014) and Nguyen and Grishman (2014) gave different weights to words with different syntactic context types or to entity head words with different argument IDs. Zeng et al. (2014) use concatenations of embeddings as features in a CNN model, according to their positions relative to the target entity mentions. Belinkov et al. (2014) enrich embeddings with linguistic features before feeding them forward to a RNN model. Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced RNN models by refining the transformation matrices with phrase types and CCG super tags. 2. Engineering of Embedding Features. A different approach to combining traditional linguistic features and embeddings is hand-engineering features with word embeddings and adding them to log-linear models. Such approaches have achieved state-of-the-art results in many tasks including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013)."
D15-1205,P14-1136,0,0.0677707,"we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, we begin with a precise construction of compositional embeddings using word embeddings in conjunction with unlexicalized features. Various feature sets used in prior work (Turian et al., 2010; Nguyen and Grishman, 2014; Hermann et al., 2014; Roth and Woodsend, 2014) are cap2 Such embeddings have a long history in NLP, including term-document frequency matrices and their lowdimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF), Brown clusters, random projections and vector space models. Recently, neural networks / deep learning have provided several popular methods for obtaining such embeddings. 1774 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1774–1784, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Class (1) AR"
D15-1205,P08-1068,0,0.0321661,".com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structur"
D15-1205,P14-1038,0,0.154946,"Missing"
D15-1205,P15-2047,0,0.0353483,"l., 2011; Plank and Moschitti, 2013). Roth and Woodsend (2014) considered features similar to ours for semantic role labeling. However, in prior work both of above approaches are only able to utilize limited information, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word embeddings. Compared"
D15-1205,P15-2029,0,0.0226376,"d, 2014; Sun et al., 2011; Plank and Moschitti, 2013). Roth and Woodsend (2014) considered features similar to ours for semantic role labeling. However, in prior work both of above approaches are only able to utilize limited information, usually one property for each word. Yet there may be different useful properties of a word which can contribute to the performances of the task. By contrast, our FCM can easily utilize these features without changing the model structures. In order to better utilize the dependency annotations, recently work built their models according to the dependency paths (Ma et al., 2015; Liu et al., 2015), which share similar motivations to the usage of On-path features in our work. Task-Specific Enhancements for Relation Classification An orthogonal direction of improving compositional models for relation classification is to enhance the models with task-specific information. For example, Hashimoto et al. (2015) trained task-specific word embeddings, and dos Santos et al. (2015) proposed a ranking-based loss function for relation classification. 9 Conclusion We have presented FCM, a new compositional model for deriving sentence-level and substructure embeddings from word em"
D15-1205,P14-5010,0,0.0084969,"Missing"
D15-1205,N04-1043,0,0.0670003,"and Yu contributed equally. https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embedd"
D15-1205,P14-2012,0,0.110196,"guistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, we begin with a precise const"
D15-1205,W15-1506,0,0.235351,") and the relation classification dataset from SemEval-2010 Task 8 (Hendrickx et al., 2010). Contributions This paper makes several contributions, including: 1. We introduce the FCM, a new compositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all"
D15-1205,P15-1062,0,0.0617909,"is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We are the first to apply neural language model embeddings to this task. Motivation and Examples Whether a word is indicative of a relation depends on multiple properties, which may relate to its context within the sentence. F"
D15-1205,P10-1040,0,0.791123,"ually. https://github.com/mgormley/pacaya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary l"
D15-1205,P13-1147,0,0.178163,"ith aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. In the following sections, w"
D15-1205,Q15-1017,1,0.853446,"ompositional embedding model for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all pairs of entity mentions, what relation exists between them, if any. For each pair of entity mentions in a sentence S, we construct an instance (y, x), where x = (M1 , M2 , S, A). S ="
D15-1205,S10-1057,0,0.357117,"Missing"
D15-1205,D14-1045,0,0.293506,"ya its lemma, its morphological features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand c"
D15-1205,D12-1110,0,0.882678,"M1 ) are different relations. Table 1 shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (SemEval), where the number of negative examples is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We"
D15-1205,N15-1155,1,0.49156,"del for relation extraction. 2. We obtain the best reported results on ACE2005 for coarse-grained relation extraction in the cross-domain setting, by combining FCM with a log-linear model. 3. We obtain results on on SemEval-2010 Task 8 competitive with the best reported results. Note that other work has already been published that builds on the FCM, such as Hashimoto et al. (2015), Nguyen and Grishman (2015), dos Santos 3 In ACE 2005, ART refers to a relation between a person and an artifact; such as a user, owner, inventor, or manufacturer relationship et al. (2015), Yu and Dredze (2015) and Yu et al. (2015). Additionally, we have extended FCM to incorporate a low-rank embedding of the features (Yu et al., 2015), which focuses on fine-grained relation extraction for ACE and ERE. This paper obtains better results than the low-rank extension on ACE coarse-grained relation extraction. 2 Relation Extraction In relation extraction we are given a sentence as input with the goal of identifying, for all pairs of entity mentions, what relation exists between them, if any. For each pair of entity mentions in a sentence S, we construct an instance (y, x), where x = (M1 , M2 , S, A). S = {w1 , w2 , ..., wn }"
D15-1205,C14-1220,0,0.307454,"elations. Table 1 shows ACE 2005 relations, and has a strong label bias towards negative examples. We also consider the task of relation classification (SemEval), where the number of negative examples is artificially reduced. Embedding Models Word embeddings and compositional embedding models have been successfully applied to a range of NLP tasks, however the applications of these embedding models to relation extraction are still limited. Prior work on relation classification (e.g. SemEval 2010 Task 8) has focused on short sentences with at most one relation per sentence (Socher et al., 2012; Zeng et al., 2014). For relation extraction, where negative examples abound, prior work has assumed that only the named entity boundaries and not their types were available (Plank and Moschitti, 2013; Nguyen et al., 2015). Other work has as1775 sumed that the order of two entities in a relation are given while the relation type itself is unknown (Nguyen and Grishman, 2014; Nguyen and Grishman, 2015). The standard relation extraction task, as adopted by ACE 2005 (Walker et al., 2006), uses long sentences containing multiple named entities with known types4 and unknown relation directions. We are the first to app"
D15-1205,P05-1053,0,0.367173,"several methods. (1) FCM these embeddings from an neural language model in isolation without fine-tuning. (2) FCM in isolaand then fine-tune them for our supervised task. tion with fine-tuning (i.e. trained as a log-bilinear The training process for the hybrid model (§ 4) 7 is also easily done by backpropagation since each Obtained from the constituency parse using the CONLL sub-model has separate parameters. 2000 chunking converter (Perl script). 1778 model). (3) A log-linear model with a rich binary feature set from Sun et al. (2011) (Baseline)— this consists of all the baseline features of Zhou et al. (2005) plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research. We exclude the Country gazetteer and WordNet features from Zhou et al. (2005). The two remaining methods are hybrid models that integrate FCM as a submodel within the log-linear model (§ 4). We consider two combinations. (4) The feature set of Nguyen and Grishman (2014) obtained by using the embeddings of heads of two entity mentions (+HeadOnly). (5) Our full FCM model (+FCM). All models use L2 regularization tuned on dev data. 6.1 Datasets and Evaluation ACE"
D15-1205,P13-1045,0,0.188701,"et al. (2015). 8 Related Work Compositional Models for Sentences In order to build a representation (embedding) for a sentence based on its component word embeddings and structural information, recent work on compositional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) (Collobert et al., 2011) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Function) (Socher et al., 2013b; Bordes et al., 2012). While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbitrary combinations of linguistic annotations available for a given task, such as word order, dependency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in N"
D15-1205,D13-1170,0,0.0209519,"et al. (2015). 8 Related Work Compositional Models for Sentences In order to build a representation (embedding) for a sentence based on its component word embeddings and structural information, recent work on compositional models (stemming from the deep learning community) has designed model structures that mimic the structure of the input. For example, these models could take into account the order of the words (as in Convolutional Neural Networks (CNNs)) (Collobert et al., 2011) or build off of an input tree (as in Recursive Neural Networks (RNNs) or the Semantic Matching Energy Function) (Socher et al., 2013b; Bordes et al., 2012). While these models work well on sentence-level representations, the nature of their designs also limits them to fixed types of substructures from the annotated sentence, such as chains for CNNs and trees for RNNs. Such models cannot capture arbitrary combinations of linguistic annotations available for a given task, such as word order, dependency tree, and named entities used for relation extraction. Moreover, these approaches ignore the differences in functions between words appearing in different roles. This does not suit more general substructure labeling tasks in N"
D15-1205,P11-1053,0,0.494673,"ogical features) with aspects of a word’s linguistic context (e.g. whether it lies between two entities or on a dependency path between them). While these help learning, they make generalization to unseen words difficult. An alternative approach to capturing lexical information relies on continuous word embeddings2 as representative of words but generalizable to new words. Embedding features have improved many tasks, including NER, chunking, dependency parsing, semantic role labeling, and relation extraction (Miller et al., 2004; Turian et al., 2010; Koo et al., 2008; Roth and Woodsend, 2014; Sun et al., 2011; Plank and Moschitti, 2013; Nguyen and Grishman, 2014). Embeddings can capture lexical information, but alone they are insufficient: in state-of-the-art systems, they are used alongside features of the broader linguistic context. In this paper, we introduce a compositional model that combines unlexicalized linguistic context and word embeddings for relation extraction, a task in which contextual feature construction plays a major role in generalizing to unseen data. Our model allows for the composition of embeddings with arbitrary linguistic structure, as expressed by hand crafted features. I"
D15-1205,D12-1042,0,0.300334,"Missing"
D15-1205,P10-1030,0,\N,Missing
D15-1205,P14-1130,0,\N,Missing
D15-1205,P14-1063,0,\N,Missing
D19-1077,N19-1253,0,0.293898,"ning with different source and target domain (Pan and Yang, 2010). A cross-lingual representation space is assumed to perform the cross-lingual transfer. Before the widespread use of cross-lingual word embeddings, task-specific models assumed coarse-grain representation like part-of-speech tags, in support of a delexicalized parser (Zeman and Resnik, 2008). More recently cross-lingual word embeddings have been used in conjunction with task-specific neural architectures for tasks like named entity recognition (Xie et al., 2018), part-of-speech tagging (Kim et al., 2017) and dependency parsing (Ahmad et al., 2019). Cross-lingual Word Embeddings. The quality of the cross-lingual space is essential for zero-shot cross-lingual transfer. Ruder et al. (2017) surveys methods for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. Conneau et al. (2017) and Artetxe et al. (2018) first show two monolingual embeddings can be aligned by learning an orthogonal mapping with only identical strings as an initial heuristic bilingual dictionary. Contextual Word Embeddings ELMo (Peters et al., 2018), a deep LSTM (Hochreiter and Schmidhuber, 1997) pretraine"
D19-1077,P18-1073,0,0.103562,"et al., 2018), sequence tagging (e.g. NER) (Tjong Kim Sang and De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016). Self-supervised objectives include language modeling, the cloze task (Taylor, 1953) and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram (Mikolov et al., 2013a). Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models have reduced the amount of cross-lingual supervision required to produce reasonable models; Conneau et al. (2017); Artetxe et al. (2018) use identical strings between languages as a pseudo bilingual dictionary to learn a mapping between monolingual-trained embeddings. Can jointly training contextual embedding models over multiple languages without explicit mappings produce an effective cross-lingual representation? Surprisingly, the answer is (partially) yes. BERT, a recently introduced pretrained model (Devlin et al., 2019), offers a multilingual model (mBERT) pretrained on concatenated Wikipedia data for 104 languages without any cross-lingual alignment (Devlin, 2018). mBERT does surprisingly well compared to cross-lingual w"
D19-1077,D18-1269,0,0.416707,"ual dictionary to learn a mapping between monolingual-trained embeddings. Can jointly training contextual embedding models over multiple languages without explicit mappings produce an effective cross-lingual representation? Surprisingly, the answer is (partially) yes. BERT, a recently introduced pretrained model (Devlin et al., 2019), offers a multilingual model (mBERT) pretrained on concatenated Wikipedia data for 104 languages without any cross-lingual alignment (Devlin, 2018). mBERT does surprisingly well compared to cross-lingual word embeddings on zeroshot cross-lingual transfer in XNLI (Conneau et al., 2018), a natural language inference dataset. Zeroshot cross-lingual transfer, also known as singlesource transfer, refers trains and selects a model in a source language, often a high resource language, then transfers directly to a target language. While XNLI results are promising, the question remains: does mBERT learn a cross-lingual space that supports zero-shot transfer? We evaluate mBERT as a zero-shot cross-lingual transfer model on five different NLP tasks: natural language inference, document classification, named entity recognition, part-of-speech tagging, and dependency parsing. We show t"
D19-1077,N19-1423,0,0.219349,"NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019), including sentence-level classification (Wang et al., 2018), sequence tagging (e.g. NER) (Tjong Kim Sang and De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016). Self-supervised objectives include language modeling, the cloze task (Taylor, 1953) and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram (Mikolov et al., 2013a). Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models have reduced the amount of cross-lingual supervision required to pr"
D19-1077,P81-1022,0,0.476005,"Missing"
D19-1077,P18-1031,0,0.121065,"nguage families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019), including sentence-level classification (Wang et al., 2018), sequence tagging (e.g. NER) (Tjong Kim Sang and De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016). Self-supervised objectives include language modeling, the cloze task (Taylor, 1953) and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram (Mikolov et al., 2013a). Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models have reduced the amount"
D19-1077,D17-1302,0,0.438316,"ng is a type of transductive transfer learning with different source and target domain (Pan and Yang, 2010). A cross-lingual representation space is assumed to perform the cross-lingual transfer. Before the widespread use of cross-lingual word embeddings, task-specific models assumed coarse-grain representation like part-of-speech tags, in support of a delexicalized parser (Zeman and Resnik, 2008). More recently cross-lingual word embeddings have been used in conjunction with task-specific neural architectures for tasks like named entity recognition (Xie et al., 2018), part-of-speech tagging (Kim et al., 2017) and dependency parsing (Ahmad et al., 2019). Cross-lingual Word Embeddings. The quality of the cross-lingual space is essential for zero-shot cross-lingual transfer. Ruder et al. (2017) surveys methods for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. Conneau et al. (2017) and Artetxe et al. (2018) first show two monolingual embeddings can be aligned by learning an orthogonal mapping with only identical strings as an initial heuristic bilingual dictionary. Contextual Word Embeddings ELMo (Peters et al., 2018), a deep LSTM"
D19-1077,W06-0115,0,0.0680853,"X X el X en es X X X X X X X X X X et fa fi fr he X X hi hr id X X X it ja X X ko la lv nl no pl pt ro X X X X X X X ru sk sl sv X X X X X hu X X X X X X X X X X X X X X X X X X X X sw th tr X X X uk X ur vi zh X X X X X X Table 1: The 39 languages used in the 5 tasks. 5 feed a pair of sentences directly into mBERT and the task-specific classification layer is the same as §4.1. We evaluate by classification accuracy. 4.3 Named Entity Recognition We use the CoNLL 2002 and 2003 NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) (4 languages) and a Chinese NER dataset (Levow, 2006). The labeling scheme is BIO with 4 types of named entities. We add a linear classification layer with softmax to obtain word-level predictions. Since mBERT operates at the subword-level while the labeling is word-level, if a word is broken into multiple subwords, we mask the prediction of non-first subwords. NER is evaluated by F1 of predicted entity (F1). Note we use a simple post-processing heuristic to obtain a valid span. 4.4 Part-of-Speech Tagging We use a subset of Universal Dependencies (UD) Treebanks (v1.4) (Nivre et al., 2016), which cover 15 languages, following the setup of Kim et"
D19-1077,N19-1392,0,0.0617324,"jointly fine-tunes with the language modeling objective. Howard and Ruder (2018) propose another fine-tuning strategy by using a different learning rate for each layer with learning rate warmup and gradual unfreezing. Concurrent work by Lample and Conneau (2019) incorporates bitext into BERT by training on pairs of parallel sentences. Schuster et al. (2019) aligns pretrained ELMo of different languages by learning an orthogonal mapping and shows strong zero-shot and few-shot cross-lingual transfer performance on dependency parsing with 5 Indo-European languages. Similar to multilingual BERT, Mulcaire et al. (2019) trains a single ELMo on distantly related languages and shows mixed results as to the benefit of pretaining. Parallel to our work, Pires et al. (2019) shows mBERT has good zero-shot cross-lingual transfer performance on NER and POS tagging. They show how subword overlap and word ordering effect mBERT transfer performance. Additionally, they show mBERT can find translation pairs and works on code-switched POS tagging. In comparison, our work looks at a larger set of NLP tasks including dependency parsing and ground the mBERT performance against previous state-of-the-art on zeroshot cross-lingu"
D19-1077,D14-1162,0,0.0876631,"ings by either joint training or post-training mappings of monolingual embeddings. Conneau et al. (2017) and Artetxe et al. (2018) first show two monolingual embeddings can be aligned by learning an orthogonal mapping with only identical strings as an initial heuristic bilingual dictionary. Contextual Word Embeddings ELMo (Peters et al., 2018), a deep LSTM (Hochreiter and Schmidhuber, 1997) pretrained with a language modeling objective, learns contextual word embeddings. This contextualized representation outperforms standalone word embeddings, e.g. Word2Vec (Mikolov et al., 2013b) and Glove (Pennington et al., 2014), with the same task-specific architecture in various downstream tasks. Instead of taking the representation from a pretrained model, GPT (Radford et al., 2018) and Howard and Ruder (2018) also fine-tune all the parameters of the pretrained model for a specific task. Also, GPT uses a transformer encoder (Vaswani et al., 2017) instead of an LSTM and jointly fine-tunes with the language modeling objective. Howard and Ruder (2018) propose another fine-tuning strategy by using a different learning rate for each layer with learning rate warmup and gradual unfreezing. Concurrent work by Lample and C"
D19-1077,N18-1202,0,0.785675,"uages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019), including sentence-level classification (Wang et al., 2018), sequence tagging (e.g. NER) (Tjong Kim Sang and De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016). Self-supervised objectives include language modeling, the cloze task (Taylor, 1953) and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram (Mikolov et al., 2013a). Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models"
D19-1077,P19-1493,0,0.108,"for each layer with learning rate warmup and gradual unfreezing. Concurrent work by Lample and Conneau (2019) incorporates bitext into BERT by training on pairs of parallel sentences. Schuster et al. (2019) aligns pretrained ELMo of different languages by learning an orthogonal mapping and shows strong zero-shot and few-shot cross-lingual transfer performance on dependency parsing with 5 Indo-European languages. Similar to multilingual BERT, Mulcaire et al. (2019) trains a single ELMo on distantly related languages and shows mixed results as to the benefit of pretaining. Parallel to our work, Pires et al. (2019) shows mBERT has good zero-shot cross-lingual transfer performance on NER and POS tagging. They show how subword overlap and word ordering effect mBERT transfer performance. Additionally, they show mBERT can find translation pairs and works on code-switched POS tagging. In comparison, our work looks at a larger set of NLP tasks including dependency parsing and ground the mBERT performance against previous state-of-the-art on zeroshot cross-lingual transfer. We also probe mBERT in different ways and show a more complete picture of the cross-lingual effectiveness of mBERT. 3 Multilingual BERT BE"
D19-1077,D16-1264,0,0.0615376,"lly, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language-specific features, and measure factors that influence cross-lingual transfer. 1 Introduction Pretrained language representations with selfsupervised objectives have become standard in a variety of NLP tasks (Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019), including sentence-level classification (Wang et al., 2018), sequence tagging (e.g. NER) (Tjong Kim Sang and De Meulder, 2003) and SQuAD question answering (Rajpurkar et al., 2016). Self-supervised objectives include language modeling, the cloze task (Taylor, 1953) and next sentence classification. These objectives continue key ideas in word embedding objectives like CBOW and skip-gram (Mikolov et al., 2013a). Code is available at https://github.com/ shijie-wu/crosslingual-nlp At the same time, cross-lingual embedding models have reduced the amount of cross-lingual supervision required to produce reasonable models; Conneau et al. (2017); Artetxe et al. (2018) use identical strings between languages as a pseudo bilingual dictionary to learn a mapping between monolingual-"
D19-1077,N19-1162,0,0.0648139,"g the representation from a pretrained model, GPT (Radford et al., 2018) and Howard and Ruder (2018) also fine-tune all the parameters of the pretrained model for a specific task. Also, GPT uses a transformer encoder (Vaswani et al., 2017) instead of an LSTM and jointly fine-tunes with the language modeling objective. Howard and Ruder (2018) propose another fine-tuning strategy by using a different learning rate for each layer with learning rate warmup and gradual unfreezing. Concurrent work by Lample and Conneau (2019) incorporates bitext into BERT by training on pairs of parallel sentences. Schuster et al. (2019) aligns pretrained ELMo of different languages by learning an orthogonal mapping and shows strong zero-shot and few-shot cross-lingual transfer performance on dependency parsing with 5 Indo-European languages. Similar to multilingual BERT, Mulcaire et al. (2019) trains a single ELMo on distantly related languages and shows mixed results as to the benefit of pretaining. Parallel to our work, Pires et al. (2019) shows mBERT has good zero-shot cross-lingual transfer performance on NER and POS tagging. They show how subword overlap and word ordering effect mBERT transfer performance. Additionally,"
D19-1077,L18-1560,0,0.142361,"oes it produce a representation for each language in its own embedding space? We consider five tasks in the zero-shot transfer setting. We assume labeled training data for each task in English, and transfer the trained model to a target language. We select a range of different tasks: document classification, natural language inference, named entity recognition, part-of-speech tagging, and dependency parsing. We cover zero-shot transfer from English to 38 languages in the 5 different tasks as shown in Tab. 1. In this section, we describe the tasks as well as task-specific layers. We use MLDoc (Schwenk and Li, 2018), a balanced subset of the Reuters corpus covering 8 languages for document classification. The 4-way topic classification task decides between CCAT (Corporate/Industrial), ECAT (Economics), GCAT (Government/Social), and MCAT (Markets). We only use the first two sentences2 of a document for classification due to memory constraint. The sentence pairs are provided to the mBERT encoder. The task-specific classification layer is a linear d 4 function mapping h12 0 ∈ Rh into R , and a softmax is used to get class distribution. We evaluate by classification accuracy. 4.2 Natural Language Inference W"
D19-1077,W02-2024,0,0.235117,"K (Perkins, 2014). 835 ar MLDoc NLI NER POS Parsing bg X X X X X ca X cs X da de X X X X X X X el X en es X X X X X X X X X X et fa fi fr he X X hi hr id X X X it ja X X ko la lv nl no pl pt ro X X X X X X X ru sk sl sv X X X X X hu X X X X X X X X X X X X X X X X X X X X sw th tr X X X uk X ur vi zh X X X X X X Table 1: The 39 languages used in the 5 tasks. 5 feed a pair of sentences directly into mBERT and the task-specific classification layer is the same as §4.1. We evaluate by classification accuracy. 4.3 Named Entity Recognition We use the CoNLL 2002 and 2003 NER shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) (4 languages) and a Chinese NER dataset (Levow, 2006). The labeling scheme is BIO with 4 types of named entities. We add a linear classification layer with softmax to obtain word-level predictions. Since mBERT operates at the subword-level while the labeling is word-level, if a word is broken into multiple subwords, we mask the prediction of non-first subwords. NER is evaluated by F1 of predicted entity (F1). Note we use a simple post-processing heuristic to obtain a valid span. 4.4 Part-of-Speech Tagging We use a subset of Universal Dependencies (UD) Tre"
D19-1077,W18-5446,0,0.0753764,"Missing"
D19-1077,D18-1034,0,0.420389,"ngual Transfer Crosslingual transfer learning is a type of transductive transfer learning with different source and target domain (Pan and Yang, 2010). A cross-lingual representation space is assumed to perform the cross-lingual transfer. Before the widespread use of cross-lingual word embeddings, task-specific models assumed coarse-grain representation like part-of-speech tags, in support of a delexicalized parser (Zeman and Resnik, 2008). More recently cross-lingual word embeddings have been used in conjunction with task-specific neural architectures for tasks like named entity recognition (Xie et al., 2018), part-of-speech tagging (Kim et al., 2017) and dependency parsing (Ahmad et al., 2019). Cross-lingual Word Embeddings. The quality of the cross-lingual space is essential for zero-shot cross-lingual transfer. Ruder et al. (2017) surveys methods for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. Conneau et al. (2017) and Artetxe et al. (2018) first show two monolingual embeddings can be aligned by learning an orthogonal mapping with only identical strings as an initial heuristic bilingual dictionary. Contextual Word Embeddin"
D19-1077,I08-3008,0,0.0398047,"nguistics using each layer of mBERT. Finally, we show how subword tokenization influences transfer by measuring subword overlap between languages. 2 Background (Zero-shot) Cross-lingual Transfer Crosslingual transfer learning is a type of transductive transfer learning with different source and target domain (Pan and Yang, 2010). A cross-lingual representation space is assumed to perform the cross-lingual transfer. Before the widespread use of cross-lingual word embeddings, task-specific models assumed coarse-grain representation like part-of-speech tags, in support of a delexicalized parser (Zeman and Resnik, 2008). More recently cross-lingual word embeddings have been used in conjunction with task-specific neural architectures for tasks like named entity recognition (Xie et al., 2018), part-of-speech tagging (Kim et al., 2017) and dependency parsing (Ahmad et al., 2019). Cross-lingual Word Embeddings. The quality of the cross-lingual space is essential for zero-shot cross-lingual transfer. Ruder et al. (2017) surveys methods for learning cross-lingual word embeddings by either joint training or post-training mappings of monolingual embeddings. Conneau et al. (2017) and Artetxe et al. (2018) first show"
D19-1077,W03-0419,0,\N,Missing
I17-3002,E17-2114,1,0.840887,"Missing"
I17-3002,P07-2045,1,0.0098071,"Missing"
N10-1025,D09-1116,1,0.704078,"7 Words-Stemmed) Global Utterance Context We now include features that incorporate information from the entire utterance. The probability of an utterance as computed by a language model is often used as a measure of fluency of the utterance. We also observe that OOV words tend to take very specific syntactic roles (more than half of them are proper nouns), which means the surrounding context will have predictive lexical and syntactic properties. Therefore, we use a syntactic language model. 7.1 Language Models We evaluated both a standard trigram language model and a syntactic language model (Filimonov and Harper, 2009a). The syntactic model estimates the joint probability of the word and its syntactic tag based on the preceding words and tags. The probability of an utterance w1n of length n is computed by summing over all latent syntactic tag assignments: (Allp(utt) = p(w1n ) = n X Y p(wi , ti |w1i−1 , ti−1 1 ) t1 ...tn i−1 We added these features to the second order CRF with BIO encoding and baseline features (Figure 3). As expected, the current words did not improve performance on unobserved OOVs. When the current words are combined with the lexical context and their lemmas, they give a significant boost"
N10-1025,D09-1087,0,0.0104768,"s well known for named entity recognition and part of speech tagging (Pereira et al., 1993). Other features such as sub-strings or baseline features (Word(1) where wi and ti are the word and tag at posiare sequences of words tion i, and w1i−1 and ti−1 1 and tags of length i − 1 starting a position 1. The model is restricted to a trigram context, i.e., i−1 i−1 p(wi , ti |wi−2 , ti−2 ); experiments that increased the order yielded no improvement. We trained the language model on 130 million words from Hub4 CSR 1996 (Garofolo et al., 1996). The corpus was parsed using a modified Berkeley parser (Huang and Harper, 2009) and tags extracted from parse trees incorporated the word’s POS, the label of its immediate parent, and the relative position of the word among its siblings. 8 The parser required separated contractions and possessives, but we recombined those words after parsing to match the LVCSR tokenization, merging their tags. Since we are considering OOV detection, the language model was restricted to LVCSR system’s vocabulary. 7 To obtain stemmed words, we use the CPAN package: http://search.cpan.org/~snowhare/Lingua-Stem-0.83. 221 8 The parent tagset of Filimonov and Harper (2009a). 60 80 MaxEnt (Base"
N10-1025,P05-1056,0,0.0294684,"Missing"
N10-1025,P93-1024,0,0.154447,"Missing"
N12-1007,P98-1012,0,0.662252,"tandard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may"
N12-1007,D08-1029,0,0.380528,", corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Name"
N12-1007,W10-4305,0,0.0546697,"Missing"
N12-1007,N10-2003,1,0.806423,"these two entities from other entities with similar references (e.g., “Steve Jones” or “Apple Corps”). As with the mention strings, the contexts may originate in different writing systems. We consider both highand low-resource approaches for mapping contexts to a common representation. 4 The Hungarian algorithm finds an optimal minimum-cost alignment. For pairwise costs between tokens, we used the Levenshtein edit distance Machine Translation (MT) For the high-resource setting, if lang(mi ) 6= English, then we translate both mi and its context si to English with an MT system. We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module. We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic). Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples— groups of topically-similar documents—share a topic distribution. The tuples need not be sentence-aligned, so training data is easier to obtain. For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve"
N12-1007,N01-1007,0,0.0412679,"orld, in which electronic media played a prominent role. A key issue for the outside world was the aggregation of information that appeared simultaneously in English, French, and various Arabic dialects. To our knowledge, we are the first to consider clustering entity mentions across languages without a priori knowledge of the quantity or types of real-world entities (a knowledge base). The cross-lingual setting introduces several challenges. First, we cannot assume a prototypical name format. For example, the Anglo-centric first/middle/last prototype used in previous name modeling work (cf. (Charniak, 2001)) does not apply to Arabic names like Abdullah ibn Abd Al-Aziz Al-Saud or Chinese names like Hu Jintao (referred to as Mr. Hu, not Mr. Jintao). Second, organization names often require both transliteration and translation. For example, the Arabic é»Qå  KñÓ È Qg Corp’ contains PPñ .  ‘General Motors  KñÓ È Qg. ‘General Motors’, transliterations of PPñ  but a translation of é»Qå ‘Corporation’. Our models are organized as a pipeline. First, for each document, we perform standard mention detection and coreference resolution. Then, we use pairwise cross-lingual similarity models to measure"
N12-1007,D07-1020,0,0.0851166,"o development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a com"
N12-1007,W99-0613,0,0.0802752,"Missing"
N12-1007,P10-1087,0,0.0685873,"Missing"
N12-1007,N09-1019,0,0.0412775,"Missing"
N12-1007,W04-0701,0,0.0255374,"n be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (200"
N12-1007,N04-1001,0,0.0272882,"1 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to oth"
N12-1007,D08-1089,1,0.805081,"xamples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation. We built a 5-gram language model from the Xinhua and AFP sections of the Gigaword corpus (LDC2007T07), in addition to all of the target side training data. In addition to the baseline Phrasal feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005"
N12-1007,N04-1002,0,0.0189204,"corpus into development and test sections. However, the usual method of splitting by document would not confine all mentions of each entity to one side of the split. We thus split the corpus by global entity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and"
N12-1007,P05-1071,0,0.0233673,"lley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008"
N12-1007,A00-1020,0,0.0578477,"ed the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 201"
N12-1007,2010.amta-papers.12,0,0.0351357,"apsed, token-based sampler, except the conditional probability p(Ea = E|E−a , Ca ) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei 64 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC (e.g., LDC2005T34 contains nearly 450,000 parallel Chinese-English names) or Wikipedia (Irvine et al., 2010). We extracted 12,860 name pairs from the parallel Arabic-English translation treebanks,9 although our experiments show that the model achieves high accuracy with significantly fewer training examples. We generated a uniform distribution of training examples by running a Bernoulli trial for each aligned name pair in the corpus. If the coin was heads, we replaced the English name with another English name chosen randomly from the corpus. MT Context Mapping For the MT context mapping method, we trained Phrasal with all data permitted under the NIST OpenMT Ar-En 2009 constrained track evaluation."
N12-1007,P03-1054,1,0.0146576,"l feature set, we used the lexicalized re-ordering model of Galley and Manning (2008). PLTM Context Mapping For PLTM training, we formed a corpus of 19,139 English-Arabic topicallyaligned Wikipedia articles. Cross-lingual links in Wikipedia are abundant: as of February 2010, there were 77.07M cross-lingual links among Wikipedia’s 272 language editions (de Melo and Weikum, 2010). To increase vocabulary coverage for our ACE2008 evaluation corpus, we added 20,000 document singletons from the ACE2008 training corpus. The 8 We tokenized all English documents with packages from the Stanford parser (Klein and Manning, 2003). For Arabic documents, we used Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task"
N12-1007,H05-1083,0,0.018586,"ier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work on similar tasks: • Multilingual coreference resolution: Adapt English within-document coreference models to other languages (Harabagiu and Maiorano, 2000; Florian et al., 2004; Luo and Zitouni, 2005). • Named entity translation: For a non-English document, produce an inventory of entities in English. An ACE2007 pilot task (Song and Strassel, 2008). • Named entity clustering: Assign semantic types to text mentions (Collins and Singer, 1999; Elsner et al., 2009). • Cross-language name search / entity linking: Match a single query name against a list of known multilingual names (knowledge base). A track in the 2011 NIST Text Analysis Conference (TAC-KBP) evaluation (Aktolga et al., 2008; McCarley, 2009; Udupa and Khapra, 2010; McNamee et al., 2011). Our work incorporates elements of the firs"
N12-1007,H05-1004,0,0.0231428,"NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ 3 (Cai and Strube, from the reference. We use Bsys 3 2010), a variant of B that was shown to penalize both twinle"
N12-1007,W07-0804,0,0.0158747,"n and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, Sayeed et al. (2009) translated Arabic name mentions to English and then formed clusters greedily using pairwise matching. To our knowledge, the cross-lingual entity clustering task is novel. However, there is significant prior work"
N12-1007,W03-0405,0,0.0160123,"tity id. We assigned one-third of the entities to development, and the remaining twothirds to test. 6 Comparison to Related Tasks and Work Our modeling techniques and task formulation can be viewed as cross-lingual extensions to cross-document coreference resolution. The classic work on this task was by Bagga and Baldwin (1998b), who adapted the Vector Space Model (VSM) (Salton et al., 1975). Gooi and Allan (2004) found effective algorithmic extensions like agglomerative clustering. Successful feature extensions to the VSM for cross-document coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other"
N12-1007,I11-1029,0,0.0779128,"Missing"
N12-1007,D09-1092,0,0.0592649,"n algorithm finds an optimal minimum-cost alignment. For pairwise costs between tokens, we used the Levenshtein edit distance Machine Translation (MT) For the high-resource setting, if lang(mi ) 6= English, then we translate both mi and its context si to English with an MT system. We use Phrasal (Cer et al., 2010), a phrase-based system which, like most public MT systems, lacks a transliteration module. We believe that this approach yields the most accurate context mapping for highresource language pairs (like English-Arabic). Polylingual Topic Model (PLTM) The polylingual topic model (PLTM) (Mimno et al., 2009) is a generative process in which document tuples— groups of topically-similar documents—share a topic distribution. The tuples need not be sentence-aligned, so training data is easier to obtain. For example, one document tuple might be the set of Wikipedia articles (in all languages) for Steve Jobs. Let D be a set of document tuples, where there is one document in each tuple for each of L languages. Each language has vocabulary Vl and each document dlt has Ntl tokens. We specify a fixed-size set of topics K. The PLTM generates the document tuples as follows: Polylingual Topic Model θt ∼ Dir(α"
N12-1007,C10-2121,1,0.8753,"t coreference have included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name u"
N12-1007,W09-1121,0,0.0206938,"s are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis and reference clusters. • CEAF (Luo, 2005): Precision and recall are computed from a maximum bipartite matching between hypothesis and reference clusters. • NVI (Reichart and Rappoport, 2009): Information-theoretic measure that utilizes the entropy of the clusters and their mutual information. Unlike the commonly-used Variation of Information (VI) metric, normalized VI (NVI) is not sensitive to the size of the data set. For the automatic setting, we must apply a different metric since the number of system chains may differ 3 (Cai and Strube, from the reference. We use Bsys 3 2010), a variant of B that was shown to penalize both twinless reference chains and spurious system chains more fairly. Evaluation Corpus The automatic evaluation of cross-lingual coreference systems requires"
N12-1007,P11-1080,0,0.0833628,"included biographical information (Mann and Yarowsky, 2003) and syntactic context (Chen and Martin, 2007). However, neither of these feature sets generalize easily to the cross-lingual setting with multiple entity types. Fleischman and Hovy (2004) added a discriminative pairwise mention classifier to a VSM-like model, much as we do. More 11 The annotators were the first author and another fluent speaker of Arabic. The annotations, corrections, and corpus split are available at http://www.spencegreen.com/research/. recent work has considered new models for web-scale corpora (Rao et al., 2010; Singh et al., 2011). Cross-document work on languages other than English is scarce. Wang (2005) used a combination of the VSM and heuristic feature selection strategies to cluster transliterated Chinese personal names. For Arabic, Magdy et al. (2007) started with the output of the mention detection and within-document coreference system of Florian et al. (2004). They clustered the entities incrementally using a binary classifier. Baron and Freedman (2008) used complete-link agglomerative clustering, where merging decisions were based on a variety of features such as document topic and name uniqueness. Finally, S"
N12-1007,strassel-etal-2008-linguistic,0,0.0300007,"ed Mada (Habash and Rambow, 2005) for orthographic normalization and clitic segmentation. 9 LDC Catalog numbers LDC2009E82 and LDC2009E88. topically-aligned tuples served as “glue” to share topics between languages, while the ACE documents distribute those topics over in-domain vocabulary.10 We used the PLTM implementation in Mallet (McCallum, 2002). We ran the sampler for 10,000 iterations and set the number of topics K = 512. 5 Task Evaluation Framework Our experimental design is a cross-lingual extension of the standard cross-document coreference resolution task, which appeared in ACE2008 (Strassel et al., 2008; NIST, 2008). We evaluate name (NAM) mentions for cross-lingual person (PER) and organization (ORG) entities. Neither the number nor the attributes of the entities are known (i.e., the task does not include a knowledge base). We report results for both gold and automatic within-document mention detection and coreference resolution. Evaluation Metrics We use entity-level evaluation metrics, i.e., we evaluate the E entity clusters rather than the mentions. For the gold setting, we report: • B 3 (Bagga and Baldwin, 1998a): Precision and recall are computed from the intersection of the hypothesis"
N12-1007,N10-1073,0,0.0571859,"Missing"
N12-1007,W09-0210,0,0.0132437,"let the data dictate the number of entity clusters. We thus consider a non-parametric Bayesian mixture model where the mixtures are multinomial distributions over the entity contexts S. Specifically, we consider a DPMM, which automatically infers the number of mixtures. Each Ca has an associated mixture θa : Ca |θa ∼ Mult(θa ) θa |G ∼ G G|α, G0 ∼ DP(α, G0 ) α ∼ Gamma(1, 1) where α is the concentration parameter of the DP prior and G0 is the base distribution with support V . For our experiments, we set G0 = Dir(π1 , . . . , πV ), where πi = PV (wi ). For inference, we use the Gibbs sampler of Vlachos et al. (2009), which can incorporate pairwise constraints. The sampler is identical to a standard collapsed, token-based sampler, except the conditional probability p(Ea = E|E−a , Ca ) = 0 if Ca cannot be merged with the chains in cluster E. This property makes the model non-exchangeable, but in practice non-exchangeable models are sometimes useful (Blei 64 Training Data and Procedures We trained our system for Arabic-English crosslingual entity clustering.8 Maxent Mention Similarity The Maxent mention similarity model requires a parallel name list for training. Name pair lists can be obtained from the LDC"
N12-1007,song-strassel-2008-entity,0,\N,Missing
N12-1007,C98-1012,0,\N,Missing
N12-1007,J98-4003,0,\N,Missing
N13-1017,W00-0405,0,0.0102334,"ors. In this paper we consider a setting where the user has prior knowledge about the end application: mining recreational drug trends from user forums, an important clinical research problem (§2). We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1). We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000). In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns. Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4). 168 Proceedings of NAACL-HLT 2013, pages 168–178, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Analyzing Drug Trends on the Web Recreational drug use imposes a significant burden on the health infrastructure"
N13-1017,N09-1041,0,0.0456298,"nds from user forums, an important clinical research problem (§2). We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1). We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000). In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns. Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4). 168 Proceedings of NAACL-HLT 2013, pages 168–178, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Analyzing Drug Trends on the Web Recreational drug use imposes a significant burden on the health infrastructure of the United States and other countries. Accurate information on drugs, usage profiles and side effects are necessary for supporting a ran"
N13-1017,E12-1021,0,0.035425,"Missing"
N13-1017,W04-1013,0,0.0191069,"y members in psychiatry and behavioral pharmacology, who have used drug forums in the past to study emerging drugs) to rate the snippets corresponding to mephedrone/MDPV. The best f-LDA system had an average score of 2.57 compared to a baseline score of 2.45 and random score of 1.63. 4.3.2 Automatic Evaluation of Recall The human judgments effectively measured a form of precision, as the quality of snippets were judged by their correspondence to the reference text, without regard to how much of the reference text was covered by all snippets. We also used the automatic evaluation metric ROUGE (Lin, 2004) as a rough estimate of summary recall: this metric computes the percentage of n-grams in the reference text that appeared in the generated summaries. We computed ROUGE for both 1-grams and 2grams. When computing n-gram counts, we applied Porter’s stemmer to all tokens. We excluded stop 176 words from 1-gram counts but included them in 2gram counts where we care about longer phrases.2 Results are shown in Table 2. We find that f-LDA1 has the highest score for both 1- and 2-grams, suggesting that it is extracting a more diverse set of relevant snippets. When performing a paired t-test across th"
N13-1017,D09-1026,0,0.0564078,"Missing"
N13-1080,P07-1056,1,0.766444,"t the domains induced by that metadata attribute. Each instance xi is drawn from a distribution xi ∼ Da specific to a set of attribute values Ai associated with each instance. Additionally, each unique set of attributes indexes a function fA .1 Ai could contain a value for each attribute, or no values for any attribute (which would index a domain-agnostic “background” distribution and labeling function). Just as a domain can change a feature’s probability and behavior, so can each metadata attribute. Examples of data for MAMD learning abound. The commonly used Amazon product reviews data set (Blitzer et al., 2007) only includes product types, but the original reviews can be attributed with author, product price, brand, and so on. Additional examples include congressional floor debate records (e.g. political party, speaker, bill) (Joshi et al., 2012). In this paper, we use restaurant reviews (Chahuneau et al., 2012), which have upto 20 metadata attributes that define domains, and congressional floor debates, with two attributes that define domains. It is difficult to apply multi-domain learning algorithms when it is unclear which metadata attribute to choose for defining the “domains”. It is possible th"
N13-1080,D12-1124,0,0.333936,"metadata attributes). We introduce the multi-attribute multi-domain (MAMD) learning problem, in which each learning instance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Geor"
N13-1080,P07-1033,0,0.690089,"Missing"
N13-1080,D08-1072,1,0.890708,"-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain le"
N13-1080,N09-1068,0,0.194056,"multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single “best” attribute. 1 Introduction Multi-Domain Learning (Evgeniou and Pontil, 2004; Daum´e III, 2007; Dredze and Crammer, 2008; Finkel and Manning, 2009; Zhang and Yeung, 2010; Saha et al., 2011) algorithms learn when training instances are spread across many domains, which impact model parameters. These algorithms use examples from each domain to learn a general model that is also sensitive to individual domain differences. However, many data sets include a host of metadata attributes, many of which can potentially define the domains to use. Consider the case of restaurant reviews, which can be categorized into domains corresponding to the cuisine, location, price range, or several other factors. For multi-domain learning, we should use the"
N13-1080,D12-1119,1,0.876767,"Missing"
N13-1080,W06-1639,0,0.150433,"ance is associated with multiple metadata attributes, each of which may impact feature behavior. We present extensions to two popular multi-domain learning algorithms, FEDA (Daum´e III, 2007) and MDR (Dredze et al., 2009). Rather than selecting a single domain division, our algorithms consider all attributes as possible distinctions and discover changes in features across attributes. We evaluate our algorithms using two different data sets – a data set of restaurant reviews (Chahuneau et al., 2012), and a dataset of transcribed speech segments from floor debates in the United States Congress (Thomas et al., 2006). We demonstrate that multi-attribute algorithms improve over their multi-domain counterparts, which can learn distinctions from only a single attribute. 2 MAMD Learning In multi-domain learning, each instance x is drawn from a domain d with distribution x ∼ Dd over a vectors space RD and labeled with a domain specific function fd with label y ∈ {−1, +1} (for binary classification). In multi-attribute multi-domain 685 Proceedings of NAACL-HLT 2013, pages 685–690, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics (MAMD) learning, we have M metadata attributes in"
N13-1080,P12-1078,0,0.0137892,"is possible that there is a single “best” attribute to use for defining domains, one that when used in multi-domain learning will yield the best classifier. To find this attribute, one must rely on one’s intuition about the problem,2 or perform an exhaustive empirical search over all attributes using some validation set. Both these strategies can be brittle, because as the nature of data changes over time so may the “best” domain distinction. Additionally, multi-domain learning was not designed to benefit from multiple helpful attributes. We note here that Eisenstein et al. (2011), as well as Wang et al. (2012), worked with a “multifaceted topic model” using the framework of sparse additive generative models (SAGE). Both those models capture interactions between topics and multiple as1 Distributions and functions that share attributes could share parameters. 2 Intuition is often critical for learning and in some cases can help, such as in the Amazon product reviews data set, where product type clearly corresponds to domain. However, for other data sets the choice may be less clear. 686 pects, and can be adapted to the case of MAMD. While our problem formulation has significant conceptual overlap wit"
N13-1097,N12-1033,0,0.0434613,"Missing"
N13-1097,W10-0701,1,0.308445,"Missing"
N13-1097,W09-3012,0,0.0300103,"Missing"
N13-1097,D11-1145,0,0.678908,", and general sentiment (Bollen et al., 2011), studying linguistic variation (Eisenstein et al., 2010) and detecting earthquakes (Sakaki et al., 2010). Similarly, Twitter has proven useful for public health applications (Dredze, 2012), primarily disease surveillance (Collier, 2012; Signorini et al., 2011), whereby public health officials track infection rates of common diseases. Standard government data sources take weeks while Twitter provides an immediate population measure. Strategies for Twitter influenza surveillance include supervised classification (Culotta, 2010b; Culotta, 2010a; Eiji Aramaki and Morita, 2011), unsupervised models for disease discovery (Paul and Dredze, 2011), keyword counting1 , tracking geographic illness propagation (Sadilek et al., 2012b), and combining tweet contents with the social network (Sadilek et al., 2012a) and location informa1 The DHHS competition relied solely on keyword counting. http://www.nowtrendingchallenge.com/ Both are related to the flu and express worry, but tell a different story. The first reports an infection of another person, while the second expresses the author’s concerned awareness. While infection tweets indicate a rise in infection rate, awareness"
N13-1097,D10-1124,0,0.0451635,"Missing"
N13-1097,P11-2008,0,0.047861,"Missing"
N13-1097,W12-3807,0,0.0145785,"Missing"
N13-1097,N12-1057,0,0.0117195,"edings of NAACL-HLT 2013, pages 789–795, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics flu but do not report an infection can corrupt infection tracking. Concerned Awareness vs. Infection (A/I) Many flu tweets express a concerned awareness as opposed to infection, including fear of getting the flu, an awareness of increased infections, beliefs related to flu infection, and preventative flu measures (e.g. flu shots.) Critically, these people do not seem to have the flu, whereas infection tweets report having the flu. This distinction is similar to modality (Prabhakaran et al., 2012a). Conflating these tweets can hurt surveillance, as around half of our annotated flu messages were awareness. Identifying awareness tweets may be of use in-and-of itself, such as for characterizing fear of illness (Epstein et al., 2008; Epstein, 2009), public perception, and discerning sentiment (e.g. flu is negative, flu shots may be positive.) We focus on surveillance improvements.2 Self vs. Other (S/O) Tweets for both awareness and infection can describe the author (self) or others. It may be that self infection reporting is more informative. We test this hypothesis by classifying tweets"
N13-1121,W12-2108,1,0.801101,"Our readily-replicable approach and publiclyreleased clusters are shown to be remarkably effective and versatile, substantially outperforming state-of-the-art approaches and human accuracy on each of the tasks studied. 1 Here, we propose and evaluate classifiers that better exploit the attributes that users explicitly provide in their user profiles, such as names (e.g., first names like Mary, last names like Smith) and locations (e.g., Brasil). Such attributes have previously been used as “profile features” in supervised user classifiers (Pennacchiotti and Popescu, 2011; Burger et al., 2011; Bergsma et al., 2012). There are several motivations for exploiting these data. Often the only information available for a user is a name or location (e.g. for a new user account). Profiles also provide an orthogonal or complementary source of information to a user’s social network and textual content; gains based on profiles alone should therefore add to gains based on other data. The decisions of profile-based classifiers could also be used to bootstrap training data for other classifiers that use complementary features. Introduction There is growing interest in automatically classifying users in social media by"
N13-1121,N10-1102,0,0.062149,"ntry: 53 possible countries United States courtland dante United States tinas twin Brazil thamires gomez Denmark marte clason Lang. ID: 9 confusable languages Bulgarian valentina getova Russian borisenko yana Bulgarian NONE Ukrainian andriy kupyna Farsi kambiz barahouei Urdu musadiq sanwal Ethnicity: 13 European ethnicities German dennis hustadt Dutch bernhard hofstede French david coste Swedish mattias bjarsmyr Portuguese helder costa Race: black or white black kerry swain black darrell foskey white ty j larocca black james n jones white sean p farrell of-the-art in detecting name ethnicity (Bhargava and Kondrak, 2010). We add special begin/end characters to the attributes to mark the prefix and suffix positions. We also use a smoothed log-count; we found this to be most effective in preliminary work. Cluster Features (Clus) indicate the soft-cluster memberships of the attributes. We have features for the top-2, 5, and 20 most similar clusters in the C 50 , C 200 , and C 1000 clusterings, respectively. Like Lin and Wu (2009), we “side-step the matter of choosing the optimal value k in k-means” by using features from clusterings at different granularities. Our feature dimensions correspond to cluster IDs; fe"
N13-1121,J92-4003,0,0.280084,"Missing"
N13-1121,D11-1120,0,0.727262,"Bergsma, Mark Dredze, Benjamin Van Durme, Theresa Wilson, David Yarowsky Department of Computer Science and Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD 21218, USA shane.a.bergsma@gmail.com, mdredze@cs.jhu.edu, vandurme@cs.jhu.edu, taw@jhu.edu, yarowsky@cs.jhu.edu Abstract and Dredze, 2011), and sociolinguistic phenomena (Eisenstein et al., 2011). Classifiers for user properties often rely on information from a user’s social network (Jernigan and Mistree, 2009; Sadilek et al., 2012) or the textual content they generate (Pennacchiotti and Popescu, 2011; Burger et al., 2011). Hidden properties of social media users, such as their ethnicity, gender, and location, are often reflected in their observed attributes, such as their first and last names. Furthermore, users who communicate with each other often have similar hidden properties. We propose an algorithm that exploits these insights to cluster the observed attributes of hundreds of millions of Twitter users. Attributes such as user names are grouped together if users with those names communicate with other similar users. We separately cluster millions of unique first names, last names, and userprovided locatio"
N13-1121,J90-1003,0,0.0521202,"rm as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique string (target location), corresponding to a structured location object that incl"
N13-1121,D10-1124,0,0.208983,"Missing"
N13-1121,P11-1137,0,0.267211,"Missing"
N13-1121,P09-1080,1,0.806767,"ies, and (2) to predict user properties based on friendships. Friendship prediction systems (e.g. Facebook’s friend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al."
N13-1121,P90-1034,0,0.466102,"75 70 65 60 10 100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if fo"
N13-1121,W10-1908,0,0.0181357,"iend suggestion tool) use features such as whether both people are computer science majors (Taskar et al., 2003) or whether both are at the same location (Crandall et al., 2010; Sadilek et al., 2012). The inverse problem has been explored in the prediction of a user’s location given the location of their peers (Backstrom et al., 2010; Cho et al., 2011; Sadilek et al., 2012). Jernigan and Mistree (2009) predict a user’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are"
N13-1121,P08-1068,0,0.0236334,"ased on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes"
N13-1121,P09-1116,0,0.499477,"s). We consider each useruser link as a single event; we count it once no matter how often two specific users interact. We extract 436M user-user links in total. Attribute-Attribute Pairs We use our profile data to map each user-user link to an attribute-attribute pair; we separately count each pair of first names, last names, and locations. For example, the firstname pair (henrik, fredrik) occurs 181 times. Rather than using the raw count, we calculate the association between attributes a1 and a2 via their pointwise mutual information (PMI), following prior work in distributional clustering (Lin and Wu, 2009): PMI(a1 , a2 ) = log P(a1 , a2 ) P(a1 )P(a2 ) PMI essentially normalizes the co-occurrence by what we would expect if the attributes were independently distributed. We smooth the PMI by adding a count of 0.5 to all co-occurrence events. The most highly-associated name attributes reflect similarities in ethnicity and gender (Table 2). The most highly-ranked associates for locations are often nicknames and alternate/misspellings of those locations. For example, the locations charm city, bmore, balto, westbaltimore, b a l t i m o r e, baltimoreee, and balitmore each have the U.S. city of baltimo"
N13-1121,P98-2127,0,0.151694,"of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to a unique str"
N13-1121,N04-1043,0,0.0365937,"er’s sexuality based on the sexuality of their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explos"
N13-1121,D10-1021,0,0.0278599,"or distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter communication patterns. We clustered user attributes based on both the communication of users with those attributes as well as substring similarity. Systems using our clusters significantly outperform state-of-the-art algorithms on each of the tasks investigated, and exceed human performance on each task as well. The power and versatility of our clusters is exemplified by the fac"
N13-1121,P93-1024,0,0.430574,"100 1000 10000 Number of training examples Figure 1: Learning curve on Race: Clus perform as well with 30 training examples as Ngm features do with 1000. data; thousands of training examples are needed for Ngm to rival the performance of Clus using only a handful. Since labeled data is generally expensive to obtain or in short supply, our method for exploiting unlabeled Twitter data can both save money and improve top-end performance. 7 Geolocation by Association There is a tradition in computational linguistics of grouping words both by the similarity of their context vectors (Hindle, 1990; Pereira et al., 1993; Lin, 1998) and directly by their statistical association in text (Church and Hanks, 1990; Brown et al., 1992). While the previous sections explored clusters built by vector similarity, we now explore a direct application of our attribute association data (§2). We wish to use this data to improve an existing Twitter geolocation system based on user profile locations. The system operates as follows: 1) normalize user-provided locations using a set of regular expressions (e.g. remove extra spacing, punctuation); 2) look up the normalized location in an alias list; 3) if found, map the alias to"
N13-1121,C10-2112,0,0.0212861,"luding namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network, a large amount of work has focused on inferring user properties based on the content they generate (e.g. Burger and Henderson (2006), Schler et al. (2006), Rao et al. (2010), Mukherjee and Liu (2010), Pennacchiotti and Popescu (2011), Burger et al. (2011), Van Durme (2012)). 9 Conclusion and Future Work We presented a highly effective and readily replicable algorithm for generating language resources from Twitter comm"
N13-1121,W09-1119,0,0.00772601,"f their Facebook friends, while Garera and Yarowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting u"
N13-1121,D12-1137,0,0.0205995,"ts, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is an important prerequisi"
N13-1121,N12-1052,0,0.0235863,"of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside from the work mentioned above that analyzes a user’s social network,"
N13-1121,P10-1040,0,0.0155684,"rowsky (2009) predict a user’s gender partly based on the gender of their conversational partner. Jha and Elhadad (2010) predict the cancer stage of users of an online cancer discussion board; they derive complementary information for prediction from both the text a user generates and the cancer stage of the people that a user interacts with. The idea of clustering data in order to provide features for supervised systems has been successfully explored in a range of NLP tasks, including namedentity-recognition (Miller et al., 2004; Lin and Wu, 2009; Ratinov and Roth, 2009), syntactic chunking (Turian et al., 2010), and dependency parsing (Koo et al., 2008; Täckström et al., 2012). In each case, the clusters are derived from the distribution of the words or phrases in text, not from their communication pattern. It would be interesting to see whether prior distributional clusters can be combined with our communication-based clusters to achieve even better performance. Indeed, there is evidence that features derived from text can improve the prediction of name ethnicity (Pervouchine et al., 2010). There has been an explosion of work in recent years in predicting user properties in social networks. Aside f"
N13-1121,D12-1005,1,0.555299,"Missing"
N13-1121,P11-1096,0,0.0180694,"s from each of the test sets, and we average their results to give a “Human” performance number. The two humans are experts in 1014 cali baby on the court macapá ap NONE NONE edinburgh blagoevgrad ternopil NONE jammu Table 5: Examples of class (left) and input (names, locations) for some of our evaluation tasks. this domain and have very wide knowledge of global names and locations. 5.2 Twitter Applications Country A number of recent papers have considered the task of predicting the geolocation of users, using both user content (Cheng et al., 2010; Eisenstein et al., 2010; Hecht et al., 2011; Wing and Baldridge, 2011; Roller et al., 2012) and social network (Backstrom et al., 2010; Sadilek et al., 2012). Here, we first predict user location at the level of the user’s location country. To our knowledge, we are the first to exploit user locations and names for this prediction. For this task, we obtain gold data from the portion of Twitter users who have GPS enabled (geocoded tweets). We were able to obtain a very large number of gold instances for this task, so selected only 10K for testing, 10K for development, and retained the remaining 782K for training. Language ID Identifying the language of users is a"
N13-1121,C98-2122,0,\N,Missing
N13-3002,D11-1021,0,0.0691843,"Missing"
N15-1002,D12-1032,1,0.826461,"s we add joint factors that capture effects between alignment variables. Each joint factor φ is comprised of a constrained binary variable zφ associated with features f (φ) that indicates when the factor is active. Together with parameters w these form additional scores sφ for the objective: sφ = w · f (φ) (2) which doesn’t use global inference.2 These features are built on top of a variety of semantic resources (PPDB (Ganitkevitch et al., 2013), WordNet (Miller, 1995), FrameNet (Baker et al., 1998)) and methods for comparing mentions (tree edit distance (Yao et al., 2013), string transducer (Andrews et al., 2012)). 4 Joint Factors Our goal is to develop joint factors that improve over the feature rich local factors baseline by considering global information. Fertility A common mistake when making independent classification decisions is to align many source items to a single target item. While each link looks promising on its own, they clearly cannot all be right. Empirically, the training set reveals that many to one alignments are uncommon; thus many to one predictions are likely errors. We add a fertility factor for predicates and arguments, where fertility is defined as the number of links to an it"
N15-1002,P98-1013,0,0.166656,"ing items i and j. Using only local features, our system would greedily select alignments. To capture global aspects we add joint factors that capture effects between alignment variables. Each joint factor φ is comprised of a constrained binary variable zφ associated with features f (φ) that indicates when the factor is active. Together with parameters w these form additional scores sφ for the objective: sφ = w · f (φ) (2) which doesn’t use global inference.2 These features are built on top of a variety of semantic resources (PPDB (Ganitkevitch et al., 2013), WordNet (Miller, 1995), FrameNet (Baker et al., 1998)) and methods for comparing mentions (tree edit distance (Yao et al., 2013), string transducer (Andrews et al., 2012)). 4 Joint Factors Our goal is to develop joint factors that improve over the feature rich local factors baseline by considering global information. Fertility A common mistake when making independent classification decisions is to align many source items to a single target item. While each link looks promising on its own, they clearly cannot all be right. Empirically, the training set reveals that many to one alignments are uncommon; thus many to one predictions are likely error"
N15-1002,P10-1143,0,0.0998996,"e this extra parameter is worth allocating a portion of training data to enable tuning. Tuning τ addresses the same problem as using an asymmetric Hamming loss, but we found that doing both led to better results.4 Since we are using a global scoring function rather than a set of classifications, τ is implemented as a test-time unary factor on every alignment. 6 Experiments Data We consider two datasets for evaluation. The first is a cross-document entity and event coreference resolution dataset called the Extended Event Coref Bank (EECB) created by Lee et al. (2012) and based on a corpus from Bejan and Harabagiu (2010). The dataset contains clusters of news articles taken from Google News with annotations about coreference over entities and events. Following the procedure of Wolfe et al. (2013), we select the first document in every cluster and pair it with every other document in the cluster. The second dataset (RF) comes from Roth and Frank (2012). The dataset contains pairs of news articles that describe the same news story, and are annotated for predicate links between the document pairs. Due to the lack of annotated arguments, we can only report predicate linking performance and the psa and asp factors"
N15-1002,H93-1039,0,0.46461,"Missing"
N15-1002,Q14-1022,0,0.109653,"s(pi ) × args(pj ) or preds(ai ) × preds(aj ) respectively. Temporal Information Temporal ordering, in contrast to textual ordering, can indicate when predicates cannot align: we expect aligned predicates in both documents to share the same temporal relations. SemEval 2013 included a task on predicting temporal relations between events (UzZaman et al., 2013). Many systems produced partial relations of events in a document based on lexical aspect and tense, as well as discourse connectives like “during” or “after”. We obtain temporal relations with CAEVO, a state-of-the-art sieve-based system (Chambers et al., 2014). TimeML (Pustejovsky et al., 2003), the format for specifying temporal relations, defines relations between predicates (e.g. immediately before and simultaneous), each with an inverse (e.g. immediately after and simultaneous respectively). We will refer to a relation as R and its inverse as R−1 . Suppose we had pa and pb in the source document, px and py in the target document, and pa R1 pb , px R2 py . Given this configuration the following alignments conflict with the in-doc relations: zax zby zay zbx In-Doc Relations * * 1 1 R1 = R2 1 * * R1 = R2−1 1 where 1 means there is a link and * mea"
N15-1002,J08-4005,0,0.0321391,"Frank (2012) and Wolfe et al. (2013) we include a Lemma baseline for identifying alignments which will align any two predicates or arguments that have the same lemmatized head word.6 The Local baseline uses the same features as Wolfe et al., but none of our joint factors. In addition to running our joint model with all factors, we measure the efficacy of each individual factor by evaluating each with the local features. For evaluation we use a generous version of F1 that is defined for alignment labels composed of sure, Gs , and possible links, Gp and the system’s proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)). P = |H ∩ Gp | |H| R= |H ∩ Gs | 2P R F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is"
N15-1002,N13-1092,1,0.866479,"Missing"
N15-1002,P13-2139,0,0.044495,"Missing"
N15-1002,W04-3250,0,0.0217182,"composed of sure, Gs , and possible links, Gp and the system’s proposed links H (following Cohn et al. (2008), Roth and Frank (2012) and Wolfe et al. (2013)). P = |H ∩ Gp | |H| R= |H ∩ Gs | 2P R F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is obviously sensitive to the lemmatizer used. We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be. 6 17 7 Results Results for EECB and RF are reported in Table 7. As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2"
N15-1002,N06-1015,0,0.434729,"e result may be links that conflict in their interpretation of the document. Figure 1 makes clear that jointly considering all links at once can aid individual decisions, for example, by including temporal ordering of predicates. The global nature of this task is similar to word alignment for machine translation (MT). Many systems consider alignment links between words individually, selecting the best link for each word independently of the other words in the sentence. Just as with an independent linking strategy in predicate argument alignment, this can lead to inconsistencies in the output. Lacoste-Julien et al. (2006) introduced a model that jointly resolved word alignments based on the introduction of quadratic variables, factors that depend on two alignment decisions which characterize patterns that span word-word links. Their approach achieved improved results even in the presence of little training data. 12 We present a global predicate argument alignment model based on considering quadratic interactions between alignment variables to captures patterns we expect in coherent discourse. We introduce factors which are comprised of a binary variable, multiple quadratic constraints on that variable, and fea"
N15-1002,D12-1045,0,0.297996,"e: viewing a multi-document task as one limited to individual pairs of sentences. This creates a mis-match between the goals of such work – considering entire documents – with the systems – consider individual sentences. In this work, we consider a system that takes a document level view in considering coreference for entities and predictions: the task of predicate argument linking. We treat this task as a global inference problem, leveraging multiple sources of semantic information identified at the document level. Global inference for this problem is mostly unexplored, with the exception of Lee et al. (2012) (discussed in § 8). Especially novel here is the use of document-level temporal constraints on events, representing a next step forward on the path to full understanding. Our approach avoids the pitfalls of local inference while still remaining fast and exact. We use the pairwise features of a very strong predicate argument aligner (Wolfe et al., 2013) (competitive with the state-of-the-art (Roth, 2014)), and add quadratic factors that constrain local decisions based on global document information. These global factors lead to superior performance compared to the previous state-of-the-art. We"
N15-1002,P06-1095,0,0.0397977,"eference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents. We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014). Our results indicate this is a useful source of information, one of the first results to show an improvement from this 7 A lexically disparate description of an entity. ˇ type of system (Glavaˇs and Snajder, 2013). We utilize an ILP to improve upon a pipelined system, similar to Roth and Yih (2004), but our work differs in that we do not use piecewise-trained classifiers. Our local similarity scores are calibrated according to a global objective by propagating the gradient back from the loss to every parameter in the model. When using piecewise training, local classif"
N15-1002,P14-5010,0,0.00383754,"F = |Gs | P +R Note that the EECB data does not have a sure and possible distinction, so Gs = Gp , resulting in standard F1. In addition to F1, we separately measure predicate and argument F1 to demonstrate where our model makes the largest improvements. We performed a one-sided paired-bootstrap test where the null hypothesis was that the joint model was no better than the Local baseline (described in Koehn (2004)). Cases where p < 0.05 are bolded. 5 https://github.com/cnap/anno-pipeline The lemma baseline is obviously sensitive to the lemmatizer used. We used the Stanford CoreNLP lemmatizer (Manning et al., 2014) and found it yielded slightly better results than previously reported as the lemma baseline (Roth and Frank, 2012), so we used it for all systems to ensure fairness and that the baseline is as strong as it could be. 6 17 7 Results Results for EECB and RF are reported in Table 7. As previously reported, using just local factors (features on pairs) improves over lemma baselines (Wolfe et al., 2013). The joint factors make statistically significant gains over local factors in almost all experiments. Fertility factors provide the largest improvements from any single constraint. A fertility penalt"
N15-1002,W12-3018,1,0.860036,"Missing"
N15-1002,N13-1110,0,0.0297615,"s are bolded. sort employed in RTE challenges. Lee et al. (2012) considered a similar problem but sought to produce clusters of entities and events rather than an alignment between two documents with the goal of improving coreference resolution. They used features which consider previous event and entity coreference decisions to make future coreference decisions in a greedy manner. This differs from our model which is built on non-greedy joint inference, but much of the signal indicating when two mentions corefer or are aligned is similar. In the context of in-document coreference resolution, Recasens et al. (2013) sought to overcome the problem of opaque mentions7 by finding highprecision paraphrases of entities by pivoting off verbs mentioned in similar documents. We address the issue of opaque mentions not by building a paraphrase table, but by jointly reasoning about entities that participate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014)."
N15-1002,S12-1030,0,0.164929,"entences from the document pair shown in Figure 1. These sentences describe the same event, although with different details. The source sentence has four predicates and four arguments, while the target has three predicates and three arguments. In this case, one of the predicates from each sentence aligns, as do three of the arguments. We also show additional information potentially helpful to determining alignments: temporal relations between the predicates. The goal of predicate argument alignment is to assign these links indicating coreferent predicates and arguments across a document pair (Roth and Frank, 2012). Previous work by Wolfe et al. (2013) formulated 1 https://github.com/hltcoe/parma2 11 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 11–20, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Figure 1: An example analysis and predicate argument alignment task between a source and target document. Predicates appear as hollow ovals, have blue mentions, and are aligned considering their arguments (dashed lines). Arguments, in black diamonds with green mentions, represent a document-level entity (coref"
N15-1002,W04-2401,0,0.0604425,"icipate in coreferent events (c.f. §4); the approaches are complementary. In this work we incorporate ordering information of events. Though we consider it an upstream task, there is a line of work trying to predict temporal relations between events (Pustejovsky et al., 2003; Mani et al., 2006; Chambers et al., 2014). Our results indicate this is a useful source of information, one of the first results to show an improvement from this 7 A lexically disparate description of an entity. ˇ type of system (Glavaˇs and Snajder, 2013). We utilize an ILP to improve upon a pipelined system, similar to Roth and Yih (2004), but our work differs in that we do not use piecewise-trained classifiers. Our local similarity scores are calibrated according to a global objective by propagating the gradient back from the loss to every parameter in the model. When using piecewise training, local classifiers must focus more on recall (in the spirit of Weiss and Taskar (2010)) than they would for an ordinary classification task with no global objective. Our method trains classifiers jointly with a global convex objective. While our training procedure requires decoding an integer program, the parameters we learn are globally"
N15-1002,S13-2001,0,0.0188965,"te: p a 1 − max zkl (7) zφasp ≥ zij k∈preds(ai ) l∈preds(aj ) where preds(ai ) finds the indices of all predicates that govern any mention of argument ai . The features f (φ) for both psa and asp are an intercept feature and a bucketed count of the size of args(pi ) × args(pj ) or preds(ai ) × preds(aj ) respectively. Temporal Information Temporal ordering, in contrast to textual ordering, can indicate when predicates cannot align: we expect aligned predicates in both documents to share the same temporal relations. SemEval 2013 included a task on predicting temporal relations between events (UzZaman et al., 2013). Many systems produced partial relations of events in a document based on lexical aspect and tense, as well as discourse connectives like “during” or “after”. We obtain temporal relations with CAEVO, a state-of-the-art sieve-based system (Chambers et al., 2014). TimeML (Pustejovsky et al., 2003), the format for specifying temporal relations, defines relations between predicates (e.g. immediately before and simultaneous), each with an inverse (e.g. immediately after and simultaneous respectively). We will refer to a relation as R and its inverse as R−1 . Suppose we had pa and pb in the source"
N15-1002,P13-2012,1,0.801492,"Missing"
N15-1002,N13-1106,1,0.844869,"Missing"
N15-1002,C98-1013,0,\N,Missing
N15-1024,W10-0701,1,0.878663,"Missing"
N15-1024,C12-1028,0,0.0227365,"igate these errors and evaluate the impact of ASR errors on entity linking using a new corpus of entity linked broadcast news transcripts. 1 Introduction Entity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL). Numerous studies have explored entity linking in a wide range of domains, including newswire (Milne and Witten, 2008; Mcnamee et al., 2009; McNamee and Dang, 2009; Dredze et al., 2010), blog posts (Ji et al., 2010), web pages (Demartini et al., 2012; Lin et al., 2012), social media (Cassidy et al., 2012; Guo et al., 2013a; Shen et al., 2013; Liu et al., 2013), email (Gao et al., 2014) and multi-lingual documents (Mayfield et al., 2011; McNamee et al., 2011; Wang et al., 2012). A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities. However, all of these studies have focused on written language, while linking of spoken language remains untested. Yet many intended applications of entity linking, such as supportin"
N15-1024,C10-1032,1,0.872236,"anscription errors can distort the context, and named entities tend to have high error rates. We propose features to mitigate these errors and evaluate the impact of ASR errors on entity linking using a new corpus of entity linked broadcast news transcripts. 1 Introduction Entity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL). Numerous studies have explored entity linking in a wide range of domains, including newswire (Milne and Witten, 2008; Mcnamee et al., 2009; McNamee and Dang, 2009; Dredze et al., 2010), blog posts (Ji et al., 2010), web pages (Demartini et al., 2012; Lin et al., 2012), social media (Cassidy et al., 2012; Guo et al., 2013a; Shen et al., 2013; Liu et al., 2013), email (Gao et al., 2014) and multi-lingual documents (Mayfield et al., 2011; McNamee et al., 2011; Wang et al., 2012). A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities. However, all of these studies have focused on written language"
N15-1024,N13-1122,0,0.332242,"evaluate the impact of ASR errors on entity linking using a new corpus of entity linked broadcast news transcripts. 1 Introduction Entity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL). Numerous studies have explored entity linking in a wide range of domains, including newswire (Milne and Witten, 2008; Mcnamee et al., 2009; McNamee and Dang, 2009; Dredze et al., 2010), blog posts (Ji et al., 2010), web pages (Demartini et al., 2012; Lin et al., 2012), social media (Cassidy et al., 2012; Guo et al., 2013a; Shen et al., 2013; Liu et al., 2013), email (Gao et al., 2014) and multi-lingual documents (Mayfield et al., 2011; McNamee et al., 2011; Wang et al., 2012). A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities. However, all of these studies have focused on written language, while linking of spoken language remains untested. Yet many intended applications of entity linking, such as supporting search (Hachey e"
N15-1024,W12-3016,0,0.228279,"Missing"
N15-1024,P13-1128,0,0.101111,"entity linking using a new corpus of entity linked broadcast news transcripts. 1 Introduction Entity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL). Numerous studies have explored entity linking in a wide range of domains, including newswire (Milne and Witten, 2008; Mcnamee et al., 2009; McNamee and Dang, 2009; Dredze et al., 2010), blog posts (Ji et al., 2010), web pages (Demartini et al., 2012; Lin et al., 2012), social media (Cassidy et al., 2012; Guo et al., 2013a; Shen et al., 2013; Liu et al., 2013), email (Gao et al., 2014) and multi-lingual documents (Mayfield et al., 2011; McNamee et al., 2011; Wang et al., 2012). A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities. However, all of these studies have focused on written language, while linking of spoken language remains untested. Yet many intended applications of entity linking, such as supporting search (Hachey et al., 2013) and identifying relevant s"
N15-1024,I11-1029,0,0.026961,"tity linking identifies for each textual mention of an entity a corresponding entry contained in a knowledge base, or indicates when no such entry exits (NIL). Numerous studies have explored entity linking in a wide range of domains, including newswire (Milne and Witten, 2008; Mcnamee et al., 2009; McNamee and Dang, 2009; Dredze et al., 2010), blog posts (Ji et al., 2010), web pages (Demartini et al., 2012; Lin et al., 2012), social media (Cassidy et al., 2012; Guo et al., 2013a; Shen et al., 2013; Liu et al., 2013), email (Gao et al., 2014) and multi-lingual documents (Mayfield et al., 2011; McNamee et al., 2011; Wang et al., 2012). A common theme across all these settings requires addressing two difficulties in linking decisions: matching the textual name mention to the form contained in the knowledge base, and using contextual clues to disambiguate similar entities. However, all of these studies have focused on written language, while linking of spoken language remains untested. Yet many intended applications of entity linking, such as supporting search (Hachey et al., 2013) and identifying relevant sources for reports (He et al., 2010; He et al., 2011), linking of spoken language is critical. Sear"
N15-1024,H92-1073,0,0.525791,"Missing"
N15-1024,W03-0419,0,0.20775,"Missing"
N15-1024,I13-1041,0,\N,Missing
N15-1155,D14-1067,0,0.0267419,"Missing"
N15-1155,D14-1082,0,0.369288,"a large number of training examples. For smaller training sets, the variance of their estimator will be high resulting in increased ~ ( to0)use ✓~ ⇡ 0 We seek generalization error on test data. -.5 .3 .8 .7 1 0 1 0 0 1 many more features (based on rich annotations such as syntactic parsing and NER) and larger label sets, which further exacerbates the problem of overfitting. We propose a new method of learning interactions between engineered features and word embeddings by combining the idea of the outer product in FCM (Yu et al., 2014) with learning feature embeddings (Collobert et al., 2011; Chen and Manning, 2014).2 Our model jointly learns feature embeddings and a tensor-based classifier which relies on the outer product between features embeddings and word embeddings. Therefore, the number of parameters are dramatically reduced since features are only represented as low-dimensional embeddings, which alleviates problems with overfitting. The resulting model benefits from both approaches: conjunctions between feature and word embeddings allow model 2 2 Collobert et al. (2011) and Chen and Manning (2014) also capture interactions between word embeddings and features by using deep convolutional networks"
N15-1155,W06-1670,0,0.0304999,"uding the “social” relation. The reason is that the asymmetric subtype, “social.role”, dominates the class: 679 of 834 total “social” relations. Setup We randomly initialize the feature embeddings Wf and pre-train 200-dimensional word embeddings on the NYT portion of Gigaword 5.0 (Parker et al., 2011) with word2vec (default setting of the toolkit) (Mikolov et al., 2013). Dependency parses are obtained from the Stanford Parser (De Marneffe et al., 2006). We use the same feature templates as Yu et al. (2014). When gold entity types are unavailable, we replace them with WordNet tags annotated by Ciaramita and Altun (2006). Learning rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings d are tuned on dev sets. We selected d from {12, 15, 20, 25, 30, 40}. We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise. For ERE, we have d=15. The weights of L2 λ was selected from {1e3, 5e-4, 1e-4}. As in prior work (Yu et al., 2014), regularization did not significantly help FCM. However for LRFCM, λ=1e-4 slightly helps. We use a learning rate of 0.05. We compare to two baselines. First, we use the features of Sun et al. (2011), who b"
N15-1155,de-marneffe-etal-2006-generating,0,0.0575853,"Missing"
N15-1155,P14-1129,0,0.0788578,"Missing"
N15-1155,P13-1088,0,0.0368571,"ure 1, “driving” is a strong indicator of the “ART” (ACE) relation because it appears on the dependency path between a person and a vehicle. Yet such conjunctions of different syntactic/semantic annotations (dependency and NER) are typically not available in compositional models. In contrast, hand-crafted features can easily capture this information, e.g. feature fi3 (Figure 1). Therefore, engineered features should be combined with learned representations in compositional models. One approach is to use the features to select specific transformations for a sub-structure (Socher et al., 2013a; Hermann and Blunsom, 2013; Hermann et al., 2014; Roth and Woodsend, 2014), which can conjoin features and word embeddings, but is impractical as the numbers of transformations will exponentially increase with additional features. Typically, less than 10 features are used. A solution 1374 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1374–1379, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics A A A0 of B 0 -.5 A f ⌦e FCT M2=taxicab T f1 … f fi [f : e] w1=“A” .8 A y=ART(M1,M2) M1=man .3 B0 of f i fi 1 0 1 0 0 1 B .7 ewi B"
N15-1155,P14-1136,0,0.250951,"& Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a co"
N15-1155,P08-1068,0,0.0380643,"s while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlev"
N15-1155,N04-1043,0,0.0547495,"es and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim"
N15-1155,P14-2012,0,0.470285,"g feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, composition of embeddings alone may"
N15-1155,P13-1147,0,0.471663,"L , which yields  T ∂`/∂s = (I[y = y 0 ] − P (y 0 |x; T, Wf ))1≤y0 ≤L , where I[x] is the indicator function equal to 1 if x is true and 0 otherwise. Then we have the following stochastic gradients, where ◦ is the tensor product: (y,x)∈D n ∂` X ∂` = ⊗ gi ⊗ ewi , (4) ∂T ∂s i=1  n n  X X ∂` ∂` ∂gi ∂` = = T◦ ◦ ewi ⊗ fi . ∂Wf ∂gi ∂Wf ∂s i=1 i=1 4 Experiments Datasets We consider two relation extraction datasets: ACE2005 and ERE, both of which contain two sets of relations: coarse relation types and fine relation (sub-)types. Prior work on English ACE 2005 has focused only on coarse relations (Plank and Moschitti, 2013; Nguyen and Grishman, 2014; Li and Ji, 2014); to the best of our knowledge, this paper establishes the first baselines for the other datasets. Since the fine-grained relations require a large number of parameters, they will test the ability Model PM’13 (S) FCM (S) LRFCM (S) BASELINE (ST) FCM (ST) LRFCM (ST) ACE-bc (|L|=11) P R F1 55.3 43.1 48.5 62.3 45.1 52.3 58.5 46.8 52.0 72.2 52.0 60.5 66.2 54.2 59.6 65.1 54.7 59.4 ACE-bc (|L|=32) P R F1 59.7 41.6 49.0 57.4 46.2 51.2 60.2 51.2 55.3 62.9 49.6 55.4 63.5 51.1 56.6 ERE (|L|=9) P R F1 68.3 52.6 59.4 65.1 56.1 60.3 76.2 64.0 69.5 73.0 65.4 69.0"
N15-1155,D14-1045,0,0.0637846,"y with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, composition of embeddings alone may not capture important synt"
N15-1155,D12-1110,0,0.16476,"n Language Technology Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature e"
N15-1155,P13-1045,0,0.432027,"Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly wit"
N15-1155,D13-1170,0,0.0395996,"Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while maintaing a small number of parameters by learning feature embeddings jointly wit"
N15-1155,P11-1053,0,0.304691,"ameters by learning feature embeddings jointly with the parameters of a compositional model. The result is a method that can scale to more features and more labels, while avoiding overfitting. We demonstrate that our model attains state-of-the-art results on ACE and ERE fine-grained relation extraction. 1 Introduction Word embeddings represent words in some lowdimensional space, where each dimension might intuitively correspond to some syntactic or semantic property of the word.1 These embeddings can be used to create novel features (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Nguyen and Grishman, 2014; Roth and Woodsend, 2014), and can also be treated as model parameters ∗ The work was done while the author was visiting JHU. Such embeddings have a long history in NLP, such as term co-occurrence frequency matrices and their low-dimensional counterparts obtained by linear algebra tools (LSA, PCA, CCA, NNMF) and word clusters. Recently, neural networks have become popular methods for obtaining such embeddings (Bengio et al., 2006; Collobert et al., 2011; Mikolov et al., 2013). 1 While compositional models aim to learn higherlevel structure representations, compositi"
N15-1155,P10-1040,0,0.374507,"R. Gormley, Mark Dredze Mo Yu ∗ Machine Intelligence Human Language Technology Center of Excellence & Translation Lab Center for Language and Speech Processing Harbin Institute of Technology Johns Hopkins University Harbin, China Baltimore, MD, 21218 gflfof@gmail.com {mgormley, mdredze}@cs.jhu.edu Abstract to build representations for higher-level structures in some compositional embedding models (Collobert et al., 2011; Collobert, 2011; Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014). Applications of embedding have boosted the performance of many NLP tasks, including syntax (Turian et al., 2010; Collobert et al., 2011), semantics (Socher et al., 2012; Socher et al., 2013b; Hermann et al., 2014), question answering (Bordes et al., 2014) and machine translation (Devlin et al., 2014). Compositional embedding models build a representation for a linguistic structure based on its component word embeddings. While recent work has combined these word embeddings with hand crafted features for improved performance, it was restricted to a small number of features due to model complexity, thus limiting its applicability. We propose a new model that conjoins features and word embeddings while mai"
N15-1155,P05-1053,0,0.300494,"ng rates, weights of L2-regularizations, the number of iterations and the size of the feature embeddings d are tuned on dev sets. We selected d from {12, 15, 20, 25, 30, 40}. We used d=30 for feature embeddings for fine-grained ACE without gold types, and d=20 otherwise. For ERE, we have d=15. The weights of L2 λ was selected from {1e3, 5e-4, 1e-4}. As in prior work (Yu et al., 2014), regularization did not significantly help FCM. However for LRFCM, λ=1e-4 slightly helps. We use a learning rate of 0.05. We compare to two baselines. First, we use the features of Sun et al. (2011), who build on Zhou et al. (2005) with additional highly tuned features for ACE-style relation extraction from years of research. We implement these in a logistic regression model BASELINE, excluding country gazetteer and WordNet features. This baseline includes gold entity types and represents a high quality feature rich model. Second, we include results from Plank and Moschitti (2013) (PM’13), who obtained improveERE (|L|=18) Correct FCM Incorrect LRFCM Correct 423 57 Incorrect 34 246 Table 2: Confusion Matrix between the results of FCM and LRFCM on the test set of ERE fine relation task. Each item in the table shows the nu"
N15-1155,P14-1038,0,\N,Missing
N15-3018,W14-2907,1,0.881586,"Missing"
N15-3018,P14-1073,1,0.741206,"on-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quicklime (Section 3.7). an example visualization. 3.6 Cross Document Coreference Resolution Cross document coreference resolution is performed via the phylogenetic entity clustering model of Andrews et al. (2014).5 Since the method is fully unsupervised we did not require a Chinese specific model. We use this system to cluster EntityMentions and store the clustering in top level C ONCRETE Clustering objects. 3.7 Creating Manual Annotations Quicklime6 is a browser-based tool for viewing and editing NLP annotations stored in a C ONCRETE document. Quicklime supports a wide array of analytics, including parse trees, token taggings, entities, mentions, and “situations” (e.g. relations.) Quicklime uses the visualization layer of BRAT (Stenetorp et al., 2012) to display some annotations but does not use the"
N15-3018,W08-0336,0,0.0139229,"tions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 ,"
N15-3018,P05-1045,0,0.00391797,"cy parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit which is a CRF model based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gaz"
N15-3018,P03-1054,0,0.0257485,"Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, and the dependency analyses as C ON CRETE DependencyParses. 3.4 Named Entity Recognition We support the two most common named entity annotation standards: the CoNLL standard (four types: person, organization, location and miscellaneous), and the ACE standard, which includes the additional types of geo-political entity, facility, weapon and vehicle. The ACE standard also includes support for nested entities. We used the Stanford CoreNLP NER toolkit wh"
N15-3018,P14-5010,0,0.0125522,"ata sets include annotations for entities and a variety of relations (Aguilar et al., 2014). The labeled entities and relations are represented by C ONCRETE EntityMentions and stored in a EntityMentionSetList. Additional annotations that are typically utilized by relation extraction systems, such as syntactic parses, are provided automatically by the pipeline. 88 3.2 Word Segmentation Chinese text processing requires the identification of word boundaries, which are not indicated in written Chinese as they are in most other languages. Our word segmentation is provided by the Stanford CoreNLP3 (Manning et al., 2014) Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on"
N15-3018,W12-3018,1,0.837337,"Missing"
N15-3018,P11-1053,0,0.0126349,"otations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path between entities), which improve the expressive strength of word embeddings. We store the extracted relations in C ON CRETE SituationMentions. See Figure 2 for 89 Figure 2: ACE entity relations viewed through Quic"
N15-3018,N03-1033,0,0.0354521,"Chinese word segmentation tool, which is a conditional random field (CRF) model with character based features and lexicon features according to Chang et al. (2008). Word segmentations decisions are represented by C ONCRETE Token objects and stored in the TokenList. We follow the Chinese Penn Treebank segmentation standard (Xue et al., 2005). Our system tracks token offsets so that segmentation is robust to unexpected spaces or line breaks within a Chinese word. 3.3 Syntax Part of speech tagging and syntactic parsing are also provided by Stanford CoreNLP. The part of speech tagger is based on Toutanova et al. (2003) adapted for Chinese, which is a log-linear model underneath. Integration with C ONCRETE was facilitated by the concrete-stanford library 4 , though supporting Chinese required significant modifications to the 3 4 http://nlp.stanford.edu/software/corenlp.shtml https://github.com/hltcoe/concrete-stanford library. Resulting tags are stored in a C ONCRETE TokenTaggingList. Syntactic constituency parsing is based on the model of Klein and Manning (2003) adapted for Chinese. We obtained dependency parses from the CoreNLP dependency converter. We store the constituency parses as a C ONCRETE Parse, a"
N15-3018,P05-1053,0,0.027847,"l based on the method in Finkel et al. (2005), plus features based on Brown clustering. For the CoNLL standard annotations, we use one CRF model to label all the four types of entities. For the ACE standard annotations, in order to deal with the nested cases, we build one tagger for each entity type. Each entity is stored in a C ON CRETE EntityMention. 3.5 Relation Extraction Relations are extracted for every pair of entity mentions. We use a log-linear model with both traditional hand-crafted features and word embedding features. The hand-crafted features include all the baseline features of Zhou et al. (2005) (excluding the Country gazeteer and WordNet features), plus several additional carefully-chosen features that have been highly tuned for ACE-style relation extraction over years of research (Sun et al., 2011). The embedding-based features are from Yu et al. (2014), which represent each word as the outer product between its word embedding and a list of its associated non-lexical features. The non-lexical features indicate the word’s relative positions comparing to the target entities (whether the word is the head of any target entity, in-between the two entities, or on the dependency path betw"
N16-1117,Q14-1043,0,0.305337,"elopment set. 7 Experimental Settings We evaluate LRFR on three tasks: relation extraction, PP attachment and preposition disambiguation (see Table 1 for a task summary). We include detailed feature templates in Table 2. PP-attachment and relation extraction are two fundamental NLP tasks, and we test our models on the largest English data sets. The preposition disambiguation task was designed for compositional semantics, which is an important application of deep learning and distributed representations. On all these tasks, we compare to the state-of-the-art. We use the same word embeddings in Belinkov et al. (2014) on PP-attachment for a fair comparison. For the other experiments, we use the same 200-d word embeddings in Yu et al. (2015). 1024 Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1. W"
N16-1117,P14-1063,0,0.024792,", 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exp"
N16-1117,D14-1082,0,0.0440731,"ositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exploits the inner structure of complex lexica"
N16-1117,D15-1205,1,0.749854,"Missing"
N16-1117,P13-1088,0,0.0204306,"lion features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to ex"
N16-1117,P14-1136,0,0.0652837,"re 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question. To address the above problems,1 we prop"
N16-1117,P10-1030,0,0.0172187,"r than the one-hot word encodings. We denote the m-dimensional word embeddings by ew ; so the transformation matrices Wi for the lexical parts are of size ri × m where m  |V |. We note that when sufficiently large labeled data is available, our model allows for fine-tuning the pre-trained word embeddings to improve the expressive strength of the model, as is common with deep network models. Remarks Our LRFRs introduce embeddings for non-lexical properties and labels, making them better suit the common setting in NLP: rich linguistic properties; and large label sets such as open-domain tasks (Hoffmann et al., 2010). The LRFR - CP better suits n-gram features, since when n increases 1, the only new parameters are the corresponding Wi . It is also very efficient during prediction (O(nr)), since the cost of transformations can be ignored with the help of look-up tables and pre-computing. 5 Learning Representations for n-gram Lexical Features of Mixed Lengths For features with n lexical parts, we can train an LRFR n model to obtain their representations. However, we often have features of varying n (e.g. both unigrams (n=1) and bigrams (n=2) as in Figure 1). 1023 We require representations for features with"
N16-1117,P08-1068,0,0.621823,"a word type is conjoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in re"
N16-1117,P14-1130,0,0.494653,"applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior work: Lei et al. (2014) applied CP to combine different views of features. Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified features, while Lei et al. (2014) generates new features (usually different from ours) by combining some “atom” features. Thus their work may ignore some useful features; it relies on binary features as supplementary but our model needs not. (2) Lei et al. (2014)’s factorization relies on views with explicit meanings, e.g. head/modifier/arc in dependency parsing, making"
N16-1117,N15-1121,0,0.0535943,"Missing"
N16-1117,P15-2047,0,0.0198918,"eved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and"
N16-1117,P15-2029,0,0.0264122,"these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views o"
N16-1117,N06-1020,0,0.0858249,"Missing"
N16-1117,N04-1043,0,0.0487803,"semantic similarity, a word type is conjoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these"
N16-1117,P14-2012,0,0.0481492,"mensional features, including PCA, alternating structural optimization (Ando and Zhang, 2005), denoising autoencoders (Vincent et al., 2008), and feature embeddings (Yang and Eisenstein, 2015). These methods treat features as atomic elements and ignore the inner structure of features, so they learn separate embedding for each feature without shared parameters. As a result, they still suffer from large parameter spaces when the feature space is very huge.5 Another line of research studies the inner structures of lexical features: e.g. Koo et al. (2008), Turian et al. (2010), Sun et al. (2011), Nguyen and Grishman (2014), Roth and Woodsend (2014), and Hermann et al. (2014) used pre-trained word embeddings to replace the lexical parts of features ; Srikumar and Manning (2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composit"
N16-1117,D14-1045,0,0.122582,"to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open question. To address the a"
N16-1117,D12-1110,0,0.0573295,"(2014), Gormley et al. (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a spe"
N16-1117,P13-1045,0,0.0424596,". (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguis"
N16-1117,D13-1170,0,0.00328851,". (2015) and Yu et al. (2015) propose splitting lexical features into different parts and employing tensors to perform classification. The above can therefore be seen as special cases of our model that only embed a certain part (view) of the complex features. This restriction also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguis"
N16-1117,P11-1053,0,0.573266,"tion in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning remains an open q"
N16-1117,N15-1163,0,0.0208344,"nored with the help of look-up tables and pre-computing. 5 Learning Representations for n-gram Lexical Features of Mixed Lengths For features with n lexical parts, we can train an LRFR n model to obtain their representations. However, we often have features of varying n (e.g. both unigrams (n=1) and bigrams (n=2) as in Figure 1). 1023 We require representations for features with arbitrary different n simultaneously. We propose two solutions. The first is a straightforward solution based on our framework, which handles each n with a (n+2)-way tensor. This strategy is commonly used in NLP, e.g. Taub-Tabib et al. (2015) have different kernel functions for different order of dependency features. The second is an approximation method which aims to use a single tensor to handle all ns. Multiple Low-Rank Tensors Suppose that we can divide the feature set S(x, y) into subsets S1 (x, y), S2 (x, y), . . . , Sn (x, y) which correspond to features with one lexical part (unigram features), two lexical parts (bigram features), . . . and n lexical parts (n-gram features), respectively. To handle these types of features, we modify the training objective as follows: X minimize `(x, y; T1 , T2 , . . . , ...Tn ), (8) T1 ,T2"
N16-1117,P10-1040,0,0.211356,"njoined with its position in the phrase to signal its role. Figure 1b shows an example in dependency parsing, where multiple types (words) are conjoined with POS tags or distance information. ∗ Paper submitted during Mo Yu’s PhD study at HIT. To avoid model over-fitting that often results from features with lexical components, several smoothed lexical representations have been proposed and shown to improve performance on various NLP tasks; for instance, word embeddings (Bengio et al., 2006) help improve NER, dependency parsing and semantic role labeling (Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Sun et al., 2011; Roth and Woodsend, 2014; Hermann et al., 2014). However, using only word embeddings is not sufficient to represent complex lexical features (e.g. φ in Figure 1c). In these features, the same word embedding conjoined with different non-lexical properties may result in features indicating different labels; the corresponding lexical feature representations should take the above interactions into consideration. Such important interactions also increase the risk of over-fitting as feature space grows exponentially, yet how to capture these interactions in representation learning"
N16-1117,N15-1069,0,0.128161,"∧ y is the conjunction of the four parts. Figure (d) is the one-hot representation of φ, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors. v(x) = 1 means the vector v has a single non-zero element in the x position. eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior work: Lei et al. (2014) applied CP to combine different views of features. Compared to their work, our usage of CP-decomposition is different in the application to feature learning: (1) We focus on dimensionality reduction of existing, well-verified feat"
N16-1117,Q15-1017,1,0.803277,"guistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Socher et al. (2013a) and Hermann and Blunsom (2013) enhanced Recursive Neural Networks by refining the transformation matrices with linguistic features (e.g. phrase types). These models are similar to ours in the sense of learning representations based on linguistic features and embeddings. Low-rank Tensor Models for NLP aim to handle the conjunction among different views of features (Cao and Khudanpur, 2014; Lei et al., 2014; Chen and Manning, 2014). Yu and Dredze (2015) proposed a model to compose phrase embeddings from words, which has an equivalent form of our CPbased method under certain restrictions. Our work applies a similar idea to exploiting the inner structure of complex features, and can handle n-gram features with different ns. Our factorization (§3) is general and easy to adapt to new tasks. More importantly, it makes the model benefit from pre-trained word embeddings as shown by the PP-attachment results. 10 Conclusion We have presented LRFR, a feature representation model that exploits the inner structure of complex lexical features and applies"
N16-1117,N15-1155,1,0.519215,"andidate head. Figure (c) shows what the fifth feature (φ) is @R like, when the candidate is “see”. As is common in multi-class classification tasks, each template generates a different feature for each label y. Thus a feature φ = wg ∧ wc ∧ u ∧ y is the conjunction of the four parts. Figure (d) is the one-hot representation of φ, which is equivalent to the outer product (i.e. a 4-way tensor) among the four one-hot vectors. v(x) = 1 means the vector v has a single non-zero element in the x position. eters by approximating the parameter tensor with a low-rank tensor: the Tucker approximation of Yu et al. (2015) but applied to each embedding type (view), or the Canonical/Parallel-Factors Decomposition (CP). Our models use fewer parameters than previous work that learns a separate representation for each feature (Ando and Zhang, 2005; Yang and Eisenstein, 2015). CP approximation also allows for much faster prediction, going from a method that is cubic in rank and exponential in the number of lexical parts, to a method linear in both. Furthermore, we consider two methods for handling features that rely on n-grams of mixed lengths. Our model makes the following contributions when contrasted with prior w"
N16-1117,P14-2107,0,0.0158469,"ion also makes their model parameters form a full rank tensor, resulting in data sparsity and high computational costs when the tensors are large. Composition Models (Deep Learning) build representations for structures based on their component word embeddings (Collobert et al., 2011; Bordes et al., 2012; Socher et al., 2012; Socher et al., 2013b). When using only word embeddings, these models achieved successes on several NLP tasks, but sometimes fail to learn useful syntactic or semantic patterns beyond the strength of combinations of word 5 For example, a state-of-the-art dependency parser (Zhang and McDonald, 2014) extracts about 10 million features; in this case, learning 100-dimensional feature embeddings involves estimating approximately a billion parameters. 1027 embeddings, such as the dependency relation in Figure 1(a). To tackle this problem, some work designed their model structures according to a specific kind of linguistic patterns, e.g. dependency paths (Ma et al., 2015; Liu et al., 2015), while a recent trend enhances compositional models with linguistic features. For example, Belinkov et al. (2014) concatenate embeddings with linguistic features before feeding them to a neural network; Soch"
N16-1117,P05-1053,0,0.157917,"al. (2015). 1024 Relation Extraction We use the English portion of the ACE 2005 relation extraction dataset (Walker et al., 2006). Following Yu et al. (2015), we use both gold entity spans and types, train the model on the news domain and test on the broadcast conversation domain. To highlight the impact of training data size we evaluate with all 43,518 relations (entity mention pairs) and a reduced training set of the first 10,000 relations. We report precision, recall, and F1. We compare to two baseline methods: 1) a loglinear model with a rich binary feature set from Sun et al. (2011) and Zhou et al. (2005) as described in Yu et al. (2015) (BASELINE); 2) the embedding model (FCM) of Gormley et al. (2015), which uses rich linguistic features for relation extraction. We use the same feature templates and evaluate on finegrained relations (sub-types, 32 labels) (Yu et al., 2015). This will evaluate how LRFR can utilize nonlexical linguistic features. PP-attachment We consider the prepositional phrase (PP) attachment task of Belinkov et al. (2014),3 where for each PP the correct head (verbs or nouns) must be selected from content words before the PP (within a 10-word window). We formulate the task a"
N16-1122,D10-1124,0,0.627365,"Missing"
N16-1122,W08-0804,1,0.684025,"t data. All other parameters used default settings. 3 Our reliance on text features created a very large feature space, but only a small fraction of these occur with any regularity. Previous work has shown feature selection helpful for geolocation (Han et al., 2014). We tried L1 regularization for feature selection without a significant change to our results. It may be that our larger volume of training data removes the need for feature selection. Alternatively, we use feature hashing (to a 31-bit feature space) which can be a form of regularization as feature collisions mitigate overfitting (Ganchev and Dredze, 2008; Weinberger et al., 2009). 4 Evaluation We report the four evaluation metrics of Han et al. (2014): city accuracy (AccCi), country accuracy (AccCo), accuracy within 161 km (100 miles) (Acc@161), and the median error in km (Median). Baselines We include two baselines: (1) the majority predictor: always predicts the most popular label. (2) alias matching: we create a list of aliases for each of the 2983 cities from the genomes dataset, which includes the smaller cities clustered together by Han et al. (2014). We search each tweet and the user’s profile location for these aliases, assigning a tw"
N16-1122,P14-5007,1,0.337563,"Missing"
N16-1122,D15-1256,0,0.124047,"Missing"
N18-1034,J92-4003,0,0.472137,"atures, or where the analyst selects a few meaningful features 365 Proceedings of NAACL-HLT 2018, pages 365–374 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by hand. A solution to this restriction is to learn lowdimensional representations of document features. Neural networks have shown wide-spread success at learning generalizable representations, often obviating the need for hand designed features (Collobert and Weston, 2008). A prime example is word embedding features in natural language processing, which supplant traditional lexical features (Brown et al., 1992; Mikolov et al., 2013; Pennington et al., 2014). Jointly learning networks that construct feature representations along with the parameters of a standard NLP model has become a common approach. For example, (Yu et al., 2015) used a tensor decomposition to jointly learn features from both word embeddings and traditional NLP features, along with the parameters of a relation extraction model. Additionally, neural networks can handle a variety of data types, including text, images and general metadata features. This makes them appropriate for addressing dimensionality reduction in DMR. We propose"
N18-1034,W10-0701,1,0.704113,"Missing"
N18-1034,E14-1056,0,0.22993,"topics fixed to 10, and dDMR architecture fixed to narrow layer widths (50, 10). Model selection was based on the macro-averaged performance on the next eight folds, and we report performance on the remaining fold. We selected models separately for each evaluation metric. For dDMR, model selection amounts to selecting the document prior architecture, and for DMR with PCA-reduced feature supervision, model selection involved selecting the PCA projection width. Evaluation Each model was evaluated according to heldout perplexity, topic coherence by normalized pointwise mutual information (NPMI) (Lau et al., 2014), and a dataset-specific predictive task. Heldout perplexity was computed by only aggregating document-topic and topic-word counts from every other token in the corpus, and evaluating perplexity on the remaining heldout tokens. This corresponds to the “document completion” evaluation method as described in (Wallach et al., 2009), where instead of holding out the words in the second half of a document, every other word is held out. NPMI (Lau et al., 2014) computes a an automatic measure of topic quality, the sum of pointwise mutual information between pairs of m most likely words normalized by"
N18-1034,Q15-1004,1,0.83965,"cted from the image. However, this model is used for image classification, not for exploring a corpus of documents as is typical of topic models. These models are computationally attractive in that they avoid approximating the posterior distribution of topic assignments given tokens by dropping the assumption that θ and φ are drawn from Dirichlet priors. Model fitting is performed by back-propagation of a max-margin cost. In contrast, we use neural networks to learn feature representations for documents, not as a replacement for the LDA generative story. This is similar to variants of SPRITE (Paul and Dredze, 2015), where many document-level factors are combined to generate a document-topic prior. In contrast to several of these models, the core of our topic model remains unchanged, meaning that dDMR is agnostic to many other extensions of LDA. There has been extensive work in modeling both textual and visual topics. Models such as Corr-LDA (Blei and Jordan, 2003) suppose that a text document and associated image features are generated by a shared latent topic. This property is shared by other topic models over images, such as STMTwitterLDA (Cai et al., 2015) and (Zhang et al., 2015). While these models"
N18-1034,D14-1162,0,0.0936899,"meaningful features 365 Proceedings of NAACL-HLT 2018, pages 365–374 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics by hand. A solution to this restriction is to learn lowdimensional representations of document features. Neural networks have shown wide-spread success at learning generalizable representations, often obviating the need for hand designed features (Collobert and Weston, 2008). A prime example is word embedding features in natural language processing, which supplant traditional lexical features (Brown et al., 1992; Mikolov et al., 2013; Pennington et al., 2014). Jointly learning networks that construct feature representations along with the parameters of a standard NLP model has become a common approach. For example, (Yu et al., 2015) used a tensor decomposition to jointly learn features from both word embeddings and traditional NLP features, along with the parameters of a relation extraction model. Additionally, neural networks can handle a variety of data types, including text, images and general metadata features. This makes them appropriate for addressing dimensionality reduction in DMR. We propose deep Dirichlet Multinomial Regression (dDMR), a"
N18-1034,D09-1026,0,0.584227,"using either downstream or upstream models. Downstream models, such as supervised LDA (Mcauliffe and Blei, 2008), assume that these additional document features are generated from each document’s topic distribution. These models are most helpful when you desire topics that are predictive of the output, such as models for predicting the sentiment of product reviews. Upstream models, such as Dirichlet Multinomial Regression (DMR), condition each document’s topic distribution on document features, such as author (RosenZvi et al., 2004), social network (McCallum et al., 2007), or document labels (Ramage et al., 2009). Previous work has demonstrated that upstream models tend to outperform downstream models in terms of model fit, as well as extracting topics that are useful in prediction of related tasks (Benton et al., 2016). DMR is an upstream topic model with a particularly attractive method for incorporating arbitrary document features. Rather than defining specific random variables in the graphical model for each new document feature, DMR treats the document annotations as features in a log-linear model. The log-linear model parameterizes the Dirichlet prior for the document’s topic distribution, makin"
P07-1056,W06-1615,1,0.840251,"ain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta. edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2 The dataset will be made available by the authors at publication time. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics respondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification. A key step in SCL is the selection of pivot features that are used to link the source and target domains. We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels. For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains. In our second extension we show how to correct misalignments using a very small number of labeled instances. Second, we evaluate the A-distance (Ben-David et al., 2006) between domain"
P07-1056,W04-3237,0,0.102684,"e any SCL features. We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters in w. As we can see, though, relearning the 50 parameters in v is quite helpful. The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure. The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation. In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only. For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline. Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000. Table 3 su"
P07-1056,N04-1001,0,0.0662213,"L w0 xi + v0 θxi , yi + λ||w||2 + µ||v||2 , min w,v i where y is the label. The weight vector w ∈ Rd weighs the original features, while v ∈ Rk weighs the projected features. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4 , µ = 0, which we have used in our results so far. Suppose now that we have trained source model weight vectors ws and vs . A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller. We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006). Then we solve P minw,v j L (w0 xj + v0 θxj , yj ) + λ||w||2 +µ||v − vs ||2 . Since we don’t want to deviate significantly from the source parameters, we set λ = µ = 10−1 . Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances. We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort. For reasons of space, for each target domain 444 dom  model base books dvd electron kitchen average 8.9 8.9 8.3 10.2 9.1 base +targ 9.0 8.9 8.5 9.9 9.1 scl scl-mi 7.4 7.8 6.0 7.0 7.1 5.8 6.1 5.5 5.6"
P07-1056,P05-1015,0,0.338398,"on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances. Similarly we would also choose one domain from either electronics or kitchen appliances, but not both. 7 Related Work Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline. Thomas et al. (2006) use discourse structure present in congressional records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation. The work most similar in spirit to ours that of Turney (2002). He used the difference in mutual information with two human-selecte"
P07-1056,W02-1011,0,0.0789665,". This is not the case for sentiment classification, however. Therefore, we require that pivot features also be good predictors of the source label. Among those features, we then choose the ones with highest mutual information to the source label. Table 1 shows the set-symmetric SCL, not SCL-MI SCL-MI, not SCL book one &lt;num&gt; so all a must a wonderful loved it very about they like weak don’t waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL, but not SCLMI (left) and vice-versa (right) 2004). On the polarity dataset, this model matches the results reported by Pang et al. (2002). When we report results with SCL and SCL-MI, we require that pivots occur in more than five documents in each domain. We set k, the number of singular vectors of the weight matrix, to 50. 4 Experiments with SCL and SCL-MI differences between the two methods for pivot selection when adapting a classifier from books to kitchen appliances. We refer throughout the rest of this work to our method for selecting pivots as SCL-MI. 3 Dataset and Baseline We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for four different product types: books, DVDs, elect"
P07-1056,W06-1639,0,0.425813,"tes well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been 440 deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time. We envision a scenario"
P07-1056,P02-1053,0,0.129593,"the result by 100. We refer to this quantity as the proxy A-distance. When it is 100, the two domains are completely distinct. When it is 0, the two domains are indistinguishable using a linear classifier. Figure 3 is a correlation plot between the proxy A-distance and the adaptation error. Suppose we wanted to label two domains out of the four in such a a completely unsupervised manner. Then he classified documents according to various functions of these mutual information scores. We stress that our method improves a supervised baseline. While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset. 14 12 BE, BK DK Adaptation Loss 10 DE 8 BD 6 4 EK 2 0 60 65 70 75 80 85 90 95 100 Proxy A-distance Figure 3: The proxy A-distance between each domain pair plotted against the average adaptation loss of as measured by our baseline system. Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen. way as to minimize our error on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either book"
P08-2009,J99-2004,0,0.0219889,"7), morphological analyzers disambiguate words before statistical tagging in Arabic (Habash and Rambow, 2005) and Czech (Hajiˇc and Hladká, 1998). This general approach has led to the serial combination of rule based and statistical taggers for efficiency and accuracy (Hajiˇc et al., 2001). While our tagger could be combined with these linguistic resources as well, as in Loftsson (2007), we show state of the art performance without these resources. Another approach to fine-grained tagging captures grammatical structures with treebased tags, such as “supertags” in the tree-adjoining grammar of Bangalore and Joshi (1999). 3 Icelandic Morphology Icelandic is notable for its morphological richness. Verbs potentially show as many as 54 different forms depending on tense, mood, voice, person and 33 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 33–36, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics number. A highly productive class of verbs also show stem vowel alternations reminiscent of Semitic verb morphology (Arabic). Noun morphology exhibits a robust case system; nouns may appear in as many as 16 different forms. The four-case system of Icelandic is simila"
P08-2009,P05-1071,0,0.0194179,"tly data-driven learning approach without external linguistic resources (morphological analyzer, lexicons, etc.). Our system achieves the best performance to date on Icelandic, Related Work Previous approaches to tagging morphologically complex languages with fine grained tagsets have considered Czech and Arabic. Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but subsequent work has collapsed the tagset to simplify tagging (Diab et al., 2004). Like previous Icelandic work (Loftsson, 2007), morphological analyzers disambiguate words before statistical tagging in Arabic (Habash and Rambow, 2005) and Czech (Hajiˇc and Hladká, 1998). This general approach has led to the serial combination of rule based and statistical taggers for efficiency and accuracy (Hajiˇc et al., 2001). While our tagger could be combined with these linguistic resources as well, as in Loftsson (2007), we show state of the art performance without these resources. Another approach to fine-grained tagging captures grammatical structures with treebased tags, such as “supertags” in the tree-adjoining grammar of Bangalore and Joshi (1999). 3 Icelandic Morphology Icelandic is notable for its morphological richness. Verbs"
P08-2009,P98-1080,0,0.0975374,"Missing"
P08-2009,P01-1035,0,0.227648,"Missing"
P08-2009,N07-2027,0,0.19995,"ine grained annotations. Additionally, we show that good performance can be achieved using a strictly data-driven learning approach without external linguistic resources (morphological analyzer, lexicons, etc.). Our system achieves the best performance to date on Icelandic, Related Work Previous approaches to tagging morphologically complex languages with fine grained tagsets have considered Czech and Arabic. Khoja (2001) first introduced a tagger for Arabic, which has 131 tags, but subsequent work has collapsed the tagset to simplify tagging (Diab et al., 2004). Like previous Icelandic work (Loftsson, 2007), morphological analyzers disambiguate words before statistical tagging in Arabic (Habash and Rambow, 2005) and Czech (Hajiˇc and Hladká, 1998). This general approach has led to the serial combination of rule based and statistical taggers for efficiency and accuracy (Hajiˇc et al., 2001). While our tagger could be combined with these linguistic resources as well, as in Loftsson (2007), we show state of the art performance without these resources. Another approach to fine-grained tagging captures grammatical structures with treebased tags, such as “supertags” in the tree-adjoining grammar of Ba"
P08-2009,P07-1096,0,0.0729909,"ment of Linguistics University of Pennsylvania Philadelphia, PA 19104 2 Introduction While part of speech (POS) tagging for English is very accurate, languages with richer morphology demand complex tagsets that pose problems for data driven taggers. In this work we consider Icelandic, a language for which a linguistic rule-based method is the current state of the art, indicating the difficulty this language poses to learning systems. Like Arabic and Czech, other morphologically complex languages with large tagsets, Icelandic can overwhelm a statistical tagger with ambiguity and data sparsity. Shen et al. (2007) presented a new framework for bidirectional sequence classification that achieved the best POS score for English. In this work, we evaluate their tagger on Icelandic and improve results with extensions for fine grained annotations. Additionally, we show that good performance can be achieved using a strictly data-driven learning approach without external linguistic resources (morphological analyzer, lexicons, etc.). Our system achieves the best performance to date on Icelandic, Related Work Previous approaches to tagging morphologically complex languages with fine grained tagsets have consider"
P08-2009,N04-4038,0,\N,Missing
P08-2009,C98-1077,0,\N,Missing
P08-2059,P07-1056,1,0.339012,"we used a similar experimental setup to Tong and Koller (2001). Each active learning algorithm was given two labeled examples, one from each class, for initial training of a classifier, and remaining data as unlabeled examples. On each round the algorithm selected a single instance for which it was then given the correct label. The algorithm updated the online classifier and evaluated it on held out test data to measure learning progress. We selected four binary NLP datasets for evaluation: 20 Newsgroups1 and Reuters (Lewis et al., 2004) (used by Tong and Koller) and sentiment classification (Blitzer et al., 2007) and spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar labels from computers, science and talk. We created 3 similar problems from Reuters from insurance, business services and retail distribution. Sentiment used 4 Amazon domains (book, dvd, electronics, kitchen). Spam used the three users from task A data. Each problem had 2000 instances except for 20 Newsgroups, which used between 1850 and 1971 instances. This creat"
P08-2059,P07-1007,0,0.0313519,"Margin. ACL required fewer labels than CW margin twice as often as the opposite occurred (8 vs 4). Note that CW Margin used more labels than CW Random in three cases, while ACL only once, and this one time only about a dozen labels were needed. To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance. 5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al., 2007), parsing (Tang et al., 2002) and word sense disambiguation (Chan and Ng, 2007). Many methods rely on entropy-based scores such as uncertainty sampling (Lewis and Gale, 1994). Others use margin based methods, such as Kim et al. (2006), who combined margin scores with corpus diversity, and Sassano (2002), who considered SVM active learning 20 Newsgroups All 0.95 0.90 1.4 0.85 0.80 0.75 PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 0.70 0.65 100 150 200 250 300 Labels 350 400 450 0.85 0.80 PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.75 500 CW Margin Labels 1.2 Test Accuracy Test Accuracy 0.90 1"
P08-2059,N06-2018,0,0.0288463,"ree cases, while ACL only once, and this one time only about a dozen labels were needed. To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance. 5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al., 2007), parsing (Tang et al., 2002) and word sense disambiguation (Chan and Ng, 2007). Many methods rely on entropy-based scores such as uncertainty sampling (Lewis and Gale, 1994). Others use margin based methods, such as Kim et al. (2006), who combined margin scores with corpus diversity, and Sassano (2002), who considered SVM active learning 20 Newsgroups All 0.95 0.90 1.4 0.85 0.80 0.75 PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 0.70 0.65 100 150 200 250 300 Labels 350 400 450 0.85 0.80 PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.75 500 CW Margin Labels 1.2 Test Accuracy Test Accuracy 0.90 100 150 200 250 300 Labels 350 400 450 1.0 0.8 Reuters 20 Newsgroups Sentiment Spam 0.6 0.4 500 0.2 0.2 0.4 0.6 0.8 1.0 ACL Labels 1.2 1.4 Figure 1: Results"
P08-2059,W07-1516,0,0.0427858,"ataset. Points above the diagonal-line demonstrate the superiority of ACL over CW Margin. ACL required fewer labels than CW margin twice as often as the opposite occurred (8 vs 4). Note that CW Margin used more labels than CW Random in three cases, while ACL only once, and this one time only about a dozen labels were needed. To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance. 5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al., 2007), parsing (Tang et al., 2002) and word sense disambiguation (Chan and Ng, 2007). Many methods rely on entropy-based scores such as uncertainty sampling (Lewis and Gale, 1994). Others use margin based methods, such as Kim et al. (2006), who combined margin scores with corpus diversity, and Sassano (2002), who considered SVM active learning 20 Newsgroups All 0.95 0.90 1.4 0.85 0.80 0.75 PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 0.70 0.65 100 150 200 250 300 Labels 350 400 450 0.85 0.80 PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.6"
P08-2059,P02-1064,0,0.0317594,"ls were needed. To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance. 5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al., 2007), parsing (Tang et al., 2002) and word sense disambiguation (Chan and Ng, 2007). Many methods rely on entropy-based scores such as uncertainty sampling (Lewis and Gale, 1994). Others use margin based methods, such as Kim et al. (2006), who combined margin scores with corpus diversity, and Sassano (2002), who considered SVM active learning 20 Newsgroups All 0.95 0.90 1.4 0.85 0.80 0.75 PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 0.70 0.65 100 150 200 250 300 Labels 350 400 450 0.85 0.80 PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.75 500 CW Margin Labels 1.2 Test Accuracy Test Accuracy 0.90 100 150 200 250 300 Labels 350 400 450 1.0 0.8 Reuters 20 Newsgroups Sentiment Spam 0.6 0.4 500 0.2 0.2 0.4 0.6 0.8 1.0 ACL Labels 1.2 1.4 Figure 1: Results averaged over 20 Newsgroups (left) and all datasets (center) showing"
P08-2059,P02-1016,0,0.0778007,"l-line demonstrate the superiority of ACL over CW Margin. ACL required fewer labels than CW margin twice as often as the opposite occurred (8 vs 4). Note that CW Margin used more labels than CW Random in three cases, while ACL only once, and this one time only about a dozen labels were needed. To conclude, not only does CW Margin outperforms PA Margin for active-learning, CW maintains additional valuable information (confidence), which further improves performance. 5 Related Work Active learning has been widely used for NLP tasks such as part of speech tagging (Ringger et al., 2007), parsing (Tang et al., 2002) and word sense disambiguation (Chan and Ng, 2007). Many methods rely on entropy-based scores such as uncertainty sampling (Lewis and Gale, 1994). Others use margin based methods, such as Kim et al. (2006), who combined margin scores with corpus diversity, and Sassano (2002), who considered SVM active learning 20 Newsgroups All 0.95 0.90 1.4 0.85 0.80 0.75 PA Random (82.53) CW Random (92.92) PA Margin (88.06) CW Margin (95.39) ACL (95.51) 0.70 0.65 100 150 200 250 300 Labels 350 400 450 0.85 0.80 PA Random (81.30) CW Random (86.67) PA Margin (83.99) CW Margin (88.61) ACL (88.79) 0.75 500 CW Ma"
P11-1072,N10-1025,1,0.916837,"m would output the closest known words (e.x. “slow it dawn”), a hybrid system could output a sequence of multi-phoneme units: s l ow, b ax, d ae n. The latter is more useful for automatically recovering the word’s orthographic form, identifying that an OOV was spoken, or improving performance of a spoken term detection system with OOV queries. In fact, hybrid systems have improved OOV spoken term detection (Mamou et al., 2007; Parada et al., 2009), achieved better phone error rates, especially in OOV regions (Rastrow et al., 2009b), and obtained state-of-the-art performance for OOV detection (Parada et al., 2010). Hybrid recognizers vary in a number of ways: sub-word unit type: variable-length phoneme units (Rastrow et al., 2009a; Bazzi and Glass, 2001) or joint letter sound sub-words (Bisani and Ney, 2005); unit creation: data-driven or linguistically motivated (Choueiter, 2009); and how they are incorporated in LVCSR systems: hierarchical (Bazzi, 2002) or flat models (Bisani and Ney, 2005). In this work, we consider how to optimally create sub-word units for a hybrid system. These units are variable-length phoneme sequences, although in principle our work can be use for other unit types. Previous me"
P11-1072,N09-1024,0,0.0291877,"ds to segment taken from raw text, a mapping between words and classes (side information indicating whether token is IV or OOV), a pronunciation dictionary D, and a letter to sound model (L2S), such as the one described in Chen (2003). The corpus W is the list of types (unique words) in the raw text input. This forces each word to have a unique segmentation, shared by all common tokens. Words are converted into phonetic representations according to their most likely dictionary pronunciation; non-dictionary words use the L2S model.2 2.1 Model Inspired by the morphological segmentation model of Poon et al. (2009), we assume P (Y, S|W ) is a log-linear model parameterized by Λ: PΛ (Y, S|W ) = 1 uΛ (Y, S, W ) Z(W ) (1) where uΛ (Y, S, W ) defines the score of the proposed segmentation S for words W and labels Y according to model parameters Λ. Sub-word units σ compose S, where each σ is a phone sequence, including the full pronunciation for vocabulary words; the collection of σs form the lexicon. Each unit σ is present in a segmentation with some context c = (φl , φr ) of the form φl σφr . Features based on the context and the unit itself parameterize uΛ . In addition to scoring a segmentation based on"
P11-1072,W04-2902,0,\N,Missing
P12-1019,P05-1063,0,0.714277,"how that up-training leads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices an"
P12-1019,D09-1116,0,0.031144,"eads to WER reduction. 1 Introduction Language models (LM) are crucial components in tasks that require the generation of coherent natural language text, such as automatic speech recognition (ASR) and machine translation (MT). While traditional LMs use word n-grams, where the n − 1 previous words predict the next word, newer models integrate long-span information in making decisions. For example, incorporating long-distance dependencies and syntactic structure can help the LM better predict words by complementing the predictive power of n-grams (Chelba and Jelinek, 2000; Collins et al., 2005; Filimonov and Harper, 2009; Kuo et al., 2009). The long-distance dependencies can be modeled in either a generative or a discriminative framework. Discriminative models, which directly distinguish correct from incorrect hypothesis, are particularly attractive because they allow the inclusion of arbitrary features (Kuo et al., 2002; Roark et al., 2007; Collins et al., 2005); these models with syntactic information have obtained state of the art results. However, both generative and discriminative LMs with long-span dependencies can be slow, for they often cannot work directly with lattices and require rescoring large N"
P12-1019,N10-1115,0,0.0160197,"states occur when Q is empty and S contains a single tree (the output). Ω is determined by the set of dependency labels r ∈ R and one of three transition types: • Shift: remove the head of Q (wj ) and place it on the top of S as a singleton tree (only wj .) • Reduce-Leftr : replace the top two trees in S (s0 and s1 ) with a tree formed by making the root of s1 a dependent of the root of s0 with label r. • Reduce-Rightr : same as Reduce-Leftr except reverses s0 and s1 . Table 1 shows the kernel features used in our dependency parser. See Sagae and Tsujii (2007) for a complete list of features. Goldberg and Elhadad (2010) observed that parsing time is dominated by feature extraction and score calculation. Substructure sharing reduces these steps for equivalent states, which are persistent throughout a candidate set. Note that there are far fewer kernel features than total features, hence the hash function calculation is very fast. We summarize substructure sharing for dependency parsing in Algorithm 1. We extend the definition of states to be {S, Q, p} where p denotes the score of the state: the probability of the action sequence that resulted in the current state. Also, fol179 while Heap 6= ∅ do πcurrent ←Hea"
P12-1019,P10-1110,0,0.0315158,"r multi-class classification, so pg (ωi |πi ) = pg (f (π) ◦ ωi ), where ◦ is a conjunction operation. In this way, states can be summarized by features. Equivalent states are defined as two states π and 0 π with an identical feature representation: π ≡ π0 iff f (π) = f (π 0 ) If two states are equivalent, then g imposes the same distribution over actions. We can benefit from this substructure redundancy, both within and between hypotheses, by saving these distributions in memory, sharing a distribution computed just once across equivalent states. A similar idea of equivalent states is used by Huang and Sagae (2010), except they use equivalence to facilitate dynamic programming for shift-reduce parsing, whereas we generalize it for improving the processing time of similar hypotheses in general models. Following Huang and Sagae, we define kernel features as the smallest set of atomic features ˜f (π) such that, ˜f (π) = ˜f (π 0 ) ⇒ π ≡ π0. H caches equivalent states in a hypothesis set and resets for each new utterance. For each state, we first check H for equivalent states before computing the action distribution; each cache hit reduces decoding time. Distributing hypotheses wi across different CPU thread"
P12-1019,D10-1002,0,0.100775,"illation Go/No-go Evaluation (Chen et al., 2006) with state of the art discriminative acoustic models. See Table 2 for a data summary. We use a modified Kneser-Ney (KN) backoff 4-gram baseline LM. Word-lattices for discriminative training and rescoring come from this baseline ASR system.6 The longspan discriminative LM’s baseline feature weight (α0 ) is tuned on dev data and hill climbing (Rastrow et al., 2011a) is used for training and rescoring. The dependency parser and POS tagger are trained on supervised data and up-trained on data labeled by the CKY-style bottom-up constituent parser of Huang et al. (2010), a state of the art broadcast news (BN) parser, with phrase structures converted to labeled dependencies by the Stanford converter. While accurate, the parser has a huge grammar (32GB) from using products of latent variable grammars and requires O(l3 ) time to parse a sentence of length l. Therefore, we could not use the constituent parser for ASR rescoring since utterances can be very long, although the shorter up-training text data was not a problem.7 We evaluate both unlabeled (UAS) and labeled dependency accuracy (LAS). 6.1 Results Before we demonstrate the speed of our models, we show th"
P12-1019,D10-1069,0,0.104863,"ities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include syntactic information in both generative and discriminative language models. For generative LMs, the syntactic information must be part of the generative process. Structured language modeling incorporates syntact"
P12-1019,P06-2089,0,0.0210613,"equivalence. The tagger is deterministic (greedy) in that it only considers the best tag at each step, so we do not store scores. However, this tagger uses a depth3 Sagae and Tsujii (2007) use a beam strategy to increase speed. Search space pruning is achieved by filtering heap states for probability greater than 1b the probability of the most likely state in the heap with the same number of actions. We use b = 100 for our experiments. 4 We note that while we have demonstrated substructure sharing for dependency parsing, the same improvements can be made to a shift-reduce constituent parser (Sagae and Lavie, 2006). w1 t1 w2 t2 ··· ··· wi ti 2 2 wi ti 1 wi wi+1 t1i t1i+1 t2i t2i+1 |T | ti+1 wi+2 wi+3 1 ti |T | lookahead search Figure 2: POS tagger with lookahead search of d=1. At wi the search considers the current state and next state. first search lookahead procedure to select the best action at each step, which considers future decisions up to depth d5 . An example for d = 1 is shown in Figure 2. Using d = 1 for the lookahead search strategy, we modify the kernel features since the decision for wi is affected by the state πi+1 . The kernel features in position i should be ˜f (πi ) ∪ ˜f (πi+1 ): ˜f (π"
P12-1019,D07-1111,0,0.275071,"Association for Computational Linguistics, pages 175–183, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic Language Models There have been several approaches to include"
P12-1019,W11-0328,0,0.164455,"al modifica175 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 175–183, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tion to the decoders used in auxiliary tools to utilize the commonalities among the set of generated hypotheses. The key idea is to share substructure states in transition based structured prediction algorithms, i.e. algorithms where final structures are composed of a sequence of multiple individual decisions. We demonstrate our approach on a local Perceptron based part of speech tagger (Tsuruoka et al., 2011) and a shift reduce dependency parser (Sagae and Tsujii, 2007), yielding significantly faster tagging and parsing of ASR hypotheses. While these simpler structured prediction models are faster, we compensate for the model’s simplicity through uptraining (Petrov et al., 2010), yielding auxiliary tools that are both fast and accurate. The result is significant speed improvements and a reduction in word error rate (WER) for both N -best list and the already fast hill climbing rescoring. The net result is arguably the first syntactic LM fast enough to be used in a real time ASR system. 2 Syntactic"
P13-2012,D12-1032,1,0.765713,"g synonym, hypernym, hyponym, meronym, and holonym edges and bucket the length. String Transducer To represent similarity between arguments that are names, we use a stochastic edit distance model. This stochastic string-tostring transducer has latent “edit” and “no edit” regions where the latent regions allow the model to assign high probability to contiguous regions of edits (or no edits), which are typical between variations of person names. In an edit region, parameters govern the relative probability of insertion, deletion, substitution, and copy operations. We use the transducer model of Andrews et al. (2012). Since in-domain name pairs were not available, we picked 10,000 entities at random from Wikipedia to estimate the transducer parameters. The entity labels were used as weak supervision during EM, as in Andrews et al. (2012). For a pair of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match t"
P13-2012,W99-0201,0,0.415464,"ry of entity disambiguation both within and across documents. While most information extraction work focuses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the"
P13-2012,P98-1013,0,0.0851903,"of mention spans, we compute the conditional log-likelihood of the two mentions going both ways, take the max, and then bucket to get binary features. We duplicate these features with copies that only fire if both mentions are tagged as PER, ORG or LOC. treat as independent experts. For each of these rule probabilities (experts), we find all rules that match the head tokens of a given alignment and have a feature for the max and harmonic mean of the log probabilities of the resulting rule set. FrameNet FrameNet is a lexical database based on Charles Fillmore’s Frame Semantics (Fillmore, 1976; Baker et al., 1998). The database (and the theory) is organized around semantic frames that can be thought of as descriptions of events. Frames crucially include specification of the participants, or Frame Elements, in the event. The Destroying frame, for instance, includes frame elements Destroyer or Cause Undergoer. Frames are related to other frames through inheritance and perspectivization. For instance the frames Commerce buy and Commerce sell (with respective lexical realizations “buy” and “sell”) are both perspectives of Commerce goods-transfer (no lexical realizations) which inherits from Transfer (with"
P13-2012,P10-1143,0,0.142587,"Missing"
P13-2012,N13-1106,1,0.858341,"Missing"
P13-2012,J08-4005,1,0.795859,"ions we perform can vary the “relatedness” of the two documents in terms of the predicates and arguments that they talk about. This reflects our expectation of real world data, where we do not expect perfect overlap in predicates and arguments between a source and target document, as you would in translation data. Lastly, we prune any document pairs that have more than 80 predicates or arguments or have a Jaccard index on bags of lemmas greater than 0.5, to give us a dataset of 328 document pairs. Metric We use precision, recall, and F1. For the RF dataset, we follow Roth and Frank (2012) and Cohn et al. (2008) and evaluate on a version of F1 that considers SURE and POSSIBLE links, which are available in the RF data. Given an alignment to be scored A and a reference alignment B which contains SURE and POSSIBLE links, Bs and Bp respectively, precision and recall are: 8 LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14, LDC2010T17, LDC2010T23, LDC2002T01, LDC2003T18, and LDC2005T05 P = 66 |A ∩ Bp | |A| R= |A ∩ Bs | |Bs | (1) EECB lemma RF lemma Roth and Frank MTC lemma PARMA PARMA PARMA F1 63.5 74.3 48.3 54.8 57.6 42.1 59.2 P 84.8 80.5 40.3 59.7 52.4 51.3 73.4 R 50.8 69.0 60.3 50.7 64.0 35.7 49.6 task di"
P13-2012,P07-1033,0,0.164696,"Missing"
P13-2012,C10-1032,1,0.882206,"Missing"
P13-2012,N13-1092,1,0.853348,"Missing"
P13-2012,P11-1095,0,0.0328258,"Missing"
P13-2012,D12-1045,0,0.367258,"ses on entities and noun phrases, there have been a few attempts at predicate, or event, disambiguation. Commonly a situational predicate is taken to correspond to either an event or a state, lexically realized in verbs such as “elect” or nominalizations such as “election”. Similar to entity coreference resolution, almost all of this work assumes unanchored mentions: predicate argument tuples are grouped together based on coreferent events. The first work on event coreference dates back to Bagga and Baldwin (1999). More recently, this task has been considered by Bejan and Harabagiu (2010) and Lee et al. (2012). As with unanchored entity disambiguation, these methods rely on clustering methods and evaluation metrics. Another view of predicate disambiguation seeks 2 PARMA (Predicate ARguMent Aligner) is a pipelined system with a wide variety of features used to align predicates and arguments in two documents. Predicates are represented as mention spans and arguments are represented as coreference chains (sets of mention spans) provided by in-document coreference resolution systems such as included in the Stanford NLP toolkit. Results indicated that the chains are of sufficient quality so as not to li"
P13-2012,W12-3018,1,0.876778,"Missing"
P13-2012,S12-1030,0,0.165538,"jamin Van Durme, Mark Dredze, Nicholas Andrews, Charley Beller, Chris Callison-Burch, Jay DeYoung, Justin Snyder, Jonathan Weese, Tan Xu† , and Xuchen Yao Human Language Technology Center of Excellence Johns Hopkins University, Baltimore, Maryland USA †University of Maryland, College Park, Maryland USA Abstract to link or align predicate argument tuples to an existing anchored resource containing references to events or actions, similar to anchored entity disambiguation (entity linking) (Dredze et al., 2010; Han and Sun, 2011). The most relevant, and perhaps only, work in this area is that of Roth and Frank (2012) who linked predicates across document pairs, measuring the F1 of aligned pairs. Here we present PARMA, a new system for predicate argument alignment. As opposed to Roth and Frank, PARMA is designed as a a trainable platform for the incorporation of the sort of lexical semantic resources used in the related areas of Recognizing Textual Entailment (RTE) and Question Answering (QA). We demonstrate the effectiveness of this approach by achieving state of the art performance on the data of Roth and Frank despite having little relevant training data. We then show that while the “lemma match” heuris"
P13-2012,C98-1013,0,\N,Missing
P14-1073,D12-1032,1,0.94161,"by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our model learns without supervision that these all refer to the the same entity. Such creative spellings are especially common on Twi"
P14-1073,P98-1012,0,0.882822,"5 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 775–785, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics §7 A minimum Bayes risk decoding procedure to pick an output clustering. The procedure is applicable to any model capable of producing a posterior over coreference decisions. We evaluate our approach by comparing to several baselines on datasets from three different genres: Twitter, newswire, and blogs. 2 Overview and Related Work Cross-document coreference resolution (CDCR) was first introduced by Bagga and Baldwin (1998b). Most approaches since then are based on the intuitions that coreferent names tend to have “similar” spellings and tend to appear in “similar” contexts. The distinguishing feature of our system is that both notions of similarity are learned together without supervision. We adopt a “phylogenetic” generative model of coreference. The basic insight is that coreference is created when an author thinks of an entity that was mentioned earlier in a similar context, and mentions it again in a similar way. The author may alter the name mention string when copying it, but both names refer to the same"
P14-1073,D08-1029,0,0.136929,"tic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multip"
P14-1073,D08-1113,1,0.913091,"Missing"
P14-1073,D13-1203,0,0.0253176,"on model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1 , . . . , xN ) denote an ordered sequence of distinct named-entity"
P14-1073,W11-2202,0,0.118784,"oximating w(p0 , x) as 1 or 0 according to whether p0 .n = x.n. We also omitted the IMH step from section 5.3. The other results we report do not use pragmatics at all, since we found that it gave only a slight improvement on Twitter. Unlike our other datasets, mentions are not annotated with entities: the reference consists of a table of 126 entities, where each row is the canonical name of one entity. Baselines. We compare to the system results reported in Figure 2 of Yogatama et al. (2012). This includes a baseline hierarchical clustering approach, the “EEA” name canonicalization system of Eisenstein et al. (2011), as well the model proposed by Yogatama et al. (2012). Like the output of our model, the output of their hierarchical clustering baseline is a mention clustering, and therefore must be mapped to a table of canonical entity names to compare to the reference table. Procedure & Results We tune our method as in previous experiments, on the initialization data used by Yogatama et al. (2012) which consists of a subset of 700 documents of the full dataset. The tuned model then produced a mention clustering on the full political blog corpus. As the mapping from clusters to a table is not fully detail"
P14-1073,N12-1007,1,0.944979,"who or what a name refers to. For instance, Wikipedia contains more than 100 variations of the name Barack Obama as redirects to the U.S. President article, including: President Obama Barack H. Obama, Jr. Barak Obamba Barry Soetoro To relate different names, one solution is to use specifically tailored measures of name similarity such as Jaro-Winkler similarity (Winkler, 1999; Cohen et al., 2003). This approach is brittle, however, and fails to adapt to the test data. Another option is to train a model like stochastic edit distance from known pairs of similar names (Ristad and Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 201"
P14-1073,N13-1122,0,0.183356,"Missing"
P14-1073,N10-1061,0,0.0246454,"tant component of a CDCR system is its model of name similarity (Winkler, 1999; Porter and Winkler, 1997), which is often fixed up front. This role is played in our system by the name mutation model, which we take to be a variant of stochastic edit distance (Ristad and Yianilos, 1996). Rather than fixing its parameters before we begin CDCR, we learn them (without supervision) as part of CDCR, by training from samples of reconstructed phylogenies. Name similarity is also an important component of within-document coreference resolution, and efforts in that area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as"
P14-1073,P10-1117,0,0.0184333,"el would be a more appropriate fit for a far larger corpus. Larger corpora also offer stronger signals that might enable our Monte Carlo methods to mix faster and detect regularities more accurately. A common error of our system is to connect mentions that share long substrings, such as different PERSONs who share a last name, or different ORGANIZATIONs that contain University of. A more powerful name mutation than the one we use here would recognize entire words, for example inserting a common title or replacing a first name with its common nickname. Modeling the internal structure of names (Johnson, 2010; Eisenstein et al., 2011; Yogatama et al., 2012) in the mutation model is a promising future direction. 9 Conclusions Our primary contribution consists of new modeling ideas, and associated inference techniques, for the problem of cross-document coreference resolution. We have described how writers systematically plunder (φ) and then systematically modify (θ) the work of past writers. Inference under such models could also play a role in tracking evolving memes and social influence, not merely in establishing strict coreference. Our model also provides an alternative to the distance-dependent"
P14-1073,W11-2213,0,0.0198391,"t area bear resemblance to our approach. Haghighi and Klein (2010) describe an “entitycentered” model where a distance-dependent Chinese restaurant process is used to pick previous coreferent mentions within a document. Similarly, Durrett and Klein (2013) learn a mention similarity model based on labeled data. Our cross-document setting has no observed mention ordering and no observed entities: we must sum over all possibilities, a challenging inference problem. The second major component of CDCR is context-based disambiguation of similar or identical names that refer to the same entity. Like Kozareva and Ravi (2011) and Green et al. (2012) we use topics as the contexts, but learn mention topics jointly with other model parameters. 3 Generative Model of Coreference Let x = (x1 , . . . , xN ) denote an ordered sequence of distinct named-entity mentions in documents d = (d1 , . . . , dD ). We assume that each document has a (single) known language, and that its mentions and their types have been identified by a named-entity recognizer. We use the objectoriented notation x.v for attribute v of mention x. Our model generates an ordered sequence x although we do not observe its order. Thus each mention x has l"
P14-1073,D12-1045,0,0.0583782,"n et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift → T-Swizzle Our m"
P14-1073,C10-2121,1,0.904437,"Missing"
P14-1073,D11-1141,0,0.149773,"Missing"
P14-1073,P11-1080,0,0.123079,"Yianilos, 1998; Green et al., 2012), but this requires supervised data in the test domain. Even the best model of name similarity is not enough by itself, since two names that are similar— even identical—do not necessarily corefer. Document context is needed to determine whether they may be talking about two different people. In this paper, we propose a method for jointly (1) learning similarity between names and (2) clustering name mentions into entities, the two major components of cross-document coreference resolution systems (Baron and Freedman, 2008; Finin et al., 2009; Rao et al., 2010; Singh et al., 2011; Lee et al., 2012; Green et al., 2012). Our model is an evolutionary generative process based on the name variation model of Andrews et al. (2012), which stipulates that names are often copied from previously generated names, perhaps with mutation (spelling edits). This can deduce that rather than being names for different entities, Barak Obamba and Barock obama more likely arose from the frequent name Barack Obama as a common ancestor, which accounts for most of their letters. This can also relate seemingly dissimilar names via multiple steps in the generative process: Taylor Swift → T-Swift"
P14-1073,P12-1040,0,0.237767,"Missing"
P14-1073,N09-1054,0,0.0156788,"ld) within-document coreference chain as the canonical mention, ignoring other mentions in the chain; we follow the same procedure in our experiments.11 Baselines & Procedure. We use the same baselines as in §8.1. On development data, modeling pragmatics as in §4.2 gave large improvements for organizations (8 points in F-measure), correcting the tendency to assume that short names like CIA were coincidental homonyms. Hence we allowed γ > 0 and tuned it on development data.12 Results are in Table 2. 8.3 Blogs Data. The CMU political blogs dataset consists of 3000 documents about U.S. politics (Yano et al., 2009). Preprocessed as described in Yogatama et al. (2012), the data consists of 10647 entity mentions. 11 10 Our single-threaded implementation took around 15 minutes per fold of the Twitter corpus on a personal laptop with a 2.3 Ghz Intel Core i7 processor (including time required to parse the data files). Typical acceptance rates for ordering and topic proposals ranged from 0.03 to 0.08. 782 That is, each within-document coreference chain is mapped to a single mention as a preprocessing step. 12 We used only a simplified version of the pragmatic model, approximating w(p0 , x) as 1 or 0 according"
P14-1073,P12-1072,0,0.345852,"stances of name variation that we would like our model to be able to learn. We also report the performance of different ablations of our full approach, in order to see which consistently helped across the different splits. We report additional experiments on the ACE 2008 corpus, and on a political blog corpus, to demonstrate that our approach is applicable in different settings. For Twitter and ACE 2008, we report the standard B3 metric (Bagga and Baldwin, 1998a). For the political blog dataset, the reference does not consist of entity annotations, and so we follow the evaluation procedure of Yogatama et al. (2012). From a single phylogeny p, we deterministically obtain a clustering e by removing the root ♦. Each of the resulting connected components corresponds to a cluster of mentions. Our model gives a distribution over phylogenies p (given observations x and learned parameters Φ)—and thus gives a posterior distribution over clusterings e, which can be used to answer various queries. A traditional query is to request a single clustering e. We prefer the clustering e∗ that minimizes Bayes risk (MBR) (Bickel and Doksum, 1977): X i,j: xi ∼xj where ∼ denotes coreference according to e0 . As explained abo"
P14-1111,W13-3520,0,0.0433198,"Missing"
P14-1111,W09-1206,0,0.049848,"Missing"
P14-1111,D08-1008,0,0.237198,"Missing"
P14-1111,P04-1061,0,0.547449,"e-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicat"
P14-1111,P08-1068,0,0.553211,"sed data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncating the bit-strings. We train 1000 Brown cl"
P14-1111,boxwell-white-2008-projecting,0,0.0592361,"Missing"
P14-1111,I11-1022,0,0.307934,"ning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Llu´ıs et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In the model from Johansson and Nugues (2008), t"
P14-1111,Q13-1018,0,0.0474088,"Missing"
P14-1111,J92-4003,0,0.152391,"g: 1. Pipeline vs. joint training (Figures 1 and 2) Unsupervised Syntax in the Pipeline Typical SRL systems are trained following a pipeline where the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Represen"
P14-1111,J07-3004,0,0.0451889,"Missing"
P14-1111,J93-2004,0,0.0462684,"useful. 4.1 Data The CoNLL-2009 Shared Task (Hajiˇc et al., 2009) dataset contains POS tags, lemmas, morphological features, syntactic dependencies, predicate senses, and semantic roles annotations for 7 languages: Catalan, Chinese, Czech, English, German, Japanese,4 Spanish. The CoNLL-2005 and -2008 Shared Task datasets provide English SRL annotation, and for cross dataset comparability we consider only verbal predicates (more details in § 4.4). To compare with prior approaches that use semantic supervision for grammar induction, we utilize Section 23 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). 4.2 Feature Template Sets Our primary feature set IGC consists of 127 template unigrams that emphasize coarse properties (i.e., properties 7, 9, and 11 in Table 1). We also explore the 31 template unigrams5 IGB described 3 To reduce hash collisions, We use MurmurHash v3 https://code.google.com/p/smhasher. 4 We do not report results on Japanese as that data was only made freely available to researchers that competed in CoNLL 2009. 5 Because we do not include a binary factor between predicate sense and semantic role, we do not include sense as a by Bj¨orkelund et al. (2009). Each of IGC and IG"
P14-1111,D11-1139,0,0.0160017,"e the relative position (Bj¨orkelund et al., 2009), geneological relationship, distance (Zhao et al., 2009), and binned distance (Koo et al., 2008) between two words in the path. From Llu´ıs et al. (2013), we use 1, 2, 3gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity). 3.4 Feature Selection Constructing all feature template unigrams and bigrams would yield an unwieldy number of features. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure (Martins et al., 2011): IGa,m = X X f ∈Tm xa p(f, xa ) log2 p(f, xa ) p(f )p(xa ) where Tm is the mth feature template, f is a particular instantiation of that template, and xa is an assignment to the variables in factor a. The probabilities are empirical estimates computed from the training data. This is simply the mutual information of the feature template instantiation with the variable assignment. This filtering approach was treated as a simple baseline in Martins et al. (2011) to contrast with increasingly popular gradient based regularization approaches. Unlike the gradient based ap1181 proaches, this filteri"
P14-1111,P05-1012,0,0.0593277,"h as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al., 2008). We create binary indicator features for each model using feature templates. Our feature template definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, Zhao et al. (2009) and Bj¨orkelund et al. (2009) and from features in syntactic dependency parsing (McDonald et al., 2005; Koo et al., 2008). Template Creation Feature templates are defined over triples of hproperty, positions, orderi. Properties, listed in Table 1, are extracted from word positions within the sentence, shown in Table 2. Single positions for a word wi include its syntactic parent, its leftmost farthest child (leftFarChild), its rightmost nearest sibling (rightNearSib), etc. Following Zhao et al. (2009), we include the notion of verb and noun supports and sections of the dependency path. Also following Zhao et al. (2009), properties from a set of positions can be put together in three possible or"
P14-1111,N04-1043,0,0.0325195,"ere the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncatin"
P14-1111,D12-1074,0,0.0885432,"ciation for Computational Linguistics, pages 1177–1187, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Modeling contributions: • Simpler joint CRF for syntactic and semantic dependency parsing than previously reported. • New application of unsupervised grammar induction: low-resource SRL. • Constrained grammar induction using SRL for distant-supervision. • Use of Brown clusters in place of POS tags for low-resource SRL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in"
P14-1111,P92-1017,0,0.483183,"e permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indicating there is no predicate-argument relationship between the two wor"
P14-1111,W07-2416,0,0.0277147,"better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with methods using distant supervision (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b) and fully unsupervised dependency parsing (Spitkovsky et al., 2013). Following prior work, we exclude punctuation from evaluation and convert the constituency trees to dependencies.12 The approach from Spitkovsky et al. (2013) 12 Naseem and Barzilay (2011) and our results use the Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et al. (2010b; 2013) use Collins (1999) head percolation rules. (SAJ’13) outperforms all other approaches, including our marginalized settings. We therefore may be able to achieve further gains in the pipeline model by considering better models of latent syntax, or better search techniques that break out of local optima. Similarly, improving the nonconvex optimization of our latent-variable CRF (Marginalized) may offer further gains. 5 Discussion and Future Work We have compared various approaches for lowresource semantic role labeling at the state-of-theart level. We find that we"
P14-1111,J08-2005,0,0.0617942,"ut semantic argument heads, not spans. Table 5 presents our results. Boxwell et al. (2011) (B’11) uses additional supervision in the form of a CCG tag dictionary derived from supervised data with (tdc) and without (tc) a cutoff. Our model does very poorly on the ’05 spanbased evaluation because the constituent bracketing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the oracle spans, where our F1 scores are higher than Boxwell et al. (2011). We also contrast with relavant high-resource methods with span/head conversions from Johansson and Nugues (2008): Punyakanok et al. (2008) (PRY’08) and Johansson and Nugues (2008) (JN’08). 50 40 Language / Dependency Parser Catalan / Marginalized Catalan / DMV+C German / Marginalized German / DMV+C 30 20 0 20000 40000 Number of Training Sentences 60000 Figure 3: Learning curve for semantic dependency supervision in Catalan and German. F1 of SRL only (without sense disambiguation) shown as the number of training sentences is increased. but a drop in performance for German. This may reflect a difference between the languages, or may reflect the difference between the annotation of the languages: both the Catalan and Spanish data o"
P14-1111,W09-1208,0,0.0850188,"arbitrary graphical structure.1 1 Introduction The goal of semantic role labeling (SRL) is to identify predicates and arguments and label their semantic contribution in a sentence. Such labeling defines who did what to whom, when, where and how. For example, in the sentence “The kids ran the marathon”, ran assigns a role to kids to denote that they are the runners; and a role to marathon to denote that it is the race course. Models for SRL have increasingly come to rely on an array of NLP tools (e.g., parsers, lemmatizers) in order to obtain state-of-the-art results (Bj¨orkelund et al., 2009; Zhao et al., 2009). Each tool is typically trained on hand-annotated data, thus placing SRL at the end of a very highresource NLP pipeline. However, richly annotated data such as that provided in parsing treebanks is expensive to produce, and may be tied to specific domains (e.g., newswire). Many languages do 1 not have such supervised resources (low-resource languages), which makes exploring SRL crosslinguistically difficult. The problem of SRL for low-resource languages is an important one to solve, as solutions pave the way for a wide range of applications: Accurate identification of the semantic roles of en"
P14-1111,D08-1016,0,0.170869,"RL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Re"
P14-1111,W10-2902,0,0.32565,"in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha"
P14-1111,P10-1130,0,0.267378,"in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha"
P14-1111,D11-1118,0,0.0375614,"Missing"
P14-1111,D13-1204,0,0.0703612,"raining. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, who use feature templates from combinations of word properties, syntactic positions including head and children, and semantic properties; and features from Bj¨orkelund et"
P14-1111,P05-1073,0,0.0765562,"acy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Llu´ıs et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions fr"
P14-1111,J03-4003,0,\N,Missing
P14-1111,W09-1201,0,\N,Missing
P14-1111,P08-1000,0,\N,Missing
P14-2089,P05-1077,0,0.0423536,"., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms another. Controlling this behavior is challenging with an unsupervised objective. However, rich prior knowledge exist"
P14-2089,D13-1170,0,0.00486117,"not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improve"
P14-2089,P10-1040,0,0.177376,"ments on word pairs. Word embeddings learned on unlabeled data are a popular tool in semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for"
P14-2089,P10-2043,0,0.0244334,"Missing"
P14-2089,N13-1092,0,0.163013,"Missing"
P14-2089,P12-1092,0,0.502229,"semantics, but may not capture the desired semantics. We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of"
P14-2089,W13-3512,0,0.0760931,"We propose a new learning objective that incorporates both a neural language model objective (Mikolov et al., 2013) and prior knowledge from semantic resources to learn improved lexical semantic embeddings. We demonstrate that our embeddings improve over those learned solely on raw text in three settings: language modeling, measuring semantic similarity, and predicting human judgements. 1 Introduction Word embeddings are popular representations for syntax (Turian et al., 2010; Collobert and Weston, 2008; Mnih and Hinton, 2007), semantics (Huang et al., 2012; Socher et al., 2013), morphology (Luong et al., 2013) and other areas. A long line of embeddings work, such as LSA and randomized embeddings (Ravichandran et al., 2005; Van Durme and Lall, 2010), has recently turned to neural language models (Bengio et al., 2006; Collobert and Weston, 2008; Turian et al., 2010). Unsupervised learning can take advantage of large corpora, which can produce impressive results. However, the main drawback of unsupervised learning is that the learned embeddings may not be suited for the task of interest. Consider semantic embeddings, which may capture a notion of semantics that improves one semantic task but harms ano"
P14-2110,W11-3407,0,0.0123015,"resentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this wor"
P14-2110,N13-1037,0,0.0202374,"facilitate multi-lingual corpus analysis. We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human annotators. 1 Figure 1: Three users discuss Mexico’s football team advancing to the Gold medal game in the 2012 Olympics in code-switched Spanish and English. can be folded in during training, the “glue” documents are required to aid in the alignment across languages. However, the ever changing vocabulary and topics of social media (Eisenstein, 2013) make finding suitable comparable corpora difficult. Standard techniques – such as relying on machine translation parallel corpora or comparable documents extracted from Wikipedia in different languages – fail to capture the specific terminology of social media. Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daum´e, 2010) similarly fail to adapt to shifting vocabularies. The result: an inability to train polylingual models on social media. In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages. Social media is filled"
P14-2110,C12-2029,0,0.0342668,"inguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans"
P14-2110,P98-1002,0,0.0283031,"rable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this work, we take an unsupervised approach, relying only on readily available document level"
P14-2110,C82-1023,0,0.176876,"in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task. In contrast to this work, we take an unsupervised approach, relying only on readily available document level language ID s"
P14-2110,N13-1131,0,0.0493679,"switched messages is given by Ling et al. (2013): 3 csLDA To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token specific language variable, and a process for identifying aligned topics. First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language. In the case of code-switched social media data, we require a pertoken language variable. However, while document level language identification (LID) systems are common place, very few languages have pertoken LID systems (King and Abney, 2013; Lignos and Marcus, 2013). To address the lack of available LID systems, we add a per-token latent language variable to the polylingual topic model. For documents that are not code-switched, we observe these variables to be the output of a document level LID system. In the case of code-switched documents, these variables are inferred during model inference. Second, polylingual topic models assume the aligned topics are from parallel or comparable corpora, which implicitly assumes that a topics popularity is balanced across languages. Topics that show up in one language necessarily show up in"
P14-2110,D08-1102,0,0.148927,"n-aligned documents 674 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English c"
P14-2110,C12-1102,0,0.105472,"23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a transl"
P14-2110,D08-1110,0,0.569987,"n-aligned documents 674 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English c"
P14-2110,li-etal-2012-mandarin,0,0.0123651,"s. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task."
P14-2110,W12-6303,0,0.0252993,"r Computational Linguistics (Short Papers), pages 674–679, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Code-Switching both conversations and messages. In the model presentation we will refer to both as “documents.” Code-switched documents has received considerable attention in the NLP community. Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982). Code-switching specifically in social media has also received some recent attention. Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors. Ling et al. (2013)"
P14-2110,P13-1018,0,0.289082,"comparable documents extracted from Wikipedia in different languages – fail to capture the specific terminology of social media. Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daum´e, 2010) similarly fail to adapt to shifting vocabularies. The result: an inability to train polylingual models on social media. In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages. Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message (Ling et al., 2013). This mixture of languages in the same context suggests alignments between words across languages through the common topics discussed in the context. We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics. Our model improves both in terms of perplexity and a human evaluation, and we provide some example analyses of social media that rely on our learned topics. Introduction Topic models (Blei et al., 2003) have become standard tools for analyz"
P14-2110,D09-1092,0,0.0740528,"Missing"
P14-2110,C98-1002,0,\N,Missing
P14-2110,D10-1124,0,\N,Missing
P14-2110,Q14-1007,0,\N,Missing
P15-2062,D12-1032,1,0.598705,"due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several datasets to support further work on this task.1 Methods for name matching, an important component to support downstream tasks such as entity linking and entity clustering, have focuse"
P15-2062,W04-3248,0,0.113265,"Missing"
P15-2062,N03-1003,0,0.170748,"clustering task. 1 Introduction A key technique in entity disambiguation is name matching: determining if two mention strings could refer to the same entity. The challenge of name matching lies in name variation, which can be attributed to many factors: nicknames, aliases, acronyms, and differences in transliteration, among others. In light of these issues, exact string match can lead to poor results. Numerous downstream tasks benefit from improved name matching: entity coreference (Strube et al., 2002), name transliteration (Knight and Graehl, 1998), identifying names for mining paraphrases (Barzilay and Lee, 2003), entity linking (Rao et al., 2013) and entity clustering (Green et al., 2012). As a result, there have been numerous proposed name matching methods (Cohen et al., 2003), with a focus on person names. Despite extensive exploration of this task, most work has focused on IndoEuropean languages in general and English in particular. These languages use alphabets as representations of written language. In contrast, other languages use logograms, which represent a word 2 Name Matching Methods Name matching originated as part of research into record linkage in databases. Initial work focused 1 The co"
P15-2062,D07-1020,0,0.0367092,"0.05) Features ALL - Jaccard similariy - Levenshtein - Simplified pairs - Pinyin pairs - Others Exact match Jaro-winkler Levenshtein Transducer SVM Precision 84.55 84.87 83.16 90.33 90.05 Dev Recall 57.46 58.35 61.13 74.92 63.90 F1 68.42 69.15 70.46 81.90 74.75 Precision 63.95 70.79 69.56 73.59 74.33 Test Recall 65.44 66.21 67.27 63.70 67.60 F1 64.69 68.42 68.40 68.29 70.81 Table 7: Results on Chinese entity clustering. (cross document coreference resolution), where the goal is identify co-referent named mentions across documents. Only a few studies have considered Chinese entity clustering (Chen and Martin, 2007), including the TAC KBP shared task, which has included clustering Chinese NIL mentions (Ji et al., 2011). We construct an entity clustering dataset from the TAC KBP entity linking data. All of the 2012 Chinese data is used as development, and the 2013 data as test. We use the system of Green et al. (2012), which allows for the inclusion of arbitrary name matching metrics. We follow their setup for training and evaluation (B3 ) and use TF-IDF context features. We tune the clustering cutoff for their hierarchical model, as well as the name matching threshold on the development data. For the tra"
P15-2062,W10-4152,0,0.186548,"chnology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 2 Machine Intelligence and Translation Lab Harbin Institute of Technology, Harbin, China npeng1@jhu.edu, gflfof@gmail.com, mdredze@cs.jhu.edu Abstract or morpheme, the most popular being Chinese which uses hanzi (汉字). This presents challenges for name matching: a small number of hanzi represent an entire name and there are tens of thousands of hanzi in use. Current methods remain largely untested in this setting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in deter"
P15-2062,W12-4511,0,0.0715765,"Missing"
P15-2062,P14-2102,1,0.856086,"antage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al.,"
P15-2062,D08-1113,0,0.0282315,"t al., 2003). The advantage of trained models is that, with sufficient training data, they can be tuned for specific tasks. While many NLP tasks rely on name matching, research on name matching techniques themselves has not been a major focus within the NLP community. Most downstream NLP systems have simply employed a static edit distance module to decide whether two names can be matched (Chen et al., 2010; Cassidy et al., 2011; Martschat et al., 2012). An exception is work on training finite state transducers for edit distance metrics (Ristad and Yianilos, 1998; Bouchard-Cˆot´e et al., 2008; Dreyer et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al."
P15-2062,I11-1029,0,0.438392,"r et al., 2008; Cotterell et al., 2014). More recently, Andrews et al. (2012) presented a phylogenetic model of string variation using transducers that applies to pairs of names string (supervised) and unpaired collections (unsupervised). Beyond name matching in a single language, several papers have considered cross lingual name matching, where name strings are drawn from two different languages, such as matching Arabic names (El-Shishtawy, 2013) with English (Freeman et al., 2006; Green et al., 2012). Additionally, name matching has been used as a component in cross language entity linking (McNamee et al., 2011a; McNamee et al., 2011b) and cross lingual entity clustering (Green et al., 2012). However, little work has focused on logograms, with the exception of Cheng et al. (2011). As we will demonstrate in § 3, there are special challenges caused by the logogram nature of Chinese. We believe this is the first evaluation of Chinese name matching. 3 Notes simplified v.s. traditional Abbreviation and traditional v.s. simplified Transliteration of Addis Ababa in Mainland and Taiwan. Different hanzi, similar pronunciations. Transliteration of Florence in Mainland and Hong Kong. Different writing and dial"
P15-2062,J01-4004,0,0.395596,"ting, despite downstream tasks in Chinese that rely on name matching (Chen et al., 2010; Cassidy et al., 2011). Martschat et al. (2012) point out errors in coreference resolution due to Chinese name matching errors, which suggests that downstream tasks can benefit from improvements in Chinese name matching techniques. This paper presents an analysis of new and existing approaches to name matching in Chinese. The goal is to determine whether two Chinese strings can refer to the same entity (person, organization, location) based on the strings alone. The more general task of entity coreference (Soon et al., 2001), or entity clustering, includes the context of the mentions in determining coreference. In contrast, standalone name matching modules are context independent (Andrews et al., 2012; Green et al., 2012). In addition to showing name matching improvements on newly developed datasets of matched Chinese name pairs, we show improvements in a downstream Chinese entity clustering task by using our improved name matching system. We call our name matching tool Mingpipe, a Python package that can be used as a standalone tool or integrated within a larger system. We release Mingpipe as well as several dat"
P15-2062,W02-1040,0,0.0355989,"Missing"
P15-2062,C10-1145,0,0.0334396,"atching. We consider two new pinyin representations. Since each Chinese character corresponds to a pinyin, we take each pinyin as a token corresponding to the Chinese character. We call this “character-pinyin”. Additionally, every Mandarin syllable (represented by a pinyin) can be spelled with a combination of an initial and a final segment. Therefore, we split each pinyin token further into the initial and final segment. We call this “segmented-pinyin”5 . Name Matching as Classification An alternate learning formulation considers name matching as a classification task (Mayfield et al., 2009; Zhang et al., 2010; Green et al., 2012). Each string pair is an instance: a positive classification means that two strings can refer to the same name. This allows for arbitrary and global features of the two strings. We use an SVM with a linear kernel. To learn possible edit rules for Chinese names we add features for pairs of n-grams. For each string, we extract all n-grams (n=1,2,3) and align n-grams between strings using the Hungarian algorithm.6 Features correspond to the aligned ngram pairs, as well as the unaligned n-grams. To reduce the number of parameters, we only include features which appear in posit"
P15-2062,N12-1007,1,\N,Missing
P15-2062,J98-4003,0,\N,Missing
P15-2067,W07-1424,0,0.157149,"Missing"
P15-2067,ferrandez-etal-2010-aligning,0,0.118792,"Missing"
P15-2067,P13-2130,0,0.15027,"Missing"
P15-2067,N13-1092,1,0.691494,"Missing"
P15-2067,P10-2045,0,0.033468,"Missing"
P15-2067,J02-3001,0,0.237857,"Missing"
P15-2067,P98-1013,0,0.13149,"Missing"
P15-2067,W10-0735,0,0.029492,"Missing"
P15-2067,P13-2121,0,0.0542521,"Missing"
P15-2067,P14-1136,0,0.035095,"Missing"
P15-2067,D08-1021,1,0.826454,"Missing"
P15-2067,W10-0907,0,0.0542799,"Missing"
P15-2067,N03-2022,0,0.0541534,"Missing"
P15-2067,P11-1144,0,0.0227914,"Missing"
P15-2067,C10-2107,0,0.0853446,"Missing"
P15-2067,N12-1086,0,0.098033,"Missing"
P15-2067,D08-1048,0,0.205318,"Missing"
P15-2067,W14-2901,1,0.897335,"Missing"
P15-2067,D07-1002,0,0.049493,"Missing"
P15-2067,C98-1013,0,\N,Missing
P16-2003,E14-1049,0,0.0187937,"Missing"
P16-2003,N15-1058,1,0.498274,"Missing"
P16-2003,P14-1018,0,0.0158107,"Missing"
P16-2003,P12-3005,0,0.0198068,"e several different types of data (views) we can use to build user representations: the text of messages they post, neighbors in their local network, articles they link to, images they upload, etc. We propose unsupervised learning of representations of users with a variant of Generalized Canonical Correlation Analysis (GCCA) (Carroll, 1968; Van De Velden and Bijmolt, 2006; Arora and Livescu, 2014; Rastogi et al., 2015), a multiview technique that learns a single, low-dimensional vector for each user best capturing information from each of their views. We believe this 1 Identified with langid (Lui and Baldwin, 2012). 14 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 14–19, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics up to the 200 most recent English tweets for these users from January and February 2015. Limits on the number of users and tweets per user were imposed so that we could operate within Twitter’s API limits. This data supports several of our prediction tasks, as well as the four sources for each user: their tweets, tweets of mentioned users, friends and followers. 3 stream in April 2015 along with all the t"
P16-2025,H05-1091,0,0.270977,"Missing"
P16-2025,D15-1141,0,0.569186,"Weston, 2008; Turian et al., 2010; Passos et al., 2014) In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008). Peng and Dredze (2015) showed the value of word segmentation to Chinese NER in social media by using character positional embeddings, which encoded word segmentation information. In this paper, we investigate better ways to incorporate word boundary information into an NER system for Chinese social media. We combine the state-of-the-art Chinese word segmentation system (Chen et al., 2015) with the best Chinese social media NER model (Peng and Dredze, 2015). Since both systems used learned representations, we propose an integrated model that allows for joint training learned representations, providing more information to the NER system about hidden representations learned from word segmentation, as compared to features based on segmentation output. Our integrated model achieves nearly Named entity recognition, and other information extraction tasks, frequently use linguistic features such as part of speech tags or chunkings. For languages where word boundaries are not readily i"
P16-2025,N15-1075,0,0.0191101,"Missing"
P16-2025,W99-0613,0,0.0410919,"Missing"
P16-2025,C10-1032,1,0.717561,"Missing"
P16-2025,W12-6307,0,0.124244,"Missing"
P16-2025,P12-1055,0,0.0341318,"Missing"
P16-2025,W10-0713,1,0.625542,"Missing"
P16-2025,P16-1101,0,0.199744,"Missing"
P16-2025,fromreide-etal-2014-crowdsourcing,0,0.0182721,"Missing"
P16-2025,I08-4013,0,0.161313,"Missing"
P16-2025,W03-0430,0,0.0659674,"Missing"
P16-2025,J05-4005,0,0.0479136,"Missing"
P16-2025,W12-6321,0,0.0509478,"Missing"
P16-2025,W14-1609,0,0.0497518,"Missing"
P16-2025,D15-1064,1,0.774481,"e@cs.jhu.edu Abstract other formal domains. While this gap is shrinking in English (Ritter et al., 2011; Cherry and Guo, 2015), it remains large in other languages, such as Chinese (Peng and Dredze, 2015; Fu et al., 2015). One reason for this gap is the lack of robust up-stream NLP systems that provide useful features for NER, such as part-of-speech tagging or chunking. Ritter et al. (2011) annotated Twitter data for these systems to improve a Twitter NER tagger, however, these systems do not exist for social media in most languages. Another approach has been that of Cherry and Guo (2015) and Peng and Dredze (2015), who relied on training unsupervised lexical embeddings in place of these upstream systems and achieved state-of-the-art results for English and Chinese social media, respectively. The same approach was also found helpful for NER in the news domain (Collobert and Weston, 2008; Turian et al., 2010; Passos et al., 2014) In Asian languages like Chinese, Japanese and Korean, word segmentation is a critical first step for many tasks (Gao et al., 2005; Zhang et al., 2006; Mao et al., 2008). Peng and Dredze (2015) showed the value of word segmentation to Chinese NER in social media by using characte"
P16-2025,I08-4010,0,0.0359614,"Missing"
P16-2025,P11-1138,0,0.0608049,"Missing"
P16-2025,N16-1030,0,0.374874,"Missing"
P16-2025,D11-1141,0,0.162071,"Missing"
P16-2025,P10-1040,0,0.110457,"Missing"
P16-2025,N15-1069,0,0.0128449,"on data is from the news domain, whereas the NER data is from social media. While it is well known that segmentation systems trained on news do worse on social media (Duan et al., 2012), we still show large improvements in applying our model to these different domains. It may be that we are able to obtain better results in the case of domain mismatch because we integrate the representations of the LSTM model directly into our CRF, as opposed to only using the predictions of the LSTM segmentation model. We plan to consider expanding our model to explicitly include domain adaptation mechanisms (Yang and Eisenstein, 2015). Discussion Huang et al. (2015) first proposed recurrent neural networks stacked with a CRF for sequential tagging tasks, as was applied to POS, chunking and NER tasks. More recent efforts have been made to add character level modeling and explore different types of RNNs (Lample et al., 2016; Ma and Hovy, 2016; Yang et al., 2016). These methods have achieved state-of-the-art results for NER on English news and several other Indo-European languages. However, this work has not considered languages that require word segmentation, nor do they consider social media. We can view our method as multi"
P16-2025,P11-1037,0,0.0512676,"Missing"
P16-2025,W06-0126,0,0.164592,"Missing"
P16-2025,D13-1061,0,0.0421534,"Missing"
P16-2025,E17-2113,0,\N,Missing
P17-1095,D12-1075,0,0.0258911,"abilities in Hy are easy to compute because Hy has the form of a language model, and prefix probabilities in Py are therefore also easy to compute (using a prefix tree for efficiency). This concludes the description of the segmental sampler. Note that the particle Gibbs procedure is unchanged. 4 Inducing parts-of-speech with type-level supervision Automatically inducing parts-of-speech from raw text is a challenging problem (Goldwater et al., 2005). Our focus here is on the easier problem of type-supervised part-of-speech induction, in which (partial) dictionaries are used to guide inference (Garrette and Baldridge, 2012; Li et al., 2012). Conditioned on the unlabeled corpus and dictionary, we use the MCMC procedure described in §3.1 to impute the latent parts-of-speech. Since dictionaries are freely available for hundreds of languages,6 we see this as a mild additional requirement in practice over the purely unsupervised setting. In prior work, dictionaries have been used as constraints on possible parts-of-speech: words appearing in the dictionary take one of their known parts1034 6 https://www.wiktionary.org/ of-speech. In our setting, however, the dictionaries are not constraints but evidence. If monthly"
P17-1095,P07-1094,0,0.0518943,"d with this but did not observe any consistent advantage to doing so in our setting. 4 The label sequence is terminated by a distinguished endof-sequence label, again written as $. 1031 x1:T are then generated conditioned on the label sequence via the corresponding Py distribution (defined in §2.3). All observations with the same label y are drawn from the same Py , and thus this subsequence of observations is distributed according to the Chinese restaurant process (1). We model y using another sequence memoizer model. This is similar to other hierarchical Bayesian models of latent sequences (Goldwater and Griffiths, 2007; Blunsom and Cohn, 2010), but again, it does not limit the Markov order (the number of preceding labels that are conditioned on). Thus, the probability of a sequence of latent types is computed in the same way as the base distribution in §2.4, that is, p(y1:T ) := T Y t=1  Gy1:t−1 (yt ) Gy1:T ($) (4) where Gv (y) denotes the conditional probability of latent label y ∈ Y given the left context v ∈ Y ∗ . Each Gv is a distribution over Y, defined recursively as G ∼ PYP(d , α , UY ) (5) Gv ∼ PYP(d|v |, α|v |, Gσ(v) ) The probability of transitioning to label yt depends on the assignments of"
P17-1095,U11-1004,0,0.0151687,"r (4).  The emission distribution p(xt |Yt = y) depends on the emissions observed from any earlier tokens of y, because of the Chinese restaurant process (1). When  is the only complication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnorma"
P17-1095,P11-1061,0,0.0282037,"section, we are interested in evaluating our proposed Bayesian model in the context of low-resource NER. Experiments We follow the experimental procedure described in Li et al. (2012), and use their released code and data to compare to their best model: a second-order maximum entropy Markov model parametrized with log-linear features (SHMM - ME). This model uses hand-crafted features designed to distinguish between different parts-of-speech, and it has special handling for rare words. This approach is surprisingly effective and outperforms alternate approaches such as cross-lingual transfer (Das and Petrov, 2011). However, it also has limitations, since words that do not appear in the dictionary will be unconstrained, and spurious or incorrect lexical entries may lead to propagation of errors. The lexicons are taken from the Wiktionary project; their size and coverage are documented by (Li et al., 2012). We evaluate our model on multi-lingual data released as part of the CoNLL 2007 and CoNLL-X shared tasks. In particular, we use the same set of languages as Li et al. (2012).7 For our method, we impute the parts-of-speech by running particle Gibbs for 100 epochs, where one epoch consists of resampling"
P17-1095,D11-1057,1,0.836785,"pes to appear ≥ c times in an unbounded sequence of IID draws from Py . When c = 1, this is equivalent to modeling the lexicon as my draws without replacement from Py .2 Unfortunately, draws without replacement are no longer IID or exchangeable: order matters. It would therefore become difficult to condition inference and learning on an observed lexicon, because we would need to explicitly sum or sample over the possibilities for the latent sequence of tokens (or stick segments). We therefore adopt the simpler deficient model. A version of our lexicon model (with c = 1) was previously used by Dreyer and Eisner (2011, Appendix C), who observed a list of verb paradigm types rather than word or entity-name types. 2.3 Prior distribution over Py We assume a priori that Py was drawn from a Pitman-Yor process (PYP) (Pitman and Yor, 1997). Both the lexicon and the ordinary corpus are observations that provide information about Py . The PYP is defined by three parameters: a concentration parameter α, a discount parameter d, and a base distribution Hy . In our case, Hy is a distribution over X = Σ∗ , the set of possible strings over a finite character alphabet Σ. For example, HLOC is used to choose new place names"
P17-1095,W12-1907,0,0.0155307,"mplication, block Metropolis-Hastings samplers have proven effective (Johnson et al., 2007). However, this approach uses dynamic programming to sample from a proposal distribution efficiently, which ¬ precludes in our case. Instead, we use sequential Monte Carlo (SMC)—sometimes called particle filtering—as a proposal distribution. Particle filtering is typically used in online settings, including word segmentation (Borschinger and Johnson, 2011), to make decisions before all of x has been observed. However, we are interested in the inference (or smoothing) problem that conditions on all of x (Dubbin and Blunsom, 2012; Tripuraneni et al., 2015). SMC employs a proposal distribution q(y |x) 1032 whose definition decomposes as follows: q(y1 |x1 ) T Y t=2 q(yt |y1:t−1 , x1:t ) (6) for T = |x|. To sample a sequence of latent labels, first sample an initial label y1 from q1 , then proceed incrementally by sampling yt from qt (· |y1:t−1 , x1:t ) for t = 2, . . . , T . The final sampled sequence y is called a particle, and is given an unnormalized importance weight of w ˜=w ˜T · p($ |y1:T ) where w ˜T was built up via w ˜t := w ˜t−1 · p(y1:t , x1:t ) p(y1:t−1 , x1:t−1 ) q(yt |y1:t−1 , x1:t ) (7) The SMC procedure"
P17-1095,P05-1045,0,0.0620648,"Missing"
P17-1095,D12-1127,0,0.0333437,"Missing"
P17-1095,P09-1012,0,0.0338895,"that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation scheme, we also observe a lexic"
P17-1095,P05-1003,0,0.0357877,"we use lexical resources to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon theref"
P17-1095,W06-2918,0,0.0369595,"f-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves training of the spelling featu"
P17-1095,L16-1521,0,0.0289298,"not fully documented. 5 5.1 Boostrapping NER with type-level supervision Data Most languages do not have corpora annotated for parts-of-speech, named-entities, syntactic parses, or other linguistic annotations. Therefore, rapidly deploying natural language technologies in a new language may be challenging. In the context of facilitating relief responses in emergencies such as natural disasters, the DARPA LORELEI (Low Resource Languages for Emergent Incidents) program has sponsored the development and release of representative “language packs” for Turkish and Uzbek with more languages planned (Strassel and Tracey, 2016). We use the named-entity annotations as part of these language packs which include persons, locations, organizations, and geo-political entities, in order to explore bootstrapping named-entity recognition from small amounts of data. We consider two types of data: ¬ in-context annotations, where sentences are fully annotated for named-entities, and  lexical resources. The LORELEI language packs lack adequate indomain lexical resources for our purposes. Therefore, we simulate in-domain lexical resources by holding out portions of the annotated development data and deriving dictionaries and nam"
P17-1095,N06-1012,0,0.0356646,"urces to guide part-of-speech induction (§4) and to bootstrap named-entity recognizers in low-resource languages (§5). Given their success, it is perhaps surprising that incorporating gazetteers or dictionaries into discriminative models (e.g. conditional random fields) may sometimes hurt performance. This phenomena is called weight under-training, in which lexical features—which detect whether a name is listed in the dictionary or gazetteer—are given excessive weight at the expense of other useful features such as spelling features that would generalize to unlisted names (Smith et al., 2005; Sutton et al., 2006; Smith and Osborne, 2006). Furthermore, discriminative training with lexical features requires sufficient annotated training data, which poses challenges for the unsupervised and low-resource settings we consider here. Our observation is that Bayesian modeling provides a principled solution. The lexicon is itself a dataset that was generated by some process. Practically, this means that lexicon entries (words or phrases) may be treated as additional observations. As a result, these entries provide information about how names are spelled. The presence of the lexicon therefore now improves trai"
P17-1095,P06-1124,0,0.0429109,"ownside is that generative models are typically less feature-rich than their globally normalized discriminative counterparts (e.g. conditional random fields). In designing our approach—the hierarchical sequence memoizer (HSM)—we aim to be reasonably expressive while retaining practically useful inference algorithms. We propose a Bayesian nonparametric model to serve as a generative distribution responsible for both lexicon and corpus data. The proposed model memoizes previously used lexical entries (words or phrases) but backs off to a character-level distribution when generating novel types (Teh, 2006; Mochihashi et al., 2009). We propose an efficient inference algorithm for the proposed model using particle Gibbs sampling (§3). Our code is available at https://github.com/noa/bayesner. 1029 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1029–1039 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1095 2 Model Our goal is to fit a model that can automatically annotate text. We observe a supervised or unsupervised training corpus. For each label y in the annotation schem"
P17-1095,P11-1087,0,\N,Missing
P17-1095,N07-1018,0,\N,Missing
P17-2048,D11-1142,0,0.0600668,"his because they create entities on the fly. We use annotated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call th"
P17-2048,P15-1034,0,0.0852611,"ate entities on the fly. We use annotated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call these “explanations” whe"
P17-2048,P98-1012,0,0.0619664,"cker.com/r/hltcoe/pocket-knowledge-basepopulation 305 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 305–310 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2048 Figure 1: High level steps of the PKB construction process. Example PKBs can be found in Table 3. The first two steps of initial search and related entities are described in §3.1, the third step of joint searching in §3.2, and finally extracting triggers in §3.3. we adopt the vector space model (Bagga and Baldwin, 1998). First, triage features are used to locate sentences in an inverted index, including the mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the dista"
P17-2048,W03-0405,0,0.103607,"e mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the distance in sentences to the nearest mention. Our attribute features are a generalization of Mann and Yarowsky (2003). They train a few ad-hoc relation extractors like birth year and occupation from seed facts. Their extractions provide high-precision signal for merging entity mentions. We found extracting all NNP* or capitalized JJ* words within 4 edges in a dependency tree was less sparse, requires no seeds, and produced similar quality attributes. We union these attributes across mentions found by the headword and NER type coreference heuristic to build a 2 fine-grain tf-idf vector. We use the same 1+d re-weighting for attributes, except where d is the distance in dependency edges to the entity mention he"
P17-2048,D12-1048,0,0.230246,"notated versions of Gigaword 5 (Parker et al., 2011; Ferraro et al., 2014) and English Wikipedia (February 24, 2016 dump) to construct our PKBs.4 We use Amazon Mechanical Turk workers as annotators. We generated our PKBs with τ = 15, ρ = 0.5, αt = 40, αc = 20, and αa = 10. These constants were tuned by hand Trigger Word Analysis At this stage we have found on the order of 2 to 20 sentences which mention the query and a related entity which will be used to determine the relation between them. There is work on rule-based (Banko et al., 2007; Fader et al., 2011; Angeli et al., 2015), supervised (Mausam et al., 2012), 2 Mentions with a score near τ may be coreferent, so we prefer low scoring mentions to avoid over-splitting entities. 3 These values depend on the query (which are more or less rare in a corpus) and pruning thresholds (for our experiments we stop at 100 query mentions) 4 We do not use the coreference annotations provided by Annotated Gigaword, only the features described in §3.1. 307 and are not sensitive to small changes. We take a subset of the PKB which covers the 15 most related entities and the one-best trigger for each. We call these “explanations” where each is a sentence with three l"
P17-2048,P09-1113,0,0.07802,"ly consider the subset of mentions that have cos θt &gt; 0, which can be efficiently retrieved via an inverted index. Any mention with a score higher than τ is considered coreferent with the query. We extract mentions in the same sentences as the query as candidate related entities if they have an NER type of PER, ORG, or LOC. We link candidate mentions against entities in the PKB using the same coreference score used to retrieve query mentions. If a candidate’s best link has a score s < τ , we promote it to an entity and add it to the PKB with probability 1 − τs .2 3.2 and distantly-supervised (Mintz et al., 2009) methods for characterizing relations in text. Our method is similar to distant supervision, where a KB of known facts is used to infer how relations are expressed, but we use supervision from the KB being constructed. We cast the problem of characterizing a relation as a search for trigger words. We state our priors on trigger words and condition on the data to find likely triggers. Predicate (triggers) and arguments are syntactically close together. Assuming the related entity mention heads are arguments, we compute the probability that these two random walks in a dependency tree end up at t"
P17-2048,D07-1074,0,0.0250731,"P17-2048 Figure 1: High level steps of the PKB construction process. Example PKBs can be found in Table 3. The first two steps of initial search and related entities are described in §3.1, the third step of joint searching in §3.2, and finally extracting triggers in §3.3. we adopt the vector space model (Bagga and Baldwin, 1998). First, triage features are used to locate sentences in an inverted index, including the mention headword and all word unigrams and bigrams (case sensitive and insensitive). Next, we use context features: a word unigram tf-idf vector. We implement a compromise between Cucerzan (2007) (used sentences before, after, and containing a mention) and Bagga and Baldwin (1998) (used any sentence in a coreference chain). Instead of running a coreference resolver, we use the high-precision heuristic of linking mentions with the same headword and NER 2 type. Terms are weighted as 1+d where d is the distance in sentences to the nearest mention. Our attribute features are a generalization of Mann and Yarowsky (2003). They train a few ad-hoc relation extractors like birth year and occupation from seed facts. Their extractions provide high-precision signal for merging entity mentions. We"
Q15-1004,D11-1024,0,0.680152,"l to jointly infer topic hierarchies and author perspective, which we apply to corpora of political debates and online reviews. We show that the model learns intuitive topics, outperforming several other topic models at predictive tasks. 1 Introduction Topic models can be a powerful aid for analyzing large collections of text by uncovering latent interpretable structures without manual supervision. Yet people often have expectations about topics in a given corpus and how they should be structured for a particular task. It is crucial for the user experience that topics meet these expectations (Mimno et al., 2011; Talley et al., 2011) yet black box topic models provide no control over the desired output. This paper presents S PRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and Mc"
Q15-1004,N13-1017,1,0.914432,"the desired output. This paper presents S PRITE, a family of topic models that provide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). S PRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existin"
Q15-1004,D10-1007,1,0.813793,"cific word distributions. This is an alternative to the more common approach to regression based topic modeling, where the variables affect the topic distributions rather than the word distributions. Our S PRITE-based model does both: the document features adjust the prior over topic distributions (through δ), but by tying together the document and topic components (with β), the document features also affect the prior over word distributions. To the best of our knowledge, this is the first topic model to condition both topic and word distributions on the same features. The topic aspect model (Paul and Girju, 2010a) is also a two-dimensional factored model that has been used to jointly model topic and perspective (Paul and Girju, 2010b). However, this model does not use structured priors over the parameters, unlike most of the models discussed in §4. An alternative approach to incorporating user preferences and expertise are interactive topic models (Hu et al., 2013), a complimentary approach to S PRITE. 9 Discussion and Conclusion We have presented S PRITE, a family of topic models that utilize structured priors to induce preferred topic structures. Specific instantiations of S PRITE are similar or eq"
Q15-1004,D09-1026,0,0.169968,"rovide a flexible framework for encoding preferences as priors for how topics should be structured. S PRITE can incorporate many types of structure that have been considered in prior work, including hierarchies (Blei et al., 2003a; Mimno et al., 2007), factorizations (Paul and Dredze, 2012; Eisenstein et al., 2011), sparsity (Wang and Blei, 2009; Balasubramanyan and Cohen, 2013), correlations between topics (Blei and Lafferty, 2007; Li and McCallum, 2006), preferences over word choices (Andrzejewski et al., 2009; Paul and Dredze, 2013), and associations between topics and document attributes (Ramage et al., 2009; Mimno and McCallum, 2008). S PRITE builds on a standard topic model, adding structure to the priors over the model parameters. The priors are given by log-linear functions of underlying components (§2), which provide additional latent structure that we will show can enrich the model in many ways. By applying particular constraints and priors to the component hyperparameters, a variety of structures can be induced such as hierarchies and factorizations (§3), and we will show that this framework captures many existing topic models (§4). After describing the general form of the model, we show h"
Q15-1004,P06-1072,0,0.0304323,"onte Carlo EM approach, using a tial derivative τt ρ−1 βkc , which adds extra weight to collapsed Gibbs sampler to sample from the posthe sparse Dirichlet prior in the objective. The terior of the topic assignments z conditioned on algorithm used in our experiments begins with the hyperparameters, then optimizing the hyperpaτ1 = 1 and optionally increases τ over time. This rameters using gradient-based optimization condiis a deterministic annealing approach, where τ tioned on the samples. corresponds to an inverse temperature (Ueda and Given the hyperparameters, the sampling equaNakano, 1998; Smith and Eisner, 2006). tions are identical to the standard LDA sampler As τ approaches infinity, the prior-annealed (Griffiths and Steyvers, 2004). The partial derivaMAP objective maxβ P (φ|β)P (β)τ approaches tive of the collapsed log likelihood L of the corpus maxβ P (φ|β) maxβ P (β). Annealing only the with respect to each hyperparameter βkc is: prior P (β) results in maximization of this term only, while the outer max chooses a good β under ∂L ∂P (β) X = + ωcv φ˜kv × (1) P (φ|β) as a tie-breaker among all β values that ∂βkc ∂βkc   v P P 0 maximize the inner max (binary-valued β).3 k k Ψ(nv + φ˜kv ) −Ψ(φ˜kv )"
Q15-1004,N12-1096,1,\N,Missing
Q15-1004,P11-1026,0,\N,Missing
Q15-1017,D10-1115,0,0.366509,"word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small"
Q15-1017,P14-1063,0,0.0883156,"q. (1) to predict a label y, the score of y given phrase p will be s(y, p) = PN T UyT ep = i Uy (λi ewi ) in log-linear models, where Uy is the parameter vector for y. This is equivalent to using a parameter tensor T to evaluate P T ×1 y ×2 f (wi , p) × the score with s0 (y, p) = N i ewi , while forcing the tensor to have a low-rank form as T ≈ U ⊗ α ⊗ ew . Here ×k indicates tensor multiplication of the kth view, and ⊗ indicates matrix outer product (Kolda and Bader, 2009). From this point of view, our work is closely related to the discriminative training methods for low-rank tensors in NLP (Cao and Khudanpur, 2014; Lei et al., 2014), while it can handle more complex ngram-to-ngram tasks, where the label y also has its embedding composed from basic word embeddings. Therefore our model can capture the above work as special cases. Moreover, we have a different method of decomposing the inputs, which results in views of lexical parts and non-lexical features. As we show in this paper, this input decomposition allows us to benefit from pre-trained word embeddings and feature weights. 8 Conclusion We have presented FCT, a new composition model for deriving phrase embeddings from word embed240 dings. Compared"
Q15-1017,P13-1039,0,0.0129296,"plying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with othe"
Q15-1017,P14-1059,0,0.0145947,"associated with one word and the embedding of the other, producing a new embedding for the phrase. Using one tensor (not word-specific) to compose two embedding vectors (has not been tested on phrase similarity tasks) (Bordes et al., 2014; Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not b"
Q15-1017,D10-1113,0,0.0240476,"Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and one of its context words; others have considered extensions to multiple words (Grefenstette et al., 2013; Dinu and Baroni, 2014). The primary drawback of these approaches is the high computational complexity, limiting their usefulness for semantics (Section 6.2.) A second approach draws on the concept of contextualization (Erk and Pad´o, 2008; Dinu and Lapata, 2010; Thater et al., 2011), which sums embeddings of multiple words in a linear combination. For example, Cheung and Penn (2013) apply contextualization to word compositions in a generative event extraction model. However, this is an indirect way to capture interactions (the transformations are still unaware of interactions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer ling"
Q15-1017,D08-1094,0,0.0614568,"Missing"
Q15-1017,W13-0109,0,0.0131256,"se the input of the model is the concatenation of word representations, matrix transformations cannot capture interactions between a word and its contexts, or between component words. There are three ways to restore these interactions: The first is to use word-specific/tensor transformations to force the interactions between component words in a phrase. In these methods, wordspecific transformations, which are usually matrices, are learned for a subset of words according to their syntactic properties (e.g. POS tags) (Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013; Erk, 2013). Composition between a word in this subset and another word becomes the multiplication between the matrix associated with one word and the embedding of the other, producing a new embedding for the phrase. Using one tensor (not word-specific) to compose two embedding vectors (has not been tested on phrase similarity tasks) (Bordes et al., 2014; Socher et al., 2013b) is a special case of this approach, where a “wordspecific transformation matrix” is derived by multiplying the tensor and the word embedding. Additionally, word-specific matrices can only capture the interaction between a word and"
Q15-1017,N13-1092,0,0.0338236,"Missing"
Q15-1017,W13-0112,0,0.0596645,"phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small task-specific datasets. An exception is the unsu"
Q15-1017,P13-1088,0,0.234528,"state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use s"
Q15-1017,P14-1136,0,0.0253992,"rning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on p"
Q15-1017,P12-1092,0,0.0873224,"ficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication,"
Q15-1017,S13-2007,0,0.0186772,"igram phrase, i.e., the input phrase or the swapped input (“word monosyllabic” for this example), and the five output candidates. The correct answer in this case should still be the pair of original input phrase and the original correct output candidate (in bold). (4) PPDB (ngram) is similar to PPDB, but in which both inputs and outputs becomes noun phrases with arbitrary lengths. and Dredze, 2014) to that between phrases. Data details appear in Table 2. Phrase Similarity Datasets We use a variety of human annotated datasets to evaluate phrase semantic similarity: the SemEval2013 shared task (Korkontzelos et al., 2013), and the noun-modifier problem (Turney2012) in Turney (2012). Both tasks provide evaluation data and training data. SemEval2013 Task 5(a) is a classification task to determine if a word phrase pair are semantically similar. Turney2012 is a task to select the closest matching candidate word for a given phrase from candidate words. The original task contained seven candidates, two of which are component words of the input phrase (seven-choice task). Followup work has since removed the components words from the candidates (five-choice task). Turney (2012) also propose a 10-choice task based on t"
Q15-1017,P14-1130,0,0.0281039,"y, the score of y given phrase p will be s(y, p) = PN T UyT ep = i Uy (λi ewi ) in log-linear models, where Uy is the parameter vector for y. This is equivalent to using a parameter tensor T to evaluate P T ×1 y ×2 f (wi , p) × the score with s0 (y, p) = N i ewi , while forcing the tensor to have a low-rank form as T ≈ U ⊗ α ⊗ ew . Here ×k indicates tensor multiplication of the kth view, and ⊗ indicates matrix outer product (Kolda and Bader, 2009). From this point of view, our work is closely related to the discriminative training methods for low-rank tensors in NLP (Cao and Khudanpur, 2014; Lei et al., 2014), while it can handle more complex ngram-to-ngram tasks, where the label y also has its embedding composed from basic word embeddings. Therefore our model can capture the above work as special cases. Moreover, we have a different method of decomposing the inputs, which results in views of lexical parts and non-lexical features. As we show in this paper, this input decomposition allows us to benefit from pre-trained word embeddings and feature weights. 8 Conclusion We have presented FCT, a new composition model for deriving phrase embeddings from word embed240 dings. Compared to existing phrase"
Q15-1017,P08-1028,0,0.0735639,"rian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts"
Q15-1017,W12-3018,0,0.0243723,"Missing"
Q15-1017,P14-2012,0,0.0363822,"actions between components), and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multi"
Q15-1017,D14-1045,0,0.0245382,", and thus has not been a popular choice for composition. The third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multiple features. Finally, ou"
Q15-1017,D11-1014,0,0.846936,"(2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tas"
Q15-1017,D12-1110,0,0.496571,"R) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition functions that rely on phrase structure and context. Other work on learning compositions relies on matrices/tensors as transformations (Socher et al., 2011; Socher et al., 2013a; Hermann and Blunsom, 2013; Baroni and Zamparelli, 2010; Socher et al., 2012; Grefenstette et al., 2013). However, this work suffers from two primary disadvantages. First, these methods have high computational complexity for dense embeddings: O(d2 ) or O(d3 ) for composing every two components with d dimensions. The high computational complexity restricts these methods to use very low-dimensional embeddings (25 or 50). While low-dimensional embeddings perform well for syntax (Socher et al., 2013a) and sentiment (Socher et al., 2013b) tasks, they do poorly on semantic tasks. Second, because of the complexity, they use supervised training with small task-specific datase"
Q15-1017,P13-1045,0,0.183107,"and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition"
Q15-1017,D13-1170,0,0.20677,"and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapata, 2008), e.g., component-wise sum/multiplication, we learn composition"
Q15-1017,I11-1127,0,0.0534138,"Missing"
Q15-1017,P10-1040,0,0.0701477,"t capture phrase structure and context. We propose efficient unsupervised and task-specific learning objectives that scale our model to large datasets. We demonstrate improvements on both language modeling and several phrase semantic similarity tasks with various phrase lengths. We make the implementation of our model and the datasets available for general use. 1 Introduction Word embeddings learned by neural language models (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013b) have been successfully applied to a range of tasks, including syntax (Collobert and Weston, 2008; Turian et al., 2010; Collobert, 2011) and semantics (Huang et al., 2012; Socher et al., 2013b; Hermann et al., 2014). However, phrases are critical for capturing lexical meaning for many tasks. For example, Collobert and Weston (2008) showed that word embeddings yielded state-of-the-art systems on word-oriented tasks (POS, NER) but performance on phrase oriented tasks, such as SRL, lags behind. We propose a new method for compositional semantics that learns to compose word embeddings into phrases. In contrast to a common approach to phrase embeddings that uses pre-defined composition operators (Mitchell and Lapa"
Q15-1017,P14-2089,1,0.58832,"gram noun phrases (obtained from the annotated data) as the input phrases for Eq. (3). A subset from January 1998 of NYT data is withheld for evaluation. We use “input embeddings” learned by word2vec. https://opennlp.apache.org/ 232 Results We evaluate the perplexity of language models that include lexical embeddings and our composed phrase embeddings from FCT using the LM objective. We use the perplexity computation method of Mikolov et al. (2013a) suitable for skip-gram models. The FCT models are trained by the HS strategy, which can output the exact probability efficiently and was shown by Yu and Dredze (2014) to obtain better performance on language modeling. Since in Section 6.1 we use FCT models trained by NCE, we also include the results of models trained by NCE. Note that scores obtained from a model trained with HS or NCE are not comparable. While the model trained by HS is efficient to evaluate perplexities, NCE training requires summation over all words in the vocabulary in the denominator of the softmax to compute perplexity, an impracticality for large vocabulary. Therefore, we report NCE loss with a fixed set of samples for NCE trained models. Model SUM (2 epochs) word2vec (2 epochs) FCT"
Q15-1017,N15-1155,1,0.853515,"third approach is to refine word-independent compositional transformations with annotation features. FCT falls under this approach. The primary advantage is that composition can rely on richer linguistic features from the context. While the embeddings of component words still cannot interact, they can interact with other information (i.e. features) of their context words, and even the global features. Recent research has created novel features based on combining word embeddings and contextual information (Nguyen and Grishman, 2014; Roth and Woodsend, 2014; Kiros et al., 2014; Yu et al., 2014; Yu et al., 2015). Yu et al. (2015) further proposed converting the contextual features into a hidden layer called feature embeddings, which is similar to the α matrix in this paper. Examples of applications to phrase semantics include Socher et al. (2013a) and Hermann and Blunsom (2013), who enhanced RNNs by refining the transformation matrices with phrase types and CCG super tags. However, these models are only able to use limited information (usually one property for each compositional transformation), whereas FCT exploits multiple features. Finally, our work is related to recent work on low-rank tensor app"
Q15-1017,C10-1142,0,0.0371531,"word representations. A traditional approach for composition is to form a point-wise combination of single word representations with compositional operators either pre-defined (e.g. elementwise sum/multiplication) or learned from data (Le and Mikolov, 2014). However, these approaches ignore the inner structure of phrases, e.g. the order of words in a phrase and its syntactic tree, and the point-wise operations are usually less expressive. One solution is to apply a matrix transformation (possibly followed by a non-linear transformation) to the concatenation of component word representations (Zanzotto et al., 2010). For longer phrases, 239 matrix multiplication can be applied recursively according to the associated syntactic trees (Socher et al., 2010). However, because the input of the model is the concatenation of word representations, matrix transformations cannot capture interactions between a word and its contexts, or between component words. There are three ways to restore these interactions: The first is to use word-specific/tensor transformations to force the interactions between component words in a phrase. In these methods, wordspecific transformations, which are usually matrices, are learned"
Q15-1035,P11-1048,0,0.0891706,"Missing"
Q15-1035,P14-1098,0,0.0954955,"Missing"
Q15-1035,W06-2920,0,0.068668,"mit. Finding this difficult to optimize, we introduce a new simpler objective function based on the L2 distance between the approximate marginals and the “true” marginals from the gold data. The goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. We compare our training method with the standard approach of conditional log-likelihood (CLL) training. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations. 2 Dependency Parsing by Belief Propagation This section describes the parser that we will train. Model A factor graph (Frey et al., 1997; Kschischang et al., 2001) defines the factorization of a probability distribution over a set of variables {Y1 , Y2 ,"
Q15-1035,N12-1004,0,0.165629,"Missing"
Q15-1035,D07-1101,0,0.868337,"pected loss on the true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyan"
Q15-1035,D09-1011,1,0.880326,"pproximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4 For the PT REE factor, the summation over vari(t) able assignments required for mα→i (yi ) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of Eisner (1996)’s algorithm 4 Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree of the factor graph rooted at the PT REE factor. We visit the nodes in topologically sorted order (from leaves to root) and update any message from the node being visited to a node that is later in the order. We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP. to compute this in polynomial time (we describe this as hypergraph parsing in §3). The resulting “structured BP” inference procedure—detailed by Smith and Eisner (2008)—is exact for first-order dependency par"
Q15-1035,P15-1030,0,0.0671212,"Missing"
Q15-1035,W05-1504,1,0.869335,"h (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token. On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009) (but we avoid pruning the fully right-branching tree, so that"
Q15-1035,C96-1058,1,0.765392,"alized such that yi bi (yi ) = 1 and approximates the marginal distribution over yi values. Messages continue to change indefinitely if the factor graph is cyclic, but in the limit, the messages may converge. Although the equations above update all messages in parallel, convergence is much faster if only one message is updated per timestep, in some well-chosen serial order.4 For the PT REE factor, the summation over vari(t) able assignments required for mα→i (yi ) in Eq. (5) equates to a summation over exponentially many projective parse trees. However, we can use an inside-outside variant of Eisner (1996)’s algorithm 4 Following Dreyer and Eisner (2009, footnote 22), we choose an arbitrary directed spanning tree of the factor graph rooted at the PT REE factor. We visit the nodes in topologically sorted order (from leaves to root) and update any message from the node being visited to a node that is later in the order. We then reverse this order and repeat, so that every message has been passed once. This constitutes one iteration of BP. to compute this in polynomial time (we describe this as hypergraph parsing in §3). The resulting “structured BP” inference procedure—detailed by Smith and Eisne"
Q15-1035,W08-0804,1,0.820726,"exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the"
Q15-1035,W96-0214,0,0.658521,"dependency parsing, our loss function is the number of missing edges in the predicted parse ˆ , relative to the reference (or “gold”) parse y ∗ : y P `(ˆ y , y ∗ ) = i: yˆi =OFF I(yi∗ = ON) (1) ˆ and y ∗ each I is the indicator function. Because y specify exactly one parent per word token, `(ˆ y, y∗) equals the directed dependency error: the number of word tokens whose parent is predicted incorrectly. Decoder To obtain a single parse as output, we use a minimum Bayes risk (MBR) decoder, which returns the tree with minimum expected loss under the model’s distribution (Bickel and Doksum, 1977; Goodman, 1996). Our ` gives the decision rule: hθ (x) = argmin Ey∼pθ (· |x) [`(ˆ y , y)] ˆ y = argmax ˆ y X i: yˆi =ON pθ (yi = ON |x) (2) (3) ˆ ranges over well-formed parses. Thus, our Here y parser seeks a well-formed parse hθ (x) whose individual edges have a high probability of being correct according to pθ (since it lacks knowledge y ∗ of which edges are truly correct). MBR is the principled way to take a loss function into account under a probabilistic model. By contrast, maximum a posteriori (MAP) decoding does not consider the loss function. It would return the single highestprobability parse even"
Q15-1035,Q15-1035,1,0.0609026,"repeated application of the chain rule. Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit—just as in Figure 2. Running backwards through the circuit, backprop computes the partial derivatives of the objective J(θ; x, y ∗ ) with respect to each intermediate quantity u—or more concisely ∗) the adjoint of u: ðu = ∂J(θ;x,y . This section ∂u gives a summary of the adjoint computations we require. Due to space constraints, we direct the reader to the extended version of this paper (Gormley et al., 2015a) for full details of all the adjoints. 5.1 Backpropagation of Decoder / Loss The adjoint of the objective itself ðJ(θ; x, y ∗ ) is always 1. So the first adjoints we must compute are those of the beliefs: ðbi (yi ) and ðbα (y α ). This corresponds to the backward pass through Figure 2 (E). Consider the simple case where J is L2 distance from (12): the variable belief adjoint is ðbi (yi ) = 2(bi (yi ) − b∗i (yi )) and trivially ðbα (y α ) = 0. If J is annealed risk from (11), we compute ðbi (yi ) by applying backpropagation recursively to our algorithm for J from §4.1. This sub-algorithm defi"
Q15-1035,D15-1205,1,0.901608,"repeated application of the chain rule. Backpropagating through an algorithm proceeds by similar application of the chain rule, where the intermediate quantities are determined by the topology of the circuit—just as in Figure 2. Running backwards through the circuit, backprop computes the partial derivatives of the objective J(θ; x, y ∗ ) with respect to each intermediate quantity u—or more concisely ∗) the adjoint of u: ðu = ∂J(θ;x,y . This section ∂u gives a summary of the adjoint computations we require. Due to space constraints, we direct the reader to the extended version of this paper (Gormley et al., 2015a) for full details of all the adjoints. 5.1 Backpropagation of Decoder / Loss The adjoint of the objective itself ðJ(θ; x, y ∗ ) is always 1. So the first adjoints we must compute are those of the beliefs: ðbi (yi ) and ðbα (y α ). This corresponds to the backward pass through Figure 2 (E). Consider the simple case where J is L2 distance from (12): the variable belief adjoint is ðbi (yi ) = 2(bi (yi ) − b∗i (yi )) and trivially ðbα (y α ) = 0. If J is annealed risk from (11), we compute ðbi (yi ) by applying backpropagation recursively to our algorithm for J from §4.1. This sub-algorithm defi"
Q15-1035,N12-1015,0,0.0385781,"des empirical risk, Domke (2011) refers to it as “learning with truncated message passing.” Our primary contribution is the application of this approximation-aware learning method in the parsing setting, for which the graphical model involves a global constraint. Smith and Eisner (2008) previously showed how to run BP in this setting (by calling the inside-outside algorithm as a subroutine). We must backpropagate the downstream objective 1 For perceptron training, utilizing inexact inference as a drop-in replacement for exact inference can badly mislead the learner (Kulesza and Pereira, 2008; Huang et al., 2012). 489 Transactions of the Association for Computational Linguistics, vol. 3, pp. 489–501, 2015. Action Editor: Sebastian Riedel. Submission batch: 4/2015; Published 8/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. function through their algorithm so that we can follow its gradient. We carefully define an empirical risk objective function (`a la ERMA) to be smooth and differentiable, yet equivalent to accuracy of the minimum Bayes risk (MBR) parse in the limit. Finding this difficult to optimize, we introduce a new simpler objective function based"
Q15-1035,W01-1812,0,0.332468,"are simple. In the next subsection, we explain how to backpropagate through the insideoutside algorithm. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the T REE factor; we leave this to future work. 5.4 Backprop of Hypergraph Inside-Outside Both the annealed risk loss function (§4.1) and the computation of messages from the PT REE factor (§5.3) use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This general treatment (Klein and Manning, 2001; Li and Eisner, 2009) enables our method to be applied to other tasks such as constituency parsing, HMM forward-backward, and hierarchical machine translation. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we ∀e and the outputs are the marginal probabilities pw (i)∀i of each node i in the hypergraph. The latter are a function of the inside βi and outside αj probabilities. We initialize αroot = 1. X Y βi = we βj (1"
Q15-1035,P10-1001,0,0.822359,"he true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyanov and Eisner (2012). We"
Q15-1035,D09-1005,1,0.946816,"ubsection, we explain how to backpropagate through the insideoutside algorithm. Though we focus here on projective dependency parsing, our techniques are also applicable to non-projective parsing and the T REE factor; we leave this to future work. 5.4 Backprop of Hypergraph Inside-Outside Both the annealed risk loss function (§4.1) and the computation of messages from the PT REE factor (§5.3) use the inside-outside algorithm for dependency parsing. Here we describe inside-outside and the accompanying backpropagation algorithm over a hypergraph. This general treatment (Klein and Manning, 2001; Li and Eisner, 2009) enables our method to be applied to other tasks such as constituency parsing, HMM forward-backward, and hierarchical machine translation. In the case of dependency parsing, the structure of the hypergraph is given by the dynamic programming algorithm of Eisner (1996). For the forward pass of the inside-outside module, the input variables are the hyperedge weights we ∀e and the outputs are the marginal probabilities pw (i)∀i of each node i in the hypergraph. The latter are a function of the inside βi and outside αj probabilities. We initialize αroot = 1. X Y βi = we βj (13) e∈I(i) αj = X j∈T ("
Q15-1035,J93-2004,0,0.0515215,"etween the approximate marginals and the “true” marginals from the gold data. The goal of this work is to account for the approximations made by a system rooted in structured belief propagation. Taking such approximations into account during training enables us to improve the speed and accuracy of inference at test time. We compare our training method with the standard approach of conditional log-likelihood (CLL) training. We evaluate our parser on 19 languages from the CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks as well as the English Penn Treebank (Marcus et al., 1993). On English, the resulting parser obtains higher accuracy with fewer iterations of BP than CLL. On the CoNLL languages, we find that on average it yields higher accuracy parsers than CLL, particularly when limited to few BP iterations. 2 Dependency Parsing by Belief Propagation This section describes the parser that we will train. Model A factor graph (Frey et al., 1997; Kschischang et al., 2001) defines the factorization of a probability distribution over a set of variables {Y1 , Y2 , . . .}. It is a bipartite graph between variables Yi and factors α. Edges connect each factor α to a subset"
Q15-1035,P09-1039,0,0.136933,", we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL and for each token prune any parents for which the marginal probability is less than 0.0001 times the maximum parent marginal for that token. On a per-token basis, we further restrict to the ten parents with highest marginal probability as in Martins et al. (2009) (but we avoid pruning the fully right-branching tree, so that some parse always exists).8 This lets us simplify the factor graph, removing variables yi corresponding to pruned edges and specializing their factors to assume yi = OFF. We train the full model’s parameters to work well on this pruned graph. Data We consider 19 languages from the CoNLL2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007) Shared Tasks. We also convert the English Penn Treebank (PTB) (Marcus et al., 1993) to dependencies using the head rules from Yamada and Matsumoto (2003) (PTB-YM). We evaluate unlabe"
Q15-1035,D10-1004,0,0.0360135,"f infinite training data drawn from the model being used (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by"
Q15-1035,P13-2109,0,0.227989,"Missing"
Q15-1035,E06-1011,0,0.301204,"ically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA” method of Stoyanov and Eisner (2012). We will also consider simpler choices of J—akin to the loss functions used by Domke (2011). Gradient Computation To compute the gradient ∇θ J(θ; x, y ∗ ) of the loss on a single sentence (x, y ∗ ) = (x(d) , y (d) ), we apply automatic differentiation (AD) in the reverse mode (Gr"
Q15-1035,P05-1012,0,0.721936,"r aim is to minimize expected loss on the true data distribution over sentence/parse pairs (X, Y ): θ ∗ = argminθ E[`(hθ (X), Y )] (9) Since the true data distribution is unknown, we substitute the expected loss over the training sample, and regularize our objective in order to reduce sampling variance. Specifically, we aim to minimize the regularized empirical risk, given by (8) with J(θ; x(d) , y (d) ) set to `(hθ (x(d) ), y (d) ). Note that 5 How slow is exact inference for dependency parsing? For certain choices of higher-order factors, polynomial time is possible via dynamic programming (McDonald et al., 2005; Carreras, 2007; Koo and Collins, 2010). However, BP will typically be asymptotically faster (for a fixed number of iterations) and faster in practice. In some other settings, exact inference is NPhard. In particular, non-projective parsing becomes NP-hard with even second-order factors (McDonald and Pereira, 2006). BP can handle this case in polynomial time by replacing the PT REE factor with a T REE factor that allows edges to cross. 6 θ is initialized to 0 when not otherwise specified. 492 this loss function would not be differentiable—a key issue we will take up below. This is the “ERMA”"
Q15-1035,C12-1122,0,0.416682,"Missing"
Q15-1035,petrov-etal-2012-universal,0,0.0136683,"MA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed length for that tuple in the training data. Following Koo and Collins (2010), we train a first-order model with CLL"
Q15-1035,N10-1117,0,0.204886,"even under the assumption of infinite training data drawn from the model being used (Wainwright, 2006). Despite this, the surrogate likelihood objective is commonly used to train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner a"
Q15-1035,N12-1054,0,0.0629473,"train CRFs. CLL and approximation-aware training are not mutually exclusive. Training a standard factor graph with ERMA and a log-likelihood objective recovers CLL exactly (Stoyanov et al., 2011). 496 Experiments 7.1 Setup Features As the focus of this work is on a novel approach to training, we look to prior work for model and feature design (§2). We add O(n3 ) second-order grandparent and arbitrary-sibling factors as in Riedel and Smith (2010) and Martins et al. (2010). We use standard feature sets for first-order (McDonald et al., 2005) and second-order (Carreras, 2007) parsing. Following Rush and Petrov (2012), we also include a version of each part-of-speech (POS) tag feature, with the coarse tags from Petrov et al. (2012). We use feature hashing (Ganchev and Dredze, 2008; Weinberger et al., 2009) and restrict to at most 20 million features. We leave the incorporation of third-order features to future work. Pruning To reduce the time spent on feature extraction, we enforce the type-specific dependency length bounds from Eisner and Smith (2005) as used by Rush and Petrov (2012): the maximum allowed dependency length for each tuple (parent tag, child tag, direction) is given by the maximum observed"
Q15-1035,P06-2101,1,0.876998,"pergraph (§5.4) for use in two modules: the softened decoder (§5.1) and computation of messages from the PT REE factor (§5.3). This allows us to go beyond Stoyanov et al. (2011) and train structured BP in an approximation-aware and lossaware fashion. 4 Differentiable Objective Functions 4.1 Annealed Risk Minimizing the test-time loss is the appropriate goal for training an approximate system like ours. That loss is estimated by the empirical risk on a large amount of in-domain supervised training data. Alas, this risk is nonconvex and piecewise constant, so we turn to deterministic annealing (Smith and Eisner, 2006) and clever initialization. Directed dependency error, `(hθ (x), y ∗ ), is not differentiable due to the argmax in the decoder hθ . So we redefine J(θ; x, y ∗ ) to be a new differentiable loss function, 1/T the annealed risk Rθ (x, y ∗ ), which approaches the loss `(hθ (x), y ∗ ) as the temperature T → 0. Our first step is to define a distribution over parses, which takes the marginals pθ (yi = ON |x) as input, or in practice, their BP approximations bi (ON): P  1/T pθ (yi =ON |x) qθ (ˆ y |x) ∝ exp (10) i:ˆ yi =ON T 493 (E) Decode and Loss (E.3) Expected Recall J(θ; x, y ∗ ) = (E.2) Inside-O"
Q15-1035,D08-1016,1,0.0656886,"ts to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc. By including increasingly global information, these features provide more information for the parser—but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding procedures, which may prevent them from predicting the best parse. For example, consider the dependency parser we will train in this paper, which is based on the work of Smith and Eisner (2008). Ostensibly, this parser finds the minimum Bayes risk (MBR) parse under a probability distribution defined by a higher-order dependency parsing model. In reality, it achieves O(n3 tmax ) runtime by relying on three approximations during inference: (1) variational inference by loopy belief propagation (BP) on a factor graph, (2) truncating inference after tmax iterations prior to convergence, and (3) a first-order pruning model to limit the number of edges considered in the higherorder model. Such parsers are traditionally trained as if the inference had been exact.1 In contrast, we train the"
Q15-1035,N12-1013,1,0.95271,"uts the parse with maximum expected recall—but for speed, this expectation is taken under a posterior distribution that is constructed only approximately, using loopy belief propagation through structured factors. We show how to adjust the model parameters to compensate for the errors introduced by this approximation, by following the gradient of the actual loss on training data. We find this gradient by backpropagation. That is, we treat the entire parser (approximations and all) as a differentiable circuit, as others have done for loopy CRFs (Domke, 2010; Stoyanov et al., 2011; Domke, 2011; Stoyanov and Eisner, 2012). The resulting parser obtains higher accuracy with fewer iterations of belief propagation than one trained by conditional log-likelihood. 1 Introduction Recent improvements to dependency parsing accuracy have been driven by higher-order features. Such a feature can look beyond just the parent and child words connected by a single edge to also consider siblings, grandparents, etc. By including increasingly global information, these features provide more information for the parser—but they also complicate inference. The resulting higher-order parsers depend on approximate inference and decoding"
Q15-1035,W03-3023,0,0.345475,"Missing"
Q15-1035,D07-1096,0,\N,Missing
W07-1017,H05-1124,1,0.320413,"a feature indicating a short text field with the words in the field (“impression length=1 and ‘pneumonia’ ”) • A feature indicating each n-gram sequence that appears in both the impression and clinical history; the conjunction of certain terms where one appears in the history and the other in the impression (e.g. “cough in history and pneumonia in impression”). 3.1.2 Learning Technique Using these feature representations, we now learn a weight vector w that scores the correct labelings of the data higher than incorrect labelings. We used a k-best version of the MIRA algorithm (Crammer, 2004; McDonald et al., 2005). MIRA is an online learning algorithm that for each training document x updates the weight vector w according to the rule: wnew = arg min kw − wold k w s.t. ∀y ∈ Yk,wold (x) : w · f (x, y ∗ (x)) − w · f (x, y) ≥ L(y ∗ (x), y) where L(y ∗ (x), y) is a measure of the loss of labeling y with respect to the correct labeling y ∗ (x). For our experiments, we set k to 30 and iterated over the training data 10 times. Two standard modifications to this approach also helped. First, rather than using just the final weight vector, we average all weight vectors. This has a smoothing effect that improves p"
W08-0804,P07-1056,1,0.176522,"he learning problem more difficult, but we show significant reductions are still possible without harming learning. We emphasize that even when using an extremely large feature space to avoid collisions, alphabet storage is eliminated. For the experiments in this paper we use Java’s hashCode function modulo the intended size rather than a random function. 3 Experiments We evaluated the effect of random feature mixing on four popular learning methods: Perceptron, MIRA (Crammer et al., 2006), SVM and Maximum entropy; with 4 NLP datasets: 20 Newsgroups1 , Reuters (Lewis et al., 2004), Sentiment (Blitzer et al., 2007) and Spam (Bickel, 2006). For each dataset we extracted binary unigram features and sentiment was prepared according to Blitzer et al. (2007). From 20 Newsgroups we created 3 binary decision tasks to differentiate between two similar 1 http://people.csail.mit.edu/jrennie/20Newsgroups/ 19 Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 19–20, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 90 90 85 85 80 80 75 75 feature mixing no feature mixing 70 feature mixing no feature mixing 70 0 10 20 30 40 50 60 70 80 90 thousands of features"
W10-0406,P05-1074,0,0.00452366,"l be to look at parallel sentences to learn rules for simplifying text. One of the advantages of the Wikipedia collection is the parallel articles in ordinary English Wikipedia and Simple Wikipedia. While the content of the articles can differ, these are excellent examples of comparable texts that can be useful for learning simplification rules. Such learning can draw from machine translation, which learns rules that translate between languages. The related task of paraphrase extraction could also provide comparable phrases, one of which can be identified as a simplified version of the other (Bannard and Callison-Burch, 2005). An additional resource available in Simple Wikipedia is the flagging of articles as not simple. By examining the revision history of articles whose flags have been changed, we can discover changes that simplified texts. Initial work on this topic has automatically learned which edits correspond to text simplifications (Yatskar et al., 2010). Text simplification may necessitate the removal of whole phrases, sentences, or even paragraphs, as, according to the writing guidelines for Wikipedia Simple (Wikipedia, 2009), the articles should not exceed 49 a specified length, and some concepts may n"
W10-0406,J96-1002,0,0.00914029,"Missing"
W10-0406,P08-1092,0,0.0264198,"ranslation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University of Cambridge (a position that Isaac Newton once had). He reti"
W10-0406,P04-3031,0,0.0404049,"ively. Each document contains at least two sentences. Additionally, the corpus contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From e"
W10-0406,E06-1032,0,0.00835772,"Missing"
W10-0406,E99-1042,0,0.553789,"ulary and grammatical structure to create a more accessible version of the text while maintaining the underlying information and content. Automated tools for text simplification are a practical way to make large corpora of text accessible to a wider audience lacking high levels of fluency in the corpus language. In this work, we investigate the potential of Simple Wikipedia to assist automatic text simplification by building a statistical classification system that discriminates simple English from ordinary English. Most text simplification systems are based on hand-written rules (e.g., PEST (Carroll et al., 1999) and its module SYSTAR (Canning et al., 2000)), and therefore face limitations scaling and transferring across domains. The potential for using Simple Wikipedia for text simplification is significant; it contains nearly 60,000 articles with revision histories and aligned articles to ordinary English Wikipedia. Using articles from Simple Wikipedia and ordinary Wikipedia, we evaluated different classifiers and feature sets to identify the most discriminative features of simple English for use across domains. These findings help further understanding of what makes text simple and can be applied a"
W10-0406,P05-1022,0,0.00327287,"of the text, we collected extracted features from the parse trees. Our features included the frequency and length of noun phrases, verb phrases, prepositional phrases, and relative clauses (including embedded structures). We also considered relative ratios, such as the ratio of noun to verb phrases, prepositional to noun phrases, and relative clauses to noun phrases. We used the length of the longest noun phrase as a signal of complexity, and we also sought features that measured how typical the sentences were of English text. We included some of the features from the parser reranking work of Charniak and Johnson (2005): the height of the parse tree and the number of right branches from the root of the tree to the furthest right leaf that is not punctuation. 5 Experiments Using the feature sets described above, we evaluated a simple/ordinary text classifier in several settings on each category. First, we considered the task of document classification, where a classifier determines whether a full Wikipedia article was from ordinary English Wikipedia or Simple Wikipedia. For each category of articles, we measured accuracy on this binary classification task using 10-fold cross-validation. In the second setting,"
W10-0406,daelemans-etal-2004-automatic,0,0.548847,"Missing"
W10-0406,E09-1027,0,0.0157352,"vocabulary words necessary to understand the topic. Additionally, words should appear on lists of basic English words, such as the Voice of America Special English words list (Voice Of America, 2009) or the Ogden Basic English list (Ogden, 1930). Idioms should be avoided as well as compounds and the passive voice as opposed to a single simple verb. To capture these properties in the text, we created four classes of features: lexical, part-of-speech, surface, and parse. Several of our features have previously been used for measuring text fluency (Alu´ısio et al., 2008; Chae and Nenkova, 2009; Feng et al., 2009; Petersen and Ostendorf, 2007). Lexical. Previous work by Feng et al. (2009) suggests that the document vocabulary is a good predictor of document readability. Simple texts are more likely to use basic words more often as opposed to more complicated, domain-specific words used in ordinary texts. To capture these features we used a unigram bag-of-words representation. We note that lexical features are unlikely to be useful unless we have access to a large training corpus that allowed the estimation of the relative frequency of words (Chae and Nenkova, 2009). Additionally, we can expect lexical"
W10-0406,D09-1116,0,0.00605388,"ple Wikipedia, we plan to continue working in a number of directions. First, we will explore additional robust indications of text difficulty. For example, Alu´ısio et al. (2008) claim that sentences that are easier to read are also easier to parse, so the entropy of the parser or confidence in the output may be indicative of a text’s difficulty. Additionally, language models trained on large corpora can assign probability scores to texts, which may indicate text difficulty. Of particular interest are syntactic language models that incorporate some of the syntactic observations in this paper (Filimonov and Harper, 2009). Our next goal will be to look at parallel sentences to learn rules for simplifying text. One of the advantages of the Wikipedia collection is the parallel articles in ordinary English Wikipedia and Simple Wikipedia. While the content of the articles can differ, these are excellent examples of comparable texts that can be useful for learning simplification rules. Such learning can draw from machine translation, which learns rules that translate between languages. The related task of paraphrase extraction could also provide comparable phrases, one of which can be identified as a simplified ver"
W10-0406,W09-3305,0,0.0274076,"the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 197"
W10-0406,D09-1087,0,0.00398774,"s contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differenc"
W10-0406,J06-4003,0,0.0236643,"used to generate features. spectively. Each document contains at least two sentences. Additionally, the corpus contains only the main text body of each article and does not consider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simpl"
W10-0406,N07-1025,0,0.0199431,"uages from Wikipedia, that stored the rich structural information of Wikipedia with XML. This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Ca"
W10-0406,2001.mtsummit-papers.68,0,0.0248274,"Missing"
W10-0406,N07-1051,0,0.00402686,"es, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differences between the texts, using cognitively motivated features based on a document’s lexical,"
W10-0406,P06-1055,0,0.00719329,"ider info boxes, tables, lists, external and crossreferences, and other structural features. The experiments that follow randomly extract documents and sentences from this collection. Before extracting features, we ran a series of natural language processing tools to preprocess the collection. First, all of the XML and “wiki markup” was removed. Each document was split into sentences using the Punkt sentence tokenizer (Kiss and Strunk, 2006) in NLTK (Bird and Loper, 2004). We then parsed each sentence using the PCFG parser of Huang and Harper (2009), a modified version of the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007), for the tree structure and part-ofspeech tags. 3 Task Setup To evaluate the feasibility of learning simple and ordinary texts, we sought to identify text properties that differentiated between these classes. Using the two document collections, we constructed a simple binary classification task: label a piece of text as either simple or ordinary. The text was labeled according to its source: simple or ordinary Wikipedia. From each piece of text, we extracted a set of features designed to capture differences between the texts, using cognitively motivated features based"
W10-0406,N06-1025,0,0.00892691,"gs, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University of Cambridge (a position that Isaac Newton once had). He retired on October 1st 2009. Table 2: Comparable sentences from the ordinary Wikipedia and Simple Wikipedia entr"
W10-0406,N10-1063,0,0.00823464,"This corpus was designed specifically for XML retrieval but has uses in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple W"
W10-0406,P07-1067,0,0.0154537,"s in natural language processing, categorization, machine translation, entity ranking, etc. YAWN (Schenkel et al., 2007), a Wikipedia XML corpus with semantic tags, is another example of exploiting Wikipedia’s structural information. Wikipedia provides XML site dumps every few weeks in all languages as well as static HTML dumps. A diverse array of NLP research in the past few years has used Wikipedia, such as for word sense disambiguation (Mihalcea, 2007), classification (Gantner and Schmidt-Thieme, 2009), machine translation (Smith et al., 2010), coreference resolution (Versley et al., 2008; Yang and Su, 2007), sentence extraction for summarization (Biadsy et al., 2008), information retrieval (M¨uller and Gurevych, 2008), and semantic role labeling (Ponzetto and Strube, 2006), to name a few. However, except for very recent work by Yatskar et al. (2010), to our knowledge there has not been comparable research in using Wikipedia for text simplification. Ordinary Wikipedia Hawking was the Lucasian Professor of Mathematics at the University of Cambridge for thirty years, taking up the post in 1979 and retiring on 1 October 2009. Simple Wikipedia Hawking was a professor of mathematics at the University"
W10-0406,N10-1056,0,\N,Missing
W10-0406,E09-1017,0,\N,Missing
W10-0406,P02-1040,0,\N,Missing
W10-0406,P06-4018,0,\N,Missing
W10-0406,W02-0109,0,\N,Missing
W10-0701,W10-0731,0,0.0359833,"Missing"
W10-0701,W10-0710,0,0.167215,"rformance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasi"
W10-0701,2009.iwslt-evaluation.16,0,0.0223923,"lable for many users. Learning across multiple annotations may improve systems (Dredze et al., 2009). Additionally, even with efforts to clean up MTurk annotations, we can expect an increase in noisy examples in data. This will push for new more robust learning algorithms that are less sensitive to noise. If we increase the size of the data ten-fold but also increase the noise, can learning still be successful? Another learning area of great interest is active learning, which has long relied on simulated user experiments. New work evaluated active learning methods with real users using MTurk (Baker et al., 2009; Ambati et al., 2010; Hsueh et al., 2009; ?). Finally, the composition of complex data set annotations from simple user inputs can transform the method by which we learn complex outputs. Current approaches expect examples of labels that exactly match the expectation of the system. Can we instead provide lower level simpler user annotations and teach systems how to learn from these to construct complex output? This would open more complex annotation tasks to MTurk. A general trend in research is that good ideas come from unexpected places. Major transformations in the field have come from crea"
W10-0701,P07-1056,1,0.0643466,"data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessible data. More than a new source for cheap data, MTurk is a source for new types of dat"
W10-0701,P10-1088,1,0.0566792,"nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for hig"
W10-0701,W10-0733,1,0.257767,"nds on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 translations per sentence for all the language pairs between English, Spanish, Urdu and Telugu. This demonstrates the feasibility of creating cheap corpora for hig"
W10-0701,W09-2416,0,0.0225701,"Missing"
W10-0701,W10-0735,0,0.0184231,"Missing"
W10-0701,D09-1030,1,0.324286,"ld standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced effo"
W10-0701,W10-0720,0,0.0173225,"Missing"
W10-0701,W10-0709,0,0.0268793,"Missing"
W10-0701,W10-0711,0,0.0259705,"Missing"
W10-0701,W10-0708,0,0.0576436,"Missing"
W10-0701,W10-0704,0,0.0105147,"Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally expensive to produce, requiring skilled translators. One of the advantages to MTurk is the diversity of the Turker population, making it an especially attractive source of MT data. Shared task papers in MT explored the full range of MT tasks, including alignments, parallel corpus creation, paraphrases and bilingual lexicons. Gao and Vogel (2010) create alignments in a 300 sentence Chinese-English corpus (Chinese aligned to English). Both Ambati and Vogel (2010) and Bloodgood and Callison-Burch (2010) explore the potential of MTurk in the creation of MT parallel corpora for evaluation and training. Bloodgood 7 and Callison-Burch replicate the NIST 2009 UrduEnglish test set of 1792 sentences, paying only $0.10 a sentence, a substantially reduced price than the typical annotator cost. The result is a data set that is still effective for comparing MT systems in an evaluation. Ambati and Vogel create corpora with 100 sentences and 3 trans"
W10-0701,W10-0722,0,0.0950833,"Missing"
W10-0701,W10-0724,0,0.0386468,"Missing"
W10-0701,W10-0732,1,0.52444,"s on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8 http://www.ischool.utexas.edu/˜cse2010/ call.htm 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation types. The goal of relation extraction is to identify relations between entities or terms in a sentence, such as born in or religion. Gormley et al. (2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisg"
W10-0701,W10-0727,0,0.020468,"t automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negative results on how these factors influence data collection. For further work on MTurk and information retrieval, readers are encouraged to see the SIGIR 2010 Workshop on Crowdsourcing for Search Evaluation.8 8 http://www.ischool.utexas.edu/˜cse2010/ call.htm 6.5 Information Extraction Information extraction (IE) seeks to identify specific types of information in natural languages. The IE papers in the shared tasks focused on new domains and genres as well as new relation typ"
W10-0701,W10-0705,0,0.0183737,"ided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions about provided texts. These questions can be used to test reading comprehension and understanding. 60 Wikipedia articles were selected, for each of which 20 questions were generated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the K NEXT knowledge extr"
W10-0701,W10-0714,0,0.0071973,"ing specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of data type and its impact on machine learning algorithms. With professional annotators, great time and care are paid to annotation guidelines and annotator training. These are difficult tasks with MTurk, which favors simple intuitive annotations"
W10-0701,W09-1904,0,0.0392972,"nical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose"
W10-0701,W10-0717,0,0.069042,"e for new types of data. Several of the papers in this workshop collected information about the annotators in addition to their annotations. This creates potential for studying how different user demographics understand language and allow for tar8 geting specific demographics in data creation. Beyond efficiencies in cost, MTurk provides access to a global user population far more diverse than those provided by more professional annotation settings. This will have a significant impact on low resource languages as corpora can be cheaply built for a much wider array of languages. As one example, Irvine and Klementiev (2010) collected data for 42 languages without worrying about how to find speakers of such a wide variety of languages. Additionally, the collection of Arabic nicknames requires a diverse and numerous Arabic speaking population (Higgins et al., 2010). In addition to extending into new languages, MTurk also allows for the creation of evaluation sets in new genres and domains, which was the focus of two papers in this workshop (Finin et al., 2010; Yetisgen-Yildiz et al., 2010). We expect to see new research emphasis on low resource languages and new domains and genres. Another factor is the change of"
W10-0701,W10-0702,0,0.0297313,"Missing"
W10-0701,kaisser-lowe-2008-creating,0,0.00454559,"edu/uid/turkit/ inter-annotator agreement of the Turkers against experts on small amounts of gold standard data, or by stating what controls you used and what criteria you used to block bad Turkers. Finally, whenever possible you should publish the data that you generate on Mechanical Turk (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Cal"
W10-0701,W10-0726,0,0.0108165,"nowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator, 6 allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required"
W10-0701,W10-0729,0,0.207466,"rated. Turkers provided 5 ratings for each of the 1,200 questions, creating a significant corpus of scored questions. Finally, Gordon et al. (2010) relied on MTurk to evaluate the quality and accuracy of automatically extracted common sense knowledge (factoids) from news and Wikipedia articles. Factoids were provided by the K NEXT knowledge extraction system. 6.2 Speech and Vision While MTurk naturally lends itself to text tasks, several teams explored annotation and collection of speech and image data. We note that one of the papers in the main track described tools for collecting such data (Lane et al., 2010). Two teams used MTurk to collect text annotations on speech data. Marge et al. (2010b) identified easy and hard sections of meeting speech to transcribe and focused data collection on difficult segments. Transcripts were collected on 48 audio clips from 4 different speakers, as well as other types of annotations. Kunath and Weinberger (2010) collected ratings of accented English speech, in which nonnative speakers were rated as either Arabic, Mandarin or Russian native speakers. The authors obtained multiple annotations for each speech sample, and tracked the native language of each annotator"
W10-0701,W10-0712,0,0.0346725,"Missing"
W10-0701,W10-0730,0,0.0101152,"ntailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams expl"
W10-0701,W10-0716,0,0.138372,"k (and your analysis scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). B"
W10-0701,W10-0718,0,0.0345404,"Missing"
W10-0701,W10-0719,0,0.107949,"Missing"
W10-0701,W10-0734,0,0.0442928,"ing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7 http://sites.google.com/site/ amtworkshop2010/ recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-Spanish equivalent by having Turkers translating the hypotheses into Spanish. The authors include a timeline of their progress, complete with total cost over the 10 days that they ran the experiments. In the area of natural language generation, Heilman and Smith (2010) explored the potential of MTurk for ranking of computer generated questions"
W10-0701,N10-1024,1,0.504587,"scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear"
W10-0701,W10-0706,1,0.771093,"scripts and HIT templates) alongside your paper so that other people can verify it. 5 Related work In the past two years, several papers have published about applying Mechanical Turk to a diverse set of natural language processing tasks, including: creating question-answer sentence pairs (Kaisser and Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows resear"
W10-0701,W02-1011,0,0.0149006,"l exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is made when it can be demonstrated against previous approaches on the same data. The reduction of data cost and the rise of independent corpus producers likely means more accessib"
W10-0701,W10-0703,0,0.0141545,"demonstrates the potential for MTurk’s impact on the creation and curation of speech and language corpora. 6.1 Traditional NLP Tasks An established core set of computational linguistic tasks have received considerable attention in the natural language processing community. These include knowledge extraction, textual entailment and word sense disambiguation. Each of these tasks requires a large and carefully curated annotated corpus to train and evaluate statistical models. Many of the shared task teams attempted to create new corpora for these tasks at substantially reduced costs using MTurk. Parent and Eskenazi (2010) produce new corpora for the task of word sense disambiguation. The study used MTurk to create unique word definitions for 50 words, which Turkers then also mapped onto existing definitions. Sentences containing these 50 words were then assigned to unique definitions according to word sense. Madnani and Boyd-Graber (2010) measured the concept of transitivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portraye"
W10-0701,W09-3607,0,0.0112431,"’s authors is a strong advocate of such a position while the other disagrees, perhaps because he himself works on unsupervised methods. Certainly, we can agree that the potential exists for a change in focus in a number of ways. In natural language processing, data drives research. The introduction of new large and widely accessible data sets creates whole new areas of research. There are many examples of such impact, the most famous of which is the Penn Treebank (Marcus. et al., 1994), which has 2910 citations in Google scholar and is the single most cited paper on the ACL anthology network (Radev et al., 2009). Other examples include the CoNLL named entity corpus (Sang and Meulder (2003) with 348 citations on Google Scholar), the IMDB movie reviews sentiment data (Pang et al. (2002) with 894 citations) and the Amazon sentiment multi-domain data (Blitzer et al. (2007) with 109 citations) . MTurk means that creating similar data sets is now much cheaper and easier than ever before. It is highly likely that new MTurk produced data sets will achieve prominence and have significant impact. Additionally, the creation of shared data means more comparison and evaluation against previous work. Progress is m"
W10-0701,W10-0721,0,0.0104382,"Missing"
W10-0701,W03-0419,0,0.0250001,"Missing"
W10-0701,D08-1027,0,0.23451,"Missing"
W10-0701,W10-0707,0,0.0217939,"r, 6 allowing for an analysis of rating accuracy between native English and non-native English annotators. Novotney and Callison-Burch (2010b) used MTurk to elicit new speech samples. As part of an effort to increase the accessibility of public knowledge, such as Wikipedia, the team prompted Turkers to narrate Wikipedia articles. This required Turkers to record audio files and upload them. An additional HIT was used to evaluate the quality of the narrations. A particularly creative data collection approach asked Turkers to create handwriting samples and then to submit images of their writing (Tong et al., 2010). Turkers were asked to submit handwritten shopping lists (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such da"
W10-0701,W10-0725,1,0.613046,"tivity of verbs in the style of Hopper and Thompson (1980), a theory that goes beyond simple grammatical transitivity – whether verbs take objects (transitive) or not – to capture the amount of action indicated by a sentence. Videos that portrayed verbs were shown to Turkers who described the actions shown in the video. Additionally, sentences containing the verbs were rated for aspect, affirmation, benefit, harm, kinesis, punctuality, and volition. The authors investigated several approaches for eliciting descriptions of transitivity from Turkers. Two teams explored textual entailment tasks. Wang and Callison-Burch (2010) created data for 7 http://sites.google.com/site/ amtworkshop2010/ recognizing textual entailment (RTE). They submitted 600 text segments and asked Turkers to identify facts and counter-facts (unsupported facts and contradictions) given the provided text. The resulting collection includes 790 facts and 203 counter-facts. Negri and Mehdad (2010) created a bi-lingual entailment corpus using English and Spanish entailment pairs, where the hypothesis and text come from different languages. The authors took a publicly available English RTE data set (the PASCAL-RTE3 dataset1) and created an English-"
W10-0701,W10-0723,0,0.00787248,"s (large vocabulary) or weather descriptions (small vocabulary) in either Arabic or Spanish. Subsequent Turkers provided a transcription and a translation. The team collected 18 images per language, 2 transcripts per image and 1 translation per transcript. 6.3 Sentiment, Polarity and Bias Two papers investigated the topics of sentiment, polarity and bias. Mellebeek et al. (2010) used several methods to obtain polarity scores for Spanish sentences expressing opinions about automative topics. They evaluated three HITs for collecting such data and compared results for quality and expressiveness. Yano et al. (2010) evaluated the political bias of blog posts. Annotators labeled 1000 sentences to determine biased phrases in political blogs from the 2008 election season. Knowledge of the annotators own biases allowed the authors to study how bias differs on the different ends of the political spectrum. 6.4 Information Retrieval Large scale evaluations requiring significant human labor for evaluation have a long history in the information retrieval community (TREC). Grady and Lease (2010) study four factors that influence Turker performance on a document relevance search task. The authors present some negat"
W10-0701,W10-0728,0,0.0517069,"(2010) automatically generate potential relation pairs in sentences by finding relation pairs appearing in news articles as given by a knowledge base. They ask Turkers if a sentence supports a relation, does not support a relation, or whether the relation makes sense. They collected close to 2500 annotations for 17 different person relation types. The other IE papers explored new genres and domains. Finin et al. (2010) obtained named entity annotations (person, organization, geopolitical entity) for several hundred Twitter messages. They conducted experiments using both MTurk and CrowdFlower. Yetisgen-Yildiz et al. (2010) explored medical named entity recognition. They selected 100 clinical trial announcements from ClinicalTrials.gov. 4 annotators for each of the 100 announcements identified 3 types of medical entities: medical conditions, medications, and laboratory test. 6.6 Machine Translation The most popular shared task topic was Machine Translation (MT). MT is a data hungry task that relies on huge corpora of parallel texts between two languages. Performance of MT systems depends on the size of training corpora, so there is a constant search for new and larger data sets. Such data sets are traditionally"
W10-0701,D09-1006,1,0.422687,"nd Lowe, 2008), evaluating machine translation quality and crowdsouring translations (Callison-Burch, 2009), paraphrasing noun-noun compouds for SemEval (Butnariu et al., 2009), human evaluation of topic models (Chang et al., 2009), and speech transcription (McGraw et al., 2010; Marge et al., 2010a; Novotney and Callison-Burch, 2010a). Others have used MTurk for novel research directions like nonsimulated active learning for NLP tasks such as sentiment classification (Hsueh et al., 2009) or doing quixotic things like doing human-in-the-loop minimum error rate training for machine translation (Zaidan and Callison-Burch, 2009). Some projects have demonstrated the superscalability of crowdsourced efforts. Deng et al. (2009) used MTurk to construct ImageNet, an annotated image database containing 3.2 million that are hierarchically categorized using the WordNet ontology (Fellbaum, 1998). Because Mechanical Turk allows researchers to experiment with crowdsourcing by providing small incentives to Turkers, other successful crowdsourcing efforts like Wikipedia or Games with a Purpose (von Ahn and Dabbish, 2008) also share something in common with MTurk. 6 Shared Task The workshop included a shared task in which participa"
W10-0701,W10-0715,0,0.0129314,"Missing"
W10-0701,ambati-etal-2010-active,0,\N,Missing
W10-0701,J93-2004,0,\N,Missing
W10-0701,mcgraw-etal-2010-collecting,0,\N,Missing
W10-0713,D09-1030,0,0.0530737,"Missing"
W10-0713,P05-1045,0,0.0521812,"Missing"
W10-0713,D08-1027,0,0.290152,"Missing"
W10-0713,strassel-etal-2008-linguistic,0,0.00523198,"es that have traditionally been the focus of named entity experiments, Twitter is far more informal and abbreviated. The collected annotations and annotation techniques will provide a first step towards the full study of named entity recognition in domains like Facebook and Twitter. We also briefly describe how to use MTurk to collect judgements on the quality of “word clouds.” 1 Introduction and Dataset Description Information extraction researchers commonly work on popular formal domains, such as news articles. More diverse studies have included broadcast news transcripts, blogs and emails (Strassel et al., 2008). However, extremely informal domains, such as Facebook, Twitter, YouTube or Flickr are starting to receive more attention. Any effort aimed at studying these informal genres will require at least a minimal amount of labeled data for evaluation purposes. This work details how to efficiently annotate large volumes of data, for information extraction tasks, at low cost using MTurk (Snow et al., 2008; CallisonBurch, 2009). This paper describes a case study for information extraction tasks involving short, informal messages from Twitter. Twitter is a large multiuser site for broadcasting short inf"
W10-0732,P07-1073,0,0.0419445,"n between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v is a value, and r is a relation that holds between them; for example (J.R.R. Tolkien, occupation, author). Our KB is Freebase1 , an online database of structured informati"
W10-0732,D07-1076,0,0.0340232,"e also present results on inter-annotator agreement. 1 2 Method 2.1 Introduction Relation extraction (RE) is the task of determining the existence and type of relation between two textual entity mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v i"
W10-0732,P09-1113,0,0.0861627,"ty mentions. Slot filling, a general form of relation extraction, includes relations between nonentities, such as a person and an occupation, age, or cause of death (McNamee and Dang, 2009). RE annotated data, such as ACE (2008), is expensive to produce so systems take different approaches to minimizing data needs. For example, tree kernels can reduce feature sparsity and generalize across many examples (GuoDong et al., 2007; Zhou et al., 2009). Distant supervision automatically generates noisy training examples from a knowledge base (KB) without needing annotations (Bunescu and Mooney, 2007; Mintz et al., 2009). While this method can quickly generate training data, it also generates many false examples. We reduce the noise in such examples by using Amazon Mechanical Turk (MTurk), which has been shown to produce Automatic generation of noisy examples To create noisy examples we use a similar approach to Mintz et al. (2009). We extract relations from a KB in the form of tuples, (e, r, v), where e is an entity, v is a value, and r is a relation that holds between them; for example (J.R.R. Tolkien, occupation, author). Our KB is Freebase1 , an online database of structured information, and our corpus is"
W10-0732,D08-1027,0,0.0515835,"Missing"
W12-2707,P05-1063,0,0.0705001,"Missing"
W12-2707,D09-1116,0,0.0443834,"Missing"
W12-2707,D09-1087,0,0.0607762,"Missing"
W12-2707,J93-2004,0,0.0425648,"Missing"
W12-2707,D07-1111,0,0.0299004,"r proceeds leftto-right, and only hypothesized structures based on w1 , . . . , wi−1 are used by the SLM to predict wi . Similarly, the specific features used by the parser are also not important: more noteworthy is that the SLM uses (h.w−3 , h.w−2 , h.w−1 ) and their POS tags to predict wi . The question is whether this yields lower perplexity than predicting wi from (wi−3 , wi−2 , wi−1 ). For the sake of completeness, we next describe the parser and SLM in some detail, but either may be skipped without loss of continuity. The Parser: We use the shift-reduce incremental dependency parser of (Sagae and Tsujii, 2007), which constructs a tree from a transition sequence governed by a maximum-entropy classifier. Shiftreduce parsing places input words into a queue Q and partially built structures are organized by a stack S. Shift and reduce actions consume the queue and build the output parse on the stack. The classifier g assigns probabilities to each action, and the probability of a state pg (π) can be computed as the product of the probabilities of a sequence of actions that resulted in the state. The parser therefore provides (multiple) syntactic analyses of the history w1 , . . . , wi−1 at each word posi"
W12-2707,W11-0328,0,0.016954,"lary we use in BN experiments has about 84K words. • WSJ setup : The training text consists of about 37M words. We use eval92+eval93 (10K words) as our evaluation set and dev93 (9K words) serves as our development set for interpolating SLMs with the baseline 4-gram model. In both cases, we sample about 20K sentences from the training text (we exclude them from training data) to serve as our heldout data for applying the bucketing algorithm and estimating λ’s. To apply the dependency parser, all the data sets are first converted to Treebank-style tokenization and POStagged using the tagger of (Tsuruoka et al., 2011)2 . Both the POS-tagger and the shift-reduce dependency parser are trained on the Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and the WSJ Penn Treebank (after converting them to dependency trees) which consists of about 1.2M tokens. Finally, we train a modified kneser-ney 4-gram LM on the tokenized training text to serve as our baseline LM, for both experiments. 5.2 Results and Analysis Table 2 shows the perplexity results for BN and WSJ experiments, respectively. It is evident that the 4gram baseline for BN is stronger than the 40M case of Table 1. Yet, the interpolated S"
W14-3207,D11-1145,0,0.0124549,"Missing"
W14-3207,D13-1133,0,0.076898,"regarding the balance between the utility of such data and the privacy of mental health related information. 1 What can we expect to learn about mental health by studying social media? How does a service like Twitter inform our knowledge in this area? Numerous studies indicate that language use, social expression and interaction are telling indicators of mental health. The well-known Linguistic Inquiry Word Count (LIWC), a validated tool for the psychometric analysis of language data (Pennebaker et al., 2007), has been repeatedly used to study language associated with all types of disorders (Resnik et al., 2013; Alvarez-Conrad et al., 2001; Tausczik and Pennebaker, 2010). Furthermore, social media is by nature social, which means that social patterns, a critical part of mental health and illness, may be readily observable in raw Twitter data. Thus, Twitter and other social media provide Introduction While mental health issues pose a significant health burden on the general public, mental health research lacks the quantifiable data available to many physical health disciplines. This is partly due to the complexity of the underlying causes of mental illness and partly due to longstanding societal stig"
W14-3207,D13-1171,0,0.019731,"Missing"
W15-1201,D11-1145,0,0.00912752,"calls for the strengthening of “information systems, evidence and research,” which necessitates new development and improvements in global mental health surveillance capabilities (World Health Organization, 2013). As a result, research on mental health has turned to web data sources (Ayers et al., 2013; Althouse et al., 2014; Yang et al., 2010; Hausner et al., 2008), with a particular focus on social media (De Choudhury, 2014; Schwartz et al., 2013a; De Choudhury et al., 2011). While many users discuss physical health conditions such as cancer or the flu (Paul and Dredze, 2011; Dredze, 2012; Aramaki et al., 2011; Hawn, 2009), some also discuss mental illness. There are a variety of motivations for users to share this information on social media: to offer or seek support, to fight the stigma of mental illness, or perhaps to offer an explanation for certain behaviors. Past mental health work has largely focused on depression, with some considering post-traumatic stress disorder (Coppersmith et al., 2014b), suicide (Tong et al., 2014; Jashinsky et al., 2014), seasonal affective disorder, and bipolar disorder (Coppersmith et al., 2014a). While these represent some of the most common mental disorders, it"
W15-1201,P14-2030,1,0.875005,"Missing"
W15-1201,D11-1120,0,0.160292,"Missing"
W15-1201,W14-3207,1,0.552941,"s on social media (De Choudhury, 2014; Schwartz et al., 2013a; De Choudhury et al., 2011). While many users discuss physical health conditions such as cancer or the flu (Paul and Dredze, 2011; Dredze, 2012; Aramaki et al., 2011; Hawn, 2009), some also discuss mental illness. There are a variety of motivations for users to share this information on social media: to offer or seek support, to fight the stigma of mental illness, or perhaps to offer an explanation for certain behaviors. Past mental health work has largely focused on depression, with some considering post-traumatic stress disorder (Coppersmith et al., 2014b), suicide (Tong et al., 2014; Jashinsky et al., 2014), seasonal affective disorder, and bipolar disorder (Coppersmith et al., 2014a). While these represent some of the most common mental disorders, it only begins to consider the range of mental health conditions for which social media could be utilized. Yet obtaining data for many conditions can be difficult, as previous techniques required the identification of affected individuals using traditional screening methods (De Choudhury, 2013; Schwartz et al., 2013b). Coppersmith et al. (2014a) proposed a novel way of obtaining mental health rela"
W15-1201,D14-1121,0,0.0361992,"Missing"
W15-1201,W14-3214,0,0.372116,"Missing"
W15-1201,W14-3213,0,0.0142255,"; Schwartz et al., 2013a; De Choudhury et al., 2011). While many users discuss physical health conditions such as cancer or the flu (Paul and Dredze, 2011; Dredze, 2012; Aramaki et al., 2011; Hawn, 2009), some also discuss mental illness. There are a variety of motivations for users to share this information on social media: to offer or seek support, to fight the stigma of mental illness, or perhaps to offer an explanation for certain behaviors. Past mental health work has largely focused on depression, with some considering post-traumatic stress disorder (Coppersmith et al., 2014b), suicide (Tong et al., 2014; Jashinsky et al., 2014), seasonal affective disorder, and bipolar disorder (Coppersmith et al., 2014a). While these represent some of the most common mental disorders, it only begins to consider the range of mental health conditions for which social media could be utilized. Yet obtaining data for many conditions can be difficult, as previous techniques required the identification of affected individuals using traditional screening methods (De Choudhury, 2013; Schwartz et al., 2013b). Coppersmith et al. (2014a) proposed a novel way of obtaining mental health related Twitter data. Using the se"
W15-1204,W14-3207,1,0.521622,"shared and unshared tasks examined Twitter users who publicly stated a diagnosis of depression or PTSD (and ageand gender-matched controls). Shared tasks are tools for fostering research communities and organizing research efforts around shared goals. They provide a forum to explore new ideas and evaluate the best-of-breed, emerging, and wild technologies. The 2015 CLPsych Shared Task consisted of three user-level binary classification tasks: PTSD vs. control, depression vs. control, and PTSD vs. depression. The first two have been addressed in a number of settings (Coppersmith et al., 2015; Coppersmith et al., 2014b; Coppersmith et al., 2014a; Resnik et al., 2013; De Choudhury et al., 2013; Rosenquist et al., 2010; Ramirez-Esparza et al., 2008), while the third task is novel. Organizing this shared task brought together many teams to consider the same problem, which had the benefit of establishing a solid foundational understanding, common standards, and a shared deep understanding of both task and data. 31 Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 31–39, c Denver, Colorado, June 5, 2015. 2015 Association for C"
W15-1204,W15-1201,1,0.635757,"ge usage. To that end, the shared and unshared tasks examined Twitter users who publicly stated a diagnosis of depression or PTSD (and ageand gender-matched controls). Shared tasks are tools for fostering research communities and organizing research efforts around shared goals. They provide a forum to explore new ideas and evaluate the best-of-breed, emerging, and wild technologies. The 2015 CLPsych Shared Task consisted of three user-level binary classification tasks: PTSD vs. control, depression vs. control, and PTSD vs. depression. The first two have been addressed in a number of settings (Coppersmith et al., 2015; Coppersmith et al., 2014b; Coppersmith et al., 2014a; Resnik et al., 2013; De Choudhury et al., 2013; Rosenquist et al., 2010; Ramirez-Esparza et al., 2008), while the third task is novel. Organizing this shared task brought together many teams to consider the same problem, which had the benefit of establishing a solid foundational understanding, common standards, and a shared deep understanding of both task and data. 31 Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 31–39, c Denver, Colorado, June 5, 20"
W15-1204,W15-1206,0,0.0976388,"s belonged together, and ultimately may provide some useful insight as to which approaches are best at capturing mental health related signals (Preotiuc-Pietro et al., 2015). 4.3 University of Minnesota, Duluth The Duluth submission took a well-reasoned rulebased approach to these tasks, and as such provides a point to examine how powerful simple, raw language features are in this context. Importantly, the Duluth systems allow one to decouple the power of an open vocabulary approach, quite independent of any complex machine learning or complex weighting schemes applied to the open vocabulary (Pedersen, 2015). 4.4 MIQ – Microsoft, IHMC, Qntfy We include a small system developed by the organizers for this shared task to examine the effect of providing qualitatively different information from the other system submissions. In this system, which we will refer to as the MIQ3 (pronounced ‘Mike’) submission, we use character language models (CLMs) to assign scores to individual tweets. These scores indicate whether the user may be suffering from PTSD, depression, or neither. The general approach is to examine how likely a sequence of characters is to be generated by a given type of user (PTSD, depression"
W15-1204,W15-1205,0,0.0920415,"Missing"
W15-1204,D13-1133,0,0.065738,"publicly stated a diagnosis of depression or PTSD (and ageand gender-matched controls). Shared tasks are tools for fostering research communities and organizing research efforts around shared goals. They provide a forum to explore new ideas and evaluate the best-of-breed, emerging, and wild technologies. The 2015 CLPsych Shared Task consisted of three user-level binary classification tasks: PTSD vs. control, depression vs. control, and PTSD vs. depression. The first two have been addressed in a number of settings (Coppersmith et al., 2015; Coppersmith et al., 2014b; Coppersmith et al., 2014a; Resnik et al., 2013; De Choudhury et al., 2013; Rosenquist et al., 2010; Ramirez-Esparza et al., 2008), while the third task is novel. Organizing this shared task brought together many teams to consider the same problem, which had the benefit of establishing a solid foundational understanding, common standards, and a shared deep understanding of both task and data. 31 Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 31–39, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics The unshared task (affec"
W15-1204,W15-1207,0,0.13944,"rvised topic models, computed on subsets of the documents for each user. Particularly, they used a variety of supervised topic-modeling approaches to find groups of words that had maximal power to differentiate between the users for each classification task. Moreover, rather than computing topics over two (typical) extreme cases – treating each tweet as an individual document or treating each users’s tweets collectively as a single document (concatenating all tweets together) – they opted for a sensible middle ground of concatenating all tweets from a given week together as a single document (Resnik et al., 2015). 4.2 University of Pennsylvania, World Well-Being Project The WWBP examined a wide variety of methods for inferring topics automatically, combined with binary unigram vectors (i.e., “did this user ever use this word?”), and scored using straightforward regression methods. Each of these topic-modeling techniques provided a different interpretation on modeling what groups of words belonged together, and ultimately may provide some useful insight as to which approaches are best at capturing mental health related signals (Preotiuc-Pietro et al., 2015). 4.3 University of Minnesota, Duluth The Dulu"
W15-1204,D14-1121,0,0.0332568,"Missing"
W16-1305,P08-1107,1,0.828547,"Missing"
W16-5614,D11-1120,0,0.409032,"Missing"
W16-5614,D13-1114,0,0.0369543,"platforms, such as Twitter, lack demographic and location characteristics available for traditional surveys. The lack of these 1 data prevents comparisons to traditional survey results. There have been a number of attempts to automatically infer user attributes from available social media data, such as a collection of messages for a user. These efforts have led to author attribute, or demographic, inference (Mislove et al., 2011; Volkova et al., 2015b; Burger et al., 2011; Volkova et al., 2015a; Pennacchiotti and Popescu, 2011; Rao and Yarowsky, 2010; Rao et al., 2010; Schwartz et al., 2013; Ciot et al., 2013; Alowibdi et al., 2013; Culotta et al., 2015) and geolocation tasks (Eisenstein et al., 2010; Han et al., 2014; Rout et al., 2013; Compton et al., 2014; Cha et al., 2015; Jurgens et al., 2015; Rahimi et al., 2016). A limitation of these content analysis methods is their reliance on multiple messages for each user (or, in the case of social network based methods, data about multiple followers or friends for each user of interest). For example, we may wish to better understand the demographics of users who tweet a particular hashtag. While having tens or hundreds of messages for each user can i"
W16-5614,N16-1122,1,0.803534,"about multiple followers or friends for each user of interest). For example, we may wish to better understand the demographics of users who tweet a particular hashtag. While having tens or hundreds of messages for each user can improve prediction accuracy, collecting more data for every user of interest may be prohibitive either in terms of API access, or in terms of the time required. In this vein, several papers have dealt with the task of geolocation from a single tweet, relying on the user’s profile location, time, tweet content and other factors to make a decision (Osborne et al., 2014; Dredze et al., 2016). This includes tools like Carmen (Dredze et al., 2013) and TwoFishes.2 For demographic prediction, several papers have explored using names to infer gender and ethnicity (Rao et al., 2011; Liu and Ruths, 2 https://bitbucket.org/mdredze/demographer Mark Dredze Human Language Technology Center of Excellence Johns Hopkins University Baltimore, MD 21211 mdredze@cs.jhu.edu http://twofishes.net/ 108 Proceedings of 2016 EMNLP Workshop on Natural Language Processing and Computational Social Science, pages 108–113, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics 2013; Be"
W16-5614,D10-1124,0,0.217305,"Missing"
W16-5614,D15-1240,0,0.067665,"prediction) for an individual. We know that not all of the “name” fields actually contain names, but we do not know how the use of nonnames in that field may be distributed across demographic groups. We did not evaluate whether thresholding had a uniform impact on prediction quality across demographic groups. Failing to produce accurate predictions (or any prediction at all) due to these factors could introduce bias into the sample and subsequent conclusions. One possible way to deal with some of these issues would be to incorporate predictions based on username, such as those as described in Jaech and Ostendorf (2015). 6 Conclusions We introduce D EMOGRAPHER, a tool that can produce high-accuracy and high-coverage results for gender inference from a given name. Our tool is comparable to or better than existing tools (particularly on Twitter data). Depending on the use case, users may prefer higher accuracy or higher coverage versions, which can be produced by changing thresholds for classification decisions. Acknowledgments This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1232825. We thank Adrian Benton for data collection, Svitl"
W16-5614,P14-5007,0,0.145682,"Missing"
W16-5614,P16-4022,0,0.0156528,"of attempts to automatically infer user attributes from available social media data, such as a collection of messages for a user. These efforts have led to author attribute, or demographic, inference (Mislove et al., 2011; Volkova et al., 2015b; Burger et al., 2011; Volkova et al., 2015a; Pennacchiotti and Popescu, 2011; Rao and Yarowsky, 2010; Rao et al., 2010; Schwartz et al., 2013; Ciot et al., 2013; Alowibdi et al., 2013; Culotta et al., 2015) and geolocation tasks (Eisenstein et al., 2010; Han et al., 2014; Rout et al., 2013; Compton et al., 2014; Cha et al., 2015; Jurgens et al., 2015; Rahimi et al., 2016). A limitation of these content analysis methods is their reliance on multiple messages for each user (or, in the case of social network based methods, data about multiple followers or friends for each user of interest). For example, we may wish to better understand the demographics of users who tweet a particular hashtag. While having tens or hundreds of messages for each user can improve prediction accuracy, collecting more data for every user of interest may be prohibitive either in terms of API access, or in terms of the time required. In this vein, several papers have dealt with the task"
W16-5614,D13-1187,0,0.1412,"Missing"
W16-5614,N15-4005,0,0.0547595,"Missing"
W16-5614,N13-1121,1,\N,Missing
W16-5905,P98-1013,0,0.378848,"of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb like “love” in the sentence “John loves Mary”. SRL and FSP are defined with respect to a schema which provides a set of frames and roles which will serve as labels for predicates and arguments. We consider two schemas, Propbank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982; Baker et al., 1998). Propbank frames concern different senses of a lexical unit (a lemma and POS tag), so the correct frame for “love” in the case above is the frame love-v-1, as opposed to love-v-2, which is only used in modal cases like “I would love to go on vacation”. In the FrameNet schema, frames are coarser grain situations which may have many lexical units which map to them. In this case frame would be Experiencer focus which could also be evoked by the adore.v or despise.v lexical units. These frames will constitute another vector f = [f1 , f2 , ...fn ] of frames for each predicate in t. Once t and f ar"
W16-5905,Q15-1039,0,0.0260718,"uction. We found that defining costs based on the Hamming loss of an action performed very poorly. We found much better results with the multiclass hinge encoding described in Lee et al. (2004). In figure 6 we show performance with various choices of roll-in and cost definitions. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perc"
W16-5905,W11-0906,0,0.0217947,"ormed very poorly. We found much better results with the multiclass hinge encoding described in Lee et al. (2004). In figure 6 we show performance with various choices of roll-in and cost definitions. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perceptron (VFP) and Locally Optimal Learning to Search (LOLS) frameworks, explaini"
W16-5905,P04-1015,0,0.561767,"ch is put on the next beam. In VFP, the core concept is a violation. A tuple (x, y, z), where x is a sentence as defined earlier and y is a string of correct actions (having zero cost/loss), and z is a string of predicted actions, is a violation if θ · f (x, z) &gt; θ · f (x, y) and z is “incorrect”. There are multiple ways of defining incorrect which yield different algorithms in the VFP family. In all variants y and z must be the same length and if there is more than one incorrect (x, y, z), the one with the largest difference in score is chosen. In the early update variant, first described by Collins and Roark (2004), z is incorrect if it differs from y only in the last position. In max violation z is incorrect if 47 Global Feature numArgs roleCooc argLoc roleCoocArgLoc full Gold f PB ∆` FN ∆` -0.4 -0.1 -0.4 -0.3 -1.2 -0.4 -2.0 -0.2 -1.5 -0.7 Auto f PB ∆` FN ∆` -1.3 +0.3 -0.1 +0.6 -1.9 +0.2 0.0 +0.2 -2.0 +0.2 Figure 1: Global model advantage using max violation VFP and freq. it differs any position. In latest update z is incorrect if it differs in the last position (but can include other differences, unlike early update). Results In figure 1 we plot the difference in performance between a model which incl"
W16-5905,N10-1138,0,0.0191829,"f the transition system on the usefulness of global features. We find that the order that actions are performed in can be as important as the training method, leading to better models with the same features and computational complexity. 2 Problem Formulation Semantic role labeling (Gildea and Jurafsky, 2002) (SRL) is the task of locating and labeling (with roles) the semantic arguments to predicates. Adding 44 Proceedings of the Workshop on Structured Prediction for NLP, pages 44–53, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics a step, frame semantic parsing (Das et al., 2010) (FSP), seeks to first disambiguate predicates by labeling them with a frame before performing SRL. Semantic roles abstract over grammatical function and provide information about particular arguments relation to an event, state, or fact. SRL has been shown to be helpful in a variety of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb li"
W16-5905,S12-1029,0,0.0184142,"ants as a linear interpolation between a global and local objective. models consistently improve over local models. 8.2 LOLS Throughout the paper we have listed relative performance. Our absolute performance is 73.0 for Propbank (dev) and 55.3 for FrameNet (dev). This falls significantly short of the work of Zhou and Xu (2015) at 81.1 (PB dev), FitzGerald et al. (2015) at 79.2 (PB dev), and 72.0 (FN). Those works used non-linear neural models with multi-task distributed representations, which are not comparable to our results. However, the models of Pradhan et al. (2013) at 77.5 (PB test) and Das et al. (2012) at 64.6 (FN test) are roughly comparable, and the performance gap is still significant. While our efforts do not advance the state of the art in SRL, we hope that they are enlightening with respect to the application of various imitation learning methods. LOLS performs a roll-in with the current policy. This causes many updates which are derived from mistakes during frame identification. Once the wrong frame is predicted, in argument identification the model’s cost incentives flip towards trying to predict ∅ for all roles so as not to incur false positives. The roles in FrameNet are defined b"
W16-5905,D15-1112,0,0.0230135,"Missing"
W16-5905,J02-3001,0,0.613884,"eness of the Violation Fixing Perceptron (VFP) (Huang et al., 2012) and Locally Optimal Learning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features. We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions. We also show that action ordering, including easy first inference, has a large impact on the quality of greedy global models. 1 Introduction In structured prediction problems, global features express dependencies between related pieces of a label and make inference non-trivial. In Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), global features and constraints have been studied extensively (Punyakanok et al., 2004; Toutanova et al., 2008; T¨ackstr¨om et al., 2015) inter alia. SRL has many phenomenon that relate labels such as syntactic control, role mutual exclusion, and structural constraints like span overlap. Previous work on inference for models with global features has studied a variety of method including dynamic programming, reranking, and ILP solvers. Greedy search and beam search are relatively understudied areas due to the difficulty in training models which perform well with the weak guarantees provided b"
W16-5905,N10-1115,0,0.031505,"th the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inference in this work (i.e. a beam size of 1). As far as we know, efficient greedy and easy first inference are mutually exclusive goals, and we focus on the latter. Our implementation uses a heap to store actions in a manner similar to Goldberg and Elhadad (2010). This way actions can be generated once, instead of once per transition, and global features perform sparse updates to the actions on the heap. For beam search, states cannot share a heap (since their histories, and thus global features, would be different), so actions generation, global features, and action sorting would have to occur at every transition. All performance values shown here are measured for the task of frame semantic parsing (FSP), meaning that we measure precision, recall, and F-measure where every index in f and k are considered predictions. Predictions in k are not correct"
W16-5905,P14-1136,0,0.0126481,"lobal feature template, we try each refinement and use the one with the best dev set F-measure when trained with LOLS. 4 Experimental Design We measure performance on two data sets, the Propbank annotations (Kingsbury and Palmer, 2002) available in the Ontonotes 5.0 corpus (Pradhan et al., 2012) and FrameNet 1.5 (Baker et al., 1998). For all learning methods we average the weights across all iterations of training (Freund and Schapire, 1999). This is explicitly called for as a part of LOLS and is also a standard trick used with the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inference in this work (i.e. a beam size of 1). As far as we know, efficient greedy and easy first inference are mutually exclusive goals, and we focus on the latter. Our implementation uses a heap to store actions in a manner similar to Goldberg and Elhadad (2010). This way actions can be generated once, instead of once per t"
W16-5905,N12-1015,0,0.0529926,"Missing"
W16-5905,kingsbury-palmer-2002-treebank,0,0.0872977,"e, or fact. SRL has been shown to be helpful in a variety of NLP tasks including information extraction, question answering, and coreference resolution. Let x refer to a sentence and its POS tags and dependency parse. For this work, we are given x and a vector of predicate locations t = [t1 , t2 , ...tn ], where each ti is a span, most often representing a single verb like “love” in the sentence “John loves Mary”. SRL and FSP are defined with respect to a schema which provides a set of frames and roles which will serve as labels for predicates and arguments. We consider two schemas, Propbank (Kingsbury and Palmer, 2002) and FrameNet (Fillmore, 1982; Baker et al., 1998). Propbank frames concern different senses of a lexical unit (a lemma and POS tag), so the correct frame for “love” in the case above is the frame love-v-1, as opposed to love-v-2, which is only used in modal cases like “I would love to go on vacation”. In the FrameNet schema, frames are coarser grain situations which may have many lexical units which map to them. In this case frame would be Experiencer focus which could also be evoked by the adore.v or despise.v lexical units. These frames will constitute another vector f = [f1 , f2 , ...fn ]"
W16-5905,W12-4501,0,0.0117087,"granularity for the global feature templates, we consider multiple refinements. A refinement of a template is the result of taking the pointwise product of the template with one or two label features templates. The label feature templates we consider are constant (a backoff feature), frame, role, and frame-role. For each global feature template, we try each refinement and use the one with the best dev set F-measure when trained with LOLS. 4 Experimental Design We measure performance on two data sets, the Propbank annotations (Kingsbury and Palmer, 2002) available in the Ontonotes 5.0 corpus (Pradhan et al., 2012) and FrameNet 1.5 (Baker et al., 1998). For all learning methods we average the weights across all iterations of training (Freund and Schapire, 1999). This is explicitly called for as a part of LOLS and is also a standard trick used with the structured perceptron. We use the local features described in Hermann et al. (2014) for argument and frame identification, but we did not use their feature embedding method since it performed about as well as the sparse feature method and was slower. We use the best refinements using the process described in §3. We are studying the fully greedy case of inf"
W16-5905,W13-3516,0,0.0336754,"Missing"
W16-5905,W04-2421,0,0.126698,"rning to Search (LOLS) (Chang et al., 2015) frameworks with respect to SRL global features. We describe problems in applying each framework to SRL and evaluate the effectiveness of some solutions. We also show that action ordering, including easy first inference, has a large impact on the quality of greedy global models. 1 Introduction In structured prediction problems, global features express dependencies between related pieces of a label and make inference non-trivial. In Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), global features and constraints have been studied extensively (Punyakanok et al., 2004; Toutanova et al., 2008; T¨ackstr¨om et al., 2015) inter alia. SRL has many phenomenon that relate labels such as syntactic control, role mutual exclusion, and structural constraints like span overlap. Previous work on inference for models with global features has studied a variety of method including dynamic programming, reranking, and ILP solvers. Greedy search and beam search are relatively understudied areas due to the difficulty in training models which perform well with the weak guarantees provided by greedy search. The Violation Fixing Perceptron (VFP) framework (Huang et al., 2012) is"
W16-5905,D10-1048,0,0.0134057,"Results In figure 2 we plot global model advantage using the freq action orderings and LOLS training. There are mixed results; some global features are actually improving over the local model (something which was not achieved by VFP training). We will return to why this is in §8.2, but first we will analyze an orthogonal aspect of the model. 7 Action Ordering So far our transition system considers actions sorted by frequency of a role, which may not be optimal. Here we measure the effect of other orderings. Easy First The first motivation is related to easy first inference (Shen et al., 2007; Raghunathan et al., 2010) inter alia. The idea is that the “easiest” decisions should be made first because there is less risk that they are wrong and may be more safely conditioned on in making future decisions than any other action. To implement this heuristic, we define two variants of the easyfirst meta action ordering. easyfirst-dynamic chooses the variable index corresponding to the highest scoring action. easyfirst-static chooses variable indices sorted by 2 Every action fills in a label and we can say whether it is right or wrong, thus the reference policy is the one which always fills in a correct label. the"
W16-5905,P07-1096,0,0.0230825,"return to in §8.2. Results In figure 2 we plot global model advantage using the freq action orderings and LOLS training. There are mixed results; some global features are actually improving over the local model (something which was not achieved by VFP training). We will return to why this is in §8.2, but first we will analyze an orthogonal aspect of the model. 7 Action Ordering So far our transition system considers actions sorted by frequency of a role, which may not be optimal. Here we measure the effect of other orderings. Easy First The first motivation is related to easy first inference (Shen et al., 2007; Raghunathan et al., 2010) inter alia. The idea is that the “easiest” decisions should be made first because there is less risk that they are wrong and may be more safely conditioned on in making future decisions than any other action. To implement this heuristic, we define two variants of the easyfirst meta action ordering. easyfirst-dynamic chooses the variable index corresponding to the highest scoring action. easyfirst-static chooses variable indices sorted by 2 Every action fills in a label and we can say whether it is right or wrong, thus the reference policy is the one which always fil"
W16-5905,Q15-1003,0,0.0242568,"Missing"
W16-5905,J08-2002,0,0.0335123,"Missing"
W16-5905,D16-1137,0,0.0130458,"ns. The best LOLS global 5 If you label a span as the Cognizer role for the frame Opinion and that span was the Cognizer role for the Judgment frame, then the label is wrong. 6 with the exception of ARG0 and ARG1 which typically correspond to proto-Agent and proto-Patient roles. 51 8.3 9 Absolute Performance Related Work Berant and Liang (2015) used imitation learning for learning a semantic parser. Choi and Palmer (2011) explored transition based SRL and proposed some global features (e.g. copy ARG0 from controlling predicates) but did not consider action (re-)ordering or imitation learning. Wiseman and Rush (2016) derive a learning to search framework which is related to LaSO (Daum´e III and Marcu, 2005). Similar to our hybrid roll-in, they “reset” the beam as soon as the oracle prefix falls off. 10 Conclusion In this work we study the use of imitation learning for greedy global models for SRL. We analyze the Violation Fixing Perceptron (VFP) and Locally Optimal Learning to Search (LOLS) frameworks, explaining how they fall short and offer some methods for improving them. We also study the effect of inference order on learning and the utility of global features, finding that it is a very important fact"
W16-5905,W04-3212,0,0.0622974,"efines a function mapping a frame to a set of roles K(fi ) which each frame must have filled explicitly (by some mention span in the sentence) or implicitly (by some other discourse entity not directly mentioned in the sentence). For the latter case we say that an unfilled role is filled by a special dummy span called ∅. For the former case, we could in principle predict any span within the sentence, but to make systems faster and more accurate, a pruning step is often used which picks out only the spans which are plausible arguments to a particular predicate conditioned on a syntactic parse (Xue and Palmer, 2004). We call this set S(ti )1 and it always 1 Extensions like the one described in T¨ackstr¨om et al. (2015) consider the role during the pruning step, but we gloss 45 includes ∅. SRL is the task of predicting a matrix k = {kij : i ∈ [1..n], j ∈ K(fi ), kij ∈ S(ti )} where kij is the location of the j th role for frame fi evoked by the predicate at ti . For the rest of this paper, we will concern ourselves with the FSP task of predicting both f and k. Transition System A transition system provides a way to break down an assignment to (f, k) into a sequence of actions. The transition systems we us"
W16-5905,P15-1109,0,0.0194193,"gure 5: Global model advantage using roleCooc and easyfirst-dynamic across VFP variations and + CLASS. Figure 6: Global model advantage using roleCooc and easyfirst-dynamic across LOLS variations: roll-in and cost function. which is a pure CLASS update, so you can think of the + CLASS variants as a linear interpolation between a global and local objective. models consistently improve over local models. 8.2 LOLS Throughout the paper we have listed relative performance. Our absolute performance is 73.0 for Propbank (dev) and 55.3 for FrameNet (dev). This falls significantly short of the work of Zhou and Xu (2015) at 81.1 (PB dev), FitzGerald et al. (2015) at 79.2 (PB dev), and 72.0 (FN). Those works used non-linear neural models with multi-task distributed representations, which are not comparable to our results. However, the models of Pradhan et al. (2013) at 77.5 (PB test) and Das et al. (2012) at 64.6 (FN test) are roughly comparable, and the performance gap is still significant. While our efforts do not advance the state of the art in SRL, we hope that they are enlightening with respect to the application of various imitation learning methods. LOLS performs a roll-in with the current policy. This"
W16-5905,C98-1013,0,\N,Missing
W16-6204,D12-1032,1,0.87472,"we use a grid search to find the hyperparameters that yield the highest score on the development split, and then use those same hyperparameters for testing with no further tuning. We compare the performance of the full pipeline (FULL), as well as a variation which does no disambiguation (NO - CONTEXT). 6 Permitted by the Twitter terms of service: https://dev. twitter.com/overview/terms/agreement-and-policy 22 Andrews et al. (2014) (P HYLO) developed a generative model for clustering entities across documents based on name and context similarity.7 Their work extended a phylogenetic name model (Andrews et al., 2012) that learns groups of name variations through string transducers by composing a phylogeny of name variation based on unlabeled data. As above, we present versions of the model with both context and name matching (FULL) as well as without context (NO - CONTEXT). Parameters are tuned on dev data as with G REEN. A unique property of TGX is its temporal ordering, where documents are timestamped and time impacts entity priors. Figure 4 shows the number of mentions for the top 10 entities over time. The curves are highly peaked, suggesting that there is a small window in time in which the entity is"
W16-6204,P14-1073,1,0.950265,"include temporal information, which can be helpful when documents (such as tweets) arrive in a specific order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity linking – or more generally"
W16-6204,P98-1012,0,0.255828,"viding a large set of annotated tweets focusing on a single event. To establish a baseline we evaluate two CDCR systems and consider the performance impact of each system component. Furthermore, we augment one system to include temporal information, which can be helpful when documents (such as tweets) arrive in a specific order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards"
W16-6204,D08-1029,0,0.19314,"performance impact of each system component. Furthermore, we augment one system to include temporal information, which can be helpful when documents (such as tweets) arrive in a specific order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies"
W16-6204,N15-1075,0,0.0124961,"an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containing references to people, and references are annotated both"
W16-6204,D07-1074,0,0.602401,": A Social Media Corpus for Entity Linking and Disambiguation Mark Dredze, Nicholas Andrews, Jay DeYoung Human Language Technology Center of Excellence Johns Hopkins University 810 Wyman Park Drive Baltimore, MD 20211 USA {mdredze,noa}@cs.jhu.edu Abstract et al., 2010; Han and Sun, 2011). Entity linking systems use the name mention and a context model to identify possible candidates and disambiguate similar entries. The context model includes a variety of information from the context, such as the surrounding text or facts extracted from the document. Though early work on the task goes back to Cucerzan (2007), the name entity linking was first introduced as part of TAC KBP 2009 (McNamee and Dang, 2009). Work on cross document coreference resolution (CDCR) has primarily focused on news articles, with little to no work for social media. Yet social media may be particularly challenging since short messages provide little context, and informal names are pervasive. We introduce a new Twitter corpus that contains entity annotations for entity clusters that supports CDCR. Our corpus draws from Twitter data surrounding the 2013 Grammy music awards ceremony, providing a large set of annotated tweets focusi"
W16-6204,C10-1032,1,0.774905,"d up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity linking – or more generally Wikification (Ratinov et al., 2011) – disambiguates reference in the context of a knowledge base, such as Wikipedia (Cucerzan, 2007; McNamee and Dang, 2009; Dredze et al., 2010; Zhang While both tasks have traditionally considered formal texts, recent work has begun to consider informal genres, which pose a number of interesting challenges, such as increased spelling variation and (especially for Twitter) reduced context for disambiguation. Yet entity disambiguation, which links mentions across documents, is especially important for social media, where understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International W"
W16-6204,W10-0713,1,0.732024,"y disambiguation, which links mentions across documents, is especially important for social media, where understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Gra"
W16-6204,fromreide-etal-2014-crowdsourcing,0,0.0182979,"especially important for social media, where understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containin"
W16-6204,N12-1007,1,0.937248,"gment one system to include temporal information, which can be helpful when documents (such as tweets) arrive in a specific order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity link"
W16-6204,N13-1122,0,0.750816,", there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containing references to people, and references are annotated both for entity linking and CDCR. To explore this task for social media data and consider the challenges, opportunities and the performance of s"
W16-6204,P11-1095,0,0.0464155,"Missing"
W16-6204,D12-1045,0,0.0189467,"Furthermore, we augment one system to include temporal information, which can be helpful when documents (such as tweets) arrive in a specific order. Finally, we include annotations linking the entities to a knowledge base to support entity linking. Our corpus is available: https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual ref"
W16-6204,P11-1037,0,0.0243065,"ich links mentions across documents, is especially important for social media, where understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards c"
W16-6204,P12-1055,0,0.0172701,"here understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containing references to people, and referen"
W16-6204,P13-1128,0,0.313941,"e background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containing references to people, and references are annotated both for entity linking and CDCR. To explore this task for social media data and consider the challenges,"
W16-6204,D15-1064,1,0.82583,"es reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus includes tweets containing references to people, and references are annotated both for entity linking and"
W16-6204,C10-2121,1,0.896458,"Missing"
W16-6204,P11-1138,0,0.0814158,"ionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity linking – or more generally Wikification (Ratinov et al., 2011) – disambiguates reference in the context of a knowledge base, such as Wikipedia (Cucerzan, 2007; McNamee and Dang, 2009; Dredze et al., 2010; Zhang While both tasks have traditionally considered formal texts, recent work has begun to consider informal genres, which pose a number of interesting challenges, such as increased spelling variation and (especially for Twitter) reduced context for disambiguation. Yet entity disambiguation, which links mentions across documents, is especially important for social media, where understanding an event often requires reading multiple short messages, as op"
W16-6204,D11-1141,0,0.350395,"across documents, is especially important for social media, where understanding an event often requires reading multiple short messages, as opposed to news articles, which have extensive background information. For example, there have now 20 Proceedings of The Fourth International Workshop on Natural Language Processing for Social Media, pages 20–25, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics been several papers to consider named entity recognition in social media, a key first step in an entity disambiguation pipeline (Finin et al., 2010; Liu et al., 2011; Ritter et al., 2011; Fromreide et al., 2014; Li et al., 2012; Liu et al., 2012; Cherry and Guo, 2015; Peng and Dredze, 2015). Additionally, some have explored entity linking in Twitter (Liu et al., 2013; Meij et al., 2012; Guo et al., 2013), and have created datasets to support evaluation. However, to date no study has evaluated CDCR on social media data,1 and there is no annotated corpus to support such an effort. In this paper we present a new dataset that supports CDCR in Twitter: the TGX corpus (Twitter Grammy X-doc), a collection of Tweets collected around the 2013 Grammy music awards ceremony. The corpus i"
W16-6204,P11-1080,0,0.0199331,"t.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity linking – or more generally Wikification (Ratinov et al., 2011) – disambiguates reference in the context of a knowledge base, such as Wikipedia (Cucerzan, 2007; McNamee and Dang, 2009; Dredze et al., 2010; Zhang While both tasks have traditionally considered formal texts, rece"
W16-6204,P12-1040,0,0.0156471,": https: //bitbucket.org/mdredze/tgx 1 Without a knowledge base, cross-document coreference resolution (CDCR) clusters mentions to form entities (Bagga and Baldwin, 1998b). Since 2011, CDCR has been included as a task in TAC-KBP (Ji et al., 2011) and has attracted renewed interest (Baron and Freedman, 2008b; Rao et al., 2010; Lee et al., 2012; Green et al., 2012; Andrews et al., 2014). Though traditionally a task restricted to small collections of formal documents (Bagga and Baldwin, 1998b; Baron and Freedman, 2008a), recent work has scaled up CDCR to large heterogenous corpora, e.g. the Web (Wick et al., 2012; Singh et al., 2011; Singh et al., 2012). Entity Disambiguation Who is who and what is what? Answering such questions is usually the first step towards deeper semantic analysis of documents, e.g., extracting relations and roles between entities and events. Entity disambiguation identifies real world entities from textual references. Entity linking – or more generally Wikification (Ratinov et al., 2011) – disambiguates reference in the context of a knowledge base, such as Wikipedia (Cucerzan, 2007; McNamee and Dang, 2009; Dredze et al., 2010; Zhang While both tasks have traditionally considere"
W16-6204,C10-1145,0,0.0525228,"Missing"
W17-1612,D11-1120,0,0.0110818,"ng data across sources. Douriez et al. (2016) describe how the New York City Taxi Dataset can be deanonymized by collecting taxi location information from four popular intersections. Narayanan and Shmatikov (2008) showed that the identify of users in the anonymized Netflix challenge data can be revealed by mining the Internet Movie Database. Combinations of public data can create new sensitivities and must be carefully evaluated on a case-by-case basis. In some cases, users may explicitly link accounts across platforms, such as including in a Twitter profile a link to a LinkedIn page or blog (Burger et al., 2011). Other times users may not make these links explicit, intentionally try to hide the connections, or the connections are inferred by the researcher, e.g. by similarity in user handles. These factors should be considered when conducting research that links users across multiple platforms. It goes without saying that linking public posts to private, sensitive fields (electronic health records) should be handled with the utmost care (Padrez et al., 2015). We strongly encourage researchers to share datasets and annotations they have created so that others can replicate research findings and develo"
W17-1612,W15-1201,1,0.872128,"to these questions provide a framework within which we can decide which avenues of research should be pursued. Virtually all technology is dual-use: it can be used for good or ill. The existence of an ill use does not mean that the technology should not be developed, nor does the existence of a good mean that it should. To focus our discussion on the pragmatic, we will use mental health research as a concrete use case. A research community has grown around using social media data to assess and understand mental health (Resnik et al., 2013; Schwartz et al., 2013; Preotiuc-Pietro et al., 2015; Coppersmith et al., 2015a; De Choudhury et al., 2016). Our discussion on the benefits and risks of such research is sharpened by the discrimination and stigma surrounding mental illness. The discrimination paired with potentially lethal outcomes put the risks and benefits of this type of research in stark relief – not sufficiently protecting users’/subjects’ privacy, may exacerbate the challenge, discourage individuals from seeking treatment and erode public trust in researchers. Similarly, insufficient research results in a cost measured in human lives – in the United States, more than 40,000 die from suicide each y"
W17-1612,W15-1204,1,0.190877,"Missing"
W17-1612,W15-1203,0,0.148051,"h less potential harm? Answers to these questions provide a framework within which we can decide which avenues of research should be pursued. Virtually all technology is dual-use: it can be used for good or ill. The existence of an ill use does not mean that the technology should not be developed, nor does the existence of a good mean that it should. To focus our discussion on the pragmatic, we will use mental health research as a concrete use case. A research community has grown around using social media data to assess and understand mental health (Resnik et al., 2013; Schwartz et al., 2013; Preotiuc-Pietro et al., 2015; Coppersmith et al., 2015a; De Choudhury et al., 2016). Our discussion on the benefits and risks of such research is sharpened by the discrimination and stigma surrounding mental illness. The discrimination paired with potentially lethal outcomes put the risks and benefits of this type of research in stark relief – not sufficiently protecting users’/subjects’ privacy, may exacerbate the challenge, discourage individuals from seeking treatment and erode public trust in researchers. Similarly, insufficient research results in a cost measured in human lives – in the United States, more than 40,0"
W17-1612,D13-1133,0,0.0144465,"there another feasible route to the good with less potential harm? Answers to these questions provide a framework within which we can decide which avenues of research should be pursued. Virtually all technology is dual-use: it can be used for good or ill. The existence of an ill use does not mean that the technology should not be developed, nor does the existence of a good mean that it should. To focus our discussion on the pragmatic, we will use mental health research as a concrete use case. A research community has grown around using social media data to assess and understand mental health (Resnik et al., 2013; Schwartz et al., 2013; Preotiuc-Pietro et al., 2015; Coppersmith et al., 2015a; De Choudhury et al., 2016). Our discussion on the benefits and risks of such research is sharpened by the discrimination and stigma surrounding mental illness. The discrimination paired with potentially lethal outcomes put the risks and benefits of this type of research in stark relief – not sufficiently protecting users’/subjects’ privacy, may exacerbate the challenge, discourage individuals from seeking treatment and erode public trust in researchers. Similarly, insufficient research results in a cost measured"
W17-2612,D16-1001,0,0.00703582,"Missing"
W17-2612,P07-1033,0,0.583604,"Missing"
W17-2612,D08-1072,1,0.830941,"anguage Technology Center of Excellence Center for Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract 2006) and supervised (Daum´e III, 2007) variants, depending on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and"
W17-2612,P15-1033,0,0.00558804,"is only one task. 2.1 BiLSTM for representation learning Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN) that models interdependencies in sequential data. It addresses the vanishing or exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) problems of vanilla RNNs by using a series of gates (input, forget and output gates) to control how memory is propagated in the hidden states of the model, and thus effectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for"
W17-2612,P16-1101,0,0.393488,"gnition in social media data. 2 Projec@on for domain one … yn Decoders for y1 task T … … Shared Representa@on Learner … yn y1 … framework based on MTL that incorporates parameter tying strategies common in domain adaptation. Our framework is based on a bidirectional long short-term memory network with a conditional random fields (BiLSTM-CRFs) (Lample et al., 2016) for sequence tagging. We consider sequence tagging problem since they are common in NLP applications and have been demonstrated to benefit from learning representations (Lample et al., 2016; Yang et al., 2016; Peng and Dredze, 2016; Ma and Hovy, 2016). This paper makes the following contributions: The shared representation learner, domain projections and task specific models can be instantiated based on the application. In this paper, we focus on sequence tagging problems. We now introduce our instantiated neural architecture for multitask domain adaptation for sequence tagging. Model We begin with a brief overview of our model, and then instantiate each layer with specific neural architectures to conduct multi-task domain adaptation for sequence tagging. Figure 1 summarizes the entire model presented in this section. A representation lear"
W17-2612,I05-3017,0,0.0473747,"Missing"
W17-2612,N09-1068,0,0.152487,"Language and Speech Processing Johns Hopkins University, Baltimore, MD, 21218 npeng1@jhu.edu, mdredze@cs.jhu.edu Abstract 2006) and supervised (Daum´e III, 2007) variants, depending on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al.,"
W17-2612,D15-1064,1,0.0571464,"ck-propagation. We output spaces need separate task specific models. use alternating optimization among each dataset For our applications to sequence tagging probwith stochastic gradient descent (SGD). To prelems, we choose Conditional Random Fields vent training from skewing the model to a specific (CRFs) (Lafferty et al., 2001) as task specific moddataset due to the optimization order, we subsamels, since it is widely used in previous work and ple the number of instances used in each epoch is shown to benefit from learning representations with a fraction λ w.r.t. the smallest dataset size, (Peng and Dredze, 2015; Lample et al., 2016; Ma which is tuned as a hyper-parameter on developand Hovy, 2016). These “Neural-CRFs” define ment data. A separate learning rate is tuned for the conditional probability of a sequence of labels each dataset, and we decay the learning rate when given the input as: results on development data do not improve af Qn k , y k , ψ(xk )) ter 5 consecutive epochs. We train for up to 30 exp W T F (yi−1 i p(y k |xk ; W ) = i=1 ,epochs and use early stopping (Caruana et al., Zk 2001; Graves et al., 2013) as measured on develwhere i indexes the position in the sequence, F is the feat"
W17-2612,D12-1119,1,0.0785808,"Missing"
W17-2612,P16-2025,1,0.943052,"et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al., 2016c; Peng and Dredze, 2016; Yang et al., 2016; Liu et al., 2016a). Given the similarity between domain adaptation and MTL, it is natural to ask: can domain adaptation benefit from jointly learning across several tasks? This paper investigates how MTL can induce better representations for domain adaptation. There are several benefits. First, learning multiple tasks provides more training data for learning. Second, MTL provides a better inductive learning bias so that the learned representations better generalize. Third, considering several tasks in domain adaptation opens up the opportunities to adapt from a different d"
W17-2612,Q16-1023,0,0.00609985,"between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representation for the whole sequence is thus denoted as h = h1:n . 2.2 m2 = [~1, ~0, ~"
W17-2612,Q17-1008,1,0.873395,"Missing"
W17-2612,P16-2038,0,0.0861928,"Missing"
W17-2612,N16-1030,0,0.419543,"ask specific models (top layer) contain one model per task. • A new domain/task mismatch setting: where you have two datasets from two different, but related domains and tasks. • State-of-the-art results on Chinese word segmentation and named entity recognition in social media data. 2 Projec@on for domain one … yn Decoders for y1 task T … … Shared Representa@on Learner … yn y1 … framework based on MTL that incorporates parameter tying strategies common in domain adaptation. Our framework is based on a bidirectional long short-term memory network with a conditional random fields (BiLSTM-CRFs) (Lample et al., 2016) for sequence tagging. We consider sequence tagging problem since they are common in NLP applications and have been demonstrated to benefit from learning representations (Lample et al., 2016; Yang et al., 2016; Peng and Dredze, 2016; Ma and Hovy, 2016). This paper makes the following contributions: The shared representation learner, domain projections and task specific models can be instantiated based on the application. In this paper, we focus on sequence tagging problems. We now introduce our instantiated neural architecture for multitask domain adaptation for sequence tagging. Model We begi"
W17-2612,D15-1206,0,0.00752568,"del, and thus effectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representat"
W17-2612,W06-0115,0,0.0326581,"Missing"
W17-2612,N15-1069,0,0.0923214,"ing on whether there exists no or some training data in the target domain. This paper considers the case of supervised domain adaptation, where we have a limited amount of target domain training data, but much more training data in a source domain. Work on domain adaptation mostly follows two approaches: parameter tying (i.e. linking similar features during learning) (Dredze and Crammer, 2008; Daum´e III, 2007, 2009; Finkel and Manning, 2009; Kumar et al., 2010; Dredze et al., 2010), and learning cross domain representations (Blitzer et al., 2006, 2007; Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2015). Often times, domain adaptation is formulated as learning a single model for the same task across domains, although with a focus on maximizing target domain performance. This is similar in spirit to multi-task learning (MTL) (Caruana, 1997) which jointly learns models for several tasks, for example. learning a single data representation common to each task (Ando and Zhang, 2005; Collobert et al., 2011; Liu et al., 2016c; Peng and Dredze, 2016; Yang et al., 2016; Liu et al., 2016a). Given the similarity between domain adaptation and MTL, it is natural to ask: can domain adaptation benefit from"
W17-2612,D16-1012,0,0.0180855,"Missing"
W17-2612,P13-2032,0,0.0648576,"Missing"
W17-2612,Y15-1009,0,0.0049188,"ectively captures long-distance dependencies between the inputs. Many NLP applications use bi-directional LSTMs (BiLSTM) (Dyer et al., 2015) to scan both left-to-right and right-to-left, which capture left and right context. The hidden vectors produced by both LSTMs are concatenated to form the final → − ← − output vector ht = ht ⊕ ht . BiLSTMs have become a common building block for learning representations in NLP and have achieved impressive performance in problems such as sequence tagging (Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016), relation classification (Xu et al., 2015; Zhang et al., 2015), and syntactic parsing (Kiperwasser and Goldberg, 2016; Cross 92 and the remaining dimensions for domain 2. The mask for domain 1 and domain 2 would be: and Huang, 2016). We use a BiLSTM as our representation learner. It produces a hidden vector for each token in the sentence, which we denote as: ht = BiLSTM(x1:n , t) m1 = [~1, ~1, ~0], (1) (2) We can then apply these masks directly to the hidden vectors h learned by the BiLSTM to produce ˆ a projected hidden state h: where x1:n denotes the whole input sequence of length n, and t denotes the t-th position. The representation for the whole seq"
W17-2612,W06-1615,0,\N,Missing
W17-2912,N13-1121,1,0.925437,"Missing"
W17-2912,P11-1137,0,0.0864936,"Missing"
W17-2912,W16-5614,1,0.851834,"Missing"
W17-2912,W15-1201,1,0.857206,"Missing"
W17-2912,N16-1122,1,0.893003,"Missing"
W17-2912,D15-1256,0,0.124348,"estimate variations of beliefs and behaviors across demographic groups. Since social media analysis relies on how people use platforms, variations in usage behaviors by different demographic groups could introduce biases in analyses and alter conclusions. For example, if one group tends to use Twitter nicknames more frequently, a name-based demographic classifier may make more errors on members of that group. Alternatively, if we use profile pictures to infer demographics and users of one demographic are less likely to share pictures of themselves, our results may under-represent that group. Pavalanathan and Eisenstein (2015) studied these issues for geolocation algorithms, finding that classifiers which infer users’ locations identify a target population that differs from the general population of Twitter. A Pew Report survey indicated that social media users’ privacy settings do vary across demographics, but did not look at specific behaviors (Madden, 2012). This paper presents a first analysis of how differences in social media behaviors between demographic groups may confound demographic inference. Our aim is to identify potential sources Demographically-tagged social media messages are a common source of data"
W17-2912,P14-1018,0,0.1201,"Missing"
W17-4405,P14-1073,1,0.859834,"gira rweba@scs.howard.edu Howard University Chris Callison-Burch ccb@cis.upenn.edu University of Pennsylvania Abstract and Dredze, 2011; Guo et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of"
W17-4405,W16-4405,1,0.751642,"et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that s"
W17-4405,W16-3909,1,0.839797,"et al., 2013) and making crucial real-time decisions such as earthquake detection (Sakaki et al., 2010). Twitter is an informal forum that imposes a limit on the number of characters per tweet, hence, the vocabulary used to express tweets are diverse. This results in the prevalence of abbreviated or misspelled words in tweets and aliases used to represent named entities. Entity name variation poses a challenge to determining what or who a name refers to (Andrews et al., 2014); identifying name variations has been shown to help in different domains such as community question answering systems (Andy et al., 2016b,a) and automatic paraphrase acquisition (Shinyama et al., 2002). For example, given the following tweets that occurred in a 5-minute time period during an event: In certain fields, real-time knowledge from events can help in making informed decisions. In order to extract pertinent realtime knowledge related to an event, it is important to identify the named entities and their corresponding aliases related to the event. The problem of identifying aliases of named entities that spike has remained unexplored. In this paper, we introduce an algorithm, EntitySpike, that identifies entities that s"
W17-4405,W16-6204,1,0.92601,"inition: Given a sequence of tweets and entity mentions, denoted by X =({e1 ,S1 },{e2 ,S2 },....{en ,Sn }), where ei represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wi"
W17-4405,N13-1122,0,0.0340283,"represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wikipedia title pages (we intend to make this dataset available to the research community). Given a set of tweets {"
W17-4405,P13-1128,0,0.0286279,"n ,Sn }), where ei represents a named entity that spikes in popularity in a given time period e.g. Julia Louis-Dreyfus, and Si represents the set of tweets that make reference to this named entity, ei , during this specified time period; the task is to create an alias list for each ei , if one exists. Background and Preliminaries In tweets collected during an on going event, there is a small window in time in which entities spike in popularity, though they have occurrences during the whole event (Dredze et al., 2016). Candidate Entity Identification: Following previous work in entity linking (Liu et al., 2013; Guo et al., 2013), we define an entity as a Wikipedia title page. An entity mention is a sequence of tokens in a tweet that can potentially link to an entity. The Grammy Awards show is mostly about famous people and so we focus only on entities belonging to the Person category. In order to construct a Wikipedia lexicon, we collect 1.5 million English Wikipedia title pages referring to Person named entities and extracted the backlinks (incoming links to the Wikipedia title page) from each of these Wikipedia title pages (we intend to make this dataset available to the research community). Give"
W18-1108,D11-1120,0,0.124265,"Missing"
W18-1108,D13-1114,0,0.0331331,"Missing"
W18-1108,W16-5614,1,0.81984,"Missing"
W18-1108,P14-1016,0,0.016566,"private, removing them from consideration. This can be an issue as the models become stale, as behaviors of individuals and organizations on Twitter continue to shift over time (Laroche et al., 2013; Liu et al., 2014; Zhu and Chen, 2015). A larger corpus would maintain its utility for longer, and ideally, the necessary data collection should be as close to automated as possible. 2 Data Our goal was the construction of a large set of Twitter accounts annotated as individual or organization. Rather than rely on manual labeling of accounts, we seek an automated method based on weak supervision (Li et al., 2014) for the discovery and labeling of these accounts. We describe our process in this section, and evaluate the efficacy of our resulting dataset by evaluating models trained on this corpus. 2.1 Twitter Lists Twitter users can create “lists,” collections of Twitter accounts organized by topic. Examples of lists include “social-justice organizations” or “volleyball teammates.” Lists are useful ways for crowdsourcing the identified and organization of Twitter accounts. We identified Twitter lists that predominantly contained either organizations or individuals. We used a search engine to find user-"
W18-1114,N13-1121,1,0.880847,"Missing"
W18-1114,D11-1120,0,0.0586319,". Data from social media platforms such as Twitter can yield key insights into population beliefs and behaviors, complementing existing methods such as traditional surveys (Velasco et al., 2014; Dredze et al., 2015). A downside of social media sources is that they often lack traditional demographic information, such as gender, ethnicity, age, and location. Twitter is one of the most popular platforms for research, but its users rarely provide such information. Numerous existing systems automatically infer missing demographics, such as gender, ethnicity, age and location (Mislove et al., 2011; Burger et al., 2011; Culotta et al., 2015; Pennacchiotti and Popescu, 2011; Rao et al., 2010; Jurgens et al., 2015; Dredze et al., 2013; Rout et al., 2013). Most methods rely on content authored by the user, 1 http://bitbucket.org/mdredze/demographer 105 Proceedings of the Second Workshop on Computational Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 105–111 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics considered both bidirectional and unidirectional RNNs. We evaluated both max-pooling and a learned weighted average2 to convert the RNN o"
W18-1114,P14-5007,0,0.0721742,"Missing"
W18-1114,N16-1122,1,0.901417,"Missing"
W18-1114,D15-1240,0,0.0404274,"Missing"
W18-1114,P14-1018,0,0.0752994,"Missing"
W18-1114,W16-5614,1,0.659803,"ta produced ambiguous results: it greatly helped the SVM model on the gender task, but appeared to hurt performance for all models on the ethnicity tasks. A possible explanation is that, because the SVM only considered simple n-gram features, the informative n-grams for gender are relatively consistent across Census and Twitter names. The neural models, however, learn much more complicated features, and the relevant features Baselines For each task we compare our best neural models against two baselines representing prior work: a name-only method and a user content method. SVM: Knowles et al. (2016) predicts gender with a linear SVM trained on character n-gram 5 6 9 Knowles et al. (2016) defines F1 as the harmonic mean of accuracy and coverage, and thus our F1 scores are substantially lower. 10 Using a two-proportion z-test, our models outperform the SVM on gender, with p < 0.01; on 2-way ethnicity, with p < 0.01; and on 3-way ethnicity, with p < 0.02. The content baseline is significantly better than our best models, using the same test, with at least p < 0.0001. https://www.ssa.gov/OACT/babynames/names.zip Many users are no longer available on Twitter. While most accounts correspond to"
W18-1114,D13-1187,0,0.0671071,"Missing"
W18-1114,W17-2912,1,0.907948,"Missing"
W18-6124,K16-1017,0,0.022316,"etermining stance. The primary advantage of this pre-training setting is that it decouples the stance classification annotated training set from a set of user embeddings. It is not always possible to have a dataset with stance labeled tweets as well as user embeddings for each tweet’s author (as is the case for our datasets). Instead, this setting allows us to utilize a stance annotated corpus, and separately create representations for a disjoint set of pre-training users, even without knowing the identity of the authors of the annotated stance tweets. This is different than work presented by Amir et al. (2016) to improve sarcasm detection, since we are not providzi = σg (Wz xi + Uz hi−1 + bz ) ri = σg (Wr xi + Ur hi−1 + br ) ni = σh (Wh xi + Uh (ri ◦ hi−1 ) + bh ) hi = zi ◦ hi−1 + (1 − zi )ni where σg and σh are elementwise sigmoid and hyperbolic tangent activation functions respectively. W∗ and U∗ are weight matrices acting over input embeddings and previous hidden states, and b∗ are bias weights. zi is the update gate (a soft mask over the previous hidden state activations), ri is the reset gate (soft mask selecting which values to preserve from the previous hidden state), ni is the new gate, and"
W18-6124,W11-1701,0,0.0239627,"ation is motivated in part by the utility of understanding the opinions expressed by a large population (Pang et al., 2008). Sentiment analysis of movie reviews (Pang et al., 2002) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and seek to measure opinion of a piece of text towards that issue. While stance classification can be expressed as a complex set of opinions and attitudes (Rosenthal et al., 2017), we confine ourselves to the task of binary stance classification, in which we seek to determine if a single message expresses support for"
W18-6124,D11-1145,0,0.0437467,"assification dataset, improvements from pre-training are only apparent when training data is limited. 1 Introduction Social media analyses often rely on a tweet classification step to produce structured data for analysis, including tasks such as sentiment (Jiang et al., 2011) and stance (Mohammad et al., 2016) classification. Common approaches feed the text of each message to a classifier which predicts a label based on the content of the tweet. However, many of these tasks benefit from knowledge about the context of the message, especially since short messages can be difficult to understand (Aramaki et al., 2011; Collier and Doan, 2011; Kwok and Wang, 2013). One of the best sources of context is the message author herself. Consider the task of stance classification, where a system must identify the stance towards a topic expressed in a tweet. Having access to the latent beliefs of the tweet’s 184 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 184–194 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics the author would support the candidate. Therefore, when given a message from this author with the target being that speci"
W18-6124,P11-1016,0,0.0487794,"pretraining method to predict user embeddings. Although the neural stance classifiers we learn are often outperformed by a baseline SVM, author embedding pre-training yields improvements over a non-pre-trained neural network on four out of five domains in the SemEval 2016 6A tweet stance classification task. In a tweet gun control stance classification dataset, improvements from pre-training are only apparent when training data is limited. 1 Introduction Social media analyses often rely on a tweet classification step to produce structured data for analysis, including tasks such as sentiment (Jiang et al., 2011) and stance (Mohammad et al., 2016) classification. Common approaches feed the text of each message to a classifier which predicts a label based on the content of the tweet. However, many of these tasks benefit from knowledge about the context of the message, especially since short messages can be difficult to understand (Aramaki et al., 2011; Collier and Doan, 2011; Kwok and Wang, 2013). One of the best sources of context is the message author herself. Consider the task of stance classification, where a system must identify the stance towards a topic expressed in a tweet. Having access to the"
W18-6124,D16-1084,0,0.0354151,"Missing"
W18-6124,P16-2003,1,0.896317,"ia stance classifiers? We may have previous messages from the user, social network information, and a variety of other types of online behaviors. How can we best summarize a wide array of user behavior in an online platform into a single, concise representation? We answer this question by exploring several representations of context encoded as a user embedding: a low-dimensional representation of the user that can be used as features by the classification system. We include a multiview user embedding method that is designed to summarize multiple types of user information into a single vector (Benton et al., 2016). How can we best use contextual information about the author in the learning process? Ideally, we would be provided a learned user representation along with every message we were asked to classify. This is unrealistic. Learning user representations requires data to be collected for each user and computation time to process that data. Neither of these are available in many production settings, where millions of messages are streamed on a given topic. It is impractical to insist that additional information be collected for each user and new representations inferred, for each tweets that the cla"
W18-6124,W16-3920,0,0.0262219,"tance class labels based on a convex combination of hidden states. For this baseline model, the RNN is fit directly to the training set, without any pre-training, i.e. training maximizes the likelihood of class labels given the input tweet. We now consider an enhancement to our base model that incorporates user embeddings. Models The stance classification tasks we consider focus on tweets: short snippets of informal text. We rely on recurrent neural networks as a base classification model, as they have been effective classifiers for this type of data (Tang et al., 2015; Vosoughi et al., 2016; Limsopatham and Collier, 2016; Yang et al., 2017; Augenstein et al., 2016). Our base classification model is a gated recurrent unit (GRU) recurrent neural network classifier (Cho et al., 2014). The GRU consumes the input text as a sequence of tokens and produces a sequence of final hidden state activations. Input layer word embeddings are initialized with GloVe embeddings pre-trained on Twitter text (Pennington et al., 2014). The update equations for the gated recurrent unit at position i in a sentence are: RNN Classifier with User Embedding Pretraining We augment the base RNN classifier with an additional final (output)"
W18-6124,W11-3702,0,0.0547067,"es of online user activity: recent user messages, their friend network, and a multiview embedding of both of these views. 2 Stance Classification The popularity of sentiment classification is motivated in part by the utility of understanding the opinions expressed by a large population (Pang et al., 2008). Sentiment analysis of movie reviews (Pang et al., 2002) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and seek to measure opinion of a piece of text towards that issue. While stance classification can be expressed as a complex set of opinions and a"
W18-6124,D15-1166,0,0.0315634,"ion was used in the SemEval 2016 stance classification task (Mohammad et al., 2016). In stance classification, the system seeks to identify the position held by the author of the message. While most work in this area infers the author’s position based only on the given message, other information about the author may be available to aid in message analysis. Consider a user who frequently expresses liberal positions on a range of political topics. Even without observing any messages from the user about a specific liberal political candidate, we can reasonably infer that 185 as the query vector (Luong et al., 2015). The equation for determining attention on the ith position for a sentence of length n is: dates model weights according to an auxiliary objective function based on available user representations. This pre-training step initializes the hidden layer weights of the stance classification neural network, so that the final resulting model improves even when observing only a single message at classification time. Finally, while our focus is stance classification, this approach is applicable to a variety of document classification tasks in which author information can provide important insights in s"
W18-6124,P07-1056,1,0.31932,"both datasets, we compare the benefit of pretraining a neural stance classifier to predict user embeddings derived from different types of online user activity: recent user messages, their friend network, and a multiview embedding of both of these views. 2 Stance Classification The popularity of sentiment classification is motivated in part by the utility of understanding the opinions expressed by a large population (Pang et al., 2008). Sentiment analysis of movie reviews (Pang et al., 2002) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and"
W18-6124,S16-1003,0,0.0607809,"Missing"
W18-6124,C10-2100,0,0.035896,"n part by the utility of understanding the opinions expressed by a large population (Pang et al., 2008). Sentiment analysis of movie reviews (Pang et al., 2002) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and seek to measure opinion of a piece of text towards that issue. While stance classification can be expressed as a complex set of opinions and attitudes (Rosenthal et al., 2017), we confine ourselves to the task of binary stance classification, in which we seek to determine if a single message expresses support for or opposition to the given t"
W18-6124,W02-1011,0,0.0312628,"ammad et al., 2016) and 2) a new gun related Twitter data set that contains messages about gun control and gun rights. On both datasets, we compare the benefit of pretraining a neural stance classifier to predict user embeddings derived from different types of online user activity: recent user messages, their friend network, and a multiview embedding of both of these views. 2 Stance Classification The popularity of sentiment classification is motivated in part by the utility of understanding the opinions expressed by a large population (Pang et al., 2008). Sentiment analysis of movie reviews (Pang et al., 2002) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (So"
W18-6124,D14-1162,0,0.0892536,"ets of informal text. We rely on recurrent neural networks as a base classification model, as they have been effective classifiers for this type of data (Tang et al., 2015; Vosoughi et al., 2016; Limsopatham and Collier, 2016; Yang et al., 2017; Augenstein et al., 2016). Our base classification model is a gated recurrent unit (GRU) recurrent neural network classifier (Cho et al., 2014). The GRU consumes the input text as a sequence of tokens and produces a sequence of final hidden state activations. Input layer word embeddings are initialized with GloVe embeddings pre-trained on Twitter text (Pennington et al., 2014). The update equations for the gated recurrent unit at position i in a sentence are: RNN Classifier with User Embedding Pretraining We augment the base RNN classifier with an additional final (output) layer to predict an auxiliary user embedding for the tweet author. The objective function used for training this output layer depends on the type of user embedding (described below). A single epoch is made over the pre-training set before fitting to train. In this case, the RNN must predict information about the tweet author in the form of an ddimensional user embedding based on the input tweet t"
W18-6124,P11-2008,0,0.101079,"Missing"
W18-6124,S17-2088,0,0.0327215,"similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and seek to measure opinion of a piece of text towards that issue. While stance classification can be expressed as a complex set of opinions and attitudes (Rosenthal et al., 2017), we confine ourselves to the task of binary stance classification, in which we seek to determine if a single message expresses support for or opposition to the given target (or neither). This definition was used in the SemEval 2016 stance classification task (Mohammad et al., 2016). In stance classification, the system seeks to identify the position held by the author of the message. While most work in this area infers the author’s position based only on the given message, other information about the author may be available to aid in message analysis. Consider a user who frequently expresses"
W18-6124,P09-1026,0,0.044838,"2) can produce overall ratings for a film; analysis of product reviews allow for better recommendations (Blitzer et al., 2007); analysis of opinions on important issues can serve as a form of public opinion polling (Tumasjan et al., 2010; Bermingham and Smeaton, 2011). Although similar to sentiment classification, stance classification concerns the identification of an author’s position with respect to a given target (Anand et al., 2011; Murakami and Raymond, 2010). This is related to the task of targeted sentiment classification, in which both the sentiment and its target must be identified (Somasundaran and Wiebe, 2009). In the case of stance classification, we are given a fixed target, e.g. a political issue, and seek to measure opinion of a piece of text towards that issue. While stance classification can be expressed as a complex set of opinions and attitudes (Rosenthal et al., 2017), we confine ourselves to the task of binary stance classification, in which we seek to determine if a single message expresses support for or opposition to the given target (or neither). This definition was used in the SemEval 2016 stance classification task (Mohammad et al., 2016). In stance classification, the system seeks"
W18-6124,D15-1167,0,0.0359694,"final softmax output layer predicts the stance class labels based on a convex combination of hidden states. For this baseline model, the RNN is fit directly to the training set, without any pre-training, i.e. training maximizes the likelihood of class labels given the input tweet. We now consider an enhancement to our base model that incorporates user embeddings. Models The stance classification tasks we consider focus on tweets: short snippets of informal text. We rely on recurrent neural networks as a base classification model, as they have been effective classifiers for this type of data (Tang et al., 2015; Vosoughi et al., 2016; Limsopatham and Collier, 2016; Yang et al., 2017; Augenstein et al., 2016). Our base classification model is a gated recurrent unit (GRU) recurrent neural network classifier (Cho et al., 2014). The GRU consumes the input text as a sequence of tokens and produces a sequence of final hidden state activations. Input layer word embeddings are initialized with GloVe embeddings pre-trained on Twitter text (Pennington et al., 2014). The update equations for the gated recurrent unit at position i in a sentence are: RNN Classifier with User Embedding Pretraining We augment the"
W18-6124,P17-2067,0,0.0223742,"fewer than 1,000 training examples. Future work will explore more effective ways in which we can represent users, and utilize the information within the classification model. We are interested in neural models that are more robust to variation in the input examples such as convolutional neural networks. Despite having data for six stance classification targets, the datasets are still small and limited. We plan to evaluating our pre-training technique on the stance classification tasks presented in Hasan and Ng (2013) and related message-level classification tasks such as rumor identification (Wang, 2017). Augenstein et al. (2016) present a stance classification model that can be applied to unseen targets, conditioning stance prediction on an encoding of the target description. Although the experiments we run here only consider models trained independently for each target, user embedding pre-training is not restricted to this scenario. We will also investigate whether user embedding pretraining benefits models that are trained on many targets jointly and those designed for unseen targets. Table 4: Test accuracy of an SVM at predicting gun control stance based on guns-related keyphrase distribu"
W18-6124,S16-1074,0,0.176491,"e of user information, the classifier can better generalize to new tweets. This pre-training can be performed on a separate, unlabeled set of tweets and user embeddings, creating flexibility in which tasks can be improved by using this method. Additionally, we find that this training scheme is most beneficial in low-data settings, further reducing the resource requirement for training new classifiers. Although semi-supervised approaches to social media stance classification are not new, they have only been performed at the message-level – predicting held-out hashtags from a tweet for example (Zarrella and Marsh, 2016). Our approach leverages additional user information that may not be contained in a single message. We evaluate our approach on two stance clasMany social media classification tasks analyze the content of a message, but do not consider the context of the message. For example, in tweet stance classification – where a tweet is categorized according to a viewpoint it espouses – the expressed viewpoint depends on latent beliefs held by the user. In this paper we investigate whether incorporating knowledge about the author can improve tweet stance classification. Furthermore, since author informati"
W18-6127,W17-5202,0,0.0468184,"Missing"
W18-6127,W12-2108,0,0.034181,"rmed from Twitter posts or single sentences, since these are lengthy enough to force the model to learn long-range dependencies, yet short enough to train our RNN models quickly enough on a single GPU. We set a maximum length (divisible by 16) for each dataset, so as to make the longest sequences more manageable while maintaining most of the variability in lengths. We truncate sequences longer than the maximum length and pad shorter sequences with a unique token. Table 1 summarizes our datasets. Twitter LID: Twitter provides a multilingual dataset3 for language identification (LID) of tweets (Bergsma et al., 2012). We used the recall-focused dataset but were only able to download a subset of the original tweets and so limited our experiments to the 43 languages for which we could download at least 1000 tweets. We preprocessed this dataset in two ways: using the utf-8 encoding and using the raw byte strings. For the utf-8 data, we limit the vocabulary to the 1365 characters that appear at least 10 times. The average and median sequence lengths were 69 and 64 characters. We truncated utf-8 sequences at 128 characters, roughly the 95 percentile of lengths. For the raw bytes data, we use the entire 198 cha"
W18-6127,W17-4408,0,0.0423714,"Missing"
W18-6127,W16-6212,0,0.0534258,"Missing"
W18-6127,E17-2068,0,0.0316377,"tion to words, this emphasizes the value of explicit word-level features. Bag-of-words or “bag of character ngrams” models naturally model the presence of a specific word, whereas a character sequence model must learn to pick that word out from its surrounding characters. In our sentiment datasets, it may be that specific words are very indicative of the sequence labels, which could in part explain the good performance of the n-gram models. This suggests that models which combine word and character features together may be particularly well-suited to our domain of informal social media texts (Joulin et al., 2017; Luong and Manning, 2016). Results Table 2 shows our results for each model architecture on each dataset. For the RNN baseline, we include the GRU results, which outperformed the LSTM in every experiment. Even though we considered more hyperparameter settings for the RNN models than for any of the CNN architectures, and despite allowing for larger RNN models, each convolutional architecture significantly4 outperformed the recurrent model. This supports the argument that CNN models are more naturally suited than RNNs to handle the lengthy sequences found in character datasets. Our models do no"
W18-6127,D15-1167,0,0.0762576,"Missing"
W18-6127,D14-1181,0,0.0380473,"ed to prevent such long-distance gradients from vanishing (Hochreiter et al., 2001; Bengio et al., 1994) by allowing constant error flow through the network; yet empirical results find that LSTMs fail to learn long-range dependencies.1 Convolutional Neural Networks (CNNs) differ from RNNs in their internal structure, which may make them more promising for modeling long sequences. Whereas RNNs construct a chain of one hidden state for every input token, convolutional models can connect input tokens with paths sublinear in the input sequence’s length. CNNs have succeeded at text classification (Kim, 2014; Zhang et al., 2015) and language modeling (Kim et al., 2016). ByteNet, introduced by Kalchbrenner et al. (2016), used dilated convolutions to capture long-range dependencies in character-level machine translation and achieve fast training times. Despite these promising results, prior work has not highlighted specific tasks or domains in which CNNs are expected to outperform RNNs. We consider the task of classifying social media posts; such user-generated text data contains many unique words through misspellings, lexical variation, and slang. Because a word-level approach requires either an i"
W18-6127,S17-2102,0,0.0457696,"Missing"
W18-6127,N16-1153,0,0.0583389,"Missing"
W18-6127,P16-1100,0,0.0298842,"mphasizes the value of explicit word-level features. Bag-of-words or “bag of character ngrams” models naturally model the presence of a specific word, whereas a character sequence model must learn to pick that word out from its surrounding characters. In our sentiment datasets, it may be that specific words are very indicative of the sequence labels, which could in part explain the good performance of the n-gram models. This suggests that models which combine word and character features together may be particularly well-suited to our domain of informal social media texts (Joulin et al., 2017; Luong and Manning, 2016). Results Table 2 shows our results for each model architecture on each dataset. For the RNN baseline, we include the GRU results, which outperformed the LSTM in every experiment. Even though we considered more hyperparameter settings for the RNN models than for any of the CNN architectures, and despite allowing for larger RNN models, each convolutional architecture significantly4 outperformed the recurrent model. This supports the argument that CNN models are more naturally suited than RNNs to handle the lengthy sequences found in character datasets. Our models do not achieve state-of-the-art"
W18-6127,S17-2088,0,0.0394578,"Missing"
W18-6127,D13-1170,0,0.0225475,"r experiments, suggesting that CNNs are superior to RNNs at learning to classify character-level data. 1 Text Classification with Sequences Deep learning has transformed text classification tasks by providing models that can fully account for word order, whereas previous methods required simplifications such as treating documents as a “bag of words.” Recurrent neural networks (RNNs) are attractive for their ability to handle variable-length sequences and have contributed huge improvements to machine translation (Bahdanau et al., 2015; Cho et al., 2014) and semantic modeling (Tai et al., 2015; Socher et al., 2013), among many other areas. Despite this widespread success, RNNs often perform poorly on long sequences – common in document classification – in which the model must learn representations than span many timesteps. If two informative tokens are far apart in a document, a training gradient must maintain information about one such token while being backpropagated through the sequence of per-token learned representations. Formulations like Long 1 See Section 3.3 of Jozefowicz et al. (2016) and Section 4 of Sundermeyer et al. (2012) for two such results. 208 Proceedings of the 2018 EMNLP Workshop W-"
W18-6127,P15-1150,0,0.0499184,"e RNN models in our experiments, suggesting that CNNs are superior to RNNs at learning to classify character-level data. 1 Text Classification with Sequences Deep learning has transformed text classification tasks by providing models that can fully account for word order, whereas previous methods required simplifications such as treating documents as a “bag of words.” Recurrent neural networks (RNNs) are attractive for their ability to handle variable-length sequences and have contributed huge improvements to machine translation (Bahdanau et al., 2015; Cho et al., 2014) and semantic modeling (Tai et al., 2015; Socher et al., 2013), among many other areas. Despite this widespread success, RNNs often perform poorly on long sequences – common in document classification – in which the model must learn representations than span many timesteps. If two informative tokens are far apart in a document, a training gradient must maintain information about one such token while being backpropagated through the sequence of per-token learned representations. Formulations like Long 1 See Section 3.3 of Jozefowicz et al. (2016) and Section 4 of Sundermeyer et al. (2012) for two such results. 208 Proceedings of the"
W19-3013,W14-3207,1,0.739312,"w these illnesses manifest across demographic subpopulations. The analysis demonstrates that cohort-based studies can help control for sampling biases, contextualize outcomes, and provide deeper insights into the data. 1 Introduction The ability of social media analysis to support computational epidemiology and improve public health practices is well established (Culotta, 2010; Paul and Dredze, 2011; Salathe et al., 2012; Paul and Dredze, 2017). The field has seen particular success around the diagnosis, quantification and tracking of mental illnesses (Hao et al., 2013; Schwartz et al., 2014; Coppersmith et al., 2014a, 2015a,c; Amir et al., 2017). These methods have utilized social media (Coppersmith et al., 2014b; Kumar et al., 2015; De Choudhury et al., 2016), as well as other online data sources (Ayers et al., 2017, 2013, 2012; Arora et al., 2016), to obtain population level estimates and trends around mental health topics. 114 Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 114–120 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics race/ethnicity and location for each cohort candidate. Age Identifying age based on the co"
W19-3013,W15-1201,1,0.829247,"he outcomes. 2 Methodology We now briefly describe our approach for cohortbased studies over social media. A more detailed description of the proposed methodology will appear in a forthcoming publication. Most works on social media analysis estimate trends by aggregating document-level signals inferred from arbitrary (and biased) data samples selected to match a predefined outcome. While some recent work has begun incorporating demographic information to contextualize analyses (Mandel et al., 2012; Mitchell et al., 2013; Huang et al., 2017, 2019) and to improve representativeness of the data (Coppersmith et al., 2015b; Dos Reis and Culotta, 2015), these studies still select on specific outcomes. We depart from these works by constructing a demographically representative digital cohort of social media users prior to the analyses, and then conducting cohort-based studies over this preselected population. While a significant undertaking in most medical studies, the vast quantities of available social media data make assembling social media cohorts feasible. Such cohorts can be used to support longitudinal and cross-sectional studies, allowing experts to contextualize the outcomes, produce externally valid tr"
W19-3013,W15-1204,1,0.776854,"Missing"
W19-3013,W16-5614,1,0.836337,"apolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics race/ethnicity and location for each cohort candidate. Age Identifying age based on the content of a user can be challenging, and exact age often cannot be determined based on language use alone. Therefore, we use discrete categories that provide a more accurate estimate of age: Teenager (below 19), 20s, 30s, 40s, 50s (50 years or older). Gender The gender was inferred using Demographer, a supervised model that predicts the (binary) gender of Twitter users with features based on the name field on the user profile (Knowles et al., 2016). Race/Ethnicity The standard formulation of race and ethnicity is not well understood by the general public, so categorizing social media users along these two axes may not be reasonable. Therefore, we use a single measure of multicultural expression that includes five categories: White (W), Asian (A), Black (B), Hispanic (H), and Other. Location The location was inferred using Carmen, an open-source library for geolocating tweets that uses a series of rules to lookup location strings in a location knowledge-base (Dredze et al., 2013). We use the inferred location to select users that live in"
W19-3013,W12-2104,0,0.0273891,"alysis demonstrates how social media based cohort studies can help to control for sampling biases and contextualize the outcomes. 2 Methodology We now briefly describe our approach for cohortbased studies over social media. A more detailed description of the proposed methodology will appear in a forthcoming publication. Most works on social media analysis estimate trends by aggregating document-level signals inferred from arbitrary (and biased) data samples selected to match a predefined outcome. While some recent work has begun incorporating demographic information to contextualize analyses (Mandel et al., 2012; Mitchell et al., 2013; Huang et al., 2017, 2019) and to improve representativeness of the data (Coppersmith et al., 2015b; Dos Reis and Culotta, 2015), these studies still select on specific outcomes. We depart from these works by constructing a demographically representative digital cohort of social media users prior to the analyses, and then conducting cohort-based studies over this preselected population. While a significant undertaking in most medical studies, the vast quantities of available social media data make assembling social media cohorts feasible. Such cohorts can be used to sup"
W19-3013,W15-1206,0,0.0370841,"Missing"
W19-3013,W15-1205,0,0.0296235,"Missing"
W19-3013,W14-3214,0,0.0972631,"Missing"
