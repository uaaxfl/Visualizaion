2021.semdeep-1.3,Embeddings for the Lexicon: Modelling and Representation,2021,-1,-1,2,0,2108,christian chiarcos,Proceedings of the 6th Workshop on Semantic Deep Learning (SemDeep-6),0,None
2021.gwc-1.33,Towards the Addition of Pronunciation Information to Lexical Semantic Resources,2021,-1,-1,1,1,2109,thierry declerck,Proceedings of the 11th Global Wordnet Conference,0,"This paper describes ongoing work aiming at adding pronunciation information to lexical semantic resources, with a focus on open wordnets. Our goal is not only to add a new modality to those semantic networks, but also to mark heteronyms listed in them with the pronunciation information associated with their different meanings. This work could contribute in the longer term to the disambiguation of multi-modal resources, which are combining text and speech."
2020.mmw-1.7,Adding Pronunciation Information to Wordnets,2020,-1,-1,1,1,2109,thierry declerck,Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020),0,"We describe on-going work consisting in adding pronunciation information to wordnets, as such information can indicate specific senses of a word. Many wordnets associate with their senses only a lemma form and a part-of-speech tag. At the same time, we are aware that additional linguistic information can be useful for identifying a specific sense of a wordnet lemma when encountered in a corpus. While work already deals with the addition of grammatical number or grammatical gender information to wordnet lemmas,we are investigating the linking of wordnet lemmas to pronunciation information, adding thus a speech-related modality to wordnets"
2020.lrec-1.395,A Multilingual Evaluation Dataset for Monolingual Word Sense Alignment,2020,-1,-1,7,0,6137,sina ahmadi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Aligning senses across resources and languages is a challenging task with beneficial applications in the field of natural language processing and electronic lexicography. In this paper, we describe our efforts in manually aligning monolingual dictionaries. The alignment is carried out at sense-level for various resources in 15 languages. Moreover, senses are annotated with possible semantic relationships such as broadness, narrowness, relatedness, and equivalence. In comparison to previous datasets for this task, this dataset covers a wide range of languages and resources and focuses on the more challenging task of linking general-purpose language. We believe that our data will pave the way for further advances in alignment and evaluation of word senses by creating new solutions, particularly those notoriously requiring data such as neural networks. Our resources are publicly available at https://github.com/elexis-eu/MWSA."
2020.lrec-1.422,Language Data Sharing in {E}uropean Public Services {--} Overcoming Obstacles and Creating Sustainable Data Sharing Infrastructures,2020,-1,-1,5,0,17562,lilli smal,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Data is key in training modern language technologies. In this paper, we summarise the findings of the first pan-European study on obstacles to sharing language data across 29 EU Member States and CEF-affiliated countries carried out under the ELRC White Paper action on Sustainable Language Data Sharing to Support Language Equality in Multilingual Europe. Why Language Data Matters. We present the methodology of the study, the obstacles identified and report on recommendations on how to overcome those. The obstacles are classified into (1) lack of appreciation of the value of language data, (2) structural challenges, (3) disposition towards CAT tools and lack of digital skills, (4) inadequate language data management practices, (5) limited access to outsourced translations, and (6) legal concerns. Recommendations are grouped into addressing the European/national policy level, and the organisational/institutional level."
2020.lrec-1.695,Recent Developments for the Linguistic Linked Open Data Infrastructure,2020,-1,-1,1,1,2109,thierry declerck,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we describe the contributions made by the European H2020 project {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-use Multilingual Linked Language Data for Knowledge Services across Sectors{'}) to the further development of the Linguistic Linked Open Data (LLOD) infrastructure. Pr{\^e}t-{\`a}-LLOD aims to develop a new methodology for building data value chains applicable to a wide range of sectors and applications and based around language resources and language technologies that can be integrated by means of semantic technologies. We describe the methods implemented for increasing the number of language data sets in the LLOD. We also present the approach for ensuring interoperability and for porting LLOD data sets and services to other infrastructures, as well as the contribution of the projects to existing standards."
2020.iwltp-1.2,On the Linguistic Linked Open Data Infrastructure,2020,-1,-1,4,0,2108,christian chiarcos,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"In this paper we describe the current state of development of the Linguistic Linked Open Data (LLOD) infrastructure, an LOD(sub-)cloud of linguistic resources, which covers various linguistic data bases, lexicons, corpora, terminology and metadata repositories.We give in some details an overview of the contributions made by the European H2020 projects {``}Pr{\^e}t-{\`a}-LLOD{''} ({`}Ready-to-useMultilingual Linked Language Data for Knowledge Services across Sectors{'}) and {``}ELEXIS{''} ({`}European Lexicographic Infrastructure{'}) to the further development of the LLOD."
2020.globalex-1.1,Modelling Frequency and Attestations for {O}nto{L}ex-Lemon,2020,-1,-1,7,0,2108,christian chiarcos,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"The OntoLex vocabulary enjoys increasing popularity as a means of publishing lexical resources with RDF and as Linked Data. The recent publication of a new OntoLex module for lexicography, lexicog, reflects its increasing importance for digital lexicography. However, not all aspects of digital lexicography have been covered to the same extent. In particular, supplementary information drawn from corpora such as frequency information, links to attestations, and collocation data were considered to be beyond the scope of lexicog. Therefore, the OntoLex community has put forward the proposal for a novel module for frequency, attestation and corpus information (FrAC), that not only covers the requirements of digital lexicography, but also accommodates essential data structures for lexical information in natural language processing. This paper introduces the current state of the OntoLex-FrAC vocabulary, describes its structure, some selected use cases, elementary concepts and fundamental definitions, with a focus on frequency and attestations."
2020.globalex-1.5,Towards an Extension of the Linking of the Open {D}utch {W}ord{N}et with {D}utch Lexicographic Resources,2020,-1,-1,1,1,2109,thierry declerck,Proceedings of the 2020 Globalex Workshop on Linked Lexicography,0,"This extended abstract presents on-going work consisting in interlinking and merging the Open Dutch WordNet and generic lexicographic resources for Dutch, focusing for now on the Dutch and English versions of Wiktionary and using the Algemeen Nederlands Woordenboek as a quality checking instance. As the Open Dutch WordNet is already equipped with a relevant number of complex lexical units, we are aiming at expanding it and proposing a new representational framework for the encoding of the interlinked and integrated data. The longer term goal of the work is to investigate if and on how senses can be restricted to particular morphological variations of Dutch lexical entries, and how to represent this information in a Linguistic Linked Open Data compliant format."
W19-5104,Using {O}nto{L}ex-Lemon for Representing and Interlinking {G}erman Multiword Expressions in {O}de{N}et and {MMORPH},2019,0,0,1,1,2109,thierry declerck,Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019),0,"We describe work consisting in porting two large German lexical resources into the OntoLex-Lemon model in order to establish complementary interlinkings between them. One resource is OdeNet (Open German WordNet) and the other is a further development of the German version of the MMORPH morphological analyzer. We show how the Multiword Expressions (MWEs) contained in OdeNet can be morphologically specified by the use of the lexical representation and linking features of OntoLex-Lemon, which also support the formulation of restrictions in the usage of such expressions."
R19-1027,Porting Multilingual Morphological Resources to {O}nto{L}ex-Lemon,2019,0,0,1,1,2109,thierry declerck,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,We describe work consisting in porting various morphological resources to the OntoLex-Lemon model. A main objective of this work is to offer a uniform representation of different morphological data sets in order to be able to compare and interlink multilingual resources and to cross-check and interlink or merge the content of morphological resources of one and the same language. The results of our work will be published on the Linguistic Linked Open Data cloud.
2019.gwc-1.34,{O}nto{L}ex as a possible Bridge between {W}ord{N}ets and full lexical Descriptions,2019,-1,-1,1,1,2109,thierry declerck,Proceedings of the 10th Global Wordnet Conference,0,"In this paper we describe our current work on representing a recently created German lexical semantics resource in OntoLex-Lemon and in conformance with WordNet specifications. Besides presenting the representation effort, we show the utilization of OntoLex-Lemon to bridge from WordNet-like resources to full lexical descriptions and extend the coverage of WordNets to other types of lexical data, such as decomposition results, exemplified for German data, and inflectional phenomena, here outlined for English data."
L18-1034,Comparing Pretrained Multilingual Word Embeddings on an Ontology Alignment Task,2018,0,4,2,1,8250,dagmar gromann,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1094,An Integrated Formal Representation for Terminological and Lexical Data included in Classification Schemes,2018,0,0,1,1,2109,thierry declerck,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1213,{E}uropean Language Resource Coordination: Collecting Language Resources for Public Sector Multilingual Information Management,2018,0,2,6,0,17507,andrea losch,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
declerck-etal-2017-multilingual,Multilingual Ontologies for the Representation and Processing of Folktales,2017,0,0,1,1,2109,thierry declerck,Proceedings of the First Workshop on Language technology for Digital Humanities in Central and (South-)Eastern {E}urope,0,"We describe work done in the field of folkloristics and consisting in creating ontologies based on well-established studies proposed by {``}classical{''} folklorists. This work is supporting the availability of a huge amount of digital and structured knowledge on folktales to digital humanists. The ontological encoding of past and current motif-indexation and classification systems for folktales was in the first step limited to English language data. This led us to focus on making those newly generated formal knowledge sources available in a few more languages, like German, Russian and Bulgarian. We stress the importance of achieving this multilingual extension of our ontologies at a larger scale, in order for example to support the automated analysis and classification of such narratives in a large variety of languages, as those are getting more and more accessible on the Web."
gromann-declerck-2017-hashtag,Hashtag Processing for Enhanced Clustering of Tweets,2017,11,0,2,1,8250,dagmar gromann,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"Rich data provided by tweets have beenanalyzed, clustered, and explored in a variety of studies. Typically those studies focus on named entity recognition, entity linking, and entity disambiguation or clustering. Tweets and hashtags are generally analyzed on sentential or word level but not on a compositional level of concatenated words. We propose an approach for a closer analysis of compounds in hashtags, and in the long run also of other types of text sequences in tweets, in order to enhance the clustering of such text documents. Hashtags have been used before as primary topic indicators to cluster tweets, however, their segmentation and its effect on clustering results have not been investigated to the best of our knowledge. Our results with a standard dataset from the Text REtrieval Conference (TREC) show that segmented and harmonized hashtags positively impact effective clustering."
W16-2017,Towards a Formal Representation of Components of {G}erman Compounds,2016,9,0,1,1,2109,thierry declerck,"Proceedings of the 14th {SIGMORPHON} Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"This paper presents an approach for the formal representation of compo- nents in German compounds. We as- sume that such a formal representa- tion will support the segmentation and analysis of unseen compounds that feature components already seen in other compounds. An extensive lan- guage resource that explicitly codes components of compounds is Ger- maNet, a lexical semantic network for German. We summarize the Ger- maNet approach to the description of compounds, discussing some of its shortcomings. Our proposed exten- sion of this representation builds on the lemon lexicon model for ontolo- gies, established by the W3C Ontol- ogy Lexicon Community Group."
L16-1386,The Open Linguistics Working Group: Developing the Linguistic Linked Open Data Cloud,2016,16,11,5,0.0876232,1255,john mccrae,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The Open Linguistics Working Group (OWLG) brings together researchers from various fields of linguistics, natural language processing, and information technology to present and discuss principles, case studies, and best practices for representing, publishing and linking linguistic data collections. A major outcome of our work is the Linguistic Linked Open Data (LLOD) cloud, an LOD (sub-)cloud of linguistic resources, which covers various linguistic databases, lexicons, corpora, terminologies, and metadata repositories. We present and summarize five years of progress on the development of the cloud and of advancements in open data in linguistics, and we describe recent community activities. The paper aims to serve as a guideline to orient and involve researchers with the community and/or Linguistic Linked Open Data."
L16-1729,Monolingual Social Media Datasets for Detecting Contradiction and Entailment,2016,13,5,4,0.465703,17846,piroska lendvai,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Entailment recognition approaches are useful for application domains such as information extraction, question answering or summarisation, for which evidence from multiple sentences needs to be combined. We report on a new 3-way judgement Recognizing Textual Entailment (RTE) resource that originates in the Social Media domain, and explain our semi-automatic creation method for the special purpose of information verification, which draws on manually established rumourous claims reported during crisis events. From about 500 English tweets related to 70 unique claims we compile and evaluate 5.4k RTE pairs, while continue automatizing the workflow to generate similar-sized datasets in other languages."
2016.gwc-1.13,Towards a {W}ord{N}et based Classification of Actors in Folktales,2016,-1,-1,1,1,2109,thierry declerck,Proceedings of the 8th Global WordNet Conference (GWC),0,"In the context of a student software project we are investigating the use of WordNet for improving the automatic detection and classification of actors (or characters) mentioned in folktales. Our starting point is the book {``}Classification of International Folktales{''}, out of which we extract text segments that name the different actors involved in tales, taking advantage of patterns used by its author, Hans-Jo Ìrg Uther. We apply on those text segments functions that are implemented in the NLTK interface to WordNet in order to obtain lexical semantic information to enrich the original naming of characters proposed in the {``}Classification of International Folktales{''} and to support their translation in other languages."
W15-5504,Towards the Representation of Hashtags in Linguistic Linked Open Data Format,2015,-1,-1,1,1,2109,thierry declerck,Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data,0,None
R15-1015,Processing and Normalizing Hashtags,2015,11,6,1,1,2109,thierry declerck,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"We present ongoing work in linguistic processing of hashtags in Twitter text, with the goal of supplying normalized hashtag content to be used in more complex natural language processing (NLP) tasks. Hashtags represent collectively shared topic designators with considerable surface variation that can hamper semantic interpretation. Our normalization scripts allow for the lexical consolidation and segmentation of hashtags, potentially leading to improved semantic classification."
W14-5803,Harmonizing Lexical Data for their Linking to Knowledge Objects in the Linked Data Framework,2014,10,0,1,1,2109,thierry declerck,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"In this position paper we discuss some of the experiences we made in describing lexical data using representation formalisms that are compatible for the publication of such data in the Linked Data framework. While we see a huge potential in the emerging Linguistic Linked Open Data, also supporting the publication of less-resourced language data on the same platform as for mainstream languages, we are wondering if, parallel to the widening of linking language data to both other language data and encyclopaedic knowledge present in the Linked Data cloud, it would not be beneficial to give more focus more on harmonization and merging of RDF encoded lexical data, instead of establishing links between such resources in the Linked Data. xefx80xa0"
W14-5805,{S}enti{M}erge: Combining Sentiment Lexicons in a {B}ayesian Framework,2014,17,5,2,0,10836,guy emerson,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"Many approaches to sentiment analysis rely on a lexicon that labels words with a prior polarity. This is particularly true for languages other than English, where labelled training data is not easily available. Existing efforts to produce such lexicons exist, and to avoid duplicated effort, a principled way to combine multiple resources is required. In this paper, we introduce a Bayesian probabilistic model, which can simultaneously combine polarity scores from several data sources and estimate the quality of each source. We apply this algorithm to a set of four German sentiment lexicons, to produce the SentiMerge lexicon, which we make publically available. In a simple classification task, we show that this lexicon outperforms each of the underlying resources, as well as a majority vote model."
W14-0602,How to semantically relate dialectal Dictionaries in the Linked Data Framework,2014,5,4,1,1,2109,thierry declerck,"Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We describe on-going work towards publishing language resources included in dialectal dictionaries in the Linked Open Data (LOD) cloud, and so to support wider access to the diverse cultural data associated with such dictionary entries, like the various historical and geographical variations of the use of such words. Beyond this, our approach allows the cross-linking of entries of dialectal dictionaries on the basis of the semantic representation of their senses, and also to link the entries of the dialectal dictionaries to lexical senses available in the LOD framework. This paper focuses on the description of the steps leading to a SKOS-XL and lemon encoding of the entries of two Austrian dialectal dictionaries, and how this work supports their cross-linking and linking to other language data in the LOD."
krieger-declerck-2014-tmo,{TMO} {---} The Federated Ontology of the {T}rend{M}iner Project,2014,10,5,2,0,37133,hansulrich krieger,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes work carried out in the European project TrendMiner which partly deals with the extraction and representation of real time information from dynamic data streams. The focus of this paper lies on the construction of an integrated ontology, TMO, the TrendMiner Ontology, that has been assembled from several independent multilingual taxonomies and ontologies which are brought together by an interface specification, expressed in OWL. Within TrendMiner, TMO serves as a common language that helps to interlink data, delivered from both symbolic and statistical components of the TrendMiner system. Very often, the extracted data is supplied as quintuples, RDF triples that are extended by two further temporal arguments, expressing the temporal extent in which an atemporal statement is true. In this paper, we will also sneak a peek on the temporal entailment rules and queries that are built into the semantic repository hosting the data and which can be used to derive useful new information."
declerck-etal-2014-skos,A {SKOS}-based Schema for {TEI} encoded Dictionaries at {ICLTT},2014,7,0,1,1,2109,thierry declerck,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"At our institutes we are working with quite some dictionaries and lexical resources in the field of less-resourced language data, like dialects and historical languages. We are aiming at publishing those lexical data in the Linked Open Data framework in order to link them with available data sets for highly-resourced languages and elevating them thus to the same Âdigital dignityÂ the mainstream languages have gained. In this paper we concentrate on two TEI encoded variants of the Arabic language and propose a mapping of this TEI encoded data onto SKOS, showing how the lexical entries of the two dialectal dictionaries can be linked to other language resources available in the Linked Open Data cloud."
declerck-krieger-2014-harmonization,Harmonization of {G}erman Lexical Resources for Opinion Mining,2014,8,1,1,1,2109,thierry declerck,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present on-going work on the harmonization of existing German lexical resources in the field of opinion and sentiment mining. The input of our harmonization effort consisted in four distinct lexicons of German word forms, encoded either as lemmas or as full forms, marked up with polarity features, at distinct granularity levels. We describe how the lexical resources have been mapped onto each other, generating a unique list of entries, with unified Part-of-Speech information and basic polarity features. Future work will be dedicated to the comparison of the harmonized lexicon with German corpora annotated with polarity information. We are further aiming at both linking the harmonized German lexical resources with similar resources in other languages and publishing the resulting set of lexical data in the context of the Linguistic Linked Open Data cloud."
W13-5501,Linguistic Linked Open Data ({LLOD}). Introduction and Overview,2013,13,2,3,0.125201,2108,christian chiarcos,"Proceedings of the 2nd Workshop on Linked Data in Linguistics ({LDL}-2013): Representing and linking lexicons, terminologies and other language data",0,None
W13-5204,Porting Elements of the {A}ustrian Baroque Corpus onto the Linguistic Linked Open Data Format,2013,4,0,2,0,40546,ulrike czeitschner,"Proceedings of the Joint Workshop on {NLP}{\\&}{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction",0,"We describe work on porting linguistic and semantic annotation applied to the Austrian Baroque Corpus (ABaC:us) to a format supporting its publication in the Linked Open Data Framework. This work includes several aspects, like a derived lexicon of old forms used in the texts and their mapping to modern German lemmas, the description of morphosyntactic features and the building of domainspecific controlled vocabularies for covering the semantic aspects of this historical corpus. As a central and recurrent topic in the texts is death and dying, a first step in our work was geared towards the establishment of a deathrelated taxonomy. In order to provide for linguistic information to their textual content, labels of the taxonomy are pointing to linked data in the field of language resources."
W13-5210,Linguistically analyzed labels of knowledge objects: How can they support {OBIE}? Lessons learned from the Monnet and {T}rend{M}iner projects,2013,0,0,1,1,2109,thierry declerck,"Proceedings of the Joint Workshop on {NLP}{\\&}{LOD} and {SWAIE}: Semantic Web, Linked Open Data and Information Extraction",0,"We are investigating the use of natural language expressions included in Knowledge Organization Systems (KOS) for supporting Ontology-Based Information Extraction (OBIE), in a multiand cross-lingual context. Very often, Knowledge Organization Systems include so-called annotation properties, in the form of labels, comments, definitions, etc, which have the purpose of introducing human readable information in the formal description of the domain modelled in the KOS. An approach developed in the Monnet project, and continued in the TrendMiner project, consists in transforming the content of annotation properties into linguistically analysed data. Natural language processing of such language expressions, also called sometimes lexicalisation of Knowledge Organisation Systems, are thus transforming the unstructured content of annotation properties into linguistically structured data, which can be used in comparing language data included in a KOS with linguistically annotated texts. If some match of linguistic features between those two types of documents can be established, corresponding segments of the textual documents can be semantically annotated with the elements of the KOS the content of the annotation property is associated with. Evidently, this semantic annotation procedure can be of great help for OBIE, relating text segment to relevant parts of thesauri, taxonomy or ontologies. But looking in more details at the language data contained in annotation properties, we can see that this data very often has to be modified in order to be better used in the context of OBIE. Also there is a need for a formal representation of such linguistically annotated language data in order to ensure interoperability with semantic data available in the Linked Data Framework. The talk will expand on those issues."
W13-2712,Integration of the Thesaurus for the Social Sciences ({T}he{S}oz) in an Information Extraction System,2013,7,1,1,1,2109,thierry declerck,"Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We present current work dealing with the integration of a multilingual thesaurus for social sciences in a NLP framework for supporting Knowledge-Driven Information Extraction in the field of social sciences. We describe the various steps that lead to a running IE system: lexicalization of the labels of the thesaurus and semi-automatic generation of domain specific IE grammars, with their subsequent implementation in a finite state engine. Finally, we outline the actual field of application of the IE system: analysis of social media for recognition of relevant topics in the context of elections."
W12-1006,Ontology-Based Incremental Annotation of Characters in Folktales,2012,9,4,1,1,2109,thierry declerck,"Proceedings of the 6th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",0,"We present on-going work on the automated ontology-based detection and recognition of characters in folktales, restricting ourselves for the time being to the analysis of referential nominal phrases occurring in such texts. Focus of the presently reported work was to investigate the interaction between an ontology and linguistic analysis of indefinite and indefinite nominal phrase for both the incremental annotation of characters in folktales text, including some inference based co-reference resolution, and the incremental population of the ontology. This in depth study was done at this early stage using only a very small textual base, but the demonstrated feasibility and the promising results of our small-scale experiment are encouraging us to deploy the strategy on a larger text base, covering more linguistic phenomena in a multilingual fashion."
declerck-etal-2012-accessing,Accessing and standardizing {W}iktionary lexical entries for the translation of labels in Cultural Heritage taxonomies,2012,5,2,1,1,2109,thierry declerck,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe the usefulness of Wiktionary, the freely available web-based lexical resource, in providing multilingual extensions to catalogues that serve content-based indexing of folktales and related narratives. We develop conversion tools between Wiktionary and TEI, using ISO standards (LMF, MAF), to make such resources available to both the Digital Humanities community and the Language Resources community. The converted data can be queried via a web interface, while the tools of the workflow are to be released with an open source license. We report on the actual state and functionality of our tools and analyse some shortcomings of Wiktionary, as well as potential domains of application."
gavrilidou-etal-2012-meta,The {META}-{SHARE} Metadata Schema for the Description of Language Resources,2012,4,24,8,0,39961,maria gavrilidou,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents a metadata model for the description of language resources proposed in the framework of the META-SHARE infrastructure, aiming to cover both datasets and tools/technologies used for their processing. It places the model in the overall framework of metadata models, describes the basic principles and features of the model, elaborates on the distinction between minimal and maximal versions thereof, briefly presents the integrated environment supporting the LRs description and search and retrieval processes and concludes with work to be done in the future for the improvement of the model."
hayashi-etal-2010-laf,{LAF}/{G}r{AF}-grounded Representation of Dependency Structures,2010,9,6,2,0,18065,yoshihiko hayashi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper shows that a LAF/GrAF-based annotation schema can be used for the adequate representation of syntactic dependency structures possibly in many languages. We first argue that there are at least two types of textual units that can be annotated with dependency information: words/tokens and chunks/phrases. We especially focus on importance of the latter dependency unit: it is particularly useful for representing Japanese dependency structures, known as Kakari-Uke structure. Based on this consideration, we then discuss a sub-typing of GrAF to represent the corresponding dependency structures. We derive three node types, two edge types, and the associated constraints for properly representing both the token-based and the chunk-based dependency structures. We finally propose a wrapper program that, as a proof of concept, converts output data from different dependency parsers in proprietary XML formats to the GrAF-compliant XML representation. It partially proves the value of an international standard like LAF/GrAF in the Web service context: an existing dependency parser can be, in a sense, standardized, once wrapped by a data format conversion process."
lendvai-etal-2010-integration,Integration of Linguistic Markup into Semantic Models of Folk Narratives: The Fairy Tale Use Case,2010,3,12,2,0.465703,17846,piroska lendvai,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Propp's influential structural analysis of fairy tales created a powerful schema for representing storylines in terms of character functions, which is directly exploitable for computational semantic analysis, and procedural generation of stories of this genre. We tackle two resources that draw on the Proppian model - one formalizes it as a semantic markup scheme and the other as an ontology -, both lacking linguistic phenomena explicitly represented in them. The need for integrating linguistic information into structured semantic resources is motivated by the emergence of suitable standards that facilitate this, as well as the benefits such joint representation would create for transdisciplinary research across Digital Humanities, Computational Linguistics, and Artificial Intelligence."
declerck-lendvai-2010-towards,Towards a Standardized Linguistic Annotation of the Textual Content of Labels in Knowledge Representation Systems,2010,6,13,1,1,2109,thierry declerck,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"WWe propose applying standardized linguistic annotation to terms included in labels of knowledge representation schemes (taxonomies or ontologies), hypothesizing that this would help improving ontology-based semantic annotation of texts. We share the view that currently used methods for including lexical and terminological information in such hierarchical networks of concepts are not satisfactory, and thus put forward â as a preliminary step to our annotation goal â a model for modular representation of conceptual, terminological and linguistic information within knowledge representation systems. Our CTL model is based on two recent initiatives that describe the representation of terminologies and lexicons in ontologies: the Terminae method for building terminological and ontological models from text (Aussenac-Gilles et al., 2008), and the LexInfo metamodel for ontology lexica (Buitelaar et al., 2009). CTL goes beyond the mere fusion of the two models and introduces an additional level of representation for the linguistic objects, whereas those are no longer limited to lexical information but are covering the full range of linguistic phenomena, including constituency and dependency. We also show that the approach benefits linguistic and semantic analysis of external documents that are often to be linked to semantic resources for enrichment with concepts that are newly extracted or inferred."
federmann-declerck-2010-extraction,"Extraction, Merging, and Monitoring of Company Data from Heterogeneous Sources",2010,5,1,2,0,6017,christian federmann,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe the implementation of an enterprise monitoring system that builds on an ontology-based information extraction (OBIE) component applied to heterogeneous data sources. The OBIE component consists of several IE modules - each extracting on a regular temporal basis a specific fraction of company data from a given data source - and a merging tool, which is used to aggregate all the extracted information about a company. The full set of information about companies, which is to be extracted and merged by the OBIE component, is given in the schema of a domain ontology, which is guiding the information extraction process. The monitoring system, in case it detects changes in the extracted and merged information on a company with respect to the actual state of the knowledge base of the underlying ontology, ensures the update of the population of the ontology. As we are using an ontology extended with temporal information, the system is able to assign time intervals to any of the object instances. Additionally, detected changes can be communicated to end-users, who can validate and possibly correct the resulting updates in the knowledge base."
W09-3741,Concept and Relation Extraction in the Finance Domain,2009,5,4,2,0,19096,mihaela vela,Proceedings of the Eight International Conference on Computational Semantics,0,"In this paper, we describe the state of our work on the possible derivation of ontological structures from textual analysis. We propose an approach to semi-automatic generation of domain ontologies from scratch, on the basis of heuristic rules applied to the result of a multi-layered processing of textual documents."
broeder-etal-2008-foundation,Foundation of a Component-based Flexible Registry for Language Resources and Technology,2008,0,7,2,0.41697,18402,daan broeder,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Within the CLARIN e-science infrastructure project it is foreseen to develop a component-based registry for metadata for Language Resources and Language Technology. With this registry it is hoped to overcome the problems of the current available systems with respect to inflexible fixed schema, unsuitable terminology and interoperability problems. The registry will address interoperability needs by refering to a shared vocabulary registered in data category registries as they are suggested by ISO."
declerck-2008-framework,A Framework for Standardized Syntactic Annotation,2008,6,10,1,1,2109,thierry declerck,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This poster presents an ISO framework for the standardization of syntactic annotation (SynAF). The normative part SynAF is concerned with a metamodel for syntactic annotation that covers both dimensions of constituency and dependency, and propose thus a multi-layered annotation framework that allows the combined and interrelated annotation of language data along both lines of consideration. This standard is designed to be used in close conjuncion with the metamodel presented in the Linguistic Annotation Framework (LAF) and with ISO 12620, Terminology and other language resources - Data categories."
W06-2707,Annotating text using the Linguistic Description Scheme of {MPEG}-7: The {DIRECT}-{INFO} Scenario,2006,3,1,1,1,2109,thierry declerck,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"We describe the way we adapted a text analysis tool for annotating with the Linguistic Description Scheme of MPEG-7 text related to and extracted from multimedia content. Practically applied in the DIRECT-INFO EC R&D project we show how such linguistic annotation contributes to semantic annotation of multimodal analysis systems, demonstrating also the use of the XML schema of MPEG-7 for supporting cross-media semantic content annotation."
declerck-vela-2006-generic,Generic {NLP} Tools for Supporting Shallow Ontology Building,2006,7,2,1,1,2109,thierry declerck,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we present on-going investigations on how complex syntactic annotation, combined with linguistic semantics, can possibly help in supporting the semi-automatic building of (shallow) ontologies from text by proposing an automated extraction of (possibly underspecified) semantic relations from linguistically annotated text."
declerck-etal-2006-multilingual,Multilingual Lexical Semantic Resources for Ontology Translation,2006,0,20,1,1,2109,thierry declerck,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe the integration of some multilingual language resources in ontological descriptions, with the purpose of providing ontologies, which are normally using concept labels in just one (natural) language, with multilingual facility in their design and use in the context of Semantic Web applications, supporting both the semantic annotation of textual documents with multilingual ontology labels and ontology extraction from multilingual text sources."
declerck-2006-synaf,{S}yn{AF}: Towards a Standard for Syntactic Annotation,2006,2,22,1,1,2109,thierry declerck,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In the paper we present the actual state of development of an international standard for syntactic annotation, called SynAF. This standard is being prepared by the Technical Committee ISO/TC 37 (Terminology and Other Language Resources), Subcommittee SC 4 (Language Resource Management), in collaboration with the European eContent Project ÂLIRICSÂ (Linguistic Infrastructure for Interoperable Resources and Systems)."
broeder-etal-2004-large,A Large Metadata Domain of Language Resources,2004,0,7,2,0.625,18402,daan broeder,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The INTERA and ECHO projects were partly intended to create a critical mass of open and linked metadata descriptions of language resources, helping researchers to understand the benefits of an increased visibility of language resources in the Internet and motivating them to participate. The work was based on the new IMDI version 3.0.3 which is a result of experiences with the earlier versions and new requirements coming from the involved partners. While in INTERA major data centers in Europe are participating, the ECHO project focuses on resources that can be seen as part of cultural heritage. Currently, 27 institutions and projects are active with the goal of having a large browsable and searchable domain by the summer of 2004. Experience shows that the creation of high quality metadata is not trivial and asks for a considerable amount of effort and skills, since manual work alone is too time consumin (Less)"
declerck-etal-2004-towards,Towards a Language Infrastructure for the Semantic Web,2004,8,8,1,1,2109,thierry declerck,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In recent years, the Internet evolved from a global medium for information exchange (directed mainly towards human users) into a xe2x80x9dglobal, virtual work environmentxe2x80x9d (for both human users and machines). Building on the world-wide-web, developments such as grid technology, web services and the semantic web contributed to this transformation, the implications of which are now slowly but clearly being integrated into all areas of the new digital society (e-business, e-government, e-science, etc.) In this conctext the semantic web allows for increasingly intelligent and therefore autonomous processing. This development brings new challenges for Human Language Technology (HLT), which require not only some adaptation of processes within the state of the art processing chain of HLT, but also changes at the infrastructure level of HLT resources."
buitelaar-etal-2004-towards,Towards Ontology Engineering Based on Linguistic Analysis,2004,11,11,5,0,6276,paul buitelaar,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we describe OntoLT, a plug -in for the widely used Protege ontology development tool that supports the interactive extraction and/or extension of ontologies from text. The OntoLT approach aims at providing an environment for the integration of linguistic analysis in ontology development. OntoLT enables the definition of mapping rules with which concepts and attributes can be extracted automatically from linguistically annotated text collections. Mapping rules are defined by use of a constraint language. Constraints are implemented as XPATH expressions over the XML-based linguistic annotation. If all constraints are satisfied, the mapping rule activates one or more operators that describe in which way the ontology should be extended if a candidate is found."
lee-etal-2004-towards,Towards an International Standard on Feature Structure Representation,2004,22,18,5,0,18931,kiyong lee,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,This paper describes the preliminary results of a joint initiative of the TEI (Text Encoding Initiative) Consortium and the ISO Committee TC 37SC 4 (language Resource management) to provide a standard for the representation and interchange of feature structures.
E03-2014,"Event-Coreference across Multiple, Multi-lingual Sources in the Mumis Project",2003,5,4,4,0,5986,horacio saggion,Demonstrations,0,"We present our work on information extraction from multiple, multi-lingual sources for the Multimedia Indexing and Searching Environment (MUMIS), a project aiming at developing technology to produce formal annotations about essential events in multimedia programme material. The novelty of our approach consists on the use of a merging or cross-document coreference algorithm that aims at combining the output delivered by the information extraction systems."
broeder-etal-2002-lrep,{LREP}: A Language Repository Exchange Protocol,2002,3,0,3,0.625,18402,daan broeder,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,The recent increase in the number and complexity of the language resources available on the Internet is followed by a similar increase of available tools for linguistic analysis. Ideally the user does not need to be confronted with the question in how to match tools with resources. If resource repositories and tool repositories offer adequate metadata information and a suitable exchange protocol is developed this matching process could be performed (semi-) automatically.
capstick-etal-2002-collate,{COLLATE}: Competence Center in Speech and Language Technology,2002,9,4,4,0,53562,joanne capstick,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents the structure and activitities of the recently established Competence Center in Speech and Language Technology in Saarbrucken. The objectives of the Competence Center are to provide a comprehensive information service about speech and language technologies, including live demonstrations of the most important language technology (LT) systems, and to advance the state of the art in the evaluation of LT systems for real-world applications. The Competence Center comprises the following components: 1. the Virtual Information Center xe2x80x9cLanguage Technology Worldxe2x80x9d (www.lt-world.org), the world's most comprehensive information resource about speech and language technology, 2. the Demonstration Center in Saarbrucken, which offers interested parties the possibility to play and experiment with different speech and language technologies, or to attend guided demonstrations, 3. the Evaluation Center, which conducts evaluations of the overall usability of language technology systems and advances knowledge of relevant usability issues and evaluation methods. The work presented in this paper was carried out by the German Research Center for Artificial Intelligence in collaboration with Saarland University in the context of the project COLLATE (COmputational Linguistics and LAnguage TEchnology for Real Life Applications), funded by the German Federal Ministry of Education and Research (www.bmbf.de)."
W01-1502,{I}ntroduction: Extending {NLP} Tools Repositories for the Interaction with Language Data Resource Repositories,2001,8,3,1,1,2109,thierry declerck,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"This short paper presents some motivations behind the organization of the ACL/EACL01 Workshop on Sharing Tools and Resources for Research and Education, concentrating on the possible connection of Tools and Resources repositories. Taking some papers printed in this volume and the ACL Natural Language Software Registry as a basis, we outline some of the steps to be done on the side of NLP tool repositories in order to achieve this goal."
W01-1017,The Automatic Generation of Formal Annotations in a Multimedia Indexing and Searching Environment,2001,11,20,1,1,2109,thierry declerck,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"We describe in this paper the MU-MIS Project (Multimedia Indexing and Searching Environment), which is concerned with the development and integration of base technologies, demonstrated within a laboratory prototype, to support automated multimedia indexing and to facilitate search and retrieval from multimedia databases. We stress the role linguistically motivated annotations, coupled with domain-specific information, can play within this environment. The project will demonstrate that innovative technology components can operate on multilingual, multisource, and multimedia information and create a meaningful and queryable database."
declerck-etal-2000-new,The New Edition of the Natural Language Software Registry (an Initiative of {ACL} hosted at {DFKI}),2000,0,5,1,1,2109,thierry declerck,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper we present the new version (4th edition) of the Natural Language Software Registry (NLSR), an initiative of the Association for Computational Linguistics (ACL) hosted at DFKI in Saarbrucken. We give a brief overview of the history of this repository for Natural Language Processing (NLP) software, list some related works and go into the details of the design and the implementation of the"
P98-2195,Natural Language Access to Software Applications,1998,4,3,7,1,46559,paul schmidt,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper reports on the ESPRIT project MELISSA (Methods and Tools for Natural-Language Interfacing with Standard Software Applications)1. MELISSA aims at developing the technology and tools enabling end users to interface with computer applications, using natural-language (NL), and to obtain a precompetitive product validated in selected enduser applications. This paper gives an overview of the approach to solving (NL) interfacing problem and outlines some of the methods and software components developed in the project."
C98-2190,Natural Language Access to Software Applications,1998,4,3,7,1,46559,paul schmidt,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper reports on the ESPRIT project MELISSA (Methods and Tools for Natural-Language Interfacing with Standard Software Applications)1. MELISSA aims at developing the technology and tools enabling end users to interface with computer applications, using natural-language (NL), and to obtain a precompetitive product validated in selected enduser applications. This paper gives an overview of the approach to solving (NL) interfacing problem and outlines some of the methods and software components developed in the project."
W97-0216,Semantic Tagging and {NLP} Applications,1997,-1,-1,1,1,2109,thierry declerck,"Tagging Text with Lexical Semantics: Why, What, and How?",0,None
A97-1006,Natural Language Dialogue Service for Appointment Scheduling Agents,1997,16,37,2,0,5120,stephan busemann,Fifth Conference on Applied Natural Language Processing,0,"Appointment scheduling is a problem faced daily by many individuals and organizations. Cooperating agent systems have been developed to partially automate this task. In order to extend the circle of participants as far as possible we advocate the use of natural language transmitted by email. We describe COSMA, a fully implemented German language server for existing appointment scheduling agent systems. COSMA can cope with multiple dialogues in parallel, and accounts for differences in dialogue behaviour between human and machine agents. NL coverage of the sublanguage is achieved through both corpus-based grammar development and the use of message extraction techniques."
C96-2180,Efficient Integrated Tagging of Word Constructs,1996,0,1,3,0,53677,andrew bredenkamp,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"We describe a robust text-handling component, which can deal with free text in a wide range of formats and can successfully identify a wide range of phenomena, including chemical formulae, dates, numbers and proper nouns. The set of regular expressions used to capture numbers in written form (sechsundzwanzig) in German is given as an example. Proper noun candidates are identified by means of regular expressions, these being then rejected or accepted on the basis of run-time interaction with the user. This tagging component is integrated in a large-scale grammar development environment, and provides direct input to the grammatical analysis component of the system by means of lift rules which convert tagged text into partial linguistic structures."
C96-1048,Dealing with Cross-Sentential Anaphora Resolution in {ALEP},1996,4,1,1,1,2109,thierry declerck,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"The experiments described here have been done in connection with the LS-GRAM project, which is concerned with the development of large scale grammars and thus foreseen the coverage of real life texts. But in order to deal with such texts, it is also necessary to process linguistic units which are larger than sentences. The resolution of cross-sentential anaphora is one of the problems we have to deal with, when we switch towards the analysis of such larger linguistic units. In order to propose an analysis of the cross-sentential anaphora, one has to be able to refer back to an antecedent, which is to be found in a preceding sentence. This will be done on the basis of an information-passing framework. Using also the simple unification technique a resolution of the pronoun can then be tried out: parts of the content information of the pronoun are going to be compared (unified) with specific parts of the content information of the (possible) antecedent."
C96-1049,"Lean Formalisms, Linguistic Theory and Applications. Grammar Development in {ALEP}.",1996,1,10,4,0,46559,paul schmidt,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper describes results achieved in a project which addresses the issue of how the gap between unification-based grammars as a scientific concept and real world applications can be narrowed down1. Application-oriented grammar development has to take into account the following parameters: Efficiency: The project chose a so called 'lean' formalism, a term-encodable language providing efficient term unification, ALEP. Coverage: The project adopted a corpus-based approach. Completeness: All modules needed from text handling to semantics must be there. The paper reports on a text handling component, Two Level morphology, word structure, phrase structure, semantics and the interfaces between these components. Mainstream approach: The approach claims to be mainstream, very much indebted to HPSG, thus based on the currently most prominent and recent linguistic theory. The relation (and tension) between these parameters are described in this paper."
