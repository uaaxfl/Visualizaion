2021.findings-emnlp.299,Automatic Bilingual Markup Transfer,2021,-1,-1,2,0,7149,thomas zenkel,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"We describe the task of bilingual markup transfer, which involves placing markup tags from a source sentence into a fixed target translation. This task arises in practice when a human translator generates the target translation without markup, and then the system infers the placement of markup tags. This task contrasts from previous work in which markup transfer is performed jointly with machine translation. We propose two novel metrics and evaluate several approaches based on unsupervised word alignments as well as a supervised neural sequence-to-sequence model. Our best approach achieves an average accuracy of 94.7{\%} across six language pairs, indicating its potential usefulness for real-world localization tasks."
2020.acl-main.146,End-to-End Neural Word Alignment Outperforms {GIZA}++,2020,24,0,2,0,7149,thomas zenkel,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models. Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection. While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems. Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training. We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets. Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality."
N19-1206,Measuring Immediate Adaptation Performance for Neural Machine Translation,2019,0,0,2,0,26174,patrick simianer,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve system performance for interactive machine translation. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU- or TER-derived metrics that do not explicitly take adaptation speed into account. We find that these metrics often do not capture immediate adaptation effects, such as zero-shot and one-shot learning of domain-specific lexical items. To this end, we propose new metrics that directly evaluate immediate adaptation performance for machine translation. We use these metrics to choose the most suitable adaptation method from a range of different adaptation techniques for neural machine translation systems."
D18-1104,Compact Personalized Models for Neural Machine Translation,2018,0,4,1,1,7150,joern wuebker,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture{--}combining a state-of-the-art self-attentive model with compact domain adaptation{--}provides high quality personalized machine translation that is both space and time efficient.
W16-2212,A Comparative Study on Vocabulary Reduction for Phrase Table Smoothing,2016,10,1,3,0,20831,yunsu kim,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,None
P16-1007,Models and Inference for Prefix-Constrained Machine Translation,2016,21,10,1,1,7150,joern wuebker,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
W15-3018,The {RWTH} {A}achen {G}erman-{E}nglish Machine Translation System for {WMT} 2015,2015,23,1,3,0.757576,30412,janthorsten peter,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the statistical machine translation system developed at RWTH Aachen University for the German!English translation task of the EMNLP 2015 Tenth Workshop on Statistical Machine Translation (WMT 2015). A phrase-based machine translation system was applied and augmented with hierarchical phrase reordering and word class language models. Further, we ran discriminative maximum expected BLEU training for our system. In addition, we utilized multiple feed-forward neural network language and translation models and a recurrent neural network language model for reranking."
W15-3033,Extended Translation Models in Phrase-based Decoding,2015,37,3,2,1,18814,andreas guta,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We propose a novel extended translation model (ETM) to counteract some problems in phrase-based translation: The lack of translation context when using singleword phrases and uncaptured dependencies beyond phrase boundaries. The ETM operates on word-level and augments the IBM models by an additional bilingual word pair and a reordering operation. Its implementation in a phrase-based decoder introduces translation and reordering dependencies for single-word phrases and dependencies across phrase boundaries. More, the model incorporates an explicit treatment of multiple and empty alignments. Its integration outperforms competitive systems that include lexical and phrase translation models as well as hierarchical reordering models on 4 language pairs significantly by 0.7% BLEU on average. Although simpler and using fewer dependencies, the ETM proves to be on par with 7-gram operation sequence models (Durrani et al., 2013b)."
N15-1175,A Comparison of Update Strategies for Large-Scale Maximum Expected {BLEU} Training,2015,43,3,1,1,7150,joern wuebker,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This work presents a flexible and efficient discriminative training approach for statistical machine translation. We propose to use the RPROP algorithm for optimizing a maximum expected BLEU objective and experimentally compare it to several other updating schemes. It proves to be more efficient and effective than the previously proposed growth transformation technique and also yields better results than stochastic gradient descent and AdaGrad. We also report strong empirical results on two large scale tasks, namely BOLT Chinese!English and WMT German!English, where our final systems outperform results reported by Setiawan and Zhou (2013) and on matrix.statmt.org. On the WMT task, discriminative training is performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature."
D15-1123,Hierarchical Incremental Adaptation for Statistical Machine Translation,2015,34,10,1,1,7150,joern wuebker,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present an incremental adaptation approach for statistical machine translation that maintains a flexible hierarchical domain structure within a single consistent model. Both weights and rules are updated incrementallyonastreamofpost-edits. Our multi-level domain hierarchy allows the system to adapt simultaneously towards local context at dierent levels of granularity, including genres and individual documents. Our experiments show consistent improvements in translation quality from all components of our approach."
D15-1165,A Comparison between Count and Neural Network Models Based on Joint Translation and Reordering Sequences,2015,36,10,4,1,18814,andreas guta,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We propose a conversion of bilingual sentence pairs and the corresponding word alignments into novel linear sequences. These are joint translation and reordering (JTR) uniquely defined sequences, combining interdepending lexical and alignment dependencies on the word level into a single framework. They are constructed in a simple manner while capturing multiple alignments and empty words. JTR sequences can be used to train a variety of models. We investigate the performances of ngram models with modified Kneser-Ney smoothing, feed-forward and recurrent neural network architectures when estimated on JTR sequences, and compare them to the operation sequence model (Durrani et al., 2013b). Evaluations on the IWSLT German!English, WMT German!English and BOLT Chinese!English tasks show that JTR models improve state-of-the-art phrasebased systems by up to 2.2 BLEU."
W14-3310,{EU-BRIDGE} {MT}: Combined Machine Translation,2014,59,18,3,0.810811,3519,markus freitag,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two European language pairs, Germanxe2x86x92English and Englishxe2x86x92German. Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems."
W14-3317,The {RWTH} {A}achen {G}erman-{E}nglish Machine Translation System for {WMT} 2014,2014,27,3,2,0.572443,27027,stephan peitz,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the German!English translation task of the ACL 2014 Eighth Workshop on Statistical Machine Translation (WMT 2014). Both hierarchical and phrase-based SMT systems are applied employing hierarchical phrase reordering and word class language models. For the phrase-based system, we run discriminative phrase training. In addition, we describe our preprocessing pipeline for German!English."
D14-1003,Translation Modeling with Bidirectional Recurrent Neural Networks,2014,42,96,3,0,40094,martin sundermeyer,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"This work presents two different translation models using recurrent neural networks. The first one is a word-based approach using word alignments. Second, we present phrase-based translation models that are more consistent with phrasebased decoding. Moreover, we introduce bidirectional recurrent neural models to the problem of machine translation, allowing us to use the full source sentence in our models, which is also of theoretical interest. We demonstrate that our translation models are capable of improving strong baselines already including recurrent neural language models on three tasks: IWSLT 2013 German!English, BOLT Arabic!English and Chinese!English. We obtain gains up to 1.6% BLEU and 1.7% TER by rescoring 1000-best lists."
2014.iwslt-evaluation.7,Combined spoken language translation,2014,55,6,2,0.810811,3519,markus freitag,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE is a European research project which is aimed at developing innovative speech translation technology. One of the collaborative efforts within EU-BRIDGE is to produce joint submissions of up to four different partners to the evaluation campaign at the 2014 International Workshop on Spoken Language Translation (IWSLT). We submitted combined translations to the GermanâEnglish spoken language translation (SLT) track as well as to the GermanâEnglish, EnglishâGerman and EnglishâFrench machine translation (MT) tracks. In this paper, we present the techniques which were applied by the different individual translation systems of RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show the combination approach developed at RWTH Aachen University which combined the individual systems. The consensus translations yield empirical gains of up to 2.3 points in BLEU and 1.2 points in TER compared to the best individual system."
2014.iwslt-evaluation.22,The {RWTH} {A}achen machine translation systems for {IWSLT} 2014,2014,-1,-1,1,1,7150,joern wuebker,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2014. We participated in both the MT and SLT tracks for the EnglishâFrench and GermanâEnglish language pairs and applied the identical training pipeline and models on both language pairs. Our state-of-the-art phrase-based baseline systems are augmented with maximum expected BLEU training for phrasal, lexical and reordering models. Further, we apply rescoring with novel recurrent neural language and translation models. The same systems are used for the SLT track, where we additionally perform punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. We are able to improve RWTH{'}s 2013 evaluation systems by 1.7-1.8{\%} BLEU absolute."
2014.amta-researchers.15,Comparison of data selection techniques for the translation of video lectures,2014,30,1,1,1,7150,joern wuebker,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"For the task of online translation of scientific video lectures, using huge models is not possible. In order to get smaller and efficient models, we perform data selection. In this paper, we perform a qualitative and quantitative comparison of several data selection techniques, based on cross-entropy and infrequent n-gram criteria. In terms of BLEU, a combination of translation and language model cross-entropy achieves the most stable results. As another important criterion for measuring translation quality in our application, we identify the number of out-of-vocabulary words. Here, infrequent n-gram recovery shows superior performance. Finally, we combine the two selection techniques in order to benefit from both their strengths."
W13-2224,The {RWTH} {A}achen Machine Translation System for {WMT} 2013,2013,-1,-1,5,0.673077,27027,stephan peitz,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,None
W13-2238,Length-Incremental Phrase Training for {SMT},2013,27,2,1,1,7150,joern wuebker,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present an iterative technique to generate phrase tables for SMT, which is based on force-aligning the training data with a modified translation decoder. Different from previous work, we completely avoid the use of a word alignment or phrase extraction heuristics, moving towards a more principled phrase generation and probability estimation. During training, we allow the decoder to generate new phrases on-the-fly and increment the maximum phrase length in each iteration. Experiments are carried out on the IWSLT 2011 Arabic-English task, where we are able to reach moderate improvements on a state-of-the-art baseline with our training method. The resulting phrase table shows only a small overlap with the heuristically extracted one, which demonstrates the restrictiveness of limiting phrase selection by a word alignment or heuristics. By interpolating the heuristic and the trained phrase table, we can improve over the baseline by 0.5% BLEU and 0.5% TER."
W13-2258,A Phrase Orientation Model for Hierarchical Machine Translation,2013,31,17,2,0.234381,5061,matthias huck,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST Chinese!English translation task. We achieve a significant improvement of 1.2 %BLEU over a typical hierarchical baseline setup and an improvement of 0.7 %BLEU over a syntax-augmented hierarchical setup. On a French!German translation task, we obtain a gain of up to 0.4 %BLEU."
D13-1138,Improving Statistical Machine Translation with Word Class Models,2013,17,30,1,1,7150,joern wuebker,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Automatically clustering words from a monolingual or bilingual training corpus into classes is a widely used technique in statistical natural language processing. We present a very simple and easy to implement method for using these word classes to improve translation quality. It can be applied across different machine translation paradigms and with arbitrary types of models. We show its efficacy on a small German!English and a larger French!German translation task with both standard phrase-based and hierarchical phrase-based translation systems for a common set of models. Our results show that with word class models, the baseline can be improved by up to 1.4% BLEU and 1.0% TER on the French!German task and 0.3% BLEU and 1.1% TER on the German!English task."
2013.iwslt-evaluation.10,The {RWTH} {A}achen machine translation systems for {IWSLT} 2013,2013,-1,-1,1,1,7150,joern wuebker,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This work describes the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign International Workshop on Spoken Language Translation (IWSLT) 2013. We participated in the EnglishâFrench, EnglishâGerman, ArabicâEnglish, ChineseâEnglish and SlovenianâEnglish MT tracks and the EnglishâFrench and EnglishâGerman SLT tracks. We apply phrase-based and hierarchical SMT decoders, which are augmented by state-of-the-art extensions. The novel techniques we experimentally evaluate include discriminative phrase training, a continuous space language model, a hierarchical reordering model, a word class language model, domain adaptation via data selection and system combination of standard and reverse order models. By application of these methods we can show considerable improvements over the respective baseline systems."
2013.iwslt-evaluation.16,{EU}-{BRIDGE} {MT}: text translation of talks in the {EU}-{BRIDGE} project,2013,52,8,3,0.810811,3519,markus freitag,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes one of the collaborative efforts within EUBRIDGE to further advance the state of the art in machine translation between two European language pairs, EnglishâFrench and GermanâEnglish. Four research institutions involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the machine translation track of the evaluation campaign at the 2013 International Workshop on Spoken Language Translation (IWSLT). We present the methods and techniques to achieve high translation quality for text translation of talks which are applied at RWTH Aachen University, the University of Edinburgh, Karlsruhe Institute of Technology, and Fondazione Bruno Kessler. We then show how we have been able to considerably boost translation performance (as measured in terms of the metrics BLEU and TER) by means of system combination. The joint setups yield empirical gains of up to 1.4 points in BLEU and 2.8 points in TER on the IWSLT test sets compared to the best single systems."
W12-3157,Phrase Model Training for Statistical Machine Translation with Word Lattices of Preprocessing Alternatives,2012,32,3,1,1,7150,joern wuebker,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"In statistical machine translation, word lattices are used to represent the ambiguities in the preprocessing of the source sentence, such as word segmentation for Chinese or morphological analysis for German. Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters, or by applying indicator features. We introduce a novel lattice design, which explicitly distinguishes between different preprocessing alternatives for the source sentence. It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model. We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously. On the news-commentary portion of the Germanxe2x86x92English WMT 2011 task we can show moderate improvements of up to 0.6% Bleu over a state-of-the-art baseline system."
W12-3158,Leave-One-Out Phrase Model Training for Large-Scale Deployment,2012,15,5,1,1,7150,joern wuebker,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"Training the phrase table by force-aligning (FA) the training data with the reference translation has been shown to improve the phrasal translation quality while significantly reducing the phrase table size on medium sized tasks. We apply this procedure to several large-scale tasks, with the primary goal of reducing model sizes without sacrificing translation quality. To deal with the noise in the automatically crawled parallel training data, we introduce on-demand word deletions, insertions, and backoffs to achieve over 99% successful alignment rate. We also add heuristics to avoid any increase in OOV rates. We are able to reduce already heavily pruned baseline phrase tables by more than 50% with little to no degradation in quality and occasionally slight improvement, without any increase in OOVs. We further introduce two global scaling factors for re-estimation of the phrase table via posterior phrase alignment probabilities and a modified absolute discounting method that can be applied to fractional counts."
P12-2006,Fast and Scalable Decoding with Language Model Look-Ahead for Phrase-based Statistical Machine Translation,2012,13,4,1,1,7150,joern wuebker,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this work we present two extensions to the well-known dynamic programming beam search in phrase-based statistical machine translation (SMT), aiming at increased efficiency of decoding by minimizing the number of language model computations and hypothesis expansions. Our results show that language model based pre-sorting yields a small improvement in translation quality and a speedup by a factor of 2. Two look-ahead methods are shown to further increase translation speed by a factor of 2 without changing the search space and a factor of 4 with the side-effect of some additional search errors. We compare our approach with Moses and observe the same performance, but a substantially better trade-off between translation quality and speed. At a speed of roughly 70 words per second, Moses reaches 17.2% Bleu, whereas our approach yields 20.0% with identical models."
C12-3061,{J}ane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation,2012,36,34,1,1,7150,joern wuebker,Proceedings of {COLING} 2012: Demonstration Papers,0,"We present Jane 2, an open source toolkit supporting both the phrase-based and the hierarchical phrase-based paradigm for statistical machine translation. It is implemented in C and provides efficient decoding algorithms and data structures. This work focuses on the description of its phrase-based functionality. In addition to the standard pipeline, including phrase extraction and parameter optimization, Jane 2 contains several state-of-the-art extensions and tools. Forced alignment phrase training can considerably reduce rule table size while learning the translation scores in a more principled manner. Word class language models can be used to integrate longer context with a reduced vocabulary size. Rule table interpolation is applicable for different tasks, e.g. domain adaptation. The decoder distinguishes between lexical and coverage pruning and applies reordering constraints for efficiency."
C12-2091,Forced Derivations for Hierarchical Machine Translation,2012,23,4,3,0.673077,27027,stephan peitz,Proceedings of {COLING} 2012: Posters,0,"We present an efficient framework to estimate the rule probabilities for a hierarchical phrasebased statistical machine translation system from parallel data. In previous work, this was done with bilingual parsing. We use a more efficient approach splitting the bilingual parsing into two stages, which allows us to train a hierarchical translation model on larger tasks. Furthermore, we apply leave-one-out to counteract over-fitting and use the expected count from the inside-outside algorithm to prune the rule set. On the WMT12 Europarl Germanxe2x86x92English and Frenchxe2x86x92English tasks, we improve translation quality by up to 1.0 BLEU and 0.9 TER while simultaneously reducing the rule set to 5% of the original size."
2012.iwslt-evaluation.7,The {RWTH} {A}achen speech recognition and machine translation system for {IWSLT} 2012,2012,36,2,6,0.673077,27027,stephan peitz,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper, the automatic speech recognition (ASR) and statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2012 are presented. We participated in the ASR (English), MT (English-French, Arabic-English, Chinese-English, German-English) and SLT (English-French) tracks. For the MT track both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated in the MT and SLT tracks, including domain adaptation via data selection, translation model interpolation, phrase training for hierarchical and phrase-based systems, additional reordering model, word class language model, various Arabic and Chinese segmentation methods, postprocessing of speech recognition output with an SMT system, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems."
W11-2142,Joint {WMT} Submission of the {QUAERO} Project,2011,25,1,3,0.710383,3519,markus freitag,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach."
W11-2149,The {RWTH} {A}achen Machine Translation System for {WMT} 2011,2011,13,3,2,0.234381,5061,matthias huck,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We describe our system for the news commentary translation task of WMT 2011. The submitted run for the French-English direction is a combination of two MOSES-based systems developed at LIG and LIA laboratories. We report experiments to improve over the standard phrase-based model using statistical post-edition, information retrieval methods to subsample out-of-domain parallel corpora and Rover to combine n-best list of hypotheses output by different systems."
2011.iwslt-papers.5,Combining translation and language model scoring for domain-specific data filtering,2011,17,25,2,0.869565,2928,saab mansour,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"The increasing popularity of statistical machine translation (SMT) systems is introducing new domains of translation that need to be tackled. As many resources are already available, domain adaptation methods can be applied to utilize these recourses in the most beneficial way for the new domain. We explore adaptation via filtering, using the crossentropy scores to discard irrelevant sentences. We focus on filtering for two important components of an SMT system, namely the language model (LM) and the translation model (TM). Previous work has already applied LM cross-entropy based scoring for filtering. We argue that LM cross-entropy might be appropriate for LM filtering, but not as much for TM filtering. We develop a novel filtering approach based on a combined TM and LM cross-entropy scores. We experiment with two large-scale translation tasks, the Arabic-to-English and English-to-French IWSLT 2011 TED Talks MT tasks. For LM filtering, we achieve strong perplexity improvements which carry over to the translation quality with improvements up to +0.4{\%} BLEU. For TM filtering, the combined method achieves small but consistent improvements over the standalone methods. As a side effect of adaptation via filtering, the fully fledged SMT system vocabulary size and phrase table size are reduced by a factor of at least 2 while up to +0.6{\%} BLEU improvement is observed."
2011.iwslt-evaluation.14,The {RWTH} {A}achen machine translation system for {IWSLT} 2011,2011,-1,-1,1,1,7150,joern wuebker,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper the statistical machine translation (SMT) systems of RWTH Aachen University developed for the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2011 is presented. We participated in the MT (English-French, Arabic-English, ChineseEnglish) and SLT (English-French) tracks. Both hierarchical and phrase-based SMT decoders are applied. A number of different techniques are evaluated, including domain adaptation via monolingual and bilingual data selection, phrase training, different lexical smoothing methods, additional reordering models for the hierarchical system, various Arabic and Chinese segmentation methods, punctuation prediction for speech recognition output, and system combination. By application of these methods we can show considerable improvements over the respective baseline systems."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,14,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
W10-1711,The {RWTH} {A}achen Machine Translation System for {WMT} 2010,2010,61,32,2,0,37774,carmen heger,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems."
P10-1049,Training Phrase Translation Models with Leaving-One-Out,2010,20,78,1,1,7150,joern wuebker,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Several attempts have been made to learn phrase translation probabilities for phrase-based statistical machine translation that go beyond pure counting of phrases in word-aligned training data. Most approaches report problems with over-fitting. We describe a novel leaving-one-out approach to prevent over-fitting that allows us to train phrase models that show improved translation performance on the WMT08 Europarl German-English task. In contrast to most previous work where phrase models were trained separately from other models used in translation, we include all components such as single word lexica and reordering models in training. Using this consistent training of phrase models we are able to achieve improvements of up to 1.4 points in BLEU. As a side effect, the phrase table size is reduced by more than 80%."
2010.iwslt-papers.11,A combination of hierarchical systems with forced alignments from phrase-based systems,2010,14,3,2,0,37774,carmen heger,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Currently most state-of-the-art statistical machine translation systems present a mismatch between training and generation conditions. Word alignments are computed using the well known IBM models for single-word based translation. Afterwards phrases are extracted using extraction heuristics, unrelated to the stochastic models applied for finding the word alignment. In the last years, several research groups have tried to overcome this mismatch, but only with limited success. Recently, the technique of forced alignments has shown to improve translation quality for a phrase-based system, applying a more statistically sound approach to phrase extraction. In this work we investigate the first steps to combine forced alignment with a hierarchical model. Experimental results on IWSLT and WMT data show improvements in translation quality of up to 0.7{\%} BLEU and 1.0{\%} TER."
2010.iwslt-evaluation.22,The {RWTH} {A}achen machine translation system for {IWSLT} 2010,2010,61,32,4,0,2928,saab mansour,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"In this paper we describe the statistical machine translation system of the RWTH Aachen University developed for the translation task of the IWSLT 2010. This year, we participated in the BTEC translation task for the Arabic to English language direction. We experimented with two state-of-theart decoders: phrase-based and hierarchical-based decoders. Extensions to the decoders included phrase training (as opposed to heuristic phrase extraction) for the phrase-based decoder, and soft syntactic features for the hierarchical decoder. Additionally, we experimented with various rule-based and statistical-based segmenters for Arabic. Due to the different decoders and the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each system. The next step would be to exploit these variations and achieve better results by combining the systems. We try different strategies for system combination and report significant improvements over the best single system."
