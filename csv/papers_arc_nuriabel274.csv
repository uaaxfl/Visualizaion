2020.lrec-1.407,The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope,2020,4,1,23,0.137101,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions."
L18-1406,Can Domain Adaptation be Handled as Analogies?,2018,0,0,1,1,17513,nuria bel,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Comunicacio presentada a: 11th International Conference on Language Resources and Evaluation, celebrada a Miyazaki, Japo, del 7 al 12 de maig del 2018."
W17-1807,Annotation of negation in the {IULA} {S}panish Clinical Record Corpus,2017,0,5,3,1,26502,montserrat marimon,Proceedings of the Workshop Computational Semantics Beyond Events and Roles,0,"This paper presents the IULA Spanish Clinical Record Corpus, a corpus of 3,194 sentences extracted from anonymized clinical records and manually annotated with negation markers and their scope. The corpus was conceived as a resource to support clinical text-mining systems, but it is also a useful resource for other Natural Language Processing systems handling clinical texts: automatic encoding of clinical records, diagnosis support, term extraction, among others, as well as for the study of clinical texts. The corpus is publicly available with a CC-BY-SA 3.0 license."
W16-2339,{C}obalt{F}: A Fluent Metric for {MT} Evaluation,2016,13,3,2,1,2508,marina fomicheva,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"Comunicacio presentada a la First Conference on Machine Translation (WMT), que es va dur a terme durant el 54th Annual Meeting of the Association for Computational Linguistics, els dies 7 a 12 d'agost de 2016 a Berlin, Alemanya."
S16-1002,{S}em{E}val-2016 Task 5: Aspect Based Sentiment Analysis,2016,13,194,16,0,18398,maria pontiki,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the SemEval 2016 shared task on Aspect Based Sentiment Analysis (ABSA), a continuation of the respective tasks of 2014 and 2015. In its third year, the task provided 19 training and 20 testing datasets for 8 languages and 7 domains, as well as a common evaluation procedure. From these datasets, 25 were for sentence-level and 14 for text-level ABSA; the latter was introduced for the first time as a subtask in SemEval. The task attracted 245 submissions from 29 teams."
L16-1140,Leveraging {RDF} Graphs for Crossing Multiple Bilingual Dictionaries,2016,3,3,3,1,8512,marta villegas,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The experiments presented here exploit the properties of the Apertium RDF Graph, principally cycle density and nodes{'} degree, to automatically generate new translation relations between words, and therefore to enrich existing bilingual dictionaries with new entries. Currently, the Apertium RDF Graph includes data from 22 Apertium bilingual dictionaries and constitutes a large unified array of linked lexical entries and translations that are available and accessible on the Web (http://linguistic.linkeddata.es/apertium/). In particular, its graph structure allows for interesting exploitation opportunities, some of which are addressed in this paper. Two {`}massive{'} experiments are reported: in the first one, the original EN-ES translation set was removed from the Apertium RDF Graph and a new EN-ES version was generated. The results were compared against the previously removed EN-ES data and against the Concise Oxford Spanish Dictionary. In the second experiment, a new non-existent EN-FR translation set was generated. In this case the results were compared against a converted wiktionary English-French file. The results we got are really good and perform well for the extreme case of correlated polysemy. This lead us to address the possibility to use cycles and nodes degree to identify potential oddities in the source data. If cycle density proves efficient when considering potential targets, we can assume that in dense graphs nodes with low degree may indicate potential errors."
L16-1353,Towards producing bilingual lexica from monolingual corpora,2016,15,1,2,0,35091,jingyi han,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Bilingual lexica are the basis for many cross-lingual natural language processing tasks. Recent works have shown success in learning bilingual dictionary by taking advantages of comparable corpora and a diverse set of signals derived from monolingual corpora. In the present work, we describe an approach to automatically learn bilingual lexica by training a supervised classifier using word embedding-based vectors of only a few hundred translation equivalent word pairs. The word embedding representations of translation pairs were obtained from source and target monolingual corpora, which are not necessarily related. Our classifier is able to predict whether a new word pair is under a translation relation or not. We tested it on two quite distinct language pairs Chinese-Spanish and English-Spanish. The classifiers achieved more than 0.90 precision and recall for both language pairs in different evaluation scenarios. These results show a high potential for this method to be used in bilingual lexica production for language pairs with reduced amount of parallel or comparable corpora, in particular for phrase table expansion in Statistical Machine Translation systems."
L16-1437,Using Contextual Information for Machine Translation Evaluation,2016,27,0,2,1,2508,marina fomicheva,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Automatic evaluation of Machine Translation (MT) is typically approached by measuring similarity between the candidate MT and a human reference translation. An important limitation of existing evaluation systems is that they are unable to distinguish candidate-reference differences that arise due to acceptable linguistic variation from the differences induced by MT errors. In this paper we present a new metric, UPF-Cobalt, that addresses this issue by taking into consideration the syntactic contexts of candidate and reference words. The metric applies a penalty when the words are similar but the contexts in which they occur are not equivalent. In this way, Machine Translations (MTs) that are different from the human translation but still essentially correct are distinguished from those that share high number of words with the reference but alter the meaning of the sentence due to translation errors. The results show that the method proposed is indeed beneficial for automatic MT evaluation. We report experiments based on two different evaluation tasks with various types of manual quality assessment. The metric significantly outperforms state-of-the-art evaluation systems in varying evaluation settings."
L16-1724,Assessing the Potential of Metaphoricity of verbs using corpus data,2016,0,0,2,1,21574,marco tredici,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The paper investigates the relation between metaphoricity and distributional characteristics of verbs, introducing POM, a corpus-derived index that can be used to define the upper bound of metaphoricity of any expression in which a given verb occurs. The work moves from the observation that while some verbs can be used to create highly metaphoric expressions, others can not. We conjecture that this fact is related to the number of contexts in which a verb occurs and to the frequency of each context. This intuition is modelled by introducing a method in which each context of a verb in a corpus is assigned a vector representation, and a clustering algorithm is employed to identify similar contexts. Eventually, the Standard Deviation of the relative frequency values of the clusters is computed and taken as the POM of the target verb. We tested POM in two experimental settings obtaining values of accuracy of 84{\%} and 92{\%}. Since we are convinced, along with (Shutoff, 2015), that metaphor detection systems should be concerned only with the identification of highly metaphoric expressions, we believe that POM could be profitably employed by these systems to a priori exclude expressions that, due to the verb they include, can only have low degrees of metaphoricity"
W15-3046,{UPF}-Cobalt Submission to {WMT}15 Metrics Task,2015,22,3,2,1,2508,marina fomicheva,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"An important limitation of automatic evaluation metrics is that, when comparing Machine Translation (MT) to a human reference, they are often unable to discriminate between acceptable variation and the differences that are indicative of MT errors. In this paper we present UPF-Cobalt evaluation system that addresses this issue by penalizing the differences in the syntactic contexts of aligned candidate and reference words. We evaluate our metric using the data from WMT workshops of the recent years and show that it performs competitively both at segment and at system levels."
W15-1510,A Word-Embedding-based Sense Index for Regular Polysemy Representation,2015,21,1,2,1,21574,marco tredici,Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,0,"We present a method for the detection and representation of polysemous nouns, a phenomenon that has received little attention in NLP. The method is based on the exploitation of the semantic information preserved in Word Embeddings. We first prove that polysemous nouns instantiating a particular sense alternation form a separate class when clustering nouns in a lexicon. Such a class, however, does not include those polysemes in which a sense is strongly predominant. We address this problem and present a sense index that, for a given pair of lexical classes, defines the degree of membership of a noun to each class: polysemy is hence implicitly represented as an intermediate value on the continuum between two classes. We finally show that by exploiting the information provided by the sense index it is possible to accurately detect polysemous nouns in the dataset."
S15-1021,Reading Between the Lines: Overcoming Data Sparsity for Accurate Classification of Lexical Relationships,2015,35,16,4,0,37316,silvia neccsulescu,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"The lexical semantic relationships between word pairs are key features for many NLP tasks. Most approaches for automatically classifying related word pairs are hindered by data sparsity because of their need to observe two words co-occurring in order to detect the lexical relation holding between them. Even when mining very large corpora, not every related word pair co-occurs. Using novel representations based on graphs and word embeddings, we present two systems that are able to predict relations between words, even when these are never found in the same sentence in a given corpus. In two experiments, we demonstrate superior performance of both approaches over the state of the art, achieving significant gains in recall."
arias-etal-2014-boosting,Boosting the creation of a treebank,2014,14,3,2,0,39481,blanca arias,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present the results of an ongoing experiment of bootstrapping a Treebank for Catalan by using a Dependency Parser trained with Spanish sentences. In order to save time and cost, our approach was to profit from the typological similarities between Catalan and Spanish to create a first Catalan data set quickly by automatically: (i) annotating with a de-lexicalized Spanish parser, (ii) manually correcting the parses, and (iii) using the Catalan corrected sentences to train a Catalan parser. The results showed that the number of parsed sentences required to train a Catalan parser is about 1000 that were achieved in 4 months, with 2 annotators."
marimon-etal-2014-iula,The {IULA} {S}panish {LSP} Treebank,2014,15,1,2,1,26502,montserrat marimon,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents the IULA Spanish LSP Treebank, a dependency treebank of over 41,000 sentences of different domains (Law, Economy, Computing Science, Environment, and Medicine), developed in the framework of the European project METANET4U. Dependency annotations in the treebank were automatically derived from manually selected parses produced by an HPSG-grammar by a deterministic conversion algorithm that used the identifiers of grammar rules to identify the heads, the dependents, and some dependency types that were directly transferred onto the dependency structure (e.g., subject, specifier, and modifier), and the identifiers of the lexical entries to identify the argument-related dependency functions (e.g. direct object, indirect object, and oblique complement). The treebank is accessible with a browser that provides concordance-based search functions and delivers the results in two formats: (i) a column-based format, in the style of CoNLL-2006 shared task, and (ii) a dependency graph, where dependency relations are noted by an oriented arrow which goes from the dependent node to the head node. The IULA Spanish LSP Treebank is the first technical corpus of Spanish annotated at surface syntactic level following the dependency grammar theory. The treebank has been made publicly and freely available from the META-SHARE platform with a Creative Commons CC-by licence."
rehm-etal-2014-strategic,"The Strategic Impact of {META}-{NET} on the Regional, National and International Level",2014,47,2,4,0.137101,60,georg rehm,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This article provides an overview of the dissemination work carried out in META-NET from 2010 until early 2014; we describe its impact on the regional, national and international level, mainly with regard to politics and the situation of funding for LT topics. This paper documents the initiativeÂs work throughout Europe in order to boost progress and innovation in our field."
de-smedt-etal-2014-clara,{CLARA}: A New Generation of Researchers in Common Language Resources and Their Applications,2014,68,0,7,0,17515,koenraad smedt,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"CLARA (Common Language Resources and Their Applications) is a Marie Curie Initial Training Network which ran from 2009 until 2014 with the aim of providing researcher training in crucial areas related to language resources and infrastructure. The scope of the project was broad and included infrastructure design, lexical semantic modeling, domain modeling, multimedia and multimodal communication, applications, and parsing technologies and grammar models. An international consortium of 9 partners and 12 associate partners employed researchers in 19 new positions and organized a training program consisting of 10 thematic courses and summer/winter schools. The project has resulted in new theoretical insights as well as new resources and tools. Most importantly, the project has trained a new generation of researchers who can perform advanced research and development in language resources and technologies."
romeo-etal-2014-cascade,A cascade approach for complex-type classification,2014,7,2,3,1,39664,lauren romeo,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The work detailed in this paper describes a 2-step cascade approach for the classification of complex-type nominals. We describe an experiment that demonstrates how a cascade approach performs when the task consists in distinguishing nominals from a given complex-type from any other noun in the language. Overall, our classifier successfully identifies very specific and not highly frequent lexical items such as complex-types with high accuracy, and distinguishes them from those instances that are not complex types by using lexico-syntactic patterns indicative of the semantic classes corresponding to each of the individual sense components of the complex type. Although there is still room for improvement with regard to the coverage of the classifiers developed, the cascade approach increases the precision of classification of the complex-type nouns that are covered in the experiment presented."
romeo-etal-2014-choosing,Choosing which to use? A study of distributional models for nominal lexical semantic classification,2014,24,3,3,1,39664,lauren romeo,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper empirically evaluates the performances of different state-of-the-art distributional models in a nominal lexical semantic classification task. We consider models that exploit various types of distributional features, which thereby provide different representations of nominal behavior in context. The experiments presented in this work demonstrate the advantages and disadvantages of each model considered. This analysis also considers a combined strategy that we found to be capable of leveraging the bottlenecks of each model, especially when large robust data is not available."
villegas-etal-2014-metadata,Metadata as Linked Open Data: mapping disparate {XML} metadata registries into one {RDF}/{OWL} registry.,2014,8,2,3,1,8512,marta villegas,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The proliferation of different metadata schemas and models pose serious problems of interoperability. Maintaining isolated repositories with overlapping data is costly in terms of time and effort. In this paper, we describe how we have achieved a Linked Open Data version of metadata descriptions coming from heterogeneous sources, originally encoded in XML. The resulting model is much simpler than the original XSD schema and avoids problems typical of XML syntax, such as semantic ambiguity and order constraint. Moreover, the open world assumption of RDF/OWL allows to naturally integrate objects from different schemas and to add further extensions, facilitating merging of different models as well as linking to external data. Apart from the advantages in terms of interoperability and maintainability, the merged repository enables end-users to query multiple sources using a unified schema and is able to present them with implicit knowledge derived from the linked data. The approach we present here is easily scalable to any number of sources and schemas."
poch-etal-2014-ranking,Ranking Job Offers for Candidates: learning hidden knowledge from Big Data,2014,9,9,2,1,39867,marc poch,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents a system for suggesting a ranked list of appropriate vacancy descriptions to job seekers in a job board web site. In particular our work has explored the use of supervised classifiers with the objective of learning implicit relations which cannot be found with similarity or pattern based search methods that rely only on explicit information. Skills, names of professions and degrees, among other examples, are expressed in different languages, showing high variation and the use of ad-hoc resources to trace the relations is very costly. This implicit information is unveiled when a candidate applies for a job and therefore it is information that can be used for learning a model to predict new cases. The results of our experiments, which combine different clustering, classification and ranking methods, show the validity of the approach."
necsulescu-etal-2014-combining,Combining dependency information and generalization in a pattern-based approach to the classification of lexical-semantic relation instances,2014,26,0,3,0,37316,silvia neccsulescu,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This work addresses the classification of word pairs as instances of lexical-semantic relations. The classification is approached by leveraging patterns of co-occurrence contexts from corpus data. The significance of using dependency information, of augmenting the set of dependency paths provided to the system, and of generalizing patterns using part-of-speech information for the classification of lexical-semantic relation instances is analyzed. Results show that dependency information is decisive to achieve better results both in precision and recall, while generalizing features based on dependency information by replacing lexical forms with their part-of-speech increases the coverage of classification systems. Our experiments also make apparent that approaches based on the context where word pairs co-occur are upper-bound-limited by the times these appear in the same sentence. Therefore strategies to use information across sentence boundaries are necessary."
J14-3001,{S}quibs: Automatic Selection of {HPSG}-Parsed Sentences for Treebank Construction,2014,21,7,2,1,26502,montserrat marimon,Computational Linguistics,0,"This article presents an ensemble parse approach to detecting and selecting high-quality linguistic analyses output by a hand-crafted HPSG grammar of Spanish implemented in the LKB system. The approach uses full agreement (i.e., exact syntactic match) along with a MaxEnt parse selection model and a statistical dependency parser trained on the same data. The ultimate goal is to develop a hybrid corpus annotation methodology that combines fully automatic annotation and manual parse selection, in order to make the annotation task more efficient while maintaining high accuracy and the high degree of consistency necessary for any foreseen uses of a treebank."
C14-1049,Using unmarked contexts in nominal lexical semantic classification,2014,17,0,3,1,39664,lauren romeo,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"The work presented here addresses the use of unmarked contexts in pattern-based nominal lexical semantic classification. We define unmarked contexts to be the counterposition of the class-indicatory, or marked, contexts. Its aim is to evaluate how unmarked contexts can be used to improve the accuracy and reliability of lexical semantic classifiers. Results demonstrate that the combined use of both types of distributional information (marked and unmarked) is crucial to improve classification. This result was replicated using two different corpora, demonstrating the robustness of the method proposed."
W13-5404,Towards the automatic classification of complex-type nominals,2013,17,2,3,1,39664,lauren romeo,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,"Comunicacio presentada a: 6th International Conference on Generative Approaches to the Lexicon, celebrada a Pisa, Italia, del 24 al 25 de setembre del 2013."
W13-5411,Class-based Word Sense Induction for dot-type nominals,2013,18,1,3,1,39664,lauren romeo,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,"Comunicacio presentada a: 6th International Conference on Generative Approaches to the Lexicon, celebrada a Pisa, Italia, del 24 al 25 de setembre del 2013."
P13-2127,Annotation of regular polysemy and underspecification,2013,17,11,3,1,20634,hector alonso,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Comunicacio presentada a: 51st Annual Meeting of the Association for Computational Linguistics, celebrat a Sofia, Bulgaria, del 4 al 9 d'agost de 2013."
2013.mtsummit-european.14,"{M}oses{C}ore: {M}oses Open Source Evaluation and Support Co-ordination for {O}ut{R}each and Exploitation {PANACEA}: Platform for Automatic, Normalised Annotation and Cost-Effective Acquisition of Language Resources for Human Language Technologies",2013,-1,-1,1,1,17513,nuria bel,Proceedings of Machine Translation Summit XIV: European projects,0,None
2013.mtsummit-european.15,"{PANACEA}: Platform for Automatic, Normalised Annotation and Cost-Effective Acquisition of Language Resources for Human Language Technologies",2013,-1,-1,1,1,17513,nuria bel,Proceedings of Machine Translation Summit XIV: European projects,0,None
W12-0907,Webservices for {B}ayesian Learning,2012,8,0,2,1,18764,muntsa padro,Proceedings of the Workshop on Computational Models of Language Acquisition and Loss,0,In this demonstration we present our web services to perform Bayesian learning for classification tasks.
vazquez-bel-2012-classification,A Classification of Adjectives for Polarity Lexicons Enhancement,2012,11,5,2,0,39600,silvia vazquez,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Subjective language detection is one of the most important challenges in Sentiment Analysis. Because of the weight and frequency in opinionated texts, adjectives are considered a key piece in the opinion extraction process. These subjective units are more and more frequently collected in polarity lexicons in which they appear annotated with their prior polarity. However, at the moment, any polarity lexicon takes into account prior polarity variations across domains. This paper proves that a majority of adjectives change their prior polarity value depending on the domain. We propose a distinction between domain dependent and domain independent adjectives. Moreover, our analysis led us to propose a further classification related to subjectivity degree: constant, mixed and highly subjective adjectives. Following this classification, polarity values will be a better support for Sentiment Analysis."
alonso-etal-2012-voting,A voting scheme to detect semantic underspecification,2012,29,2,2,1,20634,hector alonso,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The following work describes a voting system to automatically classify the sense selection of the complex types Location/Organization and Container/Content, which depend on regular polysemy, as described by the Generative Lexicon (Pustejovsky, 1995) . This kind of sense alternations very often presents semantic underspecificacion between its two possible selected senses. This kind of underspecification is not traditionally contemplated in word sense disambiguation systems, as disambiguation systems are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009) The data are characterized by the morphosyntactic and lexical enviroment of the headwords and provided as input for a classifier. The baseline decision tree classifier is compared against an eight-member voting scheme obtained from variants of the training data generated by modifications on the class representation and from two different classification algorithms, namely decision trees and k-nearest neighbors. The voting system improves the accuracy for the non-underspecified senses, but the underspecified sense remains difficult to identify"
morell-etal-2012-iula2standoff,{I}ula2{S}tandoff: a tool for creating standoff documents for the {IULACT},2012,4,3,3,0,39601,carlos morell,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Due to the increase in the number and depth of analyses required over the text, like entity recognition, POS tagging, syntactic analysis, etc. the annotation in-line has become unpractical. In Natural Language Processing (NLP) some emphasis has been placed in finding an annotation method to solve this problem. A possibility is the standoff annotation. With this annotation style it is possible to add new levels of annotation without disturbing exiting ones, with minimal knock on effects. This annotation will increase the possibility of adding more linguistic information as well as more possibilities for sharing textual resources. In this paper we present a tool developed in the framework of the European Metanet4u (Enhancing the European Linguistic Infrastructure, GA 270893) for creating a multi-layered XML annotation scheme, based on the GrAF proposal for standoff annotations."
bel-etal-2012-automatic,Automatic lexical semantic classification of nouns,2012,28,6,1,1,17513,nuria bel,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The work we present here addresses cue-based noun classification in English and Spanish. Its main objective is to automatically acquire lexical semantic information by classifying nouns into previously known noun lexical classes. This is achieved by using particular aspects of linguistic contexts as cues that identify a specific lexical class. Here we concentrate on the task of identifying such cues and the theoretical background that allows for an assessment of the complexity of the task. The results show that, despite of the a-priori complexity of the task, cue-based classification is a useful tool in the automatic acquisition of lexical semantic classes."
marimon-etal-2012-iula,The {IULA} Treebank,2012,20,18,3,1,26502,montserrat marimon,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes on-going work for the construction of a new treebank for Spanish, The IULA Treebank. This new resource will contain about 60,000 richly annotated sentences as an extension of the already existing IULA Technical Corpus which is only PoS tagged. In this paper we have focused on describing the work done for defining the annotation process and the treebank design principles. We report on how the used framework, the DELPH-IN processing framework, has been crucial in the design principles and in the bootstrapping strategy followed, especially in what refers to the use of stochastic modules for reducing parsing overgeneration. We also report on the different evaluation experiments carried out to guarantee the quality of the already available results."
poch-etal-2012-towards,Towards a User-Friendly Platform for Building Language Resources based on Web Services,2012,16,4,5,1,39867,marc poch,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the platform developed in the PANACEA project, a distributed factory that automates the stages involved in the acquisition, production, updating and maintenance of Language Resources required by Machine Translation and other Language Technologies. We adopt a set of tools that have been successfully used in the Bioinformatics field, they are adapted to the needs of our field and used to deploy web services, which can be combined to build more complex processing chains (workflows). This paper describes the platform and its different components (web services, registry, workflows, social network and interoperability). We demonstrate the scalability of the platform by carrying out a set of massive data experiments. Finally, a validation of the platform across a set of required criteria proves its usability for different types of users (non-technical users and providers)."
villegas-etal-2012-using,Using Language Resources in Humanities research,2012,7,0,2,1,8512,marta villegas,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper we present two real cases, in the fields of discourse analysis of newspapers and communication research which demonstrate the impact of Language Resources (LR) and NLP in the humanities. We describe our collaboration with (i) the Feminario research group from the UAB which has been investigating androcentric practices in Spanish general press since the 80s and whose research suggests that Spanish general press has undergone a dehumanization process that excludes women and men and (ii) the ÂMunicipals'11 onlineÂ project which investigates the Spanish local election campaign in the blogosphere. We will see how NLP tools and LRs make possible the so called Âe-Humanities research' as they provide Humanities with tools to perform intensive and automatic text analyses. Language technologies have evolved a lot and are mature enough to provide useful tools to researchers dealing with large amount of textual data. The language resources that have been developed within the field of NLP have proven to be useful for other disciplines that are unaware of their existence and nevertheless would greatly benefit from them as they provide (i) exhaustiveness -to guarantee that data coverage is wide and representative enough- and (ii) reliable and significant results -to guarantee that the reported results are statistically significant."
soria-etal-2012-flarenet,The {FL}a{R}e{N}et Strategic Language Resource Agenda,2012,1,6,2,0,30233,claudia soria,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The FLaReNet Strategic Agenda highlights the most pressing needs for the sector of Language Resources and Technologies and presents a set of recommendations for its development and progress in Europe, as issued from a three-year consultation of the FLaReNet European project. The FLaReNet recommendations are organised around nine dimensions: a) documentation b) interoperability c) availability, sharing and distribution d) coverage, quality and adequacy e) sustainability f) recognition g) development h) infrastructure and i) international cooperation. As such, they cover a broad range of topics and activities, spanning over production and use of language resources, licensing, maintenance and preservation issues, infrastructures for language resources, resource identification and sharing, evaluation and validation, interoperability and policy issues. The intended recipients belong to a large set of players and stakeholders in Language Resources and Technology, ranging from individuals to research and education institutions, to policy-makers, funding agencies, SMEs and large companies, service and media providers. The main goal of these recommendations is to serve as an instrument to support stakeholders in planning for and addressing the urgencies of the Language Resources and Technologies of the future."
E12-2001,Language Resources Factory: case study on the acquisition of Translation Memories,2012,9,1,3,1,39867,marc poch,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper demonstrates a novel distributed architecture to facilitate the acquisition of Language Resources. We build a factory that automates the stages involved in the acquisition, production, updating and maintenance of these resources. The factory is designed as a platform where functionalities are deployed as web services, which can be combined in complex acquisition chains using workflows. We show a case study, which acquires a Translation Memory for a given pair of languages and a domain using web services for crawling, sentence alignment and conversion to TMX."
C12-2100,Using Qualia Information to Identify Lexical Semantic Classes in an Unsupervised Clustering Task,2012,21,0,3,1,39664,lauren romeo,Proceedings of {COLING} 2012: Posters,0,"Acquiring lexical information is a complex problem, typically approached by relying on a number of contexts to contribute information for classificati on. One of the first issues to address in this doma in is the determination of such contexts. The work pre sented here proposes the use of automatically obtained FORMAL role descriptors as features used to draw nouns fr om the same lexical semantic class together in an unsupervised clustering task. We have dealt with three lexical semantic classes (HUMAN , LOCATION and EVENT ) in English. The results obtained show that it is possible to discriminate between elements from different lexica l semantic classes using only FORMAL role information, hence validating our initial hypothesi s. Also, iterating our method accurately accounts for fine-grained distinctions within lexical classe s, namely distinctions involving ambiguous expressions. Moreover, a filtering and bootstrappin g strategy employed in extracting FORMAL role descriptors proved to minimize effects of sparse da ta and noise in our task."
C12-2124,Automatic Extraction of Polar Adjectives for the Creation of Polarity Lexicons,2012,15,2,3,0,39600,silvia vazquez,Proceedings of {COLING} 2012: Posters,0,"Automatic creation of polarity lexicons is a crucial issue to be solved in order to reduce time andn efforts in the first steps of Sentiment Analysis. In this paper we present a methodology based onn linguistic cues that allows us to automatically discover, extract and label subjective adjectivesn that should be collected in a domain-based polarity lexicon. For this purpose, we designed an bootstrapping algorithm that, from a small set of seed polar adjectives, is capable to iterativelyn identify, extract and annotate positive and negative adjectives. Additionally, the methodn automatically creates lists of highly subjective elements that change their prior polarity evenn within the same domain. The algorithm proposed reached a precision of 97.5% for positiven adjectives and 71.4% for negative ones in the semantic orientation identification task."
W11-4604,Identification of sense selection in regular polysemy using shallow features,2011,17,2,2,1,20634,hector alonso,Proceedings of the 18th Nordic Conference of Computational Linguistics ({NODALIDA} 2011),0,"The following work describes a method to automatically classify the sense selection of the complex type Location/Organization xe2x80x93which depends on regular polysemyxe2x80x93 using shallow features, as well as a way to increase the volume of sense-selection gold standards by using monosemous data as filler. The classifier results show that grammatical features are the most relevant cues for the identification of sense selection in this instance of regular polysemy."
W11-3302,A Method Towards the Fully Automatic Merging of Lexical Resources,2011,15,5,1,1,17513,nuria bel,"Proceedings of the Workshop on Language Resources, Technology and Services in the Sharing Paradigm",0,"Lexical Resources are a critical component for Natural Language Processing applications. However, the high cost of comparing and merging different resources has been a bottleneck to obtain richer resources and a broader range of potential uses for a significant number of languages. With the objective of reducing cost by eliminating human intervention, we present a new method towards the automatic merging of resources. This method includes both, the automatic mapping of resources involved to a common format and merging them, once in this format. This paper presents how we have addressed the merging of two verb subcategorization frame lexica for Spanish, but our method will be extended to cover other types of Lexical Resources. The achieved results, that almost replicate human work, demonstrate the feasibility of the approach."
W11-3305,Interoperability and Technology for a Language Resources Factory,2011,-1,-1,2,1,39867,marc poch,"Proceedings of the Workshop on Language Resources, Technology and Services in the Sharing Paradigm",0,None
R11-1041,Towards the Automatic Merging of Lexical Resources: Automatic Mapping,2011,17,7,2,1,18764,muntsa padro,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Lexical Resources are a critical component for Natural Language Processing applications. However, the high cost of comparing and merging different resources has been a bottleneck to have richer resources with a broad range of potential uses for a significant number of languages. With the objective of reducing cost by eliminating human intervention, we present a new method for automating the merging of resources, with special emphasis in what we call the mapping step. This mapping step, which converts the resources into a common format that allows latter the merging, is usually performed with huge manual effort and thus makes the whole process very costly. Thus, we propose a method to perform this mapping fully automatically. To test our method, we have addressed the merging of two verb subcategorization frame lexica for Spanish, The results achieved, that almost replicate human work, demonstrate the feasibility of the approach."
bel-2010-handling,Handling of Missing Values in Lexical Acquisition,2010,25,3,1,1,17513,nuria bel,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this work we propose a strategy to reduce the impact of the sparse data problem in the tasks of lexical information acquisition based on the observation of linguistic cues. We propose a way to handle the uncertainty created by missing values, that is, when a zero value could mean either that the cue has not been observed because the word in question does not belong to the class, i.e. negative evidence, or that the word in question has just not been observed in the context sought by chance, i.e. lack of evidence. This uncertainty creates problems to the learner, because zero values for incompatible labelled examples make the cue lose its predictive capacity and even though some samples display the sought context, it is not taken into account. In this paper we present the results of our experiments to try to reduce this uncertainty by, as other authors do (Joanis et al. 2007, for instance), substituting zero values for pre-processed estimates. Here we present a first round of experiments that have been the basis for the estimates of linguistic information motivated by lexical classes. We obtained experimental results that show a clear benefit of the proposed approach."
villegas-etal-2010-case,A Case Study on Interoperability for Language Resources and Applications,2010,11,4,2,1,8512,marta villegas,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper reports our experience when integrating differ resources and services into a grid environment. The use case we address implies the deployment of several NLP applications as web services. The ultimate objective of this task was to create a scenario where researchers have access to a variety of services they can operate. These services should be easy to invoke and able to interoperate between one another. We essentially describe the interoperability problems we faced, which involve metadata interoperability, data interoperability and service interoperability. We devote special attention to service interoperability and explore the possibility to define common interfaces and semantic description of services. While the web services paradigm suits the integration of different services very well, this requires mutual understanding and the accommodation to common interfaces that not only provide technical solution but also ease the user{\^a}ÂÂs work. Defining common interfaces benefits interoperability but requires the agreement about operations and the set of inputs/outputs. Semantic annotation allows defining some sort of taxonomy that organizes and collects the set of admissible operations and types input/output parameters."
wittenburg-etal-2010-resource,Resource and Service Centres as the Backbone for a Sustainable Service Infrastructure,2010,0,12,2,0,39140,peter wittenburg,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Currently, research infrastructures are being designed and established in many disciplines since they all suffer from an enormous fragmentation of their resources and tools. In the domain of language resources and tools the CLARIN initiative has been funded since 2008 to overcome many of the integration and interoperability hurdles. CLARIN can build on knowledge and work from many projects that were carried out during the last years and wants to build stable and robust services that can be used by researchers. Here service centres will play an important role that have the potential of being persistent and that adhere to criteria as they have been established by CLARIN. In the last year of the so-called preparatory phase these centres are currently developing four use cases that can demonstrate how the various pillars CLARIN has been working on can be integrated. All four use cases fulfil the criteria of being cross-national."
C10-1006,Automatic Detection of Non-deverbal Event Nouns for Quick Lexicon Production,2010,12,8,1,1,17513,nuria bel,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,In this work we present the results of experimental work on the development of lexical class-based lexica by automatic means. Our purpose is to assess the use of linguistic lexical-class based information as a feature selection methodology for the use of classifiers in quick lexical development. The results show that the approach can help reduce the human effort required in the development of language resources significantly.
bel-etal-2008-automatic,Automatic Acquisition for low frequency lexical items,2008,21,0,1,1,17513,nuria bel,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper addresses a specific case of the task of lexical acquisition understood as the induction of information about the linguistic characteristics of lexical items on the basis of information gathered from their occurrences in texts. Most of the recent works in the area of lexical acquisition have used methods that take as much textual data as possible as source of evidence, but their performance decreases notably when only few occurrences of a word are available. The importance of covering such low frequency items lies in the fact that a large quantity of the words in any particular collection of texts will be occurring few times, if not just once. Our work proposes to compensate the lack of information resorting to linguistic knowledge on the characteristics of lexical classes. This knowledge, obtained from a lexical typology, is formulated probabilistically to be used in a Bayesian method to maximize the information gathered from single occurrences as to predict the full set of characteristics of the word. Our results show that our method achieves better results than others for the treatment of low frequency items."
bel-etal-2008-coldic,"{COLDIC}, a Lexicographic Platform for {LMF} compliant lexica",2008,2,3,1,1,17513,nuria bel,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Despite of the importance of lexical resources for a number of NLP applications (Machine Translation, Information Extraction, Question Answering, among others), there has been a traditional lack of generic tools for the creation, maintenance and management of computational lexica. The most direct obstacle for the development of generic tools, independent of any particular application format, was the lack of standards for the description and encoding of lexical resources. The availability of the Lexical Markup Framework (LMF) has changed this scenario and has made it possible the development of generic lexical platforms. COLDIC is a generic platform for working with computational lexica. The system has been designed to let the user concentrate on lexicographical tasks, but still being autonomous in the management of the tools. The creation and maintenance of the database, which is the core of the tool, demand no specific training in databases. A LMF compliant schema implemented in a Document Type Definition (DTD) describing the lexical resources is taken by the system to automatically configure the platform. Besides, the most standard web services for interoperability are also generated automatically. Other components of the platform include build-in functions supporting the most common tasks of the lexicographic work."
W07-1214,The {S}panish Resource Grammar: Pre-processing Strategy and Lexical Acquisition,2007,19,10,2,1,26502,montserrat marimon,{ACL} 2007 Workshop on Deep Linguistic Processing,0,"This paper describes work on the development of an open-source HPSG grammar for Spanish implemented within the LKB system. Following a brief description of the main features of the grammar, we present our approach for pre-processing and ongoing research on automatic lexical acquisition."
N07-2002,Automatic Acquisition of Grammatical Types for Nouns,2007,9,10,1,1,17513,nuria bel,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"The work we present here is concerned with the acquisition of deep grammatical information for nouns in Spanish. The aim is to build a learner that can handle noise, but, more interestingly, that is able to overcome the problem of sparse data, especially important in the case of nouns. We have based our work on two main points. Firstly, we have used distributional evidences as features. Secondly, we made the learner deal with all occurrences of a word as a single complex unit. The obtained results show that grammatical features of nouns is a level of generalization that can be successfully approached with a Decision Tree learner."
W06-1001,Lexical Markup Framework ({LMF}) for {NLP} Multilingual Resources,2006,7,17,2,0,29837,gil francopoulo,Proceedings of the Workshop on Multilingual Language Resources and Interoperability,0,"Optimizing the production, maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing (NLP). A second aspect involves optimizing the process leading to their integration in applications. With this respect, we believe that the production of a consensual specification on multilingual lexicons can be a useful aid for the various NLP actors. Within ISO, one purpose of LMF (ISO-24613) is to define a standard for lexicons that covers multilingual data."
bel-etal-2006-new,New tools for the encoding of lexical data extracted from corpus,2006,6,0,1,1,17513,nuria bel,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the methodology and tools that are the basis of our platform AAILE.4 AAILE has been built for supplying those working in the construction of lexicons for syntactic parsing with more efficient ways of visualizing and analyzing data extracted from corpus. The platform offers support using techniques such as similarity measures, clustering and pattern classification."
francopoulo-etal-2006-lexical,Lexical Markup Framework ({LMF}),2006,3,123,5,0,29837,gil francopoulo,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Optimizing the production, maintenance and extension of lexical resources is one the crucial aspects impacting Natural Language Processing (NLP). A second aspect involves optimizing the process leading to their integration in applications. With this respect, we believe that the production of a consensual specification on lexicons can be a useful aid for the various NLP actors. Within ISO, the purpose of LMF is to define a standard for lexicons. LMF is a model that provides a common standardized framework for the construction of NLP lexicons. The goals of LMF are to provide a common model for the creation and use of lexical resources, to manage the exchange of data between and among these resources, and to enable the merging of large number of individual electronic resources to form extensive global electronic resources. In this paper, we describe the work in progress within the sub-group ISO-TC37/SC4/WG4. Various experts from a lot of countries have been consulted in order to take into account best practices in a lot of languages for (we hope) all kinds of NLP lexicons."
P04-3012,Corpus representativeness for syntactic information acquisition,2004,5,1,1,1,17513,nuria bel,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"This paper refers to part of our research in the area of automatic acquisition of computational lexicon information from corpus. The present paper reports the ongoing research on corpus representativeness. For the task of inducing information out of text, we wanted to fix a certain degree of confidence on the size and composition of the collection of documents to be observed. The results show that it is possible to work with a relatively small corpus of texts if it is tuned to a particular domain. Even more, it seems that a small tuned corpus will be more informative for real parsing than a general corpus."
bel-etal-2004-cost,Cost-effective Cross-lingual Document Classification,2004,6,0,1,1,17513,nuria bel,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This article addresses the question of how to deal with text categorization when the set of documents to be classified belong to different languages. The figures we provide demonstrate that cross-lingual classification where a classifier is trained using one language and tested against another is possible and feasible provided we translate a small number of words: the most relevant terms for class profiling. The experiments we report, demonstrate that the translation of these most relevant words proves to be a cost-effective approach to cross-lingual classification."
marimon-bel-2004-lexical,Lexical Entry Templates for Robust Deep Parsing,2004,8,2,2,1,26502,montserrat marimon,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,We report on the development and employment of lexical entry templates in a large--coverage unification--based grammar of Spanish. The aim of the work reported in this paper is to provide robust deep linguistic processing in order to make the grammar more adequate for industrial NLP applications.
villegas-bel-2002-dtd,From {DTD} to relational d{B}. An automatic generation of a lexicographical station out off {ISLE} guidelines,2002,4,1,2,1,8512,marta villegas,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
bel-etal-2002-design,Design and Evaluation of a {SLDS} for {E}-Mail Access through the Telephone,2002,0,1,1,1,17513,nuria bel,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
atkins-etal-2002-resources,From Resources to Applications. Designing the Multilingual {ISLE} Lexical Entry,2002,4,10,2,0,53506,sue atkins,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
2001.mtsummit-papers.13,The {ISLE} in the ocean. Transatlantic standards for multilingual lexicons (with an eye to machine translation),2001,-1,-1,4,0,18003,nicoletta calzolari,Proceedings of Machine Translation Summit VIII,0,"The ISLE project is a continuation of the long standing EAGLES initiative, carried out under the Human Language Technology (HLT) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation, supported by NSF and EC. In this paper we concentrate on the current position of the ISLE Computational Lexicon Working Group (CLWG), whose activities aim at defining a general schema for a multilingual lexical entry (MILE), as the basis for a standard framework for multilingual computational lexicons. The needs and features of existing Machine Translation systems provide the main reference points for the process of consensual definition of the MILE. The overall structure of the MILE will be illustrated with particular attention to some of the issues raised for multilingual lexicons by the need of expressing complex transfer conditions among translation equivalents"
bel-etal-2000-simple,{SIMPLE}: A General Framework for the Development of Multilingual Lexicons,2000,7,156,1,1,17513,nuria bel,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The project LE-SIMPLE is an innovative attempt of building harmonized syntactic-semantic lexicons for twelve European languages, aimed at use in different Human Language Technology applications. SIMPLE provides a general design model for the encoding of a large amount of semantic information, spanning from ontological typing, to argument structure and terminology. SIMPLE thus provides a general framework for resource development, where state-of-the-art results in lexical semantics are coupled with the needs of Language Engineering applications accessing semantic information."
villegas-etal-2000-multilingual,Multilingual Linguistic Resources: From Monolingual Lexicons to Bilingual Interrelated Lexicons,2000,3,2,2,0,8512,marta villegas,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper describes a procedure to convert the PAROLE-SIMPLE monolingual lexicons into bilingual interrelated lexicons where each word sense of a given language is linked to the pertinent sense of the right words in one or more target lexicons. Nowadays, SIMPLE lexicons are monolingual although the ultimate goal of these harmonised monolingual lexicons is to build multilingual lexical resources. For achieving this goal it is necessary to automatise the linking among the different senses of the different monolingual lexicons, as the production of such multilingual relations by hand will be, as all tasks related with the development of linguistic resources, unaffordable in terms of human resources and time spent. The system we describe in this paper takes advantage of the SIMPLE model and the SIMPLE based lexicons so that, in the best case, it can find fully automatically the relevant sense-to-sense correspondences for determining the translational equivalence of two words in two different languages and, in the worst case, it will be able to narrow the set of admissible links between words and relevant senses. This paper also explores to what extent semantic encoding in already existing computational lexicons such as SIMPLE can help in overcoming the problems arisen when using monolingual meaning descriptions for bilingual links and aims to set the basis for defining a model for adding a bilingual layer to the SIMPLE model. This bilingual layer based on a bilingual relation model will be the basis indeed for defining the multilingual language resource we want PAROLE-SIMPLE lexicons to become."
