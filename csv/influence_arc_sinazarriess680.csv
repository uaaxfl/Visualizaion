2020.acl-main.584,S12-1013,0,0.032247,"to the output of a certain layer (Anderson et al., 2018). 6536 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6536–6542 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ent predictions, especially for atypical colours of objects that do have a strong tendency towards a certain colour.2 2 Related Work Even recent work on colour terms has mostly been using artificial datasets with descriptions of isolated colour swatches that show a single hue, primarily examining effects of context and conversational adequacy in colour naming (Baumgaertner et al., 2012; Meo et al., 2014; McMahan and Stone, 2015; Monroe et al., 2016, 2017; Winn and Muresan, 2018). However, object colours bear a range of additional challenges for perception and grounding: (i) chromatic variation due to lighting and shading (Witzel and Gegenfurtner, 2018), (ii) effects of conventionalization as in e.g. red hair (G¨ardenfors, 2004) and (iii) the inherent complexity of real-world objects (Witzel and Gegenfurtner, 2018), e.g. a tree with green leaves and a brown trunk is typically called green (see figure 1). In human cognition, several recalibration strategies support the consta"
2020.acl-main.584,Q15-1008,0,0.029799,"t al., 2018). 6536 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6536–6542 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ent predictions, especially for atypical colours of objects that do have a strong tendency towards a certain colour.2 2 Related Work Even recent work on colour terms has mostly been using artificial datasets with descriptions of isolated colour swatches that show a single hue, primarily examining effects of context and conversational adequacy in colour naming (Baumgaertner et al., 2012; Meo et al., 2014; McMahan and Stone, 2015; Monroe et al., 2016, 2017; Winn and Muresan, 2018). However, object colours bear a range of additional challenges for perception and grounding: (i) chromatic variation due to lighting and shading (Witzel and Gegenfurtner, 2018), (ii) effects of conventionalization as in e.g. red hair (G¨ardenfors, 2004) and (iii) the inherent complexity of real-world objects (Witzel and Gegenfurtner, 2018), e.g. a tree with green leaves and a brown trunk is typically called green (see figure 1). In human cognition, several recalibration strategies support the constant perception of object colours given these"
2020.acl-main.584,D16-1243,0,0.0188946,"dings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6536–6542 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ent predictions, especially for atypical colours of objects that do have a strong tendency towards a certain colour.2 2 Related Work Even recent work on colour terms has mostly been using artificial datasets with descriptions of isolated colour swatches that show a single hue, primarily examining effects of context and conversational adequacy in colour naming (Baumgaertner et al., 2012; Meo et al., 2014; McMahan and Stone, 2015; Monroe et al., 2016, 2017; Winn and Muresan, 2018). However, object colours bear a range of additional challenges for perception and grounding: (i) chromatic variation due to lighting and shading (Witzel and Gegenfurtner, 2018), (ii) effects of conventionalization as in e.g. red hair (G¨ardenfors, 2004) and (iii) the inherent complexity of real-world objects (Witzel and Gegenfurtner, 2018), e.g. a tree with green leaves and a brown trunk is typically called green (see figure 1). In human cognition, several recalibration strategies support the constant perception of object colours given these challenges. In addit"
2020.acl-main.584,Q17-1023,0,0.0294209,"Missing"
2020.acl-main.584,D14-1162,0,0.087569,"Missing"
2020.acl-main.584,P14-1068,0,0.0333623,"e the same colour. Here, E ARLY-F USION also predominantly predicts a particular but incorrect colour, i.e. similarity in the off-the-shelf embedding space does not lead to good generalization for colour tendencies. This is particularly evident with lime: The prevailing prediction of yellow suggests that the (in this case, misleading) semantic similarity to the trained object type lemon is captured. These findings support previous work on multimodal distributional semantics showing that offthe-shelf embeddings do not necessarily capture similarity with respect to visual attributes of objects (Silberer and Lapata, 2014). 5 Discussion and Conclusion As in human perception, knowledge about typical object properties seems to be a valuable source of information for visual language grounding. Our fusion models clearly outperform a bottom-up baseline that relies solely on visual input. We also showed that the fusion architecture matters: the early integration of visual and conceptual information and their shared processing appears to be beneficial when colour diagnostic objects have atypical colours. However, even Early Fusion does not yet achieve a perfect balance between top-down and bottom-up processing. Future"
2020.acl-main.584,P18-2125,0,0.0127782,"eting of the Association for Computational Linguistics, pages 6536–6542 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ent predictions, especially for atypical colours of objects that do have a strong tendency towards a certain colour.2 2 Related Work Even recent work on colour terms has mostly been using artificial datasets with descriptions of isolated colour swatches that show a single hue, primarily examining effects of context and conversational adequacy in colour naming (Baumgaertner et al., 2012; Meo et al., 2014; McMahan and Stone, 2015; Monroe et al., 2016, 2017; Winn and Muresan, 2018). However, object colours bear a range of additional challenges for perception and grounding: (i) chromatic variation due to lighting and shading (Witzel and Gegenfurtner, 2018), (ii) effects of conventionalization as in e.g. red hair (G¨ardenfors, 2004) and (iii) the inherent complexity of real-world objects (Witzel and Gegenfurtner, 2018), e.g. a tree with green leaves and a brown trunk is typically called green (see figure 1). In human cognition, several recalibration strategies support the constant perception of object colours given these challenges. In addition to bottom-up driven strateg"
2020.acl-main.584,W16-6642,1,0.930439,"knowledge-based recalibration mechanisms to the automatic classification of object colours. Mojsilovic (2005) and Van de Weijer et al. (2007) propose pixelwise approaches for modeling colour naming in natural images, accounting for factors such as illumination and non-uniform object colours. Van de Weijer et al. (2007) assign colour terms as labels to colour values of individual pixels and then average over these labels to obtain a colour term for an image region. We use their model as one of our baselines in Section 4. However, they do not take into account object-specific colour tendencies. Zarrieß and Schlangen (2016) classify colour histograms for objects in real-world images. They train object-specific classifiers that recalibrate a bottom-up classifier, but only obtain a small improvement from recalibration. We implement a general top-down component that can be 2 Code and data for this project are available at: https://github.com/clause-jena/colour-term-grounding integrated with bottom-up processing in different ways. 3 Models We focus on the effect of knowledge in language grounding and adopt a slightly idealized setting for modeling: we assume that the object type is available during training and test"
2020.coling-main.172,D14-1086,0,0.251912,"for a given object, it is important to verify that this apparent naming variation is not a mere consequence of, for instance, lexical errors or different people naming different objects in the same scene. In this work, we assess the factors that affect the collection of object naming data in the typical Language & Vision (L&V) setup, and test whether these factors impact the accuracy or apparent accuracy of a L&V object labeling model. Existing work in L&V has relied on data collection methods that prompt natural language users to freely talk about or refer to particular objects in an image (Kazemzadeh et al., 2014; Yu et al., 2016; Silberer et al., 2020). Common to these methods is the use of images as a proxy for the actual context of language use (i.e., the real world), and the use of bounding boxes drawn onto the image to indicate the target object that is to be named. These two common aspects, essentially simulating real-world linguistic reference, have enabled large-scale data collection leveraging existing Computer Vision datasets. However, both aspects also introduce potential confounding factors, such as referential uncertainty, where a bounding box may fail to uniquely identify the target obje"
2020.coling-main.172,2020.lrec-1.710,1,0.78992,"Missing"
2020.coling-main.172,D19-1514,0,0.0283849,"e the consistent response set), and inadequate names of various types. We now use it to define a diagnostic evaluation method for object naming models, one which is more fine-grained than the predominant single-label evaluation. It also allows to assess whether models are affected by the same issues as humans, in particular referential and visual uncertainty. We apply our evaluation to Bottom-Up (Anderson et al., 2018) as a representative L&V object naming model, which has been widely used for transfer learning in L&V (Lu et al., 2019; Gao et al., 2019; Chen et al., 2019; Cadene et al., 2019; Tan and Bansal, 2019, inter alia). In contrast to existing works in Computer Vision research (Hoiem et al., 2012) on diagnosing the effects of object or image characteristics on model performance, ManyNames allows us to compare the model against an upper bound of the human performance in object naming, estimated via the verification annotations. Our analysis focuses on two questions: First, can an object detector that was trained in a single-label setting (i.e., towards predicting unique ground truth names) account for the naming variation inherent in human object naming behavior? Second, does the model exhibit a"
2020.coling-main.172,P17-1023,1,0.808481,"ostly concerned with naming particular object instances situated in naturalistic images. In such a setting, naming preferences are more nuanced: humans may prefer different names for instances of the same class (e.g., Fig. 1a-b), and even disagree in their choice for the same instance (Graf et al., 2016; Silberer et al., 2020). While Graf et al. (2016) modeled object naming in a controlled setting, Ordonez et al. (2016) and Mathews et al. (2015) used ImageNet data (Deng et al., 2009), where images show more realistic, yet still isolated objects annotated with WordNet synsets (Fellbaum, 1998). Zarrieß and Schlangen (2017) train a classification-based model for names produced in referring expressions in the RefCOCO data (Yu et al., 2016), where objects are situated in complex scenes and names might be affected by context. In contrast to our work, existing approaches did not have access to name annotations from many different annotators 1894 (a) C bird 27/duck 8 I chicken E singleton (b) duck 33/bird 3 — — (c) (d) (e) bear 16/polar bear 6/animal 3 table 23/counter 5 dog 24/dalmatian 2 ball 5 |dog 3 food 6 |tabletop;desk wheel 4 |ornament;toy;statue other-obj |inadequate other-objs |singletons other-obj |singleto"
2020.inlg-1.38,W05-0909,0,0.216605,"s are guaranteed to express correct information, at the cost of naturalness, and so can serve as an upper bound on semantic metrics. 4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also parse the generated instructions using our instruction parser, to extract what was mentioned as target block and as landmark. We can t"
2020.inlg-1.38,P18-1182,0,0.0350021,"Missing"
2020.inlg-1.38,gargett-etal-2010-give,0,0.0225821,"asanmi et al., 2019; Park et al., 2019), the task is to verbalise what is different between two otherwise very similar images. Our task contains this, but goes beyond it in that it also has to be verbalised how that difference can be effected. In that work, specialised architectures are presented that can more easily extract differences. Here, we wanted to start by exploring more standard captioning approaches as a baseline in order to fully understand the requirements of our task, leaving further architectural adaptations to future work. After early work in the context of the GIVE challenge (Gargett et al. (2010), Byron et al. (2007)), there is some renewed interest in instruction giving. K¨ohn et al. (2020) presented an instruction giving platform called MC-Saar-Instruct where players can interact with a bot in the Minecraft world which instructs them to build something. This is very related to our interest in this project; for now, however, that work still assumes a symbolic representation as input. In the field of natural language generation, the production of referring expressions is a wellestablished task (Krahmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and"
2020.inlg-1.38,W19-1608,0,0.130953,"hmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can learn and express. 3 Data: BLOCKS and BLOCKSgen We will now describe how we can make use of data collected for instruction following for our task of instruction giving. 3.1 BLOCKS : Instruction Following Bisk et al. (2016) collected the BLOCKS dataset in order to study instruction following in a simple visual environment. The environment consists of up to 20 blocks of the same size, which are placed on a board. The blocks are uniquely labelled either with a number between 1 and 20, or with the logo of a major company; thi"
2020.inlg-1.38,W19-8668,0,0.130424,"hmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can learn and express. 3 Data: BLOCKS and BLOCKSgen We will now describe how we can make use of data collected for instruction following for our task of instruction giving. 3.1 BLOCKS : Instruction Following Bisk et al. (2016) collected the BLOCKS dataset in order to study instruction following in a simple visual environment. The environment consists of up to 20 blocks of the same size, which are placed on a board. The blocks are uniquely labelled either with a number between 1 and 20, or with the logo of a major company; thi"
2020.inlg-1.38,2020.sigdial-1.7,0,0.0933723,"Missing"
2020.inlg-1.38,J12-1006,0,0.0241617,"Missing"
2020.inlg-1.38,W04-1013,0,0.023979,"4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also parse the generated instructions using our instruction parser, to extract what was mentioned as target block and as landmark. We can then compare these 4 https://github.com/salaniz/pycocoevalcap We train all of our models on the BLOCKS dataset using t"
2020.inlg-1.38,D18-1287,0,0.0542746,"Missing"
2020.inlg-1.38,P02-1040,0,0.106509,"states, we can also use the template generator described above to generate instructions for the test set. The generated instructions are guaranteed to express correct information, at the cost of naturalness, and so can serve as an upper bound on semantic metrics. 4.3 Evaluation Metrics In the test set, we have available both the symbolic representation of the states (and hence objectively know what the required change is) and the set of reference instructions E. We define metrics making use of either. We use common metrics from caption generation: BLEU-4, measuring token overlap up to 4grams (Papineni et al., 2002); CIDEr, measuring overlap based on the consensus of reference instructions (Vedantam et al., 2015), METEOR (Banerjee and Lavie, 2005), measuring unigram overlap with advanced normalization like stemming and synonym comparison, and ROUGE-L (Lin, 2004) which measures similarity based on longest common subsequences. We apply each individual metric by comparing the generated instruction against all available reference instructions for the respective image pair using the pycocoevalcap library.4 To better analyse task performance, where it matters that the blocks are correctly referred to, we also"
2020.inlg-1.38,W18-6563,1,0.853284,"t al. (2010), Byron et al. (2007)), there is some renewed interest in instruction giving. K¨ohn et al. (2020) presented an instruction giving platform called MC-Saar-Instruct where players can interact with a bot in the Minecraft world which instructs them to build something. This is very related to our interest in this project; for now, however, that work still assumes a symbolic representation as input. In the field of natural language generation, the production of referring expressions is a wellestablished task (Krahmer and van Deemter, 2012), increasingly also tackled with neural methods (Zarrieß and Schlangen, 2018; Castro Ferreira et al., 2018). IG-BA includes this; but as will become clear in the next section, the data that we use here allows us to factor it out, as references to objects can simply be done via unique names. The complexity in our task comes from the spatial language required to denote locations, which is something not found to that degree neither in image captioning nor referring expression generation. Generation of spatial expressions has seen some attention in recent years, e.g. by Ghanimifard and Dobnik (2019a), who investigate the spatial language that neural language models can le"
2020.lrec-1.710,P15-2017,0,0.192301,"chical variation (chihuahua vs. dog), which has been discussed at length in the theoretical literature, and other less well studied phenomena like cross-classification (cake vs. dessert). Keywords: object naming, language and vision, computer vision 1. Introduction A central issue in Language & Vision (L&V) is how speakers refer to objects. This is most prominent for referring expression generation and interpretation (Kazemzadeh et al., 2014; Mao et al., 2015; Yu et al., 2016), but it also pervades virtually any other L&V task, such as caption generation or visual dialogue (Fang et al., 2015; Devlin et al., 2015; Das et al., 2017; De Vries et al., 2017). One of the central components of referring expressions are object names; for instance, speakers may name the left object in Figure 1 cake, food, or dessert, a.o. This aspect of reference has been understudied in Computational Linguistics and L&V; as a consequence, it is not clear to what extent current L&V models capture human naming behavior. In the same way that it has proven useful to model referring expressions in visual scenes in an isolated fashion, for which systems are required to integrate and reason over the visual scene and context objects"
2020.lrec-1.710,D14-1086,0,0.834256,"ted with an object is much higher in our dataset than in existing corpora for Language and Vision, such that ManyNames provides a rich resource for studying phenomena like hierarchical variation (chihuahua vs. dog), which has been discussed at length in the theoretical literature, and other less well studied phenomena like cross-classification (cake vs. dessert). Keywords: object naming, language and vision, computer vision 1. Introduction A central issue in Language & Vision (L&V) is how speakers refer to objects. This is most prominent for referring expression generation and interpretation (Kazemzadeh et al., 2014; Mao et al., 2015; Yu et al., 2016), but it also pervades virtually any other L&V task, such as caption generation or visual dialogue (Fang et al., 2015; Devlin et al., 2015; Das et al., 2017; De Vries et al., 2017). One of the central components of referring expressions are object names; for instance, speakers may name the left object in Figure 1 cake, food, or dessert, a.o. This aspect of reference has been understudied in Computational Linguistics and L&V; as a consequence, it is not clear to what extent current L&V models capture human naming behavior. In the same way that it has proven u"
2020.lrec-1.710,J12-1006,0,0.071128,"Missing"
2020.lrec-1.710,P15-1027,0,0.031434,"d fashion, for which systems are required to integrate and reason over the visual scene and context objects in which an object is presented, we believe there is value in modeling object names on their own. Specifically, questions that need addressing regarding object naming are (1) how much naming variation is attested and what factors drive the choice of a name (object category? individual properties of the object? context?); see e.g. Rohde et al. (2012) and Graf et al. (2016); (2) how to make L&V models more human-like with respect to naming, improving the design of L&V architectures (e.g., Lazaridou et al. (2015); Ordonez et al., (2016); Zhao et al., (2017)). In this paper, we present a new dataset, ManyNames, to provide richer possibilities for both analysis and modeling of human naming behavior. Existing resources in L&V that provide object names can be exploited for this area of inquiry to a limited extent only, as we will detail in Section 3, because their low number of annotations per item prevents reliable assessment of naming preferences, on the one hand, and variation, on the other. We chose a creation methodology for ManyNames that overcome these shortcomings in particular. cake (53), food (1"
2020.lrec-1.710,P14-5010,0,0.00242758,"x contains several objects); (iv) the most frequent elicited name is of the same domain as the VG name. This yielded 25, 596 images (we discarded 5, 497). We then did 3 more collection rounds, obtaining a total of 36 annotations per object. Figure 5 shows the instructions for these rounds; they were accompanied by a FAQ solving common issues. We shuffled the set of images per task between rounds, and workers could only participate in one round, to avoid workers annotating an instance more than once. Overall 841 workers4 and image in VG is chosen at most once). 3 We obtained tags with CoreNLP (Manning et al., 2014). 4 5796 Participation was restricted to residents of the UK, USA, Figure 4: Instructions for AMT annotators for the first round (whole instructions showed more examples, see Figure 5). Figure 5: Instructions for AMT annotators for rounds 2 to 4. took part in the data elicitation, with a median of 261 instances (range = [9, 17K]) per worker. 5. Analysis As shown in Table 1 above, ManyNames gathers many more names per object than previous datasets: 35.3 on average, compared to 1−7. It also contains the most variability, since objects have on average 5.7 names (compared to 1 − 1.9). Figure 6 sho"
2020.lrec-1.710,J91-4003,0,0.703822,"ators often chose a more general name than the VG annotators. In a qualitative analysis, we found the following types of variation in the data, illustrated with examples in Figure 6: Cross-classification: a substantial group are names conceptualizing alternative aspects of the same object (e.g. toast/dessert, image C). Conceptual disagreement: as we did not filter objects for prototypicality, our data mirrors a certain amount of disagreement between speakers as to what an object is (bed/bench, image J). Metonymy: we find examples reminiscent of metonymy discussed in the linguistic literature (Pustejovsky, 1991) where logically related parts of an object stand in as its name (burger/basket, image B). Issues with WordNet: due to WordNet’s fine-grained hierarchy, it is difficult to retrieve certain loose synonyms or hypernyms (robe/dress, image not shown). 6. Conclusion The question of how people choose names for objects presented visually is relevant for Language and Vision, Computational Linguistics, Computer Vision, Cognitive Science, 5 To detect hypernyms, we use the hypernym closure of the synset with a depth of 10; the other relations are straightforward. The coverage of WordNet for our name data"
2021.hcinlp-1.11,N18-1177,0,0.061589,"Missing"
2021.hcinlp-1.11,2020.acl-main.642,0,0.0375756,"mulations that are highly hypothetical (”what would the vessel behave if we changed the blood flow drastically to ...”). These cases are in fact simulations of possible solutions helping the user to visualize and explore the solution space, which are much more convenient and intuitive expressed using natural language which can be supported by strong dialogue models that adapt to the context. Next to these improvements on the level of dialogue modeling, recent developments in Language & Vision focus on grounding verbal utterances in visual inputs as, for instance, in visual question answering (Huang et al., 2020; Khademi, 2020). Visual dialogue (Das et al., 2017; Wang et al., 2020) extends the dialogue modelling task to the visual modality. Mixed-initiative visual dialogue, as e.g. in Ilinykh et al. (2019), aims at modeling interactions in which both dialogue partners can talk and act, which could be an interesting setting for visual exploration tasks. We believe that these successes in neural dialogue modelling and the integration of different modalities as in visual dialogue can lead to new possibilities for interactive systems in VIS, as we will discuss below. 69 Visual Grounding Visual language g"
2021.hcinlp-1.11,2020.acl-main.604,0,0.0209489,"to set-up even for non-experts (of NLP) and do not require large amounts of training data, as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, such as in medicine (Meuschke et al., 2016, 2017) or cultural-technical scenarios (Lawonn et al., 2016), require the visual explora"
2021.hcinlp-1.11,2020.emnlp-main.327,0,0.0582308,"Missing"
2021.hcinlp-1.11,2020.acl-main.232,0,0.0135288,"as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, such as in medicine (Meuschke et al., 2016, 2017) or cultural-technical scenarios (Lawonn et al., 2016), require the visual exploration of complex models. Figure 2 shows an example of a 3D-mesh of an aneurysm and a corresponding"
2021.hcinlp-1.11,2020.acl-main.643,0,0.0422793,"ighly hypothetical (”what would the vessel behave if we changed the blood flow drastically to ...”). These cases are in fact simulations of possible solutions helping the user to visualize and explore the solution space, which are much more convenient and intuitive expressed using natural language which can be supported by strong dialogue models that adapt to the context. Next to these improvements on the level of dialogue modeling, recent developments in Language & Vision focus on grounding verbal utterances in visual inputs as, for instance, in visual question answering (Huang et al., 2020; Khademi, 2020). Visual dialogue (Das et al., 2017; Wang et al., 2020) extends the dialogue modelling task to the visual modality. Mixed-initiative visual dialogue, as e.g. in Ilinykh et al. (2019), aims at modeling interactions in which both dialogue partners can talk and act, which could be an interesting setting for visual exploration tasks. We believe that these successes in neural dialogue modelling and the integration of different modalities as in visual dialogue can lead to new possibilities for interactive systems in VIS, as we will discuss below. 69 Visual Grounding Visual language grounding in thes"
2021.hcinlp-1.11,2020.acl-main.565,0,0.0179315,"rate an appropriate visualization output, as e.g. in (Yu and Silva, 2020). These grammars are relatively easy to set-up even for non-experts (of NLP) and do not require large amounts of training data, as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, such as in medicine"
2021.hcinlp-1.11,2020.acl-main.459,0,0.0274772,"tput, as e.g. in (Yu and Silva, 2020). These grammars are relatively easy to set-up even for non-experts (of NLP) and do not require large amounts of training data, as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, such as in medicine (Meuschke et al., 2016, 2017) or cultural-technic"
2021.hcinlp-1.11,D16-1127,0,0.108098,"Missing"
2021.hcinlp-1.11,D17-1230,0,0.0220346,"y address fundamentally different interaction and evaluation set-ups. Goal- or task-oriented systems are typically designed towards helping the user to achieve a very specific goal in a given context. For instance, in instruction-following and -generation (Fried et al., 2017, 2018), a user or system needs to reach a specific position in an environment by following navigation instructions. Here, the interaction is often asymmetric in the sense that the modalities to be used by the partners are very restricted (the instruction follower acts, the giver speaks). Open-domain dialogue systems, like Li et al. (2017); Adiwardana et al. (2020) are not bound to a goal and therefore require a high awareness of context, personality and variety of the dialogue system as Santhanam and Shaikh (2019) point out. Generally speaking, the field of VIS is interested in the development of techniques for creating visual models (Brehmer and Munzner, 2013; Liu et al., 2014; Amar et al., 2005). A visual model is data that is mapped into a visually perceivable space by representing concepts in the data through visual concepts to make them easily perceivable and understandable by humans. This supports research and education"
2021.hcinlp-1.11,2020.findings-emnlp.316,0,0.0966411,"Missing"
2021.hcinlp-1.11,2020.acl-main.328,0,0.0399348,"guage, but the query-based approach used in many NLIs still seems to lack flexibility. Figure 1: Goal-oriented NLI as used in Yu and Silva (2020), created from: https://visflow.org/demo/ 3 3.1 Existing Work Natural Language Interfaces in VIS A range of recent papers have looked into integrating NLP in VIS systems, by implementing NLIs that translate a natural language query to a visualization command in some programming language. This allows users to “talk to some dataset”, as illustrated in Figure 1. Existing NLIs are applied in systems that create and manipulate, e.g., chart visualizations (Shao and Nakashole, 2020). Similar interfaces are proposed in Narechania et al. (2020); Huang et al. (2019); Yu and Silva (2020); Fu et al. (2020); Chowdhury et al. (2021); Setlur et al. (2016) These existing NLIs are mostly applied in the field of visual analytics. Here, the user has a concrete goal in mind, i.e. some manipulation of the underlying data table (e.g. aggregation, filtering). Dimara and Perin (2019) point out that the exact understanding of the users’ goal is important in these interfaces and one current approach for improving the inference of the users’ intent is to predict it based on activity logs. S"
2021.hcinlp-1.11,2020.acl-main.229,0,0.0166362,"e large amounts of training data, as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, such as in medicine (Meuschke et al., 2016, 2017) or cultural-technical scenarios (Lawonn et al., 2016), require the visual exploration of complex models. Figure 2 shows an example of a 3D"
2021.hcinlp-1.11,2020.acl-main.219,0,0.0262661,"queries and then generate an appropriate visualization output, as e.g. in (Yu and Silva, 2020). These grammars are relatively easy to set-up even for non-experts (of NLP) and do not require large amounts of training data, as most state-of-the-art dialogue systems developed in NLP. An important limitation of this approach, however, is that such semantic grammars are designed to translate directly between a given user query in natural language and some underlying data query language like e.g. SQL. This means that, in 3.2 Language & Vision A lot of recent work in NLP tackles dialogue modelling (Shuster et al., 2020; Qin et al., 2020; Ham et al., 2020; Rameshkumar and Bailey, 2020) or question answering (Baheti et al., 2020; Liu et al., 2020). Goal-based dialogue covers navigation Zhu et al. (2020), manipulation (Jayannavar et al., 2020) or classical information presentation tasks (Andreas et al., 2020). A central problem in these mod68 4 Future Work Uncertainty We believe that a fruitful direction for more flexible NLP-based systems in VIS is to look at scenarios where users might not have a concrete manipulation task or goal in mind, but want to explore a complex visual model. Numerous applications, su"
2021.hcinlp-1.11,2020.emnlp-main.269,0,0.0261452,"f we changed the blood flow drastically to ...”). These cases are in fact simulations of possible solutions helping the user to visualize and explore the solution space, which are much more convenient and intuitive expressed using natural language which can be supported by strong dialogue models that adapt to the context. Next to these improvements on the level of dialogue modeling, recent developments in Language & Vision focus on grounding verbal utterances in visual inputs as, for instance, in visual question answering (Huang et al., 2020; Khademi, 2020). Visual dialogue (Das et al., 2017; Wang et al., 2020) extends the dialogue modelling task to the visual modality. Mixed-initiative visual dialogue, as e.g. in Ilinykh et al. (2019), aims at modeling interactions in which both dialogue partners can talk and act, which could be an interesting setting for visual exploration tasks. We believe that these successes in neural dialogue modelling and the integration of different modalities as in visual dialogue can lead to new possibilities for interactive systems in VIS, as we will discuss below. 69 Visual Grounding Visual language grounding in these scenarios captures not only the grounding of words in"
2021.inlg-1.41,D16-1125,0,0.0288461,"in, e.g., psycholinguistics and experimental pragmatics (Keysar et al., 1998; Galati and Brennan, 2010; Degen and Tanenhaus, 2016, 2019). Similarly, models of pragmatic reasoning and their applications in NLP face the challenge that fully rational language generation is computationally costly or even intractable (Reiter, 1991; White et al., 2020). Recent work on pragmatics in NLP has taken interest in the Rational Speech Acts (RSA) model (Frank and Goodman, 2012) which resulted in implementations of frameworks that model the generation of informative language with so-called rational speakers (Andreas and Klein, 2016; Fried et al., 2018; Shen et al., 2019). Cohn-Gordon et al. (2018) use an image captioning set-up inspired by classical reference games (see Figure 1) to show that a ‘slow’ rational speaker which reasons internally about the informativeness of utterances generated by a plain neural language model is communicatively more effective than a ‘fast’ literal speaker that produces the most likely utterance for the target as predicted by the language model. More generally, recent work in NLG has shown a lot of interest in reasoning or decoding methods that extend neural generation models with addition"
2021.inlg-1.41,W19-0109,0,0.0326907,"veness than a fully rational speaker. 2 ing word-level image captioning models with RSA and Vedantam et al. (2017)’s discriminative beam search. Next to these trade-offs, rational speakers in RSA, which apply complex recursive reasoning using an internal listener and speaker, incur a high computational cost, particularly in generation setups with large candidate spaces where exhaustive search is not tractable. Therefore, recent works have implemented incremental decoding schemes that reason about discriminativeness at every timestep, during unrolling the language model (Vedantam et al., 2017; Cohn-Gordon et al., 2019). CohnGordon et al. (2018)’s character-level approach fully confronts pragmatic reasoning: the neural language model captions images in a character-bycharacter fashion such that each character can be internally scored for its informativeness by the rational speaker. While this incremental generation and reasoning scheme makes it possible to search a large space of potential utterances, it is still extremely costly as the internal, recursive reasoning of the speaker is applied at every character. In this paper, we propose a very simple but highly efficient relaxation of fully rational and incre"
2021.inlg-1.41,N18-1177,0,0.0524873,"Missing"
2021.inlg-1.41,P19-1365,0,0.10185,"ch reasons internally about the informativeness of utterances generated by a plain neural language model is communicatively more effective than a ‘fast’ literal speaker that produces the most likely utterance for the target as predicted by the language model. More generally, recent work in NLG has shown a lot of interest in reasoning or decoding methods that extend neural generation models with additional objectives that cannot be easily achieved by decoding the underlying neural language model with greedy or beam search (e.g., Li et al., 2016; Vedantam et al., 2017; Vijayakumar et al., 2018; Ippolito et al., 2019; Holtzman et al., 2020; Tam, 2020). Reasoning schemes like RSA provide an attractive, since explicit and theoretically motivated, way of incorporating linguistically plausible, communicative strategies into a neural generation framework. At the same time, however, RSA and various related decoding methods have been found to not achieve a good balance between different dimensions of output quality. For instance, Ippolito et al. (2019) investigates a range of decoding methods that aim at increasing the lexical diversity of image captions or responses in dialogue and report on a very clear qualit"
2021.inlg-1.41,C18-1147,0,0.0240344,"Missing"
2021.inlg-1.41,2021.sigdial-1.43,1,0.857687,"n framework. At the same time, however, RSA and various related decoding methods have been found to not achieve a good balance between different dimensions of output quality. For instance, Ippolito et al. (2019) investigates a range of decoding methods that aim at increasing the lexical diversity of image captions or responses in dialogue and report on a very clear quality-diversity trade-off: the more the decoding procedure (e.g., sampling) increases diversity and deviates from the predictions of the underlying language model, the more the generated expressions decrease in quality. Recently, Schüz et al. (2021) found similar trade-offs for decod371 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 371–376, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics S0 S1 Sx a group of people riding on the backs of horses two brown hornes grazing in a fenced grassy field two horses in a field in front of a field Figure 1: Captions for the target image (large), generated by a literal (S0 ), rational (S1 ) and mixed speaker (Sx ), with rationality parameter α = 5 and beam search. The captions by S1 and Sx are more discriminat"
2021.inlg-1.41,N19-1410,0,0.0215293,"pragmatics (Keysar et al., 1998; Galati and Brennan, 2010; Degen and Tanenhaus, 2016, 2019). Similarly, models of pragmatic reasoning and their applications in NLP face the challenge that fully rational language generation is computationally costly or even intractable (Reiter, 1991; White et al., 2020). Recent work on pragmatics in NLP has taken interest in the Rational Speech Acts (RSA) model (Frank and Goodman, 2012) which resulted in implementations of frameworks that model the generation of informative language with so-called rational speakers (Andreas and Klein, 2016; Fried et al., 2018; Shen et al., 2019). Cohn-Gordon et al. (2018) use an image captioning set-up inspired by classical reference games (see Figure 1) to show that a ‘slow’ rational speaker which reasons internally about the informativeness of utterances generated by a plain neural language model is communicatively more effective than a ‘fast’ literal speaker that produces the most likely utterance for the target as predicted by the language model. More generally, recent work in NLG has shown a lot of interest in reasoning or decoding methods that extend neural generation models with additional objectives that cannot be easily achi"
2021.lantern-1.5,W16-3202,0,0.0275259,"Missing"
2021.lantern-1.5,P16-1200,0,0.0240511,"modal NER models that identify mentions of entities in a text and link them to corresponding images or image regions, cf. AsgariChenaghlu et al. (2020) for a similar proposal. Event detection is another text-based NLP task that has been approached with the use of CNNs (Nguyen and Grishman, 2015) and, more recently, 3 56 and consequently save on expensive manual annotation Acknowledgements attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. Parsing these texts as a discourse is also a topic of ongoing research (Liu et al., 2017a; Li et al., 2016). To the best of our knowledge, research in Vision & Language has hardly been inspired by these classical, entity-centric task in text processing. This general impression is corroborated by the very comprehensive V"
2021.lantern-1.5,P17-1188,0,0.0219872,"person, location and organisation, however there is a number of NER tools with varying tag sets. One way to model texts of the type illustrated in Fig. 1 would be to move towards multi-modal NER models that identify mentions of entities in a text and link them to corresponding images or image regions, cf. AsgariChenaghlu et al. (2020) for a similar proposal. Event detection is another text-based NLP task that has been approached with the use of CNNs (Nguyen and Grishman, 2015) and, more recently, 3 56 and consequently save on expensive manual annotation Acknowledgements attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. Parsing these texts as a discourse is also a topic of ongoing research (Liu et al., 2017a; Li et al., 2016). To the best of our knowled"
2021.lantern-1.5,D14-1086,0,0.0419109,"s . Typical captions consist of a single sentence that directly refers to the image. Other work has looked at more fine-grained referential relations such as referring expressions in the Medium In addition to the image content, we observe that the original medium is highly relevant in figuring out its semantic relation. The domain 1 Discussion 2 They could be seen as being linked to the text as a whole, but this is not particularly informative for information extraction or similar tasks. for Fig. 2, see supplementary material 55 form of noun phrases that identify specific objects in an image (Kazemzadeh et al., 2014). Work on even more fine-grained resolution that captures object parts is relatively rare, but see (H¨urlimann and Bos, 2016). A complementary trend is to use texts that are (slightly) longer than image captions, such as image paragraphs that describe the image content in a sequence of sentences (Krause et al., 2017) or dialogues that center on identifying an object in a sequence of turns (de Vries et al., 2017) or an image from a set of images (Das et al., 2017). All of these datasets are crowdsourced and target a referential task on a specific, fixed level, i.e. image-sentence, object-phrase"
2021.lantern-1.5,P17-1164,0,0.0202969,"person, location and organisation, however there is a number of NER tools with varying tag sets. One way to model texts of the type illustrated in Fig. 1 would be to move towards multi-modal NER models that identify mentions of entities in a text and link them to corresponding images or image regions, cf. AsgariChenaghlu et al. (2020) for a similar proposal. Event detection is another text-based NLP task that has been approached with the use of CNNs (Nguyen and Grishman, 2015) and, more recently, 3 56 and consequently save on expensive manual annotation Acknowledgements attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. Parsing these texts as a discourse is also a topic of ongoing research (Liu et al., 2017a; Li et al., 2016). To the best of our knowled"
2021.lantern-1.5,D19-1469,0,0.0209129,"lections of sentences and images from the same documents; or different documents, for instances of non-relatedness. This information is used at test time to estimate the individual links between the sentences and images of a given document. Hessel et al. (2019) is highly relevant to the concerns discussed in this paper because it has some success in grappling with the comparatively large amounts of text in the Wikipedia article genre. Diverse Referential Relations There is some initial work on datasets and tasks that capture more varied semantic or discursive relations between image and text: Kruk et al. (2019) tag the image intent in multi-modal Twitter posts, distinguishing between intents like ‘provocative’, ‘expressive’ or ‘promotive’. Their annotations assign a global label to the image which captures the relation to the text as a whole. This goes beyond literal image descriptions, but still does not capture structurally diverse referential relations. Alikhani et al. (2019) investigate text-image coherence in recipe texts that describe sequences of consecutive actions in a cooking context. Structurally, the recipe’s text is already segmented, with an image aligned to each step. Alikhani et al."
2021.lantern-1.5,2020.lrec-1.526,0,0.0392749,"), combining referential relations on two different levels. None of these tasks and models, however, deal with image-text pairs where significant parts of the text have no relation to the visual content, thereby circumventing the need to identify fragments that do indeed stand in a referential relation to a given image. rhetorical purpose and authorial intent of each picture seems to be more or less uniform. That is, images are included to illustrate (as opposed to being provocative or expressive). Likewise, the semiotics of these images are overwhelmingly parallel to the content of the text. Muraoka et al. (2020) work with a more coarsegrained and somewhat simplified version of the problem discussed in this paper. Their task is to correctly predict the physical alignment of images and sections in Wikipedia articles. This approach utilizes the inherent document structure3 , however our observations (see Section 2) call into question the presupposition that alignment in layout entails alignment in content. A similar text-image matching task is discussed in Hessel et al. (2019), where the authors seek to match the images in a document to the most relevant sentences in it (leaving out the captions). Their"
2021.lantern-1.5,P15-2060,0,0.015008,"n (NER) is a very well-known NLP task that is useful in a range of applications (Li et al., 2020). The most standard named entity categories are person, location and organisation, however there is a number of NER tools with varying tag sets. One way to model texts of the type illustrated in Fig. 1 would be to move towards multi-modal NER models that identify mentions of entities in a text and link them to corresponding images or image regions, cf. AsgariChenaghlu et al. (2020) for a similar proposal. Event detection is another text-based NLP task that has been approached with the use of CNNs (Nguyen and Grishman, 2015) and, more recently, 3 56 and consequently save on expensive manual annotation Acknowledgements attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. P"
2021.lantern-1.5,D16-1035,0,0.0119015,"ments attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. Parsing these texts as a discourse is also a topic of ongoing research (Liu et al., 2017a; Li et al., 2016). To the best of our knowledge, research in Vision & Language has hardly been inspired by these classical, entity-centric task in text processing. This general impression is corroborated by the very comprehensive V&L survey of Mogadala et al. (2019). 5 This work was supported by a grant from the Federal Ministry of Education and Research (BMBF, grant No. 01UG2120A). References Malihe Alikhani, Sreyasi Nag Chowdhury, Gerard de Melo, and Matthew Stone. 2019. CITE: A corpus of image-text discourse relations. In Proceedings of the 2019 Conference of the North American Chapter of the Association fo"
2021.lantern-1.5,D19-1514,0,0.0172909,"at describe the image content in a sequence of sentences (Krause et al., 2017) or dialogues that center on identifying an object in a sequence of turns (de Vries et al., 2017) or an image from a set of images (Das et al., 2017). All of these datasets are crowdsourced and target a referential task on a specific, fixed level, i.e. image-sentence, object-phrase, object-dialogue, image-dialogue. It is worth noting that, internally, many recent largescale models in V&L process object-phrase relations while encoding image-sentence pairs (Lee et al., 2018; Anderson et al., 2018; Kottur et al., 2018; Tan and Bansal, 2019; Lu et al., 2020), combining referential relations on two different levels. None of these tasks and models, however, deal with image-text pairs where significant parts of the text have no relation to the visual content, thereby circumventing the need to identify fragments that do indeed stand in a referential relation to a given image. rhetorical purpose and authorial intent of each picture seems to be more or less uniform. That is, images are included to illustrate (as opposed to being provocative or expressive). Likewise, the semiotics of these images are overwhelmingly parallel to the cont"
2021.lantern-1.5,P19-1139,0,0.0184583,"hat identify mentions of entities in a text and link them to corresponding images or image regions, cf. AsgariChenaghlu et al. (2020) for a similar proposal. Event detection is another text-based NLP task that has been approached with the use of CNNs (Nguyen and Grishman, 2015) and, more recently, 3 56 and consequently save on expensive manual annotation Acknowledgements attention mechanisms (Liu et al., 2017b). Multimodal event detection could be useful to capture referential relations as shown in Fig. 2c. Finally, models that represent or encode relations between entities (Lin et al., 2016; Zhang et al., 2019) in a multi-modal text would be an extremely useful tool in our setting. As a step towards processing comparatively large chunks of text, discourse segmentation (Braud et al. (2017), Iruskieta et al. (2019)) splits documents into elementary Discourse Units. Parsing these texts as a discourse is also a topic of ongoing research (Liu et al., 2017a; Li et al., 2016). To the best of our knowledge, research in Vision & Language has hardly been inspired by these classical, entity-centric task in text processing. This general impression is corroborated by the very comprehensive V&L survey of Mogadala"
2021.reinact-1.7,D16-1125,0,0.0236093,"ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-Gordon et al. (2018) tried to generate pragmatically informative captions, by decoding general captioning models, at testing time, to produce captions that discriminate target images from a given set of distractor images. This corresponds more closely to approaches such as the IA, as it takes place over a finite set of symbolic (word) tokens and leaves the literal generation process untouched. In this work we use the methods proposed by Vedantam et al. (2017) and Cohn-Gordon et al. (2018) and adapt them to neural REG. For evaluation, we roughly follow the experim"
2021.reinact-1.7,J03-1003,0,0.163475,"Missing"
2021.reinact-1.7,N18-2070,0,0.309034,"we apply existing decoding strategies from discriminative image captioning to REG and evaluate them in terms of pragmatic informativity, likelihood to groundtruth annotations and linguistic diversity. Our results show general effectiveness, but a relatively small gain in informativity, raising important questions for REG in general. 1 Greedy Beam ESλ0.5 RSAα1.0 traffic light traffic light red light stop light Figure 1: Example from RefCOCO. ES and RSA describe the target (marked green) less ambiguously. Decoding and reasoning methods for discriminative image captioning (Vedantam et al., 2017; Cohn-Gordon et al., 2018) could represent a middle ground in this regard: During inference, predictions from a captioning model are reranked according to pragmatic principles, boosting contextually informative and inhibiting ambiguous utterances. This offers interesting similarities to traditional REG, as it is carried out through explicit algorithms and targets symbolic representations (e.g. word tokens). Discriminative decoding has been shown to be effective for image captioning (Vedantam et al., 2017; Cohn-Gordon et al., 2018; Sch¨uz et al., 2021). In this work, we investigate discriminative decoding for REG, adapt"
2021.reinact-1.7,P89-1009,0,0.446866,"ecoding increases informativity and diversity, although the results are less clear than expected. We attribute this, in part, to the way human annotations are collected, highlighting implications for REG research in general. Introduction In recent years, neural models have become the workhorses for Referring Expression Generation (REG, e.g. Mao et al., 2016; Yu et al., 2016; Zarrieß and Schlangen, 2018), as in other tasks in the Vision and Language (V&L) domain (Mogadala et al., 2019). In REG, this was accompanied by a major shift in how the task was conceptualized. Classical approaches (e.g. Dale, 1989; Dale and Reiter, 1995) mostly investigated rule-based procedures to determine combinations of properties that distinguish target objects from distractors, based on knowledge bases of objects and associated attributes (Krahmer and van Deemter, 2019). Recent work in REG has shifted to more natural settings (e.g. objects in photographs, cf. Figure 1), but at the expense of interpretability: Since continuous representations have replaced knowledge bases as the input, pragmatic processes in neural REG no longer operate on symbolic properties, but are deeply woven into model architectures and trai"
2021.reinact-1.7,D15-1224,0,0.0219878,"sets of attribute-value pairs, which apply to the target, but rule out distractor objects, such as the Incremental Algorithm (IA, Dale and Reiter, 1995). This algorithm iterates over the attribute set in a pre-defined order, selects an attribute if it rules out objects from the set of distractors and terminates when the set is empty. It has been refined, extended and tested in subsequent work (Krahmer et al., 2003; Mitchell et al., 2010; van Deemter et al., 2012; Clarke et al., 2013). In recent years, neural models have enabled REG set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-Gordon et al. (2018) t"
2021.reinact-1.7,D14-1086,0,0.241464,"or finding distinguishing sets of attribute-value pairs, which apply to the target, but rule out distractor objects, such as the Incremental Algorithm (IA, Dale and Reiter, 1995). This algorithm iterates over the attribute set in a pre-defined order, selects an attribute if it rules out objects from the set of distractors and terminates when the set is empty. It has been refined, extended and tested in subsequent work (Krahmer et al., 2003; Mitchell et al., 2010; van Deemter et al., 2012; Clarke et al., 2013). In recent years, neural models have enabled REG set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-"
2021.reinact-1.7,C18-1147,0,0.0387548,"Missing"
2021.reinact-1.7,2020.coling-main.177,0,0.0255399,"l Algorithm (IA, Dale and Reiter, 1995). This algorithm iterates over the attribute set in a pre-defined order, selects an attribute if it rules out objects from the set of distractors and terminates when the set is empty. It has been refined, extended and tested in subsequent work (Krahmer et al., 2003; Mitchell et al., 2010; van Deemter et al., 2012; Clarke et al., 2013). In recent years, neural models have enabled REG set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-Gordon et al. (2018) tried to generate pragmatically informative captions, by decoding general captioning models, at testing time, t"
2021.reinact-1.7,W10-4210,0,0.0751773,"Missing"
2021.reinact-1.7,P16-1058,1,0.828753,"ply to the target, but rule out distractor objects, such as the Incremental Algorithm (IA, Dale and Reiter, 1995). This algorithm iterates over the attribute set in a pre-defined order, selects an attribute if it rules out objects from the set of distractors and terminates when the set is empty. It has been refined, extended and tested in subsequent work (Krahmer et al., 2003; Mitchell et al., 2010; van Deemter et al., 2012; Clarke et al., 2013). In recent years, neural models have enabled REG set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-Gordon et al. (2018) tried to generate pragmatically informative capt"
2021.reinact-1.7,2020.inlg-1.7,0,0.0264842,"ale and Reiter, 1995). This algorithm iterates over the attribute set in a pre-defined order, selects an attribute if it rules out objects from the set of distractors and terminates when the set is empty. It has been refined, extended and tested in subsequent work (Krahmer et al., 2003; Mitchell et al., 2010; van Deemter et al., 2012; Clarke et al., 2013). In recent years, neural models have enabled REG set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016, 2018; Tanaka et al., 2019; Liu et al., 2020; Kim et al., 2020; Panagiaris et al., 2020, 2021), representing scenes with many different types of real-world objects. Most commonly, neural REG models follow the encoderdecoder scheme and are trained end-to-end. Based on low-level visual representations as the input, various aspects of the task are modeled jointly, e.g. lexicalization and content selection. pragmatic adaption as in traditional REG. In image captioning, e.g. Andreas and Klein (2016); Vedantam et al. (2017); Cohn-Gordon et al. (2018) tried to generate pragmatically informative captions, by decoding general captioning models, at testing time, to produce captions that d"
2021.reinact-1.7,W18-6563,1,0.800614,"(2017) and Cohn-Gordon et al. (2018). We compare them to standard greedy and beam search decoding, in terms of informativity, likelihood to ground-truth annotations, and linguistic diversity. We show that discriminative decoding increases informativity and diversity, although the results are less clear than expected. We attribute this, in part, to the way human annotations are collected, highlighting implications for REG research in general. Introduction In recent years, neural models have become the workhorses for Referring Expression Generation (REG, e.g. Mao et al., 2016; Yu et al., 2016; Zarrieß and Schlangen, 2018), as in other tasks in the Vision and Language (V&L) domain (Mogadala et al., 2019). In REG, this was accompanied by a major shift in how the task was conceptualized. Classical approaches (e.g. Dale, 1989; Dale and Reiter, 1995) mostly investigated rule-based procedures to determine combinations of properties that distinguish target objects from distractors, based on knowledge bases of objects and associated attributes (Krahmer and van Deemter, 2019). Recent work in REG has shifted to more natural settings (e.g. objects in photographs, cf. Figure 1), but at the expense of interpretability: Sin"
2021.reinact-1.7,P02-1040,0,0.11088,"th metrics drop if rationality is increased. In most cases, this also applies to RSA. This corresponds to the general findings in Sch¨uz et al. (2021): With higher rationality, ES and RSA generate expressions that deviate further from the model predictions, resulting in lower n-gram overlap. Similarly, the diversity results in Table 2 confirm the findings in Sch¨uz et al. (2021). Discriminative decoding increases TTR (T1 , T2 ) and coverage (cov.), indicating that pragmatic reasoning leads to more variation and the usage of a larger vocabulary. Evaluation Likelihood is measured through BLEU1 (Papineni et al., 2002) and CIDEr (Vedantam et al., 2015) scores, calculated using the RefCOCO API2 . For Diversity, we calculate the type-token ratio (TTR) for unigrams and bigrams, and the proportion of the model vocabulary used (coverage). Importantly, we look at global diversity, i.e. the 1 Results 4.2 Informativity For informativity, ES and RSA outperform greedy and beam search (cf. Table 1, det.), although https://github.com/yufengm/Adaptive https://github.com/lichengunc/refer 49 T1 testA T2 cov. T1 testB T2 cov. T1 testA+ T2 cov. T1 7.0 6.6 29.2 27.8 4.1 3.6 10.6 10.3 46.5 48.6 5.6 5.2 8.7 9.4 28.2 32.7 4.2 4"
2021.reinact-1.7,2021.sigdial-1.43,1,0.862041,"Missing"
2021.sigdial-1.43,D16-1125,0,0.324993,"local diversity, i.e., generating diverse sets of descriptions for individual stimuli (van Miltenburg et al., 2018). Hence, for this group of methods, we focus on the widely used sampling approaches Top-K (Fan et al., 2018) and Nucleus sampling (Holtzman et al., 2020), cf. Section 3.2. Apart from diversity, recent work focused on generating more specific, accurate or detailed, yet (more or less) neutral descriptions (Liu et al., 2018; Dai and Lin, 2017; Luo et al., 2018; Vered et al., 2019). Other works have extended the task to pragmatically informative captioning, given a specific context (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Here, neural captioning models are trained on standard image description datasets and decoded, at testing time, to produce captions that discriminate target images from a given set of distractor images. This setting, which we adopt for our evaluation of pragmatic informativity, is very similar to the Referring Expression Generation (REG) task (Krahmer and van Deemter, 2011; Dale and Reiter, 1995; Yu et al., 2017). In our experiments we use the methods proposed by Vedantam et al. (2017) and Cohn-Gordon et al. (2018) (adapted to word level deco"
2021.sigdial-1.43,N18-2070,0,0.294488,"different contexts, the same types of entities could be described differently, resulting in higher diversity when considering all generated utterances. 411 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 411–422 July 29–31, 2021. ©2021 Association for Computational Linguistics With this in mind, we investigate whether linguistic diversity is triggered by simulating pragmatic objectives during the decoding of neural language models. We use recent approaches from discriminative and pragmatically informative captioning (Vedantam et al., 2017; Cohn-Gordon et al., 2018) that generate unambiguous descriptions of a target image in the context of distractor images and compare them to sampling- and search-based generation. To the best of our knowledge, no detailed comparison has yet been made between decoding strategies maximising diversity on the one and informativity on the other hand. We assess the effect of decoding along three dimensions: (i) likelihood, i.e. overlap with ground-truth captions, (ii) lexical diversity as in van Miltenburg et al. (2018) and (iii) pragmatic informativity measured in terms of the performance of a pre-trained image retrieval mod"
2021.sigdial-1.43,2020.pam-1.14,0,0.465492,"and argue that it should not result from randomness but from principles of intentional and goal-oriented language use, as formulated by e.g. Grice (1975) or Clark (1996). In particular, we hypothesize that linguistic variation in image descriptions should arise as a by-product from reasoning about different ways of referring to objects and scenes in coordination with an interlocutor. This builds upon a long tradition of linguistic research showing that speakers consider the pragmatic informativity of their lexical choices (Brown, 1958; Brennan and Clark, 1996; Grondelaers and Geeraerts, 2003; Coppock et al., 2020). For example, the more specific word “collie” might be preferred over the more common word “dog” when speakers need to unambiguously identify an entity in a context with other, similar entities (Cruse, 1977; Graf et al., 2016). Hence, in different contexts, the same types of entities could be described differently, resulting in higher diversity when considering all generated utterances. 411 Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 411–422 July 29–31, 2021. ©2021 Association for Computational Linguistics With this in mind, we investi"
2021.sigdial-1.43,W16-6636,0,0.0258018,"sufficiently distinguish their target image from others and exhibit human-like strategies for referring (e.g. Dai and Lin, 2017; Luo et al., 2018; Liu et al., 2019; McMahan and Stone, 2020; Takmaz et al., 2020). Diverse outputs are desirable in both open-ended dialogue and more constrained tasks like image captioning (Ippolito et al., 2019), and needed for, e.g., generating entertaining responses in chit-chat dialogues (Li et al., 2016a), responses with certain personality traits (Mairesse and Walker, 2011), or accounting for variation in referring expressions (Viethen and Dale, 2010; Castro Ferreira et al., 2016). In neural image captioning (Bernardi et al., 2016), various approaches have been presented to generate more diverse captions (e.g. Wang et al., 2016; Shetty et al., 2017; Dai et al., 2017; Wang et al., 2017; Li et al., 2018; Lindh et al., 2018; Dai et al., 2018; Chen et al., 2019; Deshpande et al., 2019; Liu et al., 2019; Wang et al., 2020). Ippolito et al. (2019) describe different decoding methods for increasing diversity in image captioning, e.g. Diverse Beam Search (Vijayakumar et al., 2016) or sampling from sets of candidate tokens. Not all methods are applicable in our setting, since t"
2021.sigdial-1.43,P15-2017,0,0.0666636,"Missing"
2021.sigdial-1.43,P18-1082,0,0.39112,"lack of diversity in neural sequence-to-sequence models is often attributed to their standard training and decoding objective, i.e. likelihood, and the corresponding decoding method, i.e. beam search, which seems too biased towards highly probable and generic output (Li et al., 2016b; Vijayakumar et al., 2016; Shao et al., 2017; Kulikov et al., 2019; Holtzman et al., 2020). A commonly adopted solution is to relax the likelihood objective and sample candidate words during decoding, thereby introducing randomness into the generation process at testing time (Wen et al., 2015; Shao et al., 2017; Fan et al., 2018; Ippolito et al., 2019; Holtzman et al., 2020; Wolf et al., 2019; Panagiaris et al., 2021). In this paper, we take a different perspective on diversity and argue that it should not result from randomness but from principles of intentional and goal-oriented language use, as formulated by e.g. Grice (1975) or Clark (1996). In particular, we hypothesize that linguistic variation in image descriptions should arise as a by-product from reasoning about different ways of referring to objects and scenes in coordination with an interlocutor. This builds upon a long tradition of linguistic research sho"
2021.sigdial-1.43,P19-1365,0,0.572406,"2018)’s notion of corpus-level global diversity as “the ability to use (many different combinations of) many different words”. Reproducing the diversity of natural language remains a key challenge in neural generation, despite all progress in recent years. Neural generation systems in various tasks, but most notably in image captioning (Vinyals et al., 2015) and conversation ∗ Work done while at Friedrich Schiller University Jena modeling (Vinyals and Le, 2015) have been found to produce bland, generic and repetitive utterances (Li et al., 2016b; Dai et al., 2017; van Miltenburg et al., 2018; Ippolito et al., 2019). This lack of diversity in neural sequence-to-sequence models is often attributed to their standard training and decoding objective, i.e. likelihood, and the corresponding decoding method, i.e. beam search, which seems too biased towards highly probable and generic output (Li et al., 2016b; Vijayakumar et al., 2016; Shao et al., 2017; Kulikov et al., 2019; Holtzman et al., 2020). A commonly adopted solution is to relax the likelihood objective and sample candidate words during decoding, thereby introducing randomness into the generation process at testing time (Wen et al., 2015; Shao et al.,"
2021.sigdial-1.43,N16-1014,0,0.230204,"diversity in image captioning, following van Miltenburg et al. (2018)’s notion of corpus-level global diversity as “the ability to use (many different combinations of) many different words”. Reproducing the diversity of natural language remains a key challenge in neural generation, despite all progress in recent years. Neural generation systems in various tasks, but most notably in image captioning (Vinyals et al., 2015) and conversation ∗ Work done while at Friedrich Schiller University Jena modeling (Vinyals and Le, 2015) have been found to produce bland, generic and repetitive utterances (Li et al., 2016b; Dai et al., 2017; van Miltenburg et al., 2018; Ippolito et al., 2019). This lack of diversity in neural sequence-to-sequence models is often attributed to their standard training and decoding objective, i.e. likelihood, and the corresponding decoding method, i.e. beam search, which seems too biased towards highly probable and generic output (Li et al., 2016b; Vijayakumar et al., 2016; Shao et al., 2017; Kulikov et al., 2019; Holtzman et al., 2020). A commonly adopted solution is to relax the likelihood objective and sample candidate words during decoding, thereby introducing randomness into"
2021.sigdial-1.43,W18-3024,0,0.0205709,"ptioning, e.g. Diverse Beam Search (Vijayakumar et al., 2016) or sampling from sets of candidate tokens. Not all methods are applicable in our setting, since the authors focus on local diversity, i.e., generating diverse sets of descriptions for individual stimuli (van Miltenburg et al., 2018). Hence, for this group of methods, we focus on the widely used sampling approaches Top-K (Fan et al., 2018) and Nucleus sampling (Holtzman et al., 2020), cf. Section 3.2. Apart from diversity, recent work focused on generating more specific, accurate or detailed, yet (more or less) neutral descriptions (Liu et al., 2018; Dai and Lin, 2017; Luo et al., 2018; Vered et al., 2019). Other works have extended the task to pragmatically informative captioning, given a specific context (Andreas and Klein, 2016; Vedantam et al., 2017; Cohn-Gordon et al., 2018). Here, neural captioning models are trained on standard image description datasets and decoded, at testing time, to produce captions that discriminate target images from a given set of distractor images. This setting, which we adopt for our evaluation of pragmatic informativity, is very similar to the Referring Expression Generation (REG) task (Krahmer and van D"
2021.sigdial-1.43,W19-8609,0,0.0610195,"Missing"
2021.sigdial-1.43,2020.sigdial-1.22,0,0.0359576,"leads to increased diversity. Finally, we show that even simple pragmatic constraints lead to variation which is linguistically plausible. 2 Background Criteria for high-quality and human-like descriptions of images have been discussed much in work on image captioning, pragmatics and dialogue. Besides conformity with ground truth annotations, suggestions include, for example, that descriptions should exhibit human-like diversity, sufficiently distinguish their target image from others and exhibit human-like strategies for referring (e.g. Dai and Lin, 2017; Luo et al., 2018; Liu et al., 2019; McMahan and Stone, 2020; Takmaz et al., 2020). Diverse outputs are desirable in both open-ended dialogue and more constrained tasks like image captioning (Ippolito et al., 2019), and needed for, e.g., generating entertaining responses in chit-chat dialogues (Li et al., 2016a), responses with certain personality traits (Mairesse and Walker, 2011), or accounting for variation in referring expressions (Viethen and Dale, 2010; Castro Ferreira et al., 2016). In neural image captioning (Bernardi et al., 2016), various approaches have been presented to generate more diverse captions (e.g. Wang et al., 2016; Shetty et al.,"
2021.sigdial-1.43,C18-1147,0,0.536557,"Missing"
2021.sigdial-1.43,P02-1040,0,0.113061,"Missing"
2021.sigdial-1.43,D17-1235,0,0.0658215,"Missing"
2021.sigdial-1.43,2020.emnlp-main.353,0,0.0918739,"Missing"
2021.sigdial-1.43,U10-1013,0,0.0667878,"Missing"
2021.sigdial-1.43,D15-1199,0,0.078302,"Missing"
2021.sigdial-1.43,W18-6563,1,0.800396,"ese dimensions will be the basis of our analysis, as reflected in our evaluation criteria (see Section 4). Technically, the decoding methods are very generic and should be compatible with most neural NLG models. 3.1 Likelihood: Greedy and Beam Search Greedy Search At each time step, the word with the highest probability is appended to the output sequence. Search terminates when the end token or the maximal sequence length is reached. Beam Search keeps a fixed number of hypotheses and expands them simultaneously at each step (Graves, 2012). While this method allows for different modifications (Zarrieß and Schlangen, 2018), we use a standard approach: static beam widths, no pruning or length normalization, and terminate if the top candidate has the end token as its final segment or reaches the maximal sequence length. 3.2 Diversity: Nucleus and Top-K sampling We take Nucleus (Holtzman et al., 2020) and Top-K sampling (Fan et al., 2018) as widely used examples of sampling-based methods aimed at increasing diversity. Both strategies are very similar in that they sample from truncated language model distributions, from which the tail of low-probability tokens have been removed that would potentially lead to flawed"
C10-2163,W02-1503,1,0.801455,"tween a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be p"
C10-2163,H01-1035,0,0.0430209,"se study on cross-lingual induction of lexical resources for deep, broad-coverage syntactic analysis of German. We use a parallel corpus to induce a classifier for German participles which can predict their syntactic category. By means of this classifier, we induce a resource of adverbial participles from a huge monolingual corpus of German. We integrate the resource into a German LFG grammar and show that it improves parsing coverage while maintaining accuracy. 1 Introduction Parallel corpora are currently exploited in a wide range of induction scenarios, including projection of morphologic (Yarowsky et al., 2001), syntactic (Hwa et al., 2005) and semantic (Pad´o and Lapata, 2009) resources. In this paper, we use crosslingual data to learn to predict whether a lexical item belongs to a specific syntactic category that cannot easily be learned from monolingual resources. In an application test scenario, we show that this prediction method can be used to obtain a lexical resource that improves deep, grammarbased parsing. The general idea of cross-lingual induction is that linguistic annotations or structures, which are not available or explicit in a given language, can be inferred from another language w"
C10-2163,W10-2106,1,0.214349,"However, this latter perspective has been less prominent in the NLP community so far. This paper investigates a cross-lingual induction method based on an exemplary problem arising in the deep syntactic analysis of German. This showcase is the syntactic flexibility of German participles, being morphologically ambiguous between verbal, adjectival and adverbial readings, and it is instructive for several reasons: first, the phenomenon is a notorious problem for linguistic analysis and annotation of German, such that standard German resources do not represent the underlying analysis. Second, in Zarrieß et al. (2010), we showed that integrating the phenomenon of adverbial participles in a naive way into a broadcoverage grammar of German leads to significant parsing problems, due to spurious ambiguities. Third, it is completely straightforward to detect adverbial participles in cross-lingual data since in other languages, e.g. English or French, adverbs are often morphologically marked. In this paper, we use instances of adverbially translated participles in a parallel corpus to bootstrap a classifier that is able to identify an adverbially used participle based on its monolingual syntactic context. In con"
C10-2163,W02-2018,0,0.0205207,"To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the results from the grammar-based identification, the high accuracy of the classifier suggests th"
C10-2163,P07-1123,0,0.0605871,"Missing"
C10-2163,W96-0213,0,0.182566,"nces from this training set, and then divide it into a set of positive and negative instances. To do this, we use the filtering mechanisms already proposed in Zarrieß et al. (2010). These filters apply on the type level, such that we first identify the positive types (46 total) and then use all instances of these types in the 4891 sentences as positive instances of adverbial participles (1978 instances). The remaining sentences are used as negative instances. For the training of the classifier, we use maximum-entropy classification, which is also commonly used for the general task of tagging (Ratnaparkhi, 1996). In particular, we use the open source TADM tool for parameter estimation (Malouf, 2002). The tags of the words surrounding the participles are used as features in the classification task. We explore different sizes of the context window, where the trigram window is the most succesful (see Table 1). Beyond the trigram window, the results of the classifier start decreasing again, probably because of too many misleading features. Generally, this experiment shows that the grammar-based identification is more precise, but that the classifier still performs surprisingly well. Compared to the resul"
C10-2163,P02-1035,0,0.0239769,". However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule increases the number of timeouts, i.e. sentences that cannot be parsed in a predefined amount of time (20 seconds). Therefore, we observe a drop in parsing accuracy although grammar coverage is improved. As a solution, we induced a lexical resource of adverbial participles based on their adverb"
C10-2163,rohrer-forst-2006-improving,1,0.844851,"t inflected, the surface form of a German participle is ambiguous between a verbal, predicative or adverbial use. 2.1 Participles in the German LFG In order to account for sentences like (1-c), an intuitive approach would be to generally allow for adverb conversion of participles in the grammar. However, in Zarrieß et al. (2010), we show that such a rule can have a strong negative effect on the overall performance of the parsing system, despite the fact that it produces the desired syntactic and semantic analysis for specific sentences. This problem was illustrated using a German LFG grammar (Rohrer and Forst, 2006) constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser and a stochastic disambiguation component which is based on a loglinear probability model (Riezler et al., 2002). In Zarrieß et al. (2010), we found that the naive implementation of adverbial participles in the German LFG, i.e. in terms of a general grammar rule that allows for participles-adverb conversion, leads to spurious ambiguities that mislead the disambiguation component of the grammar. Moreover, the rule in"
D12-1085,P11-2040,0,0.0226499,"Missing"
D12-1085,W11-2832,0,0.241299,"algorithm for tree linearization. We obtain statistically significant improvements on six typologically different languages: English, German, Dutch, Danish, Hungarian, and Czech. 1 (1) a. *It is federal support should try to what achieve b. *It is federal support should try to achieve what c. *It is try to achieve what federal support should Introduction There is a growing interest in language-independent data-driven approaches to natural language generation (NLG). An important subtask of NLG is surface realization, which was recently addressed in the 2011 Shared Task on Surface Realisation (Belz et al., 2011). Here, the input is a linguistic representation, such as a syntactic dependency tree lacking all precedence information, and the task is to determine a natural, coherent linearization of the words. The standard data-driven approach is to traverse the dependency tree deciding locally at each node on the relative order of the head and its children. The shared task results have proven this approach to be both effective and efficient when applied to English. ROOT SBJ OBJ PRD NMOD SBJ VC OPRD IM It is what federal support should try to achieve Figure 1: A non-projective example from the CoNLL 2009"
D12-1085,C10-1012,1,0.879608,"s correspond to the lower and upper bound from the nonlifted and the gold-lifted baseline. It clearly emerges from this figure that the range of improvements obtainable from lifting is closely tied to the general 936 We also evaluated our linearizer on the data of 2011 Shared Task on Surface Realisation, which is based on the English CoNLL 2009 data (like our previous evaluations) but excludes information on morphological realization. For training and evaluation, we used the exact set up of the Shared Task. For the morphological realization, we used the morphological realizer of Bohnet et al. (2010) that predicts the word form using shortest edit scripts. For the language model (LM), we use a 5-gram model with Kneser-Ney (Kneser and Ney, 1995) smoothing derived from 11 million sentences of the Wikipedia. In Table 6, we compare our two linearizers (with and without lifting) to the two top systems of the 2011 Shared Task on Surface Realisation, (Bohnet et al., 2011) and (Guo et al., 2011). Without the lifting, our system reaches a score comparable to the topranked system in the Shared Task. With the lifting, we get a small7 but statistically significant improvement in BLEU such that our sy"
D12-1085,W11-2835,1,0.79302,"method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Imme"
D12-1085,P98-1026,0,0.161246,"Missing"
D12-1085,W06-2920,0,0.0163117,"Missing"
D12-1085,W07-2303,0,0.0304587,"surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on"
D12-1085,P01-1024,0,0.0448278,"tical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization hav"
D12-1085,P07-1041,0,0.0267414,"d linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. Ho"
D12-1085,N09-2057,0,0.133446,"LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002"
D12-1085,C02-1036,0,0.215683,"a and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techniques to lift edges in a preprocessing step to a surface realizer. Their objective is the same as ours: by lifting, they avoid crossing edges. However, contrary to our work, they use phrase-structure syntax and focus on a limited number of cases of crossing branches in German only. 3 Lifting Dependency Edges In this section, we describe the first of the two stages in our approach, namely the classifier that lifts edges in dependency trees. The classifier we aim to train is meant to predict liftings on a given unordered dependency tree, yielding a tree that, wit"
D12-1085,P01-1029,0,0.0320148,"structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et"
D12-1085,W11-2833,0,0.0579172,"Missing"
D12-1085,W09-1201,0,0.070125,"Missing"
D12-1085,P09-1091,0,0.0665074,"rarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machine-learning techni"
D12-1085,P98-1106,0,0.0820264,"eration has addressed certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process tha"
D12-1085,P95-1024,0,0.0216643,"obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG,"
D12-1085,kow-belz-2012-lg,0,0.0286802,"percentage of nonprojective edges in our data sets, which are however important to linearize correctly (see Figure 1). linearizer. In particular, we wanted to check whether the lifting-based linearizer produces more natural word orders for sentences that had a non-projective tree in the corpus, and maybe less natural word orders on originally projective sentences. Therefore, we divided the evaluated items into originally projective and non-projective sentences. We asked four annotators to judge 60 sentence pairs comparing the lifting-based against the nonlifted linearizer using the toolkit by Kow and Belz (2012). All annotators are students, two of them have a background in linguistics. The items were randomly sampled from the subset of the development set containing those sentences where the linearizers produced different surface realizations. The items are subdivided into 30 originally projective and 30 originally non-projective sentences. For each item, we presented the original context sentence from the corpus and the pair of automatically produced linearizations for the current sentence. The annotators had to decide on two criteria: (i) which sentence do they prefer? (ii) how fluent is that sent"
D12-1085,P98-1116,0,0.302823,"tion systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word"
D12-1085,P05-1013,0,0.592937,"certain aspects of the problem. – OA# NK SB OC – Das Mandat will er zur¨ uckgeben . the.ACC mandate.ACC want.3SG he.NOM return.INF . ’He wants to return the mandate.’ Figure 2: German object fronting with complex verb introducing a non-projective edge. In this paper, we aim for a general data-driven approach that can deal with various causes for nonprojectivity and will work for typologically different languages. Our technique is inspired by work in data-driven multilingual parsing, where non-projectivity has received considerable attention. In pseudo-projective parsing (Kahane et al., 1998; Nivre and Nilsson, 2005), the parsing algorithm is restricted to projective structures, but the issue is side-stepped by converting non-projective structures to projective ones prior to training and application, and then restoring the original structure afterwards. Similarly, we split the linearization task in two stages: initially, the input tree is modified by lifting certain edges in such a way that new orderings become possible even under a projectivity constraint; the second stage is the original, projective linearization step. In parsing, projectivization is a deterministic process that lifts edges based on the"
D12-1085,E89-1014,0,0.400442,"onent. This classifier has to be trained on suitable data, and it is an empirical question whether the projective linearizer can take advantage of this preceding lifting step. We present experiments on six languages with varying degrees of non-projective structures: English, German, Dutch, Danish, Czech and Hungarian, which exhibit substantially different word order properties. Our approach achieves significant improvements on all six languages. On German, we also report results of a pilot human evaluation. 929 2 Related Work An important concept for tree linearization are word order domains (Reape, 1989). The domains are bags of words (constituents) that are not allowed to be discontinuous. A straightforward method to obtain the word order domains from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surf"
D12-1085,C04-1097,0,0.282171,"ear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours i"
D12-1085,seeker-kuhn-2012-making,1,0.896576,"Missing"
D12-1085,W11-2834,0,0.0293792,"from dependency trees and to order the words in the tree is to use each word and its children as domain and then to order the domains and contained words recursively. As outlined in the introduction, the direct mapping of syntactic trees to domains does not provide the possibility to obtain all possible correct word orders. Linearization systems can be roughly distinguished as either rule-based or statistical systems. In the 2011 Shared Task on Surface Realisation (Belz et al., 2011), the top performing systems were all statistical dependency realizers (Bohnet et al., 2011; Guo et al., 2011; Stent, 2011). Grammar-based approaches map dependency structures or phrase structures to a tree that represents the linear precedence. These approaches are mostly able to generate non-projective word orders. Early work was nearly exclusively applied to phrase structure grammars (e.g. (Kathol and Pollard, 1995; Rambow and Joshi, 1994; Langkilde and Knight, 1998)). Concerning dependency-based frameworks, Br¨oker (1998) used the concept of word order domains to separate surface realization from linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees)"
D12-1085,vincze-etal-2010-hungarian,0,0.062248,"Missing"
D12-1085,E09-1097,0,0.0255443,"(2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal and postverbal constituents for German focusing on constituent order at the sentence level. The work most similar to ours is that of Gamon et al. (2002). They use machin"
D12-1085,D09-1043,0,0.0262488,"rom linear precedence trees. Similarly, Duchier and Debusmann (2001) differentiate Immediate Dominance trees (ID-trees) from Linear Precedence trees (LPtrees). Gerdes and Kahane (2001) apply a hierarchical topological model for generating German word order. Bohnet (2004) employs graph grammars to map between dependency trees and linear precedence trees represented as hierarchical graphs. In the frameworks of HPSG, LFG, and CCG, a grammarbased generator produces word order candidates that might be non-projective, and a ranker is used to select the best surface realization (Cahill et al., 2007; White and Rajkumar, 2009). Statistical methods for linearization have recently become more popular (Langkilde and Knight, 1998; Ringger et al., 2004; Filippova and Strube, 2009; Wan et al., 2009; He et al., 2009; Bohnet et al., 2010; Guo et al., 2011). They typically work by traversing the syntactic structure either bottom-up (Filippova and Strube, 2007; Bohnet et al., 2010) or topdown (Guo et al., 2011; Bohnet et al., 2011). These linearizers are mostly applied to English and do not deal with non-projective word orders. An exception is Filippova and Strube (2007), who contribute a study on the treatment of preverbal"
D12-1085,C98-1112,0,\N,Missing
D12-1085,C98-1102,0,\N,Missing
D12-1085,C98-1026,0,\N,Missing
D17-1100,D14-1005,0,0.10202,"patially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image,"
D17-1100,P14-1023,0,0.052968,"s as girl and lady in Figure 1 do. Similar to our training procedure for situational embeddings, we now learn 300-dimensional word embeddings that predict occurrences of a word based on co-referential contexts, pairing each word with all words from referring expressions describing the same object. Textual Context (TXT) We learn standard distributional word embeddings from our corpus, ignoring extra-linguistic context. We train a skip-gram model (Mikolov et al., 2013) with negative sampling with window width 5, 300 dimensions. For comparison, we also use the textual word embeddings provided by Baroni et al. (2014), trained on a much larger web corpus (5word context window, 10 negative samples, 400 dimensions). We distinguish the two textual embeddings using the subscripts TXTref , TXTweb . 2.2 Situational Grounding (SIT) Visual Grounding (VIS) Given a set of referring expressions containing the word w and their corresponding referent (oj , rj ), w ∈ rj , we can derive a visual context for the word w by averaging over the visual representations of its referents visj , as proposed for instance by Kiela and Bottou (2014). The visual context of a word can be seen as a ‘visual prototype’. We derive represen"
D17-1100,D15-1242,0,0.0515475,"Missing"
D17-1100,W11-2501,0,0.0929366,"Missing"
D17-1100,P15-2020,0,0.0449432,"Missing"
D17-1100,P12-1015,0,0.110083,"ts denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the regio"
D17-1100,E17-2016,0,0.0453696,"Missing"
D17-1100,Q13-1016,0,0.0318762,"e provides a learner not only with an example of a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1"
D17-1100,N15-1097,0,0.122025,"ous representations for words; in the following, we evaluate them for how well they predict semantic relations. Similarity We evaluate on some similarity data sets, reporting Spearman ρ correlations between human ratings and cosine similarities for word vectors. We use the MEN (Bruni et al., 2012) and 960 Silberer and Lapata (2014)’s data with semantic (SemSim) and visual similarity (VisSim) ratings. Model MEN SemSim VisSim Compat. Hyp.Dir. # pairs 989 2041 2041 4843 334 Compatibility As generic semantic similarity judgements are known to be “fuzzy” (Faruqui et al., 2016), we also evaluate on Kruszewski and Baroni (2015)’s benchmark on semantic compatibility. They define two words as being semantically compatible “if they can potentially refer to the same thing”. We expect our denotational and visual embeddings to be highly useful for this task. We report unsupervised results obtained from cosine similarities between word embeddings. VIS TXT ref DEN SIT DEN k TXT ref 0.404 0.550 0.646 0.470 0.654 0.469 0.584 0.583 0.468 0.632 0.427 0.484 0.491 0.371 0.531 0.241 0.230 0.163 0.134 0.207 78.14 55.69 81.14 59.58 79.94 TXT web 0.799 0.708 0.578 0.262 90.42 Table 1: Word similarity and relatedness evaluation Hypern"
D17-1100,N16-1043,0,0.0407168,"Missing"
D17-1100,W16-2506,0,0.146012,"atedness We now have four different continuous representations for words; in the following, we evaluate them for how well they predict semantic relations. Similarity We evaluate on some similarity data sets, reporting Spearman ρ correlations between human ratings and cosine similarities for word vectors. We use the MEN (Bruni et al., 2012) and 960 Silberer and Lapata (2014)’s data with semantic (SemSim) and visual similarity (VisSim) ratings. Model MEN SemSim VisSim Compat. Hyp.Dir. # pairs 989 2041 2041 4843 334 Compatibility As generic semantic similarity judgements are known to be “fuzzy” (Faruqui et al., 2016), we also evaluate on Kruszewski and Baroni (2015)’s benchmark on semantic compatibility. They define two words as being semantically compatible “if they can potentially refer to the same thing”. We expect our denotational and visual embeddings to be highly useful for this task. We report unsupervised results obtained from cosine similarities between word embeddings. VIS TXT ref DEN SIT DEN k TXT ref 0.404 0.550 0.646 0.470 0.654 0.469 0.584 0.583 0.468 0.632 0.427 0.484 0.491 0.371 0.531 0.241 0.230 0.163 0.134 0.207 78.14 55.69 81.14 59.58 79.94 TXT web 0.799 0.708 0.578 0.262 90.42 Table 1:"
D17-1100,N10-1011,0,0.0347955,"r with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode informa"
D17-1100,N15-1016,0,0.071782,"co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image, the full representation"
D17-1100,P14-2050,0,0.269625,"expressions provide richly structured contexts that go beyond just linking individual expressions with their denotations. As an example consider the scene in Figure 1 depicting several referents and corresponding referring expressions produced by different speakers. This scene provides a learner not only with an example of a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of image"
D17-1100,E17-1016,0,0.0460185,"Missing"
D17-1100,Q14-1006,0,0.649018,"able and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the region. Following (Schlangen et al., 2016), we also add 7 features that encode information about the region relative to the image, the full representation hence is a vector of"
D17-1100,P16-2074,0,0.0298582,"71.64 VIS TXT ref DEN 70.14 68.49 73.67 71.63 71.57 74.32 TXT web 69.16 71.89 Table 4: Accuracies for co-referential expression detection top txtref den vis red text den vis small txtref den vis upper, bototm, bottom, bottem upper, topmost, tippy, above upper, above, of, corner yellow, purple, maroon, blue maroon, redman, reddish, allmiddle and, purple, yellow, pink large, smaller, big, tiny smaller, smallest, little, littiest directly, of, between, slightly Table 5: Top nearest neighbours for some example adjectives embeddings on this task (see previous findings on e.g. predicting antonyms (Nguyen et al., 2016)), the clear advange of denotational over visual embeddings is noteworthy. Whereas visual grounding is relatively effective for modeling compatibility between nouns (see Table 1), it does not seem to capture attribute meaning accurately as illustrated in Table 5. Here, the average of all visual objects referred to as e.g. small seems to be rather noisy and lead to high similarity with rather random words (directly) whereas denotational embeddings model accurate compatibility relations between e.g. small-smaller. Training From R EFER I T, we extract 161K training and 18K test pairs, dividing in"
D17-1100,E17-2012,0,0.0567279,"Missing"
D17-1100,P16-1115,1,0.842473,"a referent for the word lady, it also provides the information that lady can co-refer with girl, and that its denotations can spatially / situationally co-occur with those of table and cake. From these types of information we infer word embeddings, following the method from Levy and Goldberg (2014) for training embeddings on arbiIntroduction Various routes for linking language to extralinguistic context have been explored in recent years. A lot of research has looked at integrating visual representations, either directly (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Yu et al., 2016; Schlangen et al., 2016) or through mapping into a multi-modal distributional space (Feng and Lapata, 2010; Bruni et al., 2012; Kiela and Bottou, 2014; Lazaridou et al., 2015). Young et al. (2014) have explored a less direct link, by representing the extension of phrasal expressions as sets of images, and deriving from this a precise notion of denotational similarity. In very re959 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 959–965 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics a 1024 dimensional representation of the regi"
D17-1100,P14-1068,0,0.0687445,"Missing"
D17-1100,D14-1086,0,\N,Missing
E12-1078,J08-1001,0,0.0278889,"on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to const"
E12-1078,N04-1015,0,0.0247948,"i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in"
E12-1078,E06-1040,0,0.021211,"gests that sentenceinternal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be transl"
E12-1078,W10-1834,0,0.0221106,"Missing"
E12-1078,P09-1092,1,0.943535,"actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course not always the case), one can take advantage of 767 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 767–776, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics these reflexes. This explains in part the fairly high baseline performance of n-gram language models in the surface realization task. And the effect can indeed be taken much further: the discriminative training experiments of Cahill and Riester (2009) show how effective it is to systematically take advantage of asymmetry patterns in the morphosyntactic reflexes of the discourse notion of information status (i.e., using a feature set with well-chosen purely sentence-bound features). These observations give rise to the question: in the light of the difficulty in obtaining reliable discourse information on the one hand and the effectiveness of exploiting the reflexes of discourse in the sentence-internal material on the other – can we nevertheless expect to gain something from adding sentence-external feature information? We propose two scena"
E12-1078,W07-2303,1,0.935476,"pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfe"
E12-1078,P09-2025,1,0.85901,"rnal realisation implicitly carries a lot of imformation about discourse context. On average, the morphosyntactic properties of constituents in a text are better approximates of their discourse status than actual coreference relations. This result feeds into a number of research questions concerning the representation of discourse and its application in generation systems. Although we should certainly not expect a computational model to achieve a perfect accuracy in the constituent ordering task – even humans only agree to a certain extent in rating word order variants (Belz and Reiter, 2006; Cahill, 2009) – the average accuracy in the 60’s for prediction of Vorfeld occupance is still moderate. An obvious direction would be to further investigate more complex representations of discourse that take into account the relations between utterances, such as topic shifts. Moreover, it is not clear whether the effects we find for linearisation in this paper carry over to other levels of generation such as tactical generation where syntactic functions are not fully specified. In a broader perspective, our results underline the need for better formalisations of discourse that can be translated into featu"
E12-1078,P10-1020,0,0.0149304,"ited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentence, the so-called Vorfeld (or “prefield”). Filippova and Strube (2007) show that once the Vorfeld (i.e. the constituent that precedes the finite verb) is correctly determined, the prediction of the order in the Mittelfeld (i.e. the constituents that follow the finite verb) is very easy. Cheung and Penn (2010) extend the approach of Filippova and Strube (2007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of r"
E12-1078,J10-3005,0,0.0482,"Missing"
E12-1078,N09-2057,0,0.0249541,"btained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial posit"
E12-1078,J95-2003,0,0.940218,"ference relations. As we get the same effect in both setups – the sentenceexternal features do not improve over a baseline that captures basic morphosyntactic properties of the constituents – we conclude that sentenceinternal realisation is actually a relatively accurate predictor of discourse context, even more accurate than information that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries i"
E12-1078,J09-1003,0,0.017767,"007) and augment a sentence-internal constituent ordering model with sentence-external features inspired from the entity grid model proposed by Barzilay and Lapata (2008). 2 3 Related Work In the generation literature, most works on exploiting sentence-external discourse information are set in a summarisation or content ordering framework. Barzilay and Lee (2004) propose an account for constraints on topic selection based on probabilistic content models. Barzilay and Lapata (2008) propose an entity grid model which represents the distribution of referents in a discourse for sentence ordering. Karamanis et al. (2009) use Centering-based metrics to assess coherence in an information ordering system. Clarke and LaMotivation While there would be many ways to construe or represent discourse context (e.g. in terms of the global discourse or information structure), we concentrate on capturing local coherence through the distribution of discourse referents in a text. These discourse referents basically correspond to the constituents that our surface realisation model has to put in the right order. As the order of referents or constituents is arguably influenced by the information structure of a sentence given th"
E12-1078,W05-0311,0,0.0198734,"nd Zinsmeister, 2009) have made some good progress towards a coherenceoriented account of at least the left edge of the German clause structure, the Vorfeld constituent. What makes the technological application of theoretical insights even harder is that for most relevant factors, automatic recognition cannot be performed with high accuracy (e.g., a coreference accuracy in the 70’s means there is a good deal of noise) and for the higher-level notions such as the information-structural focus, interannotator agreement on real corpus data tends to be much lower than for core-grammatical notions (Poesio and Artstein, 2005; Ritz et al., 2008). On the other hand, many of the relevant discourse factors are reflected indirectly in properties of the sentence-internal material. Most notably, knowing the shape of referring expressions narrows down many aspects of givenness and salience of its referent; pronominal realizations indicate givenness, and in German there are even two variants of the personal pronoun (er and der) for distinguishing salience. So, if the generation task is set in such a way that the actual lexical choice, including functional categories such as determiners, is fully fixed (which is of course"
E12-1078,J04-3003,0,0.0644019,"Missing"
E12-1078,C04-1097,0,0.267231,"ormation that can be obtained from coreference and lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted t"
E12-1078,ritz-etal-2008-annotation,0,0.025282,"Missing"
E12-1078,rohrer-forst-2006-improving,0,0.0276056,"nt that investigates sentence-external context in a surface realisation task. The sentence-external context is represented in terms of lexical chain features and compared to sentence-internal models which are based on morphosyntactic features. The experiment thus targets a generation scenario where no coreference information is available and aims at assessing whether relatively naive context information is also useful. 4.1 System Description We carry out our first experiment in a regeneration set-up with two components: a) a largescale hand-crafted Lexical Functional Grammar (LFG) for German (Rohrer and Forst, 2006), used to parse and regenerate a corpus sentence, b) a stochastic ranker that selects the most appropriate regenerated sentence in context according to an underlying, linguistically motivated feature model. In contrast to fully statistical linearisation methods, our system first generates the full set of sentences that correspond to the grammatically well-formed realisations of the intermediate syntactic representation.1 This representation is an f-structure, which underspecifies the order of constituents and, to some extent, their morphological realisation, such that the output sentences cont"
E12-1078,2005.mtsummit-papers.15,0,0.019652,"lexical chain relations. pata (2010) have improved a sentence compression system by capturing prominence of phrases or referents in terms of lexical chain information inspired by Morris and Hirst (1991) and Centering (Grosz et al., 1995). In their system, discourse context is represented in terms of hard constraints modelling whether a certain constituent can be deleted or not. In the linearisation or surface realisation domain, there is a considerable body of work approximating information structure in terms of sentence-internal realisation (Ringger et al., 2004; Filippova and Strube, 2009; Velldal and Oepen, 2005; Cahill et al., 2007). Cahill and Riester (2009) improve realisation ranking for German – which mainly deals with word order variation – by representing precedence patterns of constituents in terms of asymmetries in their morphosyntactic properties. As a simple example, a pattern exploited by Cahill and Riester (2009) is the tendency of definite elements tend to precede indefinites, which, on a discourse level, reflects that given entities in a sentence tend to precede new entities. Other work on German surface realisation has highlighted the role of the initial position in the German sentenc"
E12-1078,J91-1002,0,\N,Missing
E17-2014,P14-1132,0,0.0888803,"dictors capturing referential meaning is promising to account for the fact that the negative instances used for training word predictors vary in their degree of semantic similarity to the positive instances of a word. We explored two different ways of integrating this information—by undersampling and by directly predicting similarity—and found the prediction approach to work better, especially for low- and medium-frequent words that have a range of lexically similar neighbors in the model’s vocabulary. In a similar vein, zero-shot learning approaches to object recognition (Frome et al., 2013; Lazaridou et al., 2014; Norouzi et al., 2013) have transferred visual knowledge from known object classes to unknown classes via distributional similarity. Here, we show that visual knowledge can be Where similarities do not help In Table 4, we can see results for words where similarity-based training does not help. For words with more than 50 training instances, distributional similarities degrade performance most for adjectives and words expressing visual attributes (color, shape, location). In these cases, distributional similarities group attributes from the same scale (color or location), but do not account fo"
E17-2014,P14-1023,0,0.0310392,"neural network, “GoogLeNet” (Szegedy et al., 2015), that was trained on data from the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image: the (relative) coordinates of two corners, its (relative) area, distance to the center, and orientation of the image. The full representation hence is a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with 5-word context window, 10 negative samples, 400 dimensions). 4 Individual Words As shown in Table 1, the similarity-based training has a strong positive effect for entry-level nouns, whereas the effect on the overall vocabulary is rather small. This further suggests that distributional similarities improve certain word predictors substantially, whereas others might be affected even negatively. Therefore, in the following, we report average precision for individual words, namely for those cases where similarity-based regression has the strongest positive or negative effect as com"
E17-2014,P16-2074,0,0.155292,"tween object names. We find that the latter, similarity-based training method leads to substantial improvements for particular words such as entry-level nouns or hypernyms, whereas predictors for other words such as adjectives do not benefit from distributional knowledge. These results suggest that, in principle, semantic relatedness might be promising knowledge source for training more accurate visual models of referential word use, but it also supports recent findings showing that distributional models do not capture all aspects of semantic relatedness equally well (Rubinstein et al., 2015; Nguyen et al., 2016). Someone who knows the meaning of the word child will most probably know a) how to distinguish children from other entities in the real world and b) that child is related to other words, such as girl, boy, mother, etc. Traditionally, these two aspects of lexical meaning—which, following (Marconi, 1997), we may call referential and inferential, respectively—have been modeled in quite distinct settings. Semantic similarity has been a primary concern for distributional models of word meaning that treat words as vectors which are aggregated over their contexts, cf. (Turney and Pantel, 2010; Erk,"
E17-2014,D14-1086,0,0.212512,"ve been referred to by very similar words. (E.g., undersampling boy instances as negative instances for the child classifier.) This should allow the word classifier to focus on visual distinctions between objects that are semantically more important. When compiling the training set of a WAP - NOSIM classifier for word w, we look at its 10 most similar words in the vocabulary according to a distributional model (trained with word2vec, see below) and remove their instances from the set of negative instances ¬w. Data As training data, we use the training split of the REFERIT corpus collected by (Kazemzadeh et al., 2014), which is based on the medium-sized SAIAPR image collection (Grubinger et al., 2006) (99.5k image regions). For testing, we use the training section of REFCOCO corpus collected by (Yu et al., 2016), which is based on the MSCOCO collection (Lin et al., 2014) containing over 300k images with object segmentations. This gives us a large enough test set to make stable predictions about the quality of individual word predictors, which often only have a few positive instances in the test set of the REFERIT corpus. We follow (Schlangen et al., 2016) and select words with a minimum frequency of 40 in"
E17-2014,P15-2119,0,0.0199925,"ibutional similarities between object names. We find that the latter, similarity-based training method leads to substantial improvements for particular words such as entry-level nouns or hypernyms, whereas predictors for other words such as adjectives do not benefit from distributional knowledge. These results suggest that, in principle, semantic relatedness might be promising knowledge source for training more accurate visual models of referential word use, but it also supports recent findings showing that distributional models do not capture all aspects of semantic relatedness equally well (Rubinstein et al., 2015; Nguyen et al., 2016). Someone who knows the meaning of the word child will most probably know a) how to distinguish children from other entities in the real world and b) that child is related to other words, such as girl, boy, mother, etc. Traditionally, these two aspects of lexical meaning—which, following (Marconi, 1997), we may call referential and inferential, respectively—have been modeled in quite distinct settings. Semantic similarity has been a primary concern for distributional models of word meaning that treat words as vectors which are aggregated over their contexts, cf. (Turney a"
E17-2014,P16-1115,1,0.721325,"hat identify objects in images, by exploiting distributional similarity information during training. We show that for certain words (such as entry-level nouns or hypernyms), we can indeed learn better referential word meanings by taking into account their semantic similarity to other words. For other words, there is no or even a detrimental effect, compared to a learning setup that presents even semantically related objects as negative instances. 1 This paper extends upon recent work on learning models of referential word use on large-scale corpora of images paired with referring expressions (Schlangen et al., 2016). As in previous approaches in HRI, that work treats words during training and application as independent predictors, with no relations between them. Our starting assumption here is that this misses potentially useful information: e.g., that the costs for confusing referents of child vs. boy should be much lower than for confusing referents of child vs. car. We thus investigate whether knowledge about semantic similarities between words can be exploited to learn more accurate visual word predictors, accounting for this intuition that certain visual object distinctions are semantically more imp"
E17-2014,P16-1058,1,0.922997,"sifiers are trained with logistic regression (using `1 penalty). (This is the (Schlangen et al., 2016) model.) 3 Experimental Set-up We focus on assessing to what extent similaritybased visual word predictors capture the referential meaning of a word in a more accurate way, and distinguish its potential referents from other random objects. To factor out effects of compositionality and context that arise in reference generation or resolution, we measure how well a predictor for a word w is able to retrieve from a sampled test set objects that have been referred to by w (Schlangen et al., 2016; Zarrieß and Schlangen, 2016a) evaluate on full referring expressions). Undersampling similar objects (WAP - NOSIM) As discussed above, it is intuitive to assume that a visual classifier that distinguishes referents of a word from other objects in an image should be less penalized for making errors on objects that are categorically related. For instance, the classifier for child should be less penalized for giving high probabilities to referents of boy than to referents of car. A straightforward way to introduce these differences during training is by undersampling negative instances that have been referred to by very si"
E17-2014,W16-6642,1,0.928942,"sifiers are trained with logistic regression (using `1 penalty). (This is the (Schlangen et al., 2016) model.) 3 Experimental Set-up We focus on assessing to what extent similaritybased visual word predictors capture the referential meaning of a word in a more accurate way, and distinguish its potential referents from other random objects. To factor out effects of compositionality and context that arise in reference generation or resolution, we measure how well a predictor for a word w is able to retrieve from a sampled test set objects that have been referred to by w (Schlangen et al., 2016; Zarrieß and Schlangen, 2016a) evaluate on full referring expressions). Undersampling similar objects (WAP - NOSIM) As discussed above, it is intuitive to assume that a visual classifier that distinguishes referents of a word from other objects in an image should be less penalized for making errors on objects that are categorically related. For instance, the classifier for child should be less penalized for giving high probabilities to referents of boy than to referents of car. A straightforward way to introduce these differences during training is by undersampling negative instances that have been referred to by very si"
L16-1019,C14-1189,0,0.0257379,"ional database that can be easily processed and queried across the different experimental settings in PentoRef. 2. Related Work Compared to other resources used in dialogue research, PentoRef follows a tradition perhaps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, conseq"
L16-1019,D15-1224,0,0.0148599,"xt. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental settings that have been invest"
L16-1019,W10-4302,1,0.760804,"erring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not"
L16-1019,D14-1086,0,0.0700623,"and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects of the different domains on the reference task (Gkatzia et al., 2015), the comparability of existing reference corpora is limited as they are based on very different types of visual stimuli. PentoRef provides an unusually wide spectrum of experimental setting"
L16-1019,P15-1029,1,0.854429,"is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for systematic studies of referring"
L16-1019,W13-4030,1,0.855465,"ration (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the sub-corpora could not be directly exploited for"
L16-1019,koolen-krahmer-2010-tuna,0,0.0962069,"aps best exemplified by the HCRC Map Task Corpus (Anderson et al., 1991; MacMahon et al., 2006) in that it combines the naturalness of unscripted conversation with the advantages of taskoriented dialogue, such as careful control over aspects of the linguistic and extralinguistic context. Recent comparable data collection efforts are relatively rare, but see (Tokunaga et al., 2012; Gatt and Paggio, 2014). Related studies in REG research showed that the linguistic phenomena found in the elicited referring expressions vary widely with the modality, task, and audience, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013). Inspired by a recently increasing interest in image description and labelling tasks, data sets of real-world photographs (paired with references to specific entities in the image) have also been created for REG (Kazemzadeh et al., 2014; Gkatzia et al., 2015). Real-world images pose interesting challenges for REG, as the set of visual attributes and, consequently, the distractor objects (objects present in the scene which are not the target of a referring expression) cannot be directly controlled. 125 Although attempts have been made to systematically assess the effects"
L16-1019,W10-4210,0,0.0784762,"Missing"
L16-1019,W09-3905,1,0.796519,"resolution (RR) and referring expression generation (REG). The corpus is a meta-collection that bundles up a range of experimental data collected over recent years in the Dialogue Systems Group, first at Potsdam University and then Bielefeld University, and by collaborators. The individual sub-corpora have been used for empirical studies of conversational behaviour in spoken language interaction as well as work on building statistical reference resolution systems in situated environments, in German and English (Fern´andez et al., 2006; Schlangen and Fern´andez, 2007; Fern´andez et al., 2007; Schlangen et al., 2009; Heintze et al., 2010; Kennington et al., 2013; Kennington and Schlangen, 2015). The common property of the experiments in this collection is that participants have to produce spoken referring expressions to puzzle pieces in a game, normally to instruct another player to carry out a certain move on the Pentomino game board. At the same time, some important parameters of the respective experimental settings were manipulated, such as the way communication was mediated (speech channel and/or visual channel), and the presentation of the scene (virtual or real-world). The original versions of the"
L16-1019,tokunaga-etal-2012-rex,0,0.141485,"Missing"
P11-1101,W10-4201,0,0.0187591,"no positive effect on the voice and precedence accuracy. The n-best evaluations even suggest that the LM scores negatively impact the ranker: the accuracy for the top 3 sentences increases much less as compared to the model that does not integrate LM scores.6 The n-best performance of a realisation ranker is practically relevant for re-ranking applications such as Velldal (2008). We think that it is also conceptually interesting. Previous evaluation studies suggest that the original corpus sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not"
P11-1101,W10-4237,0,0.0289516,"Missing"
P11-1101,W05-1601,0,0.202581,"ecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007)."
P11-1101,C10-1012,0,0.0336633,"in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these annotations are not suitable for full generation since they are often incomplete. Thus, it is not clear to which degree these annotations are actually underspecified for certain paraphrases. 2.2 Linguistic Background In competition-based linguistic theories (Optimality Theory and related frameworks), the use of argument alternations is construed as an effect of markedness hierarchies (Aissen, 1999; Aissen, 2003). Argument functions (subject, object, . . . ) on the one hand and th"
P11-1101,P09-1092,1,0.897952,"grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalen"
P11-1101,W07-2303,1,0.906034,"rube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However, they note that these"
P11-1101,D09-1046,0,0.0256955,"Missing"
P11-1101,P07-1041,0,0.351701,"on the person scale than the agent, but an active is grammatically possible. Bresnan et al. (2007) correlate the use of the English dative alternation to a number of features such as givenness, pronominalisation, definiteness, constituent length, animacy of the involved verb arguments. These features are assumed to reflect the discourse acessibility of the arguments. Interestingly, the properties that have been used to model argument alternations in strict word order languages like English have been identified as factors that influence word order in free word order languages like German, see Filippova and Strube (2007) for a number of pointers. Cahill and Riester (2009) implement a model for German word order variation that approximates the information status of constituents through morphological features like definiteness, pronominalisation etc. We are not aware of any corpus-based generation studies investigating how these properties relate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a cor"
P11-1101,N09-2057,0,0.0127313,"valuation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related"
P11-1101,W07-1203,0,0.135806,"g-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use o"
P11-1101,P10-1160,0,0.0208723,"ntation for Sentence (4) in Figure 2, the realiser will not generate an active realisation since the agent role cannot be instantiated by any phrase in the grammar. However, depending on the exact context there are typically options for realising the subject phrase in an active with very little descriptive content. Ideally, one would like to account for these phenomena in a meaning representation that underspecifies the lexicalisation of discourse referents, and also captures the reference of implicit arguments. Especially the latter task has hardly been addressed in NLP applications (but see Gerber and Chai (2010)). In order to work around that problem, we implemented some simple heuristics which underspecify the realisation of certain verb arguments. These rules define: 1. a set of pronouns (generic and neutral pronouns, universal quantifiers) that correspond to “trivial” agents in active and implicit agents 1010 in passive sentences; 2. a set of prepositional adjuncts in passive sentences that correspond to subjects in active sentence (e.g. causative and instrumental prepositions like durch “by means of”); 3. certain syntactic contexts where special underspecification devices are needed, e.g. coordin"
P11-1101,J95-2003,0,0.192951,"nt” features (ScalAl.): combinations of voice and role properties with morphological properties, e.g. “subject is singular”, “agent is 3rd person in active voice” (these are surface-independent, identical for each alternation candidate). The model for which we present our results is based on sentence-internal features only; as Cahill and Riester (2009) showed, these feature carry a considerable amount of implicit information about the discourse context (e.g. in the shape of referring expressions). We also implemented a set of explicitly inter-sentential features, inspired by Centering Theory (Grosz et al., 1995). This model did not improve over the intra-sentential model. Evaluation Measures In order to assess the general quality of our generation ranking models, we 4 The language model is trained on the German data release for the 2009 ACL Workshop on Machine Translation shared task, 11,991,277 total sentences. LM Ling. Model Match BLEU NIST Match BLEU NIST SEMn 68.2 10.72 15.04 0.68 12.95 27.66 0.759 13.14 SEMh 75.8 7.28 11.89 0.65 12.69 26.38 0.747 13.01 Table 2: Evaluation of Experiment 1 use several standard measures: a) exact match: how often does the model select the original corpus sentence,"
P11-1101,P98-1116,0,0.0627281,"king the risk of occasional overgeneration. The paper is structured as follows: Section 2 situates our methodology with respect to other work on surface realisation and briefly summarises the relevant theoretical linguistic background. In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich lingui"
P11-1101,W05-1510,0,0.0227506,"sentence is not always the only optimal realisation of a given linguistic input (Cahill and Forst, 2010; Belz and Kow, 2010). Humans seem to have varying preferences for word order contrasts in certain contexts. The n-best evaluation could reflect the behaviour of a ranking model with respect to the range of variations encountered in real discourse. The pilot human evaluation in the next Section deals with this question. 6 Human Evaluation Our experiment in Section 5.3 has shown that the accuracy of our linguistically informed ranking model dramatically increases when we consider the three 6 (Nakanishi et al., 2005) also note a negative effect of including LM scores in their model, pointing out that the LM was not trained on enough data. The corpus used for training our LM might also have been too small or distinct in genre. 1014 best sentences rather than only the top-ranked sentence. This means that the model sometimes predicts almost equal naturalness for different voice realisations. Moreover, in the case of word order, we know from previous evaluation studies, that humans sometimes prefer different realisations than the original corpus sentences. This Section investigates agreement in human judgemen"
P11-1101,A00-2026,0,0.142822,"In Section 3, we present our generation architecture and the design of the input representation. Section 4 describes the setup for the experiments in Section 5. In Section 1008 6, we present the results from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal"
P11-1101,P00-1061,1,0.519387,"rd lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not direc"
P11-1101,P02-1035,0,0.0210818,"e realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In mult"
P11-1101,C04-1097,0,0.120095,"sults from the human evaluation. 2 Related Work 2.1 Generation Background The first widely known data-driven approach to surface realisation, or tactical generation, (Langkilde and Knight, 1998) used language-model ngram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work"
P11-1101,rohrer-forst-2006-improving,0,0.198238,"elate to argument alternations in free word order languages. 3 Generation Architecture Our data-driven methodology for investigating factors relevant to surface realisation uses a regeneration set-up2 with two main components: a) a grammar-based component used to parse a corpus sentence and map it to all its meaning-equivalent surface realisations, b) a statistical ranking component used to select the correct, i.e. contextually most appropriate surface realisation. Two variants of this set-up that we use are sketched in Figure 1. We generally use a hand-crafted, broad-coverage LFG for German (Rohrer and Forst, 2006) to parse a corpus sentence into a f(unctional) structure3 and generate all surface realisations from a given 2 Compare the bidirectional competition set-up in some Optimality-Theoretic work, e.g., (Kuhn, 2003). 3 The choice among alternative f-structures is done with a discriminative model (Forst, 2007). 1009 Sntx Snty SVM Ranker SVM Ranker Snta1 Snta2 ... Sntam LFG grammar Sntb1 Snta1 Snta2 ... Sntbn LFG Grammar FSa FSb Reverse Sem. Rules SEM FSa LFG grammar Sem. Rules FS1 LFG Grammar Snti Snti Figure 1: Generation pipelines f-structure, following the generation approach of Cahill et al. (20"
P11-1101,W06-1661,0,0.236129,"i, 2000; Marciniak and Strube, 2005; Belz, 2005, a.o.). Work on data-driven approaches has led to insights into the importance of linguistic features for sentence linearisation decisions (Ringger et al., 2004; Filippova and Strube, 2009). The availability of discriminative learning techniques for the ranking of candidate analyses output by broad-coverage grammars with rich linguistic representations, originally in parsing (Riezler et al., 2000; Riezler et al., 2002), has also led to a revival of interest in linguistically sophisticated reversible grammars as the basis for surface realisation (Velldal and Oepen, 2006; Cahill et al., 2007). The grammar generates candidate analyses for an underlying representation and the ranker’s task is to predict the contextually appropriate realisation. The work that is most closely related to ours is Velldal (2008). He uses an MRS representation derived by an HPSG grammar that can be underspecified for information status. In his case, the underspecification is encoded in the grammar and not directly controlled. In multilingually oriented linearisation work, Bohnet et al. (2010) generate from semantic corpus annotations included in the CoNLL’09 shared task data. However"
P11-1101,W09-2602,1,0.842156,"oach of Cahill et al. (2007). F-structures are attributevalue matrices representing grammatical functions and morphosyntactic features; their theoretical motivation lies in the abstraction over details of surface realisation. The grammar is implemented in the XLE framework (Crouch et al., 2006), which allows for reversible use of the same declarative grammar in the parsing and generation direction. To obtain a more abstract underlying representation (in the pipeline on the right-hand side of Figure 1), the present work uses an additional semantic construction component (Crouch and King, 2006; Zarrieß, 2009) to map LFG f-structures to meaning representations. For the reverse direction, the meaning representations are mapped to f-structures which can then be mapped to surface strings by the XLE generator (Zarrieß and Kuhn, 2010). For the final realisation ranking step in both pipelines, we used SVMrank, a Support Vector Machine-based learning tool (Joachims, 1996). The ranking step is thus technically independent from the LFG-based component. However, the grammar is used to produce the training data, pairs of corpus sentences and the possible alternations. The two pipelines allow us to vary the de"
P11-1101,C98-1112,0,\N,Missing
P13-1152,W10-4226,0,0.214049,"off (1992) mentions Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. b. *John homed him with an order. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs whi"
P13-1152,W07-2302,0,0.0189498,"lippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical meth"
P13-1152,W11-2832,0,0.265471,"Example (1) where the sentence planning component needs to have access to the lexical knowledge that “order” and not “home” can be realized as a verb in English. (1) a. b. *John homed him with an order. John ordered him home. In recent data-driven generation research, the focus has somewhat shifted from full data-to-text systems to approaches that isolate well-defined subproblems from the NLG pipeline. In particular, the tasks of surface realization and referring expression generation (REG) have received increasing attention using a number of available annotated data sets (Belz and Kow, 2010; Belz et al., 2011). While these single-task approaches have given rise to many insights about algorithms and corpus-based modelling for specific phenomena, they can hardly deal with aspects of the architecture and interaction between generation levels. This paper suggests a middle ground between full data-to-text and single-task generation, combining two well-studied NLG problems. We integrate a discourse-level approach to REG with sentence-level surface realization in a data-driven framework. We address this integrated task with a set of components that can be trained on flexible inputs which allows us to syst"
P13-1152,W05-1601,0,0.0230272,"Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG researc"
P13-1152,C10-1012,0,0.0218968,"nment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described"
P13-1152,W11-2835,0,0.025502,"alysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments co"
P13-1152,D12-1085,1,0.883203,"Missing"
P13-1152,C10-1011,0,0.0710366,"threatened the two men p:0   dem Ehemann v:1 , ihn v:1 zusammenzuschlagen. beat up. him  the husband    Er v:1 gab deshalb seine v:1 Brieftasche ohne Gegenwehrag:v,the:p heraus. gave therefore his wallet without resistanceag:v,the:p out. He    Anschließend nahmen ihm v:1 die R¨auber p:0 noch die Armbanduhrposs:v ab und fl¨uchtetenag:p . Afterwards took also the watchposs:v off and fleedag:p . him  the robbers Figure 2: Example text with RE annotations, oval boxes mark victim mentions, square boxes mark perp mentions, heads of implicit arguments are underlined the Bohnet (2010) dependency parser to obtain an automatic annotation of shallow or surface dependencies for the corpus sentences. The deep syntactic dependencies are derived from the shallow layer by a set of hand-written transformation rules. The goal is to link referents to their main predicate in a uniform way, independently of the surface-syntactic realization of the verb. We address passives, nominalizations and possessives corresponding to the contexts where we annotated implicit referents (see above). The transformations are defined as follows: 1. remove auxiliary nodes, verb morphology and finiteness,"
P13-1152,P09-1092,0,0.0280708,"egenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually ap"
P13-1152,P89-1009,0,0.315072,"ights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents ha"
P13-1152,P84-1107,0,0.138315,"and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et"
P13-1152,P07-1041,0,0.018174,"the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) w"
P13-1152,P10-1160,0,0.0256792,"line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level gene"
P13-1152,J95-2003,0,0.12689,"alians]p on [a young man]v . [Two italians]p are on trial because of an attack on [a young man]v . Sentence (2-a) is incoherent because the syntactic surface obscurs the intended meaning that “two italians” and “the two men” refer to the same referent. In order to generate the natural Sentence (2-b), the RE component needs information about linear precedence of the two perp instances and the nominalization of “attack”. These types of interactions between referential and syntactic realization have been thoroughly discussed in theoretical accounts of textual coherence, as e.g. Centering Theory (Grosz et al., 1995). The integrated modelling of REG and surface realization leads to a considerable expansion of the choice space. In a sentence with 3 referents that each have 10 RE candidates and can be freely ordered, the number of surface realizations increases from 6 to 6·103 , assuming that the remaining words can not be syntactically varied. Thus, even when the generation problem is restricted to these tasks, a fully integrated architecture faces scalability issues on realistic corpus data. In this work, we assume a modular set-up of the generation system that allows for a flexible ordering of the single"
P13-1152,P88-1020,0,0.263536,"rameters of the generation architecture: 1) the sequential order of the modules, 2) parallelization of modules, 3) joint vs. separate modelling of implicit referents. Our results suggest that the interactions between RE and syntax can be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For rea"
P13-1152,P98-1116,0,0.120748,"medt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphos"
P13-1152,W05-0618,0,0.187792,"ations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropria"
P13-1152,A00-1017,0,0.059807,"n be modelled in sequential generation architecture where the RE component has access to information about syntactic realization and an approximative, intermediate linearization. Such a system is reminiscent of earlier work in rulebased generation that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and"
P13-1152,A00-2026,0,0.0370846,"ion (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body o"
P13-1152,W94-0319,0,0.166386,"terleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000"
P13-1152,C04-1097,0,0.0334446,"r surface realization, the standard set-up is to regenerate from syntactic representations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004)"
P13-1152,S12-1030,0,0.0118512,"about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportcaster text. Both address a restricted domain where a direct alignment between units in the non-linguistic representation and the linguistic ut"
P13-1152,W09-2417,0,0.0358082,"Missing"
P13-1152,seeker-kuhn-2012-making,1,0.802563,"n 3.2). (4) a. b. (x,lemma,VV,y) → (x,lemma,VVFIN,y) (x,¨uberfallen/attack,VV,y) → (x,bei/at,PREP,y), ¨ (z,Uberfall/attack,NN,x),(q,der/the,ART,z) The baseline for the verb transformation component is a two-step procedure: 1) pick a lexical4.1.3 LIN: Linearization For linearization, we use the state-of-the-art dependency linearizer described in Bohnet et al. (2012). We train the linearizer on an automatically parsed version of the German TIGER treebank (Brants et al., 2002). This version was produced with the dependency parser by Bohnet (2010), trained on the dependency conversion of TIGER by Seeker and Kuhn (2012). 4.2 Architectures Depending on the way the generation components are combined in an architecture, they will have access to different layers of the input representation. The following definitions of architectures recur to the layers introduced in Section 3.3. 4.2.1 First Pipeline The first pipeline corresponds most closely to a standard generation pipeline in the sense of (Reiter and Dale, 1997). REG is carried out prior to surface realization such that the RE component does not have access to surface syntax or word order whereas the SYN component has access to fully specified RE slots. 1551"
P13-1152,P04-1052,0,0.0926647,"Missing"
P13-1152,E12-2021,0,0.0583566,"Missing"
P13-1152,W12-1506,0,0.0114807,"n the non-linguistic representation and the linguistic utterance can be learned. Marciniak and Strube (2005) propose an ILP model for global optimization in a generation task that is decomposed into a set of classifiers. Bohnet et al. (2011) deal with multi-level generation in a statistical framework and in a less restricted domain. They adopt a standard sequential pipeline approach. Recent corpus-based generation approaches faced the problem that existing standard treebank representations for parsing or other analysis tasks do not necessarily fit the needs of generation (Bohnet et al., 2010; Wanner et al., 2012). Zarrieß et al. (2011) discuss the problem of an input representation that is appropriately underspecified for the realistic generation of voice alternations. 3 The Data Set The data set for our generation experiments consists of 200 newspaper articles about robbery events. The articles were extracted from a large German newspaper corpus. A complete example text with RE annotations is given in Figure 2, Table 1 summarizes some data set statistics. 3.1 RE annotation The RE annotations mark explicit and implicit mentions of referents involved in the robbery event described in an article. Explic"
P13-1152,W94-0316,0,0.050514,"tion that implements an interactive or revision-based feedback between discourse-level planning and linguistic realisation (Hovy, 1988; Robin, 1993). 2 Related Work Despite the common view of NLG as a pipeline process, it is a well-known problem that highlevel, conceptual knowledge and low-level linguistic knowledge are tightly interleaved (Danlos, 1984; Mellish et al., 2000). In rule-based, strictly sequential generators these interactions can lead to a so-called generation gap, where a downstream module cannot realize a text or sentence plan generated by the preceding modules (Meteer, 1991; Wanner, 1994). For this reason, a number of other architectures has been proposed, see De Smedt et al. (1996) for an overview. For reasons of tractability and scalability, many practical NLG systems still have been designed as sequential pipelines that follow the basic layout of macroplanning-microplanning-linguistic realization (Reiter, 1994; Cahill et al., 1999; Bateman and Zock, 2003). In recent data-driven research on NLG, many single tasks have been addressed with corpusbased methods. For surface realization, the standard set-up is to regenerate from syntactic representations that have been produced f"
P13-1152,P11-1101,1,0.876126,"ic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instances of a referent in a text. Belz and Varges (2007)’s GREC data set includes annotations of implicit subjects in coordinations. Zarrieß et al. (2011) deal with implicit subjects in passives, proposing a set of heuristics for adding these agents to the generation input. Roth and Frank (2012) acquire automatic annotations of implicit roles for the purpose of studying coherence patterns in texts. Implicit referents have also received attention for the analysis of semantic roles (Gerber and Chai, 2010; Ruppenhofer et al., 2010). Statistical methods for data-to-text generation have been explored only recently. Belz (2008) trains a probabilistic CFG to generate weather forecasts, Chen et al. (2010) induce a synchronous grammar to generate sportc"
P13-1152,E12-1078,1,0.848057,"resentations that have been produced for realistic corpus sentences. The first widely known statistical approach by Langkilde and Knight (1998) used language-model n-gram statistics on a word lattice of candidate realisations to guide a ranker. Subsequent work explored ways of exploiting linguistically annotated data for trainable generation models (Ratnaparkhi, 2000; Belz, 2005). Work on data-driven approaches has led to insights about the importance of linguistic features for sentence 1548 linearization decisions (Ringger et al., 2004; Filippova and Strube, 2007; Cahill and Riester, 2009). (Zarrieß et al., 2012) have recently argued that the good performance of these linguistically motivated word order models, which exploit morphosyntactic features of noun phrases (i.e. referents), is related to the fact that these morphosyntactic features implicitly encode a lot of knowledge about the underlying discourse or information structure. A considerable body of REG research has been done in the paradigm established by Dale (1989; 1995). More closely related to our work are approaches in the line of Siddarthan and Copestake (2004) or Belz and Varges (2007) who generate contextually appropriate REs for instan"
P13-1152,W09-0629,0,\N,Missing
P13-1152,S10-1008,0,\N,Missing
P13-1152,C98-1112,0,\N,Missing
P16-1058,J95-3003,0,0.0639703,"., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995; DeVault et al., 2005). Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can tailor, extend, adapt, revise or correct her referring expressions in case the listener signals that he did not understand. Thus, human speakers can flexAs our second contribution, we extend our probabilistic word selection model to work in a simple interactive installment component that tries to avoid semantically inadequate words as much as possible and only expands the expression in case of misunder"
P16-1058,D14-1086,0,0.0919433,"semantically appropriate expressions. In a human evaluation, we observe that users are sensitive to inadequate object names - which unfortunately are not unlikely to be generated from low-level visual input. We propose a solution inspired from human task-oriented interaction and implement strategies for avoiding and repairing semantically inaccurate words. We enhance a word-based REG with contextaware, referential installments and find that they substantially improve the referential success of the system. 1 girl in front anywhere brown Figure 1: Example images and REs from the ReferIt corpus (Kazemzadeh et al., 2014) other objects in the scene but does not overload the listener with unnecessary information. Figure 1 illustrates this with two examples from a corpus of REs collected from human subjects for objects in images (Kazemzadeh et al., 2014). Research on referring expression generation (REG) has mostly focussed on (ii), modeling pragmatic adequacy in attribute selection tasks, using as input a fully specified, symbolic representation of the visual attributes of an object and its distractors in a scene (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012). In this paper, we follow a more recent tren"
P16-1058,P06-1131,0,0.0389732,"as much as possible and only expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with high confidence. Our work is"
P16-1058,P15-1029,1,0.785423,"rties of the target referent, and (ii) is pragmatically and contextually appropriate, i.e. distinguishes the target from 610 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 610–620, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics rithms like (Dale and Reiter, 1995). We find that a context-aware installment strategy greatly improves referential success as it helps to avoid and repair misunderstandings and offers a combined treatment of semantic and pragmatic adequacy. the low-level visual features of an object (Kennington and Schlangen, 2015). As our first contribution, we train this model on the ReferIt corpus (Kazemzadeh et al., 2014) and define decoding mechanisms tailored to REG. Large-scale recognition of objects and their attributes in images is still a non-trivial task. Consequently, REG systems now face the challenge of dealing with semantically inadequate expressions. For instance, in Figure 1, the system might not precisely distinguish between man or woman and generate an inadequate, confusing RE like man in the middle. Therefore, we focus on evaluating our system in an object identification task with users, in contrast"
P16-1058,P05-3001,0,0.0997705,"partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995; DeVault et al., 2005). Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can tailor, extend, adapt, revise or cor"
P16-1058,P15-2017,0,0.0537773,"Van Deemter, 2012). In this paper, we follow a more recent trend (Kazemzadeh et al., 2014; Gkatzia et al., 2015) and investigate REG on real-world images. In this setting, a low-level visual representation of an image (a scene) segmented into regions (objects), including the region of the target referent, constitutes the input. This task is closely related to the recently very active field of image-to-text generation, where deep learning approaches have been used to directly map low-level visual input to natural language sentences, e.g. (Vinyals et al., 2015; Chen and Lawrence Zitnick, 2015; Devlin et al., 2015). Similarly, we propose to cast REG on images as a word selection task. Thus, we base this work on a model of perceptually grounded word meaning, which associates words with classifiers that predict their semantic appropriateness given Introduction A speaker who wants to refer to an object in a visual scene will try to produce a referring expression (RE) that (i) is semantically adequate, i.e. accurately describes the visual properties of the target referent, and (ii) is pragmatically and contextually appropriate, i.e. distinguishes the target from 610 Proceedings of the 54th Annual Meeting of"
P16-1058,koolen-krahmer-2010-tuna,0,0.0245709,"e object types and, consequently, about potential distractors of the target. This does not apply to REG on real-world images which, as we will show in this paper, triggers some new challenges and research questions for this field. Subsequent work has shown that human speakers do not necessarily produce minimally distinguishing expressions (van Deemter et al., 2006; Viethen and Dale, 2008; Koolen et al., 2011), and has tried to account for the wide range of factors - such as different speakers, modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and disting"
P16-1058,J12-1006,0,0.180256,"Missing"
P16-1058,W12-1621,0,0.0167311,"favourably to the non-dynamic version of our system (see Section 3). This question is, however, closely linked to another, more intricate question: what is the best strategy to realize installments that, on the one hand, provide enough information so that a user can eventually identify the referent and, on the other hand, avoid misleading words? To date, even highly interactive systems do not generally treat installments, or if they do, only realise them via templates, e.g. (Stoia et al., 2006; Staudte et al., 2012; Garoufi and Koller, 2013; Dethlefs and Cuay´ahuitl, 2015). As pointed out by Liu et al. (2012), data-driven approaches are not straightforward to set-up, due to the “mismatched perceptual basis” between a human listener and an REG system. Based on the insights of our error analysis in Section 3.4, we will rely on a general installment strategy that is mostly targeted at avoiding semantically inadequate object names, and emphasizing the fact that location words generated by the system convey more reliable information. We have implemented two versions of this general strategy: (i) pattern-based installments that always avoid object names in their initial expression and dynamically extend"
P16-1058,W10-4210,0,0.498036,"Missing"
P16-1058,P16-1115,1,0.792542,"egative instances are randomly samples from the complementary set of utterances (e.g. not containing red). We used this relatively simple model in our work, because first of all we wanted to test wether it scales from a controlled domain of typical reference game scenes (Kennington and Schlangen, 2015) to real-world images. Second, as compared to standard object recognisers that predict abstract image labels annotated in e.g. ImageNet (Deng et al., 2009), this model directly captures the relation between actual words used in REs and visual properties of the corresponding referents. Following (Schlangen et al., 2016), we can easily base our classifiers on such a high-performance convolutional neural network (Szegedy et al., 2015), by applying it on our images and extracting the final fully-connected layer before the classification layer (see Section 3.1). The ReferIt corpus We train and evaluate our system on the ReferIt data set collected by Kazemzadeh et al. (2014). The basis of the corpus is a collection of “20,000 still natural images taken from locations around the world” (Grubinger et al., 2006), which was augmented by Escalante et al. (2010) with segmentation masks identifying objects in the images"
P16-1058,W06-1412,0,0.230702,"lly inadequate words as much as possible and only expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with"
P16-1058,W11-2845,0,0.0924581,"y expands the expression in case of misunderstanding. We present an algorithm that generates these installments depending on the context, based on ideas from traditional REG algo611 2.3 ibly split and adapt their REs over several utterances during an interaction, a phenomenon called “reference in installments”. In a corpus analysis of the S-GIVE domain, (Striegnitz et al., 2012) showed that installments are pervasive in humanhuman interaction in a task-oriented environment. However, while there has been research on goaloriented and situated REG (Stoia et al., 2006; Kelleher and Kruijff, 2006; Striegnitz et al., 2011; Garoufi and Koller, 2013), installments have been rarely implemented and empirically tested in interactive systems. A noticeable exception is the work by Fang et al. (2014) who use reinforcement learning to induce an installment strategy that is targeted at robots that have uncertain knowledge about the objects in their environment. Using relatively simple computer-generated scenes and a standard representations of objects as sets of attributes, they learn a strategy that first guides the user to objects that the system can recognize with high confidence. Our work is targeted at more complex"
P16-1058,W12-1504,0,0.786605,", modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for achieving common ground in taskoriented interaction (Fang et al., 2014) and is attested in dialogue data (Striegnitz et al., 2012). The connection between reference in installments on the one and the status of distractors and distinguishing expressions on the other hand is relatively unexplored, though it seems natural to combine the two perspectives (DeVault et al., 2005). Figure 1 shows an example for very a simple but highly effective expression - it mentions color as a salient and distinguishing property while avoiding a potentially unclear object name. Task-oriented REG has looked at reference as a collaborative process where a speaker and a listener try to reach a common goal (Clark and Wilkes-Gibbs, 1986; Heeman a"
P16-1058,W06-1420,0,0.0544983,"Missing"
P16-1058,W08-1109,0,0.0281354,"r object from D, if the target and distractor have different values. This is mostly based on the assumption that we have objects of particular types (e.g. people, furniture, etc.) and that the system has perfect knowledge about these object types and, consequently, about potential distractors of the target. This does not apply to REG on real-world images which, as we will show in this paper, triggers some new challenges and research questions for this field. Subsequent work has shown that human speakers do not necessarily produce minimally distinguishing expressions (van Deemter et al., 2006; Viethen and Dale, 2008; Koolen et al., 2011), and has tried to account for the wide range of factors - such as different speakers, modalities, object categories - that are related to attribute selection, cf. (Mitchell et al., 2010; Koolen and Krahmer, 2010; Clarke et al., 2013; Tarenskeen et al., 2015). Research on reference in human interaction has noticed that conversation partners try to minimize their joint effort and often prefer to present simple expressions that can be expanded on or repaired, if necessary (Clark and Wilkes-Gibbs, 1986). This strategy, called “referring in installments” is very effective for"
P16-1058,W09-0629,0,\N,Missing
P16-1058,D15-1224,0,\N,Missing
P16-1115,H89-2010,0,0.072733,"Missing"
P16-1115,D14-1086,0,0.146161,"r. For uniquely referring expressions (“the red cross”), what is required is to pick the most likely candidate from the distribution: [[the]] = λx. arg max x (5) … The “Words-As-Classifiers” Model … 3 Figure 2: Image 27437 from IAPR TC -12 (left), with region masks from SAIAPR TC -12 (middle); “brown shirt guy on right” is a referring expression in R EFER I T G AME for the region singled out on the right done manually and provide close maskings of the objects. This extended dataset is also known as “SAIAPR TC -12” (for “segmented and annotated IAPR TC -12”). The third component is provided by Kazemzadeh et al. (2014), who collected a large number of expressions referring to (presegmented) objects from these images, using a crowd-sourcing approach where two players were paired and a director needed to refer to a predetermined object to a matcher, who then selected it. (An example is given in Figure 2 (right).) This corpus contains 120k referring expressions, covering nearly all of the 99.5k regions from SAIAPR TC -12.3 The average length of a referring expression from this corpus is 3.4 tokens. The 500k token realise 10,340 types, with 5785 hapax legomena. The most frequent tokens (other than articles and"
P16-1115,P15-1029,1,0.878484,"lassifiers Model David Schlangen Sina Zarrieß Casey Kennington Dialogue Systems Group // CITEC // Faculty of Linguistics and Literary Studies Bielefeld University, Germany first.last@uni-bielefeld.de Abstract HRI work (&gt; 300, see below). More formally, the task is to retrieve, given a referring expression e and an image I, the region bb∗ of the image that is most likely to contain the referent of the expression. As candidate regions, we use both manually annotated regions as well as automatically computed ones. As our starting point, we use the “words-asclassifiers” model recently proposed by Kennington and Schlangen (2015). It has before only been tested in a small domain and with specially designed features; here, we apply it to real-world photographs and use learned representations from a convolutional neural network (Szegedy et al., 2015). We learn models for between 400 and 1,200 words, depending on the training data set. As we show, the model performs competitive with the state of the art (Hu et al., 2016; Mao et al., 2016) on the same data sets. Our background interest in situated interaction makes it important for us that the approach we use is ‘dialogue ready’; and it is, in the sense that it supports i"
P16-1115,Q13-1016,0,0.0616644,"nting these with positional information, we show that the model achieves performance competitive with the state of the art in a reference resolution task (given expression, find bounding box of its referent), while, as we argue, being conceptually simpler and more flexible. 1 Introduction A common use of language is to refer to objects in the shared environment of speaker and addressee. Being able to simulate this is of particular importance for verbal human/robot interfaces (HRI), and the task has consequently received some attention in this field (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Here, we study a somewhat simpler precursor task, namely that of resolution of reference to objects in static images (photographs), but use a larger set of object types than is usually done in 2 Related Work The idea of connecting words to what they denote in the real world via perceptual features goes back at least to Harnad (1990), who coined “The Symbol Grounding Problem”: “[H]ow can the semantic interpretation of a formal symbol system be made intrinsic to the system, rather than just parasitic on the meanings in our heads?” The pro1 The code for reproducing the results reported in this"
P16-1115,Q14-1017,0,0.0439729,"uently taken approach is to use a convolutional neural network (CNN) to map the image to a dense vector (which we do as well, as we will describe below), and then condition a neural language model (typically, an LSTM) on this to produce an output string (Vinyals et al., 2015; Devlin et al., 2015). Fang et al. (2015) modify this approach somewhat, by using what they call “word detectors” first to specifically propose words for image regions, out of which the caption is then generated. This has some similarity to our word models as described below, but again is tailored more towards generation. Socher et al. (2014) present a more compositional variant of this type of approach where sentence representations are composed along the dependency parse of the sentence. The representation of the root node is then mapped into a multimodal space in which distance between sentence and image representation can be used to guide image retrieval, which is the task in that paper. Our approach, in contrast, composes on the level of denotations and not that of representation. Two very recent papers carry this type of approach over to the problem of resolving references to objects in images. Both (Hu et al., 2015) and (Ma"
P16-1115,P16-1058,1,0.79994,"pler domain; for our domain, new and more richly annotated data such as VISUALgenome looks promising for learning a wide variety of relations.9 The use of denotations / extensions might make possible transfer of methods from extensional semantics, e.g. for the addition of operators such as negation or generalised quantifiers. The design of the model, as mentioned in the introduction, makes it amenable for use in interactive systems that learn; we are currently exploring this avenue. Lastly, the word/object classifiers also show promise in the reverse task, generation of referring expressions (Zarrieß and Schlangen, 2016). All this is future work. In its current state— besides, we believe, strongly motivating this future work—, we hope that the model can also serve as a strong baseline to other future approaches to reference resolution, as it is conceptually simple and easy to implement. Acknowledgments We thank Hu et al. (2016), Mao et al. (2016) and Tamara Berg for giving us access to their data. Thanks are also due to the anonymous reviewers for their very insightful comments. We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, whi"
P16-1115,P15-2017,0,\N,Missing
P17-1023,P14-1023,0,0.0467697,"age and Word Embeddings Following Schlangen et al. (2016), we derive representations of our visual inputs with a convolutional neural network, ‘GoogleNet’ (Szegedy et al., 2015), which was trained on the ImageNet corpus (Deng et al., 2009), and extract the final fully-connected layer before the classification layer, to give us a 1024 dimensional representation of the region. We add 7 features that encode information about the region relative to the image, thus representing each object as a vector of 1031 features. As distributional word vectors, we use the word2vec representations provided by Baroni et al. (2014) (trained with CBOW, 5-word context window, 10 negative samples, 400 dimensions). 4 4.1 Lexical Mapping Through Individual Word Classifiers Three Models of Interfacing Visual and Distributional Information Direct Cross-Modal Mapping Following Lazaridou et al. (2014), referential meaning can be represented as a translation function that projects visual representations of objects to linguistic representations of words in a distributional vector space. Thus, in contrast to standard object recognition systems or the other models we will use here, cross-modal mapping does not treat words as individ"
P17-1023,D15-1003,0,0.0379694,"uman users under natural, interactive conditions (Kazemzadeh et al., 2014), and train and test on the corresponding head nouns in these REs. This is similar to picture naming setups used in psycholinguistic research (cf. Levelt et al. (1991)) and based on the simplifying assumption that the name used for referring to an object can be determined successfully without looking at other objects in the image. We now summarise the details of our setup: Cross-modal transfer Rather than fusing different modalities into a single, joint space, other work has looked at cross-modal mapping between spaces. Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between distributional information and conceptual knowledge representing quantifiers and predicates. More related to our work are cross-modal mapping models,that learn to transfer from a representation of an object or image in the visual space to a vector in a distributional space (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014). Here, the motivation is to exploit the rich lexical knowledge encoded in a"
P17-1023,D14-1086,0,0.599207,"et al. (2002) use computer vision techniques to process a video feed, and to compute colour, positional and spatial features. These features are then associated in a learning process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene currently presented to the video feed. Whereas Roy’s work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). Importantly, the lowlevel visual features that can be extracted from these scenes correspond less directly to particular word classes. Moreover, the visual scenes contain many different types of objects, which poses new challenges for REG. For instance, Zarrieß and Schlangen (2016) find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturbing for users. Whereas Zarrieß and Schlangen (2016) propose a strategy to avoid object names when the systems confiden"
P17-1023,D14-1005,0,0.0199454,"o actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus of referring expression"
P17-1023,N10-1011,0,0.0874076,"and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop"
P17-1023,J12-1006,0,0.0539971,"Missing"
P17-1023,P14-1132,0,0.465117,"oint space, other work has looked at cross-modal mapping between spaces. Herbelot and Vecchi (2015) present a model that learns to map vectors in a distributional space to vectors in a set-theoretic space, showing that there is a functional relationship between distributional information and conceptual knowledge representing quantifiers and predicates. More related to our work are cross-modal mapping models,that learn to transfer from a representation of an object or image in the visual space to a vector in a distributional space (Socher et al., 2013; Frome et al., 2013; Norouzi et al., 2013; Lazaridou et al., 2014). Here, the motivation is to exploit the rich lexical knowledge encoded in a distributional space for learning visual classifications. In practice, these models are mostly used for zeroshot learning where the test set contains object categories not observed during training. When tested on standard object recognition tasks, transfer, however, comes at a price. Frome et al. (2013) and Norouzi et al. (2013) both find that it slightly degrades performance as compared to a plain object classification using standard accuracy metrics (called flat “hit @k metric” in their paper). Interestingly though,"
P17-1023,P15-1027,0,0.242826,"predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-know"
P17-1023,P16-1115,1,0.622953,"milarity. Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (Zarrieß and Schlangen, 2017). The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual"
P17-1023,N15-1016,0,0.354013,"predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it seems to generalize less well than expected, i.e. tends to reproduce word vectors observed during training (Lazaridou et al., 2015a). In this work, we present a model that exploits distributional knowledge for learning referential word meaning as well, but explore and compare different ways of combining visual and lexical aspects of referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-know"
P17-1023,P14-1068,0,0.0447118,"referential word meaning. to actual words via various translation procedures. We are interested in learning referential appropriateness and extensional word meanings directly from actual human referring expressions (REs) paired with objects in images, using an existing object recognizer for feature extraction. Multi-modal distributional semantics Distributional semantic models are a well-known method for capturing lexical word meaning in a variety of tasks (Turney and Pantel, 2010; Mikolov et al., 2013; Erk, 2016). Recent work on multimodal distributional vector spaces (Feng and Lapata, 2010; Silberer and Lapata, 2014; Kiela and Bottou, 2014; Lazaridou et al., 2015b; Kottur et al., 2016) has aimed at capturing semantic similarity even more accurately by integrating distributional and perceptual features associated with words (mostly taken from images) into a single representation. 3 Task and Data We define object naming as follows: Given an object x in an image, the task is to predict a word w that could be used as the head noun of a realistic referring expression. (Cf. discussion above: “bird” when naming a robin, but “penguin” when naming a penguin.) To get at this, we develop our approach using a corpus"
P17-1023,P16-1058,1,0.915408,"process a video feed, and to compute colour, positional and spatial features. These features are then associated in a learning process with certain words, resulting in an association of colour features with colour words, spatial features with prepositions, etc., and based on this, these words can be interpreted with reference to the scene currently presented to the video feed. Whereas Roy’s work still looked at relatively simple scenes with graphical objects, research on REG has recently started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Zarrieß and Schlangen, 2016; Mao et al., 2015). Importantly, the lowlevel visual features that can be extracted from these scenes correspond less directly to particular word classes. Moreover, the visual scenes contain many different types of objects, which poses new challenges for REG. For instance, Zarrieß and Schlangen (2016) find that semantic errors related to mismatches between nouns (e.g. the system generates tree vs. man) are particularly disturbing for users. Whereas Zarrieß and Schlangen (2016) propose a strategy to avoid object names when the systems confidence is low, we focus on improving the generation of"
P17-1023,E17-2014,1,0.686247,"can be mapped to. However, distributional representations of word meaning are known to capture a rather fuzzy notion of lexical similarity, e.g. car is similar to van and to street. A cross-modal transfer model is “forced” to learn to map objects into the same area in the semantic space if their names are distributionally similar, but regardless of their actual visual similarity. Indeed, we have found in a recent study that the contribution of distributional information to learning referential word meanings is restricted to certain types of words and does not generalize across the vocabulary (Zarrieß and Schlangen, 2017). The goal of this work is to learn a model of referential word meaning that makes accurate object naming predictions and goes beyond treating words as independent, mutually exclusive labels in a flat classification scheme. We extend upon work on learning models of referential word use from corpora of images paired with referring expressions (Schlangen et al., 2016; Zarrieß and Schlangen, 2017) that treats words as individual 244 rors”. To the best of our knowledge, this pattern has not been systematically investigated any further. Another known problem with cross-modal transfer is that it see"
P17-1023,D15-1224,0,\N,Missing
P19-1063,J12-1006,0,0.060858,"Missing"
P19-1063,D17-1098,0,0.0266983,"re, we show that this strategy often improves reference resolution accuracies of an automatic listener. 2 build a pragmatic speaker that produces more discriminative captions, applying equation 2 at each step of the inference process. They evaluate their model in a reference game where an automatic listener (trained on a different portion of the image data) is used to test whether the generated caption singles out the target image among a range of distractor images. A range of related articles have extended neural captioning models with decoding procedures geared towards vocabulary expansion (Anderson et al., 2017; Agrawal et al., 2018) or contextually discriminative scene descriptions (Andreas and Klein, 2016; Vedantam et al., 2017). Previous work on REG commonly looks at visual scenes with multiple referents of identical or similar categories. Here, speakers typically produce expressions composed of a head noun, which names the category of the target, and a set of attributes, which distinguish the target from distractor referents of the same category (Krahmer and Van Deemter, 2012). Our work adds an additional dimension of uncertainty to this picture, namely a setting where the category of the target"
P19-1063,D16-1125,0,0.0954734,"stener. 2 build a pragmatic speaker that produces more discriminative captions, applying equation 2 at each step of the inference process. They evaluate their model in a reference game where an automatic listener (trained on a different portion of the image data) is used to test whether the generated caption singles out the target image among a range of distractor images. A range of related articles have extended neural captioning models with decoding procedures geared towards vocabulary expansion (Anderson et al., 2017; Agrawal et al., 2018) or contextually discriminative scene descriptions (Andreas and Klein, 2016; Vedantam et al., 2017). Previous work on REG commonly looks at visual scenes with multiple referents of identical or similar categories. Here, speakers typically produce expressions composed of a head noun, which names the category of the target, and a set of attributes, which distinguish the target from distractor referents of the same category (Krahmer and Van Deemter, 2012). Our work adds an additional dimension of uncertainty to this picture, namely a setting where the category of the target itself might not be known to the model and, hence, cannot be named with reasonable accuracy. In t"
P19-1063,N18-2070,0,0.2948,"eaker. To do that, it needs to calculate P (u|r), as in Equation 1. While previous work on RSA typically equates P (u|r) with S0 (u|r), we are going to modify the way this probIn turn, the “pragmatic speaker” S1 reasons about which utterance is more discriminative and will be resolved to the target by the pragmatic listener: L0 (r|u) ∗ P (u) ui ∈U L0 (r|ui ) ∗ P (ui ) S1 (u|r) ∝ P Model (2) (S0 and L0 are components of the recursive reasoning of S1 and not in fact separate agents.) There has been some previous work on leveraging RSA-like reasoning for neural language generation. For instance, Cohn-Gordon et al. (2018) implement the literal speaker as a neural captioning model trained on non-discriminative image descriptions. On top of this neural semantics, they 655 ability is calculated. Thus, we assume that our listener has hidden beliefs about the category of the referent, that we can marginalize over as follows: P (u|r) = X P (u, ci |r) = ci ∈C category-specific words and resort to describing other visual properties like colour or location.1 Similar to Cohn-Gordon et al. (2018), we use incremental, word-level inference to decode the pragmatic speaker model in a greedy fashion: X P (u, ci , r) P (r) ci"
P19-1063,W18-6563,1,0.835379,"of “knowing” which words risk being inaccurate for referring to novel objects. The following Section 3 describes how we modify the RSA approach for reasoning in such a zero-shot reference game. Background We investigate referring expression generation (REG henceforth), where the goal is to compute an utterance u that identifies a target referent r among other referents R in a visual scene. Research on REG has a long tradition in natural language generation (Krahmer and Van Deemter, 2012), and has recently been re-discovered in the area of Language & Vision (Mao et al., 2016; Yu et al., 2016; Zarrieß and Schlangen, 2018). These latter models for REG essentially implement variants of a standard neural image captioning architecture (Vinyals et al., 2015), combining a CNN and an LSTM to generate an utterance directly from objects marked via bounding boxes in real-world images. Our approach combines such a neural REG model with a reasoning component that is inspired by theory-driven Bayesian pragmatics and RSA (Frank and Goodman, 2012). We will briefly sketch this approach here. The starting point in RSA is a model of a “literal speaker”, S0 (u|r), which generates utterances u for the target r. The “pragmatic lis"
W06-2108,J91-4003,0,0.096552,"Missing"
W09-2602,W08-1707,0,0.0668004,"Missing"
W09-2602,1999.mtsummit-1.20,0,0.0600361,"onal structure which basically represents the surface independent grammatical relations of a sentence, it constitutes a particularly appropriate basis for large-scale, multilingual syntax. Parallel grammar development bears the practical advantage that the resources developped for a particular language can often easily be ported to related languages. Kim et al. (2003) report that the Korean ParGram grammar was constructed in two months by adapting the Japanese grammar for Korean. Moreover, parallel grammars have a straightforward application in multilingual NLP tasks like machine translation (Frank, 1999). A general motivation for multilingual, deep grammars are higher-level NLP tasks which involve some kind of semantic or meaningsensititive processing (Butt and King, 2007). The work presented in this paper shows that parallel grammar development not only facilitates porting of grammars, but substantially facilitates the development of resources and applications that involve such a parallel grammar. We rely on the semantic conversion system presented in (Crouch and King, 2006) to implement a system that derives semantic representations from LFG fstructures for German. Due to the parallelism of"
W09-2602,W07-1403,0,0.0237887,"Missing"
W09-2602,W06-3907,0,0.0168928,"tial relation between matrix and complement in example (4) . whom the speaker is not committed to, i.e. which aren’t veridical. In our system, the veridicality inferences that these embeddings exhibit are computed by further knowledge representation modules that explicitely represent the speaker commitment of a context (Bobrow et al., 2007b). Concerning the complements of clause-embedding verbs, these inferences are modelled via a lexical verb classification that basically distinguishes implicatives (manage to TRUE - don’t manage to FALSE ) and factives (know that TRUE don’t know that TRUE ) (Nairn et al., 2006). Veridicality entailments of sentential complements are treated as a interaction of the lexical class of the subordinating verb and the polarity of the context. (4) (5) Seine Freundin brachte ihn dazu, ein Haus zu bauen. His girlfriend made him build a house. Moreover, the semantics of clauseembedding verbs shows subtle distinctions with resepct to other linguistic features (apart from the polarity of the context) that can trigger a particular speaker commitment. For instance, in languages that have a morphological aspect marking (like Frensh, in the following example), the following aspectua"
W09-2602,C04-1180,0,0.0369271,"airs instead of sentences and their theoretically correct representations. However, in the context of this work, we didn’t focus on a semantic application, but we wanted to assess the portability of the semantic representations to other languages directly. Adopting such a theorydriven perspective on semantic grammar development, the only possibility to account for the accuracy of the semantic construction is to manually inspect the output of the system for a necessarily small set of input sentences. Moreover, the transfer scenario complicates the assessment of the system’s coverage. While in (Bos et al., 2004), the coverage of the meaning construction can be quanti3.2 A Parallel Testsuite In consequence to these considerations on evaluation, a central aspect of our development metholodogy is a testsuite of German sentences which represents the “core semantics” that our systems covers. The multi13 tinguishes. For instance, while the English grammar encodes oblique arguments the same way it encodes direct objects, the German grammar has a formally slightly different analysis such that rules which fire on obliques in English, don’t fire for German input. Now, the final parallel testsuite comprises 200"
W09-2602,I05-6001,0,0.0305788,"to normalized semantic relations. The representation simplifies many phenomena usually discussed in the formal semantic literature (see the next section), but is tailored for use in Question Answering (Bobrow et al., 2007a) or Textual Entailment (Bobrow et al., 2007b) applications. The semantic conversion was implemented by means of the XLE platform, used for grammar development in the ParGram project. It makes use of the built-in transfer module to convert LFG f-structures to semantic representations. The idea to use transfer rules to model a semantic concstruction has also been pursued by (Spreyer and Frank, 2005) who use the transfer module to model a RMRS semantic construction for the German treebank TIGER . 2 The basic idea of the representation exemplified in figure 1 is to represent the syntactic arguments and adjuncts of the main predicate in terms of semantic roles of the context introduced by the main predicate or some higher semantic operator. Thus, the grammatical roles of the main verb in sentence (1) are semantically normalized such that the subject of the passive becomes a theme and an unspecified agent is introduced, see figure 1. The role of the modifiers are speci2.1 The Semantic Repres"
W09-2602,Y06-1061,0,0.0165039,"ssive becomes a theme and an unspecified agent is introduced, see figure 1. The role of the modifiers are speci2.1 The Semantic Representation As a first example, a simplified f-structure analysis for the following sentence and the corresponding semantic representation are given in figure 1. (1) F-Structure Rewriting as an LFG Semantics Since the early days of LFG, there has been research on interfacing LFG syntax with various semantic formalisms (Dalrymple, 1999). For the English and Japanese ParGram grammar, a broad-coverage, glue semantic construction has been implemented by (Crouch, 1995; Umemoto, 2006). In contrast to these approaches, the semantic con11 In the afternoon, John was seen in the park. fied in terms of their head preposition. This type of semantic representation is inspired by Neo-Davidsonian event semantics (Parsons, 1990). Other semantic properties of the event introduced by the main verb such as tense or nominal properties such as quantification and cardinality are explicitely encoded as conventionalized predications. The contexts can be tought of as propositions or possible worlds. They are headed by an operator that can recursively embed further contexts. Context embedding"
W09-2602,W02-1503,0,0.12678,"emantic analysis raises many questions about appropriate meaning representations as well as engineering problems concerning the development and evaluation strategies of semantic processing systems. The general aim of this work is to explore wide-coverage LFG syntax as a backbone for linguistically motivated semantic processing. Research in the framework of LFG has traditionally adopted a crosslingual perspective on linguistic theory (Bresnan, 2000). In the context of the ParGram project, a number of high quality, broad-coverage grammars for several languages have been produced over the years (Butt et al., 2002; Butt and King, 2007).1 The project’s research methodology particularly focusses on parallelism which means that the researchers rely on a common syntactic theory as well as development tools, but which also concerns parallelism on the level of syntactic analyses. As the LFG formalism assumes a two-level syntax that diThis paper reports on the development of a core semantics for German which was implemented on the basis of an English semantics that converts LFG f-structures to flat meaning representations in a Neo-Davidsonian style. Thanks to the parallel design of the broad-coverage LFG gram"
W09-2904,W02-2001,0,0.323471,"rs. We will show that these correspondences can be reliably detected on dependency-parsed, wordaligned sentences and are able to identify various MWE patterns. In a monolingual setting, the task of MWE extraction is usually conceived of as a lexical association problem where distributional measures model the syntactic and semantic idiosyncracy exhibited by MWEs, e.g. (Pecina, 2008). This approach generally involves two main steps: 1) the extraction of a candidate list of potential MWEs, often constrained by a particular target pattern of the detection method, like verb particle constructions (Baldwin and Villavicencio, 2002) or verb PP combinations (Villada Moir´on and Tiedemann, 2006), 2) the ranking of this candidate list by an appropriate assocation measure. The crosslingual MWE identification we present in this paper is, a priori, independent of any specific association measure or syntactic pattern. The translation scenario allows us to adopt a completely data-driven definition of what constitutes an MWE: Given a parallel corpus, we propose to consider those tokens in a target language as MWEs which correspond to a single lexical item in the source language. The intuition is that if a group of lexical items i"
W09-2904,P05-1074,0,0.0859961,"Missing"
W09-2904,cyrus-2006-building,0,0.0276178,"blematic and challenging type of translational correspondence in our gold standard. While the MWE literature typically discusses the distinction between collocations and MWEs, the boarderline between paraphrases and MWEs is not really clear. On the hand, paraphrases, as we classified them here, are transparent combinations of lexical items, like in the example below ensure that something increases. However, semantically, these transparent combinations can also be rendered by an atomic expression increase. A further problem raised by paraphrases is that they often involve translational shifts (Cyrus, 2006). These shifts are hard to identify automatically and present a general challenge for semantic processing of parallel corpora. An example is given below. (6) The committee set out to equip the institutions with a political instrument. Verb preposition combinations: While this class isn’t discussed very often in the MWE literature, it can nevertheless be considered as an idiosyncratic combination of lexical items. Sag et al (2002) propose an analysis within an MWE framework. (7) Sie werden den Treibhauseffekt verschlimmern. They will the green house effect aggravate. (8) They will add to the gr"
W09-2904,W09-0208,0,0.0192418,"rm of one-to-many translations. By contrast, previous approaches to paraphrase extraction made more explicit use of crosslingual semantic information. In (Bannard and CallisonBurch, 2005), the authors use the target language as a pivot providing contextual features for identifying semantically similar expressions. Paraphrasing is however only partially comparable to the crosslingual MWE detection we propose in this paper. Recently, the very pronounced context dependence of monolingual pairs of semantically similar expressions has been recognized as a major challenge in modelling word meaning (Erk and Pado, 2009). The idea that parallel corpora can be used as a linguistic resource that provides empirical evidence for monolingual idiosyncrasies has already Related Work The problem sketched in this paper has clear conncetions to statistical MT. So-called phrase-based translation models generally target whole sentence alignment and do not necessarily recur to linguistically motivated phrase correspondences (Koehn et al., 2003). Syntax-based translation that specifies formal relations between bilingual parses was 29 References been exploited in, e.g. morphology projection (Yarowsky et al., 2001) or word s"
W09-2904,N03-1017,0,0.0109718,"paper. Recently, the very pronounced context dependence of monolingual pairs of semantically similar expressions has been recognized as a major challenge in modelling word meaning (Erk and Pado, 2009). The idea that parallel corpora can be used as a linguistic resource that provides empirical evidence for monolingual idiosyncrasies has already Related Work The problem sketched in this paper has clear conncetions to statistical MT. So-called phrase-based translation models generally target whole sentence alignment and do not necessarily recur to linguistically motivated phrase correspondences (Koehn et al., 2003). Syntax-based translation that specifies formal relations between bilingual parses was 29 References been exploited in, e.g. morphology projection (Yarowsky et al., 2001) or word sense disambiguation (Dyvik, 2004). While in a monolingual setting, it is quite tricky to come up with theoretical or empirical definitions of sense discriminations, the crosslingual scenario offers a theory-neutral, data-driven solution: Since ambiguity is an idiosyncratic property of a lexical item in a given language, it is not likely to be mirrored in a target language. Similarly, our approach can also be seen as"
W09-2904,2005.mtsummit-papers.11,0,0.0234508,"machine translation, but also for crosslingual induction of morphological, syntactic and semantic analyses (Yarowsky et al., 2001; Dyvik, 2004). In this paper, we propose an approach to the identification of multiword expressions (MWEs) that exploits translational correspondences in a parallel corpus. We will consider in translations of the following type: ¨ (1) Der Rat sollte unsere Position berucksichtigen. The Council should our position consider. (2) The Council should take account of our position. This sentence pair has been taken from the German - English section of the Europarl corpus (Koehn, 2005). It exemplifies a translational correspondence between an English MWE take account of and a German simplex verb ber¨ucksichtigen. In the following, we refer to such correspondences as one-to-many translations. Based on a study of verb translations in Europarl, we will explore to what extent one-to-many translations provide evidence for MWE realization in the target language. It will turn out that crosslingual corre23 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 23–30, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP one-to-many translations correspon"
W09-2904,nivre-etal-2006-maltparser,0,0.0289514,"from our candidate set. Coordination can be discarded by imposing the condition on the configuration not to contain a coordination relation. This Generate-and-Filter strategy now extracts a set of sentences where we are likely to find a good one-to-one or one-to-many translation for the source verb. behindert X1 Y1 Y2 an X2 to obstacle create Figure 1: Example of a typical syntactic MWE configuration Data We word-aligned the German and English portion of the Europarl corpus by means of the GIZA++ tool. Both portions where assigned flat syntactic dependency analyses by means of the MaltParser (Nivre et al., 2006) such that we obtain a parallel resource of word-aligned dependency parses. Each sentence in our resource can be represented by the triple (DG , DE , AG,E ). DG is the set of dependency triples (s1 , rel, s2 ) such Alignment Post-editing In the final alignment step, one now needs to figure out which lexical material in the aligned syntactic configurations actually corresponds to the translation of the source item. The intuition discussed in 3.2 was that all 27 the items lying on a path between the root item and the terminals belong to the translation of the source item. However, these items ma"
W09-2904,J03-1002,0,0.0164145,"35.7 0.5 17 182 verschlimmern (v4 ) 30.2 21.5 28.6 44.5 275 Table 1: Proportions of types of translational correspondences (token-level) in our gold standard. terns exhibited by one-to-many translations. We constructed a gold standard covering all English translations of four German verb lemmas extracted from the Europarl Corpus. These verbs subcategorize for a nominative subject and an accusative object and are in the middle frequency layer (around 200 occurrences). We extracted all sentences in Europarl with occurences of these lemmas and their automatic word alignments produced by GIZA++ (Och and Ney, 2003). These alignments were manually corrected on the basis of the crosslingual word alignment guidelines developped by (Grac¸a et al., 2008). For each of the German source lemmas, our gold standard records four translation categories: one-to-one, one-to-many, many-to-one, many-tomany translations. Table 1 shows the distribution of these categories for each verb. Strikingly, the four verbs show very different proportions concerning the types of their translational correspondences. Thus, while the German verb anheben (en. increase) seems to have a frequent parallel realization, the verbs bezwecken"
W09-2904,H94-1027,0,0.32763,"source item. However, these items may have other syntactic dependents that may also be part of the one-to-many translation. As an example, consider the configuration in figure 1 where the article an which is part of the LVC create an obstacle to has to be aligned to the German source verb. Thus, for a set of items ti for which there is a dependency relation (tx , rel, ti ) ∈ DE such that tx is an element of our target configuration, we need to decide whether (s1 , ti ) ∈ AG,E . This translation problem now largely parallels collocation translation problems discussed in the literature, as in (Smadja and McKeown, 1994). But, crucially, our syntactic filtering strategy has substantially narrowed down the number of items that are possible parts of the one-to-many translation. Thus, a straightforward way to assemble the translational correspondence is to compute the correlation or association of the possibly missing items with the given translation pair as proposed in (Smadja and McKeown, 1994). Therefore, we propose the following alignment post-editing algorithm: Given the source item s1 and the set of target items T , where each ti ∈ T is an element of our target configuration, The output translation can the"
W09-2904,W06-2405,0,0.308057,"Missing"
W09-2904,J97-3002,0,0.190991,"Missing"
W09-2904,H01-1035,0,0.0371309,"tions that exhibit a correspondence between a single lexical item in the source language and a group of lexical items in the target language. We show that these correspondences can be reliably detected on dependency-parsed, word-aligned sentences. We propose an extraction method that combines word alignment with syntactic filters and is independent of the structural pattern of the translation. 1 Introduction Parallel corpora have proved to be a valuable resource not only for statistical machine translation, but also for crosslingual induction of morphological, syntactic and semantic analyses (Yarowsky et al., 2001; Dyvik, 2004). In this paper, we propose an approach to the identification of multiword expressions (MWEs) that exploits translational correspondences in a parallel corpus. We will consider in translations of the following type: ¨ (1) Der Rat sollte unsere Position berucksichtigen. The Council should our position consider. (2) The Council should take account of our position. This sentence pair has been taken from the German - English section of the Europarl corpus (Koehn, 2005). It exemplifies a translational correspondence between an English MWE take account of and a German simplex verb ber¨"
W10-2106,rohrer-forst-2006-improving,1,0.942795,"Missing"
W10-2106,H01-1035,0,0.15152,"3). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival participles) do not only occur as attributive modifier"
W10-2106,W02-1503,1,0.901049,"ller et al., 1995) assigns the tag “ADJD” to predicative adjectives as well as adverbs. A Broad-Coverage LFG for German Lexical Functional Grammar (LFG) (Bresnan, 2000) is a constraint-based theory of grammar. It posits two levels of representation, c(onstituent)structure and f(unctional)- structure. C-structure is represented by contextfree phrase-structure trees, and captures surface grammatical configurations. F-structures approximate basic predicateargument and adjunct structures. The experiments reported in this paper use the German LFG grammar constructed as part of the ParGram project (Butt et al., 2002). The grammar is implemented in the XLE, a grammar development environment which includes a very efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and"
W10-2106,2005.mtsummit-papers.11,0,0.00413936,"ic morphological tags, see (Dipper, 2003). parallel corpora and cross-lingual NLP induction techniques. Since adverbs are often overtly marked in other languages (i.e. the ly-suffix in English), adverbial participles can be straightforwadly detected on word-aligned parallel text. We describe the ingretation of the automatically induced resource of adverbial participles into the German LFG, and provide a detailed evaluation of its effect on the grammar, see Section 5. While the use of parallel resources is rather familiar in a wide range of NLP domains, such as statistical machine translation (Koehn, 2005) or annotation projection (Yarowsky et al., 2001), our work shows that they can be exploited for very specific problems that arise in deep linguistic analysis (see Section 4). In this way, highprecision, data-oriented induction techniques can clearly improve rule-based system development through combining the benefits of high empirical accuracy and little manual effort. 2 3 Participles in the German LFG 3.1 Analysis The morphosyntactic ambiguity of German participles presents a notorious difficulty for theoretical and computational analysis. The reason is that adjectives (i.e. adjectival parti"
W10-2106,nivre-etal-2006-maltparser,0,0.107237,"Missing"
W10-2106,J03-1002,0,0.00381894,"erbial translation for other reasons. A typical configuration is exemplified in (6) where the German main verb vorlegen is translated as the verb-adverb combination put forward. c. Apr`es l’ e´ largissement a` l’ Est, la tendance sera davantage a` la lib´eralisation. In the following, we describe experiments on Europarl where we automatically extract and filter adverbially translated German participles. 4.1 Data We base our experiments on the German, English, French and Dutch part of the Europarl corpus. We automatically word-aligned the German part to each of the others with the GIZA++ tool (Och and Ney, 2003). Note that, due to divergences in sentence alignment and tokenisation, the three word-alignments are not completely synchronised. Moreover, each of the 4 languages has been automatically PoS tagged using the TreeTagger (Schmid, 1994). In addition, the German and English parts have been parsed with MaltParser (Nivre et al., 2006). Since we want to limit our investigation to those participles that are not already recorded as lexicalised adjective or adverb in the DMOR morphology, we first have to generate the set of participle candidates from the tagged Europarl data. We extract all distinct wo"
W10-2106,P02-1035,0,0.169128,"ry efficient LFG parser. Within the spectrum of appraoches to natural language parsing, XLE can be considered a hybrid system combining a hand-crafted grammar with a number of automatic ambiguity management techniques: (i) c-structure pruning where, based on information from statstically obtained parses, some trees are ruled out before fstructure unification (Cahill et al., 2007), (ii) an Optimaly Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and (iii) a stochastic disambiguation component which is based on a log-linear probability model (Riezler et al., 2002) and works on the packed representations. The German LFG grammar integrates a morphological component which is a variant of (2) a. Das Experiment hat ihn begeistert. ‘The experiment has enthused him.’ b. Er scheint von dem Experiment begeistert. ‘He seems enthusiastic about the experiment.’ c. Er hat begeistert experimentiert. ‘He has experimented in an enthusiastic way’ or: ‘He was enthusiastic when he experimented.’ For performance reasons, the German LFG does not cover free predicatives at the moment. In the context of our crosslingual induction approach, the distinction between predicative"
W10-2106,W08-1705,1,\N,Missing
W13-2130,D11-1131,0,0.0247308,"Missing"
W13-2130,P98-1116,0,0.122387,"head (or first argument) of the triple relation. Note that some triple relations are not found (e.g. the base relation), since they are implicit in the language. We describe our contribution to the Generating from Knowledge Bases (KBgen) challenge. Our system is learned in a bottom-up fashion, by inducing a probabilistic grammar that represents alignments between strings and parts of a knowledge graph. From these alignments, we extract information about the linearization and lexical choices associated with the target knowledge base, and build a simple generate-and-rank system in the style of (Langkilde and Knight, 1998).1 2 Semantic Parsing and Alignments A first step in building our generator involves finding alignments between phrases and their groundings in the target knowledge base. Figure 1 shows an example sentence from training paired with the corresponding triple relations. A partial lexicon is provided, indicating the relation between a subset of words and their concepts. Using the triples, we automatically construct a probabilistic context-free grammar (PCFG) by converting these triples to rewrite rules, using ideas from (B¨orschinger et al., 2011). The right hand side of the rules represent the co"
W13-2130,C98-1112,0,\N,Missing
W13-3608,W02-1503,0,0.0851341,"Missing"
W13-3608,W13-1703,0,0.0939194,"egories. We focused on the two “nominal” error categories: We introduce here a participating system of the CoNLL-2013 Shared Task “Grammatical Error Correction”. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system. 1 Introduction The CoNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidat"
W13-3608,W10-4236,0,0.0258517,"k “Grammatical Error Correction”. We focused on the noun number and article error categories and constructed a supervised learning system for solving these tasks. We carried out feature engineering and we found that (among others) the f-structure of an LFG parser can provide very informative features for the machine learning system. 1 Introduction The CoNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidates sitting for the Cambridge ESOL First Certificate in English (FCE) examination. In 2013, the CoNLL-2013 Shared Task has continue"
W13-3608,W11-2838,0,0.0446482,"Missing"
W13-3608,W12-2006,0,0.0626592,"oNLL-2013 Shared Task aimed at identifying and correcting grammatical errors in the NUCLE learner corpus of English (Dahlmeier et al., 2013). This task has become popular in the natural language processing (NLP) community in the last few years (Dale and Kilgariff, 2010), which manifested in the organization of shared tasks. In 2011, the task Helping Our Own (HOO 2011) was held (Dale and Kilgariff, 2011), which targeted the promotion of NLP tools and techniques in improving the textual quality of papers written by non-native speakers of English within the field of NLP. The next year, HOO 2012 (Dale et al., 2012) specifically focused on the correction of determiner and preposition errors in a collection of essays written by candidates sitting for the Cambridge ESOL First Certificate in English (FCE) examination. In 2013, the CoNLL-2013 Shared Task has continued this direction of research. The CoNLL-2013 Shared Task is based on the NUCLE corpus, which consists of about 1,400 1.1 Article and Determiner Errors This error type involved all kinds of errors which were related to determiners and articles (ArtOrDet). It required multiple correction strategies. On the one hand, superfluous articles or determin"
W13-3608,C08-1022,0,0.0640972,"Missing"
W13-3608,P02-1035,0,0.0477098,"grammatical errors in the sentence result in unusual constituency subtree patterns that could manifest in minimal governing phrases having too long spans for instance. The relative position of the candidate position inside the smallest dominating noun and prepositional phrases was also incorporated as a feature since this information might carry some information for noun errors. (ii) an Optimality Theory-style constraint mechanism for filtering and ranking competing analyses (Frank et al., 2001), and (iii) a stochastic disambiguation component which is based on a log-linear probability model (Riezler et al., 2002) and works on the packed representations. Although we use a deep, hand-crafted LFG grammar for processing the data, our approach is substantially different from other grammar-based approaches to CALL. For instance, Fortmann and Forst (2004) supplement a German LFG developed for newspaper text with so-called malrules that accept marked or ungrammatical input of some predefined types. In our work, we apply an LFG parser developed for standard texts to get a rich feature representation that can be exploited by a classifier. While malrules would certainly be useful for finding other error types, s"
W13-3608,W08-1705,0,\N,Missing
W15-4705,P11-2040,0,0.0289975,"ity. Compared to the large body of work on automatic evaluation measures, there has been little research that assessed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion of human and automatic evaluation for text quality. They proposed a Turing-style test where participants are asked to judge whether a text was generated by a computer or written by a human. Belz and Kow (2010) showed that higher agreement between human raters can be obtained if they compare two automatically generated texts, instead of assigning scores to texts in isolation. Belz and Kow (2011) found that human judges preferred to use continuous rating scales over discrete rating scales. Siddharthan and Katsos (2012) investigated two offline measures inspired from psycholinguistic studies of sentence processing for assessing text readability, namely magnitude estimation and sentence recall. They demonstrate that the sentence recall method did not discriminate well between sentences of differing fluency if sentences were short. On the other hand, human judgements, did not discriminate well between surface level disfluencies and breakdowns in comprehension. 3 is covered by a mask or m"
W15-4705,W11-2832,0,0.0951389,"icult. Many aspects of linguistic well-formedness and naturalness play a role for assessing the quality of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradig"
W15-4705,P09-2025,0,0.139749,"ty of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text"
W15-4705,E14-1074,0,0.0577951,"Missing"
W15-4705,W13-2111,0,0.0140876,"effects were accounted for by the objective reading measures that are (mostly) outside of conscious control. Section 2 provides background on research in NLG evaluation. Section 3 introduces our MCR paradigm. The generation framework we used to collect our experimental material is presented in Section 4. Section 5 describes the experimental design. The models are discussed in Section 6. 2 Background on NLG Evaluation In recent years, the NLG community has become increasingly interested in comparative evaluation between NLG systems (Gatt and Belz, 2010; Koller et al., 2010; Belz et al., 2011; Banik et al., 2013; Hastie and Belz, 2014). Generally, evaluation methods for assessing NLG systems fall into three main categories: 1) automatic evaluation methods that compare system output against one or multiple reference texts, 2) human evaluation methods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators,"
W15-4705,P14-2074,0,0.112985,"atically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psy"
W15-4705,W10-4201,0,0.186019,"s have to reflect on and differentiate between detailed, linguistic aspects of text quality, and assign scores precisely and systematically across a set of generated outputs that potentially contain various types of linguistic defects. The rating task turns increasingly difficult if they have to compare texts with multiple sentences and multiple types of linguistic defects, e.g. fluency on the sentence level, clarity and coherence on the text level. Consequently, low agreement between raters, and even inconsistencies between ratings of the same human judge have been found in previous studies (Belz and Kow, 2010; Cahill and Forst, 2010; Dethlefs et al., 2014). Standard evaluation methods for, e.g. text summarisation tend to avoid possible interactions between local sentence-level and global text-level defects. Instead, they focus on coherence and content (Nenkova, 2006; Owczarzak et al., 2012). In particular, this is due to the fact that independently rating coherence and clarity locally for each sentence and globally for an entire text is tedious, unnatural, tiring and hardly achievable for human judges. Typically, human evaluation of NLG output is based on user ratings. We collected ratings and rea"
W15-4705,W08-1113,0,0.0286819,"LEU (Papineni et al., 2002) or NIST (Doddington, 2002), that measure the n-gram overlap between the system and some reference text, sentence or phrase. The advantage of such automatic and cheap evaluation methods can be enormous. If tightly integrated in the development cycle of an NLG system, they allow fast and empirically optimised implementation decisions. In turn, a lot of research on NLG evaluation focussed on defining and validating automatic evaluation measures. Such a metric is typically considered valid if it correlates well with human judgements of text quality (Stent et al., 2005; Foster, 2008; Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014). However, automatic evaluation measures in NLG still have a range of known conceptual deficits, i.e. they do not reflect appropriateness of content (Reiter and Belz, 39 2009), or meaning (Stent et al., 2005). Thus, many studies and evaluation challenges in NLG additionally collect human ratings to assess the quality. Compared to the large body of work on automatic evaluation measures, there has been little research that assessed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion"
W15-4705,hardcastle-scott-2008-evaluate,0,0.122947,"nerated texts. This method combines the sensitivity of eye tracking with the cost effectiveness of a rating study. The automatically generated texts are presented to human raters in a sentence-by-sentence, mouse-contingent way such that a number of parameters of the reading process are recorded, e.g. the time that people spent looking at single sentences and an entire text. We hypothesized that these parameters are more informative for the quality of a text than the user ratings of clarity and fluency. As objective criteria for text quality are hardly available in NLG (Dale and Mellish, 1998; Hardcastle and Scott, 2008), we did not compare reading times and ratings on manual, potentially flawed annotations of text quality. Instead, we selected experimental material from a corpus-based generation framework that combines sentence-level linearisation and text-level referring expression generation (Zarrieß and Kuhn, 2013). We based our study on a set of texts that were available in 3 versions: (i) the “gold standard” corpus text, (ii) automatically linearised texts where word order deviated from the original corpus and contained potential fluency-related defects, (iii) texts with potential defects in referring e"
W15-4705,hastie-belz-2014-comparative,0,0.261096,"ted for by the objective reading measures that are (mostly) outside of conscious control. Section 2 provides background on research in NLG evaluation. Section 3 introduces our MCR paradigm. The generation framework we used to collect our experimental material is presented in Section 4. Section 5 describes the experimental design. The models are discussed in Section 6. 2 Background on NLG Evaluation In recent years, the NLG community has become increasingly interested in comparative evaluation between NLG systems (Gatt and Belz, 2010; Koller et al., 2010; Belz et al., 2011; Banik et al., 2013; Hastie and Belz, 2014). Generally, evaluation methods for assessing NLG systems fall into three main categories: 1) automatic evaluation methods that compare system output against one or multiple reference texts, 2) human evaluation methods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators, extrinsic factors cannot"
W15-4705,J09-4008,0,0.59262,"for assessing the quality of an automatically generated text. On the sentence-level, this includes grammatical and morpho-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to asse"
W15-4705,W12-2203,0,0.138468,"sed the validity of human evaluation methods. Hardcastle and Scott (2008) provided an extensive discussion of human and automatic evaluation for text quality. They proposed a Turing-style test where participants are asked to judge whether a text was generated by a computer or written by a human. Belz and Kow (2010) showed that higher agreement between human raters can be obtained if they compare two automatically generated texts, instead of assigning scores to texts in isolation. Belz and Kow (2011) found that human judges preferred to use continuous rating scales over discrete rating scales. Siddharthan and Katsos (2012) investigated two offline measures inspired from psycholinguistic studies of sentence processing for assessing text readability, namely magnitude estimation and sentence recall. They demonstrate that the sentence recall method did not discriminate well between sentences of differing fluency if sentences were short. On the other hand, human judgements, did not discriminate well between surface level disfluencies and breakdowns in comprehension. 3 is covered by a mask or masking pattern. Only if the reader moves the mouse cursor over a particular section of text, the mask is removed and the text"
W15-4705,W13-2104,0,0.0249444,"cy, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading times and eye movements to quantify how well human re"
W15-4705,P04-1011,0,0.0165298,"o-syntactic correctness, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading ti"
W15-4705,W02-2103,0,0.131716,"Missing"
W15-4705,J11-3002,0,0.0222029,"ess, lexical meaning, fluency, and stylistic appropriateness. On the text-level, further criteria related to coherence, text structure, and content should be considered. One of the most widely applied and least controversial NLG evaluation methods is to collect human ratings. Human ratings have been used for system comparison in a number of NLG shared tasks (Gatt and Belz, 2010; Belz et al., 2011), for validating other automatic evaluation methods in NLG (Reiter and Belz, 2009; Cahill, 2009; Elliott and Keller, 2014), and for training statistical components of NLG systems (Stent et al., 2004; Mairesse and Walker, 2011; Howcroft et al., 2013). When no extrinsic tasks or factors for evaluating an NLG system are available, human judges are typically asked to rate the quality of texts or sentences according to several linguistic criteria, such as ‘A: how fluent is the text?’ and ‘B: how clear and understandable is In other disciplines of linguistic research, a range of experimental paradigms have been established that provide more systematic and objective means to assess human text reading. In particular, psycholinguistic approaches typically use objective measures such as reading times and eye movements to qu"
W15-4705,P13-1152,1,0.82319,"me that people spent looking at single sentences and an entire text. We hypothesized that these parameters are more informative for the quality of a text than the user ratings of clarity and fluency. As objective criteria for text quality are hardly available in NLG (Dale and Mellish, 1998; Hardcastle and Scott, 2008), we did not compare reading times and ratings on manual, potentially flawed annotations of text quality. Instead, we selected experimental material from a corpus-based generation framework that combines sentence-level linearisation and text-level referring expression generation (Zarrieß and Kuhn, 2013). We based our study on a set of texts that were available in 3 versions: (i) the “gold standard” corpus text, (ii) automatically linearised texts where word order deviated from the original corpus and contained potential fluency-related defects, (iii) texts with potential defects in referring expressions and linearisation which are likely to deteriorate clarity or coherence on the discourse level. We controlled the broad type of linguistic defects but not the details of each sentence or text. We argue that an objective evaluation method for NLG should clearly distinguish coherence and surface"
W15-4705,W12-2601,0,0.0185242,"f they have to compare texts with multiple sentences and multiple types of linguistic defects, e.g. fluency on the sentence level, clarity and coherence on the text level. Consequently, low agreement between raters, and even inconsistencies between ratings of the same human judge have been found in previous studies (Belz and Kow, 2010; Cahill and Forst, 2010; Dethlefs et al., 2014). Standard evaluation methods for, e.g. text summarisation tend to avoid possible interactions between local sentence-level and global text-level defects. Instead, they focus on coherence and content (Nenkova, 2006; Owczarzak et al., 2012). In particular, this is due to the fact that independently rating coherence and clarity locally for each sentence and globally for an entire text is tedious, unnatural, tiring and hardly achievable for human judges. Typically, human evaluation of NLG output is based on user ratings. We collected ratings and reading time data in a simple, low-cost experimental paradigm for text generation. Participants were presented corpus texts, automatically linearised texts, and texts containing predicted referring expressions and automatic linearisation. We demonstrate that the reading time metrics outper"
W15-4705,P02-1040,0,0.0968426,"hods where human readers are asked to judge a text, typically with respect to several criteria. If the NLG component is embedded in an end-to-end system, such as a dialogue system, 3) extrinsic factors of task success and usefulness of the NLG output can be measured. For corpus-based NLG components such as surface realisers or referring expression generators, extrinsic factors cannot be assessed, but in this case, reference or gold text outputs are often available. Langkilde (2002) first suggested to use automatic evaluation measures inspired from methods in machine translation, such as BLEU (Papineni et al., 2002) or NIST (Doddington, 2002), that measure the n-gram overlap between the system and some reference text, sentence or phrase. The advantage of such automatic and cheap evaluation methods can be enormous. If tightly integrated in the development cycle of an NLG system, they allow fast and empirically optimised implementation decisions. In turn, a lot of research on NLG evaluation focussed on defining and validating automatic evaluation measures. Such a metric is typically considered valid if it correlates well with human judgements of text quality (Stent et al., 2005; Foster, 2008; Reiter and Be"
W16-6642,D10-1115,0,0.0956907,"Missing"
W16-6642,S12-1013,0,0.320058,"Missing"
W16-6642,D15-1224,0,0.100185,"features (Roy, 2002; Roy and Reiter, 2005). In this paradigm, colour terms have received special attention. Intuitively, a model of perceptually grounded meaning should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled settings, speakers use colour terms in flexible, context-dependent ways (Baumgaertn"
W16-6642,D14-1086,0,0.565309,"should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled settings, speakers use colour terms in flexible, context-dependent ways (Baumgaertner et al., 2012; Meo et al., 2014). Therefore, probabilistic models and classifiers, allowing for variable thresholds and boundaries between regions in a colour spac"
W16-6642,J12-1006,0,0.119945,"Missing"
W16-6642,W11-2702,0,0.0310755,"ows: input is a feature vector x, a visual representation of a referent in an image, and output is a label y, a colour term for the referent. For the sake of simplicity, we only consider training and testing instances that contain colour terms and do not 248 model the decision whether a colour term should be generated at all. In standard NLG terminology, we are only interested in realisation, and not in content selection. A lot of research on REG has actually focussed on content selection, assuming perfect knowledge about appropriate colour terms for referents in a scene, cf. (Pechmann, 1989; Viethen and Dale, 2011; Viethen et al., 2012; Krahmer and Van Deemter, 2012; Koolen et al., 2013). The classifiers We used a multilayer perceptron that learns a function from colour histograms (or ConvNet features) to colour terms, i.e. defining an input layer corresponding to the dimensions of the colour histogram and an output layer of 11 nodes. We did not extensively tune the hyper parameters for our different visual inputs, but tested some parameter settings of the perceptron trained on RGB histograms, singling out a development set of 500 instances from the training set described above. We report results for t"
W16-6642,Q14-1006,0,0.0243045,"rpus data and model the connection between words and non-symbolic perceptual features (Roy, 2002; Roy and Reiter, 2005). In this paradigm, colour terms have received special attention. Intuitively, a model of perceptually grounded meaning should associate words for colour with particular points or regions in a colour space, e.g. (Mojsilovic, 2005). On the other hand, their visual association seems to vary with the linguistic context such as ‘red’ in the context of ‘hair’, ‘car’ or ‘wine’ (Roy and Reiter, 2005). Recently, large-scale data sets of real-world images and image descriptions, e.g. (Young et al., 2014), or referring expressions (Kazemzadeh et al., 246 2014; Gkatzia et al., 2015) have become available and can now serve as a realistic test bed for models of language grounding. In this paper, we use the ReferIt corpus (Kazemzadeh et al., 2014) to assess the performance of classifiers that predict colour terms from low-level visual representations of their corresponding image regions. A number of studies on colour naming have looked at experimental settings where speakers referred to simple objects or colour swatches instantiating a single value in a colour space. Even in these controlled setti"
W17-3509,P12-3018,1,0.821994,"ilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-up: the system presents an image with several objects in a visual scene on the screen and the user’s task is to click on the object referred to. While generating and s"
W17-3509,W12-1814,1,0.854737,"ilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-up: the system presents an image with several objects in a visual scene on the screen and the user’s task is to click on the object referred to. While generating and s"
W17-3509,P05-3001,0,0.0423899,"n language. Theoretically, it is well known that this change in modality fundamentally changes human production of referring expressions: Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an increm"
W17-3509,D14-1086,0,0.0345634,"has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demons"
W17-3509,E09-1081,1,0.766258,"non-verbal reactions of the user (i.e. her mouse movements) and adapts the generated utterances to these actions in an incremental fashion. At the same time, the system tries to be as cooperative as possible: if the user shows no reaction for a certain amount of time, the previous expression is expanded, i.e. the system splits its referring expression over several utterances, which is usually known as “reference in installments”, cf. (Zarrieß and Schlangen, 2016). Figure 1 illustrates the architecture of Refer-iTTS, which conceptually follows the framework of the Incremental Unit (IU) model (Schlangen and Skantze, 2009), and two example interactions. User actions and the system’s generation and synthesis decisions happen concurrently, coordinated and monitored by an action manager (AM) module. Thus, besides decisions related to content planning and realization (e.g. attribute selection and ordering), a spoken installment-based REG system has to make a number of high-level decisions related to the delivery and timing of its own output. Using the Zarrieß and Schlangen (2016)’s generator, the system orders its 72 Proceedings of The 10th International Natural Language Generation conference, pages 72–73, c Santia"
W17-3509,W12-1504,0,0.0312451,"that this change in modality fundamentally changes human production of referring expressions: Given the real-time constraints of situated interaction, a speaker often has to start uttering before she has found the optimal expression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation ("
W17-3509,P16-1058,1,0.911868,"pression, but at the same time, she can observe the listener’s reaction while speaking and extend, adapt, or correct her referring expressions accordingly (Clark and Wilkes-Gibbs, 1986; Clark and Krych, 2004). Practically, spoken and interactive REG has been rarely studied empirically or implemented in realistic systems, but see (DeVault et al., 2005; Staudte et al., 2012; Striegnitz et al., 2012; Fang et al., 2014). We present Refer-iTTS, a system that is meant to support research on real-time spoken REG and builds upon recent approaches to REG from realworld images (Kazemzadeh et al., 2014; Zarrieß and Schlangen, 2016). We use the recently proposed words-as-classifiers (WAC) model for generation from low-level visual inputs and integrate it with InproTk (Baumann and Schlangen, 2012b), an opensource framework for incremental dialogue processing (http://wwwhomes.uni-bielefeld. de/dschlangen/inpro/). Importantly, InproTk features an incremental text-to-speech synthesis implementation (iTTS) (Baumann and Schlangen, 2012a) allowing for fine-grained, incremental manipulation of the audio signal (e.g. interruption, pausing, resumption, continuation). We will show an interactive demonstration of the following set-u"
W17-3516,D10-1049,0,0.025787,"iptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a secondary (optional) task, we will allow generation systems that accommodate processing in the other direction to compete on the task of semantic parsing, sp : x → z, or generating function representations from text input. 3.1 Main Research Questions Recent data-driven approaches in NLG have been successful in modeling end-to-end generation from unaligned input-output, cf. (Angeli et al., 2010; Mairesse and Young, 2014; Duˇsek and Jurcicek, 117 2015; Wen et al., 2016). However, these system have been mostly tested on datasets (e.g., in the restaurant domain) that require describing very similar entities, entities that are encoded in MRs that have considerable lexical overlap with the target text output. A central research question is whether these end-toend approaches scale to NLG settings that involve substantially harder lexicalization problems, such as with our datasets where the overlap is considerably less. Similarly, generating source code documentation also involves describi"
W17-3516,W13-2111,0,0.169779,"cerns the problem of generating well-formed, natural language descriptions from non-linguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL outp"
W17-3516,W09-0603,0,0.706082,"n input to function representations within known software libraries allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these rep"
W17-3516,W10-4217,0,0.0243533,"nguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarci"
W17-3516,W16-6626,0,0.0538738,"pairs for each language. By having two separate sets according to language, we can see whether generation quality differs between different types of programming languages. Taking an idea from the recent CoNLL 2017 shared task on dependency parsing, the third evaluation track will include examples from a surprise programming language that has not been observed during the training phase. The idea is to see how generation systems generalize to unobserved languages where the inputs vary slightly. 5 Evaluation, Baselines and Scheduling Following other data-to-text shared tasks (Banik et al., 2013; Colin et al., 2016) and previous work on text generating from code (Iyer et al., 2016), we will use automatic evaluation metrics such as BLEU and METEOR to evaluate system output. We will also perform fluency-based human evaluation on a subset of each test set using student volunteers from the Institute for Natural Language Processing (IMS), at the University of Stuttgart, Germany. To establish baseline results, we have already started a pilot study that uses phrase-based SMT to do generation. Such models have previously been used to establish strong baseline generation results (Belz and Kow, 2009; Wong and Moon"
W17-3516,P15-1044,0,0.0451726,"Missing"
W17-3516,P17-1017,0,0.0319106,"ning pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarcity problem, it also raises some new issues. For instance, sentences or utterances are produced by crowd-workers without much context, which puts to question the naturalness of the resulting text. Novikova et al. (2016) compare collecting data from lo"
W17-3516,P16-1195,0,0.223221,"o randomly generate input representations) yet it still corresponds to a formal language. We expect that there is relatively little lexical correspondence between source code representations and verbal descriptions and that this is an interesting challenge for data-driven NLG, as simple “alignment” methods might fail to predict lexicalization. While natural language generation in technical domains has long been of interest to the NLG community (Reiter et al., 1995), there has been renewed interest in this and other closely related topics over the last few years in NLP (Allamanis et al., 2015; Iyer et al., 2016; Yin and Neubig, 2017), making a shared task on the topic rather timely. While preparing the final version of this paper, we learned about the work of (Miceli Barone and Sennrich, 2017), who similarly look at generating text from automatically mined Python projects, using a similar set of tools as ours. This interest seems largely related to the wider availability of new data resources in the technical domain, especially through technical websites such as Github and StackOverflow. Rather than focus on unconstrained source code representations, as done in some of these studies, we believe that"
W17-3516,N12-1093,0,0.0214323,"ies allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions sh"
W17-3516,P09-1011,0,0.0881319,"elated Work Data-to-text generation concerns the problem of generating well-formed, natural language descriptions from non-linguistic, formal meaning representations (Gatt and Krahmer, 2017). In our case, the input to a given generation system is a source code representation. In order to learn a natural language generation (NLG) system from data, a parallel corpus containing pairs of inputs and outputs must be constructed. In many studies on data-to-text generation, these parallel resources are relatively small, cf. work on sportscasting (Chen and Mooney, 2008), weather reporting (Belz, 2008; Liang et al., 2009), or biology facts (Banik et al., 2013). We follow similar efforts to build automatic parallel resources (Belz and Kow, 2010) by mining example software libraries for (raw) pairs of short text descriptions and function representations. A recent trend is the use of crowd-sourcing to obtain parallel NLG data (Wen et al., 2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaur"
W17-3516,J14-4003,0,0.0196845,"presentations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a secondary (optional) task, we will allow generation systems that accommodate processing in the other direction to compete on the task of semantic parsing, sp : x → z, or generating function representations from text input. 3.1 Main Research Questions Recent data-driven approaches in NLG have been successful in modeling end-to-end generation from unaligned input-output, cf. (Angeli et al., 2010; Mairesse and Young, 2014; Duˇsek and Jurcicek, 117 2015; Wen et al., 2016). However, these system have been mostly tested on datasets (e.g., in the restaurant domain) that require describing very similar entities, entities that are encoded in MRs that have considerable lexical overlap with the target text output. A central research question is whether these end-toend approaches scale to NLG settings that involve substantially harder lexicalization problems, such as with our datasets where the overlap is considerably less. Similarly, generating source code documentation also involves describing a wide variety of funct"
W17-3516,I17-2053,0,0.208056,"Missing"
W17-3516,W16-6627,0,0.0160461,"from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets. 1 2. Python Documentation # from decimal.Context max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. aNALoGuE Challenge (Novikova and Rieser, 2016) MR input: name[Bibmbap House] food[French] priceRange[cheap], area[riverside] near[Clare Hall] NL output: Near Clare Hall, in the riverside area, Bibimbap serves French food in the price range cheap. Figure 1: Example source code documentation, or docstrings in 1-2, and an example MR/text pair. Introduction Source code libraries are collections of computer programs/instructions expressed in a target programming language that aim to solve some set of problems. Within these libraries, the designers of the code often use natural language to describe how various internal components work. For exam"
W17-3516,W16-6644,0,0.0128252,"2016; Novikova 116 et al., 2016; Gardent et al., 2017). Crowd-workers are presented with some meaning representation (MR, e.g., triples from a knowledge base) and asked to verbalize these representations in natural language. For example, the MR input in Figure 1.3 in the restaurant domain is verbalized as the NL output text. While this method allows for fast annotation, and thus solves the data scarcity problem, it also raises some new issues. For instance, sentences or utterances are produced by crowd-workers without much context, which puts to question the naturalness of the resulting text. Novikova et al. (2016) compare collecting data from logic-based MRs, of the type shown in Figure 1.3, and pictorial MRs, and find that the former approach leads to less natural and less informative descriptions. This seems to be related to the problem that the natural language sentence is a very close verbalization of the “logic” input, i.e., many terms in the MR can be simply taken up in the sentence. Our approach relies on naturally occurring verbal descriptions produced by human developers. Our input data (source code representations) seems more abstract than previously used representations e.g. in the restauran"
W17-3516,D17-2012,1,0.436876,"guage Processing, University of Stuttgart, Germany kyle@ims.uni-stuttgart.de ‡ Dialogue Systems Group // CITEC, Bielefeld University, Germany sina.zarriess@uni-bielefeld.de Abstract 1. Java Documentation *Returns the greater of two long values */ ... public static long max(long a, long b) We propose a new shared task for tactical datato-text generation in the domain of source code libraries. Specifically, we focus on text generation of function descriptions from example software projects. Data is drawn from existing resources used for studying the related problem of semantic parser induction (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a), and spans a wide variety of both natural languages and programming languages. In this paper, we describe these existing resources, which will serve as training and development data for the task, and discuss plans for building new independent test sets. 1 2. Python Documentation # from decimal.Context max(self, a, b): """"""Compares two values numerically and returns the maximum"""""" 3. aNALoGuE Challenge (Novikova and Rieser, 2016) MR input: name[Bibmbap House] food[French] priceRange[cheap], area[riverside] near[Clare Hall] NL output: Near Clare Hall, in the riversi"
W17-3516,N16-1015,0,0.0601972,"Missing"
W17-3516,N06-1056,0,0.0768151,"sivity of the generation input to function representations within known software libraries allows for more controlled experimentation. On the resource side, our datasets are taken from (Richardson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descri"
W17-3516,N07-1022,0,0.150289,"rdson and Kuhn, 2017b; Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the funct"
W17-3516,P17-1041,0,0.0390958,"input representations) yet it still corresponds to a formal language. We expect that there is relatively little lexical correspondence between source code representations and verbal descriptions and that this is an interesting challenge for data-driven NLG, as simple “alignment” methods might fail to predict lexicalization. While natural language generation in technical domains has long been of interest to the NLG community (Reiter et al., 1995), there has been renewed interest in this and other closely related topics over the last few years in NLP (Allamanis et al., 2015; Iyer et al., 2016; Yin and Neubig, 2017), making a shared task on the topic rather timely. While preparing the final version of this paper, we learned about the work of (Miceli Barone and Sennrich, 2017), who similarly look at generating text from automatically mined Python projects, using a similar set of tools as ours. This interest seems largely related to the wider availability of new data resources in the technical domain, especially through technical websites such as Github and StackOverflow. Rather than focus on unconstrained source code representations, as done in some of these studies, we believe that limiting the expressiv"
W17-3516,W13-2130,1,0.798733,"Richardson and Kuhn, 2017a). These resources have been used to study the problem of semantic parser induction, which is the inverse of the proposed data-to-text task. Given the close connection between the two tasks, there is often considerable overlap between the techniques used to solve either problem, techniques that are largely drawn from work on statistical machine translation (Wong and Mooney, 2006; Belz and Kow, 2009) and parsing (Zettlemoyer and Collins, 2012; Konstas and Lapata, 2012). While some approaches to generation explicitly use semantic parsing methods (Wong and Mooney, 2007; Zarrieß and Richardson, 2013), a more systematic investigation into the relation between these two tasks seems missing, which is a topic that we hope to address in this shared task. 3 Task Description Given a collection of datasets consisting of text x and function representation z pairs, or D = {(x, z)i }ni , the goal is to create a generation system that can produce well-formed, natural language descriptions from these representations, or gen : z → x. As discussed above, such descriptions should not only cover what the associated function does in general, but should also describe the function’s various parameters. As a"
W17-5529,D12-1008,0,0.0180129,"led 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phases of the call. Figure 2: Example interaction (gray: caller, white: t"
W17-5529,W12-1509,0,0.0152181,"led 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phases of the call. Figure 2: Example interaction (gray: caller, white: t"
W17-5529,W09-3942,0,0.0830098,"Missing"
W17-5529,W13-4062,0,0.0486303,"Missing"
W17-5529,P10-1103,0,0.0471879,"Missing"
W17-5529,W11-0101,0,0.0420632,"Missing"
W17-5529,E09-1081,1,0.740467,"eaker. Finally, Figure 4 illustrates the temporal sequencing 5 Related Work To the best of our knowledge, delayed information presentation has so far not been systematically studied. Various systems, however, addressed the problem in an ad-hoc manner. The 243 task information is not only not detrimental to the interaction, but might in fact be beneficial. From a broader perspective, we see this study on time buying as contributing to research on incremental generation and information presentation for dialogue systems, cf. (Skantze and Hjalmarsson, 2010), and incremental processing in general (Schlangen and Skantze, 2009). In this line of research, it is typically acknowledged that dialogue systems should be set up in a way such that they are able to start speaking before a complete plan of what to say has been built. Skantze and Hjalmarsson (2010) present a model for incremental generation that includes the ability to insert small speech segments for hesitations and fillers, in case the system has not fully planned the current utterance. It is unclear how such a system would be able to deal with scenarios similar to the ones we have investigated in this work. Similarly, other work has looked at appropriate ti"
W17-5529,W10-4301,0,0.0228819,"d filler, which can occur very frequently or rarely depending on the speaker. Finally, Figure 4 illustrates the temporal sequencing 5 Related Work To the best of our knowledge, delayed information presentation has so far not been systematically studied. Various systems, however, addressed the problem in an ad-hoc manner. The 243 task information is not only not detrimental to the interaction, but might in fact be beneficial. From a broader perspective, we see this study on time buying as contributing to research on incremental generation and information presentation for dialogue systems, cf. (Skantze and Hjalmarsson, 2010), and incremental processing in general (Schlangen and Skantze, 2009). In this line of research, it is typically acknowledged that dialogue systems should be set up in a way such that they are able to start speaking before a complete plan of what to say has been built. Skantze and Hjalmarsson (2010) present a model for incremental generation that includes the ability to insert small speech segments for hesitations and fillers, in case the system has not fully planned the current utterance. It is unclear how such a system would be able to deal with scenarios similar to the ones we have investig"
W17-5529,P04-1011,0,0.0376124,"ted via audio only, through high-quality headsets. Each agent handled 10 calls (from the same caller, but treating each as separate), after two training calls. We had 10 participants (balanced for gender), all native German speakers. To provide some control over the interaction, the task was set up so that after a greeting provided by a recording, C formulated their request in one turn (ostensibly, addressing a dialogue system that processed it) which A could hear, but not intervene Introduction How to best present information in a dialogue system is a central, and hence well-studied problem (Stent et al., 2004; Demberg and Moore, 2006; Rieser et al., 2010; Dethlefs et al., 2012b; Wen et al., 2015). What has received less attention is the question of what a system should do until it can present information, in the case that retrieval of this information takes time. 1 A domain in which it is, to this date, realistic that a request needs significant time to be processed, as anyone who has recently used flight search engines can attest. 241 Proceedings of the SIGDIAL 2017 Conference, pages 241–246, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics Figure 1: Phas"
W17-5529,W15-4639,0,0.045545,"Missing"
W17-5529,E06-1009,0,\N,Missing
W18-6547,P15-2017,0,0.0305367,"ication game where two players have the goal of finding each other in a visual environment. To reach this goal, the players need to describe images representing their current location. We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison. 1 Introduction Automatic description generation from real-world images has emerged as a key task in vision & language in recent years (Fang et al., 2015; Devlin et al., 2015; Vinyals et al., 2015; Bernardi et al., 2016), and datasets like Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014) or Microsoft CoCo (Lin et al., 2014; Chen et al., 2015) are typically considered to be general benchmarks for visual and linguistic image understanding. By exploiting these sizeable data collections and recent advances in computer vision (e.g. ConvNets, attention, etc.), image description models have 397 Proceedings of The 11th International Natural Language Generation Conference, pages 397–402, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Com"
W18-6547,D14-1086,0,0.157532,"”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected the GuessWhat? corpus of dialogues in which one player has to ask polar questions in order to identify the correct referent in the pool"
W18-6547,W16-6615,0,0.0802347,"been treated as a problem that can be addressed through the design of instructions (e.g., “do not give people names”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected th"
W18-6547,E17-4001,0,0.0233641,"Missing"
W18-6547,W17-3503,0,0.187571,"Missing"
W18-6547,W16-3207,0,0.233645,"Missing"
W18-6547,W10-0721,0,0.0482062,"linguistic definition and foundation of image description as a task remains unclear and is a matter of ongoing debate, e.g. see (van Miltenburg et al., 2017) for a conceptual discussion of the task from a cross-lingual perspective. According to (Bernardi et al., 2016), image description generation involves generating a textual description (typically a sentence) that verbalizes the most salient aspects of the image. In practice, however, researchers have observed that eliciting descriptions from naive subjects (i.e. mostly crowd-workers) at a consistent level of quality is a non-trivial task (Rashtchian et al., 2010), as workers seem to interpret the task in different ways. Thus, previous works have developed relatively elaborate instructions and quality checking conventions for being able to systematically collect image descriptions. In this paper, we argue that problems result from the fact that the task is typically put to the workers without providing any further context. This entirely monological setting essentially suggests that determining the salient aspects of an image (like highly important objects, object properties, scene properties) can be solved in a general, “neutral” way, by humans and sys"
W18-6547,P16-1115,1,0.857683,". However, it has been treated as a problem that can be addressed through the design of instructions (e.g., “do not give people names”, “do not describe unimportant details”, (Chen et al., 2015)). (van Miltenburg et al., 2016; van Miltenburg, 2017) later investigated the range of pragmatic phenomena to be found in such caption corpora, with the conclusion that the instructions do not sufficiently control for them and leave it to the labellers to make their own decisions. It is one contribution of the present paper to show that providing a task context results in more constrained descriptions. Schlangen et al. (2016) similarly noted that referring expressions in a corpus that was collected in a (pseudo-)interactive setting (Kazemzadeh et al., 2014), where the describers were provided with immediate feedback about whether their expression was understood, were more concise than those collected in a monological setting (Mao et al., 2016). Similar to MeetUp, the use of various dialogue game set-ups has lately been established for dialogue data collection. Das et al. (2017) designed the “Visual Dialog” task where a human asks an agent about the content of an image. De Vries et al. (2017) similarly collected th"
W18-6547,E12-2021,0,0.0579231,"Missing"
W18-6547,N03-1033,0,0.025141,"Missing"
W18-6547,Q14-1006,0,0.147059,"to describe images representing their current location. We analyse a dataset from this domain and show that the nature of image descriptions found in MeetUp! is diverse, dynamic and rich with phenomena that are not present in descriptions obtained through a simple image captioning task, which we ran for comparison. 1 Introduction Automatic description generation from real-world images has emerged as a key task in vision & language in recent years (Fang et al., 2015; Devlin et al., 2015; Vinyals et al., 2015; Bernardi et al., 2016), and datasets like Flickr8k (Hodosh et al., 2013), Flickr30k (Young et al., 2014) or Microsoft CoCo (Lin et al., 2014; Chen et al., 2015) are typically considered to be general benchmarks for visual and linguistic image understanding. By exploiting these sizeable data collections and recent advances in computer vision (e.g. ConvNets, attention, etc.), image description models have 397 Proceedings of The 11th International Natural Language Generation Conference, pages 397–402, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics various map locations. While similar in some respects, MeetUp is distinguished by being a symmetrical tas"
W18-6563,W17-3207,0,0.111961,"r activation functions. Mao et al. (2015), on the other hand, adopt a simple linear layer for decoding the LSTM. In the following, we will investigate how these modeling decisions affect the performance, and how they interact with different search methods during inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (B"
W18-6563,D15-1224,0,0.0889948,"y different than determining the length of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrease performance as comp"
W18-6563,D17-1210,0,0.0260847,"Missing"
W18-6563,D17-1227,0,0.0245425,"ring inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (Bahdanau et al., 2014) uses a shrinking beam where beam size is reduced each time a completed hypothesis is found, and search terminates when the beam size has reached 0. Another shortcoming of beam search observed in previous work is that the beam"
W18-6563,D14-1086,0,0.785488,"am search, is conceptually different than determining the length of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrea"
W18-6563,P17-4012,0,0.177613,"el according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source sentence, which is not available in REG. Another common NMT framework (Bahdanau et al., 2014) uses a shrinking beam where beam size is reduced each time a completed hypothesis is found, and search terminates when the beam size has reached 0. Another shortcoming of beam search observed in previous work is that the beam tends to contain many candidates that share the same (m"
W18-6563,J03-1003,0,0.0633258,"Missing"
W18-6563,J12-1006,0,0.444858,"Missing"
W18-6563,1983.tc-1.13,0,0.75122,"Missing"
W18-6563,W10-4210,0,0.304201,"Missing"
W18-6563,P02-1040,0,0.102814,"short. However, the fact that CIDEr scores still improve in some cases suggests that beam search leads to linguistically more well-formed expressions (expressions with a lot of repetitions are avoided, e.g. the blue blue shirt). they collected additional expressions for the testsets, resulting in 10 expressions per objects. As we did not have access to these additional expressions at the time of writing, we follow Yu et al. (2016) and evaluate on the original RefCOCO collections with 3 expressions on average per object. In the experiments below, we look at three measures: BLEU1 for unigrams (Papineni et al., 2002), CIDEr (Vedantam et al., 2015) and lenr (length ratio) as provided by the MSCOCO evaluation server (Chen et al., 2015). We are interested in the length ratio as a simple approximation of traditionally used measures in REG (Gatt and Belz, 2010), reflecting whether the generation output contains too much or too little information (attributes or words). BLEU1 gives us an indication of the lexical overlap between output and target, whereas CIDEr operates on the level of n-grams. 6.2 6.3 Modified beam search In Table 3, we report performance of the region and global model with the linear decoder a"
W18-6563,P16-1058,1,0.923285,"of a good translation: a translation is complete when it covers the meaning of the words in the source sentence, and indeed, the length of the source is used as criterion in beam search for MT (see Section 2.3). A referring expression, on the other hand, is complete when it describes the visual target in a pragmatically adequate way, i.e. when it neither provides too little nor too much information. 2.2 Neural REG from real-world images More recently, research on REG has started to investigate set-ups based on real-world images (Kazemzadeh et al., 2014; Gkatzia et al., 2015; Mao et al., 2015; Zarrieß and Schlangen, 2016), representing scenes with many different types of real-world objects. Here, the input to the REG system is defined as a low-level visual representation such that various aspects of the task have to be addressed, including lexicalization and content selection. Inspired by research on image captioning, Mao et al. (2015) proposed the first neural end-to-end model for REG that uses a CNN to repWe explore a range of different variants of beam search that have been proposed for MT and, interestingly, find that most of them decrease performance as compared to simple greedy decoding in REG. Whereas g"
W18-6563,D13-1022,0,0.024316,"who also use dropout in the decoding layer, but only linear activation functions. Mao et al. (2015), on the other hand, adopt a simple linear layer for decoding the LSTM. In the following, we will investigate how these modeling decisions affect the performance, and how they interact with different search methods during inference.2 We distinguish two variants of our model according to their decoding layer: We now turn to (neural) MT, where decoding algorithms for sequence generation have been investigated in detail. Here, beam search is the standard method for syntax- and phrase-based models (Rush et al., 2013), as well as for neural encoderdecoders (Freitag and Al-Onaizan, 2017). However, an important difference between the two is that candidates in phrase-based MT are completed in the same number of steps, whereas neural models generate hypotheses of different length and are biased for shorter output (Huang et al., 2017). To counteract this bias, OpenNMT (Klein et al., 2017) adopts three metrics for normalizing the coverage, length and end of sentence of candidate translations. Unfortunately, two of these metrics (coverage and end of sentence normalization) are based on the length of the source se"
W18-6563,P16-1115,1,0.84577,"entation that the LSTM is conditioned on. Whereas Mao et al. (2015) extract visual representations of the region representing the target referent and the global image, Yu et al. (2016) report a slightly detrimental effect of including these global context features. Thus, we distinguish two variants of the model according to its visual representation: Target: 4103-dimensional vector, obtained by cropping the image to the target region, resizing to 224 × 224, extracting its CNN pre-softmax features with VGG19 (Simonyan and Zisserman, 2014) and concatenating 7 spatial features of the region (see Schlangen et al. (2016) for these) Global+target: 8119-dimensional vector, obtained by extracting the CNN pre-softmax features with VGG19 (Simonyan and Zisserman, 2014) for the entire image, and concatenating it with targetonly Training We set the word embedding layer size to 512, and the hidden state to 1024. We optimized with ADAM (with α = 0.001), and the batch size set to 50. The word embedding layer is initialized with random weights. The number of training epochs was tuned for each model on the validation set. 4 Decoding Strategies lp(y) = We now explain the different decoding strategies that will be combined"
W18-6906,W13-4063,0,0.030859,"at to say that matters: an utterance that is appropriate at a particular point in time, might already be perceived as inappropriate or confusing shortly after. To the best of our knowledge, aspects of monitoring and timing have not been addressed in datadriven NLG frameworks, though incremental processing has been shown to be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right"
W18-6906,W17-5527,0,0.013082,"s inappropriate or confusing shortly after. To the best of our knowledge, aspects of monitoring and timing have not been addressed in datadriven NLG frameworks, though incremental processing has been shown to be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right 5 6 7 8 IG: yes IG: turn left . yes IG: a little more IG: so that it's diagonal IG: a little more IG: exac"
W18-6906,P15-2017,0,0.0227588,"re the task would be to generate a verbal instruction that enables the IF to execute a particular action or achieve a state change of the environment, while the system (the IG) is given the current and the goal state of an environment as an image. This would be natural extension of existing language generation systems that are able to generate descriptions of real-world images (Bernardi et al., 2016), or referring expressions to objects in real-world images (Yu et al., 2017). At the same time, it would require systems to go beyond the commonly used CNN-LSTM architecture (Vinyals et al., 2015; Devlin et al., 2015; Mao et al., 2016; Yu et al., 2017) as these currently only map visual representations of single images or objects to verbal output. Instead, a visually grounded instruction generation system needs to reason about expressions that relate the current visual state to a target state, such as place the block to the right (source state) as the highest block on the board (target state) in Figure 1. Conceptually, the problem of generating instructions in object assembly domains is similar to generating relational referring expressions • vision: generating instructions from a lowlevel visual represen"
W18-6906,W12-1504,0,0.0162724,"ctures (Andreas et al., 2016). However, none of these models is designed for generating relational structures in verbal expressions, such as instructions. 3 Spoken language dynamics From research on situated spoken dialogue, it is well known that spoken and written language bear very different affordances. In spoken communication, listeners react, both non-verbally and verbally, to what speakers are saying, while they are saying it; and speakers adapt what they are saying, based on the reactions (or lack thereof) that they get, while they are speaking. The field of Conversation Analysis (see (Stivers and Sidnell, 2012) for a recent overview) and, taking up and further developing some of their ideas, the work of Herbert (Clark, 1996) has done much to shed light on the intricate strategies that interactants follow to coconstruct dialogue in this way. Figure 2 illustrates some prominent strategies that speakers use to achieve task success in spoken communication, with an instruction giving example taken from our PentoRef data (Zarrieß et al., 2016). Here, the IF has to assemble an object out of Pentomino pieces while the IG observes his actions over a camera feed. During a time span of approximately 30 seconds"
W18-6906,W11-2845,0,0.0340559,"stems: here, a human instruction follower (IF) and an agent as the instruction giver (IG) have to achieve a common goal in a visual environment (e.g. find a route or treasure, assemble an object). The IG knows how to complete the task (e.g. where the treasure is, how the object looks like) but cannot affect the environment. The IF can affect the environment and the objects in it, but needs the IG’s instructions to achieve the goal. In the context of the GIVE challenge (Byron et al., 2007), this setting has received considerable attention in the NLG community for some time (Byron et al., 2009; Striegnitz et al., 2011), but has not been developed further since then. Generally, we believe that future approaches to instruction giving in NLG should extend GIVE along the following dimensions, in order to enable transfer of NLG technology to real-world applications like robots or dialogue systems: Figure 1: Instruction example in the BLOCKS data set (Bisk et al., 2018) the visually present objects and their properties. In the meantime, a lot of research in human-robot interaction has be done on modeling instructions in more realistic visual environments, though this community has often focussed on grounding verb"
W18-6906,D14-1086,0,0.0275169,"a given input to some written output, meaning that the environment does not change while the system is producing output Introduction The past decade has seen substantial progress in data-driven methods for natural language generation (NLG). It is now widely agreed that datadriven techniques are needed to obtain NLG systems that are adaptive and human-like (Belz, 2008), domain-independent (Wen et al., 2016), and – with recent methods from vision & language cf. (Bernardi et al., 2016) – suitable for agents that interact with humans in a physical environment (such as dialogue systems or robots) (Kazemzadeh et al., 2014). Despite this progress, however, data-driven NLG is rarely used in current real-world interactive systems, where more traditional (template-based) approaches for generating verbal output still persist. • perfect input: NLG systems are often trained on perfect representations of an environment or a knowledge base • one-shot output: NLG systems do not need to monitor whether the listener has actually understood the output, strategies that are frequent in conversation (revision, correction, installments) do not have to be considered • no temporal dimension: NLG systems assume that their output i"
W18-6906,P17-1063,0,0.0221661,"task-oriented conversation in shared visual space from (Zarrieß et al., 2016): the joint task for the IF and IG is to build a puzzle out of Pentomino pieces where the IF can manipulate pieces on a physical gameboard and the IG sees the outline of the puzzle, observes the IF’s actions in real-time (over a camera feed) and instructs the IF over headphones; the overall interaction time shown here is approx. 30 secconds; utterances have been translated to English from German transciptions hand-crafted strategies for repair and revision very clearly improve the referential success of the system. (Villalba et al., 2017) propose a formal approach to generating contrastive referring expressions which is designed for similar scenarios. What is clearly missing to date, however, is a data-driven NLG framework that encompasses these various aspects of conversational grounding and timing in interaction. 5 of the IEEE Conference on Computer Vision and Pattern Recognition, pages 39–48. Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Natural Language Engineering, 14(4):431455. Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erku"
W18-6906,J12-1006,0,0.0228558,"Missing"
W18-6906,D16-1233,0,0.0391542,"Missing"
W18-6906,L16-1019,1,0.928306,"speakers adapt what they are saying, based on the reactions (or lack thereof) that they get, while they are speaking. The field of Conversation Analysis (see (Stivers and Sidnell, 2012) for a recent overview) and, taking up and further developing some of their ideas, the work of Herbert (Clark, 1996) has done much to shed light on the intricate strategies that interactants follow to coconstruct dialogue in this way. Figure 2 illustrates some prominent strategies that speakers use to achieve task success in spoken communication, with an instruction giving example taken from our PentoRef data (Zarrieß et al., 2016). Here, the IF has to assemble an object out of Pentomino pieces while the IG observes his actions over a camera feed. During a time span of approximately 30 seconds, the IG produces 18 short utterances in total that instruct the IF what to do next (e.g. turn to the left), confirm the IF’s action (exactly), or repair what she is currently doing (to the left, this is to the right). Also, interestingly, the final step of the instruction (i.e. how to put the target piece to its target location, image 10-12 in Figure 2) is left underspecified by the IG as it is obvious to the IF how to complete th"
W18-6906,P16-1058,1,0.82785,"be highly effective in experimental or rule-based settings, cf. (Skantze and Hjalmarsson, 2013; Skantze et al., 2014; Buß and Schlangen, 2010). In the dialogue community, specific tasks that involve timing have been modelled in a data-driven way, such as barge-in detection (Selfridge et al., 2013), end-of-utterance detection (Raux and Eskenazi, 2012; Maier et al., 2017)), or turn-taking (Skantze, 2017) . Even less work has been carried out on NLG systems that are able to produce revision, repair or correction utterances which can be essential to achieve task success, as shown in Figure 2. In (Zarrieß and Schlangen, 2016), we have explored an installment-based approach in a referring expression generation system for objects in real-world images, and found that even simple, 29 1 IG: then you take the green W ... top right 2 3 4 IG: and you turn it to the left IG: uh now it's to the right 5 6 7 8 IG: yes IG: turn left . yes IG: a little more IG: so that it's diagonal IG: a little more IG: exactly IG: and now put to the left next to the T 9 10 11 12 IG: yes exactly IG: like this IG: exactly IG: into this spot IG: now you take the pink piece over there ... IG: to the left IG: this is to the right Figure 2: Example"
W19-8618,J12-1006,0,0.070837,"Missing"
W19-8618,P02-1040,0,0.107222,"tribute descriptions of objects. We investigate whether object sketches lead to improvements in neural generation of descriptions of real-world objects, especially for attributes related to shape and orientation. In the following, we present our ongoing work on generating detailed attribute descriptions of object by grounding in images and hand-drawn sketches. We first introduce the Draw-and-Tell dataset (Section 3), then describe a basic recurrent neural network architecture for generating attribution descriptions (Section 4). We carry out an automatic evaluation based on measures like BLEU (Papineni et al., 2002), vocabulary size, and the average length of generated descriptions. In addition, we provide a qualitative analysis and discussion on how incorporating sketches can benefit the task of generating fine-grained attribute descriptions. to integrate realistic and abstract visual inputs. 2 3 Multimodal Embedding Space For being able to model REG with multiple input modalities (images and sketches), we need to be able to represent these inputs as visual embeddings or features transferred from a CNN. Here, we rely on previous work that has mapped different modalities into joint vector spaces, as in t"
W19-8618,W17-3506,0,0.0198129,"network was trained and optimized to project photo vectors as close as possible to corresponding sketch vectors, while in the mean time, as distinguishable as possible from other photo vectors. In this work, we used the pretrained models and took the output vector from the last fully connected layers as feature vectors (in 1024 dimension) to represent images and sketches. Next, we describe how we train a natural language generation model with the extracted vectors. Considering the small size of the Draw-and-Tell dataset, we built a basic Recurrent Neural Network model for the generation task (Tanti et al., 2017). The model takes a text vector and a visual feature vector as inputs, and predicts a sequence of tokens to describe the target object in the input visual vector. Therefore, the generated tokens are conditioned on the input visual feature vector. The network includes an embedding layer, an LSTM layer, and a softmax layer. We encode word tokens as a one-hot vector, and concatenate the vector with image feature vectors (i.e., injecting image information into the network). An embedding layer takes the concatenated vector as input. The size of the LSTM layer is 512. The model was implemented using"
W19-8618,I17-2061,1,0.888293,"linguistically and lexically relatively constrained, and does not cover language that can be used to describe fine-grained differences between object parts and shapes (see Section 2) . Thus, Achlioptas et al. (2019) propose to go back to carefully designed datasets with graphical, abstract objects in order to elicit complex descriptions of object attributes and also to have access to more fine-grained representations of an object’s geometry and topology. We explore modeling of fine-grained attribute descriptions of objects in real-world images, based on the Draw-and-Tell dataset introduced by Han and Schlangen (2017). This dataset was collected in a controlled procedure akin to traditional REG setups, resulting in fine-grained attribute descriptions and a rich vocabulary, see Figure 1 for an example. As the Draw-and-Tell data was originally designed for sketch-based image retrieval (Eitz et al., 2012; Sangkloy et al., 2016), each image is paired with hand-drawn sketches depicting the object in it. As illustrated in Figure 1(b), these sketches are somewhat distorted and abstract away from many visual properties of the complex real-world objects (e.g. colour). Yet they provide a clear outline of the object’"
W19-8618,W18-6563,1,0.904854,"y given through sketches can help such a model to learn to accurately ground detailed language referring expressions to object shapes. Our results are encouraging. 1 (a) (b) Figure 1: (a) Photo of a starfish; (b) Sketch of the starfish in (a). Starfish attribute description: Top view, legs bend, thin legs, on sand. Introduction Recent work in referring expression generation (REG) has focused more and more on large-scale image datasets (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016) and models that incorporate a state-of-the-art vision component (Mao et al., 2016; Yu et al., 2017; Zarrieß and Schlangen, 2018). As compared to traditional REG settings (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012), these works have led to substantial advances in terms of the complexity of visual inputs that can be processed and the visual object categories that can be covered. At the same time, it is questionable whether these recent benchmarks for real-world REG constitute an equally big step forward in terms of the language that needs to be modeled. As noted by Achlioptas et al. (2019), the vocabulary and attributes learnt by state-of-the-art REG models is linguistically and lexically relatively constraine"
W19-8618,D14-1086,0,0.138051,"y, it constitutes an interesting challenge for CNN-LSTM architectures used in state-ofthe-art image captioning models. We explore whether the additional modality given through sketches can help such a model to learn to accurately ground detailed language referring expressions to object shapes. Our results are encouraging. 1 (a) (b) Figure 1: (a) Photo of a starfish; (b) Sketch of the starfish in (a). Starfish attribute description: Top view, legs bend, thin legs, on sand. Introduction Recent work in referring expression generation (REG) has focused more and more on large-scale image datasets (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016) and models that incorporate a state-of-the-art vision component (Mao et al., 2016; Yu et al., 2017; Zarrieß and Schlangen, 2018). As compared to traditional REG settings (Dale and Reiter, 1995; Krahmer and Van Deemter, 2012), these works have led to substantial advances in terms of the complexity of visual inputs that can be processed and the visual object categories that can be covered. At the same time, it is questionable whether these recent benchmarks for real-world REG constitute an equally big step forward in terms of the language that needs to be mod"
W19-8621,D14-1086,0,0.130205,"compose linguistic expressions from that, for the given addressee and under the constraints of the given communicative intention. Many of the decisions involved in this do not only require general visual and linguistic competences, but are well-known to be affected by the task, the context and the intended addressee. Consequently, recent progress in the area of NLG, Language & Vision has been made by moving from generic settings like image captioning (Lin et al., 2014; Chen et al., 2015; Hodosh et al., 2013; Plummer et al., 2015) to task-oriented settings like referring expression generation (Kazemzadeh et al., 2014; Yu et al., 2016) or interactive visual question answering (Das et al., 2017; De Vries et al., 2017). As shown by Ilinykh et al. (2018), task-based image descriptions substantially differ in terms of their linguistic properties (e.g. occurrence of referring expressions, attribute types) from their “neutral” counterparts. ∗ 1 See (Gatt and Krahmer, 2018) for a survey on this traditional area in NLG. 2 This setting is somewhat similar to that of Lin et al. (2015), who collected texts meant to describe a scene to someone who can’t see it, but it is tuned even more towards (imagined) interaction."
W19-8621,J95-2003,0,0.309365,"Missing"
W19-8621,J86-3001,0,0.781078,"Missing"
W19-8621,W18-6547,1,0.845366,"the decisions involved in this do not only require general visual and linguistic competences, but are well-known to be affected by the task, the context and the intended addressee. Consequently, recent progress in the area of NLG, Language & Vision has been made by moving from generic settings like image captioning (Lin et al., 2014; Chen et al., 2015; Hodosh et al., 2013; Plummer et al., 2015) to task-oriented settings like referring expression generation (Kazemzadeh et al., 2014; Yu et al., 2016) or interactive visual question answering (Das et al., 2017; De Vries et al., 2017). As shown by Ilinykh et al. (2018), task-based image descriptions substantially differ in terms of their linguistic properties (e.g. occurrence of referring expressions, attribute types) from their “neutral” counterparts. ∗ 1 See (Gatt and Krahmer, 2018) for a survey on this traditional area in NLG. 2 This setting is somewhat similar to that of Lin et al. (2015), who collected texts meant to describe a scene to someone who can’t see it, but it is tuned even more towards (imagined) interaction. We also collected data for about 4 times as many images. Work done while at Bielefeld University. 152 Proceedings of The 12th Internati"
W19-8621,P16-1058,1,0.885744,"Missing"
ziering-etal-2012-corpus,W07-1203,0,\N,Missing
ziering-etal-2012-corpus,P11-1101,1,\N,Missing
ziering-etal-2012-corpus,rohrer-forst-2006-improving,0,\N,Missing
