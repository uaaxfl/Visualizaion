2008.amta-papers.20,P07-2045,0,0.0052993,"e accepts a term and domain as input, and transforms the given term into an expanded query. The query is submitted to a search engine to retrieve mixed-code documents. After that, TermMine extracts candidate translations in the returned document snippets and ranks them according to surface patterns, frequency, and distance from the given term. In our prototype, TermMine returns the term translations to the user directly; alternatively, these term translations can be used as additional input to a traditional machine translation system. Some statistical machine translation tools, such as Moses (Koehn et al, 2007), accept the source sentence with preselected word translation candidates. In this way, we may be able to use DSTs to improve the performance of MT systems. The rest of the paper is organized as follows. We review the related work in the next section. Then we present our method for automatically learning to expand a given term into an effective query for a domain (Section 3). We describe the experiments carried out to assess the proposed method. As part of our evaluation, we compare the quality of the translations by TermMine against a state-of-the-art machine translation system and other IR-b"
2008.amta-papers.20,J90-2002,0,0.662487,"Missing"
2008.amta-papers.20,J93-2003,0,0.0221253,"Missing"
2008.amta-papers.20,N04-1034,0,0.0321181,"ased statistical models. While SMT systems establish a general translation model from a parallel corpus, we propose a method for learning to find domain-specific translations via Web search. Parallel corpora are considered better data sources for quality translation information but are limited by lower availability. As an effort to cope with the data sparseness problem, Fung and Yee (1998) advocated using comparable corpora, which are considered more readily available in large quantity. Shao and Ng (2004) described a similar method for extracting new word translations from comparable corpora. Munteanu et al. (2004) also described a system that extracts parallel sentences from comparable corpora as additional training data for SMT systems. Recent research has begun to emphasize phrase translation to improve on word-based SMT approach. Cao and Li (2002) proposed an approach for translating short, base noun phrases based on a bilingual dictionary. The translation candidates for words in a phrase are combined and validated using Web page counts. Koehn and Knight (2003) described a system that builds a noun phrase-based translation subsystem leading to further We present a new terminology translation system,"
2008.amta-papers.20,C02-1011,0,0.0177094,"quality translation information but are limited by lower availability. As an effort to cope with the data sparseness problem, Fung and Yee (1998) advocated using comparable corpora, which are considered more readily available in large quantity. Shao and Ng (2004) described a similar method for extracting new word translations from comparable corpora. Munteanu et al. (2004) also described a system that extracts parallel sentences from comparable corpora as additional training data for SMT systems. Recent research has begun to emphasize phrase translation to improve on word-based SMT approach. Cao and Li (2002) proposed an approach for translating short, base noun phrases based on a bilingual dictionary. The translation candidates for words in a phrase are combined and validated using Web page counts. Koehn and Knight (2003) described a system that builds a noun phrase-based translation subsystem leading to further We present a new terminology translation system, TermMine, that automatically learns to find domain-specific term translations. TermMine uses an unsupervised training method to learn effective query expansions (QE) automatically during training by analyzing a set of terms for each domain"
2008.amta-papers.20,W01-1413,0,0.0162016,"se-to-phrase translations induced from word alignment information. The phrase-based approach significantly improves the translation quality. However, in the situations of single-word noun phrases, phrase-based approach has problem producing correct translation. In contrast, we propose to use domain information, provided by the user or automatically derived from the (sentential or paragraph) context, to help find more appropriate translations. Round the same time, researchers began to turn to the Web and proposed new methods for translating phrases. In a study more closely related to our work, Nagata et al. (2001) introduced a system for extracting the English translations of a given Japanese technical term by collecting and scoring translation candidates co-occurring with the given term in mixed-code Web pages. In addition to using Web texts, Lu et al. (2002) proposed a new method that uses anchor texts and hyperlink structure to find term translations for cross-language information retrieval. More recently, Wu et al. (2005) introduced a method for learning source-target surface patterns to find translation of proper names and technical terms on the Web. Our setting, approach, and evaluation are subst"
2008.amta-papers.20,C04-1089,0,0.0207021,"1993) described how to automatically align words and translations in the parallel sentences and build word-based statistical models. While SMT systems establish a general translation model from a parallel corpus, we propose a method for learning to find domain-specific translations via Web search. Parallel corpora are considered better data sources for quality translation information but are limited by lower availability. As an effort to cope with the data sparseness problem, Fung and Yee (1998) advocated using comparable corpora, which are considered more readily available in large quantity. Shao and Ng (2004) described a similar method for extracting new word translations from comparable corpora. Munteanu et al. (2004) also described a system that extracts parallel sentences from comparable corpora as additional training data for SMT systems. Recent research has begun to emphasize phrase translation to improve on word-based SMT approach. Cao and Li (2002) proposed an approach for translating short, base noun phrases based on a bilingual dictionary. The translation candidates for words in a phrase are combined and validated using Web page counts. Koehn and Knight (2003) described a system that buil"
2008.amta-papers.20,P98-1069,0,0.0595308,"subsequently the word translation ambiguity is resolved by using an n-gram model of the target language. Brown et al. (1993) described how to automatically align words and translations in the parallel sentences and build word-based statistical models. While SMT systems establish a general translation model from a parallel corpus, we propose a method for learning to find domain-specific translations via Web search. Parallel corpora are considered better data sources for quality translation information but are limited by lower availability. As an effort to cope with the data sparseness problem, Fung and Yee (1998) advocated using comparable corpora, which are considered more readily available in large quantity. Shao and Ng (2004) described a similar method for extracting new word translations from comparable corpora. Munteanu et al. (2004) also described a system that extracts parallel sentences from comparable corpora as additional training data for SMT systems. Recent research has begun to emphasize phrase translation to improve on word-based SMT approach. Cao and Li (2002) proposed an approach for translating short, base noun phrases based on a bilingual dictionary. The translation candidates for wo"
2008.amta-papers.20,P05-3010,1,0.89997,"ropriate translations. Round the same time, researchers began to turn to the Web and proposed new methods for translating phrases. In a study more closely related to our work, Nagata et al. (2001) introduced a system for extracting the English translations of a given Japanese technical term by collecting and scoring translation candidates co-occurring with the given term in mixed-code Web pages. In addition to using Web texts, Lu et al. (2002) proposed a new method that uses anchor texts and hyperlink structure to find term translations for cross-language information retrieval. More recently, Wu et al. (2005) introduced a method for learning source-target surface patterns to find translation of proper names and technical terms on the Web. Our setting, approach, and evaluation are substantially different from other Web-based translation approach. More recently, Web-based term translation systems have begun to expand queries to increase the chance of retrieving snippets that contain translations. Huang et al. (2005) proposed a pseudo relevance feedback approach for improving search results by augmenting the second round query with translations of high-frequency words found in the first-round returne"
2008.amta-papers.20,P03-1040,0,0.0134582,"dily available in large quantity. Shao and Ng (2004) described a similar method for extracting new word translations from comparable corpora. Munteanu et al. (2004) also described a system that extracts parallel sentences from comparable corpora as additional training data for SMT systems. Recent research has begun to emphasize phrase translation to improve on word-based SMT approach. Cao and Li (2002) proposed an approach for translating short, base noun phrases based on a bilingual dictionary. The translation candidates for words in a phrase are combined and validated using Web page counts. Koehn and Knight (2003) described a system that builds a noun phrase-based translation subsystem leading to further We present a new terminology translation system, TermMine, that automatically learns to find domain-specific term translations. TermMine uses an unsupervised training method to learn effective query expansions (QE) automatically during training by analyzing a set of terms for each domain of interest, and extracts indicative target words for each individual domain. For example, TermMine learns that “ 市 場 ” and “ 價 格 ” are important TL keywords for the FINANCE domain because it occurs frequently in mixed"
D07-1106,C02-1011,0,0.0200485,"h to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search. Nagata et al. (2001) described how to exploit proximity and redundancy to extract translation for a given term. Lu, Chien, and Lee (2002) describe a method for name translation based on mining of anchor texts. More recently, Zhang, Huang, and Vogel (2005) proposed to use occurring words to expand queries for searching and extracting transliterations. Oh and Isahara (2006) use phonetic-similarity to recognize transliteration pairs on th"
D07-1106,2003.mtsummit-papers.17,0,0.0306915,"# of correct 263 67 23 answers Applicability 0.60 0.29 0.09 Precision 0.89 0.46 0.52 Recall 0.53 0.13 0.05 F-measure 0.66 0.21 0.08 6 Comparison with Previous Work Machine transliteration has been an area of active research. Most of the machine transliteration method attempts to model the transliteration process of mapping between graphemes and phonemes. Knight and Graehl (1998) proposed a multilayer model and a generate-and-test approach to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search"
D07-1106,W03-1502,0,0.0326456,"Missing"
D07-1106,J03-3001,0,0.121625,"Missing"
D07-1106,J98-4003,0,0.15155,"ability 0.48 0.60 # Correct Answers 209 263 Precision 0.88 0.89 Recall 0.42 0.53 F-measure 0.57 0.66 Table 11. Performance evaluation of three systems Yahoo! Method TermMine Google QE+ Translate Babelfish Evaluation # of cases done 300 146 44 # of correct 263 67 23 answers Applicability 0.60 0.29 0.09 Precision 0.89 0.46 0.52 Recall 0.53 0.13 0.05 F-measure 0.66 0.21 0.08 6 Comparison with Previous Work Machine transliteration has been an area of active research. Most of the machine transliteration method attempts to model the transliteration process of mapping between graphemes and phonemes. Knight and Graehl (1998) proposed a multilayer model and a generate-and-test approach to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and"
D07-1106,P03-1040,0,0.0310623,"Missing"
D07-1106,P93-1003,0,0.154202,"Missing"
D07-1106,W03-0317,1,0.85599,"ultilayer model and a generate-and-test approach to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search. Nagata et al. (2001) described how to exploit proximity and redundancy to extract translation for a given term. Lu, Chien, and Lee (2002) describe a method for name translation based on mining of anchor texts. More recently, Zhang, Huang, and Vogel (2005) proposed to use occurring words to expand queries for searching and extracting transliterations. Oh and Isahara (2006) use phonetic-simil"
D07-1106,P04-1021,0,0.0338426,"answers Applicability 0.60 0.29 0.09 Precision 0.89 0.46 0.52 Recall 0.53 0.13 0.05 F-measure 0.66 0.21 0.08 6 Comparison with Previous Work Machine transliteration has been an area of active research. Most of the machine transliteration method attempts to model the transliteration process of mapping between graphemes and phonemes. Knight and Graehl (1998) proposed a multilayer model and a generate-and-test approach to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search. Nagata et al. (2001)"
D07-1106,lin-etal-2004-extraction,1,0.904594,"Missing"
D07-1106,W01-1413,0,0.296795,"and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search. Nagata et al. (2001) described how to exploit proximity and redundancy to extract translation for a given term. Lu, Chien, and Lee (2002) describe a method for name translation based on mining of anchor texts. More recently, Zhang, Huang, and Vogel (2005) proposed to use occurring words to expand queries for searching and extracting transliterations. Oh and Isahara (2006) use phonetic-similarity to recognize transliteration pairs on the Web. In contrast to previous work, we propose a simple method for extracting transliterations based on a statistical model trained automatically on a bilingual name list via unsup"
D07-1106,I05-1040,0,0.0238128,"Missing"
D07-1106,P04-1024,0,0.0161031,"nd phonemes. Knight and Graehl (1998) proposed a multilayer model and a generate-and-test approach to perform back transliteration from Japanese to English based on the model. In our work we address an issue of producing transliteration by way of search. Goto et al. (2003), and Li et al. (2004) proposed a grapheme-based transliteration model. Hybrid transliteration models were described by AlOnaizan and Knight (2002), and Oh et al. (2005). Recently, some of the machine transliteration study has begun to consider the problem of extracting names and their transliterations from parallel corpora (Qu and Grefenstette 2004, Lin, Wu and Chang 2004; Lee and Chang 2003, Li and Grefenstette 2005). Cao and Li (2002) described a new method for base noun phrase translation by using Web data. Kwok, et al. (2001) described a system called CHINET for cross language name search. Nagata et al. (2001) described how to exploit proximity and redundancy to extract translation for a given term. Lu, Chien, and Lee (2002) describe a method for name translation based on mining of anchor texts. More recently, Zhang, Huang, and Vogel (2005) proposed to use occurring words to expand queries for searching and extracting transliteratio"
I05-1046,P02-1005,0,0.0262722,"Missing"
I05-1046,N04-1007,0,0.0312177,"g the category of the question. The expanded query is the submitted to a search engine in order to bias the search engine to return passages that are more likely to contain answers to the question. Experimental results indicate the expanded query indeed outperforms the approach of directly using the keywords in the question. 2 Related Work Recent work in Question Answering has attempted to convert the original input question into a query that is more likely to retrieve the answers. Hovy et al. [2] utilized WordNet hypernyms and synonyms to expand queries to increase recall. Hildebrandt et al. [4] looked up in a pre-compiled knowledge base and a dictionary to expand a definition question. However, blindly expanding a word using its synonyms or dictionary gloss may cause undesirable effects. Furthermore, it is difficult to determine which of many related word senses should be considered when expanding the query. Radev et al. [5] proposed a probabilistic algorithm called QASM that learns the best query expansion from a natural language question. The query expansion takes the form of a series of operators, including INSERT, DELETE, REPLACE, etc., to paraphrase a factual question into the"
I05-1046,P97-1063,0,0.0111234,"econd longest river in the world? 3.3 Learning Best Transforms This section describes the procedure for learning transforms Ts which convert the question pattern Qp into bigrams in relevant APs. Word Alignment Across Q and AP We use word alignment techniques developed for statistical machine translation to find out the association between question patterns in Q and bigrams in AP. The reason why we use bigrams in APs instead of unigrams is that bigrams tend to have more unique meaning than single words and are more effective in retrieving relevant passages. We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs. The method involves preprocessing steps for each (Q, AP) pair so as to filter useless information: 1. Perform part-of-speech tagging on Q and AP. 2. Replace all instances of A with the tag &lt;ANS> in APs to indicate the location of the answers. 3. Identify the question pattern, Qp and keywords which are not a named entity. We denote the question pattern and keywords as q1, q2, ..., qn. 4. Convert AP into bigrams and eliminate bigrams with low term frequency (tf) or high document frequency (df). Bigrams composed of two function words are also removed, resulting i"
I05-1046,O04-1020,1,0.879638,"Missing"
lin-etal-2004-extraction,C02-1099,0,\N,Missing
lin-etal-2004-extraction,W02-2017,0,\N,Missing
lin-etal-2004-extraction,W98-1005,0,\N,Missing
lin-etal-2004-extraction,W03-0317,1,\N,Missing
lin-etal-2004-extraction,P98-1036,0,\N,Missing
lin-etal-2004-extraction,C98-1036,0,\N,Missing
lin-etal-2004-extraction,P02-1051,0,\N,Missing
lin-etal-2004-extraction,O03-1001,1,\N,Missing
lin-etal-2004-extraction,J98-4003,0,\N,Missing
N09-1029,P05-1033,0,0.0647972,"ifferent circumstances. Therefore, while phrase-based SMT moves from words to phrases as the basic unit of translation, implying effective local reordering within phrases, it suffers when determining phrase reordering, especially when phrases are longer than three words (Koehn et al., 2003). There have been much effort made to improve reordering model in SMT. For example, researchers have been studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) from source parse trees as features. In their work, however, inverted rules are allow"
N09-1029,N03-1017,0,0.0268676,"model, since the training data can not possibly cover all such similar cases. In this paper we present an improved reordering model based on BTG, with bilingual linguistic features from neighboring blocks. To avoid data sparseness problem, both source and target words are classified; we perform part-of-speech (POS) tagging on source language, and word classifica255 Related Work In statistical machine translation, reordering model is concerned with predicting correct orders of target language sentence given a source language one and translation pairs. For example, in phrase-based SMT systems (Koehn et al., 2003; Koehn, 2004), distortion model is used, in which reordering probabilities depend on relative positions of target side phrases between adjacent blocks. However, distortion model can not model long-distance reordering, due to the lack of context information, thus is difficult to predict correct orders under different circumstances. Therefore, while phrase-based SMT moves from words to phrases as the basic unit of translation, implying effective local reordering within phrases, it suffers when determining phrase reordering, especially when phrases are longer than three words (Koehn et al., 2003"
N09-1029,koen-2004-pharaoh,0,0.286992,"EBTG, first words of blocks are considered as the features, which are then used to train a ME model 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 該 項 計劃 Nes Nf Nv the details of 14 49 50 the plan 14 18 的 詳情 DE Na A2 tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the model used in our BTG-based SMT systems. Section 4 formally describes our bilingual linguistic reordering model. Section 5 and Section 6 explain the implementation of o"
N09-1029,P07-2045,0,0.040628,"cks are considered as the features, which are then used to train a ME model 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 該 項 計劃 Nes Nf Nv the details of 14 49 50 the plan 14 18 的 詳情 DE Na A2 tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is organized as follows. Section 2 reviews the related work. Section 3 describes the model used in our BTG-based SMT systems. Section 4 formally describes our bilingual linguistic reordering model. Section 5 and Section 6 explain the implementation of our systems. We show the experim"
N09-1029,P03-1054,0,0.00277332,", since they focus on syntactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 Preo ( A1 , A2 , order) reo where order  {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering example (a), for example, the feature set is {&quot;S1L=three&quot;, &quot;S2L=ago&quot;, &quot;T1L=3&quot;, &quot;T2L=前&quot;} where &quot;S1&quot; and &quot;T1&quot; denote source and target phrases from the block A1. Rule (3) is lexical translation ru"
N09-1029,H05-1021,0,0.0198124,"slation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 2005), reordering probabilities are assigned uniformly during decoding, and can be tuned depending on different language pairs. It is clear, however, that this kind of model would suffer when the dominant rule is wrongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 254 Human Language Technologies: The 2009 Annual C"
N09-1029,W03-1726,0,0.264274,"ious works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 Preo ( A1 , A2 , order) reo where order  {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering example (a), for example, the feature set is {&quot;S1L=three&quot;, &quot;S2L=ago&quot;, &quot;T1L=3&quot;, &quot;T2L=前&quot;} where &quot;S1&quot; and &quot;T1&quot; denote source and target phrases from the block A1. Rule (3) is lexical translation rule, which translates source phrase x into target phrase y. We use the same feature functions as typical phrase-based SMT systems (Koehn et al., 20"
N09-1029,E99-1010,0,0.0504779,"Chinese tagset, the CKIP tagset pro257 vides more fine-grained tags, including many tags with semantic information (e.g., Nc for place nouns, Nd for time nouns), and verb transitivity and subcategorization (e.g., VA for intransitive verbs, VC for transitive verbs, VK for verbs that take a clause as object). On the other hand, using the POS features in combination with the lexical features in target language will cause another sparseness problem in the phrase table, since one source phrase would map to multiple target ones with different POS sequences. As an alternative, we use mkcls toolkit (Och, 1999), which uses maximum-likelihood principle to perform classification on target side. After classification, the toolkit produces a many-to-one mapping between English tokens and class numbers. Therefore, there is no ambiguity of word class in target phrases and word class features can be used independently to avoid data sparseness problem and the phrase table remains unchanged. As mentioned in Section 1, features based on words are not representative enough in some cases, and tend to cause sparseness problem. By classifying words we are able to linguistically generalize the features, and hence p"
N09-1029,J03-1002,0,0.00433681,"Missing"
N09-1029,P03-1021,0,0.00477885,"es are applied during decoding: A  A1 A2  (1) A  A1 A2 (2) A x/ y y 6 (3) where A1 and A2 are blocks in source order. Straight rule (1) and inverted rule (2) are reordering rules. They are applied for predicting target-side order when combining two blocks, and form the reordering model with the distributions 256  Plm ( A1 , A2 )lm  Preo ( A1 , A2 , order)reo or P( A)  Plm ( A) lm  Ptrans ( x |y ) where Plm ( A) lm and Plm ( A1 , A2 ) lm are respectively the usual and incremental score of language model. To tune all lambda weights above, we perform minimum error rate training (Och, 2003) on the development set described in Section 7. Let B be the set of all blocks with source side sentence C. Then the best translation of C is the target side of the block A , where A  argmax P( A) AB 4 Bilingual Linguistic Model In this section, we formally describe the problem we want to address and the proposed method. 4.1 Problem Statement We focus on extracting features representative of the two neighboring blocks being considered for reordering by the decoder, as described in Section 3. We define S(A) and T(A) as the information on source and target side of a block A. For two neighborin"
N09-1029,P02-1040,0,0.0836438,"Missing"
N09-1029,N07-1051,0,0.0329957,"tactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 Preo ( A1 , A2 , order) reo where order  {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering example (a), for example, the feature set is {&quot;S1L=three&quot;, &quot;S2L=ago&quot;, &quot;T1L=3&quot;, &quot;T2L=前&quot;} where &quot;S1&quot; and &quot;T1&quot; denote source and target phrases from the block A1. Rule (3) is lexical translation rule, which translates sou"
N09-1029,P96-1021,0,0.0439247,"ual linguistic model outperforms the state-of-the-art phrase-based and BTG-based SMT systems by improvements of 2.41 and 1.31 BLEU points respectively. 1 3年 three years 前 three A2 (a) years 後 A2 after A1 ago 3年 A1 (b) Figure 1: Two reordering examples, with straight rule applied in (a), and inverted rule in (b). Introduction Bracketing Transduction Grammar (BTG) is a special case of Synchronous Context Free Grammar (SCFG), with binary branching rules that are either straight or inverted. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations o"
N09-1029,I05-1007,0,0.0128032,"nformation is used in LABTG and Zhang's work, their models are syntax-oriented, since they focus on syntactic labels. Boundary POS is considered in LABTG only when source phrases are not syntactic phrases. In contrast to the previous works, we present a reordering model for BTG that uses bilingual information including class-level features of POS and word classes. Moreover, our model is dedicated to boundary features and considers different combinations of phrase lengths, rather than only first/tail words. In addition, current state-of-the-art Chinese parsers, including the one used in LABTG (Xiong et al., 2005), lag beyond in inaccuracy, compared with English parsers (Klein and Manning, 2003; Petrov and Klein 2007). In our work, we only use more reliable information such as Chinese word segmentation and POS tagging (Ma and Chen, 2003). 3 Preo ( A1 , A2 , order) reo where order  {straight, inverted}. In MEBTG, a ME reordering model is trained using features extracted from reordering examples of aligned parallel corpus. First words on neighboring blocks are used as features. In reordering example (a), for example, the feature set is {&quot;S1L=three&quot;, &quot;S2L=ago&quot;, &quot;T1L=3&quot;, &quot;T2L=前&quot;} where &quot;S1&quot; and &quot;T1&quot; deno"
N09-1029,P06-1066,0,0.372028,"d rule in (b). Introduction Bracketing Transduction Grammar (BTG) is a special case of Synchronous Context Free Grammar (SCFG), with binary branching rules that are either straight or inverted. BTG is widely adopted in SMT systems, because of its good trade-off between efficiency and expressiveness (Wu, 1996). In BTG, the ratio of legal alignments and all possible alignment in a translation pair drops drastically especially for long sentences, yet it still covers most of the syntactic diversities between two languages. It is common to utilize phrase translation in BTG systems. For example in (Xiong et al., 2006), source sentences are segmented into phrases. Each On the other hand, there are various proposed BTG reordering models to predict correct orientations between neighboring blocks (bilingual phrases). In Figure 1, for example, the role of reordering model is to predict correct orientations of neighboring blocks A1 and A2. In flat model (Wu, 1996; Zens et al., 2004; Kumar and Byrne, 2005), reordering probabilities are assigned uniformly during decoding, and can be tuned depending on different language pairs. It is clear, however, that this kind of model would suffer when the dominant rule is wro"
N09-1029,I08-1066,0,0.101498,"nt rule is wrongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 該 項 計劃 Nes Nf Nv the details of 14 49 50 the plan 14 18 的 詳情 DE Na A2 tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is org"
N09-1029,C08-1127,0,0.135827,"nt rule is wrongly applied. Predicting orientations in BTG depending on context information can be achieved with lexical features. For example, Xiong et al. (2006) proposed MEBTG, based on maximum entropy (ME) classification with words as features. In MEBTG, first words of blocks are considered as the features, which are then used to train a ME model 254 Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 254–262, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics for predicting orientations of neighboring blocks. Xiong et al. (2008b) proposed a linguistically annotated BTG (LABTG), in which linguistic features such as POS and syntactic labels from source-side parse trees are used. Both MEBTG and LABTG achieved significant improvements over phrase-based Pharaoh (Koehn, 2004) and Moses (Koehn et al., 2007) respectively, on Chinese-to-English translation tasks. 該 項 計劃 Nes Nf Nv the details of 14 49 50 the plan 14 18 的 詳情 DE Na A2 tion on target one, as shown in Figure 2. Additionally, features are extracted and classified depending on lengths of blocks in order to obtain a more informed model. The rest of this paper is org"
N09-1029,C04-1030,0,0.0348825,"Missing"
N09-1029,D07-1056,0,0.0808652,"studying CKY parsing over the last decade, which considers translations and orientations of two neighboring block according to grammar rules or context information. In hierarchical phrase-based systems (Chiang, 2005), for example, SCFG rules are automatically learned from aligned bilingual corpus, and are applied in CKY style decoding. As an another application of CKY parsing technique is BTG-based SMT. Xiong et al. (2006) and Xiong et al. (2008a) developed MEBTG systems, in which first or tail words from reordering examples are used as features to train ME-based reordering models. Similarly, Zhang et al. (2007) proposed a model similar to BTG, which uses first/tail words of phrases, and syntactic labels (e.g. NP and VP) from source parse trees as features. In their work, however, inverted rules are allowed to apply only when source phrases are syntactic; for nonsyntactic ones, blocks are combined straight with a constant score. More recently, Xiong et al. (2008b) proposed LABTG, which incorporates linguistic knowledge by adding features such as syntactic labels and POS from source trees to improve their MEBTG. Different from Zhang's work, their model do not restrict non-syntactic phrases, and applie"
N09-1029,2005.iwslt-1.8,0,\N,Missing
O03-3005,P91-1023,0,0.118231,"Missing"
O03-3005,macklovitch-etal-2000-transsearch,0,0.0207684,"that each sentence is associated with translation counterpart in a second language. It could be extremely useful for bilingual lexicographers, human translators and second language learners. Pierre Isabelle, in 1993, pointed out: “existing translations contain more solutions to more translation problems than any other existing resource.” It is particularly useful and convenient when the resource of existing translations is made available on the Internet. Web based bilingual concordances have proved to be very useful and popular. For example, the English-French concordance system, TransSearch (Macklovitch et al. 2000), provides a familiar interface for the users who only need to type in the expression in question, a list of citations will come up and it is easy to scroll down until one finds one that is useful. In addition to the similar functionalities provided by TransSearch, TotalRecall comes with an additional feature making the solution more easily recognized: the user not only gets all the citations related to the expression in question, but also gets to see the translation counterpart highlighted. TotalRecall extends the translation memory technology and provide an interactive tool intended for tran"
O03-3005,1992.tmi-1.7,0,0.184354,"Missing"
O03-3005,P94-1012,0,0.0439706,"Missing"
O04-1020,P03-1003,0,0.0355589,"Missing"
O04-1020,P84-1044,0,0.103822,"Missing"
O04-1020,P97-1063,0,0.249938,"Missing"
O13-1005,C96-2184,0,\N,Missing
O13-1005,J03-1002,0,\N,Missing
O13-1005,W10-4107,0,\N,Missing
O13-1006,N10-1019,0,0.0212302,"nnecessary, and misuse of articles, prepositions, noun number, and verb form. Recently, the state-of-the-art research on GED has been surveyed by Leacock et al. (2010). In our work, we address serial errors in English learners’ writing related to the proposition and verb form, an aspect that has not been dealt with in most GED research. We also consider the issues of broadening the training data for better coverage, and coping with data sparseness when unseen events happen. Researchers have looked at grammatical errors related to the most common prepositions (e.g., De Felice and Pulman, 2007; Gamon 2010). In the research area of detecting verb form errors, methods based on template related to parse tree, maximum entropy with lexical and POS features have been proposed (e.g., Lee and Seneff, 2006; Izumi et al. 2003). The Longman Dictionary of Common Errors, second edition (LDOCE) is the result of analyzing errors encoded in the Longman Learners’ Corpus. The LDOCE shows that 56 Proceedings of the Twenty-Fifth Conference on Computational Linguistics and Speech Processing (ROCLING 2013) grammatical errors in learners’ writing are mostly isolated, but there are certainly a lot of consecutive error"
O13-1006,P03-2026,0,0.0817678,"Missing"
O13-5002,C96-2184,0,0.0300505,"Missing"
O13-5002,W09-3412,0,0.0355679,"Missing"
O13-5002,W03-1726,0,0.122673,"Missing"
O13-5002,J03-1002,0,0.0127667,"Missing"
O13-5002,W10-4107,0,0.0401607,"Missing"
O13-5003,N10-1019,0,0.0268983,"Missing"
O13-5003,I08-1059,0,0.0698436,"Missing"
O13-5003,P03-2026,0,0.0718537,"Missing"
O13-5003,W09-3010,0,0.0247922,"Missing"
O13-5003,D13-1074,0,0.0240367,"Missing"
O13-5003,P11-1019,0,0.0422916,"Missing"
O13-5003,han-etal-2010-using,0,\N,Missing
O13-5003,E87-1007,0,\N,Missing
O14-5003,I08-1050,0,0.068039,"Missing"
O14-5003,W06-3309,0,0.0795391,"Missing"
O14-5003,J02-4002,0,0.211973,"Missing"
O14-5003,P06-4011,1,0.690405,"Missing"
O16-1030,O02-2002,0,0.153604,"Missing"
O16-1030,C10-1133,0,0.0762745,"Missing"
P03-2040,P91-1023,0,0.141464,"Missing"
P03-2040,1992.tmi-1.7,0,0.135075,"Missing"
P03-2040,P94-1012,0,0.270359,"Missing"
P03-2040,J93-1004,0,\N,Missing
P03-2040,macklovitch-etal-2000-transsearch,0,\N,Missing
P04-3004,J90-2002,0,0.0491972,"texts offer more answers for the learner than any teacher or reference work do. However, it is important to provide easy access for translators and learning writers alike to find the relevant and informative citations quickly. For instance, the English-French concordance system, TransSearch provides a familiar interface for the users (Macklovitch et al. 2000). The user type in the expression in question, a list of citations will come up and it is easy to scroll down until one finds translation that is useful much like using a search engine. TransSearch exploits sentence alignment techniques (Brown et al 1990; Gale and Church 1990) to facilitate bilingual search at the granularity level of sentences. In this paper, we describe a bilingual concordancer which facilitate search and visualization with fine granularity. TotalRecall exploits subsentential and word alignment to provide a new kind of bilingual concordancer. Through the interactive interface and clustering of short subsentential bi-lingual citations, it helps translators and non-native speakers find ways to translate or express them-selves in a foreign language. 2 Aligning the corpus Central to TotalRecall is a bilingual corpus and a set o"
P04-3004,P91-1023,0,0.467169,"Missing"
P04-3004,1993.tmi-1.17,0,0.0569166,"Missing"
P04-3004,J00-2004,0,0.0461367,"Missing"
P05-3010,C02-1011,0,0.461963,"Missing"
P05-3010,J98-4003,0,0.353169,"Missing"
P05-3010,P03-1040,0,0.306876,"Missing"
P05-3010,W01-1413,0,0.16275,"Missing"
P05-3010,P91-1036,0,0.0550095,"se E, we assume E is a proper name and evaluate each candidate translation C by the likelihood of C as the transliteration of E using the transliteration model described in (Lin, Wu and Chang 2004). Figure 4. The distribution of distances between source and target terms in Web pages. Pattern FE EF E(F F（E F(E F.E EwF E,F F》（E F」（E 5000 Count 4000 3000 2000 1000 0 Count -4 -3 -2 -1 0 1 2 3 4 63 111 369 2182 4961 2252 718 91 34 Figure 5. The distribution of distances between source and target terms in Web pages. Distance (5) Expanding the tentative translation. Based on a heuristics proposed by Smadja (1991) to expand bigrams to full collocations, we extend the top-ranking candidate with count n on both sides, while keeping the count greater than n/2 (empirically determined). Note that the constant n is set to 10 in the experiment described in Section 4. (6) Final ranking. Rank the expanded versions of candidates by occurrence count and output the ranked list. 4 Experimental results We took the answers of the first 215 questions on a quiz Website (www.quiz-zone.co.uk) and handtranslations as the training data to obtain a of surface patterns. For all but 17 source terms, we are able to find at lea"
P05-3010,lin-etal-2004-extraction,1,\N,Missing
P10-2021,W09-2107,0,0.0882658,"Missing"
P13-4024,J10-4007,0,0.0124859,"013 Association for Computational Linguistics query in cluster display mode will control that two words have been labeled verb-object by a parser. Moreover, n-grams grouped by object topic/domain give the learner an overview of the usage of the verb. For example the verb absorb takes clusters of objects related to the topics liquid, energy, money, knowledge, and population. Figure 1. An example Linggle search for the query “absorb $N.” This tendency of predicates to prefer certain classes of arguments is defined by Wilks (1978) as selectional preferences and widely reported in the literature. Erk and Padó (2010) extend experiments on selectional preference induction to inverse selectional preference, considering the restriction imposed on predicates. Inverse sectional preference is also implemented in linggle (e.g. “$V apple”). Linggle presents clusters of synonymous collocates (adjectives, nouns and verbs) of a query keyword. We obtained the clusters by building on Lin and Pantel’s (2002) large-scale repository of dependencies and word similarity scores. Using the method proposed by Ritter and Etzioni (2010) we induce selectional preference with a Latent Dirichlet Allocation (LDA) model to seed the"
P13-4024,P98-2127,0,0.152522,"ering (e.g., What were the Capitals of ancient China?) . In contrast to the previous research in Web scale linguistic search engines, we present a system that supports queries with keywords, wildcard words, POS, synonyms, and additional regular expression (RE) operators and displays the results according the count, similarity, and topic with clusters of synonyms or conceptually related words. We exploit and combine the power of both LDA analysis and distributional similarity to provide meaningful semantic classes that are constrained with members of high similarity. Distributional similarity (Lin 1998) and LDA topics become two angles of attack to view language usage and corpus patterns. 3 Linggle Functionalities The syntax of Linggle queries involves basic regular expression of keywords enriched with wildcard PoS and synonyms. Linggle queries can be either pattern-based commands or natural language questions. The natural language queries are currently handled by simple string matching based on a limited set of questions and command pairs provided by a native speaker informant. 3.1 Natural language queries The handling of queries formulated in natural language has been implemented with hand"
P13-4024,C02-1144,0,0.0283164,"a system that provides quick access to the Google Web 1T n-gram with RE-like queries (alternator “|”, one arbitrary word “*”, arbitrary number of words between two specified words “…”). In contrast to Linggle, NetSpeak does not support PoS wildcard or conceptual clustering. An important function in both Linggle and NetSpeak is synonym query. NetSpeak uses WordNet (Fellbaum 2010) synsets to support synonym match. But WordNet synsets tend to contain very little synonyms, leading to poor coverage. Alternatively, one can use the distributional approach to similarity based on a very large corpus. Lin and Pantel (2002) report efforts to build a large repository of dependencies extracted from large corpora such as Wikipedia, and provide similarity between words (demo.patrickpantel.com). We use these results both for handling synonym queries and to organize the n-grams into semantic classes. More recently, Ritter and Etzioni (2010) propose to apply an LDA model (Blei et al. 2003) to 140 the problem of inducing selectional preference. The idea is to consider the verbs in a corpus as the documents of a traditional LDA model. The arguments of the verb that are encountered in the corpus are treated as the words c"
P13-4024,lin-etal-2010-new,0,0.0717132,"Missing"
P13-4024,N10-1012,0,0.0533757,"Missing"
P13-4024,P10-1044,0,0.0203934,"Missing"
P13-4024,C98-2122,0,\N,Missing
P15-4024,W10-0405,0,0.0678877,"Missing"
P15-4024,P07-1011,0,0.0540817,"Missing"
P15-4024,W05-1514,0,0.0286156,"sim(A, B) = 4 P if |A |= 6 |B|. word-sim(Ai , Bi ), otherwise. Experiments and Results For training, we used a collection of approximately 3,000 examples for 700 headwords obtained from online Macmillan English Dictionary (Rundel 2007), to develop the templates of patterns. The headwords include nouns, verbs, adjectives, and adverbs. We then proceeded to extract writing grammar patterns and examples from the British National Corpus (BNC, with 100 million words), CiteseerX corpus (with 460 million words) and Taiwan Degree Thesis Corpus (with 10 million words). First, we used Tsujii POS Tagger (Tsuruoka and Tsujii 2005) to generate tagged sentences. We applied the proposed method to generate suggestions for each of the 700 content keywords in Academic Keyword List. 4.1 Technical Architecture WriteAhead was implemented in Python and Flask Web framework. We stored the suggestions in JSON format using PostgreSQL for faster access. WriteAhead server obtains client input from a popular browser (Safari, Chrome, or Firefox) dynamically with AJAX techniques. For uninterrupted service and ease of scaling up, we chose to host WriteAhead on Heroku, a cloud-platform-as-aservice (PaaS) site. 4.2 Demo script Evaluating Wr"
P15-4024,W00-0507,0,0.23966,"Missing"
P15-4024,W11-1422,0,\N,Missing
P15-4024,P96-1010,0,\N,Missing
W12-2035,W07-1604,0,0.0823599,"Missing"
W12-2035,W12-2006,0,0.0292011,"Missing"
W12-2035,W07-1607,0,0.0629018,"Missing"
W12-2035,W97-0301,0,0.0464826,"aining and testing of the preposition error detection and correction modules. Input sentence Tagger & Parser Determiner Error Detection Determiner Determiner Choice Preposition Error Detection Preposition Preposition Error Choice Output Figure 1. System Architecture (Run-Time) 296 Word This virus affects the defence system Base form This virus affect the defence system POS DT NN VBZ DT NN NN Chunk B-NP I-NP B-VP B-NP I-NP I-NP NE O O O O O O . . . O O Table 1. The tag result of sample sentence. 2.2 Determiners In this section, we investigate the performance of two maximum entropy classifiers (Ratnaparkhi, 1997), one for determining whether a noun phrase has a determiner or not and the other for selecting the appropriate determiner if one is needed. From the British National Corpus (BNC), we extract 22,552,979 noun phrases (NPs). For determining which features are useful for this task, all NPs are divided into two sets, 20 million cases as a training set and the others as a validation set. For the classifier (named the DetClassifier hereafter) trained for predicting whether a NP has a determiner or not, the label set contains two labels: “Zero” and “DET.” On the other hand, for the classifier (named"
W12-2035,W10-4236,0,\N,Missing
W13-3603,P13-4024,1,0.737636,"tion on the location of the location of cell location of cell phone on the location the location of location of cell the location location of MW4 MW3 MW2 Table 2. Trigram information of ‘location’ and ‘locations’ in back-off model MW3 n-gram location on the location the location of location of cell on the locations the locations of locations of cell locations Freq. S3 304,400 3,794,400 1,400 18,200 374,000 200 4M 0.04 M then measure the ratio of the counts of the original and replaced n-grams in a corpus. The frequency counts are obtained by querying a linguistic search engine Linggle (Joanne Boisson et al. 2013), a web-scale linguistic search engine based on Google Web1T (Brants and Franz, 2006). The sum of n-gram counts, Sk with the word w (original or replacement) in the ith position is defined as 2.1 Overview In this section, we give an overview of our system. Figure 1 shows the architecture of the error correction system. In this study, we focus on five different grammatical error types, including the improper usage of Determiner (ArtOrDet), Noun Number (Nn), Verb-Tenses (Vform), Subject-Verb Agreement (SVA), and Preposition (Prep). In order to deal with these different types of errors systematic"
W13-3603,W13-1703,0,0.127097,"Missing"
W13-3603,N12-1067,0,0.189797,"Missing"
W13-3603,D10-1094,0,0.0516178,"Missing"
W13-3603,J90-1003,0,\N,Missing
W13-3603,W13-3601,0,\N,Missing
W13-4408,W03-1726,0,0.0929847,"lected by comparing the original text and its OCR results. Similarly, Zhuang et al. (2004) proposed an effective approach using OCR to recognize possible confusion set. In addition, Zhuang et al. (2004) also used a multiknowledge based statistical language model, the n-gram language model, and Latent Semantic Analysis. However, the experiments by Zhuang et al. (2004) seem to show that the simple n-gram model performs the best. In recent years, Chinese spelling checkers have incorporated word segmentation. The method proposed by Huang et al. (2007) incorporates Sinica Word Segmentation System (Ma and Chen, 2003) to detect typos. With a characterbased bigram language model and the rule-based methods of dictionary knowledge and confusion set, the method determines whether the word is a typo or not. There are many more systems that use word segmentation to detect errors. For example, in Hung and Wu (2009), the given sentence is segmented using a bigram language model. In addition, the method also uses confusion set and common error templates manually edited and provided by Ministry of Education in Taiwan. Chen and Wu (2010) modified the system proposed by Hung and Wu (2009), by combining statistic-based"
W13-4408,J03-1002,0,0.00804178,"wo translations are possible for this candidate: “氣憤 qi fen” and “氣氛 qi fen”. We use a simple, publicly available decoder written in Python to correct potential spelling errors found in the detection module. The decoder reads in a Chinese sentence at a time and attempts to “translate” the sentence into a correctly spelled one. The decoder translates monotonically without reordering the Chinese words and phrases using two models — translation probability model and the language model. These two models read from a data directory containing two text files containing a translation model in GIZA++ (Och and Ney, 2003) format, and a language model in SRILM (Stolcke et al., 2011) format. These two models are stored in memory for quick access. The decoder invokes the two modules to load the translation and language models and decodes the input sentences, storing the result in output. The decoder computes the probability of the output sentences according to the models. It works by summing over all possible ways that the model could have generated the corrected sentence from the input sentence. Although in general covering all possible corrections in the translation and language models is intractable, but a maj"
W13-4408,W10-4107,0,0.0430754,"statistical methods (Hung and Wu, 2009; Chen and Wu, 2010). Rule-based methods use knowledge resources such as a dictionary to identify a word as a typo if the word is not in the dictionary, and provide similar words in the dictionary as sug49 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 49–53, Nagoya, Japan, 14 October 2013. and Section 5 present the experimental data and evaluation results. And we conclude in Section 6. 2 matching module generated automatically to detect and correct typos based on language model. In a work closer to our method, Wu et al. (2010) adopts the noise channel model, a framework used both in spell checkers and machine translation systems. The system combined statistic-based method and template matching with the help of a dictionary and a confusion set. They also used word segmentation to detect errors, but they did not use an existing word segmentation as Huang et al. (2007) did, because it might regard a typo as a new word. They used a backward longest first approach to segment sentences with an online dictionary sponsored by MOE, and a templates with a confusion set. The system also treat Chinese spelling check as a kind"
W13-4408,C96-2184,0,0.123418,"are considered as strongly similar and are retained. For example, the code of “徵 zheng” and “微 wei” are strongly similar in shape, since in their corresponding codes “竹人 山土大” and “竹人山山大”, differ only in one place. Experimental Setting To train our model, we used several corpora including Sinica Chinese Balanced Corpus, TWWaC (Taiwan Web as Corpus), a Chinese dictionary, and a confusion set. We describe the data sets in more detail below. Sinica Corpus ""Academia Sinica Balanced Corpus of Modern Chinese"", or Sinica Corpus for short, is the first balanced Chinese corpus with part-of-speech tags (Huang et al., 1996). Current size of the corpus is about 5 million words. Texts are segmented according to the word segmentation standard proposed by the ROC Computational Linguistic Society. We use the corpus to generate the frequency of bigram, trigram and 4-gram for training translation model and to train the n-gram language model. TWWaC (Taiwan Web as Corpus) We use TWWaC for obtaining more language information. TWWaC is a corpus gathered from the Web under the .tw domain, containing 1,817,260 Web pages, 30 billions Chinese characters. We use the corpus to generate the frequency of all character n-grams for"
W13-4411,N03-1028,0,0.0139232,"Missing"
W13-4411,P00-1032,0,0.0622192,"Missing"
W13-4411,W03-0430,0,0.253178,"Missing"
W13-4411,W09-3412,0,0.0537182,"Missing"
W13-4411,P02-1006,0,\N,Missing
W14-1712,W12-2025,0,0.0329696,"Missing"
W14-1712,N12-1067,0,0.0775705,"Missing"
W14-1712,W13-1703,0,0.0562161,"Missing"
W14-1712,N12-1029,0,0.061715,"Missing"
W14-1712,P03-1054,0,\N,Missing
W14-1712,S13-1035,0,\N,Missing
W14-6832,W13-4408,1,0.578764,"Missing"
W14-6832,W13-4406,0,0.0620612,"Missing"
W14-6832,O13-1005,1,\N,Missing
Y14-1034,P03-1012,0,0.0481514,"ent dotplot (see figure on the right) 抵達 德黑蘭 Note that the dark cells represent links in the 後 intersection of two alignments, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transliteration-based feature"
Y14-1034,J90-2002,0,0.478876,"keTwo with those produced by Giza++ with GDFA (Section 4 and Section 5) over a set of parallel sentences with hand-annotated word alignment. 2 Related Work Machine translation (MT) has been an area of active research. (Dorr, 1993) summarizes various approaches to MT, while (Lopez, 2007) surveys recent work on statistical machine translation (SMT). We focus on the first part of developing an SMT system, namely, aligning words in a given parallel corpus. The state of the art in word alignment focuses on automatically learning generative translation models via Expectation Maximization algorithm (Brown et al., 1990; Brown et al., 1993). (Och and Ney, 2003) describe Giza++, an implementation of the !283 PACLIC 28 Input: ... He made this remark after Heinonen arrived in Tehran. ÷ / ( w˛Á µT ∑—- å |h ⇡ ⇧ «q ⇥ ... Initial word alignments in two directions (En-Ch and Ch-En): he(÷) made this remark(/) after(( å) heinonen(w˛Á |h ⇧ «q) arrived(µT) in tehran(∑—-) ÷(he) / ( w˛Á(remark heinonen) µT(arrive in) ∑—-(tehran) å(after) |h(made) ⇡(this) ⇧ «q Crosslingual relatedness: ra n a rk s th i he(÷) made(|h) this(⇡) remark(«q) ma he de Output: = .25 . = sim(remark, talk) te h x-sim(remark, «q) in o ne n a rr ive d"
Y14-1034,J93-2003,0,0.0501737,"duced by Giza++ with GDFA (Section 4 and Section 5) over a set of parallel sentences with hand-annotated word alignment. 2 Related Work Machine translation (MT) has been an area of active research. (Dorr, 1993) summarizes various approaches to MT, while (Lopez, 2007) surveys recent work on statistical machine translation (SMT). We focus on the first part of developing an SMT system, namely, aligning words in a given parallel corpus. The state of the art in word alignment focuses on automatically learning generative translation models via Expectation Maximization algorithm (Brown et al., 1990; Brown et al., 1993). (Och and Ney, 2003) describe Giza++, an implementation of the !283 PACLIC 28 Input: ... He made this remark after Heinonen arrived in Tehran. ÷ / ( w˛Á µT ∑—- å |h ⇡ ⇧ «q ⇥ ... Initial word alignments in two directions (En-Ch and Ch-En): he(÷) made this remark(/) after(( å) heinonen(w˛Á |h ⇧ «q) arrived(µT) in tehran(∑—-) ÷(he) / ( w˛Á(remark heinonen) µT(arrive in) ∑—-(tehran) å(after) |h(made) ⇡(this) ⇧ «q Crosslingual relatedness: ra n a rk s th i he(÷) made(|h) this(⇡) remark(«q) ma he de Output: = .25 . = sim(remark, talk) te h x-sim(remark, «q) in o ne n a rr ive d in = sim(make, publi"
Y14-1034,P12-2026,1,0.857078,"alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transliteration-based features derived from transliteration model in a different setting. Word alignment is closely related to measuring word similarity, and especially in the form of crosslingual relatedness. Much work has been done on word similarity and crosslingual relatedness. Early research efforts have been devoted to design the knowledge-based measures, based, in particular, on WordNet (Fellbaum, 1999). Researchers have extensively investigated WordNet an"
Y14-1034,P06-2014,0,0.0227388,"is that previous methods use manually labeled data (typically hundreds sentences with thousands of word-translation relations) to train a word alignment model. In contrast, we take a self learning approach and automatically generate labelled training data. More specifically, We train our model based on a much larger training set (hundred of thousand of word-translation instances in partially labeled sentences) based on self learning. Recently, some researchers have begun using syntax in word alignment, by incorporating features such as inversion transduction grammar or parse tree. Supervised (Cherry and Lin, 2006; Setiawan et al., 2010) and unsupervised (Pauls et al., 2010) methods have been proposed, showing that syntax can improve alignment performance. All these features can be used to training the classifier used in TakeTwo. In a word alignment approach closer to our method, (Deng and Zhou, 2009) propose a method to optimize word alignment combination to derive a more effective phrase table. Similarly, (Nakov and Tiedemann, 2012) propose combining word-level and character-Level alignment models for improving machine translation between two closely-related languages. In contrast to the previous res"
Y14-1034,P09-2058,0,0.0148143,"l based on a much larger training set (hundred of thousand of word-translation instances in partially labeled sentences) based on self learning. Recently, some researchers have begun using syntax in word alignment, by incorporating features such as inversion transduction grammar or parse tree. Supervised (Cherry and Lin, 2006; Setiawan et al., 2010) and unsupervised (Pauls et al., 2010) methods have been proposed, showing that syntax can improve alignment performance. All these features can be used to training the classifier used in TakeTwo. In a word alignment approach closer to our method, (Deng and Zhou, 2009) propose a method to optimize word alignment combination to derive a more effective phrase table. Similarly, (Nakov and Tiedemann, 2012) propose combining word-level and character-Level alignment models for improving machine translation between two closely-related languages. In contrast to the previous research in word alignment, we present a system that automatically generates instances of word-translation relations based on self learning, with the goal of training a model to estimate translation probability for effective word alignment. We exploit the inherent crosslingual regularity in para"
Y14-1034,H91-1026,0,0.540468,"rk, /) 他 after(å) heinonen(w˛Á) arrived(µT) in(µT) 是 tehran(∑—-) . (⇥) 在 海諾寧 Alignment dotplot (see figure on the right) 抵達 德黑蘭 Note that the dark cells represent links in the 後 intersection of two alignments, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliter"
Y14-1034,P03-1011,0,0.0537433,") 在 海諾寧 Alignment dotplot (see figure on the right) 抵達 德黑蘭 Note that the dark cells represent links in the 後 intersection of two alignments, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transli"
Y14-1034,J97-2004,1,0.619041,"nen(w˛Á) arrived(µT) in(µT) 是 tehran(∑—-) . (⇥) 在 海諾寧 Alignment dotplot (see figure on the right) 抵達 德黑蘭 Note that the dark cells represent links in the 後 intersection of two alignments, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical te"
Y14-1034,P08-1113,0,0.0391384,"Missing"
Y14-1034,P98-2127,0,0.0241249,"the Web. We use similar transliteration-based features derived from transliteration model in a different setting. Word alignment is closely related to measuring word similarity, and especially in the form of crosslingual relatedness. Much work has been done on word similarity and crosslingual relatedness. Early research efforts have been devoted to design the knowledge-based measures, based, in particular, on WordNet (Fellbaum, 1999). Researchers have extensively investigated WordNet and other taxonomic structure in an attempt to calculate the word similarity by counting conceptual distance (Lin, 1998b). On the other hand, there has been much work on distributional word similarity, for example, (Lin, !284 PACLIC 28 1998a). In the area of cross-lingual relatedness, (Michelbacher et al., 2010) present a graph-based method for building a a cross-lingual thesaurus. The method uses two monolingual corpora and a basic dictionary to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. In the research area of supervised training for word alignment, (Moore, 2005) demonstrates that a discriminative model with the main feature of"
Y14-1034,P05-1057,0,0.0229715,"present a graph-based method for building a a cross-lingual thesaurus. The method uses two monolingual corpora and a basic dictionary to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. In the research area of supervised training for word alignment, (Moore, 2005) demonstrates that a discriminative model with the main feature of Log Likelihood Ratio (LLR) could result in a smaller model comparable to more complex generative EM models in alignment accuracy. (Taskar et al., 2005) independently propose a similar approach. (Liu et al., 2005) also propose a log-linear model incorporating features (alignment probability, POS correspondence and bilingual dictionary coverage). The main difference from our current work is that previous methods use manually labeled data (typically hundreds sentences with thousands of word-translation relations) to train a word alignment model. In contrast, we take a self learning approach and automatically generate labelled training data. More specifically, We train our model based on a much larger training set (hundred of thousand of word-translation instances in partially labeled sentences) based on"
Y14-1034,2007.mtsummit-tutorials.1,0,0.0290391,"and fill in valid links [made, |h] and [remark, «q], leading to an improved alignment. The rest of the paper is organized as follows. We review the related work in the next section. Then we present our method for TakeTwo (Section 3). To evaluate the performance of TakeTwo, we compare the quality of alignments produced by TakeTwo with those produced by Giza++ with GDFA (Section 4 and Section 5) over a set of parallel sentences with hand-annotated word alignment. 2 Related Work Machine translation (MT) has been an area of active research. (Dorr, 1993) summarizes various approaches to MT, while (Lopez, 2007) surveys recent work on statistical machine translation (SMT). We focus on the first part of developing an SMT system, namely, aligning words in a given parallel corpus. The state of the art in word alignment focuses on automatically learning generative translation models via Expectation Maximization algorithm (Brown et al., 1990; Brown et al., 1993). (Och and Ney, 2003) describe Giza++, an implementation of the !283 PACLIC 28 Input: ... He made this remark after Heinonen arrived in Tehran. ÷ / ( w˛Á µT ∑—- å |h ⇡ ⇧ «q ⇥ ... Initial word alignments in two directions (En-Ch and Ch-En): he(÷) ma"
Y14-1034,H05-1011,0,0.0219792,"mpt to calculate the word similarity by counting conceptual distance (Lin, 1998b). On the other hand, there has been much work on distributional word similarity, for example, (Lin, !284 PACLIC 28 1998a). In the area of cross-lingual relatedness, (Michelbacher et al., 2010) present a graph-based method for building a a cross-lingual thesaurus. The method uses two monolingual corpora and a basic dictionary to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. In the research area of supervised training for word alignment, (Moore, 2005) demonstrates that a discriminative model with the main feature of Log Likelihood Ratio (LLR) could result in a smaller model comparable to more complex generative EM models in alignment accuracy. (Taskar et al., 2005) independently propose a similar approach. (Liu et al., 2005) also propose a log-linear model incorporating features (alignment probability, POS correspondence and bilingual dictionary coverage). The main difference from our current work is that previous methods use manually labeled data (typically hundreds sentences with thousands of word-translation relations) to train a word a"
Y14-1034,W01-1413,0,0.0103124,"nts, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transliteration-based features derived from transliteration model in a different setting. Word alignment is closely related to measuring word similari"
Y14-1034,P12-2059,0,0.0241581,"self learning. Recently, some researchers have begun using syntax in word alignment, by incorporating features such as inversion transduction grammar or parse tree. Supervised (Cherry and Lin, 2006; Setiawan et al., 2010) and unsupervised (Pauls et al., 2010) methods have been proposed, showing that syntax can improve alignment performance. All these features can be used to training the classifier used in TakeTwo. In a word alignment approach closer to our method, (Deng and Zhou, 2009) propose a method to optimize word alignment combination to derive a more effective phrase table. Similarly, (Nakov and Tiedemann, 2012) propose combining word-level and character-Level alignment models for improving machine translation between two closely-related languages. In contrast to the previous research in word alignment, we present a system that automatically generates instances of word-translation relations based on self learning, with the goal of training a model to estimate translation probability for effective word alignment. We exploit the inherent crosslingual regularity in parallel corpora and use automatically annotated data for training a discriminative model. 3 The TakeTwo Aligner Aligning words and translat"
Y14-1034,J03-1002,0,0.0191785,"DFA (Section 4 and Section 5) over a set of parallel sentences with hand-annotated word alignment. 2 Related Work Machine translation (MT) has been an area of active research. (Dorr, 1993) summarizes various approaches to MT, while (Lopez, 2007) surveys recent work on statistical machine translation (SMT). We focus on the first part of developing an SMT system, namely, aligning words in a given parallel corpus. The state of the art in word alignment focuses on automatically learning generative translation models via Expectation Maximization algorithm (Brown et al., 1990; Brown et al., 1993). (Och and Ney, 2003) describe Giza++, an implementation of the !283 PACLIC 28 Input: ... He made this remark after Heinonen arrived in Tehran. ÷ / ( w˛Á µT ∑—- å |h ⇡ ⇧ «q ⇥ ... Initial word alignments in two directions (En-Ch and Ch-En): he(÷) made this remark(/) after(( å) heinonen(w˛Á |h ⇧ «q) arrived(µT) in tehran(∑—-) ÷(he) / ( w˛Á(remark heinonen) µT(arrive in) ∑—-(tehran) å(after) |h(made) ⇡(this) ⇧ «q Crosslingual relatedness: ra n a rk s th i he(÷) made(|h) this(⇡) remark(«q) ma he de Output: = .25 . = sim(remark, talk) te h x-sim(remark, «q) in o ne n a rr ive d in = sim(make, publish) = .32, he x-sim(m"
Y14-1034,J04-4002,0,0.0303385,"with high predicted probabiliy, are added to the results. In Step (4), we attempt to fill in links which are probably wordtranslation pairs, if the link is not in conflict with the current alignment. In Step (5), we execute the FINAL-AND step the same way as in GDFA. Experiments and Evaluation TakeTwo. TakeTwo (no fill-in). Giza++: grow-diag-final-and. Giza++: intersection. Giza++: union. We manually aligned 300 random selected sentences with English and Chinese words as the reference answers. For simplicity, we do not distinguished between sure and uncertain alignment links as described in (Och and Ney, 2004). For preprocessing and generating syntactic features, we used the Genia Tagger and CKIP Word Segmenter to generate tokens and parts of speech. We also used the Wikipedia Dump (English) to build distributional word similarity measure. In order to train a classifier for word-translation relation, we used SVM classifier with the tool libsvm. We used lexical, morphological, transliteration, and syntactic features, as described in Section 3.2.2. For simplicity, we used an empirically determined values for the thresholds of similarity constraint in T akeT wo. 4.2 Evaluation Metrics Each word-transl"
Y14-1034,N10-1014,0,0.0158535,"undreds sentences with thousands of word-translation relations) to train a word alignment model. In contrast, we take a self learning approach and automatically generate labelled training data. More specifically, We train our model based on a much larger training set (hundred of thousand of word-translation instances in partially labeled sentences) based on self learning. Recently, some researchers have begun using syntax in word alignment, by incorporating features such as inversion transduction grammar or parse tree. Supervised (Cherry and Lin, 2006; Setiawan et al., 2010) and unsupervised (Pauls et al., 2010) methods have been proposed, showing that syntax can improve alignment performance. All these features can be used to training the classifier used in TakeTwo. In a word alignment approach closer to our method, (Deng and Zhou, 2009) propose a method to optimize word alignment combination to derive a more effective phrase table. Similarly, (Nakov and Tiedemann, 2012) propose combining word-level and character-Level alignment models for improving machine translation between two closely-related languages. In contrast to the previous research in word alignment, we present a system that automaticall"
Y14-1034,D10-1052,0,0.0374544,"Missing"
Y14-1034,H05-1010,0,0.0399836,"area of cross-lingual relatedness, (Michelbacher et al., 2010) present a graph-based method for building a a cross-lingual thesaurus. The method uses two monolingual corpora and a basic dictionary to build two monolingual word graphs, with nodes representing words and edges representing linguistic relations between words. In the research area of supervised training for word alignment, (Moore, 2005) demonstrates that a discriminative model with the main feature of Log Likelihood Ratio (LLR) could result in a smaller model comparable to more complex generative EM models in alignment accuracy. (Taskar et al., 2005) independently propose a similar approach. (Liu et al., 2005) also propose a log-linear model incorporating features (alignment probability, POS correspondence and bilingual dictionary coverage). The main difference from our current work is that previous methods use manually labeled data (typically hundreds sentences with thousands of word-translation relations) to train a word alignment model. In contrast, we take a self learning approach and automatically generate labelled training data. More specifically, We train our model based on a much larger training set (hundred of thousand of word-tr"
Y14-1034,Q13-1024,0,0.011739,"e on the right) 抵達 德黑蘭 Note that the dark cells represent links in the 後 intersection of two alignments, while the gray 發表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transliteration-based features derived from transli"
Y14-1034,D07-1106,1,0.833193,"表 cells represent links in the rest of the union 這 項 談話 。 . Figure 2: An example TakeTwo session and results IBM models, which has since become the tool of choice for developing SMT systems. As an alternative to the EM algorithm, researchers have been exploring various knowledge sources for word alignment, using automatically derived lexicons or handcrafted dictionaries (Gale and Church, 1991; Ker and Chang, 1997), or syntactic structure (Gildea, 2003; Cherry and Lin, 2003; Wang and Zong, 2013). There has been work on translating phrases using mixed-code web-pages (e.g., (Nagata et al., 2001; Wu and Chang, 2007)). Similarly, (Lin et al., 2008) propose a method that performs word alignment for parenthetic translation phrases to improve the performance of SMT systems. Researchers have also studied sublexical models for machine transliteration (Knight and Graehl, 1998). More recently, (Chang et al., 2012) introduce a method for learning a CRF model to find translations and transliterations of technical terms on the Web. We use similar transliteration-based features derived from transliteration model in a different setting. Word alignment is closely related to measuring word similarity, and especially in"
Y14-1034,C98-2122,0,\N,Missing
Y14-1034,J98-4003,0,\N,Missing
Y14-1034,michelbacher-etal-2010-building,0,\N,Missing
