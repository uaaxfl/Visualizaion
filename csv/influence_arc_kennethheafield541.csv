2010.amta-papers.34,C08-1005,0,0.0760964,"e have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is central to system combination and most schemes have some form of n-gram match features. Of these, the simplest consider only unigram matches (Ayan et al., 2008; Heafield et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 20"
2010.amta-papers.34,D08-1024,0,0.0428803,"lem here is that the decoder sees mostly the same λ each time and the optimizer sees mostly the same output each time, missing potentially better but different weights. Random restarts inside the optimizer do not solve this problem because this technique only finds better weights subject to decoded hypotheses. As the number of features increases (in some experiments to 39), the problem becomes more severe because the space of feature weights is much larger than the explored space. We propose a simulated annealing method to address problems with MERT, leaving other tuning methods such as MIRA (Chiang et al., 2008) and lattice MERT to future work. Specifically, when the decoder is given weights λ to use for decoding in iteration 0 ≤ j &lt; 10, it instead uses weights µ sampled according to j j µi ∼ U λi , 2 − λi 10 10     where U is the uniform distribution and subscript i denotes the ith feature. This sampling is done on a per-sentence basis, so the first sentence is decoded with different weights than the second sentence. The amount of random perturbation decreases linearly each iteration until the 10th and subsequent iterations where weights are used in the normal, unperturbed, fashion. The process"
2010.amta-papers.34,N10-1141,0,0.0367115,"r than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword"
2010.amta-papers.34,D09-1125,0,0.0222909,"rder of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword lexical choice, and possibly selection of the backbone. Flexible ordering schemes use a reordering model (He and Toutanova, 2009) or dynamically switch backbones (Heafield et al., 2009) to create word orders not seen in any single translation. For example, these translations appear in NIST MT09 (Peterson et al., 2009a): “We have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our"
2010.amta-papers.34,W09-0408,1,0.875916,"ckbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this search space, the impact of our match features is limited to selection at the word level, multiword lexical choice, and possibly selection of the backbone. Flexible ordering schemes use a reordering model (He and Toutanova, 2009) or dynamically switch backbones (Heafield et al., 2009) to create word orders not seen in any single translation. For example, these translations appear in NIST MT09 (Peterson et al., 2009a): “We have not the interest of control on the Palestinians life,” and “We do not have a desire to control the lives of the Palestinians.” Flexible ordering schemes consider “have not” versus “do not have” separately from “Palestinians life” versus “lives of the Palestinians.” Our match features have the most impact here because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is cen"
2010.amta-papers.34,W09-0406,0,0.391648,"ed, these comprise a combination scheme that differs from others in three key ways: the search space is more flexible, the features consider matches longer than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with"
2010.amta-papers.34,P08-2021,0,0.565426,"best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point. 1 Introduction System combination merges the output of several machine translation systems to form an improved translation. While individual systems perform similarly overall, human evaluators report different error distributions for each system (Peterson et al., 2009b). For example, some systems are weak at word order while others have more trouble with nouns and verbs. Existing system combination techniques (Rosti et al., 2008; Karakos et al., 2008; Leusch et al., 2009a) ignore these distinctions by learning a single weight for each system. This weight is used in word-level decisions and therefore captures only lexical choice. We see two problems: system behavior differs in more ways than captured by a single weight and further current features only guide decisions at the word level. To remedy this situation, we propose new features that account for multiword behavior, each with a separate set of system weights. 2 Features Most combination schemes generate many hypothesis combinations, score them using a battery of features, and search"
2010.amta-papers.34,P07-2045,0,0.00669912,"esis with highest score to output. Formally, the system generates hypothesis h, evaluates feature function f , and multiplies linear weight vector λ by feature vector f (h) to obtain score λT f (h). The score is used to rank final hypotheses and to prune partial hypotheses during beam search. The beams contain hypotheses of equal length. With the aim of improving score and therefore translation quality, this paper focuses on the structure of features f and their corresponding weights λ. The feature function f consists of the following feature categories: Length Length of the hypothesis, as in Koehn et al. (2007). This compensates, to first order, for the impact of length on other features. LM Log probability from an SRI (Stolcke, 2002) language model. When the language model scores a word, it finds the longest n-gram in the model with the same word and context. We use the length n as a second feature. The purpose of this second feature is to provide the scoring model with limited control over language model backoff penalties. System 1: Supported Proposal of France System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1"
2010.amta-papers.34,N04-1022,0,0.608903,"from others in three key ways: the search space is more flexible, the features consider matches longer than unigrams, and system weights differ by task. 3 Related Work System combination takes a variety of forms that pair a space of hypothesis combinations with features to score these hypotheses. Here, we are primarily interested in three aspects of each combination scheme: the space of hypotheses, features that reward n-gram matches with system outputs, and the system weights used for those features. 3.1 Search Spaces Hypothesis selection (Hildebrand and Vogel, 2009) and minimum Bayes risk (Kumar and Byrne, 2004) select from k-best lists output by each system. In the limit case for large k, DeNero et al. (2010) adopt the search spaces of the translation systems being combined. Confusion networks preserve the word order of one k-best list entry called the backbone. The backbone is chosen by hypothesis selection (Karakos et al., 2008; Sim et al., 2007) or jointly with decoding (Leusch et al., 2009a; Rosti et al., 2008). Other k-best entries are aligned to the backbone. The search space consists of choosing each word from among the alternatives aligned with it, keeping these in backbone order. With this"
2010.amta-papers.34,1983.tc-1.13,0,0.684335,"Missing"
2010.amta-papers.34,P03-1021,0,0.264981,"that capture lexical and multiword agreement with each system. The weight on match count cs,n corresponds to confidence in n-grams from system s. However, this weight also accounts for correlation between features, which is quite high within the same system and across related systems. Viewed as language modeling, each cs,n is a miniature language model trained on the translated sentence output by system s and jointly interpolated with a traditional language model and with peer models. As described further in Section 4, we jointly tune the weights λ using modified minimum error rate training (Och, 2003). In doing so, we simultaneously learn several weights for each system, one for each length n-gram output by that system. The unigram weight captures confidence in lexical choice while weights on longer n-gram features capture confidence in word order and phrasal choices. These features are most effective with a variety of hypotheses from which to choose, so in Section 5 we describe a search space with more flexible word order. Combined, these comprise a combination scheme that differs from others in three key ways: the search space is more flexible, the features consider matches longer than u"
2010.amta-papers.34,P02-1040,0,0.0903278,"ere because word order is less constrained. This is the type of search space we use in our experiments. 3.2 N -gram Match Features Agreement is central to system combination and most schemes have some form of n-gram match features. Of these, the simplest consider only unigram matches (Ayan et al., 2008; Heafield et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tu"
2010.amta-papers.34,P07-1040,0,0.0716064,"so system-level weights suffice here. For methods that use k-best lists, system weight may be moderated by some decreasing function of rank in the k-best list (Ayan et al., 2008; Zhao and He, 2009). Minimum Bayes risk (Kumar and Byrne, 2004) takes the technique a step further by using the overall system scores that determined the ranking. 4 Parameter Tuning Key to our model is jointly tuning the feature weights λ. In our experiments, weight vector λ is tuned using minimum error rate training (MERT) (Och, 2003) towards BLEU (Papineni et al., 2002). We also tried tuning towards TER minus BLEU (Rosti et al., 2007) and METEOR (Lavie and Denkowski, 2010), finding at best minor improvement in the targeted metric with longer tuning time. This may be due to underlying systems tuning primarily towards BLEU. In ordinary MERT, the decoder produces hypotheses given weights λ and the optimizer selects λ to rank the best hypotheses at the top. These steps alternate until λ converges or enough iterations happen. As the feature weights converge, the k-best lists output also converge. In our experiments, we use k = 300. For long sentences, this is a small fraction of the hypotheses that our flexible ordering scheme"
2010.amta-papers.34,W08-0329,0,0.34164,"f 6.67 BLEU over the best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point. 1 Introduction System combination merges the output of several machine translation systems to form an improved translation. While individual systems perform similarly overall, human evaluators report different error distributions for each system (Peterson et al., 2009b). For example, some systems are weak at word order while others have more trouble with nouns and verbs. Existing system combination techniques (Rosti et al., 2008; Karakos et al., 2008; Leusch et al., 2009a) ignore these distinctions by learning a single weight for each system. This weight is used in word-level decisions and therefore captures only lexical choice. We see two problems: system behavior differs in more ways than captured by a single weight and further current features only guide decisions at the word level. To remedy this situation, we propose new features that account for multiword behavior, each with a separate set of system weights. 2 Features Most combination schemes generate many hypothesis combinations, score them using a battery of"
2010.amta-papers.34,2006.amta-papers.25,0,0.0492678,"ccording to human judges (CallisonBurch et al., 2009). The organizers of the following WMT also dropped Hungarian. Official tuning and evaluation sets are used, except for MT09 Arabic-English where only unsequestered portions are used for evaluation. Language model training data for WMT is constrained to the provided English from monolingual and French-English corpora. There was no constrained informal system combination track for MT09 so we use a model trained on the Gigaword (Graff, 2003) corpus. Scores are reported using uncased BLEU (Papineni et al., 2002) from mteval-13a.pl, uncased TER (Snover et al., 2006) 0.7.25, and METEOR (Lavie and Denkowski, 2010) 1.0 with Adequacy-Fluency parameters. For each source language, we selected a few subsets of systems to combine and picked the set that combined best on tuning data. Performance is surprisingly good on Arabic and competitive with top MT09 combinations. On French and Spanish, Google scored much higher than did other systems. Like Leusch et al. (2009b), we show no gain over Google on these source languages. Most system combination schemes showed larger gains in MT09 than in WMT. In addition to different language pairs, one possible explanation is t"
2010.amta-papers.34,W06-3110,0,0.0516877,"field et al., 2009; Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tuning n-gram weights. 3.3 System Weighting Different and correlated system strengths make it important to weight systems when combining their votes on n-grams. The simplest method treats these system weights as a hyper parameter (Heafield et al., 2009; Hildebrand and Vogel, 2009). The hyper parameter mig"
2010.amta-papers.34,N09-2052,0,0.270683,"Rosti et al., 2008; Zhao and Jiang, 2009). Some schemes go beyond unigrams but with fixed weight. Karakos (2009) uses n-gram matches to select the backbone but only unigrams for decoding. Kumar and Byrne (2004) use arbitrary evaluation metric to measure similarity. BLEU (Papineni et al., 2002) is commonly used for this purpose and quite similar to our match features, although we have tunable linear n-gram and length weights instead of fixed geometric weights. Several schemes expose a separate feature for each n-gram length (Hildebrand and Vogel, 2009; Leusch et al., 2009a; Zens and Ney, 2006; Zhao and He, 2009). Some of these are conceptualized as a language model that, up to edge effects, exposes the log ratio of (n + 1)-gram matches to n-gram matches. An equivalent linear combination of these features exposes the log n-gram match counts directly. These separate features enable tuning n-gram weights. 3.3 System Weighting Different and correlated system strengths make it important to weight systems when combining their votes on n-grams. The simplest method treats these system weights as a hyper parameter (Heafield et al., 2009; Hildebrand and Vogel, 2009). The hyper parameter might be set to an incr"
2010.amta-papers.34,W09-0407,0,\N,Missing
2010.amta-papers.34,W09-0401,0,\N,Missing
2011.iwslt-evaluation.24,2009.iwslt-papers.4,1,0.871245,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,P10-4002,0,0.0502427,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,W09-0424,0,0.0335352,"nslation. While most features can be summed over grammar rules that comprise a constituent, language models examine cross-constituent N -grams. A straightforward dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applic"
2011.iwslt-evaluation.24,J07-2003,0,0.117216,"ard dynamic programming algorithm [1] accounts for these N grams by combining hypotheses only if their first N − 1 and last N − 1 words are the same. This algorithm takes O(V 2N −2 ) time and space per constituent, where V is the vocabulary size. That is too expensive, so practical decoders implement approximate search by estimating the probability of sentence fragments for purposes of pruning and prioritization. We focus on the decoders [2, 3, 4] that build translations bottom-up by recursively concatenating sentence fragments, estimating their score after each rule application. Cube pruning [5] is a commonly-implemented method to prioritize and prune grammar rule applications. Within a hypergraph node, each non-terminal has a set of possible values. Applying a rule consists of choosing a value for each non-terminal and scoring. Cube pruning estimates that the score under rule application will be the product (or sum in log space) of the scores of the rule itself and of each value. It then uses these estimates to prioritize rule applications, starting with the highest estimated score. This process continues until the pop limit is reached, which acts as a hard limit on the number of ru"
2011.iwslt-evaluation.24,P06-1098,0,0.251231,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,D10-1027,0,0.256726,"tate, the decoder recombines them, thus efficiently reasoning over many sentence fragments via dynamic programming. To increase recombination, it is desirable to encode less than 2N –2 words where possible. In this paper, we make three improvements related to state and concatenation: 1. Minimizing the number of words encoded by left state, enabling more recombination. 2. Encoding left state using pointers into the language model’s data structure, making concatenation faster. 3. Avoiding queries that will not impact the estimated score, speeding concatenation. 183 2. Related Work Some decoders [6, 7] avoid left state entirely by building translations left-to-right. Hypotheses may therefore recombine, for purposes of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to i"
2011.iwslt-evaluation.24,P07-1019,0,0.165067,"s of language modeling, when their right states are equal. Typically, these decoders use beam search, where the beam consists of approximately comparable hypotheses, such as those of equal length. These decoders have the advantage that more recombinations do happen, although they risk repeating work because constituents are evaluated in multiple different contexts. The purpose of our work here is not to decide whether one search algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store t"
2011.iwslt-evaluation.24,W08-0402,0,0.491083,"algorithm or approximation is better, but simply to improve the commonlyimplemented bottom-up strategy. A faster alternative to bottom-up cube pruning is cube growing [8] that lazily generates hypotheses for each constituent instead of generating a fixed number. Like cube pruning, cube growing generates sentence fragments, recombines them into hypotheses, and ranks hypotheses according to estimated language model probabilities. The improvements we discuss here are therefore complementary, since the effect of our work is to improve recombination and ranking within each constituent. Prior work [9] described and implemented algorithms to minimize left and right state in the context of a bottom-up chart decoder. For hypotheses shorter than N –1 words, they store the entire hypothesis in state. In our work, we apply state minimization to all hypotheses, including those shorter than N –1 words. For hypotheses longer than N –1 words, we minimize state in the same way that [9] does. While [9] described an “inefficient implementation of the prefix- and suffix-lookup”, we store the additional information with each n-gram entry, incurring minimal overhead and reusing lookups already performed i"
2011.iwslt-evaluation.24,W11-2123,1,0.837556,"e n-gram that it matched with each query. Decoders can use this information to store at most n words in left or right state, depending on the position of the words being queried. However, this does not fully minimize state, as w1n may be matched by the model, but w1n v may not be in the model for any word v. In this case w1 may be safely omitted from right state but this information is hidden from the decoder. Similarly, were the toolkit to indicate that vw1n does not appear for any word v (i.e. w1n does not extend left), then wn could be omitted from left state. In this work, we extend KenLM [12] to store and expose the necessary information. It implements two data structures, probing and trie. The probing data structure is a hash table from n-grams to probability and backoff and is byte-aligned for speed. The trie data structure is a reverse trie similar to SRILM and IRSTLM but with bit-level packing (i.e. it uses 31 bits to store probability since the sign bit is always negative). Since entries that do not extend right have zero backoff, a special backoff value flags n-grams that do not extend right; this information is provided to the decoder, as is the length of n-gram matched. We"
2011.iwslt-evaluation.24,P02-1040,0,0.0855548,"Missing"
2011.iwslt-evaluation.24,2005.mtsummit-papers.11,1,0.0814087,"Missing"
2011.iwslt-evaluation.24,W11-2103,1,0.794863,"Missing"
2011.iwslt-evaluation.24,J03-4003,0,\N,Missing
2020.acl-main.152,D17-1098,0,0.0140791,"m optimised for F1 will produce the sentences that can truly improve NMT performance. beddings directly with added negative examples. Wieting et al. (2019) obtain sentence embeddings from sub-word embeddings and train a simpler model to distinguish positive and negative examples. Artetxe and Schwenk (2019) refine Guo et al. (2018)’s work and achieve state-of-the-art by looking at the margins of cosine similarities between pairs of nearest neighbours. In our work, using NMT as a similarity scorer relies on constrained decoding (Hokamp and Liu, 2017), which has been applied on image captioning (Anderson et al., 2017) and keyword generation (Lian et al., 2019). 5 6 Related Work A typical parallel corpus mining workflow first aligns parallel documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate all documents into a single language, and shortlist candidate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no"
2020.acl-main.152,J93-1004,0,0.884988,"el documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate all documents into a single language, and shortlist candidate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to compare bilingual sentence embeddings with dynamic programming in linear runtime. There are also research efforts on parallel sentence extraction without the reliance on document alignment. Munteanu and Marcu (2002) acquire parallel phrases from comparable corpora using bilingual tries and seed dict"
2020.acl-main.152,P19-1309,0,0.21073,"aligning documents then aligning sentences within documents (Uszkoreit et al., 2010). However, translated websites may not have matching document structures. More recent methods focus on direct sentence alignment. The results from Building and Using ∗ Equal contribution. Comparable Corpora (BUCC) shared task show that direct sentence alignment can be done by sentence-level lexical comparison, neural comparison or a combination of the two (Zweigenbaum et al., 2017, 2018). A state-of-the-art method maps all sentences to multilingual sentence embeddings and compares them using vector similarity (Artetxe and Schwenk, 2019). Such sentence embeddings are produced by neural encoders, but the rise of the attention mechanism demonstrates that sentence embeddings alone are insufficient to obtain full translation quality (Bahdanau et al., 2015). To exploit quality gains from the attention mechanism, we propose to use a full NMT system with attention to score potentially parallel sentences. The way we avoid pairwise scoring is inspired by constrained decoding in NMT, where the choice of output tokens is constrained to a predefined list (Hokamp and Liu, 2017). Our method works as follows: We designate one language as so"
2020.acl-main.152,W18-6317,0,0.0857147,"e-000081259” and “de-000081260” are the same German sentence, and so are “en-000036940” and “en-000036941” on the English side. Gold alignments only include (de-000081259, en-000036940) and (de000081260, en-000036941), but not the other two. Lastly, it still remains unknown if a system optimised for F1 will produce the sentences that can truly improve NMT performance. beddings directly with added negative examples. Wieting et al. (2019) obtain sentence embeddings from sub-word embeddings and train a simpler model to distinguish positive and negative examples. Artetxe and Schwenk (2019) refine Guo et al. (2018)’s work and achieve state-of-the-art by looking at the margins of cosine similarities between pairs of nearest neighbours. In our work, using NMT as a similarity scorer relies on constrained decoding (Hokamp and Liu, 2017), which has been applied on image captioning (Anderson et al., 2017) and keyword generation (Lian et al., 2019). 5 6 Related Work A typical parallel corpus mining workflow first aligns parallel documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate"
2020.acl-main.152,W19-5207,0,0.0155051,". In our work, using NMT as a similarity scorer relies on constrained decoding (Hokamp and Liu, 2017), which has been applied on image captioning (Anderson et al., 2017) and keyword generation (Lian et al., 2019). 5 6 Related Work A typical parallel corpus mining workflow first aligns parallel documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate all documents into a single language, and shortlist candidate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to com"
2020.acl-main.152,W19-5301,0,0.0424323,"Missing"
2020.acl-main.152,P91-1022,0,0.841655,"mining workflow first aligns parallel documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate all documents into a single language, and shortlist candidate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to compare bilingual sentence embeddings with dynamic programming in linear runtime. There are also research efforts on parallel sentence extraction without the reliance on document alignment. Munteanu and Marcu (2002) acquire parallel phrases from comparable corpo"
2020.acl-main.152,P17-1141,0,0.145407,"gs and compares them using vector similarity (Artetxe and Schwenk, 2019). Such sentence embeddings are produced by neural encoders, but the rise of the attention mechanism demonstrates that sentence embeddings alone are insufficient to obtain full translation quality (Bahdanau et al., 2015). To exploit quality gains from the attention mechanism, we propose to use a full NMT system with attention to score potentially parallel sentences. The way we avoid pairwise scoring is inspired by constrained decoding in NMT, where the choice of output tokens is constrained to a predefined list (Hokamp and Liu, 2017). Our method works as follows: We designate one language as source and one language as target, and build a trie over all target sentences. Then we translate each source sentence to the target language, but constrain left-to-right beam search to follow the trie. In other words, every translation hypothesis is a prefix of some sentence in the target language. Rather than freely choosing which token to extend by, a hypothesis is limited to extensions that exist in the target language corpus. In effect, we are using beam search to limit target language candidates for each source sentence. Our work"
2020.acl-main.152,W18-6478,0,0.0238818,"t, instead of comparing translated text or neural similarity, we use an NMT model to directly score and retrieve sentences onthe-fly during decoding. Second, we approximate pairwise comparison with beam search, so only the top-scoring hypotheses need to be considered at each decoding step. 1672 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1672–1678 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2 Methodology NMT systems can assign a conditional translation probability to an arbitrary sentence pair. Filtering based on this (Junczys-Dowmunt, 2018) won the WMT 2018 shared task on parallel corpus filtering (Koehn et al., 2018). Intuitively, we could score every pair of source and target sentences using a translation system in quadratic time, then return pairs that score highly for further filtering. We approximate this with beam search. 2.1 Trie-constrained decoding We build a prefix tree (trie) containing all sentences in the target language corpus (Figure 1). Then we translate each sentence in the source language corpus using the trie as a constraint on output in the target language. NMT naturally generates translations one token at a"
2020.acl-main.152,P18-4020,1,0.889524,"Missing"
2020.acl-main.152,W17-3204,0,0.0233701,"rch can only find local optima, and a genuine parallel sentence cannot be recovered once it is pruned. Thus the method is vulnerable when parallel sentences have different word ordering. For example, “Por el momento, estoy bebiendo un caf´e” (English: “At the moment, I am drinking a coffee”) can hardly match “I am drinking a coffee at the moment”, because an NMT system will have very low probability of generating a reordered translation, unless using an undesirably large beam size. Moreover, compared to methods that consider textual overlap, NMT is sensitive to domain mismatch and rare words (Koehn and Knowles, 2017). When a system is confused by rare words in the source, we observe that the overly zealous language model in the decoder generates a fluent sentence in the trie rather than a translation. This problem is alleviated when our systems are fine-tuned on indomain data, as shown in Table 2 that there is a gain in F1. Finally we discuss the limitation of evaluating our method on the BUCC task. First, our method based on NMT can be liable to favour 1675 machine-translated texts, whereas the BUCC data is unlikely to contain those. Next, we notice that some parallel sentences in BUCC data are not inclu"
2020.acl-main.152,W02-1037,0,0.217409,"mparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to compare bilingual sentence embeddings with dynamic programming in linear runtime. There are also research efforts on parallel sentence extraction without the reliance on document alignment. Munteanu and Marcu (2002) acquire parallel phrases from comparable corpora using bilingual tries and seed dictionaries. Azpeitia et al. (2018) computes Jaccard similarity of lexical translation overlap. Leong et al. (2018) use an autoencoder and a maximum entropy classifier. Bouamor and Sajjad (2018) consider cosine similarity between averaged multilingual word embeddings. Guo et al. (2018) design a dual encoder model to learn multilingual sentence emConclusion and Future Work We bring a new insight into using NMT as a similarity scorer for sentences in different languages. By constraining on a target side trie during"
2020.acl-main.152,J03-3002,0,0.64384,"machine translation. Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus. We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search. When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions. 1 Introduction Having large and high-quality parallel corpora is critical for neural machine translation (NMT). One way to create such a resource is to mine the web (Resnik and Smith, 2003). Once texts are crawled from the web, they form large collections of data in different languages. To find parallel sentences, a natural way is to score sentence similarity between all possible sentence pairs and extract the topscoring ones. This poses two major challenges: 1. Accurately determining the semantic similarity of a sentence pair in two languages. 2. Efficiently scoring sentence similarity for all possible pairs across two languages. Scoring each source sentence against each target sentence results in unaffordable quadratic time complexity. A typical workflow reduces the search com"
2020.acl-main.152,W18-6488,0,0.043494,"Missing"
2020.acl-main.152,P16-1162,0,0.0493786,"er, an off-the-shelf tool which scores sentence similarity at sentence pair level (S´anchez-Cartagena et al., 2018). Filtering is optional for post-expansion pruning. 2.3 Trie implementation The trie used in our NMT decoding should be fast to query and small enough to fit in memory. We use an array of nodes as the basic data structure. Each node contains a key corresponding to a vocabulary item, as well as a pointer to another array containing all possible continuations in the next level. Binary search is used to find the correct continuations to the next level. With byte pair encoding (BPE) (Sennrich et al., 2016), we can always keep the maximum vocabulary size below 65535, which allows us to use 2-byte integers as keys, minimising memory usage. To integrate the trie into the decoder, we maintain external pointers to possible children nodes in the trie for each active hypothesis. When the hypotheses are expanded at each time step, the pointers are advanced to the next trie depth level. This ensures that cross-referencing the trie has a negligible effect on decoding speed. 3 3.1 Experiments BUCC shared task We evaluate our method on the BUCC shared task, which requires participants to extract parallel s"
2020.acl-main.152,W11-4624,0,0.0252736,"idate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to compare bilingual sentence embeddings with dynamic programming in linear runtime. There are also research efforts on parallel sentence extraction without the reliance on document alignment. Munteanu and Marcu (2002) acquire parallel phrases from comparable corpora using bilingual tries and seed dictionaries. Azpeitia et al. (2018) computes Jaccard similarity of lexical translation overlap. Leong et al. (2018) use an autoencoder and a maximum entropy classifier. Bouamor and Sajjad (2018) consider cosine similarity between averaged multilingu"
2020.acl-main.152,C10-1124,0,0.237929,"To find parallel sentences, a natural way is to score sentence similarity between all possible sentence pairs and extract the topscoring ones. This poses two major challenges: 1. Accurately determining the semantic similarity of a sentence pair in two languages. 2. Efficiently scoring sentence similarity for all possible pairs across two languages. Scoring each source sentence against each target sentence results in unaffordable quadratic time complexity. A typical workflow reduces the search complexity in a coarse-to-fine manner by aligning documents then aligning sentences within documents (Uszkoreit et al., 2010). However, translated websites may not have matching document structures. More recent methods focus on direct sentence alignment. The results from Building and Using ∗ Equal contribution. Comparable Corpora (BUCC) shared task show that direct sentence alignment can be done by sentence-level lexical comparison, neural comparison or a combination of the two (Zweigenbaum et al., 2017, 2018). A state-of-the-art method maps all sentences to multilingual sentence embeddings and compares them using vector similarity (Artetxe and Schwenk, 2019). Such sentence embeddings are produced by neural encoders"
2020.acl-main.152,P19-1453,0,0.126805,"86 84 90 n/a* n/a (3) De→En P R F1 88 61 72 (4) En→De P R F1 96 59 73 (3) ∪ (4) P R F1 81 75 81 96 93 91 98 91 90 96 86 91 73 81 84 83 86 87 79 82 86 88 86 88 87 87 91 91 87 91 * Bicleaner does not have a published classifier model for Ru-En. Table 1: Precision, recall and F1 of our methods on BUCC sample set. data. We fine-tune our De→En and En→De systems on News Commentary, excluding the sentence pairs which appear in BUCC train or test sets. As BUCC submissions are asked not to use News Commentary, this is only used to contrast with our own results on the train set. Azpeitia et al. (2018) Wieting et al. (2019) Artetxe and Schwenk (2019) (v2) pre-expansion + CE ∪ BC + fine-tuning Train 84.3 77.5 91.9 83.0 85.5 Test 85.5 n/a* 95.6 83.9 n/a * Wieting et al. directly evaluated on the public train set. Table 2: F1 scores of our method and other methods on BUCC De-En train and test sets. 4 Results and Analysis Experiments on the sample data in Table 1 show that pre-expansion pruning outperforms postexpansion by about 10 F1 points. This can be explained by the fact that the decoder has a better chance to generate the correct target sentence if the available vocabulary is constrained. For both variants, th"
2020.acl-main.152,W17-2512,0,0.0229218,"ainst each target sentence results in unaffordable quadratic time complexity. A typical workflow reduces the search complexity in a coarse-to-fine manner by aligning documents then aligning sentences within documents (Uszkoreit et al., 2010). However, translated websites may not have matching document structures. More recent methods focus on direct sentence alignment. The results from Building and Using ∗ Equal contribution. Comparable Corpora (BUCC) shared task show that direct sentence alignment can be done by sentence-level lexical comparison, neural comparison or a combination of the two (Zweigenbaum et al., 2017, 2018). A state-of-the-art method maps all sentences to multilingual sentence embeddings and compares them using vector similarity (Artetxe and Schwenk, 2019). Such sentence embeddings are produced by neural encoders, but the rise of the attention mechanism demonstrates that sentence embeddings alone are insufficient to obtain full translation quality (Bahdanau et al., 2015). To exploit quality gains from the attention mechanism, we propose to use a full NMT system with attention to score potentially parallel sentences. The way we avoid pairwise scoring is inspired by constrained decoding in"
2020.acl-main.152,P06-1062,0,0.0387858,"amples. Artetxe and Schwenk (2019) refine Guo et al. (2018)’s work and achieve state-of-the-art by looking at the margins of cosine similarities between pairs of nearest neighbours. In our work, using NMT as a similarity scorer relies on constrained decoding (Hokamp and Liu, 2017), which has been applied on image captioning (Anderson et al., 2017) and keyword generation (Lian et al., 2019). 5 6 Related Work A typical parallel corpus mining workflow first aligns parallel documents to limit the search space for sentence alignment. Early methods rely on webpage structure (Resnik and Smith, 2003; Shi et al., 2006). Later, Uszkoreit et al. (2010) translate all documents into a single language, and shortlist candidate document pairs based on TFIDF-weighted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, B"
2020.acl-main.152,D19-1136,0,0.0109313,"ted n-grams. Recently, Guo et al. (2019) suggest a neural method to compare document embeddings obtained from sentence embeddings . With the assumption that matched documents are parallel (no cross-alignment), sentence alignment can be done by comparing sentence length in words (Brown et al., 1991) or characters (Gale and Church, 1993), which is then improved by adding lexical features (Varga et al., 2005). After translating texts into the same language, BLEU can also be used to determine parallel texts, by anchoring the most reliable alignments first (Sennrich and Volk, 2011). Most recently, Thompson and Koehn (2019) propose to compare bilingual sentence embeddings with dynamic programming in linear runtime. There are also research efforts on parallel sentence extraction without the reliance on document alignment. Munteanu and Marcu (2002) acquire parallel phrases from comparable corpora using bilingual tries and seed dictionaries. Azpeitia et al. (2018) computes Jaccard similarity of lexical translation overlap. Leong et al. (2018) use an autoencoder and a maximum entropy classifier. Bouamor and Sajjad (2018) consider cosine similarity between averaged multilingual word embeddings. Guo et al. (2018) desi"
2020.acl-main.417,bojar-etal-2012-joy,0,0.0615774,"Missing"
2020.acl-main.417,P91-1022,0,0.752852,"ctors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents in"
2020.acl-main.417,buck-etal-2014-n,1,0.887657,"Missing"
2020.acl-main.417,W16-2347,1,0.931151,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,W11-1218,0,0.0324271,"Missing"
2020.acl-main.417,W16-2365,1,0.928849,"set of patterns for language marking or simple Levenshtein distance (Le et al., 2016). Content matching requires crossing the language barrier at some point, typically by using bilingual dictionaries or translating one of the documents into the other document’s language (Uszkoreit et al., 2010). Documents may be represented by vectors over word frequencies, typically td-idf-weighted. Vectors may also be constructed over bigrams (Dara and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sente"
2020.acl-main.417,2012.eamt-1.60,0,0.0178318,"2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creat"
2020.acl-main.417,P05-1074,0,0.166175,"s Translation. We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems. 1 2 Introduction Parallel corpora are essential for building highquality machine translation systems and have found uses in many other natural language applications, such as learning paraphrases (Bannard and Callison-Burch, 2005; Hu et al., 2019) or cross-lingual projection of language tools (Yarowsky et al., 2001). We report on work to create the largest publicly available parallel corpora by crawling hundreds of thousands of web sites, using open source tools. The processing pipeline consists of the steps: crawling, text extraction, document alignment, sentence alignment, and sentence pair filtering. We describe these steps in detail in Sections 4–8. For some of these steps we evaluate several methods empirically in terms of their impact on machine translation quality. We provide the data resources used in these ev"
2020.acl-main.417,W19-5435,1,0.894745,"Missing"
2020.acl-main.417,2005.mtsummit-papers.11,1,0.214853,"modular pipeline that allows harvesting parallel corpora from multilingual websites or from preexisting or historical web crawls such as the one available as part of the Internet Archive.2 1 2 https://github.com/bitextor/bitextor https://archive.org/ Related Work While the idea of mining the web for parallel data has been already pursued in the 20th century (Resnik, 1999), the most serious efforts have been limited to large companies such as Google (Uszkoreit et al., 2010) and Microsoft (Rarrick et al., 2011), or targeted efforts on specific domains such as the Canadian Hansards and Europarl (Koehn, 2005). The book Bitext Alignment (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (201"
2020.acl-main.417,W19-5404,1,0.891381,"Missing"
2020.acl-main.417,W18-6453,1,0.89611,"Missing"
2020.acl-main.417,W19-5438,0,0.038302,"Missing"
2020.acl-main.417,W16-2371,0,0.0501587,"Missing"
2020.acl-main.417,I08-2120,0,0.0456513,"age/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A large collection of corpora is maintained at the OPUS web site4 (Tiede"
2020.acl-main.417,L16-1147,0,0.0317824,"t (Tiedemann, 2011) describes some of the challenges in greater detail. 2.1 Acquisition Efforts Most publicly available parallel corpora are the result of targeted efforts to extract the translations from a specific source. The French–English Canadian Hansards3 were used in the earliest work on statistical machine translation. A similar popular corpus is Europarl (Koehn, 2005), used throughout the WMT evaluation campaign. Multi-lingual web sites are attractive targets. Rafalovitch and Dale (2009); Ziemski et al. (2015) extract data from the United Nations, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English."
2020.acl-main.417,W16-2372,0,0.0140892,"ra and Lin, 2016) or even higher order n-grams 4 http://opus.lingfil.uu.se/ (Uszkoreit et al., 2010). The vectors are then typically matched with cosine similarity (Buck and Koehn, 2016a). The raw vectors may be recentered around the mean vector for a web domain (Germann, 2016) Document alignment quality can be improved with additional features such ratio of shared links, similarity of link URLs, ratio of shared images, binary feature indicating if the documents are linked, DOM structure similarity (Espl`a-Gomis et al., 2016), same numbers (Papavassiliou et al., 2016), or same named entities (Lohar et al., 2016). Guo et al. (2019) introduce the use of document embeddings, constructed from sentence embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuris"
2020.acl-main.417,2007.mtsummit-papers.42,0,0.0236436,"e in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these efforts rely on methods and implementations that are quite specific for each use case, not documented in great detail, and not publicly available. A discussion of the pitfalls during the construction of parallel corpora is given by Kaalep and Veskis (2007). A la"
2020.acl-main.417,W03-0320,0,0.154143,"ons, T¨ager (2011) from European Patents, Lison and Tiedemann (2016) from a collection of TV and movie subtitles. Cettolo et al. (2012) explain the creation of a multilingual parallel corpus of subtitles from the TED Talks website which is popular due to its use in the IWSLT evaluation campaign. 3 https://www.isi.edu/natural-language/ download/hansard/ 4555 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555–4567 c July 5 - 10, 2020. 2020 Association for Computational Linguistics There are also various efforts targeted at a single language pair. Martin et al. (2003) build a parallel corpus for Inuktitut–English. Utiyama and Isahara (2003); Fukushima et al. (2006) worked on creating Japanese–English corpora. Uchiyama and Isahara (2007) report on the efforts to build a Japanese–English patent corpus and Macken et al. (2007) on efforts on a broad-based Dutch–English corpus. Li and Liu (2008) mine the web for a Chinese–English corpus. A large Czech–English corpus from various sources was collected (Bojar et al., 2010), linguistically annotated (Bojar et al., 2012), and has been continuously extended to over 300 million words (Bojar et al., 2016). All these e"
2020.acl-main.417,moore-2002-fast,0,0.25649,"nce embeddings, to the document alignment task. 2.3 Sentence Alignment Early sentence aligners (Brown et al., 1991; Gale and Church, 1993) use scoring functions based only on the number of words or characters in each sentence and alignment algorithms based on dynamic programming. Europarl, for example, used metadata to align paragraphs, typically consisting of 2-5 sentences, and using Gale and Church (1993)’s method to align sentences within corresponding paragraphs. Later work added lexical features and heuristics to speed up search, such as limiting the search space to be near the diagonal (Moore, 2002; Varga et al., 2005). More recent work introduced scoring methods that use MT to get both documents into the same language (Sennrich and Volk, 2010) or use pruned phrase tables from a statistical MT system (Gomes and Lopes, 2016). Both methods “anchor” highprobability 1–1 alignments in the search space and then fill in and refine alignments. They later propose an extension (Sennrich and Volk, 2011) in which an SMT system is bootstrapped from an initial alignment and then used in Bleualign. Vecalign (Thompson and Koehn, 2019) is a sentence alignment method that relies on bilingual sentence emb"
2020.acl-main.417,J05-4003,0,0.157471,"Our work exploits web sites that provide roughly the same content in multiple languages, leading us to the assumption to find pairs of web pages which are translations of each other, with translated sentences following the same order. This assumption does not hold in less consistently translated web content such as Wikipedia, or accidental parallel sentence found in news stories about the same subject matter written in multiple languages. There have been increasing efforts to mine sentence pairs from large pools of multi-lingual text, which are treated as unstructured bags of sen4557 tences. Munteanu and Marcu (2005) use document retrieval and a maximum entropy classifier to identify parallel sentence pairs in a multi-lingual collection of news stories. Bilingual sentence embeddings (Guo et al., 2018) and multilingual sentence embeddings (Artetxe and Schwenk, 2018) were tested on their ability to reconstruct parallel corpora. This lead to work to construct WikiMatrix, a large corpus of parallel sentences from Wikipedia (Schwenk et al., 2019) based on cosine distance of their crosslingual sentence embeddings. 3 Identifying Multi-Lingual Web Sites Since the start of the collection effort in 2015, we identif"
2020.acl-main.688,D18-1399,0,0.0595878,"Missing"
2020.acl-main.688,W17-4715,1,0.894071,"Missing"
2020.acl-main.688,Y17-1038,0,0.206211,"rmed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, ou"
2020.acl-main.688,P19-1120,0,0.0716333,"Missing"
2020.acl-main.688,D18-1101,0,0.0453867,"Missing"
2020.acl-main.688,W18-6325,0,0.190356,"an off-the-shelf transfer learning baseline and simplified versions of the transfer learning scheme. If a simplified version recovers some of the quality gains of full transfer learning, it suggests that the simplified version has captured some of the information being transferred. Since information may be transferred redundantly, our claims are limited to sufficiency rather than exclusivity. Transferring word embeddings is not straightforward since languages have different vocabularies. Zoph et al. (2016) claimed that vocabulary alignment is not necessary, while Nguyen and Chiang (2017) and Kocmi and Bojar (2018) suggest a joint vocabulary. We find that the vocabulary has to be aligned before transferring the embedding to achieve a substantial improvement. Transfer learning without the embedding or with vocabulary mismatches is still possible, but with lower quality. Conversely, transferring only the word embeddings can be worse than transferring nothing at all. A rudimentary model of machine translation consists of alignment and token mapping. We hypothesize that these capabilities are transferred across languages. To test this, we experiment with transferring from auto-encoders that learn purely dia"
2020.acl-main.688,J82-2005,0,0.629768,"Missing"
2020.acl-main.688,N18-1032,0,0.136034,"transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was trained from scratch"
2020.acl-main.688,D18-1398,0,0.118764,"transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was trained from scratch"
2020.acl-main.688,I17-2050,0,0.475249,"sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was tra"
2020.acl-main.688,Q17-1024,0,0.0607087,"Missing"
2020.acl-main.688,N19-1119,0,0.0382838,"Missing"
2020.acl-main.688,P18-4020,1,0.876608,"Missing"
2020.acl-main.688,W18-6319,0,0.0194858,"task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six decoder layers for all experiments with the default hyperparameters (Vaswani et al., 2017). Training and decoding use Marian (Junczys-Dowmunt et al., 2018), while evaluation uses SacreBLEU (Post, 2018). 1 http://www.panl10n.net/english/ OutputsIndonesia2.htm Results Parent En→De En→Ru De→En Ru→En High-Resource Datasets We use German-English and Russian-English datasets for our parent models. Our GermanEnglish dataset is taken from the WMT17 news translation task (Bojar et al., 2017). Our RussianEnglish is taken from the WMT18 task (Bojar et al., 2018). For both pairs, we preprocess the input with byte-pair encoding (Sennrich et al., 2016b). 3.2 3.4 My→En 4.0 17.5 17.8 17.3 17.1 BLEU Id→En 20.6 27.5 27.4 26.3 26.8 Tr→En 19.0 20.2 20.3 20.1 20.6 Table 1: Transfer learning performance across d"
2020.acl-main.688,E17-2025,0,0.0320856,"omputational Linguistics, pages 7701–7710 c July 5 - 10, 2020. 2020 Association for Computational Linguistics period, though only real language parents yielded faster training. 2 Related Work Transfer learning has been successfully used in lowresource scenarios for NMT. Zoph et al. (2016) gain 5 BLEU points in Uzbek–English by transferring from French–English. Their style of transfer learning copies the entire model, including word embeddings, ignoring the vocabulary mismatch between parent and child. They used separate embeddings for source and target language words, whereas tied embeddings (Press and Wolf, 2017; Vaswani et al., 2017) have since become the de-facto standard in low-resource NMT. Tied embeddings provide us with the opportunity to revisit some of their findings. In Section 5, we find an English–English copy model does work as a parent with tied embeddings, whereas Zoph et al. (2016) reported no gains from a copy model with untied embeddings. Methods to cope with vocabulary mismatch have improved since Zoph et al. (2016). Kocmi and Bojar (2018) suggest that a shared vocabulary between the parent language and the child is beneficial, though this requires knowledge of the child languages w"
2020.acl-main.688,N18-2084,0,0.0237937,"ngs, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments hav"
2020.acl-main.688,P16-1009,1,0.917615,"out the experimental setup used for the rest of the paper. 3.1 Low-Resource Datasets We use the following datasets: Burmese–English: For our My→En parallel data, we used 18k parallel sentences from the Asian Language Treebank (ALT) Project (Ding et al., 2018, 2019) collected from news articles. Indonesian–English: Id→En parallel data consists of 22k news-related sentences, which are taken from the PAN Localization BPPT corpus.1 This dataset does not have a test/validation split. Hence we randomly sample 2000 sentences to use as test and validation sets. We augment our data by backtranslating (Sennrich et al., 2016a) News Crawl from 2015. Our total training set (including the back-translated sentences) consists of 88k pairs of sentences. Turkish–English: Tr→En data comes from the WMT17 news translation task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six"
2020.acl-main.688,P16-1162,1,0.714863,"out the experimental setup used for the rest of the paper. 3.1 Low-Resource Datasets We use the following datasets: Burmese–English: For our My→En parallel data, we used 18k parallel sentences from the Asian Language Treebank (ALT) Project (Ding et al., 2018, 2019) collected from news articles. Indonesian–English: Id→En parallel data consists of 22k news-related sentences, which are taken from the PAN Localization BPPT corpus.1 This dataset does not have a test/validation split. Hence we randomly sample 2000 sentences to use as test and validation sets. We augment our data by backtranslating (Sennrich et al., 2016a) News Crawl from 2015. Our total training set (including the back-translated sentences) consists of 88k pairs of sentences. Turkish–English: Tr→En data comes from the WMT17 news translation task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six"
2020.acl-main.688,P18-1072,0,0.0509496,"Missing"
2020.acl-main.688,D16-1163,0,0.335754,"arning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation sy"
2020.amta-research.10,D17-1300,0,0.0203578,"e 112 Baseline (fp32) Quantized (int8) CPUs 1 2 4 1 2 4 Time (s) 1260.8 841.6 575.8 511.6 435.9 334.3 6:6 Layers Tok/Sec 33.3 49.9 73.0 82.1 96.4 125.7 BLEU 22.1 22.1 22.1 22.0 22.0 22.0 Time (s) 585.0 404.7 283.2 285.7 242.3 173.0 20:2 Layers Tok/Sec BLEU 71.8 23.0 103.8 23.0 148.3 23.0 147.0 22.8 173.4 22.8 242.9 22.8 Table 3: CPU decoding times and SacreBLEU (Post, 2018) scores for FI-EN newstest2019 with and without 8-bit quantization for both standard (6:6 layer) and deep encoder (20:2 layer) transformer models as described in §3. Models use a vocabulary selection shortlist of 200 items (Devlin, 2017) and translate one sentence at a time (batch size of 1). Benchmarks are run on an EC2 c5.12xlarge instance (Cascade Lake processor) and limited to using 1, 2, or 4 CPU cores. Ott et al. (2018) Plateau-Reduce DE–EN BLEU Time 34.7 30h 34.9 28h EN–FI BLEU Time 20.1 14h 20.7 12h Table 4: SacreBLEU (Post, 2018) scores (newstest2019) and training times (8 NVIDIA V100 GPUs) for a 20 encoder 2 decoder layer transformer using the training setup described by Ott et al. (2018) and plateau-reduce, both implemented in Sockeye 2. also require additional computation per update (synchronizing data across dist"
2020.amta-research.10,E17-3017,0,0.0214239,"amazon.com Amazon Kenneth Heafield∗ translate@kheafield.com Efficient Translation Limited Abstract We present Sockeye 2, a modernized and streamlined version of the Sockeye neural machine translation (NMT) toolkit. New features include a simplified code base through the use of MXNet’s Gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production. 1 Introduction Sockeye (Hieber et al., 2017) is a versatile toolkit for research in the fast-moving field of NMT. Since the initial release, it has been used in at least 25 scientific publications, including winning submissions to WMT evaluations (Schamper et al., 2018). Sockeye also powers Amazon Translate, showing industrial-strength performance in addition to the flexibility needed in academic environments. Moreover, we are excited to see that hardware manufacturers are contributing to optimizing MXNet (Chen et al., 2015) and Sockeye for speed. Intel has demonstrated large performance gains for Sockeye inference on Intel Skylake proc"
2020.amta-research.10,W18-6301,0,0.0794504,"ime (s) 585.0 404.7 283.2 285.7 242.3 173.0 20:2 Layers Tok/Sec BLEU 71.8 23.0 103.8 23.0 148.3 23.0 147.0 22.8 173.4 22.8 242.9 22.8 Table 3: CPU decoding times and SacreBLEU (Post, 2018) scores for FI-EN newstest2019 with and without 8-bit quantization for both standard (6:6 layer) and deep encoder (20:2 layer) transformer models as described in §3. Models use a vocabulary selection shortlist of 200 items (Devlin, 2017) and translate one sentence at a time (batch size of 1). Benchmarks are run on an EC2 c5.12xlarge instance (Cascade Lake processor) and limited to using 1, 2, or 4 CPU cores. Ott et al. (2018) Plateau-Reduce DE–EN BLEU Time 34.7 30h 34.9 28h EN–FI BLEU Time 20.1 14h 20.7 12h Table 4: SacreBLEU (Post, 2018) scores (newstest2019) and training times (8 NVIDIA V100 GPUs) for a 20 encoder 2 decoder layer transformer using the training setup described by Ott et al. (2018) and plateau-reduce, both implemented in Sockeye 2. also require additional computation per update (synchronizing data across distributed GPUs and checking reduced precision operations for overflow). This overhead can be amortized by significantly increasing the effective batch size; gradients are aggregated per-GPU for"
2020.amta-research.10,W18-6319,0,0.226328,"ne in an external advisory capacity. 1 https://www.intel.ai/amazing-inference-performance-with-intel-xeonscalable-processors/#gs.wrgsji 2 https://mxnet.apache.org/versions/1.6/api/python/docs/api/gluon/index.html Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 110 DE–EN EN–DE FI–EN EN–FI Layers BLEU Latency BLEU Latency BLEU Latency BLEU Latency 6:6 35.5 602 37.9 791 22.2 575 20.5 808 10:10 35.4 970 37.8 1238 22.3 863 20.8 1258 34.8 293 37.6 357 23.2 257 20.9 368 20:2 Table 1: SacreBLEU scores (Post, 2018) and single-sentence latency in milliseconds on newstest2019 for models trained on WMT19 constrained data with varying numbers of encoder and decoder layers. Latency values are the 90th percentile of translation time when translating each sentence individually (no batching). We measure single sentence decoding latency on an EC2 c5.2xlarge instance with 4 CPU cores. We report the average over three independent training runs. by converting them into computation graphs for maximum performance. Adopting this programming model significantly simplifies Sockeye 2’s training and inference code, reduci"
2020.amta-research.10,N18-3014,0,0.014874,"talizing the first character of each word. We compare a baseline model that was trained on cased input (no source factors) against all “SF-*” methods. The factored models also use BPE type factors as introduced by Sennrich and Haddow (2016). Models use the 20:2 transformer architecture and training settings described in §3. Shown in Table 2, encoding case information with source factors is an effective way to improve robustness against case variation with the two versions of “SF-case” performing best. 3.2 Quantization for Inference Sockeye 2 now supports 8-bit quantized matrix multiplication (Quinn and Ballesteros, 2018) on CPUs based on the intgemm library.3 By scaling values such that 127 corresponds to the maximum absolute value found in a tensor, matrix multiplication can be conducted with 8-bit integer representations in place of the default 32-bit floating-point representations without significant degradation of overall model accuracy. Parameters can either be quantized offline and stored in a smaller model file or quantized on the fly at loading time. Activations are quantized on the fly while other operators that consume far less runtime remain as 32-bit floats. Latency-sensitive applications typicall"
2020.amta-research.10,W18-6426,0,0.0174914,"res include a simplified code base through the use of MXNet’s Gluon API, a focus on state of the art model architectures, distributed mixed precision training, and efficient CPU decoding with 8-bit quantization. These improvements result in faster training and inference, higher automatic metric scores, and a shorter path from research to production. 1 Introduction Sockeye (Hieber et al., 2017) is a versatile toolkit for research in the fast-moving field of NMT. Since the initial release, it has been used in at least 25 scientific publications, including winning submissions to WMT evaluations (Schamper et al., 2018). Sockeye also powers Amazon Translate, showing industrial-strength performance in addition to the flexibility needed in academic environments. Moreover, we are excited to see that hardware manufacturers are contributing to optimizing MXNet (Chen et al., 2015) and Sockeye for speed. Intel has demonstrated large performance gains for Sockeye inference on Intel Skylake processors.1 NVIDIA is working on significant performance improvements for Sockeye’s Transformer (Vaswani et al., 2017) implementation through fused operators and an optimized beam search. This paper discusses Sockeye 2’s streamli"
2020.amta-research.10,W16-2209,0,0.0639044,"y the number of decoder layers. For WMT19 FI-EN and EN-FI benchmarks (Barrault et al., 2019), the 20:2 model outperforms both the 6:6 model and the 10:10 model in terms of BLEU. The 20:2 model also has roughly half the decoding latency of the 6:6 model and roughly one third the latency of the 10:10 model. The relative efficiency of encoder versus decoder layers can be attributed to (1) the ability to parallelize across input tokens, (2) attention to only input tokens, and (3) not needing to run beam search on the source side. 3.1 Source Factors Sockeye supports source factors in the spirit of Sennrich and Haddow (2016), additional representations that are combined with word embeddings prior to the first encoder layer. In Sockeye 2, we improve source factor support by allowing different types of embedding combinations (concatenation, summation, or average), as well as weight sharing between source factor and word embeddings. As an example application, we use source factors to represent input case. Variations in case pose a challenge for machine translation systems as different orthographic variations are considered to be independent by the translation model (e.g., “case” is different from “Case” and both are"
2020.emnlp-main.211,W19-5301,0,0.0403231,"Missing"
2020.emnlp-main.211,2020.acl-main.360,0,0.0697694,"Missing"
2020.emnlp-main.211,D19-5546,0,0.0261853,"Missing"
2020.emnlp-main.211,W18-6412,1,0.840192,"is considered a low-resource, even with additional back-translated data. In contrast, English→German is a high-resource language pair with English not being a target language. We trained and decoded our models using the Marian machine translation toolkit (JunczysDowmunt et al., 2018a). Turkish→English We use all the parallel data allowed by the constrained condition of the WMT18 (Bojar et al., 2018). The corpus consists of ~200 000 parallel sentences plus an additional 800 000 sampled from News Crawl and backtranslated using a shallow NMT model trained on the existing small bilingual corpora (Haddow et al., 2018). We use the development and test sets provided in 2016. We also evaluate on the 2017 and 2018 testsets. The preprocessing follows the steps of normalisation, tokenisation, truecasing using Moses scripts, and BPE segmentation (Sennrich et al., 2016). The vocabulary is shared and contains 36000 words. The architecture is transformer-big (Vaswani et al., 2017), trained using default recommended settings for such a model in Marian toolkit.4 The models trained until cross-entropy has stopped improving for 10 consecutive validations, and select model checkpoints with highest BLEU scores. English→Ge"
2020.emnlp-main.211,W18-6415,0,0.0428739,"Missing"
2020.emnlp-main.211,W19-5321,0,0.0148829,"until cross-entropy has stopped improving for 10 consecutive validations, and select model checkpoints with highest BLEU scores. English→German To measure impact on the speed of a highly optimized system, we follow the Workshop on Neural Generation and Translation 2020 Efficiency Shared task.5 The shared task specified English→German translation under the WMT 2019 data condition (Barrault et al., 2019). As is standard for efficient translation, we applied teacher-student training (Kim and Rush, 2016) using the sentence-level system submitted by Microsoft to the WMT 2019 News Translation Task (Junczys-Dowmunt, 2019). The student models have a standard 6-layers transformer encoder (Vaswani et al., 2017) but the decoder is a faster two-layer Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019). The embedding dimension is 256, feed-forward network size is 1536. The models use shared vocabulary of 32,000 subword units created with SentencePiece (Kudo and Richardson, 2018). All student models were trained on 13M sentences of available parallel data, using the concatenated English-German WMT testsets from 20162018 as a validation set.6 The models were trained 3 Comparisons are based on their reported number"
2020.emnlp-main.211,P18-4020,1,0.900195,"Missing"
2020.emnlp-main.211,W18-2716,1,0.883364,"Missing"
2020.emnlp-main.211,D16-1139,0,0.0206918,"et al., 2017), trained using default recommended settings for such a model in Marian toolkit.4 The models trained until cross-entropy has stopped improving for 10 consecutive validations, and select model checkpoints with highest BLEU scores. English→German To measure impact on the speed of a highly optimized system, we follow the Workshop on Neural Generation and Translation 2020 Efficiency Shared task.5 The shared task specified English→German translation under the WMT 2019 data condition (Barrault et al., 2019). As is standard for efficient translation, we applied teacher-student training (Kim and Rush, 2016) using the sentence-level system submitted by Microsoft to the WMT 2019 News Translation Task (Junczys-Dowmunt, 2019). The student models have a standard 6-layers transformer encoder (Vaswani et al., 2017) but the decoder is a faster two-layer Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019). The embedding dimension is 256, feed-forward network size is 1536. The models use shared vocabulary of 32,000 subword units created with SentencePiece (Kudo and Richardson, 2018). All student models were trained on 13M sentences of available parallel data, using the concatenated English-German WMT"
2020.emnlp-main.211,D18-2012,0,0.0199691,"WMT 2019 data condition (Barrault et al., 2019). As is standard for efficient translation, we applied teacher-student training (Kim and Rush, 2016) using the sentence-level system submitted by Microsoft to the WMT 2019 News Translation Task (Junczys-Dowmunt, 2019). The student models have a standard 6-layers transformer encoder (Vaswani et al., 2017) but the decoder is a faster two-layer Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019). The embedding dimension is 256, feed-forward network size is 1536. The models use shared vocabulary of 32,000 subword units created with SentencePiece (Kudo and Richardson, 2018). All student models were trained on 13M sentences of available parallel data, using the concatenated English-German WMT testsets from 20162018 as a validation set.6 The models were trained 3 Comparisons are based on their reported numbers, which use non-standard tokenized BLEU (Post, 2018). 2668 4 Available via --task transformer-big. https://sites.google.com/view/wngt20 6 The validation sentences were not teacher-translated. 5 until BLEU stopped improving for 20 consecutive validations to overfit the teacher, and the checkpoint with highest BLEU scores was selected. Since a student model sho"
2020.emnlp-main.211,W18-6319,0,0.0203687,"ers transformer encoder (Vaswani et al., 2017) but the decoder is a faster two-layer Simpler Simple Recurrent Unit (SSRU) (Kim et al., 2019). The embedding dimension is 256, feed-forward network size is 1536. The models use shared vocabulary of 32,000 subword units created with SentencePiece (Kudo and Richardson, 2018). All student models were trained on 13M sentences of available parallel data, using the concatenated English-German WMT testsets from 20162018 as a validation set.6 The models were trained 3 Comparisons are based on their reported numbers, which use non-standard tokenized BLEU (Post, 2018). 2668 4 Available via --task transformer-big. https://sites.google.com/view/wngt20 6 The validation sentences were not teacher-translated. 5 until BLEU stopped improving for 20 consecutive validations to overfit the teacher, and the checkpoint with highest BLEU scores was selected. Since a student model should mimic the teacher as closely as possible, we did not use regularization like dropout and label smoothing. Other training hyperparameters were Marian defaults for training a Transformer Base model.7 Student models have sharp probability distributions so we translate using beam size 1. Th"
2020.emnlp-main.211,K16-1029,0,0.0233349,"Missing"
2020.emnlp-main.211,P16-1162,0,0.517694,"tation described by Vaswani et al. (2017) initialises attention matrices based on the embedding dimension and those matrices are split into separate heads. That means the fewer heads there are set to be in a model, the larger they are. To compare models with different number of heads fairly, we fix their size to a constant instead. We use all the parallel data allowed by the constrained condition of the WMT17 news task (Bojar et al., 2017) for English→German (4.56M sentences) following a standard preprocessing: normalisation, tokenisation, truecasing using Moses scripts, and BPE segmentation (Sennrich et al., 2016) with 36000 subwords. We tried training a model with 32 heads but could not due to memory constraints. For that reason, we start with a typical transformer-big (Vaswani et al., 2017) architecture using recommended hyperparameters. It has 16 heads of size 64 (64 × 16 = 1024). Then, we train Voita et al. (2019) pruning Using the same language pair and dataset, we tried a pruning method presented by Voita et al. (2019). We used their Tensorflow implementation2 with their training scripts, in which they set up a transformer-base architecture that it to be pruned globally. The pruning scheme requir"
2020.emnlp-main.211,P19-1282,0,0.0606447,"Missing"
2020.emnlp-main.211,P19-1580,0,0.404483,"al., 2019) to prune in early training, achieving a better trade-off between pruning and quality than pruning after training (Voita et al., 2019). Our main goal is faster inference speed for machine translation deployment with minimal impact on quality. Pruning heads means they can be removed from the model entirely (with other heads shifted down), resulting in a layer configured to have fewer heads. Unlike most work on pruning (Zhu and Gupta, 2017; Gale et al., 2019), there is no need for sparse matrices, block-sparse matrix operators, or additional masking. In particular, we go further than Voita et al. (2019) by removing rather than masking. In this paper we combine findings of both Voita et al. (2019) (“what”) and Frankle and Carbin (2019) (“how”) to prune attention heads. First, we define a training scheme based on an iterative approach that does not require full convergence of a model each time partial pruning takes place. To analyse the impact of pruning in a variety of settings, we experiment with a stock and highly optimised system across two language pairs: Turkish→English and English→German. We present and analyse our results in Sections 7 and 8. Our key findings show that: 1. The lottery"
2020.ngt-1.1,W19-5301,0,0.0795298,"Missing"
2020.ngt-1.1,N19-1423,0,0.00776584,"o 15 system submission papers. We elicted two double-blind reviews for each submission, avoiding conflicts of interest. With regards to thematology there were 8 papers with a focus on Natural Language Generation and 8 with the application of Machine Translation 1 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 1–9 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d in mind. The underlying emphasis across submissions was placed this year on capitalizing on the use of pre-training models (e.g., BERT; (Devlin et al., 2019) especially for low-resource datasets. The quality of the accepted publications was very high; there was a significant drop in numbers though in comparison to last year (36 accepted papers from 68 submissions) which is most likely due to the extra overhead on conducting research under lockdown policies sanctioned globally due to COVID19 pandemic. 3 GPU is relatively small compared to the NVIDIA V100 GPU, but the newer Turing architecture introduces support for 4-bit and 8-bit integer operations in Tensor Cores. In practice, however, participants used floating-point operations on the GPU even t"
2020.ngt-1.1,D13-1176,0,0.0312763,"Second, we describe the results of the three shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document-level generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language and 3) STAPLE task: creation of as many possible translations of a given input text. This last shared task was organised by Duolingo. 1 Introduction 2 Neural sequence to sequence models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) are the workhorse behind a wide variety of different natural language processing tasks such as machine translation, generation, summarization and simplification. The 4th Workshop on Neural Machine Translation and Generation (WNGT 2020) provided a forum for research in applications of neural models to machine translation and other language generation tasks (including summarization, NLG from structured data, dialog response generation, among others). Overall, the workshop was held with two goals. First, it aimed to synthesize the current state of"
2020.ngt-1.1,W04-1013,0,0.0806851,"of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differences. In particular, we"
2020.ngt-1.1,2020.ngt-1.28,0,0.0265263,"tput, but in certain cases, it is desirable to have many possible translations of a given input text. At Duolingo, the world’s largest online language-learning platform,7 we grade translationbased challenges with sets of human-curated acceptable translation options. Given the many ways of expressing a piece of text, these sets are slow to create, and may be incomplete. This process is ripe for improvement with the aid of rich multi-output translation and paraphrase systems. To this end, we introduce a shared task called STAPLE: Simultaneous Translation and Paraphrasing for Language Education (Mayhew et al., 2020). 4.4 5.1 4.3 Baselines We prepared two baselines for different tracks: FairSeq-19 We use FairSeq (Ng et al., 2019) (WMT’19 single model6 ) for MT and MT+NLG tracks. Submitted Systems One team participated in the task, who focused on the German-English MT track of the task. In this shared task, participants are given a training set consisting of 2500 to 4000 English sentences (or prompts), each of which is paired with a list of comprehensive translations in the target language, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English promp"
2020.ngt-1.1,D18-1325,0,0.021768,"guage, weighted and ordered by normalized learner response frequency. At test time, participants are given 500 English prompts, and are required to produce the set of comprehensive translations for each prompt. We also provide a high-quality automatic reference translation for each prompt, in the event that a participant wants to work on paraphrase-only approaches. The target languages were Hungarian, Japanese, Korean, Portuguese, and Vietnamese. Team FJWU developed a system around Transformer-based sequence-to-sequence model. Additionally, the model employed hierarchical attention following (Miculicich et al., 2018) for both encoder and decoder to account for the documentlevel context. The system was trained in a twostage process, where a base (sentence-level) NMT model was trained followed by the training of hierarchcal attention networks component. To handle the scarcity of in-domain translation data, they experimented with upsizing the in-domain data up to three times to construct training data. Their ablation experiments showed that this upsizing of in-domain data is effective at increasing the BLEU score. 4.5 Task Description 5.2 Submitted Systems There were 20 participants who submitted to the deve"
2020.ngt-1.1,W19-5333,0,0.0306894,"Missing"
2020.ngt-1.1,P02-1040,0,0.107839,"guage. mostly driven by the number of parameters and 8-bit quantization. OpenNMT’s small lower-quality models have low CPU RAM and Docker image size; UEdin is Pareto-optimal for higher-quality models. OpenNMT was the only team to optimize for these metrics in their system description. In their multicore CPU submission, OpenNMT shared memory amongst processes while other participants simply used multiple processes with copies of the model. 4 4.1 Evaluation Measures We employ standard evaluation metrics for the tasks above along two axes following (Hayashi et al., 2019): Textual Accuracy: BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as measures for surface-level texutal accuracy compared to reference summaries. Document Generation and Translation Task Following the previous workshop, we continued with the shared task of document-level generation and translation. This task is motivated as the central evaluation testbed for document-level generation systems with different types of inputs by providing parallel dataset consisting of structured tables and text in two languages. We host various tracks within the testbed based on input and output constraints and investigate and contrast the system differen"
2020.ngt-1.1,W18-6319,0,0.0283845,"Missing"
2020.ngt-1.26,J82-2005,0,0.677534,"Missing"
2020.ngt-1.26,W18-2716,1,0.935344,"uce head pruning. On GPUs, we used 16-bit floating-point tensor cores. On CPUs, we customized 8-bit quantization and multiple processes with affinity for the multicore setting. To reduce model size, we experimented with 4-bit log quantization but use floats at runtime. In the shared task, most of our submissions were Pareto optimal with respect the trade-off between time and quality. 1 Introduction This paper describes the University of Edinburgh’s submissions to the Workshop on Neural Generation and Translation (WNGT) 2020 Efficiency Shared Task1 using the Marian machine translation toolkit (Junczys-Dowmunt et al., 2018a). The task has GPU, single-core CPU, and multi-core CPU tracks. Our submissions focus on the tradeoff between translation quality and speed; we also address model size after submission. Starting from an ensemble of 4 transformer-big teacher models, we trained a variety of student configurations and on top of that sometimes pruned transformer heads. For the decoding process, we explored the use of lower precision GEMM for both our CPU and GPU submissions. Small models appear to be more sensitive to quantization than large models. Most of our single-CPU submissions had a memory leak, which als"
2020.ngt-1.26,D16-1139,0,0.0445004,"cluded because it contains English sentences longer than 100 words and participants were promised input would be at most 100 words. We refer to the task’s metric as WMT1*. All BLEU scores are reported using sacrebleu.2 The CPU tracks used an Intel Xeon Platinum 8275CL while the GPU track used an NVIDIA T4. For speed, the official input has 1 million lines of text with 15,048,961 space-separated words. 3 Teacher-student training Following Junczys-Dowmunt et al. (2018b) and Kim et al. (2019), all our optimized models are students created using interpolated sequence-level knowledge distillation (Kim and Rush, 2016), and trained on data generated from a teacher system. Teacher We used the sentence-level EnglishGerman system from Microsoft’s constrained submission to the WMT’19 News Translation Task (Junczys-Dowmunt, 2019). It is an ensemble of four deep transformer-big models (Vaswani et al., 2017), each with 12 blocks of layers in encoder and decoder, model size of 1024, filter size of 4096, 2 BLEU+case.mixed+lang.en-de+numrefs.1+s mooth.exp+test.wmt*+tok.13a+version.1.4.8 for various WMT test sets. 1 https://sites.google.com/view/wngt20/ efficiency-task 218 Proceedings of the 4th Workshop on Neural Gen"
2020.ngt-1.26,E17-2068,0,0.047061,"Missing"
2020.ngt-1.26,W18-6415,0,0.063345,"Missing"
2020.ngt-1.26,W19-5321,0,0.0553523,"sacrebleu.2 The CPU tracks used an Intel Xeon Platinum 8275CL while the GPU track used an NVIDIA T4. For speed, the official input has 1 million lines of text with 15,048,961 space-separated words. 3 Teacher-student training Following Junczys-Dowmunt et al. (2018b) and Kim et al. (2019), all our optimized models are students created using interpolated sequence-level knowledge distillation (Kim and Rush, 2016), and trained on data generated from a teacher system. Teacher We used the sentence-level EnglishGerman system from Microsoft’s constrained submission to the WMT’19 News Translation Task (Junczys-Dowmunt, 2019). It is an ensemble of four deep transformer-big models (Vaswani et al., 2017), each with 12 blocks of layers in encoder and decoder, model size of 1024, filter size of 4096, 2 BLEU+case.mixed+lang.en-de+numrefs.1+s mooth.exp+test.wmt*+tok.13a+version.1.4.8 for various WMT test sets. 1 https://sites.google.com/view/wngt20/ efficiency-task 218 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 218–224 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d Model Emb. FFN Enc./Dec. Depth Voc. Params. Size WMT"
2020.ngt-1.26,P02-1040,0,0.107128,"Starting from an ensemble of 4 transformer-big teacher models, we trained a variety of student configurations and on top of that sometimes pruned transformer heads. For the decoding process, we explored the use of lower precision GEMM for both our CPU and GPU submissions. Small models appear to be more sensitive to quantization than large models. Most of our single-CPU submissions had a memory leak, which also impacted speed; we report results before and after fixing the leak. Samsung R&D Institute Poland m.chudyk@samsung.com Shared Task Summary The task measures quality approximated by BLEU (Papineni et al., 2002), speed, model size, Docker image size, and memory consumption of a machine translation system from English to German for the WMT 2019 data condition (Barrault et al., 2019). We did not optimize Docker image size (using stock Ubuntu) or memory consumption (preferring large batches for speed). The task intentionally did not specify a test set until after submissions were made. This was later revealed to be the average of BLEU from WMT test sets from 2010 through 2019, inclusive. However, the 2012 test set was excluded because it contains English sentences longer than 100 words and participants"
2020.ngt-1.26,P19-1580,0,0.0298845,"the teacher’s performance than on test sets from previous years, which consist of both translations and translationese. For example, the teacher achieves 42.4 and 42.2 BLEU on originally English and originally German subsets of the WMT16 test set, respectively, while the Base student model has 42.5 and only 35.6 BLEU. We think the reason for this is that student models were trained solely on teachertranslated data without back-translations. 4 Attention pruning Attention is one of the most expensive operations in the transformer architecture, yet many of the heads can be pruned after training (Voita et al., 2019). Moreover, the lottery ticket hypothesis (Frankle 6 219 Available via --task transformer-base. BLEU WMT19 WMT1* Model Enc. heads Params. Size WPS Tiny 888888 15.7M 61MB 41.5 32.9 2050 Tiny.Steady.i12 Tiny.Steady.i14 201234 001113 14.5M 14.3M 56MB 55MB 41.1 40.8 32.4 32.1 2282 2350 Tiny.Pushy.i6 Tiny.Pushy.i7 222222 111111 14.5M 14.3M 56MB 55MB 41.4 40.2 32.4 31.5 2298 2346 Table 2: Students with pruned encoder attention. Words per second (WPS) is evaluated in float32 with a single CPU core on the official input (Section 2). and Carbin, 2018) and subsequent work on pruning optimisation (Frankl"
2020.ngt-1.4,D19-5608,1,0.723021,"nce. Our result is in line with (Huang et al., 2019), who show that reducing the model size by using fewer layers degrades quality. Logarithmic-based quantisation has been shown to perform better when compared to fixed-point quantisation using both architectures. The RNN model seems to be more robust towards the compression. RNN models exhibit reduced quality degradation in all compression scenarios. We hypothesise that the gradients computed with a highly compressed model are very noisy, thus resulting in noisy parameter updates. Our finding is in line with prior research (Chen et al., 2018; Aji and Heafield, 2019), which state Transformer is more sensitive towards noisy training conditions. Training Routine We prepare our 4-bit quantisation model by retraining from a full precision model. We also store the quantisation errors to be considered for the next update. In this subsection, we answer the question of whether it is necessary to perform these steps. We explore the preparation of the 4-bit model if trained from scratch. Similarly, we explore 4-bit model preparation without an error feedback mechanism. For this experiment, we use optimised scaling and 32-bit bias when applying 4-bit log quantisatio"
2020.ngt-1.4,W17-4710,0,0.0438041,"Missing"
2020.ngt-1.4,P18-1008,0,0.0162302,"the worst performance. Our result is in line with (Huang et al., 2019), who show that reducing the model size by using fewer layers degrades quality. Logarithmic-based quantisation has been shown to perform better when compared to fixed-point quantisation using both architectures. The RNN model seems to be more robust towards the compression. RNN models exhibit reduced quality degradation in all compression scenarios. We hypothesise that the gradients computed with a highly compressed model are very noisy, thus resulting in noisy parameter updates. Our finding is in line with prior research (Chen et al., 2018; Aji and Heafield, 2019), which state Transformer is more sensitive towards noisy training conditions. Training Routine We prepare our 4-bit quantisation model by retraining from a full precision model. We also store the quantisation errors to be considered for the next update. In this subsection, we answer the question of whether it is necessary to perform these steps. We explore the preparation of the 4-bit model if trained from scratch. Similarly, we explore 4-bit model preparation without an error feedback mechanism. For this experiment, we use optimised scaling and 32-bit bias when apply"
2020.ngt-1.4,P02-1040,0,0.106748,"i ∗ signki ∗ 2ji +ki ) (8) Computing power is obtained by using a bit-shift, while computing signji ∗ signki can be performed using bitwise xor, therefore avoiding expensive multiplication instructions (Miyashita et al., 2016). 4 4.1 Experiments Experiment Setup We use systems for the WMT 2017 English-toGerman news translation task for our experiment, which differs from the WNGT shared task setting previously reported. We use back-translated monolingual corpora (Sennrich et al., 2016a) and byte pair encoding (Sennrich et al., 2016b) to preprocess the corpus. Quality is measured based on BLEU (Papineni et al., 2002) score using sacreBLEU script (Post, 2018). We first pre-train baseline models with both Transformer and RNN architectures. Our Transformer model consists of six encoder and six decoder layers with tied embedding. Our deep RNN model consists of eight layers of bidirectional LSTM. Models were trained synchronously with a dynamic batch size of 40 GB per batch using the Marian toolkit (Junczys-Dowmunt et al., 2018). The models are trained until we observe no improvement in 10 consecutive validations. Models are optimised with the Adam optimiser (Kingma and Ba, 2014). The rest of the hyperparamete"
2020.ngt-1.4,W18-6319,0,0.0294875,"d by using a bit-shift, while computing signji ∗ signki can be performed using bitwise xor, therefore avoiding expensive multiplication instructions (Miyashita et al., 2016). 4 4.1 Experiments Experiment Setup We use systems for the WMT 2017 English-toGerman news translation task for our experiment, which differs from the WNGT shared task setting previously reported. We use back-translated monolingual corpora (Sennrich et al., 2016a) and byte pair encoding (Sennrich et al., 2016b) to preprocess the corpus. Quality is measured based on BLEU (Papineni et al., 2002) score using sacreBLEU script (Post, 2018). We first pre-train baseline models with both Transformer and RNN architectures. Our Transformer model consists of six encoder and six decoder layers with tied embedding. Our deep RNN model consists of eight layers of bidirectional LSTM. Models were trained synchronously with a dynamic batch size of 40 GB per batch using the Marian toolkit (Junczys-Dowmunt et al., 2018). The models are trained until we observe no improvement in 10 consecutive validations. Models are optimised with the Adam optimiser (Kingma and Ba, 2014). The rest of the hyperparameter Low-precision Dot Products To improve th"
2020.ngt-1.4,N18-3014,0,0.147682,"Missing"
2020.ngt-1.4,K16-1029,0,0.207806,"Precision Alham Fikri Aji and Kenneth Heafield School of Informatics, University of Edinburgh 10 Crichton Street Edinburgh EH8 9AB Scotland a.fikri@ed.ac.uk, kheafiel@inf.ed.ac.uk Abstract As such, research on model quantisation for NMT tasks remains limited. We find that the model can be compressed at up to 4-bit precision without sacrificing quality. We first explore the use of logarithmic-based quantisation over fixed-point quantisation (Miyashita et al., 2016) based on the empirical findings that parameter distribution is not uniform, but instead concentrated near zero (Lin et al., 2016; See et al., 2016). The magnitude of a parameter also varies across layers; therefore, we propose an improved method of scaling the quantization centres. We also notice that biases do not quantise very well. However, since biases do not consume a noticeable amount of memory, they can be left unquantised. Lastly, we explore the significance of re-training in the model compression scenario. We adopt an error feedback mechanism (Seide et al., 2014) to preserve the quantisation error rather than discarding it at every update during re-training. Quantization is one way to compress Neural Machine Translation (NMT) mo"
2020.ngt-1.4,W17-4739,1,0.849786,"are quantised on the fly, while intermediate activations (such as tanh) are not quantised. We use the same log-based quantisation procedure described in Section 3.1 when training the model. However, we only attempt a fixed predetermined scale. Running the slower EM approach to optimise the scale before every dot product would not be fast enough for inference applications. Training with Quantised Dot Products Our log-quantised activation is a step function, as illustrated in in Figure 2. Therefore, the deriva38 settings on both models follow the suggested configurations (Vaswani et al., 2017; Sennrich et al., 2017). We use wmt2016 as the test set. 4.2 from a pre-trained model and error feedback are necessary to produce a high-quality 4-bit model. Removing either of them degrades the quality. BLEU score is dramatically reduced if we train the model from scratch. Likewise, the quantised model is practically unable to learn without the error feedback mechanism. As shown in Table 1, the quantised model achieved a 34.31 BLEU score without re-training. Re-training said model barely improves the BLEU to 34.45 without the error feedback mechanism. 4-bit Transformer Model In this experiment subsection, we explor"
2020.ngt-1.4,P16-1009,0,0.108131,"Missing"
2020.ngt-1.4,P16-1162,0,0.148384,"Missing"
2020.ngt-1.4,W18-2716,1,0.916358,"lip the value into the given range since we have limited quantisation centres. This then decodes to v 0 ≈ v as v 0 = signS2q . In practice, the sign is stored with q. 3.2 vi0 S = arg min Therefore, given a positive x, we can find the quantised magnitude of q with respect to rounding scheme in normal space by: t = clip(|v|/S, [1, 2 2 q = dlog2 ( t)e 3 X i (a2i S) − X (ai vi ) = 0 i S X i a2i = X (ai vi ) i P (ai vi ) S = Pi 2 i ai P (sign(vi )2qi vi ) S = Pi qi 2 i (sign(vi )2 ) P qi i (2 |vi |) S= P qi i4 Selecting the Scaling Factor There are a few heuristics to choose a scaling factor of S. Junczys-Dowmunt et al. (2018) and Jacob et al. (2018) scale the model based on its maximum value, which can be very unstable–especially during re-training. Alternatively, Lin et al. (2016) and Hubara et al. (2016) use a pre-defined step size for fixed-point quantization. Our objective is to select a scaling factor S such that the quantised parameters are as close to the original as possible. Therefore, we optimise S such that it minimises the squared error between the original and the compressed parameters. We propose a method to fit S by minimising the SME. We start with an initial scale S based on the (7) We optimise S"
2020.wmt-1.17,2020.ngt-1.4,1,0.775989,"our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized back to float32. Despite the extra quantization and de-quantization involved, the increased speed at which 8-bit integer multiplication is performed more than compensates for it. Bogoychev et al. (2020) observe that smaller student presets lose BLEU when quantized. In order to counteract that, we perform model fine tuning following the work of Aji and Heafield (2020): We replace the GEMM routine implementation with a custom one that is damaged, according to the quan7 For example, for handling HTML tags in translated texts. 8 https://github.com/kpu/intgemm 194 4 Results In Table 4, we show the performance of the three models in terms of BLEU scores for the WMT 2020 cs↔en test sets and translation speed. Teacher models ran on an Nvidia GeForce GTX 1080 with a batch size of 16. Student models were run on a single CPU core on an Intel Intel(R) Xeon(R) CPU E5-2680 0 @ 2.70GHz with a batch size of 64. It should be noted that we made no effort to optimize the te"
2020.wmt-1.17,2016.amta-researchers.10,0,0.0683014,"Missing"
2020.wmt-1.17,D17-1300,0,0.0148403,"nd perform several thousand minibatch updates of the model. The damaged GEMM implementation can only produce 255 unique float values (corresponding to the 8-bit integer dequantization range) and the model quickly learns to work with those values and recovers some of the BLEU lost compared to untuned quantized model. Quantized Models Floating point operations are computationally more expensive than integer operations. However, as Han et al. (2016) have shown, neural network inference does not require the high precision of representation and computation that 32-bit floating point numbers offer. Devlin (2017) suggests a simple quantization mechanism for quantizing parameters to 16-bit integer precision and notes that support for off-the-shelf 8-bit integer matrix multiplication is lacking. Bogoychev et al. (2020) fill that gap and provide an 8-bit quantization and fine tuning scheme for Marian based on the intgemm library;8 we used that scheme for our models. The model parameters are quantized offline from float32 to int8, and during translation, the activations are quantized just prior to each GEMM operation. The GEMM operation is performed in 8-bit integers, and then the result is de-quantized b"
2020.wmt-1.17,N13-1073,0,0.196044,"Missing"
2020.wmt-1.17,2020.ngt-1.1,1,0.698048,"th the teacher model to generate the training set D0 . 3. Train a small student model on D0 . Introduction The conventional set-up of the WMT Shared Tasks on News Translation emphasizes translation quality (however measured) above all else. Constraints on the data that may be used for training in the ‘constrained’ track establish a level playing field in terms of the information available to the translation model and its training process, but there are no constraints on the computational power and effort spent to achieve the results. In contrast, the WNGT Shared Task on Efficient Translation (Heafield et al., 2020) encourages participants to submit systems that are both accurate and efficient during inference (i.e., translation). So far, there has been little interaction between the two tasks. With our joint submission between the University of Edinburgh (UEDIN) and Charles University, Prague (CUNI), we strive to bridge this gap. We submitted small, efficient systems that distilled knowledge from a more powerful teacher model via sequence-level knowledge distillation (Kim and 1 Bogoychev et al. (2020) report translation speeds of up to 3135 source words per second on a single CPU thread; the actual thro"
2020.wmt-1.17,E17-2068,0,0.0666085,"Missing"
2020.wmt-1.17,P18-4020,1,0.873993,"Missing"
2020.wmt-1.17,W18-2716,1,0.833283,"sion to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student mode"
2020.wmt-1.17,D16-1139,0,0.0795642,"beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data produced by knowledge distillation (Kim and Rush, 2016), where the target side of the parallel data is the teacher model’s translation of the source side. The basic idea is that the teacher guides the student towards translations that can be achieved with the teacher’s knowledge. 3 To create artificial training data for the students, we used the original parallel section of the CzEng 2.0 dataset but no back-translations. Instead, we translated ca. 40 million sentences from the monoStudent Models The smaller, more efficient student models were trained by UEDIN with the Marian NMT toolkit 3 3.1 Student Model Architectures The student models use the"
2020.wmt-1.17,N03-1017,0,0.0630476,"Missing"
2020.wmt-1.17,D18-2012,0,0.0346137,"hypotheses selected over the respective beam ranks. For the monolingual data, for which we obviously have no human reference translations, we simply chose the highest-scoring translation. Sentence pairs where the translation contained the same whitespace-separated sequence of words three or more times in a row, or the same sequence of one or more characters in five or more subsequent repetitions (which can happen when the recursive decoder goes into a loop) were discarded. We subsequently tokenized the synthetic teaching data (source and translations by the teacher model) with SentencePiece (Kudo and Richardson, 2018), 193 using a joint vocabulary for both languages with a size of 32,000 tokens. This vocabulary is also used by the final systems. The tokenized training data was word-aligned in both translation directions with FastAlign (Dyer et al., 2013). Directional word alignments were then symmetrized with the growdiag-final-and symmetrization algorithm (Koehn et al., 2003). These word alignments serve mainly three purposes: (a) to guide the attention mechanism during training of the student models (Liu et al., 2016) with guided alignment (Chen et al., 2016); (b) to produce shortlists of translation can"
2020.wmt-1.17,C16-1291,0,0.0504752,"Missing"
2020.wmt-1.17,W18-6424,1,0.836123,"t. value size checkpoints avg. back-translation beam search alpha max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary subm"
2020.wmt-1.17,2020.wmt-1.28,1,0.73264,". Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”; Popel, 2020). However, the CUNI submission used a beam size of 4 instead of 8 as used in this work, resulting in a BLEU score on the WMT 2020 en→ test set that is 0.2 lower than the BLEU score reported in Tab. 4. The cs→en teacher model used in this work has only 6 encoder layers as opposed to the 12 encoder layers used in CUNI’s primary submission to the Shared Task, resulting in a BLEU score on the WMT 2020 test set that is 1.0 BLEU points lower than the score achieved by the model used for CUNI’s primary submission. (Junczys-Dowmunt et al., 2018a).4 The students were trained on artificial training data"
2020.wmt-1.17,W18-6319,0,0.0332871,"Missing"
2020.wmt-1.17,P16-1009,0,0.0423625,"max training length teacher cs→en 32K yes 6 6 self-attention yes 1024 4096 16 64 64 8 block-BT 1.0 150 a en→cs student base tiny 32K yes 12 6 self-attention yes 1024 4096 16 64 64 8 block BT 1.0 150 32K 32K yes yes 6 6 2 2 SSRU SSRU yes yes 512 256 2048 1536 8 8 64 64 64 64 exp. smoothinga none none 1.0 1.0 200 200 Exponential smoothing with α = 0.0001. CzEng 2.0 dataset (Kocmi et al., 2020),3 consisting of genuine (authentic) parallel data as well as monolingual news data translated by CUNI’s transformer systems from WMT 2018 (Popel, 2018) to generate back-translated synthetic training data (Sennrich et al., 2016). Rather than shuffling and mixing authentic and synthetic training data, the teacher models were trained on alternating blocks of authentic and synthetic data (“block-regime backtranslation” (block-BT); Popel et al., 2020), spending about 10 hours of training time on each block. The model parameters for the final teacher models were obtained by checkpoint averaging over the last 8 checkpoints of the training process, saved in hourly intervals. The en→cs teacher model used in this work also produced CUNI’s primary submission to the WMT 2020 Shared Task on News Translation (“CUNI-Transformer”;"
2020.wmt-1.17,W18-1819,0,0.0673101,"Missing"
2021.acl-short.15,2020.acl-main.619,0,0.0552378,"Missing"
2021.acl-short.15,2020.acl-main.690,0,0.0943773,"even small reduction in BLEU could result in more instances of biased translations, especially in female context sentences. 6 with the additional modified sentences, the augmented data set equally represents both genders. Vanmassenhove et al. (2018), Stafanoviˇcs et al. (2020) and Saunders et al. (2020) propose a dataannotation scheme in which the NMT model is trained to obey gender-specific tags provided with the source sentence. While Escud´e Font and Costa-juss`a (2019) employ pre-trained wordembeddings which have undergone a “debiasing” process (Bolukbasi et al., 2016; Zhao et al., 2018). Saunders and Byrne (2020) and Costa-juss`a and de Jorge (2020) propose domain-adaptation on a carefully curated data set that “corrects” the model’s misgendering problems. Costa-juss`a et al. (2020) consider variations involving the amount of parameter-sharing between different language directions in multilingual NMT models. Related Work Previous research investigating gender bias in NMT has focused on data bias, ranging from assessment to mitigation. For example, Stanovsky et al. (2019b) adapted an evaluation data set for co-reference resolution to measure gender biases in machine translation. The sentences in this t"
2021.acl-short.15,2020.gebnlp-1.4,0,0.0395269,"Missing"
2021.acl-short.15,P16-1162,0,0.0338986,"th {(6, 4), (6, 2), (6, 1)}. For each of these 7 configurations, we train AAN versions. Next, we save quantized and non-quantized versions for the 14 models, and decode with beam sizes of 1 and 5. We repeat our analysis for English to Spanish and English to German directions, using WMT13 En-Es and WMT14 En-De data sets, respectively. For the En-Es we limited the training data to 4M sentence pairs (picked at random without replacement) to ensure that the training for the two language directions have comparable data sizes. We apply Byte-Pair Encoding (BPE) with 32k merge operations to the data (Sennrich et al., 2016). We measure decoding times and BLEU scores for the model’s translations using the WMT test sets. Next, we evaluate each model’s performance on SimpleGEN, specifically calculating the percent of correctly gendered nouns, incorrectly gendered nouns as well as inconclusive results. Table 3 shows an example of our evaluation protocol for an example source sentences and four possible translations. We deem the first two as correct even though the second translation incorrectly translates “funny” as “feliz” since we focus on the translation of “physician” only. The third translation is deemed incorr"
2021.acl-short.15,2020.wmt-1.73,0,0.0222507,"Missing"
2021.acl-short.15,P19-1164,0,0.135079,"“lady” when translating the occupation-noun, rendering it in with the masculine gender “doctor/m´edico”. and their impact on gender biases in an NMT system, complementing existing work on data bias. We explore optimizations choices such as (i) search (changing the beam size in beam search); (ii) architecture configurations (changing the number of encoder and decoder layers); (iii) model based speedups (using Averaged attention networks (Zhang et al., 2018)); and (iv) 8-bit quantization of a trained model.. Prominent prior work on gender bias evaluation forces the system to “guess” the gender (Stanovsky et al., 2019a) of certain occupation nouns in the source sentence. Consider, the English source sentence “That physician is funny.”, containing no information regarding the physician’s gender. When translating this sentence into Spanish (where the occupation nouns are explicitly specified for gender), an NMT model is forced to guess the gender of the physician and choose between masculine forms, doctor/m´edico or feminine forms doctora/m´edica. While investigating bias in these settings is valuable, in this paper, we hope to highlight that the problem is much worse — despite an explicit gender reference i"
2021.acl-short.15,D18-1334,0,0.0640033,"Missing"
2021.acl-short.15,P19-1176,0,0.0190096,"horses in NLP and MT (Vaswani et al., 2017). While transformers are faster to train compared to their predecessors, Recurrent Neural Network (RNN) encoderdecoders (Bahdanau et al., 2014; Luong et al., 2015), transformers suffer from slower decoding speed. Subsequently, there has been interest in improving the decoding speed of transformers. Shallow Decoders (SD): Shallow decoder models simply reduce the decoder depth and increase the encoder depth in response to the observation that decoding latency is proportional to the number of decoder layers (Kim et al., 2019; Miceli Barone et al., 2017; Wang et al., 2019; Kasai et al., 2020). Alternatively, one can employ SD models without increasing the encoder layers resulting in smaller (and faster) models. Average Attention Networks (AAN): Average Attention Networks reduce the quadratic complexity of the decoder attention mechanism to linear time by replacing the decoder-side self-attention with an average-attention operation using a fixed weight for all time-steps (Zhang et al., 2018). This results in a ≈ 3-4x decoding speedup over the standard transformer. 4 Experimental Setup Our objective is not to compare the various optimization methods against each"
2021.acl-short.15,P18-1166,0,0.0690817,":::: Table 1: Translation of a simple source sentence by 4 different commercial English to Spanish MT systems. All of these systems fail to consider the token “lady” when translating the occupation-noun, rendering it in with the masculine gender “doctor/m´edico”. and their impact on gender biases in an NMT system, complementing existing work on data bias. We explore optimizations choices such as (i) search (changing the beam size in beam search); (ii) architecture configurations (changing the number of encoder and decoder layers); (iii) model based speedups (using Averaged attention networks (Zhang et al., 2018)); and (iv) 8-bit quantization of a trained model.. Prominent prior work on gender bias evaluation forces the system to “guess” the gender (Stanovsky et al., 2019a) of certain occupation nouns in the source sentence. Consider, the English source sentence “That physician is funny.”, containing no information regarding the physician’s gender. When translating this sentence into Spanish (where the occupation nouns are explicitly specified for gender), an NMT model is forced to guess the gender of the physician and choose between masculine forms, doctor/m´edico or feminine forms doctora/m´edica. W"
2021.acl-short.15,D18-1521,0,0.0155237,"eotypical cases and even small reduction in BLEU could result in more instances of biased translations, especially in female context sentences. 6 with the additional modified sentences, the augmented data set equally represents both genders. Vanmassenhove et al. (2018), Stafanoviˇcs et al. (2020) and Saunders et al. (2020) propose a dataannotation scheme in which the NMT model is trained to obey gender-specific tags provided with the source sentence. While Escud´e Font and Costa-juss`a (2019) employ pre-trained wordembeddings which have undergone a “debiasing” process (Bolukbasi et al., 2016; Zhao et al., 2018). Saunders and Byrne (2020) and Costa-juss`a and de Jorge (2020) propose domain-adaptation on a carefully curated data set that “corrects” the model’s misgendering problems. Costa-juss`a et al. (2020) consider variations involving the amount of parameter-sharing between different language directions in multilingual NMT models. Related Work Previous research investigating gender bias in NMT has focused on data bias, ranging from assessment to mitigation. For example, Stanovsky et al. (2019b) adapted an evaluation data set for co-reference resolution to measure gender biases in machine translati"
2021.emnlp-demo.20,W19-5301,0,0.0584533,"Missing"
2021.emnlp-demo.20,2020.ngt-1.1,1,0.737531,", 2015; Vaswani et al., 2017) is pervasive but has a reputa2.1 Translation Engine tion for high computational cost. The combination of the typically high computational cost, however, For the translation engine core, we used the same has pushed its delivery to the cloud, with a number Marian fork as the one used by Bogoychev et al. of cloud providers available (Google, Microsoft, (2020) for participating in the 2020 Workshop Facebook, Amazon, Baidu, etc.). Using a cloud on Neural Generation and Translation’s efficiency based translation provider carries an inherent pri- shared task (WNGT 2020, Heafield et al., 2020). vacy risk, as users lose control of their data once it We introduce binary lexical shortlists and streamenters the web. Potential issues include public dis- lined binary model loading to the codebase, resultclosure due to not understanding terms of service ing in a comparable translation speed, but slightly (Tomter et al., 2017), contractors reading user data faster loading time. We also add sentence splitting (Lerman, 2019), use of user data for advertising, and formatting preservation are handled by a C++ and data breaches. wrapper around Marian.2 To preserve privacy, we made a translation"
2021.emnlp-demo.20,P18-4020,1,0.877431,"Missing"
2021.emnlp-demo.20,D16-1139,0,0.0227397,"understanding terms of service ing in a comparable translation speed, but slightly (Tomter et al., 2017), contractors reading user data faster loading time. We also add sentence splitting (Lerman, 2019), use of user data for advertising, and formatting preservation are handled by a C++ and data breaches. wrapper around Marian.2 To preserve privacy, we made a translation sys2.2 Translation Models tem that runs locally: translateLocally. Once a translation model is downloaded, it does not use an Our models are built with knowledge distillaInternet connection. Running locally is challeng- tion (Kim and Rush, 2016), use lexical shortlists ing due to a number of factors: the model needs to (Schwenk et al., 2007; Le et al., 2012; Devlin et al., be small enough to download on a user hardware; 2014; Bogoychev et al., 2020) to reduce the size of translation latency can’t be hidden by splitting and the output layer, 8-bit integer arithmetic, and the parallelising the translation of a large documents 1 https://github.com/XapaJIaMnu/ across multiple machines; consumer hardware has translateLocally 2 highly variable computing power; availability of https://github.com/browsermt/ GPU computational resources can’t"
2021.emnlp-demo.20,N12-1005,0,0.0242431,"reading user data faster loading time. We also add sentence splitting (Lerman, 2019), use of user data for advertising, and formatting preservation are handled by a C++ and data breaches. wrapper around Marian.2 To preserve privacy, we made a translation sys2.2 Translation Models tem that runs locally: translateLocally. Once a translation model is downloaded, it does not use an Our models are built with knowledge distillaInternet connection. Running locally is challeng- tion (Kim and Rush, 2016), use lexical shortlists ing due to a number of factors: the model needs to (Schwenk et al., 2007; Le et al., 2012; Devlin et al., be small enough to download on a user hardware; 2014; Bogoychev et al., 2020) to reduce the size of translation latency can’t be hidden by splitting and the output layer, 8-bit integer arithmetic, and the parallelising the translation of a large documents 1 https://github.com/XapaJIaMnu/ across multiple machines; consumer hardware has translateLocally 2 highly variable computing power; availability of https://github.com/browsermt/ GPU computational resources can’t be assumed. bergamot-translator 168 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pr"
2021.emnlp-demo.20,2021.eacl-demos.34,0,0.178384,"sed on CPUID. We also provide a model management screen However, other kernels like activation functions are where a user may delete downloaded models, or currently compiled without multiple versions and import custom models, as shown in Figure 5. will be somewhat faster if compiled explicitly for a Our translation engine preserves whitespace be- particular vectorised instruction set. 170 3 Figure 4: Settings selection for the translation engine. Comparison against existing solutions We compare against two existing desktop machine translation solutions: Argos Translate6 and OPUSCAT MT Engine (Nieminen, 2021). They both have slightly different use-cases and support different translation languages. We compare BLEU scores (Papineni et al., 2002) on a WMT19 test set (Barrault et al., 2019) for the English-German language pair, as well as wall-clock and CPU time. We measure only the time necessary for the actual translation. We ignore startup time and issue a translation of an unrelated text before running our test in order to discard any lazy initialisation time. As only translateLocally supports all three platforms, we do pairwise comparison, once on Windows for OPUS-CAT vs translateLocally, and onc"
2021.emnlp-demo.20,P02-1040,0,0.116865,"te downloaded models, or currently compiled without multiple versions and import custom models, as shown in Figure 5. will be somewhat faster if compiled explicitly for a Our translation engine preserves whitespace be- particular vectorised instruction set. 170 3 Figure 4: Settings selection for the translation engine. Comparison against existing solutions We compare against two existing desktop machine translation solutions: Argos Translate6 and OPUSCAT MT Engine (Nieminen, 2021). They both have slightly different use-cases and support different translation languages. We compare BLEU scores (Papineni et al., 2002) on a WMT19 test set (Barrault et al., 2019) for the English-German language pair, as well as wall-clock and CPU time. We measure only the time necessary for the actual translation. We ignore startup time and issue a translation of an unrelated text before running our test in order to discard any lazy initialisation time. As only translateLocally supports all three platforms, we do pairwise comparison, once on Windows for OPUS-CAT vs translateLocally, and once on macOS for Argos Translate vs translateLocally. 3.1 Quality comparison For the quality comparison we used the following models: • For"
2021.emnlp-demo.20,D07-1045,0,0.0924891,"Missing"
buck-etal-2014-n,P02-1040,0,\N,Missing
buck-etal-2014-n,P13-2071,0,\N,Missing
buck-etal-2014-n,D07-1090,0,\N,Missing
buck-etal-2014-n,D07-1091,0,\N,Missing
buck-etal-2014-n,E14-4029,0,\N,Missing
buck-etal-2014-n,P07-2045,0,\N,Missing
buck-etal-2014-n,N04-1022,0,\N,Missing
buck-etal-2014-n,N10-2012,0,\N,Missing
buck-etal-2014-n,P10-1089,0,\N,Missing
buck-etal-2014-n,P07-1019,0,\N,Missing
buck-etal-2014-n,2005.mtsummit-papers.11,0,\N,Missing
buck-etal-2014-n,W13-2201,1,\N,Missing
buck-etal-2014-n,2012.iwslt-papers.17,0,\N,Missing
buck-etal-2014-n,2013.iwslt-evaluation.3,0,\N,Missing
buck-etal-2014-n,D10-1026,0,\N,Missing
buck-etal-2014-n,P13-2121,1,\N,Missing
buck-etal-2014-n,P13-1135,0,\N,Missing
D12-1107,D07-1090,0,0.0471064,"o and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007). Whether to use one smoothing technique or the other then becomes largely an issue of training costs and quality after quantization. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N w"
D12-1107,W11-2103,1,0.847704,"k Y k−1 k−1 k rules) and the order in which rules are tried during p(w1 )p(wk |w1 ) b(wj ) cube pruning. j=f 1173 3.3 Combined Scheme Our two language model modifications can be trivially combined by using lower-order probabilities on the left of a fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998)"
D12-1107,J07-2003,0,0.528898,"ability and backoff. We will show that the probability and backoff values in a language model can be collapsed into a single value for each n-gram without changing sentence probability. This transformation saves memory by halving the number of values stored per entry, but it makes rest cost estimates worse. Specifically, the rest cost pessimistically assumes that the model will back off to unigrams immediately following the sentence fragment. The two modifications can be used independently or simultaneously. To measure the impact of their different rest costs, we experiment with cube pruning (Chiang, 2007) in syntactic machine transla2 Other smoothing techniques, including Witten-Bell (Witten and Bell, 1991), do not make this assumption. 1170 tion. Cube pruning’s goal is to find high-scoring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop l"
D12-1107,P10-4002,0,0.0701019,"Missing"
D12-1107,W06-3113,0,0.0321054,"rnal to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams which all have backoff 1). This makes the storage requirement for higher-quality modified Kneser-Ney smoothing comparable to stupid backoff (Brants et al., 2007)."
D12-1107,D10-1026,0,0.0733215,"h passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per value (except N -grams whic"
D12-1107,2011.iwslt-evaluation.24,1,0.682728,"are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language model 3. Backoff b from the N -gram language model where c(w1n ) is the number of times w1n appears in the training dat"
D12-1107,W11-2123,1,0.86198,"options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and b"
D12-1107,P07-1019,0,0.0445105,"oring sentence fragments for the root non-terminal in the parse tree. It does so by going bottom-up in the parse tree, searching for high-scoring sentence fragments for each non-terminal. Within each non-terminal, it generates a fixed number of high-scoring sentence fragments; this is known as the pop limit. Increasing the pop limit therefore makes search more accurate but costs more time. By moderating the pop limit, improved accuracy can be interpreted as a reduction in CPU time and vice-versa. 2 Related Work Vilar and Ney (2011) study several modifications to cube pruning and cube growing (Huang and Chiang, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and"
D12-1107,P07-2045,1,0.0206737,"ete sentence is held constant. We first show how to improve rest cost quality over standard practice by using additional space. Then, conversely, we show how to compress the language model by making a pessimistic rest cost assumption1 . Language models are designed to assign probability to sentences. However, approximate search algorithms use estimates for sentence fragments. If the language model has order N (an N -gram model), then the first N − 1 words of the fragment have incomplete context and the last N − 1 words have not been completely used as context. Our baseline is common practice (Koehn et al., 2007; Dyer et al., 2010; Li et al., 2009) that uses lower-order entries from the language model for the first words in the fragment and no rest cost adjustment for the last few words. Formally, the baseline estimate for sentence fragment w1k is Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lowerorder entries in an N -gram model to score the"
D12-1107,2005.mtsummit-papers.11,1,0.0635177,"fragment and by charging all backoff penalties on the right of a fragment. The net result is a language model that uses the same memory as the baseline but has better rest cost estimates. 4 Experiments To measure the impact of different rest costs, we use the Moses chart decoder (Koehn et al., 2007) for the WMT 2011 German-English translation task (Callison-Burch et al., 2011). Using the Moses pipeline, we trained two syntactic German-English systems, one with target-side syntax and the other hierarchical with unlabeled grammar rules (Chiang, 2007). Grammar rules were extracted from Europarl (Koehn, 2005) using the Collins parser (Collins, 1999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Fea"
D12-1107,W08-0402,0,0.841283,"tion. 3 Contribution 3.1 Better Rest Costs As alluded to in the introduction, the first few words of a sentence fragment are typically scored using lower-order entries from an N -gram language model. However, Kneser-Ney smoothing (Kneser and Ney, 1995) conditions lower-order probabilities on backing off. Specifically, lower-order counts are adjusted to represent the number of unique extensions an n-gram has: ( |{w0 : c(w0n ) &gt; 0} |if n < N a(w1n ) = c(w1n ) if n = N ways back off to w1n−1 or fewer words4 . This criterion is the same as used to minimize the length of left language model state (Li and Khudanpur, 2008) and can be retrieved for each n-gram without using additional memory in common data structures (Heafield et al., 2011). Where it is unknown if the model will back off, we use a language model of the same order to produce a rest cost. Specifically, there are N language models, one of each order from 1 to N . The models are trained on the same corpus with the same smoothing parameters to the extent that they apply. We then compile these into one data structure where each n-gram record has three values: 1. Probability pn from the n-gram language model 2. Probability pN from the N -gram language"
D12-1107,W09-0424,0,0.0571311,"Missing"
D12-1107,P03-1021,0,0.00735297,"999) for syntax on the English side. The language model interpolates, on the WMT 2010 test set, separate models built on Europarl, news commentary, and the WMT news data for each year. Models were built and interpolated using SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) and the default pruning settings. In all scenarios, the primary language model has order 5. For lower-order rest costs, we also built models with orders 1 through 4 then used the n-gram model to score n-grams in the 5-gram model. Feature weights were trained with MERT (Och, 2003) on the baseline using a pop limit of 1000 and 100-best output. Since final feature values are unchanged, we did not re-run MERT in each condition. Measurements were collected by running the decoder on the 3003-sentence test set. 4.1 Rest Costs as Prediction Scoring the first few words of a sentence fragment is a prediction task. The goal is to predict what the probability will be when more context becomes known. In order to measure performance on this task, we ran the decoder on the hierarchical system with a pop limit of 1000. Every time more context became known, we logged5 the prediction e"
D12-1107,P02-1040,0,0.0836066,"cause lower variance means cube pruning’s relative rankings are more accurate. Our lower-order rest costs are better across the board in terms of absolute bias, mean squared error, and variance. 4.2 Pop Limit Trade-Offs The cube pruning pop limit is a trade-off between search accuracy and CPU time. Here, we measure how our rest costs improve (or degrade) that trade-off. Search accuracy is measured by the average model score of single-best translations. Model scores are scale-invariant and include a large constant factor; higher is better. We also measure overall performance with uncased BLEU (Papineni et al., 2002). CPU time is the sum of user and system time used by Moses divided by the number of sentences (3003). Timing includes time to load, though files were forced into the disk cache in advance. Our test machine has 64 GB of RAM and 32 cores. Results are shown in Figures 3 and 4. Lower-order rest costs perform better in both systems, reaching plateau model scores and BLEU with less CPU time. The gain is much larger for tarPop 2 10 50 500 700 Baseline Model BLEU -105.56 20.45 -104.74 21.13 -104.31 21.36 -104.25 21.33 -104.25 21.34 CPU 3.29 5.21 23.30 54.61 64.08 Lower Order CPU Model BLEU 3.68 -105."
D12-1107,P07-1065,0,0.0615394,"re could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned solely with the values stored by these data structures. Quantization (Whittaker and Raj, 2001; Federico and Bertoldi, 2006) uses less bits to store each numerical value at the expense of model quality, including scores of full sentences, and is compatible with our approach. In fact, the lower-order probabilities might be quantized further than normal since these are used solely for rest cost purposes. Our compression technique reduces storage from two values, probability and backoff, to one value, theoretically halving the bits per"
D12-1107,2008.iwslt-papers.8,0,0.0205576,"ng, 2007). Most relevant is their use of a class-based language model for the first of two decoding passes. This first pass is cheaper because translation alternatives are likely to fall into the same class. Entries are scored with the maximum probability over class members (thereby making them no longer normalized). Thus, paths that score highly in this first pass may contain high-scoring paths under the lexicalized language model, so the second pass more fully explores these options. The rest cost estimates we describe here could be applied in both passes, so our work is largely orthogonal. Zens and Ney (2008) present rest costs for phrasebased translation. These rest costs are based on factors external to the sentence fragment, namely output that the decoder may generate in the future. Our rest costs examine words internal to the sentence fragment, namely the first and last few words. We also differ by focusing on syntactic translation. A wide variety of work has been done on language model compression. While data structure compression (Raj and Whittaker, 2003; Heafield, 2011) and randomized data structures (Talbot and Osborne, 2007; Guthrie and Hepple, 2010) are useful, here we are concerned sole"
D12-1107,J03-4003,0,\N,Missing
D17-1045,W16-2323,0,0.034101,"Missing"
D17-1045,P02-1040,0,0.137016,"Missing"
D18-1327,P17-2021,0,0.213734,"tention mechanism (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequence-to2 2.1 Related Work Seq2seq Neural Parsing Using linearized parse trees within sequential frameworks was first done in the context of neural parsing. Vinyals et al. (2015) parsed using an attentional seq2seq model; they used linearized, unlexicalized parse trees on the target side and sentences on the source side. In addition, as in this work, they used an exter"
D18-1327,D17-1209,0,0.0496211,"Missing"
D18-1327,D16-1025,0,0.0196305,"an task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines. 1 Introduction Neural machine translation (NMT) typically makes use of a recurrent neural network (RNN) -based encoder and decoder, along with an attention mechanism (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. Howev"
D18-1327,D14-1179,0,0.0506089,"Missing"
D18-1327,D16-1257,0,0.0328225,"ject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequence-to2 2.1 Related Work Seq2seq Neural Parsing Using linearized parse trees within sequential frameworks was first done in the context of neural parsing. Vinyals et al. (2015) parsed using an attentional seq2seq model; they used linearized, unlexicalized parse trees on the target side and sentences on the source side. In addition, as in this work, they used an external parser to create synthetic parsed training data, resulting in improved parsing performance. Choe and Charniak (2016) adopted a similar strategy, using linearized parses in an RNN language modeling framework. 2.2 NMT with Source Syntax Among the first proposals for using source syntax in NMT was that of Luong et al. (2016), who introduced a multi-task system in which the source data was parsed and translated using a shared encoder and two decoders. More radical changes to 2961 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2961–2966 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics the standard NMT paradigm have a"
D18-1327,W17-4715,1,0.886732,"Missing"
D18-1327,N16-1101,0,0.0614408,"se2seq baseline, although the mixed RNN decoder attended only to words. As the mixed RNN model outperformed the parallel RNN model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework. 2.4 Multi-Source NMT Multi-source methods in neural machine translation were first introduced by Zoph and Knight (2016) for multilingual translation. They used one encoder per source language, and combined the resulting sentence representations before feeding them into the decoder. Firat et al. (2016) expanded on this by creating a multilingual NMT system with multiple encoders and decoders. Libovick`y and Helcl (2017) applied multi-source NMT to multimodal translation and automatic post-editing and explored different strategies for combining attention over the two sources. In this paper, we apply the multi-source framework to a novel task, syntactic neural machine translation. 3 NMT with Linearized Source Parses We propose a multi-source method for incorporating source syntax into NMT. This method makes use of linearized source parses; we describe these parses in section 3.1. Throughout t"
D18-1327,D17-1012,0,0.0376875,"Missing"
D18-1327,D13-1176,0,0.0598491,"rchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English→German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines. 1 Introduction Neural machine translation (NMT) typically makes use of a recurrent neural network (RNN) -based encoder and decoder, along with an attention mechanism (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are"
D18-1327,P07-2045,0,0.0051117,"This baseline does not use the parsed data. Experimental Setup Data We base our experiments on the WMT17 (Bojar et al., 2017) English (EN) → German (DE) news translation task. All 5.9 million parallel training sentences are used, but no monolingual data. Validation is done on newstest2015, while newstest2016 and newstest2017 are used for testing. We train a shared BPE vocabulary with 60k merge operations on the parallel training data. For the parsed data, we break words into subwords after applying the Stanford parser. We tokenize and truecase the data using the Moses tokenizer and truecaser (Koehn et al., 2007). Parse2seq The second baseline we consider is a slight modification of the mixed RNN model proposed by Li et al. (2017). This uses an identical architecture to the seq2seq baseline (except for a longer maximum sentence length in the encoder). Instead of using sequential data on the source side, the linearized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq. 5 4.2 System seq2seq parse2seq multi-source lex multi-source unlex Results Implementation The models"
D18-1327,P17-1064,0,0.314207,"et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequence-to2 2.1 Related Work Seq2seq Neural Parsing Using linearized parse trees within sequential frameworks was first done in the context of neural parsing. Vinyals et al. (2015) parsed using an attentional seq2seq model; they used linearized, unlexicalized parse trees on the target side and sentences on the source side. In addition, as in this work, they used an external parser to cre"
D18-1327,P17-2031,0,0.114763,"Missing"
D18-1327,Q16-1037,0,0.0352964,"shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines. 1 Introduction Neural machine translation (NMT) typically makes use of a recurrent neural network (RNN) -based encoder and decoder, along with an attention mechanism (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized"
D18-1327,N16-1004,0,0.0205071,"and the other for syntax. However, they combined these representations at the word level, whereas we combine them on the sentence level. Their mixed RNN model is also similar to our parse2seq baseline, although the mixed RNN decoder attended only to words. As the mixed RNN model outperformed the parallel RNN model, we do not attempt to compare our model to parallel RNN. These models are similar to ours in that they incorporate linearized parses into NMT; here, we utilize a multi-source framework. 2.4 Multi-Source NMT Multi-source methods in neural machine translation were first introduced by Zoph and Knight (2016) for multilingual translation. They used one encoder per source language, and combined the resulting sentence representations before feeding them into the decoder. Firat et al. (2016) expanded on this by creating a multilingual NMT system with multiple encoders and decoders. Libovick`y and Helcl (2017) applied multi-source NMT to multimodal translation and automatic post-editing and explored different strategies for combining attention over the two sources. In this paper, we apply the multi-source framework to a novel task, syntactic neural machine translation. 3 NMT with Linearized Source Par"
D18-1327,P14-5010,0,0.00679581,"for combining attention over the two sources. In this paper, we apply the multi-source framework to a novel task, syntactic neural machine translation. 3 NMT with Linearized Source Parses We propose a multi-source method for incorporating source syntax into NMT. This method makes use of linearized source parses; we describe these parses in section 3.1. Throughout this paper, we refer to standard sentences that do not contain any explicit syntactic information as sequential; see Table 1 for an example. 3.1 Linearized Source Parses We use an off-the-shelf parser, in this case Stanford CoreNLP (Manning et al., 2014), to create binary constituency parses. These parses are linearized as shown in Table 1. We tokenize the opening parentheses with the node label (so each node label begins with a parenthesis) but keep the closing parentheses separate from the words they follow. For our task, the parser failed on one training sentence of 5.9 million, which we discarded, and succeeded on all test sentences. It took roughly 16 hours to parse the 5.9 million training sentences. Following Sennrich et al. (2016b), our networks operate at the subword level using byte pair encoding (BPE) with a shared vocabulary on th"
D18-1327,W17-4707,0,0.15407,"et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequence-to2 2.1 Related Work Seq2seq Neural Parsing Using linearized parse trees within sequential frameworks was first done in the context of neural parsing. Vinyals et al. (2015) parsed using an attentional seq2seq model; they used linearized, unlexicalized parse trees on the target side and sentences on the source side. In addition, as in this work, they used an external parser to create synthetic parsed tr"
D18-1327,P02-1040,0,0.10085,"data on the source side, the linearized parses are used. We allow the system to attend equally to words and node labels on the source side, rather than restricting the attention to words. We refer to this baseline as parse2seq. 5 4.2 System seq2seq parse2seq multi-source lex multi-source unlex Results Implementation The models are implemented in Neural Monkey (Helcl and Libovick`y, 2017). They are trained using Adam (Kingma and Ba, 2015) and have minibatch size 40, RNN size 512, and dropout probability 0.2 (Gal and Ghahramani, 2016). We train to convergence on the validation set, using BLEU (Papineni et al., 2002) as the metric. For sequential inputs and outputs, the maximum sentence length is 50 subwords. For parsed inputs, we increase maximum sentence length to 150 subwords to account for the increased length due to the parsing labels; we still use a maximum output length of 50 subwords for these systems. Table 2 shows the performance on EN→DE translation for each of the proposed systems and the baselines, as approximated by BLEU score. The multi-source systems improve strongly over both baselines, with improvements of up to 1.5 BLEU over the seq2seq baseline and up to 1.1 BLEU over the parse2seq bas"
D18-1327,P16-1009,0,0.0307122,"or an example. 3.1 Linearized Source Parses We use an off-the-shelf parser, in this case Stanford CoreNLP (Manning et al., 2014), to create binary constituency parses. These parses are linearized as shown in Table 1. We tokenize the opening parentheses with the node label (so each node label begins with a parenthesis) but keep the closing parentheses separate from the words they follow. For our task, the parser failed on one training sentence of 5.9 million, which we discarded, and succeeded on all test sentences. It took roughly 16 hours to parse the 5.9 million training sentences. Following Sennrich et al. (2016b), our networks operate at the subword level using byte pair encoding (BPE) with a shared vocabulary on the source and target sides. However, the parser operates at the word level. Therefore, we parse then break into subwords, so a leaf may have multiple tokens without internal structure. The proposed method is tested using both lexicalized and unlexicalized parses. In unlexicalized parses, we remove the words, keeping only the node labels and the parentheses. In lexicalized parses, the words are included. Table 1 shows an example of the three source sentence formats: sequential, lexicalized"
D18-1327,P16-1162,0,0.0400569,"or an example. 3.1 Linearized Source Parses We use an off-the-shelf parser, in this case Stanford CoreNLP (Manning et al., 2014), to create binary constituency parses. These parses are linearized as shown in Table 1. We tokenize the opening parentheses with the node label (so each node label begins with a parenthesis) but keep the closing parentheses separate from the words they follow. For our task, the parser failed on one training sentence of 5.9 million, which we discarded, and succeeded on all test sentences. It took roughly 16 hours to parse the 5.9 million training sentences. Following Sennrich et al. (2016b), our networks operate at the subword level using byte pair encoding (BPE) with a shared vocabulary on the source and target sides. However, the parser operates at the word level. Therefore, we parse then break into subwords, so a leaf may have multiple tokens without internal structure. The proposed method is tested using both lexicalized and unlexicalized parses. In unlexicalized parses, we remove the words, keeping only the node labels and the parentheses. In lexicalized parses, the words are included. Table 1 shows an example of the three source sentence formats: sequential, lexicalized"
D18-1327,D16-1159,0,0.0203252,"-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines. 1 Introduction Neural machine translation (NMT) typically makes use of a recurrent neural network (RNN) -based encoder and decoder, along with an attention mechanism (Bahdanau et al., 2015; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). However, it has been shown that RNNs require some supervision to learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). Therefore, explicitly incorporating syntactic information into NMT has the potential to improve performance. This is particularly true for source syntax, which can improve the model’s representation of the source language. Recently, there have been a number of proposals for using linearized representations of parses within standard NMT (Aharoni and Goldberg, 2017; Li et al., 2017; Nadejde et al., 2017). Linearized parses are advantageous because they can inject syntactic information into the models without significant changes to the architecture. However, using linearized parses in a sequenc"
D18-1327,P15-1150,0,0.0393673,"irst proposals for using source syntax in NMT was that of Luong et al. (2016), who introduced a multi-task system in which the source data was parsed and translated using a shared encoder and two decoders. More radical changes to 2961 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2961–2966 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics the standard NMT paradigm have also been proposed. Eriguchi et al. (2016) introduced tree-tosequence NMT; this model took parse trees as input using a tree-LSTM (Tai et al., 2015) encoder. Bastings et al. (2017) used a graph convolutional encoder in order to take labeled dependency parses of the source sentences into account. Hashimoto and Tsuruoka (2017) added a latent graph parser to the encoder, allowing it to learn soft dependency parses while simultaneously learning to translate. 2.3 Linearized Parse Trees in NMT The idea of incorporating linearized parses into seq2seq has been adapted to NMT as a means of injecting syntax. Aharoni and Goldberg (2017) first did this by parsing the target side of the training data and training the system to generate parsed translat"
D18-1332,D17-1045,1,0.807552,"even larger batches by processing multiple mini-batches and summing their gradients locally without sending them to the optimizer. This still increases speed because communication is reduced (Table 1). We introduce parameter τ , which is the number of iterations a GPU performs locally before communicating externally as if it had run one large batch. The Words1 The Tesla P100 has 16 GB of GPU memory and we opt to use 10 GBs of mini-batches and the rest is used to store model parameters, shards, optimizers and additional system specific elements such as the cache vectors for gradient dropping (Aji and Heafield, 2017). Warmup Lowering initial learning rate Goyal et al. (2017) lower the initial learning rate and gradually increase it over a number of minibatches until it reaches a predefined maximum. This technique is also adopted in the work of Vaswani et al. (2017). This is the canonical way to perform warmup for neural network training. 2.2.2 Local optimizers We propose an alternative warm up strategy and compare it with the canonical method. Since we emulate large batches by running multiple smaller batches, it makes sense to consider whether to optimize locally between each batch by adapting the concep"
D18-1332,P18-4020,1,0.871141,"Missing"
D18-1332,P18-2051,0,0.0651113,"Missing"
D18-1332,W16-2323,0,0.0310632,"nt and reducing the frequency of gradient communicaVRAM 3 GB 7 GB 10 GB 20* GB 30* GB 40* GB τ 1 1 1 2 2 4 Words 3080 7310 10448 20897 31345 41794 WPS 19.5k 36.6k 40.2k 44.2k 46.0k 47.6k Table 1: Relationship between the GPU Memory (VRAM) budget for batches (* means emulated by summing τ smaller batches), number of source words processed in each batch and wordsper-second (WPS) measured on a shallow model. 2 Experiments This section introduces each optimization along with an intrinsic experiment on the WMT 2016 Romanian→English task (Bojar et al., 2016). The translation system is equivalent to Sennrich et al. (2016), which was the first place constrained system (and tied for first overall in the WMT16 2991 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2991–2996 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics shared task.). The model is a shallow bidirectional GRU (Bahdanau et al., 2014) encoder-decoder trained on 2.6 million parallel sentences. Due to variable-length sentences, machine translation systems commonly fix a memory budget then pack as many sentences as possible into a dynamicallysized batch. The"
D19-1373,D17-1045,1,0.88859,"Missing"
D19-1373,D19-5608,1,0.490083,"uncompressed locally computed gradient in an effort to better approximate the true global gradient. Formally, let Gt be the compressed global gradient at time t and Lnt be the gradient computed locally on node n. These will be combined into Ctn that will be used to update the parameters. An arguably na¨ıve method sums the two gradients. With a scale-invariant optimizer like Adam, this is equivalent to averaging. Combining With Local Gradients Recent work suggests that the Transformer is sensitive to noisy gradients, resulting in substantially worse models (Chen et al., 2018; Ott et al., 2018; Aji and Heafield, 2019). Consistent with these findings, both gradient sparsification and federated averaging yield low-quality Transformer models in our experiments. In gradient sparsification, noise comes from both thresholding and the errorfeedback mechanism, which causes stale gradients. Federated averaging also introduces stale updates as this approach delays model synchronization. Previous work has shown that both The term Gt − Stn equals to the sum of all sparse gradients from other nodes (or approximates it when the all-reduce compresses the result). The local gradient Lnt used for updating does not include"
D19-1373,D18-1332,1,0.853617,"Missing"
D19-1373,P18-1008,0,0.0300035,"e compressed global gradient and the uncompressed locally computed gradient in an effort to better approximate the true global gradient. Formally, let Gt be the compressed global gradient at time t and Lnt be the gradient computed locally on node n. These will be combined into Ctn that will be used to update the parameters. An arguably na¨ıve method sums the two gradients. With a scale-invariant optimizer like Adam, this is equivalent to averaging. Combining With Local Gradients Recent work suggests that the Transformer is sensitive to noisy gradients, resulting in substantially worse models (Chen et al., 2018; Ott et al., 2018; Aji and Heafield, 2019). Consistent with these findings, both gradient sparsification and federated averaging yield low-quality Transformer models in our experiments. In gradient sparsification, noise comes from both thresholding and the errorfeedback mechanism, which causes stale gradients. Federated averaging also introduces stale updates as this approach delays model synchronization. Previous work has shown that both The term Gt − Stn equals to the sum of all sparse gradients from other nodes (or approximates it when the all-reduce compresses the result). The local gradi"
D19-1373,W18-6301,0,0.0193327,"cation frequency (McMahan et al., 2017). In federated averaging, workers do not exchange gradients. Instead, each worker uses its local gradient to update its own local parameters. Each worker updates their local parameters by averaging across other nodes once every few steps. In contrast with gradient dropping, federated averaging mainly uses the worker’s local gradients for parameter updates. Gradients from other workers are not directly communicated and are therefore not taken into account by the optimizer. 3 noisy and stale gradients damage the model’s quality (McMahan and Streeter, 2014; Ott et al., 2018; Dutta et al., 2018). To address noisy updates in gradient sparsification, we combine the compressed global gradient and the uncompressed locally computed gradient in an effort to better approximate the true global gradient. Formally, let Gt be the compressed global gradient at time t and Lnt be the gradient computed locally on node n. These will be combined into Ctn that will be used to update the parameters. An arguably na¨ıve method sums the two gradients. With a scale-invariant optimizer like Adam, this is equivalent to averaging. Combining With Local Gradients Recent work suggests that t"
D19-1373,P18-4020,1,0.888814,"Missing"
D19-1373,P02-1040,0,0.103077,"Missing"
D19-1373,W17-4739,1,0.85609,"0 Convergence per-update (Deep RNN) validation BLEU We perform our neural machine translation experiments on the following architectures. Transformer: We train a Transformer model with six encoder and six decoder layers with tied embeddings. The model has 62M parameters. We train the model on the WMT 2017 English to German dataset with back-translated monolingual corpora (Sennrich et al., 2016b) and byte-pair encoding (Sennrich et al., 2016c), consisting of 19.1M sentence pairs. Model performance is validated on newstest2016 and tested on newstest2017. Deep RNN: We also train a deep RNN model Sennrich et al. (2017) with eight layers of bidirectional LSTM consisting of 225M parameters. We train the model with the same English to German dataset from the Transformer experiment. Shallow RNN: Our shallow RNN model is based on the winning system by Sennrich et al. (2016a) and is a single layer bidirectional encoderdecoder LSTM with attention consisting of 119M parameters. We train this model on WMT 2016 Romanian to English dataset, consisting of 2.5M sentence pairs. We also apply byte-pair encoding to this dataset. Model performance is validated on newsdev2016 and tested on newstest2016. We apply layer normal"
D19-1373,W16-2323,0,0.0163246,"plied by 4 for the 4-node setting. The single-node learning rates were optimized in the sense that further increasing them damages performance. Warm-up: Learning rate warm-up helps over30 20 10 0 0 2 4 6 num updates x1000 8 10 Convergence per-update (Deep RNN) validation BLEU We perform our neural machine translation experiments on the following architectures. Transformer: We train a Transformer model with six encoder and six decoder layers with tied embeddings. The model has 62M parameters. We train the model on the WMT 2017 English to German dataset with back-translated monolingual corpora (Sennrich et al., 2016b) and byte-pair encoding (Sennrich et al., 2016c), consisting of 19.1M sentence pairs. Model performance is validated on newstest2016 and tested on newstest2017. Deep RNN: We also train a deep RNN model Sennrich et al. (2017) with eight layers of bidirectional LSTM consisting of 225M parameters. We train the model with the same English to German dataset from the Transformer experiment. Shallow RNN: Our shallow RNN model is based on the winning system by Sennrich et al. (2016a) and is a single layer bidirectional encoderdecoder LSTM with attention consisting of 119M parameters. We train this m"
D19-1373,P16-1009,0,0.0166051,"plied by 4 for the 4-node setting. The single-node learning rates were optimized in the sense that further increasing them damages performance. Warm-up: Learning rate warm-up helps over30 20 10 0 0 2 4 6 num updates x1000 8 10 Convergence per-update (Deep RNN) validation BLEU We perform our neural machine translation experiments on the following architectures. Transformer: We train a Transformer model with six encoder and six decoder layers with tied embeddings. The model has 62M parameters. We train the model on the WMT 2017 English to German dataset with back-translated monolingual corpora (Sennrich et al., 2016b) and byte-pair encoding (Sennrich et al., 2016c), consisting of 19.1M sentence pairs. Model performance is validated on newstest2016 and tested on newstest2017. Deep RNN: We also train a deep RNN model Sennrich et al. (2017) with eight layers of bidirectional LSTM consisting of 225M parameters. We train the model with the same English to German dataset from the Transformer experiment. Shallow RNN: Our shallow RNN model is based on the winning system by Sennrich et al. (2016a) and is a single layer bidirectional encoderdecoder LSTM with attention consisting of 119M parameters. We train this m"
D19-1373,P16-1162,0,0.03337,"plied by 4 for the 4-node setting. The single-node learning rates were optimized in the sense that further increasing them damages performance. Warm-up: Learning rate warm-up helps over30 20 10 0 0 2 4 6 num updates x1000 8 10 Convergence per-update (Deep RNN) validation BLEU We perform our neural machine translation experiments on the following architectures. Transformer: We train a Transformer model with six encoder and six decoder layers with tied embeddings. The model has 62M parameters. We train the model on the WMT 2017 English to German dataset with back-translated monolingual corpora (Sennrich et al., 2016b) and byte-pair encoding (Sennrich et al., 2016c), consisting of 19.1M sentence pairs. Model performance is validated on newstest2016 and tested on newstest2017. Deep RNN: We also train a deep RNN model Sennrich et al. (2017) with eight layers of bidirectional LSTM consisting of 225M parameters. We train the model with the same English to German dataset from the Transformer experiment. Shallow RNN: Our shallow RNN model is based on the winning system by Sennrich et al. (2016a) and is a single layer bidirectional encoderdecoder LSTM with attention consisting of 119M parameters. We train this m"
D19-5608,D17-1045,1,0.60109,"xperiments to different hyper-parameter configurations. One direction is to investigate wether vanilla asynchronous Trasnformer can be trained under different optimizers. Another direction is to experiment with more workers where gradients in asynchronous SGD are more stale. In the opposite direction, some work has added noise to gradients or increased staleness, typically to cut computational costs. Recht et al. (2011) propose a lock-free asynchronous gradient update. Lossy gradient compression by bit quantization (Seide et al., 2014; Alistarh et al., 2017) or threshold based sparsification (Aji and Heafield, 2017; Lin et al., 2017) also introduce noisy gradient updates. On top of that, these techniques store unsent gradients to be added into the next gradient, increasing staleness for small gradients. Dean et al. (2012) mention that communication overload can be reduced by reducing gradient pushes and parameter synchronization frequency. In McMahan et al. (2017), each processor independently updates its own local model and periodically synchronize the parameter by averaging across other processors. Ott et al. (2018) accumulates gradients locally, before sending it to the parameter server. Bogoychev et"
D19-5608,W17-4710,0,0.0539758,"heme that Transformers are more sensitive to noisy gradients. actually cause Adam to move faster because they have smaller coefficient of variation. An example appears in Table 2: updating with 1 moves faster than individually applying -1 and 2. 4 Ablation Study We conduct ablation experiments to investigate the poor performance in asynchronous Transformer training for the neural machine translation task. 4.1 Experiment Setup Our experiments use systems for the WMT 2017 English to German news translation task. The Transformer is standard with six encoder and six decoder layers. The RNN model (Barone et al., 2017) is based on the winning WMT17 submission (Sennrich et al., 2017) with 8 layers. Both models use back-translated monolingual corpora (Sennrich et al., 2016a) and byte-pair encoding (Sennrich et al., 2016b). We follow the rest of the hyperparameter settings on both Transformer and RNN models as suggested in the papers (Vaswani et al., 2017; Sennrich et al., 2017). Both models were trained on four GPUs with a dynamic batch size of 10 GB per GPU using the Marian toolkit (JunczysDowmunt et al., 2018). Both models are trained for 8 epochs or until reaching five continuous validations without loss i"
D19-5608,D18-1332,1,0.90816,"apart the impact of batch size and stale gradients, we perform a series of experiments on both recurrent neural networks (RNNs) and Transformers manipulating batch size and injecting staleness. Out experiments show that small batch sizes slightly degrade quality while stale gradients substantially degrade quality. To restore convergence, we propose a hybrid method that computes gradients asynchronously, sums gradients as they arise, and updates less often. Gradient summing has been applied to increase batch size or reduce communication (Dean et al., 2012; Lian et al., 2015; Ott et al., 2018; Bogoychev et al., 2018); we find it also reduces harmful staleness. In a sense, updating less often increases staleness because gradients are computed with respect to parameters that could have been updated. However, if staleness is measured by the number of intervening updates to the model, then staleness is reduced because updates happen less often. Empirically, our hybrid method converges comparably to synchronous SGD, preserves final model quality, and runs faster because processors are not idle. Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so synchronous SGD has become"
D19-5608,W18-6301,0,0.170368,"computed. To tease apart the impact of batch size and stale gradients, we perform a series of experiments on both recurrent neural networks (RNNs) and Transformers manipulating batch size and injecting staleness. Out experiments show that small batch sizes slightly degrade quality while stale gradients substantially degrade quality. To restore convergence, we propose a hybrid method that computes gradients asynchronously, sums gradients as they arise, and updates less often. Gradient summing has been applied to increase batch size or reduce communication (Dean et al., 2012; Lian et al., 2015; Ott et al., 2018; Bogoychev et al., 2018); we find it also reduces harmful staleness. In a sense, updating less often increases staleness because gradients are computed with respect to parameters that could have been updated. However, if staleness is measured by the number of intervening updates to the model, then staleness is reduced because updates happen less often. Empirically, our hybrid method converges comparably to synchronous SGD, preserves final model quality, and runs faster because processors are not idle. Asynchronous stochastic gradient descent (SGD) converges poorly for Transformer models, so s"
D19-5608,P18-1008,0,0.146469,"Missing"
D19-5608,W18-6319,0,0.0338696,"with 8 layers. Both models use back-translated monolingual corpora (Sennrich et al., 2016a) and byte-pair encoding (Sennrich et al., 2016b). We follow the rest of the hyperparameter settings on both Transformer and RNN models as suggested in the papers (Vaswani et al., 2017; Sennrich et al., 2017). Both models were trained on four GPUs with a dynamic batch size of 10 GB per GPU using the Marian toolkit (JunczysDowmunt et al., 2018). Both models are trained for 8 epochs or until reaching five continuous validations without loss improvement. Quality is measured on newstest2016 using sacreBLEU (Post, 2018), preserving newstest2017 as test for later experiments. The Transformer’s learning rate is linearly warmed up for 16k updates. We apply an inverse square root learning rate decay following Vaswani et al. (2017) for both models. All of these experiments use the Adam optimizer, which has shown to perform well on a variety of tasks (Kingma and Ba, 2014) and was used in the original Transformer paper (Vaswani et al., 2017). For subsequent experiments, we will use a learning rate of 0.0003 for Transformers and 0.0006 for RNNs. These were near the top in both asynchronous and synchronous settings ("
D19-5608,W17-4739,1,0.888438,"ctually cause Adam to move faster because they have smaller coefficient of variation. An example appears in Table 2: updating with 1 moves faster than individually applying -1 and 2. 4 Ablation Study We conduct ablation experiments to investigate the poor performance in asynchronous Transformer training for the neural machine translation task. 4.1 Experiment Setup Our experiments use systems for the WMT 2017 English to German news translation task. The Transformer is standard with six encoder and six decoder layers. The RNN model (Barone et al., 2017) is based on the winning WMT17 submission (Sennrich et al., 2017) with 8 layers. Both models use back-translated monolingual corpora (Sennrich et al., 2016a) and byte-pair encoding (Sennrich et al., 2016b). We follow the rest of the hyperparameter settings on both Transformer and RNN models as suggested in the papers (Vaswani et al., 2017; Sennrich et al., 2017). Both models were trained on four GPUs with a dynamic batch size of 10 GB per GPU using the Marian toolkit (JunczysDowmunt et al., 2018). Both models are trained for 8 epochs or until reaching five continuous validations without loss improvement. Quality is measured on newstest2016 using sacreBLEU ("
D19-5608,P18-4020,1,0.906845,"Missing"
D19-5608,P16-1009,0,0.0632356,"ample appears in Table 2: updating with 1 moves faster than individually applying -1 and 2. 4 Ablation Study We conduct ablation experiments to investigate the poor performance in asynchronous Transformer training for the neural machine translation task. 4.1 Experiment Setup Our experiments use systems for the WMT 2017 English to German news translation task. The Transformer is standard with six encoder and six decoder layers. The RNN model (Barone et al., 2017) is based on the winning WMT17 submission (Sennrich et al., 2017) with 8 layers. Both models use back-translated monolingual corpora (Sennrich et al., 2016a) and byte-pair encoding (Sennrich et al., 2016b). We follow the rest of the hyperparameter settings on both Transformer and RNN models as suggested in the papers (Vaswani et al., 2017; Sennrich et al., 2017). Both models were trained on four GPUs with a dynamic batch size of 10 GB per GPU using the Marian toolkit (JunczysDowmunt et al., 2018). Both models are trained for 8 epochs or until reaching five continuous validations without loss improvement. Quality is measured on newstest2016 using sacreBLEU (Post, 2018), preserving newstest2017 as test for later experiments. The Transformer’s lear"
D19-5608,P16-1162,0,0.0426012,"ample appears in Table 2: updating with 1 moves faster than individually applying -1 and 2. 4 Ablation Study We conduct ablation experiments to investigate the poor performance in asynchronous Transformer training for the neural machine translation task. 4.1 Experiment Setup Our experiments use systems for the WMT 2017 English to German news translation task. The Transformer is standard with six encoder and six decoder layers. The RNN model (Barone et al., 2017) is based on the winning WMT17 submission (Sennrich et al., 2017) with 8 layers. Both models use back-translated monolingual corpora (Sennrich et al., 2016a) and byte-pair encoding (Sennrich et al., 2016b). We follow the rest of the hyperparameter settings on both Transformer and RNN models as suggested in the papers (Vaswani et al., 2017; Sennrich et al., 2017). Both models were trained on four GPUs with a dynamic batch size of 10 GB per GPU using the Marian toolkit (JunczysDowmunt et al., 2018). Both models are trained for 8 epochs or until reaching five continuous validations without loss improvement. Quality is measured on newstest2016 using sacreBLEU (Post, 2018), preserving newstest2017 as test for later experiments. The Transformer’s lear"
D19-5608,W18-6401,0,\N,Missing
D19-5610,D18-1549,0,0.0472328,"Missing"
D19-5610,W17-4715,1,0.910255,"st held-out 3.4 2.7 21.3 19.3 19.4 17.0 22.6 20.7 3.7 3.1 20.9 18.9 3.7 2.9 22.3 20.4 23.0 20.6 23.0 20.3 23.6 21.1 Table 3: BLEU scores for the initial multilingual models and zero-resource models without monolingual data, for the baselines with pivot monolingual data, and for our proposed zero-resource models with pivot monolingual data. We report results on the test set (newstest2015) and the held-out set (newstest2016). For the baselines and the initial multilingual models, we use consider both direct (zero-shot) and pivot translation. 5.1 The first is based on the copied corpus method of Currey et al. (2017). We train an identical model to the initial multilingual model, but with additional EN→EN pseudo-parallel training data from the EN monolingual corpus. Thus, this model is trained on DE↔EN, RU↔EN, and EN→EN data. We do not fine-tune this model with any pseudo-parallel data. For the multilingual baseline, direct source→target translation does very poorly for DE→RU. Although the performance is somewhat more reasonable for RU→DE, direct translation still lags far behind pivot (source→EN→target) translation for this model. Our results differ from those of Johnson et al. (2017), who showed reasona"
D19-5610,D18-1045,0,0.0136594,"anguage into each zero-shot language. Similarly, Lakew et al. (2017) improved low-resource zero-shot NMT by back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivo"
D19-5610,D16-1026,0,0.0506992,"Missing"
D19-5610,D18-1039,0,0.20189,"able to translate between the zeroshot language pairs. On the other hand, multilingual NMT with shared encoders and decoders (Ha et al., 2016; Johnson et al., 2017) is more successful at zero-shot NMT, although its performance still lags behind pivoting. Several modifications to the multilingual NMT architecture have been proposed with the goal of improving zero-shot NMT performance; here, we review some such modifications. Lu et al. (2018) added an interlingua layer to the multilingual NMT model; this layer transforms language-specific encoder outputs into languageindependent decoder inputs. Platanios et al. (2018) updated the shared encoder/decoder multilingual NMT model by adding a contextual parameter generator. This generator generates the encoder and decoder parameters for a given source 3 Zero-Resource NMT with Pivot Monolingual Data In this paper, we concentrate on zero-resource NMT between two languages X and Y given a pivot language Z. We assume access to X↔Z and Y↔Z parallel corpora, but no direct X↔Y parallel corpus. Our goal is to use additional monolingual data in the pivot language Z to improve both 100 target pivot source pivot NMT training target pivot source pivot (a) An initial multili"
D19-5610,E17-3017,0,0.0333555,"pus described in section 3.3 (concatenated with the origial data). Like the other zero-resource baseline, this baseline is only evaluated on direct translation (not on pivot translation). 4.2 Baselines with Monolingual Data Models In addition to the initial models, we compare our proposed zero-resource NMT methods to two baselines trained with monolingual EN data. For both of these baselines, we evaluate both direct zero-shot translation and pivot translation through EN. All models in our experiments are based on the transformer architecture (Vaswani et al., 2017). We use the Sockeye toolkit (Hieber et al., 2017) to run all experiments. We find that the default Sockeye hyperparameters work well, so we stick with those throughout. We use beam search with 103 initial models baselines proposed models BLEU multilingual direct multilingual pivot Lakew et al., 2017 Firat et al., 2016 copied corpus direct copied corpus pivot back-translation direct back-translation pivot pivot from scratch pivot fine-tune pivot-parallel combined RU→DE test held-out 15.2 14.5 21.7 20.2 14.4 13.2 21.0 18.3 10.2 9.5 21.1 19.9 14.8 14.1 22.4 20.9 22.3 21.5 22.4 21.5 22.5 21.6 DE→RU test held-out 3.4 2.7 21.3 19.3 19.4 17.0 22.6"
D19-5610,P16-1009,0,0.345152,"resource zero-shot NMT by back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivotlanguage data to build a zero-resource NMT system. Although we use a basic multilin"
D19-5610,W18-2703,0,0.341533,"rly, Lakew et al. (2017) improved low-resource zero-shot NMT by back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivotlanguage data to build a zero-resource NMT"
D19-5610,P16-1162,0,0.724937,"resource zero-shot NMT by back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivotlanguage data to build a zero-resource NMT system. Although we use a basic multilin"
D19-5610,D16-1160,0,0.0798379,"y back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivotlanguage data to build a zero-resource NMT system. Although we use a basic multilingual NMT system as the b"
D19-5610,W18-6309,0,0.20809,"at et al. (2016) first attempted zero-shot NMT with a multilingual model consisting of several encoders and decoders, but found that without fine-tuning, the model was not able to translate between the zeroshot language pairs. On the other hand, multilingual NMT with shared encoders and decoders (Ha et al., 2016; Johnson et al., 2017) is more successful at zero-shot NMT, although its performance still lags behind pivoting. Several modifications to the multilingual NMT architecture have been proposed with the goal of improving zero-shot NMT performance; here, we review some such modifications. Lu et al. (2018) added an interlingua layer to the multilingual NMT model; this layer transforms language-specific encoder outputs into languageindependent decoder inputs. Platanios et al. (2018) updated the shared encoder/decoder multilingual NMT model by adding a contextual parameter generator. This generator generates the encoder and decoder parameters for a given source 3 Zero-Resource NMT with Pivot Monolingual Data In this paper, we concentrate on zero-resource NMT between two languages X and Y given a pivot language Z. We assume access to X↔Z and Y↔Z parallel corpora, but no direct X↔Y parallel corpus."
D19-5610,W18-2710,0,0.204629,"017) improved low-resource zero-shot NMT by back-translating directly between the two zero-shot languages and fine-tuning on the resulting corpus. Park et al. (2017) combined both of these methods and also included NMT-generated sentences on the target side of the pseudo-parallel corpora. pivot language. Although there have been several explorations into using parallel corpora through a pivot language to improve NMT (Firat et al., 2016; Lakew et al., 2017; Park et al., 2017) and using monolingual source and target corpora in NMT (Edunov et al., 2018; Gulcehre et al., 2015; Hoang et al., 2018; Niu et al., 2018; Sennrich et al., 2016a; Zhang and Zong, 2016), this is to our knowledge the first attempt at using monolingual pivot-language data to augment NMT training. Leveraging monolingual pivot-language data is worthwhile because the pivot language is often the highest-resource language of the three (e.g. it is often English), so we expect there to be more high-quality monolingual pivot data than monolingual source or target data in many cases. Thus, we make use of parallel source↔pivot data, parallel target↔pivot data, and monolingual pivotlanguage data to build a zero-resource NMT system. Although"
D19-5632,W18-2716,1,0.798396,"more than 4x faster than last year’s fastest submission at more than 3 points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is slightly faster than last year’s fastest RNN-based submissions, but outperforms them by more than 4 BLEU and 10 BLEU points respectively. 1 Introduction This paper describes the submissions of the “Marian” team to the Workshop on Neural Generation and Translation (WNGT 2019) efficiency shared task (Hayashi et al., 2019). The goal of the task is to build NMT systems on CPUs and GPUs placed on the Pareto Frontier of efficiency and accuracy. Marian (Junczys-Dowmunt et al., 2018a) is an efficient neural machine translation (NMT) toolkit written in pure C++ based on dynamic computational graphs.1 Marian is a research tool which can ∗ 1 First authors with equal contribution. https://github.com/marian-nmt/marian be used to define state-of-the-art systems that at the same time can produce truly deployment-ready models across different devices. This is accomplished within a single execution engine that does not require specialized, inference-only decoders. Our submissions to last year’s edition of the same shared task defined the Pareto frontiers for translation quality v"
D19-5632,D16-1139,0,0.0240319,"ss stated differently, our student is a single model that follows the Transformer-base configuration (model size 512, filter size 2048, 6 blocks) with modifications. See Section 3 for details. For all models, we use the same vocabulary of 32,000 subwords, computed with SentencePiece (Kudo and Richardson, 2018). The training data is provided by the shared task organizers and restricted to about 4 Million sentences from the WMT news translation task for English-German. Use of other data is not permitted. We again implement the interpolated sequencelevel knowledge distillation method proposed by Kim and Rush (2016): The teacher ensemble is used to forward-translate the training data and collect 8-best lists for each sentence. Choosing the best translation for each sentence based on sentence-level BLEU compared to the original target, we create a synthetic target data. The student is trained on the original source and this synthetic forward translated target. Table 1 contains BLEU scores of the teacher ensemble (T) and a student model distilled from this teacher (Student ← T). The gap is 2.4 BLEU. 2.1 Knowledge distillation with noisy backward-forward translation In our experience, student training benef"
D19-5632,D18-2012,0,0.0346385,"ain four forward (ende) and four inverse (de-en) teacher models according to the Transformer-big configuration (model size 1024, filter size 4096, 6 blocks, file size 813 MiB) from Vaswani et al. (2017). We think of a teacher as the set of all models that have been used to create the artificial training data. Unless stated differently, our student is a single model that follows the Transformer-base configuration (model size 512, filter size 2048, 6 blocks) with modifications. See Section 3 for details. For all models, we use the same vocabulary of 32,000 subwords, computed with SentencePiece (Kudo and Richardson, 2018). The training data is provided by the shared task organizers and restricted to about 4 Million sentences from the WMT news translation task for English-German. Use of other data is not permitted. We again implement the interpolated sequencelevel knowledge distillation method proposed by Kim and Rush (2016): The teacher ensemble is used to forward-translate the training data and collect 8-best lists for each sentence. Choosing the best translation for each sentence based on sentence-level BLEU compared to the original target, we create a synthetic target data. The student is trained on the ori"
D19-5632,D17-1300,0,0.0189122,"been computed with a setup from our WNMT2018 submission (Junczys-Dowmunt et al., 2018b). On batchlevel, a shortlist selects the 75 most common target words and up to 75 most probable translations per input-batch word. This set of words is used to create an output vocabulary matrix over a couple of hundred words instead of 32,000 which reduces the computational load with no loss in quality (compare with GPU-bound BLEU scores in Figure 1b). For systems left of the dotted black line, matrix multiplication is executed with mixed 32-bit (Intel’s MKL library) and 16-bit (own implementation based on Devlin (2017)) kernels. All systems right of the dotted line, are the same model as “SSRUTied” without re-training, but executed with different runtime optimizations. In this section we discuss new runtime optimizations which will be available in Marian v1.9. 4.1 8-bit matrix multiplication with packing The AWS m5.large target platform for CPU-bound decoding is equipped with an Intel Xeon Platinum 8175 CPU. This CPU supports 8-bit integer instructions with AVX-512 (Advanced Vector eXtensions512) which can be used to accelerate deep neural network models (Wu et al., 2016; Rodriguez et al., 2018; Bhandare et"
D19-5632,D18-1045,0,0.0296904,"EU compared to the original target, we create a synthetic target data. The student is trained on the original source and this synthetic forward translated target. Table 1 contains BLEU scores of the teacher ensemble (T) and a student model distilled from this teacher (Student ← T). The gap is 2.4 BLEU. 2.1 Knowledge distillation with noisy backward-forward translation In our experience, student training benefits from forward-translated data that was not seen during teacher training. Since we do not have access to additional monolingual source data, we generate noisy back-translated sentences (Edunov et al., 2018), one set per inverse teacher model. Noisy sentences are generated by sampling from the output softmax distribution via added Gumbel noise. We then use the forward (en-de) teacher ensemble to translate the sampled English sentences into German and choose the best output from the 8-best list measured against the original target. This increases the training corpus 5-fold. Training on this new data reduces the gap to the teacher to 1.3 BLEU; a single teacher model is only 0.4 BLEU better. BLEU Teacher (T) Single teacher model 28.9 28.0 Student without teacher Student ← T Student ← T with 4×NBFT 2"
D19-5632,P18-4020,1,0.842655,"Missing"
D19-5632,D18-1477,0,0.0270981,"der parameters in L2-cache during translation. Moving from SSRU to SSRU-Tied in Figure 1a, we see a 1.39x speed up and a small drop of 0.2 BLEU. The GPU is largely unaffected since cachelocality is less of an issue here. 2 AANs parallelize better during training since the average can be computed non-recurrently, however for the small student models the increase in training time is negligible. 3 We are describing the SRU based on V1 of the preprint on Arxiv from September 2017. Subsequent updates and publications seem to have changed the implementation, resulting in more complex variants, e.g. Lei et al. (2018). We implemented the SRU at time of publication of V1 and missed the updates, but our variant seems to work just fine. 283 4 The transformer uses ReLU non-linearities everywhere. It seems however that student-sized models trained from scratch behave worse when using either SRU or SSRU compared to all the alternatives. There is however no difference between the SRU and SSRU which seems to confirm that the reset-gate rt can be dropped when the additive skipconnection is already present. 6 It does when projections of the encoder into decoder space for purpose of applying cross-attention can be ca"
D19-5632,W18-2715,0,0.0173407,"marized in Table 3. We report model configurations, architectures, dimensions, depth, number of parameters, file sizes in MiB for CPU and GPU, translation speed in words per second and BLEU for newstest2014, omitting newstest2015. Time has been measured by the shared-task organizers on AWS m5.large (CPU) and p3.x2large (GPU) instances. Until this moment, we kept model dimensions and decoder depth constant while optimizing a configuration that corresponds to the Microsoft inproduction models (bold row in Table 3). For the final shared task submissions, we vary model dimensions and — similar to Senellart et al. (2018) — decoder depth in order to explore the trade-offs between quality and speed on the Pareto frontier. We train a shallower base-configuration “(1) Base” with two tied decoder layers and small loss in BLEU compared to the 6-layer version. The speedup is significant on both device types. To cover higher-quality models, we add a “(2) Large” configuration with improved BLEU but slower translation speed. As in the previous year, we do not submit models below 26 BLEU, but due to the improved teacher-student training, we can cut down model size drastically before that threshold is reached. We are see"
D19-5632,P18-1166,0,0.0611285,"discuss the influence of self-regression mechanisms in Section 3.1 and parameter tying in Section 3.2. Architecture-indepedent but devicespecific optimizations for the CPU are detailed in Section 4 and for the GPU in Section 5. More general optimizations are outlined in Section 4.2. Performance has been measured with Marian v1.7, measurements are self-reported by Marian. 3.1 SSRU instead of self-attention or AAN In previous work (Junczys-Dowmunt et al., 2018b) and later experiments, we found that replacing the self-attention mechanims in Transformer decoders with an Average Attention Network (Zhang et al., 2018) or modern RNN variants does not affect student quality while resulting in faster decoding on GPU and CPU. This is mainly caused by reducing the decoder complexity from O(n2 ) to O(n) over the number of output tokens n. In Figure 1 we see how switching from a vanilla Transformer-base variant to a student with AAN improves speed on both devices, but more so on the GPU. While we had good results for AANs in JunczysDowmunt et al. (2018b), we feel somewhat uneasy about the flat element-wise average used to accumulate over inputs. RNNs share the linear computational complexity of the AAN over input"
N13-1116,D07-1090,0,0.0402154,"by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces"
N13-1116,W11-2103,1,0.819339,"or the beam is full. After the loop terminates, the beam is given to the root node of the state tree; other nodes will be built lazily as described in §3.2. Overall, the algorithm visits hypergraph vertices in bottom-up order. Our beam filling algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (O"
N13-1116,W12-3102,1,0.129996,"Missing"
N13-1116,D12-1103,0,0.0589301,"e same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes are leaves, then the p"
N13-1116,P05-1033,0,0.0725644,"vity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a time-accuracy trade-off: larger k increases both CPU time and accuracy. We contribute a new beam filling algorithm that improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model"
N13-1116,J07-2003,0,0.343299,"this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previous paragraph are special cases of a directed acyclic hypergraph. As used here, the difference from a normal graph is that an edge can go from one vertex to any number of vertices; this number is the arity of th"
N13-1116,P10-4002,0,0.169629,"unt for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer c"
N13-1116,2010.iwslt-papers.8,0,0.14625,"entry then pushing multiple entries. However, our queue entries are a group of hypotheses while cube pruning’s entries are a single hypothesis. Hypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation system"
N13-1116,2011.iwslt-evaluation.24,1,0.848843,"’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the few a  ` Korea) and therefore includes estimate log r(the)r(few |the). Estimates are updated as words are revealed. Continuing the example, “is (  )[0+ ] .” has best child “is (the  Korea)[0+ ] .” In this best child, the estimate r(.) is updated to r(. |Korea). Similarly, r(the) is replaced with r(the |is). Updates examine only words that have been revealed: r(few |the) remains unrevised. Updates are computed efficiently by using pointers (Heafield et al., 2011) with KenLM. To summarize, the language model computes r(wn |w1n−1 ) r(wn |win−1 ) in a single call. In the popular reverse trie data structure, the language model visits win while retrieving w1n , so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language mo"
N13-1116,D12-1107,1,0.862517,"score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its descendants no longer contribute to the maximum. The score of partial edge “is (  )[0+ ] .” is the sum of scores from its two parts: edge “is v .” and bread crumb (  )[0+ ]. The edge’s score includes estimated log probability log r(is)r(.) as explained earlier. The bread crumb’s score comes from its highest-scoring descendent (the"
N13-1116,2009.iwslt-papers.4,1,0.374035,"e is updated to account for new language model context. Each edge score includes a log language model probability and possibly additive features. Whenever there is insufficient context to compute the language model probability of a word, an estimate r is used. For example, edge “is v .” incorporates estimate log r(is)r(.) into its score. The same applies to hypotheses: (the few a  ` Korea) includes estimate log r(the)r(few |the) d(n[i]) i=c d(n[c+ ]) = d(n[c]) Partial Edge d(n[c + 1+ ]) 962 because the words in left state are those with insufficient context. In common practice (Chiang, 2007; Hoang et al., 2009; Dyer et al., 2010), the estimate is taken from the language model: r = p. However, querying the language model with incomplete context leads Kneser-Ney smoothing (Kneser and Ney, 1995) to assume that backoff has occurred. An alternative is to use average-case rest costs explicitly stored in the language model (Heafield et al., 2012). Both options are used in the experiments3 . The score of a bread crumb is the maximum score of its descendants as defined in §3.3. For example, the bread crumb (  )[1+ ] has a lower score than (  )[0+ ] because the best child (the  Korea)[0+ ] and its des"
N13-1116,D09-1007,0,0.0138539,"ypotheses are usually fully scored before being placed in the priority queue. An alternative prioritizes hypotheses by their additive score. The additive score is the edge’s score plus the score of each component hypothesis, ignoring the non-additive aspect of the language model. When the additive score is used, the language model is only called k times, once for each hypothesis popped from the queue. Cube pruning can produce duplicate queue entries. Gesmundo and Henderson (2010) modified the algorithm prevent duplicates instead of using a hash table. We include their work in the experiments. Hopkins and Langmead (2009) characterized cube pruning as A* search (Hart et al., 1968) with an inadmissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then"
N13-1116,P07-1019,0,0.154769,"algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous vertices), selects the top k by score, and discards the remaining hypotheses. This is expensive: just"
N13-1116,D10-1027,0,0.0793645,"racy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the beams in previous verti"
N13-1116,P12-1064,0,0.0293356,", so the cost is the same as a single query. Moreover, when the language model earlier provided estimate r(wn |win−1 ), it also returned a data-structure pointer t(win ). Pointers are retained in hypotheses, edges, and partial edges for each word with an estimated probability. When context is revealed, our algorithm queries the language model with new context w1i−1 and pointer t(win ). The language model uses this pointer to immediately retrieve denominator r(wn |win−1 ) and as a starting point to retrieve numerator r(wn |w1n−1 ). It can therefore avoid looking 3 We also tested upper bounds (Huang et al., 2012; Carter et al., 2012) but the result is still approximate due to beam pruning and initial experiments showed degraded performance. 963 n−1 up r(wn ), r(wn |wn−1 ), . . . , r(wn |wi+1 ) as would normally be required with a reverse trie. 3.6 Priority Queue Our beam filling algorithm is controlled by a priority queue containing partial edges. The queue is populated by converting all outgoing hypergraph edges into partial edges and pushing them onto the queue. After this initialization, the algorithm loops. Each iteration begins by popping the top-scoring partial edge off the queue. If all nodes"
N13-1116,D11-1127,0,0.00569847,"Missing"
N13-1116,W01-1812,0,0.0533662,"nes groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and"
N13-1116,W12-3139,1,0.1598,"d suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The result is a substantial improvement over the time-accuracy trade-off presented by cube pruning. The search spaces mentioned in the previou"
N13-1116,2005.mtsummit-papers.11,1,0.0198184,"ing algorithm runs in each vertex, making use of state trees in vertices below. The top of the tree contains full hypotheses. If a K-best list is desired, packing and extraction works the same way as with cube pruning. 4 Experiments Performance is measured by translating the 3003sentence German-English test set from the 2011 Workshop on Machine Translation (Callison-Burch et al., 2011). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -1"
N13-1116,W08-0402,0,0.739541,"can be expressed as weights on edges that sum to form hypothesis features. However, log probability from an N –gram language model is non958 Proceedings of NAACL-HLT 2013, pages 958–968, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics additive because it examines surface strings across edge and vertex boundaries. Non-additivity makes search difficult because locally optimal hypotheses may not be globally optimal. In order to properly compute the language model score, each hypothesis is annotated with its boundary words, collectively referred to as its state (Li and Khudanpur, 2008). Hypotheses with equal state may be recombined, so a straightforward dynamic programming approach (Bar-Hillel et al., 1964) simply treats state as an additional dimension in the dynamic programming table. However, this approach quickly becomes intractable for large language models where the number of states is too large. Beam search (Chiang, 2005; Lowerre, 1976) approximates the straightforward algorithm by remembering a beam of up to k hypotheses1 in each vertex. It visits each vertex in bottom-up order, each time calling a beam filling algorithm to select k hypotheses. The parameter k is a"
N13-1116,P08-1023,0,0.0325037,"iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypothes"
N13-1116,D09-1078,0,0.0191413,"al and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears some similarity to our algorithm in that partially overlapping state will be collapsed and efficiently handled together. However, the key advatage to our approach is that groups have a score that can be used for pruning before the group is expanded, enabling pruning without first constructing the intersected automaton. 2.5 Coarse-to-Fine Coarse-to-fine (Petrov et al., 2008) performs multiple pruning passes, each time with more detail. Search is a subroutine of coarse-to-fine and our work is inside search, so the two are compatible. There are several for"
N13-1116,P03-1021,0,0.0146071,"). Two translation models were built, one hierarchical (Chiang, 2007) and one with target syntax. The target-syntax system is based on English parses from the Collins (1999) parser. Both were trained on Europarl (Koehn, 2005). The language model interpolates models built on Europarl, news commentary, and news data provided by the evaluation. Interpolation weights were tuned on the 2010 test set. Language models were built with SRILM (Stolcke, 2002), modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998), default pruning, and order 5. Feature weights were tuned with MERT (Och, 2003), beam size 1000, 100-best output, and cube pruning. Systems were built with the Moses (Hoang et al., 2009) pipeline. Measurements were collected by running the decoder on all 3003 sentences. For consistency, all -101.4 Average model score Average model score -101.4 -101.5 -101.6 This work Additive cube pruning Cube pruning 0 -101.5 -101.6 1 2 CPU seconds/sentence This work Gesmundo 1 Gesmundo 2 Cube pruning 0 1 2 CPU seconds/sentence Figure 5: Hierarchial system in Moses with our algorithm, cube pruning with additive scores, and cube pruning with full scores (§2.3). The two baselines overlap."
N13-1116,P02-1040,0,0.114857,"dec 1.56 to 2.24 times as fast as the best baseline. At first, this seems to suggest that cdec is faster. In fact, the opposite is true: comparing Figures 5 and 6 reveals that cdec has a higher parsing cost than Moses5 , thereby biasing the speed ratio towards 1. In subsequent experiments, we use Moses because it more accurately reflects search costs. 4.3 Average-Case Rest Costs Previous experiments used the common-practice probability estimate described in §3.5. Figure 7 shows the impact of average-case rest costs on our algorithm and on cube pruning in Moses. We also looked at uncased BLEU (Papineni et al., 2002) scores, finding that our algorithm attains near-peak BLEU in less time. The relationship between model score and BLEU is noisy due to model errors. 4 The glue rule builds hypotheses left-to-right. In Moses, glued hypotheses start with <s> and thus have empty left state. In cdec, sentence boundary tokens are normally added last, so intermediate hypotheses have spurious left state. Running cdec with the Moses glue rule led to improved time-accuracy performance. The improved version is used in all results reported. We accounted for constant-factor differences in feature definition i.e. whether <"
N13-1116,D08-1012,0,0.480051,"Missing"
N13-1116,P11-1008,0,0.00997303,"admissible heuristic. Their analysis showed deep and unbalanced search trees. Our work can be interpreted as a partial rebalancing of these search trees. 2.4 Exact Algorithms A number of exact search algorithms have been developed. We are not aware of an exact algorithm that tractably scales to the size of hypergraphs and language models used in many modern machine translation systems (Callison-Burch et al., 2012). The hypergraph and language model can be compiled into an integer linear program. The best hypothesis can then be recovered by taking the dual and solving by Lagrangian relaxation (Rush and Collins, 2011). However, that work only dealt with language models up to order three. Iglesias et al. (2011) represent the search space as a recursive transition network and the language model as a weighted finite state transducer. Using standard finite state algorithms, they intersect the two automatons then exactly search for the highestscoring paths. However, the intersected automaton is too large. The authors suggested removing low probability entries from the language model, but this form of pruning negatively impacts translation quality (Moore and Quirk, 2009; Chelba et al., 2010). Their work bears so"
N13-1116,W96-0108,0,0.0624739,"ast as with cube pruning in common cases. 1  at  in  North Korea with the DPRK Figure 1: Hypotheses are grouped by common prefixes and suffixes. Introduction This work presents a new algorithm to search a packed data structure for high-scoring hypotheses when the score includes an N –gram language model. Many natural language processing systems have this sort of problem e.g. hypergraph search in hierarchical and syntactic machine translation (Mi et al., 2008; Klein and Manning, 2001), lattice rescoring in speech recognition, and confusion network decoding in optical character recognition (Tong and Evans, 1996). Large language models have been shown to improve quality, especially in machine translation (Brants et al., 2007; Koehn and Haddow, 2012). However, language models make search computationally expensive because they examine surface words without regard to the structure of the packed search space. Prior work, including cube pruning (Chiang, 2007), has largely treated the language model as a black box. Our new search algorithm groups hypotheses by common prefixes and suffixes, exploiting the tendency of the language model to score these hypotheses similarly. An example is shown in Figure 1. The"
N13-1116,P06-1098,0,0.0721756,"improves the time-accuracy trade-off over the popular cube pruning algorithm (Chiang, 2007) discussed in §2.3. The algorithm is based on the observation that competing hypotheses come from the same imput, so their language model states are often similar. Grouping hypotheses by these similar words enables our algorithm to reason over multiple hypotheses at once. The algorithm is fully described in §3. 2 Related Work 2.1 Alternatives to Bottom-Up Search Beam search visits each vertex in the hypergraph in bottom-up (topological) order. The hypergraph can also be searched in left-to-right order (Watanabe et al., 2006; Huang and Mi, 2010). Alternatively, hypotheses can be generated on demand with cube growing (Huang and Chiang, 2007), though we note that it showed little improvement in Moses (Xu and Koehn, 2012). All of these options are compatible with our algorithm. However, we only experiment with bottom-up beam search. 1 We use K to denote the number of fully-formed hypotheses requested by the user and k to denote beam size. 959 2.2 Exhaustive Beam Filling Originally, beam search was used with an exhaustive beam filling algorithm (Chiang, 2005). It generates every possible hypothesis (subject to the be"
N13-1116,J03-4003,0,\N,Missing
N18-1055,P06-1032,0,0.12591,"ethods from statistical machine translation (SMT), especially the phrase-based variant. For the CoNLL 2014 benchmark on grammatical error correction (Ng et al., 2014), Junczys-Dowmunt and Grundkiewicz (2016) established a set of methods for GEC by SMT that remain state-of-the-art. Systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017) that improve on results by Junczys-Dowmunt and Grundkiewicz (2016) use their set-up as a backbone for more complex systems. The view that GEC can be approached as a machine translation problem by translating from erroneous to correct text originates from Brockett et al. (2006) and resulted in many systems (e.g. Felice et al., 2014; Susanto et al., 2014) that represented the current state-of-the-art at the time. In the field of machine translation proper, the emergence of neural sequence-to-sequence methods and their impressive results have lead to a paradigm shift away from phrase-based SMT towards neural machine translation (NMT). During WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, bu"
N18-1055,2011.mtsummit-papers.1,0,0.0502984,"n the paper. We did not see any differences compared to smaller beams. – 70.8 – 9.5 32.9 30.9 47.2 47.0 52.1 52.5 44.0 Average of 4 Ensemble of 4 40.0 t-S rc ou +D ro p +D om Optimizer instability Junczys-Dowmunt and Grundkiewicz (2016) noticed that discriminative parameter tuning for GEC by phrase-based SMT leads to unstable M2 results between tuning runs. This is a well-known effect for SMT parameter tuning and Clark et al. (2011) recommend reporting results for multiple tuning runs. Junczys-Dowmunt and Grundkiewicz (2016) perform four tuning runs and calculate parameter centroids following Cettolo et al. (2011). Neural sequence-to-sequence training is discriminative optimization and as such prone to instability. We already try to alleviate this by averaging over eight best checkpoints, but as seen in Table 3, results for M2 remain unstable for runs with differently initialized weights. An amplitude of 3 points M2 on the CoNLL-2014 test set is larger than most improvements reported in recent papers. None of the recent works on neural GEC account for instability, hence it is unclear if observed outcomes are actual improvements or lucky picks among byproducts of instability. We therefore strongly sugge"
N18-1055,W17-5037,0,0.471992,"we segment training and test/dev data accordingly. Segmentation is reversed before evaluation. Table 2: Statistics for test and development data. 2.1 Training and test data To make our results comparable to state-of-the-art results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our met"
N18-1055,P11-2031,0,0.0393378,"thub.com/grammatical/ neural-naacl2018 5 https://github.com/marian-nmt/marian 6 We used a larger beam-size than usual due to experiments with re-ranking of n-best lists not included in the paper. We did not see any differences compared to smaller beams. – 70.8 – 9.5 32.9 30.9 47.2 47.0 52.1 52.5 44.0 Average of 4 Ensemble of 4 40.0 t-S rc ou +D ro p +D om Optimizer instability Junczys-Dowmunt and Grundkiewicz (2016) noticed that discriminative parameter tuning for GEC by phrase-based SMT leads to unstable M2 results between tuning runs. This is a well-known effect for SMT parameter tuning and Clark et al. (2011) recommend reporting results for multiple tuning runs. Junczys-Dowmunt and Grundkiewicz (2016) perform four tuning runs and calculate parameter centroids following Cettolo et al. (2011). Neural sequence-to-sequence training is discriminative optimization and as such prone to instability. We already try to alleviate this by averaging over eight best checkpoints, but as seen in Table 3, results for M2 remain unstable for runs with differently initialized weights. An amplitude of 3 points M2 on the CoNLL-2014 test set is larger than most improvements reported in recent papers. None of the recent"
N18-1055,N12-1067,0,0.398337,"nd Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beg"
N18-1055,W13-1703,0,0.405661,"C, however, an analysis on advantages of word versus sub-word or character level segmentation is beyond the scope of this paper. A set of 50,000 monolingual BPE units is trained on the error-annotated data and we segment training and test/dev data accordingly. Segmentation is reversed before evaluation. Table 2: Statistics for test and development data. 2.1 Training and test data To make our results comparable to state-of-the-art results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores"
N18-1055,W17-3203,0,0.207254,"aining objective for GEC. We investigate how to leverage monolingual data for neural GEC by transfer learning in Section 4 and experiment with language model ensembling in Section 5. Section 6 explores deep NMT architectures. In Section 7, we provide an overview of the experiments and how results relate to the JFLEG benchmark. We also recommend a model-independent toolbox for neural GEC. 2 A trustable baseline for neural GEC In this section, we combine insights from JunczysDowmunt and Grundkiewicz (2016) for grammatical error correction by phrase-based statistical machine translation and from Denkowski and Neubig (2017) for trustable results in neural machine translation to propose a trustable baseline for neural grammatical error correction. 596 Test/Dev set Sent. Annot. Metric CoNLL-2013 test CoNLL-2014 test JFLEG dev JFLEG test 1,381 1,312 754 747 1 2 4 4 M2 M2 GLEU GLEU large-vocabulary problem of NMT. This is a well established procedure in neural machine translation and has been demonstrated to be generally superior to UNK-replacement methods. It has been largely ignored in the field of grammatical error correction even when word segmentation issues have been explored (Ji et al., 2017; Schmaltz et al.,"
N18-1055,W17-4724,0,0.0793217,"al methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with some overlap with our methods. However, our results stay ahead on all benchmarks while using simpler models. 595 Proceedings of NAACL-HLT 2018, pages 595–606 c New Orleans, Louisiana, June 1 - 6,"
N18-1055,N13-1073,0,0.0285722,"NUCLE corpus ten times to the training corpus. This can also be seen as similar to Junczys-Dowmunt and Grundkiewicz (2016) who tune phrase-based SMT parameters on the entire NUCLE corpus. Respectable improvements on both CoNLL test sets (+Domain-Adapt. in Table 4) are achieved. 3.3 Λ L(x, y, a) = − Ty X t=1 λ(xat , yt ) = λ(xat , yt ) log P (yt |x, y&lt;t ),  Λ if xat 6= yt , 1 otherwise where (x, y) is a training sentence pair and a is a word alignment at ∈ {0, 1, . . . , Tx } such that source token xat generates target token yt . Alignments are computed for each sentence pair with fast-align (Dyer et al., 2013). 599 7 Output embeddings are encoded in the last output layer of a neural language or translation model. output embedding Pre-trained embeddings Pre-trained decoder parameters Randomly initialized parameters first output layer second cGRU block attention mechanism bidirectional GRU first cGRU block source embedding target embedding encoder decoder Figure 2: Parameters pretrained on monolingual data are marked with colors. Blue indicates pre-trained embeddings with word2vec, red parameters have been pre-trained with the GRU-based language model only. All embedding layers have tied parameters."
N18-1055,W14-1702,0,0.498932,"Missing"
N18-1055,P17-1070,0,0.497382,"ards neural machine translation (NMT). During WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) publ"
N18-1055,D16-1161,1,0.919021,"kens. Among these the Lang-8 corpus is quite noisy and of low quality. The Cambridge Learner Corpus (CLC) by Nicholls (2003) — probably the best resource in this list — is non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult. Contrasting this with Fig. 1, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models. Current state-of-the-art GEC systems based on SMT, however, all include large-scale indomain language models either following the steps outlined in Junczys-Dowmunt and Grundkiewicz (2016) or directly re-using their domain-adapted Common-Crawl language model. It seems that the current state of neural methods in GEC reflects the behavior for NMT systems trained on smaller data sets. Based on this, we conclude that we can think of GEC as a lowresource, or at most mid-resource, machine translation problem. This means that techniques proposed for low-resource (neural) MT should be applicable to improving neural GEC results. In this work we show that adapting techniques from low-resource (neural) MT and SMT-based GEC methods allows neural GEC systems to catch up to and outperform SM"
N18-1055,P18-4020,1,0.882714,"Missing"
N18-1055,W17-3204,0,0.025851,"te-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with some overlap with our methods. However, our results stay ahead on all benchmarks while using simpler models. 595 Proceedings of NAACL-HLT 2018, pages 595–606 c New Orleans, Louisiana, June 1 - 6,"
N18-1055,D17-1156,0,0.0234167,"nd the objective function were modified. In this section we investigate if these techniques can be generalized to deeper or different architectures. 6.1 Architectures Dev Prec. Rec. Test +Pretrain-Dec. +GRU-LM 40.3 41.6 65.2 62.2 32.2 36.6 54.1 54.6 +Deep-RNN +Deep-RNN-LM 41.1 41.9 64.3 61.3 35.2 40.2 55.2 55.5 +Transformer +Transformer-LM 41.5 42.9 63.0 61.9 38.9 40.2 56.1 55.8 Table 8: Shallow (Pretrain-Dec.) versus deep ensembles, with and without corresponding language models. We consider two state-of-the-art NMT architectures implemented in Marian: Deep RNN A deep RNN-based model (Miceli Barone et al., 2017) proposed by Sennrich et al. (2017a) for their WMT 2017 submissions. This model is based on the shallow model we used until now. It has single layer RNNs in the encoder and decoder, but increases depth by stacking multiple GRU-style blocks inside one RNN cell. A single RNN step passes through all blocks before recursion. The encoder RNN contains 4 stacked GRU blocks, the decoder 8 (1 + 7 due to the conditional GRU). Following Sennrich et al. (2017a), we enable layer-normalization in the RNN-layers. State and embedding dimensions used throughout this work and in Sennrich et al. (2017a) are the"
N18-1055,W17-4710,0,0.0254019,"nd the objective function were modified. In this section we investigate if these techniques can be generalized to deeper or different architectures. 6.1 Architectures Dev Prec. Rec. Test +Pretrain-Dec. +GRU-LM 40.3 41.6 65.2 62.2 32.2 36.6 54.1 54.6 +Deep-RNN +Deep-RNN-LM 41.1 41.9 64.3 61.3 35.2 40.2 55.2 55.5 +Transformer +Transformer-LM 41.5 42.9 63.0 61.9 38.9 40.2 56.1 55.8 Table 8: Shallow (Pretrain-Dec.) versus deep ensembles, with and without corresponding language models. We consider two state-of-the-art NMT architectures implemented in Marian: Deep RNN A deep RNN-based model (Miceli Barone et al., 2017) proposed by Sennrich et al. (2017a) for their WMT 2017 submissions. This model is based on the shallow model we used until now. It has single layer RNNs in the encoder and decoder, but increases depth by stacking multiple GRU-style blocks inside one RNN cell. A single RNN step passes through all blocks before recursion. The encoder RNN contains 4 stacked GRU blocks, the decoder 8 (1 + 7 due to the conditional GRU). Following Sennrich et al. (2017a), we enable layer-normalization in the RNN-layers. State and embedding dimensions used throughout this work and in Sennrich et al. (2017a) are the"
N18-1055,C12-2084,0,0.387332,"Missing"
N18-1055,W17-5039,0,0.317175,"Missing"
N18-1055,E17-2037,0,0.359355,"al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Foll"
N18-1055,W14-1701,0,0.581413,"results in the field of GEC, we limit our training data strictly to public resources. In the case of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preproc"
N18-1055,W13-3601,0,0.321815,"of error-annotated data, as marked in Table 1, these are the NUCLE (Dahlmeier et al., 2013) and Lang8 NAIST (Mizumoto et al., 2012) data sets. We do not include the FCE corpus (Yannakoudakis et al., 2011) to maintain comparability to JunczysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization"
N18-1055,D17-1039,0,0.0626855,"Missing"
N18-1055,Q16-1013,0,0.0340052,"zysDowmunt and Grundkiewicz (2016) and Chollampatt and Ng (2017). We strongly urge the community to not use the non-public CLC corpus for training, unless contrastive results without this corpus are provided as well. We choose the CoNLL-2014 shared task test set (Ng et al., 2014) as our main benchmark and the test set from the 2013 edition of the shared task (Ng et al., 2013) as a development set. For these benchmarks we report MaxMatch (M2 ) scores (Dahlmeier and Ng, 2012). Where appropriate, we will provide results on the JFLEG dev and test sets (Napoles et al., 2017) using the GLEU metric (Sakaguchi et al., 2016) to demonstrate the generality of our methods. Table 2 summarizes test/dev set statistics for both tasks. For most our experiments, we report M2 on CoNLL-2013 test (Dev) and precision (Prec.), recall (Rec.), M2 (Test) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enc"
N18-1055,D14-1102,0,0.08616,"Missing"
N18-1055,I17-2062,0,0.083227,"Missing"
N18-1055,D17-1298,0,0.400682,"ring WMT 2017 (Bojar et al., 2017) authors of pure phrase-based systems offered “unconditional surrender”1 to NMT-based methods. Based on these developments, one would expect to see a rise of state-of-the-art neural methods for GEC, but as Junczys-Dowmunt and Grundkiewicz (2016) already noted, this is not the case. Interestingly, even today, the top systems on established GEC benchmarks are still mostly phrase-based or hybrid systems (Chollampatt and Ng, 2017; Yannakoudakis et al., 2017; Napoles and CallisonBurch, 2017). The best “pure” neural systems (Ji et al., 2017; Sakaguchi et al., 2017; Schmaltz et al., 2017) are several percent behind.2 If we look at recent MT work with this in mind, we find one area where phrased-based SMT dominates over NMT: low-resource machine translation. Koehn and Knowles (2017) analyze the behavior of NMT versus SMT for English-Spanish systems trained on 0.4 million to 385.7 million words of parallel data, illustrated in Figure 1. Quality for NMT 1 Ding et al. (2017) on their news translation shared task poster http://www.cs.jhu.edu/˜huda/papers/ jhu-wmt-2017.pdf 2 After submission of this work, Chollampatt and Ng (2018) published impressive new results for neural GEC with"
N18-1055,W17-4739,1,0.892616,"included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of BPE segments determines the size of the vocabulary of our models, i.e. 50,000 entries. Source and target side use the same vocabulary. To avoid overfitting, we use variational dropout (Gal and Ghahramani, 2016) over GRU steps and input embeddings with probability 0.2. We optimize with Adam (K"
N18-1055,E17-3017,1,0.840934,"Missing"
N18-1055,W16-2309,0,0.0297254,"t) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of B"
N18-1055,P16-1162,0,0.0866913,"t) on the CoNLL-2014 test set. 2.2 Preprocessing and sub-words As both benchmarks, CoNLL and JFLEG, are provided in NLTK-style tokenization (Bird et al., 2009), we use the same tokenization scheme for our training data. We truecase line beginnings and escape special characters using scripts included with Moses (Koehn et al., 2007). Following Sakaguchi et al. (2017), we apply the Enchant3 spell-checker to the JFLEG data before evaluation. No spellchecking is used for the CoNLL test sets. We follow the recommendation by Denkowski and Neubig (2017) to use byte-pair encoding (BPE) sub-word units (Sennrich et al., 2016b) to solve the 3 2.3 Model and training procedure Implementations of all models explored in this work4 are available in the Marian5 toolkit (JunczysDowmunt et al., 2018). The attentional encoderdecoder model in Marian is a re-implementation of the NMT model in Nematus (Sennrich et al., 2017b). The model differs from the model introduced by Bahdanau et al. (2014) by several aspects, the most important being the conditional GRU with attention for which Sennrich et al. (2017b) provide a concise description. All embedding vectors consist of 512 units; the RNN states of 1024 units. The number of B"
N18-1055,P11-1019,0,0.723877,"Public 57.1K 1.9M 30.9K 1.9M 1.2M 25.0M 0.5M 29.2M Yes Yes Yes No Table 1: Statistics for existing GEC training data sets. Data sets marked with * are used in this work. starts low for small corpora, outperforms SMT at a corpus size of about 15 million words, and with increasing size beats SMT with a large in-domain language model. Table 1 lists existing training resources for the English as-a-second-language (ESL) grammatical error correction task. Publicly available resources, NUS Corpus of Learner English (NUCLE) by Dahlmeier et al. (2013), Lang-8 NAIST (Mizumoto et al., 2012) and CLC FCE (Yannakoudakis et al., 2011) amount to about 27M tokens. Among these the Lang-8 corpus is quite noisy and of low quality. The Cambridge Learner Corpus (CLC) by Nicholls (2003) — probably the best resource in this list — is non-public and we would strongly discourage reporting results that include it as training data as this makes comparisons difficult. Contrasting this with Fig. 1, we see that for about 20M tokens NMT systems start outperforming SMT models without additional large language models. Current state-of-the-art GEC systems based on SMT, however, all include large-scale indomain language models either following"
N18-1055,D17-1297,0,0.140577,"ed in transfer learning. In general, one first trains a neural model on high-resource data and then uses the resulting parameters to initialize parameters of a new model meant to be trained on lowresource data only. Various settings are possible, e.g. initializing from models trained on large outof-domain data and continuing on in-domain data (Miceli Barone et al., 2017) or using related language pairs (Zoph et al., 2016). Models can also be partially initialized by pre-training monolingual language models (Ramachandran et al., 2017) or only word-embeddings (Gangi and Federico, 2017). In GEC, Yannakoudakis et al. (2017) apply pretrained monolingual word-embeddings as initializations for error-detection models to re-rank SMT n-best lists. Approaches based on pre-training with monolingual data appear to be particularly wellsuited to the GEC task. Junczys-Dowmunt and Grundkiewicz (2016) published 300GB of compressed monolingual data used in their work to create a large domain-adapted Common-Crawl ngram language model.8 We use the first 100M lines. Preprocessing follows section 2.2 including BPE segmentation. 4.1 Pre-training embeddings Similarly to Gangi and Federico (2017) or Yannakoudakis et al. (2017), we us"
N18-1055,D16-1163,0,0.025615,"Missing"
N18-1055,W16-2323,0,\N,Missing
P13-2121,W13-2212,1,0.735043,"e built an unpruned model (Table 1) on 126 billion tokens. Estimation used a machine with 140 GB RAM and six hard drives in a RAID5 configuration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8 Large 28.2 33.4 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran out"
P13-2121,D10-1026,0,0.0246209,"ing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the commun"
P13-2121,D07-1090,0,0.84713,"Each MapReduce performs three copies over the network when only one is required. Arrows denote copies over the network (i.e. to and from a distributed filesystem). Both options use local disk within each reducer for merge sort. contributes an efficient multi-pass streaming algorithm using disk and a user-specified amount of RAM. Introduction 2 Relatively low perplexity has made modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998) a popular choice for language modeling. However, existing estimation methods require either large amounts of RAM (Stolcke, 2002) or machines (Brants et al., 2007). As a result, practitioners have chosen to use less data (Callison-Burch et al., 2012) or simpler smoothing methods (Brants et al., 2007). Backoff-smoothed n-gram language models (Katz, 1987) assign probability to a word wn in context w1n−1 according to the recursive equation ( p(wn |w1n−1 ), if w1n was seen n−1 p(wn |w1 ) = b(w1n−1 )p(wn |w2n ), otherwise Related Work Brants et al. (2007) showed how to estimate Kneser-Ney models with a series of five MapReduces (Dean and Ghemawat, 2004). On 31 billion words, estimation took 400 machines for two days. Recently, Google estimated a pruned Knese"
P13-2121,D12-1107,1,0.71805,"on and put forward several scenarios in which a single machine scale-up approach is more cost effective in terms of both raw performance and performance per dollar. Brants et al. (2007) contributed Stupid Backoff, a simpler form of smoothing calculated at runtime from counts. With Stupid Backoff, they scaled to 1.8 trillion tokens. We agree that Stupid Backoff is cheaper to estimate, but contend that this work makes Kneser-Ney smoothing cheap enough. Another advantage of Stupid Backoff has been that it stores one value, a count, per n-gram instead of probability and backoff. In previous work (Heafield et al., 2012), we showed how to collapse probability and backoff into a single value without changing sentence-level probabilities. However, local scores do change and, like Stupid Backoff, are no longer probabilities. MSRLM (Nguyen et al., 2007) aims to scalably estimate language models on a single machine. Counting is performed with streaming algorithms similarly to this work. Their parallel merge sort also has the potential to be faster than ours. The biggest difference is that their pipeline delays some computation (part of normalization and all of interpolation) until query time. This means that it ca"
P13-2121,W12-3102,1,0.6588,"Missing"
P13-2121,W11-2123,1,0.520052,"Missing"
P13-2121,D11-1125,0,0.0714439,"ontexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Scaling Acknowledgements"
P13-2121,N12-1047,0,0.255787,"he quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scalable; efficient merge sort makes it fast. In future work, we plan to extend to the Common Crawl corpus and improve parallelism. Scaling Acknowledgements We built an unpruned model (Table 1) on 126 billion tokens."
P13-2121,P07-2045,1,0.0338753,"00 800 Tokens (millions) 1000 Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010"
P13-2121,2005.mtsummit-papers.11,1,0.0337569,"simultaneously. 10 8 6 4 2 0 0 200 400 600 800 Tokens (millions) 1000 Figure 5: CPU usage (system plus user). Each n-gram record is an array of n vocabulary identifiers (4 bytes each) and an 8-byte count or probability and backoff. At peak, records are stored twice on disk because lazy merge sort is not easily amenable to overwriting the input file. Additional costs are the secondary backoff file (4 bytes per backoff) and the vocabulary in plaintext. 5 Experiments Experiments use ClueWeb09.7 After spam filtering (Cormack et al., 2011), removing markup, selecting English, splitting sentences (Koehn, 2005), deduplicating, tokenizing (Koehn et al., 2007), and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Bett"
P13-2121,P02-1040,0,0.102307,"ration (sustained read: 405 MB/s). It took 123 GB RAM, 2.8 days wall time, and 5.4 CPU days. A summary of Google’s results from 2007 on different data and hardware appears in §2. We then used this language model as an additional feature in unconstrained Czech-English, French-English, and Spanish-English submissions to the 2013 Workshop on Machine Translation.8 Our baseline is the University of Edinburgh’s phrase-based Moses (Koehn et al., 2007) submission (Durrani et al., 2013), which used all constrained data specified by the evaluation (7 billion tokens of English). It placed first by BLEU (Papineni et al., 2002) among constrained submissions in each language pair we consider. In order to translate, the large model was quantized (Whittaker and Raj, 2001) to 10 bits and compressed to 643 GB with KenLM (Heafield, 8 Large 28.2 33.4 32.6 Table 2: Uncased BLEU results from the 2013 Workshop on Machine Translation. We estimated unpruned language models in binary format on sentences randomly sampled from ClueWeb09. SRILM and IRSTLM were run until the test machine ran out of RAM (64 GB). For our code, the memory limit was set to 3.5 GB because larger limits did not improve performance on this small data. Resu"
P13-2121,P07-1065,0,0.048315,"and truecasing, 126 billion tokens remained. 5 Backoffs only exist if the n-gram is the context of some n + 1-gram, so merging skips n-grams that are not contexts. 6 With quantization (Whittaker and Raj, 2001), the quantizer is trained in a first pass and applied in a second pass. 7 693 http://lemurproject.org/clueweb09/ 1 393 2 3,775 3 17,629 4 39,919 Source Czech French Spanish 5 59,794 Table 1: Counts of unique n-grams (in millions) for the 5 orders in the large LM. 5.1 Estimation Comparison 2011) then copied to a machine with 1 TB RAM. Better compression methods (Guthrie and Hepple, 2010; Talbot and Osborne, 2007) and distributed language models (Brants et al., 2007) could reduce hardware requirements. Feature weights were retuned with PRO (Hopkins and May, 2011) for Czech-English and batch MIRA (Cherry and Foster, 2012) for French-English and Spanish-English because these worked best for the baseline. Uncased BLEU scores on the 2013 test set are shown in Table 2. The improvement is remarkably consistent at 0.8 BLEU point in each language pair. 6 Conclusion Our open-source (LGPL) estimation code is available from kheafield.com/code/kenlm/ and should prove useful to the community. Sorting makes it scala"
P14-2022,koen-2004-pharaoh,0,0.702223,"eses that have exactly the same state can be recombined and efficiently handled via dynamic programming, but there is no special handling for partial agreement. Therefore, features are repeatedly consulted regarding hypotheses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason"
P14-2022,N10-2003,1,0.85227,", improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix. The present phra"
P14-2022,W08-0402,0,0.766032,"and Chiang, 2007). Sections 3.2 and later show our contribution. 131 a few nations  diplomatic are which  have diplomatic Figure 3: Target phrases arranged into a trie. Set in italic, leaves reveal parts of the phrase that are irrelevant to the language model. countries Figure 2: Hypothesis suffixes arranged into a trie. The leaves indicate source coverage and any other hypothesis state. arrange the target phrases into a prefix trie. An example is shown in Figure 3. Similar to the hypothesis trie, the depth may be shorter than N − 1 in cases where the language model will provably back off (Li and Khudanpur, 2008). The trie can also be short because the target phrase has fewer than N − 1 words. We currently store this trie data structure directly in the phrase table, though it could also be computed on demand to save memory. Empirically, our phrase table uses less RAM than Moses’s memory-based phrase table. As an optimization, a trie reveals multiple words when there would otherwise be no branching. This allows the search algorithm to make decisions only when needed. Following Heafield et al. (2013), leaves in the trie take the score of the underlying hypothesis or target phrase. Non-leaf nodes take th"
P14-2022,D11-1003,0,0.0660812,"detailed models, pruning after each pass. The key difference in our work is that, rather than refining models in lock step, we effectively refine the language model on demand for hypotheses that score well. Moreover, their work was performed in syntactic machine translation while we address issues specific to phrase-based translation. Our baseline is cube pruning (Chiang, 2007; Huang and Chiang, 2007), which is both a way to organize search and an algorithm to search through cross products of sets. We adopt the same search organization (Section 3.1) but change how cross products are searched. Chang and Collins (2011) developed an exact decoding algorithm based on Lagrangian relaxation. However, it has not been shown to tractably scale to 5-gram language models used by many modern translation systems. 3 0 word 1 word the cat . 2 words cat the cat cat the . the 3 words cat . the cat . cat the . . the cat Figure 1: Stacks to translate the French “le chat .” into English. Filled circles indicate that the source word has been translated. A phrase translates “le chat” as simply “cat”, emphasizing that stacks are organized by the number of source words rather than the number of target words. 3.1 Search Organizat"
P14-2022,P02-1040,0,0.0891183,"illion words on the English side. The bitext contains data from several sources, including news articles, UN proceedings, Hong Kong government documents, online forum data, and specialized sources such as an idiom translation table. We also trained our language model on the English half of this bitext using unpruned interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). The system has standard phrase table, length, distortion, and language model features. We plan to implement lexicalized reordering in future work; without this, the test system is 0.53 BLEU (Papineni et al., 2002) point behind a state-of-theart system. We set the reordering limit to R = 15. The phrase table was pre-pruned by applying the same heuristic as Moses: select the top 20 target phrases by score, including the language model. Priority Queue Search proceeds in a best-first fashion controlled by a priority queue. For each source phrase, we convert the compatible hypotheses into a trie. The target phrases were already converted into a trie when the phrase table was loaded. We then push the root (, ) boundary pair into the priority queue. We do this for all source phrases under consideration, put"
P14-2022,J07-2003,0,0.842189,"heses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once. Our primary contribution is a new search algorithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state c"
P14-2022,D08-1012,0,0.175522,"Missing"
P14-2022,2011.iwslt-evaluation.24,1,0.87672,"age of the source sentence and the state of other features. Each source phrase translates to a set of target phrases. Because these phrases will be appended to a hypothesis, the first few words matter the most to the language model. We therefore times the weight of the language model. This has the effect of cancelling out the estimate made 132 3.6 when the phrase was scored in isolation, replacing it with a more accurate estimate based on available context. These score adjustments are efficient to compute because the decoder retained a pointer to “that” in the language model’s data structure (Heafield et al., 2011). 3.4 We build hypotheses from left-to-right and manage stacks just like cube pruning. The only difference is how the k elements of these stacks are selected. When the decoder matches a hypothesis with a compatible source phrase, we immediately evaluate the distortion feature and update future costs, both of which are independent of the target phrase. Our future costs are exactly the same as those used in Moses (Koehn et al., 2007): the highest-scoring way to cover the rest of the source sentence. This includes the language model score within target phrases but ignores the change in language m"
P14-2022,C12-3061,0,0.0506586,"be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix. The present phrasebased setting is simp"
P14-2022,N13-1116,1,0.84069,"sis trie, the depth may be shorter than N − 1 in cases where the language model will provably back off (Li and Khudanpur, 2008). The trie can also be short because the target phrase has fewer than N − 1 words. We currently store this trie data structure directly in the phrase table, though it could also be computed on demand to save memory. Empirically, our phrase table uses less RAM than Moses’s memory-based phrase table. As an optimization, a trie reveals multiple words when there would otherwise be no branching. This allows the search algorithm to make decisions only when needed. Following Heafield et al. (2013), leaves in the trie take the score of the underlying hypothesis or target phrase. Non-leaf nodes take the maximum score of their descendants. Children of a node are sorted by score. lated, and the reordering limit. Second, the decoder searches through these matches to select k high-scoring hypotheses for placement in the stack. We improve this second step. The decoder provides our algorithm with pairs consisting of a hypothesis and a compatible source phrase. Each source phrase translates to multiple target phrases. The task is to grow these hypotheses by appending a target phrase, yielding n"
P14-2022,P08-1025,0,0.0213756,"tribution in this paper is efficiently ignoring coverage when evaluating the language model. In contrast, syntactic machine translation hypotheses correspond to contiguous spans in the source sentence, so in prior work we simply ran the search algorithm in every span. Another improvement upon Heafield et al. (2013) is that we previously made no effort to exploit common words that appear in translation rules, which are analogous to phrases. In this work, we explicitly group target phrases by common prefixes, doing so directly in the phrase table. Coarse-to-fine approaches (Petrov et al., 2008; Zhang and Gildea, 2008) invoke the decoder multiple times with increasingly detailed models, pruning after each pass. The key difference in our work is that, rather than refining models in lock step, we effectively refine the language model on demand for hypotheses that score well. Moreover, their work was performed in syntactic machine translation while we address issues specific to phrase-based translation. Our baseline is cube pruning (Chiang, 2007; Huang and Chiang, 2007), which is both a way to organize search and an algorithm to search through cross products of sets. We adopt the same search organization (Sect"
P14-2022,W11-2123,1,0.835512,"been found. 133 15 -28.0 Uncased BLEU Average model score -27.5 14 -28.5 -29.0 -29.5 13 This Work Moses 0 1 2 3 4 CPU seconds/sentence This Work Moses 0 1 2 3 4 CPU seconds/sentence Figure 4: Performance of our decoder and Moses for various stack sizes k. Moses (Koehn et al., 2007) revision d6df825 was compiled with all optimizations recommended in the documentation. We use the inmemory phrase table for speed. Tests were run on otherwise-idle identical machines with 32 GB RAM; the processes did not come close to running out of memory. The language model was compiled into KenLM probing format (Heafield, 2011) and placed in RAM while text phrase tables were forced into the disk cache before each run. Timing is based on CPU usage (user plus system) minus loading time, as measured by running on empty input; our decoder is also faster at loading. All results are single-threaded. Model score is comparable across decoders and averaged over all 1677 sentences; higher is better. The relationship between model score and uncased BLEU (Papineni et al., 2002) is noisy, so peak BLEU is not attained by the highest search accuracy. Stack 10 100 1000 10000 Model Moses This -29.96 -29.70 -28.68 -28.54 -27.87 -27.8"
P14-2022,P07-1019,0,0.78424,"ding hypotheses that differ only in ways irrelevant to their score, such as coverage of the source sentence. Our decoder bundles hypotheses into equivalence classes so that features can focus on the relevant parts of state. We pay particular attention to the language model because it is responsible for much of the hypothesis state. As the decoder builds translations from left to right (Koehn, 2004), it records the last N − 1 words of each hypothesis so that they can be used as context to score the first N − 1 words of a phrase, where N is the order of the language model. Traditional decoders (Huang and Chiang, 2007) try thousands of combinations of hypotheses and phrases, hoping to find ones that the language model likes. Our algorithm instead discovers good combinations in a coarse-to-fine manner. The algorithm exploits the fact that hypotheses often share the same suffix and phrases often share the same prefix. These shared suffixes and prefixes allow the algorithm to coarsely reason over many combinations at once. Our primary contribution is a new search algorithm that exploits the above observations, namely that state can be divided into pieces relevant to each feature and that language model state c"
P14-2022,P07-2045,0,0.166231,"are both approximate, improvement can be used to increase speed or accuracy. When tuned to attain the same accuracy, our algorithm is 4.0–7.7 times as fast as the Moses decoder with cube pruning. 1 Introduction Translation speed is critical to making suggestions as translators type, mining for parallel data by translating the web, and running on mobile devices without Internet connectivity. We contribute a fast decoding algorithm for phrase-based machine translation along with an implementation in a new open-source (LGPL) decoder available at http://kheafield.com/code/. Phrase-based decoders (Koehn et al., 2007; Cer et al., 2010; Wuebker et al., 2012) keep track of several types of state with translation hypothe2 Related Work Our previous work (Heafield et al., 2013) developed language model state refinement for bottom130 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 130–135, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics up decoding in syntatic machine translation. In bottom-up decoding, hypotheses can be extended to the left or right, so hypotheses keep track of both their prefix and suffix"
P15-2063,D13-1195,0,0.212265,"language modeling. 1 2 Related Work Automata have been emulated on CPUs with AT&T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudom´ın et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has t"
P15-2063,eisele-chen-2010-multiun,0,0.0149696,"expressions. Instead of strings, we can match vocabulary indices. Spaces are unnecessary since indices have fixed length and the unknown word has an index. 6 6.2 Language Modeling We benchmarked against the fastest reported language models, DALM’s reverse trie (Yasuhara et al., 2013) and KenLM’s linear probing (Heafield, 2011). Both use stateful queries. For surface strings, time includes the cost of vocabulary lookup. For vocabulary identifiers, we converted words to bytes then timed custom query programs. Unpruned models were trained on the English side of the French–English MultiUN corpus (Eisele and Chen, 2010). Perplexity was computed on 2.6 GB of tokenized text from the 2013 English News Crawl (Bojar et al., 2014). Experiments We benchmarked a Tarari T2540 PCI express device from 2011 against several CPU baselines. It has 2 GB of DDR2 RAM and 5 cores. A singlethreaded CPU program controls the device and performs arithmetic. The program scaled linearly to control four devices, so it is not a bottleneck. Wall clock time, except loading, is the minimum from three runs on an otherwise-idle machine. Models and input were in RAM before each run. 6.1 Ken DA 1 core 5 cores 37.8 40.3 6.6 2.1 42.4 43.6 16.2"
P15-2063,D12-1107,1,0.86453,"Association for Computational Linguistics 5.1 When an expression matches, the hardware can output a constant to the CPU, output the span matched, push a symbol onto the stack, pop from the stack, or halt. There is little meaning to the order in which the expressions appear in the program. All expressions are able to match at any time, but can condition on the top of the stack. This is similar to the flex tool (Lesk and Schmidt, 1975), which refers to stack symbols as start conditions. 4 The backoff algorithm normally requires storing probability p and backoff b with each seen n–gram. However, Heafield et al. (2012) used telescoping series to prove that probability and backoff can be collapsed into a single function q Qn n n−1 n−1 i=1 b(wi ) q(wn |w1 ) = p(wn |w1 ) Qn−1 n−1 ) i=1 b(wi This preserves sentence-level probabilities.1 Because the hardware lacks user-accessible arithmetic, terms are sent to the CPU. Sending just q for each token instead of p and various backoffs b reduces communication and CPU workload. We also benefit from a simplified query procedure: for each word, match as much context as possible then return the corresponding value q. Language Identification We exactly replicate the model"
P15-2063,W11-2123,1,0.922208,"(Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use the fastest baselines from both. Introduction Larger data sizes and more detailed models have led to adoption of specialized hardware for natural language processing. Graphics processing units (GPUs) are the most common, with applications to neural networks (Oh and Jung, 2004) and parsing (Johnson, 2011). Field-programmable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In this work, we go further down the hardware hierarchy by performing language identification an"
P15-2063,P13-1135,0,0.014181,"ble with regular expressions and access to a stack. It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises 3 Programming Model The fundamental programming unit is a POSIX regular expression including repetition, line boundaries, and trailing context. For example, a[bc] matches “ab” and “ac”. 384 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 5.1 When an expression"
P15-2063,2005.mtsummit-papers.11,0,0.148508,"Missing"
P15-2063,D13-1023,0,0.391574,"1). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Yasuhara et al., 2013) is an efficient trie-based representation using double arrays while KenLM (Heafield, 2011) has traditional tries and a linear probing hash table. We use the fastest baselines from both. Introduction Larger data sizes and more detailed models have led to adoption of specialized hardware for natural language processing. Graphics processing units (GPUs) are the most common, with applications to neural networks (Oh and Jung, 2004) and parsing (Johnson, 2011). Field-programmable gate arrays (FPGAs) are faster and more customizable, so grammars can be encoded in gates (Ciressan et al., 2000). In th"
P15-2063,W11-2921,0,0.0262758,"as a stack. One core is 2.4 times as fast at language identification and 1.8 to 6 times as fast at part-of-speech language modeling. 1 2 Related Work Automata have been emulated on CPUs with AT&T FSM (Mohri et al., 2000) and OpenFST (Allauzen et al., 2007), on GPUs (Rudom´ın et al., 2005; He et al., 2015), and on FPGAs (Sidhu and Prasanna, 2001; Lin et al., 2006; Korenek, 2010). These are candidates for the ASIC we use. In particular, gappy pattern matching (He et al., 2015) maps directly to regular expressions. GPUs have recently been applied to the related problem of parsing (Johnson, 2011; Yi et al., 2011). These operate largely by turning a sparse parsing problem into a highly-parallel dense problem (Canny et al., 2013) and by clustering similar workloads (Hall et al., 2014). Since the hardware used in this paper is a deterministic pushdown automaton, parsing ambiguous natural language is theoretically impossible without using the CPU as an oracle. Hall et al. (2014) rely on communication between the CPU and GPU, albeit for efficiency reasons rather than out of necessity. Work on efficiently querying backoff language models (Katz, 1987) has diverged from a finite state representation. DALM (Ya"
P15-2063,I11-1062,0,0.0345138,"1 ) i=1 b(wi This preserves sentence-level probabilities.1 Because the hardware lacks user-accessible arithmetic, terms are sent to the CPU. Sending just q for each token instead of p and various backoffs b reduces communication and CPU workload. We also benefit from a simplified query procedure: for each word, match as much context as possible then return the corresponding value q. Language Identification We exactly replicate the model of langid.py (Lui and Baldwin, 2012) to identify 97 languages. Their Na¨ıve Bayes model has 7,480 features fi , each of which is a string of up to four bytes (Lui and Baldwin, 2011). Inference amounts to collecting the count ci of each feature and computing the most likely language l given model p. Y l∗ = argmax p(l) p(fi |l)ci l 5.2 Greedy Matching Language models are greedy in the sense that, for every word, they match as much leading context as possible. We map this onto greedy regular expressions, which match as much trailing context as possible, by reversing the input and n–grams.2 Unlike language identification, we run the hardware in a greedy mode that scans until a match is found, reports the longest such match, and resumes scanning afterwards. The trailing conte"
P15-2063,P12-3005,0,0.257118,"It is therefore a deterministic pushdown transducer. Prior work used the hardware mostly as intended, by scanning hard drive contents against a small set of patterns for digital forensics purposes (Lee et al., 2008). The purposes of this paper are to introduce the natural language processing community to the hardware and evaluate performance. We chose the related tasks of language identification and language modeling because they do not easily map to regular expressions. Fast language classification is essential to using the web as a corpus (Smith et al., 2013) and packages compete on speed (Lui and Baldwin, 2012). Extensive literature on fast language models comprises 3 Programming Model The fundamental programming unit is a POSIX regular expression including repetition, line boundaries, and trailing context. For example, a[bc] matches “ab” and “ac”. 384 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 384–389, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 5.1 When an expression matches, the hardware can output a constant to the CPU"
P15-2063,Q15-1007,0,\N,Missing
P15-2063,P14-1020,0,\N,Missing
P15-2063,U11-1006,0,\N,Missing
P16-1083,N15-1027,0,0.0197045,"rpolated model that sums to more than one. An example is shown in Table 1. Instead of building separate models then weighting, Zhang and Chiang (2014) show how to train Kneser-Ney models (Kneser and Ney, 1995) on weighted data. Their work relied on prescriptive weights from domain adaptation techniques rather than tuning weights, as we do here. Our exact normalization approach relies on the backoff structure of component models. Several approximations support general models: ignoring normalization (Chen et al., 1998), noisecontrastive estimation (Vaswani et al., 2013), and self-normalization (Andreas and Klein, 2015). In future work, we plan to exploit the structure of other features in high-quality unnormalized loglinear language models (Sethy et al., 2014). Ignoring normalization is particularly common in speech recognition and machine translation. This is one of our baselines. Unnormalized models can also be compiled into a single model by multiplying the weighted probabilities and backoffs.1 Many use unnormalized models because weights can be jointly tuned along with other feature weights. However, Haddow (2013) showed that linear interpolation weights can be jointly tuned by pairwise ranked optimizat"
P16-1083,W13-2201,0,0.0325787,"rmalization cost would introduce bias. 882 6 Experiments We perform experiments for perplexity, query speed, memory consumption, and effectiveness in a machine translation system. Individual language models were trained on English corpora from the WMT 2016 news translation shared task (Bojar et al., 2016). This includes the seven newswires (afp, apw, cna, ltw, nyt, wpb, xin) from English Gigaword Fifth Edition (Parker et al., 2011); the 2007–2015 news crawls;4 News discussion; News commmentary v11; English from Europarl v8 (Koehn, 2005); the English side of the French-English parallel corpus (Bojar et al., 2013); and the English side of SETIMES2 (Tiedemann, 2009). We additionally built one language model trained on the concatenation of all of the above corpora. All corpora were preprocessed using the standard Moses (Koehn et al., 2007) scripts to perform normalization, tokenization, and truecasing. To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data. All language models are 5-gram backoff language models trained with modified Kneser-Ney smoothing (Chen and Goodman, 1998) using lmplz (Heafield et al., 201"
P16-1083,P07-2045,0,0.00716874,"sh corpora from the WMT 2016 news translation shared task (Bojar et al., 2016). This includes the seven newswires (afp, apw, cna, ltw, nyt, wpb, xin) from English Gigaword Fifth Edition (Parker et al., 2011); the 2007–2015 news crawls;4 News discussion; News commmentary v11; English from Europarl v8 (Koehn, 2005); the English side of the French-English parallel corpus (Bojar et al., 2013); and the English side of SETIMES2 (Tiedemann, 2009). We additionally built one language model trained on the concatenation of all of the above corpora. All corpora were preprocessed using the standard Moses (Koehn et al., 2007) scripts to perform normalization, tokenization, and truecasing. To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data. All language models are 5-gram backoff language models trained with modified Kneser-Ney smoothing (Chen and Goodman, 1998) using lmplz (Heafield et al., 2013). Also to prevent SRILM from running out of RAM, we pruned singleton trigrams and above. For linear interpolation, we tuned weights using IRSTLM. To work around SRILM’s limitation of ten models, we interpolated the first ten t"
P16-1083,2005.mtsummit-papers.11,0,0.0586856,"also considered minibatches, though grouping tuning data to reduce normalization cost would introduce bias. 882 6 Experiments We perform experiments for perplexity, query speed, memory consumption, and effectiveness in a machine translation system. Individual language models were trained on English corpora from the WMT 2016 news translation shared task (Bojar et al., 2016). This includes the seven newswires (afp, apw, cna, ltw, nyt, wpb, xin) from English Gigaword Fifth Edition (Parker et al., 2011); the 2007–2015 news crawls;4 News discussion; News commmentary v11; English from Europarl v8 (Koehn, 2005); the English side of the French-English parallel corpus (Bojar et al., 2013); and the English side of SETIMES2 (Tiedemann, 2009). We additionally built one language model trained on the concatenation of all of the above corpora. All corpora were preprocessed using the standard Moses (Koehn et al., 2007) scripts to perform normalization, tokenization, and truecasing. To prevent SRILM from running out of RAM, we excluded the large monolingual CommonCrawl data, but included English from the parallel CommonCrawl data. All language models are 5-gram backoff language models trained with modified Kn"
P16-1083,N13-1035,0,0.0143055,"8), noisecontrastive estimation (Vaswani et al., 2013), and self-normalization (Andreas and Klein, 2015). In future work, we plan to exploit the structure of other features in high-quality unnormalized loglinear language models (Sethy et al., 2014). Ignoring normalization is particularly common in speech recognition and machine translation. This is one of our baselines. Unnormalized models can also be compiled into a single model by multiplying the weighted probabilities and backoffs.1 Many use unnormalized models because weights can be jointly tuned along with other feature weights. However, Haddow (2013) showed that linear interpolation weights can be jointly tuned by pairwise ranked optimization (Hopkins and May, 2011). In theory, normalized log-linear interpolation weights can be jointly tuned in the same way. &lt;unk> A B Sum p1 0.4 0.6 1 p2 0.2 0.8 1 pL 0.3 0.4 0.6 1.3 Zero 0.3 0.3 0.4 1 Table 1: Linearly interpolating two models p1 and p2 with equal weight yields an unnormalized model pL . If gaps are filled with zeros instead, the model is normalized. To work around this problem, SRILM (Stolcke, 2002) uses zero probability instead of the unknown word probability for new words. This produce"
P16-1083,P13-2121,1,0.844066,"Missing"
P16-1083,W11-2123,1,0.849416,"on TED (72.58 perplexity versus 75.91 for offline linear interpolation). However, it performs worse on news. In future work, we plan to investigate whether log-linear wins when all corpora are outof-domain since it favors agreement by all models. Table 6 compares the speed and memory performance of the competing methods. While the log-linear tuning is much slower, its compilation is faster compared to the offline linear model’s long run time. Since the model formats are the same for the concatenation and log-linear, they share the fastest query speeds. Query speed was measured using KenLM’s (Heafield, 2011) faster probing data structure.5 6.2 BLEU 18.40 18.02 18.00 18.27 18.15 BLEU-c 17.91 17.55 17.53 17.82 17.70 Table 7: Machine translation performance comparison in an end-to-end system. jointly tuned normalized log-linear interpolation to future work. 7 Conclusion Normalized log-linear interpolation is now a tractable alternative to linear interpolation for backoff language models. Contrary to Hsu (2007), we proved that these models can be exactly collapsed into a single backoff language model. This solves the query speed problem. Empirically, compiling the log-linear model is faster than SRIL"
P16-1083,D11-1125,0,0.0346957,"future work, we plan to exploit the structure of other features in high-quality unnormalized loglinear language models (Sethy et al., 2014). Ignoring normalization is particularly common in speech recognition and machine translation. This is one of our baselines. Unnormalized models can also be compiled into a single model by multiplying the weighted probabilities and backoffs.1 Many use unnormalized models because weights can be jointly tuned along with other feature weights. However, Haddow (2013) showed that linear interpolation weights can be jointly tuned by pairwise ranked optimization (Hopkins and May, 2011). In theory, normalized log-linear interpolation weights can be jointly tuned in the same way. &lt;unk> A B Sum p1 0.4 0.6 1 p2 0.2 0.8 1 pL 0.3 0.4 0.6 1.3 Zero 0.3 0.3 0.4 1 Table 1: Linearly interpolating two models p1 and p2 with equal weight yields an unnormalized model pL . If gaps are filled with zeros instead, the model is normalized. To work around this problem, SRILM (Stolcke, 2002) uses zero probability instead of the unknown word probability for new words. This produces a model that sums to one, but differs from what users might expect. IRSTLM (Federico et al., 2008) asks the user to"
P16-1083,P07-1065,0,0.0392789,"λi pi (x |w1n )λi + 4.2 x6∈s(w1n ) i x∈s(w1n ) i The first term agrees with the claim, so we focus on the case where x 6∈ s(w1n ). By definition of s, all models back off. X Y pi (x |w1n )λi x6∈s(w1n ) i = X Y x6∈s(w1n ) i  = pi (x |w2n )λi bi (w1n )λi X Y x6∈s(w1n ) i  = Z(w2n ) −  pi (x |w2n )λi  Y bi (w1n )λi i X Y x∈s(w1n ) i  Y pi (x |w2n )λi bi (w1n )λi i Streaming Computation Part of the point of offline interpolation is that there may not be enough RAM to fit all the component models. Moreover, with compression techniques that rely on immutable models (Whittaker and Raj, 2001; Talbot and Osborne, 2007), a mutable version of the combined model may not fit in RAM. Instead, we construct the offline model with disk-based streaming algorithms, using the framework we designed for language model estimation (Heafield et al., 2013). Our pipeline (Figure 1) has four conceptual steps: merge probabilities, apply backoffs, normalize, and output. Applying backoffs and normalization are performed in the same pass, so there are three total passes. 4.2.1 Merge Probabilities This step takes the union of n–grams and multiplies probabilities from component models. We This is the second term of the claim. 880 a"
P16-1083,D13-1140,0,0.0313695,"bability to these new words, leading to an interpolated model that sums to more than one. An example is shown in Table 1. Instead of building separate models then weighting, Zhang and Chiang (2014) show how to train Kneser-Ney models (Kneser and Ney, 1995) on weighted data. Their work relied on prescriptive weights from domain adaptation techniques rather than tuning weights, as we do here. Our exact normalization approach relies on the backoff structure of component models. Several approximations support general models: ignoring normalization (Chen et al., 1998), noisecontrastive estimation (Vaswani et al., 2013), and self-normalization (Andreas and Klein, 2015). In future work, we plan to exploit the structure of other features in high-quality unnormalized loglinear language models (Sethy et al., 2014). Ignoring normalization is particularly common in speech recognition and machine translation. This is one of our baselines. Unnormalized models can also be compiled into a single model by multiplying the weighted probabilities and backoffs.1 Many use unnormalized models because weights can be jointly tuned along with other feature weights. However, Haddow (2013) showed that linear interpolation weights"
P16-1083,P14-1072,0,0.0216395,"polation: normalization when component models have different vocabularies and offline interpolation. 3.1 Vocabulary Differences Language models are normalized with respect to their vocabulary, including the unknown word. X p1 (x) = 1 x∈vocab(p1 ) Related Work If two models have different vocabularies, then the combined vocabulary is larger and the sum is taken over more words. Component models assign their unknown word probability to these new words, leading to an interpolated model that sums to more than one. An example is shown in Table 1. Instead of building separate models then weighting, Zhang and Chiang (2014) show how to train Kneser-Ney models (Kneser and Ney, 1995) on weighted data. Their work relied on prescriptive weights from domain adaptation techniques rather than tuning weights, as we do here. Our exact normalization approach relies on the backoff structure of component models. Several approximations support general models: ignoring normalization (Chen et al., 1998), noisecontrastive estimation (Vaswani et al., 2013), and self-normalization (Andreas and Klein, 2015). In future work, we plan to exploit the structure of other features in high-quality unnormalized loglinear language models (S"
P18-4020,P07-2045,1,0.0306875,"Missing"
P18-4020,E17-2025,0,0.0357645,"83.9 73.0 67.9 54.8 46.6 35.3 40 23.5 20 12.4 60 0 100 Deep RNN 80 60 40 20 7.8 42.5 33.437.1 28.2 22.8 17.2 13.1 0 100 Transformer 80 54.9 49.0 42.9 37.6 30.4 23.4 16.4 60 40 20 0 9.1 1 2 3 4 5 6 7 8 Number of GPUs Figure 1: Training speed in thousands of source tokens per second for shallow RNN, deep RNN and Transformer model. Dashed line projects linear scale-up based on single-GPU performance. ory to maximize speed and memory usage. This guarantees that a chosen memory budget will not be exceeded during training. All models use tied embeddings between source, target and output embeddings (Press and Wolf, 2017). Contrary to Sennrich et al. (2017a) or Vaswani et al. (2017), we do not average checkpoints, but maintain a continuously updated exponentially averaged model over the entire training run. Following Vaswani et al. (2017), the learning rate is set to 0.0003 and decayed as the inverse square root of the number of updates after 16,000 updates. When training the transformer model, a linearly growing learning rate is used during the first 16,000 iterations, starting with 0 until the base learning rate is reached. 118 W a¨ Si hl e e n ei n Ta en s be tatu - rfe h ss la im tz M e fe nu¨ s . tleg e E"
P18-4020,W17-4774,1,0.824066,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,E17-3017,1,0.852816,"Missing"
P18-4020,I17-1013,1,0.851844,"s. Scaling is not linear (dashed lines), but close. The tokens-per-second rate (w/s) for Nematus on the same data on a single GPU is about 2800 w/s for the shallow model. Nematus does not have multiGPU training. Marian achieves about 4 times faster training on a single GPU and about 30 times faster training on 8 GPUs for identical models. Translation speed. The back-translation of 10M sentences with a shallow model takes about four State-of-the-art in Neural Automatic Post-Editing In our submission to the Automatic Post-Editing shared task at WMT-2017 (Bojar et al., 2017b) and follow-up work (Junczys-Dowmunt and Grundkiewicz, 2017a,b), we explore multiple neural architectures adapted for the task of automatic postediting of machine translation output as implementations in Marian. We focus on neural end-to-end models that combine both inputs mt (raw MT output) and src (source language input) in a single neural architecture, modeling {mt, src} → pe directly, where pe is post-edited corrected output. These models are based on multi-source neural translation models introduced by Zoph and Knight (2016). Furthermore, we investigate the effect of hard-attention models or neural transductors (Aharoni and Goldberg, 2016) which"
P18-4020,P16-1162,1,0.624087,"work, we implemented many efficient meta- ple scripts at https://github.com/marian-nmt/ algorithms. These include multi-device (GPU or marian-examples. 117 test2017 UEdin WMT17 (single) +Ensemble of 4 +R2L Reranking 33.9 35.1 36.2 27.5 28.3 28.3 Deep RNN (single) +Ensemble of 4 +R2L Reranking 34.3 35.3 35.9 27.7 28.2 28.7 Transformer (single) +Ensemble of 4 +R2L Reranking 35.6 36.4 36.8 28.8 29.4 29.5 Source tokens per second ×103 test2016 Source tokens per second ×103 System • preprocessing of training data, tokenization, true-casing4 , vocabulary reduction to 36,000 joint BPE subword units (Sennrich et al., 2016) with a separate tool.5 • training of a shallow model for backtranslation on parallel WMT17 data; • translation of 10M German monolingual news sentences to English; concatenation of artificial training corpus with original data (times two) to produce new training data; • training of four left-to-right (L2R) deep models (either RNN-based or Transformer-based); • training of four additional deep models with right-to-left (R2L) orientation; 6 • ensemble-decoding with four L2R models resulting in an n-best list of 12 hypotheses per input sentence; • rescoring of n-best list with four R2L models, a"
P18-4020,N18-1055,1,0.879647,"Missing"
P18-4020,P17-4012,0,0.0756084,"lconquers-patent-translation-in-majorwipo-roll-out/ Marian has minimal dependencies (only Boost and CUDA or a BLAS library) and enables barrierfree optimization at all levels: meta-algorithms such as MPI-based multi-node training, efficient batched beam search, compact implementations of new models, custom operators, and custom GPU kernels. Intel has contributed and is optimizing a CPU backend. Marian grew out of a C++ re-implementation of Nematus (Sennrich et al., 2017b), and still maintains binary-compatibility for common models. Hence, we will compare speed mostly against Nematus. OpenNMT (Klein et al., 2017), perhaps one of the most popular toolkits, has been reported to have training speed competitive to Nematus. Marian is distributed under the MIT license and available from https://marian-nmt. github.io or the GitHub repository https: //github.com/marian-nmt/marian. 2 Design Outline We will very briefly discuss the design of Marian. Technical details of the implementations will be provided in later work. 2.1 Custom Auto-Differentiation Engine The deep-learning back-end included in Marian is based on reverse-mode auto-differentiation with dynamic computation graphs and among the established mach"
W09-0408,W05-0909,1,0.278739,"h this constraint disallows. Alignments indicate where words are synchronous. Words near an alignment are also likely to be synchronous even without an explicit alignment. For example, in the fragments “even more serious, you” and “even worse, you” from WMT 2008, “serious” and “worse” do not align but do share relative position from other alignments, suggesting these are synchronous. We formalize this by measuring the relative position of frontiers from alignments on each side. For example, Alignment Sentences from different systems are aligned in pairs using a modified version of the METEOR (Banerjee and Lavie, 2005) matcher. This identifies alignments in three phases: exact matches up to case, WordNet (Fellbaum, 1998) morphology matches, and shared WordNet synsets. These sources of alignments are quite precise and unable to pick up on looser matches such as “mentioned” and “said” that legitimately appear in output from different systems. Artificial alignments are intended to fill gaps by using surrounding alignments as clues. If a word is not aligned to any word in some other sentence, we search left and right for words that are aligned into that sentence. If these alignments are sufficiently close to ea"
W09-0408,P05-3026,1,0.872605,"nd unable to pick up on looser matches such as “mentioned” and “said” that legitimately appear in output from different systems. Artificial alignments are intended to fill gaps by using surrounding alignments as clues. If a word is not aligned to any word in some other sentence, we search left and right for words that are aligned into that sentence. If these alignments are sufficiently close to each other in the other sentence, words between them are considered for artificial alignment. An artificial alignment is added if a matching part of speech is found. The algorithm is described fully by Jayaraman and Lavie (2005). 2.2 Synchronization Phrases Switching between systems is permitted outside phrases or at phrase boundaries. We find phrases in two ways. Alignment phrases are maximally 57 hypotheses are detected on insertion and packed, with the combined hypothesis given the highest score of those packed. Once a beam contains the top scoring partial hypotheses of length l, these hypotheses are extended to length l + 1 and placed in another beam. Those hypotheses reaching end of sentence are placed in a separate beam, which is equivalent to packing them into one final hypothesis. Once we remove partial hypot"
W09-0408,P08-2021,0,0.12206,"ent of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 1 Introduction Many systems for machine translation, with different underlying approaches, are of competitive quality. Nonetheless these approaches and systems have different strengths and weaknesses. By offsetting weaknesses with strengths of other systems, combination can produce higher quality than does any component system. One approach to system combination uses confusion networks (Rosti et al., 2008; Karakos et al., 2008). In the most common form, a skeleton sentence is chosen from among the one-best system outputs. This skeleton determines the ordering of the final combined sentence. The remaining outputs are aligned with the skeleton, producing a list of alternatives for each word in the skeleton, which comprises a confusion network. A decoder chooses from the original skeleton word and its alternatives to produce a final output sentence. While there are a number of variations on this theme, our approach differs fundamentally in that the effective skeleton changes on a per-phrase basis. 2 System The system c"
W09-0408,W07-0734,1,0.765805,"version 1.04 of IBM-style BLEU (Papineni et al., 2002) in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (Snover et al., 2006). For each source language, we exThe N -Gram and Overlap features are intended to improve fluency across phrase boundaries. Features are combined using a log-linear model trained as discussed in Section 3. Hypotheses are scored using the geometric average score of each word in the hypothesis. 2.5 Tuning Search Of note is that a word’s score is impacted only by its alignments and the n-gram found by the language model. Therefore two partial hypotheses that differ only in words preceding the n-gram and in their average score are in some sense dup"
W09-0408,P02-1040,0,0.0827738,"monolingual and French-English data provided by the contest. order−ngram N -Gram 31 using language model order and length of ngram found. overlap Overlap order−1 where overlap is the length of intersection between the preceding and current n-grams. 3 Given the 502 sentences made available for tuning by WMT 2009, we selected feature weights for scoring, a set of systems to combine, confidence in each selected system, and the type and distance s of synchronization. Of these, only feature weights can be trained, for which we used minimum error rate training with version 1.04 of IBM-style BLEU (Papineni et al., 2002) in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (S"
W09-0408,W08-0329,0,0.159798,"data showed improvement of 2 BLEU and 1 METEOR point over the best HungarianEnglish system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task. 1 Introduction Many systems for machine translation, with different underlying approaches, are of competitive quality. Nonetheless these approaches and systems have different strengths and weaknesses. By offsetting weaknesses with strengths of other systems, combination can produce higher quality than does any component system. One approach to system combination uses confusion networks (Rosti et al., 2008; Karakos et al., 2008). In the most common form, a skeleton sentence is chosen from among the one-best system outputs. This skeleton determines the ordering of the final combined sentence. The remaining outputs are aligned with the skeleton, producing a list of alternatives for each word in the skeleton, which comprises a confusion network. A decoder chooses from the original skeleton word and its alternatives to produce a final output sentence. While there are a number of variations on this theme, our approach differs fundamentally in that the effective skeleton changes on a per-phrase basis"
W09-0408,2006.amta-papers.25,0,0.0373435,") in case-insensitive mode. We treated the remaining parameters as a model selection problem, using 402 randomly sampled sentences for training and 100 sentences for evaluation. This is clearly a small sample on which to evaluate, so we performed two folds of crossvalidation to obtain average scores over 200 untrained sentences. We chose to do only two folds due to limited computational time and a desire to test many models. We scored systems and our own output using case-insensitive IBM-style BLEU 1.04 (Papineni et al., 2002), METEOR 0.6 (Lavie and Agarwal, 2007) with all modules, and TER 5 (Snover et al., 2006). For each source language, we exThe N -Gram and Overlap features are intended to improve fluency across phrase boundaries. Features are combined using a log-linear model trained as discussed in Section 3. Hypotheses are scored using the geometric average score of each word in the hypothesis. 2.5 Tuning Search Of note is that a word’s score is impacted only by its alignments and the n-gram found by the language model. Therefore two partial hypotheses that differ only in words preceding the n-gram and in their average score are in some sense duplicates. With the same set of used words and same"
W09-0408,2005.eamt-1.20,1,\N,Missing
W10-1744,P05-1074,0,0.379455,"which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using"
W10-1744,W08-0309,0,0.0907409,"Missing"
W10-1744,P02-1040,0,0.085232,"ecoded with different weights than the second sentence. The amount of random perturbation decreases linearly each iteration until the 10th and subsequent iterations whose learned weights are not perturbed. We emphasize that the point is to introduce randomness in sentences decoded during MERT, and therefore considered during parameter tuning, and not on the specific formula presented in this system description. In practice, this technique increases the number of iterations and decreases the difference in tuning scores following MERT. In our experiments, weights are tuned towards uncased BLEU (Papineni et al., 2002) or the combined metric TERBLEU (Snover et al., 2006). 6.2 Hyperparameters In total, we tried 1167 hyperparameter configurations, limited by CPU time during the evaluation period. For each of these configurations, the feature weights were fully trained with MERT and scored on the same tuning set, which we used to select the submitted combinations. Because these configurations represent a small fraction of the hyperparameter space, we focused on values that work well based on prior experience and tuning scores as they became available: Parameter Optimization Feature Weights Feature weights are"
W10-1744,N10-1031,1,0.818569,"use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using a battery of features. Improvements this year include unigram paraphrase alignment, support for all target languages, new features, language modeling without pruning, and more parameter optimization. This paper describes our scheme with emphasis on improved areas. 3 Alignment System outputs are aligned at the token level using a variant of the METEOR (Denkowski and Lavie, 2010) aligner. This identifies, in decreasing order of priority: exact, stem,"
W10-1744,W08-0329,0,0.0387352,"ction 5.2 details our training data and backoff features. less systems are able to vote on a word order decision mediated by the bigram and trigram features. We find that both versions have their advantages, and therefore include two sets of match features: one that counts only exact alignments and another that counts all alignments. We also tried copies of the match features at the stem and synonym level but found these impose additional tuning cost with no measurable improvement in quality. Since systems have different strengths and weaknesses, we avoid assigning a single system confidence (Rosti et al., 2008) or counting n-gram matches with uniform system confidence (Hildebrand and Vogel, 2009). The weight on match feature ms,n corresponds to our confidence in ngrams from system s. These weights are fully tunable. However, there is another hyperparameter: the maximum length of n-gram considered; we typically use 2 or 3 with little gain seen above this. Features are combined into a score using a linear model. Equivalently, the score is the dot product of a weight vector with the vector of our feature values. The weight vector is a parameter optimized in Section 6. 5.1 Match Features The n-gram matc"
W10-1744,W09-0408,1,0.899763,"ely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candidate combinations by forming left-to-right paths through the aligned system outputs, and scoring these candidates using a battery of features. Improvements this year include unigram paraphrase alignment, support for all target languages, new features, language modeling without pruning, and more parameter optimization. This paper describes our scheme with emphasis on improved areas. 3 Alignment System outputs are aligned at the to"
W10-1744,2006.amta-papers.25,0,0.15525,"Work Confusion networks (Rosti et al., 2008) are the most popular form of system combination. In this approach, a single system output acts as a backbone to which the other outputs are aligned. This backbone determines word order while other outputs vote for substitution, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translatio"
W10-1744,W09-0406,0,0.188752,"to vote on a word order decision mediated by the bigram and trigram features. We find that both versions have their advantages, and therefore include two sets of match features: one that counts only exact alignments and another that counts all alignments. We also tried copies of the match features at the stem and synonym level but found these impose additional tuning cost with no measurable improvement in quality. Since systems have different strengths and weaknesses, we avoid assigning a single system confidence (Rosti et al., 2008) or counting n-gram matches with uniform system confidence (Hildebrand and Vogel, 2009). The weight on match feature ms,n corresponds to our confidence in ngrams from system s. These weights are fully tunable. However, there is another hyperparameter: the maximum length of n-gram considered; we typically use 2 or 3 with little gain seen above this. Features are combined into a score using a linear model. Equivalently, the score is the dot product of a weight vector with the vector of our feature values. The weight vector is a parameter optimized in Section 6. 5.1 Match Features The n-gram match features reward agreement between the candidate combination and underlying system out"
W10-1744,P08-2021,0,0.0456918,"i et al., 2008) are the most popular form of system combination. In this approach, a single system output acts as a backbone to which the other outputs are aligned. This backbone determines word order while other outputs vote for substitution, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-co"
W10-1744,2005.mtsummit-papers.11,0,0.02645,"6. 5.1 Match Features The n-gram match features reward agreement between the candidate combination and underlying system outputs. For example, feature m1,1 counts tokens in the candidate that also appear in system 1’s output for the sentence being combined. Feature m1,2 counts bigrams appearing in both the candidate and the translation suggested by system 1. Figure 2 shows example feature values. 5.2 System 1: Supported Proposal of France We built language models for each of the five target languages with the aim of using all constrained data. For each language, we used the provided Europarl (Koehn, 2005) except for Czech, News Commentary, and News monolingual corpora. In addition, we used: System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Language Model Trigram 1 1 ˇ Czech CzEng (Bojar and Zabokrtsk´ y, 2009) sections 0–7 English Gigaword Fourth Edition (Parker et al., 2009), Giga-FrEn, and CzEng (Bojar and ˇ Zabokrtsk´ y, 2009) sections 0–7 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. French Gigaword Second Edition"
W10-1744,W09-0407,0,0.020231,"ion, deletion, and insertion operations. Essentially, the backbone is edited to produce a combined output which largely preserves word order. Our approach differs in that we allow paths to switch between sentences, effectively permitting the backbone to switch at every word. Other system combination techniques typically use TER (Snover et al., 2006) or ITGs (Karakos et al., 2008) to align system outputs, meaning they depend solely on positional information to find approximate matches; we explicitly use stem, synonym, and paraphrase data to find alignments. Our use of paraphrases is similar to Leusch et al. (2009), though they learn a monolingual phrase table while we apply cross-lingual pivoting (Bannard and Callison-Burch, 2005). Introduction System combination merges the output of several machine translation systems into a single improved output. Our system combination scheme, submitted to the Workshop on Statistical Machine Translation (WMT) 2010 as cmu-heafield-combo, is an improvement over our previous system (Heafield et al., 2009), called cmu-combo in WMT 2009. The scheme consists of aligning 1-best outputs from each system using the METEOR (Denkowski and Lavie, 2010) aligner, identifying candi"
W10-1744,P03-1021,0,\N,Missing
W11-2117,P05-1074,0,0.0452899,"is year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV penalty. Language models often have poor OOV estimates for translation becaus"
W11-2117,W10-1751,1,0.928007,"to switch after each word. Interestingly, BBN (Rosti et al., 2010) this year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV"
W11-2117,P10-4002,0,0.0325963,"e concerned with combining translations into German that may be segmented differently. These can be due to stylistic choices; for example both “jahrzehnte lang” and “jahrzehntelang” appear with approximately equal frequency as shown in Table 1. Translation systems add additional biases due to the various preprocessing approaches taken by individual sites and inherent biases in models such as word alignment. In order to properly align differently segmented words, we normalize by segmenting all system outputs and our language model training data using 148 the single-best segmentation from cdec (Dyer et al., 2010). Running our system therefore produces segmented German output. Internally, we tuned towards segmented references but for final output it is desirable to rejoin compound words. Since the cdec segmentation was designed for GermanEnglish translation, no corresponding desegmenter was provided. We created a German desegmenter in the natural way: segment German words then invert the mapping to identify words that should be rejoined. To do so, we ran every word from the German monolingual data and system outputs through the cdec segmenter, counted both the compounded and segmented versions in the m"
W11-2117,W10-1744,1,0.851764,"otone. This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translation’s system combination task. We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks. 1 Twice that produced by nuclear plants Introduction We participated in all ten tracks of the 2011 Workshop on Machine Translation system combination task as cmu-heafield-combo. This uses a system combination scheme that builds on our prior work (Heafield and Lavie, 2010), especially with respect to language modeling and handling nonEnglish languages. We present a summary of the system, describe improvements, list the data used (all of the constrained monolingual data), and present automatic results in anticipation of human evaluation by the workshop. 2 Alignment Our Combination Scheme Given single-best outputs from each system, the scheme aligns system outputs then searches a space based on these alignments. The scheme is a continuation of our previous system (Heafield and Lavie, 2010) so we describe unchanged parts of the system in less detail, preferring in"
W11-2117,W11-2123,1,0.694484,"an important role here because frequent anonymization markers such as “[firstname]” do not appear in the large language model. To scale to larger language models, we use 147 BigFatLM1 , an open-source builder of large unpruned models with modified Kneser-Ney smoothing. Then, we filter the models to the system outputs. In order for an n-gram to be queried, all of the words must appear in system outputs for the same sentence. This enables a filtering constraint stronger than normal vocabulary filtering, which permits ngrams supported only by words in different sentences. Finally, we use KenLM (Heafield, 2011) for inference at runtime. Our primary use of data is for language modeling. We used essientially every constrained resource available and appended them together to build one large model. For every language, we used the provided Europarl v6 (Koehn, 2005), News Crawl, and News Commentary corpora. In addition, we used: English Gigaword Fourth Edition (Parker et al., 2009) and the English parts of United Nations documents, Giga-FrEn, and CzEng (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–7. For the Haitian Creole-English tasks, we built a separate language model on the SMS messages and used it alo"
W11-2117,W09-0406,0,0.0266442,"inexact matches collect more votes that better handle word order, we use both sets of features. However, 146 the limit N may be different i.e. Ne = 2 counts exact matches up to length 2 and Na = 3 counts inexact matches up to length 3. System 1: Supported Proposal of France System 2: Support for the Proposal of France Candidate: Support for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we"
W11-2117,P07-2045,0,0.00915827,"Missing"
W11-2117,2005.mtsummit-papers.11,0,0.299678,"of the system outputs. This is the complement of our bigram match count features (and, since, we have a length feature, the same up to rearranging weights). However, they threshold it to indicate whether the bigram appears at all instead of how many systems support the bigram. 4 Resources The resources we use are constrained to those provided for the shared task. For the paraphrase matches described in Section 2.1, METEOR (Denkowski and Lavie, 2010) trains its paraphrase tables via pivoting (Bannard and Callison-Burch, 2005). The phrase tables are trained using parallel data from Europarl v6 (Koehn, 2005) (fr-en, es-en, de-en, and es-de), news commentary (fr-en, es-en, de-en, and cz-en), United Nations (fr-en and es-en), and CzEng (cz-en) (Bojar ˇ and Zabokrtsk´ y, 2009) sections 0–8. 4.1 Language Modeling As with previous versions of the system, we use language model log probability as a feature to bias translations towards fluency. We add a second feature per language model that counts OOVs, allowing MERT to independently tune the OOV penalty. Language models often have poor OOV estimates for translation because they come not from new text in the same language but from new text in a differen"
W11-2117,W10-1746,0,0.0200391,"4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of one system; by contrast, ours can piece together"
W11-2117,P02-1040,0,0.0824806,"ave that a score of n2 . The total score is a sum of these squares, favoring compounds that cover more words. Maximizing the score is a fast and exact dynamic programming algorithm. Casing of unchanged words comes from equally-weighted system votes at the character level while casing of rejoined words is based on the majority appearance in the corpus; this is almost always initial capital. We ran our desegmenter followed by the workshop’s provided detokenizer to produce the submitted output. 5 Results We tried many variations on the scheme, such as selecting different systems, tuning to BLEU (Papineni et al., 2002) or METEOR (Denkowski and Lavie, 2010), and changing the structure of the match count features from Section 2.3. To try these, we ran MERT 242 times, or about 24 times for each of the ten tasks in which we participated. Then we selected the best performing systems on the tuning set and submitted them, with the secondary system chosen to meaningfully differ from the primary while still scoring well. Once the evaluation released references, we scored against them to generate Table 2. On the featured Haitian Creole task, we show no and sometimes even negative improvement. This we attribute to the"
W11-2117,W10-1748,0,0.0204424,"e approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of one system; by contrast, ours can piece together word orders taken from multiple systems. In a loose sense, our approach is a confusion network where the backbone is permitted to switch after each word. Interestingly, BBN (Rosti et al., 2010) this year added a novel-bigram penalty that penalizes bigrams in the output if they do not appear in one of the system outputs. This is th"
W11-2117,2006.amta-papers.25,0,0.0543806,"upport for Proposal of France System 1 System 2 Unigram 4 5 Bigram 2 3 Trigram 1 1 Figure 2: Example match feature values with two systems and matches up to length three. Here, “Supported” counts because it aligns with “Support”. 3 Related Work Hypothesis selection (Hildebrand and Vogel, 2009) selects an entire sentence at a time instead of picking and merging words. This makes the approach less flexible, in that it cannot synthesize new sentences, but also less risky by avoiding matching and related problems entirely. While our alignment is based on METEOR, other techniques are based on TER (Snover et al., 2006), Inversion Transduction Grammars (Narsale, 2010), and other alignment methods. These use exact alignments and positional information to infer alignments, ignoring the content-based method used by METEOR. This means they might align content words to function words, while we never do. In practice, using both signals would likely work better. Confusion networks (Rosti et al., 2010; Narsale, 2010) are the dominant method for system combination. These base their word order on one system, dubbed the backbone, and have all systems vote on editing the backbone. Word order is largely fixed to that of"
W11-2123,P10-4002,0,0.128308,"ession techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies. 2 Data Structures We implement two data structures: P ROBING, designed for speed, and T RIE, optimized for memory. The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties. An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little me"
W11-2123,W06-3113,0,0.0249179,"ct the invalid probability, using the node only if it leads to a longer match. By contrast, BerkeleyLM’s hash and compressed variants will return incorrect results based on an n − 1-gram. 2.2.1 Quantization Floating point values may be stored in the trie exactly, using 31 bits for non-positive log probability and 32 bits for backoff5 . To conserve memory at the expense of accuracy, values may be quantized using q bits per probability and r bits per backoff6 . We allow any number of bits from 2 to 25, unlike IRSTLM (8 bits) and BerkeleyLM (17 − 20 bits). To quantize, we use the binning method (Federico and Bertoldi, 2006) that sorts values, divides into equally sized bins, and averages within each bin. The cost of storing these averages, in bits, is [32(N − 1)2q + 32(N − 2)2r Because there are comparatively few unigrams, we elected to store them byte-aligned and unquantized, making every query faster. Unigrams also have 64-bit overhead for vocabulary lookup. Using cn to denote the number of n-grams, total memory consumption of T RIE, in bits, is (32 + 32 + 64 + 64)c1 + N −1 X (dlog2 c1 e + q + r + dlog2 cn+1 e)cn + n=2 (dlog2 c1 e + q)cN plus quantization tables, if used. The size of T RIE is particularly sens"
W11-2123,W09-1505,0,0.105891,"coders. IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption. MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization. Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems w"
W11-2123,D10-1026,0,0.382073,"ongest matching 1 fY −1 SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption. MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization. Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the M"
W11-2123,P07-2045,0,0.302611,"re several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies. 2 Data Structures We implement two data structures: P ROBING, designed for speed, and T RIE, optimized for memory. The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties. An important subproblem of language model storage is therefore sparse mapping: storing values for s"
W11-2123,2005.mtsummit-papers.11,0,0.141413,"Missing"
W11-2123,W09-0424,0,0.0640508,"elease code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics speed and offer lower memory consumption than lossless alternatives. Performance improvements transfer to the Moses (Koehn et al., 2007), cdec (Dyer et al., 2010), and Joshua (Li et al., 2009) translation systems where our code has been integrated. Our open-source (LGPL) implementation is also available for download as a standalone package with minimal (POSIX and g++) dependencies. 2 Data Structures We implement two data structures: P ROBING, designed for speed, and T RIE, optimized for memory. The set of n-grams appearing in a model is sparse, and we want to efficiently find their associated probabilities and backoff penalties. An important subproblem of language model storage is therefore sparse mapping: storing values for sparse keys using little memory then retrieving values gi"
W11-2123,P02-1040,0,0.11168,"obabilities and backoffs. It also does not prune, so comparing to our pruned model would be unfair. Using RandLM and the documented 1 settings (8-bit values and 256 false-positive probability), we built a stupid backoff model on the same data as in Section 5.2. We used this data to build an unpruned ARPA file with IRSTLM’s Time (m) Pack Variant T RIE 82.9 Ken T RIE 8 bits 82.7 T RIE 4 bits 83.2 Stupid 8 bits 218.7 Rand Backoff 8 bits 337.4 RAM (GB) Res Virt 12.16 14.39 8.41 9.41 7.74 8.55 5.07 5.18 7.17 7.28 BLEU 27.24 27.22 27.09 25.54 25.45 Table 4: CPU time, memory usage, and uncased BLEU (Papineni et al., 2002) score for single-threaded Moses translating the same test set. We ran each lossy model twice: once with specially-tuned weights and once with weights tuned using an exact model. The difference in BLEU was minor and we report the better result. improved-kneser-ney option and the default three pieces. Table 4 shows the results. We elected run Moses single-threaded to minimize the impact of RandLM’s cache on memory use. RandLM is the clear winner in RAM utilization, but is also slower and lower quality. However, the point of RandLM is to scale to even larger data, compensating for this loss in q"
W11-2123,P11-1027,0,0.381968,"p(wn |w1n−1 ) where w1n is an n-gram. Backoff-smoothed models estimate this probability based on the observed entry with longest matching 1 fY −1 SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption. MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization. Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 187–197, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computationa"
W11-2123,P07-1065,0,0.0540446,"queries. This paper presents methods to query N -gram language models, minimizing time and space costs. Queries take the form p(wn |w1n−1 ) where w1n is an n-gram. Backoff-smoothed models estimate this probability based on the observed entry with longest matching 1 fY −1 SRILM 1.5.12 (Stolcke, 2002) is a popular toolkit based on tries used in several decoders. IRSTLM 5.60.02 (Federico et al., 2008) is a sorted trie implementation designed for lower memory consumption. MITLM 0.4 (Hsu and Glass, 2008) is mostly designed for accurate model estimation, but can also compute perplexity. RandLM 0.2 (Talbot and Osborne, 2007) stores large-scale models in less memory using randomized data structures. BerkeleyLM revision 152 (Pauls and Klein, 2011) implements tries based on hash tables and sorted arrays in Java with lossy quantization. Sheffield Guthrie and Hepple (2010) explore several randomized compression techniques, but did not release code. TPT Germann et al. (2009) describe tries with better locality properties, but did not release code. These packages are further described in Section 3. We substantially outperform all of them on query http://kheafield.com/code/kenlm 187 Proceedings of the 6th Workshop on Sta"
W13-2212,D07-1090,0,0.0383023,"4.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33 +.33 18.02 +.07 20.26 +.20 29.36 +.60 30.39 +.36 34.44 +.57 30.10 +.44 16.16 +.35 18.62 +.27 24.05 +.30 18.84 +.40 Constrained Unconstrained ∆ fr-en 31.46 32.24 +.78 es-en 30.59 31.37 +.78 cs-en 27.38 28.16 +.78 24.33 25.14 +.81 ru-en Table 18: Gain on newstest2013 from the unconstrained language model. Our time on shared machines with 1 TB is limited so Russian–English was run after the deadline and German–English"
W13-2212,W12-3102,1,0.0720198,"ut we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Ins"
W13-2212,N12-1047,0,0.0890278,"hese systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics apart with many more than a couple of dozen features. Instead, we used k-best MIRA (Cherry and Foster, 2012). For the different language pairs, we saw improvements in BLEU of -.05 to +.39, with an average of +.09. There was only a minimal change in the length ratio (Table 1) de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg MERT 22.11 (1.010) 30.00 (1.023) 30.42 (1.021) 25.54 (1.022) 16.08 (0.995) 29.26 (0.980) 31.92 (0.985) 17.38 (0.967) – k-best MIRA 22.10 (1.008) 30.11 (1.026) 30.63 (1.020) 25.49 (1.024) 16.04 (1.001) 29.65 (0.982) 31.95 (0.985) 17.42 (0.974) – The lexical features were restricted to the 50 most frequent words. All these features together only gave minor improvements (Table 3)."
W13-2212,N09-1025,0,0.025899,"f domain features: Translation Table Smoothing with Kneser-Ney Discounting de-en fr-en es-en cs-en en-de en-fr en-es en-cs avg sparse 22.02 30.24 30.61 25.49 15.93 29.81 32.02 17.28 – Table 3: Sparse features Table 1: Tuning with k-best MIRA instead of MERT (cased BLEU scores with length ratio) 1.3 baseline 22.10 30.11 30.63 25.49 16.04 29.65 31.95 17.42 – Sparse Features A significant extension of the Moses system over the last couple of years was the support for large numbers of sparse features. This year, we tested this capability on our big WMT systems. First, we used features proposed by Chiang et al. (2009): #d base. 2 22.10 4 30.11 3 30.63 9 25.49 2 16.122 4 29.65 3 31.95 9 17.42 – indicator 22.14 +.04 30.34 +.23 30.88 +.25 25.58 +.09 16.14 +.02 29.75 +.10 32.06 +.11 17.45 +.03 +.11 ratio 22.07 –.03 30.29 +.18 30.64 +.01 25.58 +.09 15.96 –.16 29.71 +.05 32.13 +.18 17.35 –.07 +.03 subset 22.12 +.02 30.15 +.04 30.82 +.19 25.46 –.03 16.01 –.11 29.70 +.05 32.02 +.07 17.44 +.02 +.03 Table 4: Sparse domain features When combining the domain features and the other sparse features, we see roughly additive gains (Table 5). We use the domain indicator feature and the other sparse features in subsequent e"
W13-2212,P05-1066,1,0.569778,"Missing"
W13-2212,P11-1105,1,0.298607,"us (translation and reordering) decisions spanning across phrasal boundaries thus overcoming the problematic phrasal independence assumption in the phrase-based model. In the OSM model, the reordering decisions influence lexical selection and vice versa. Lexical generation is strongly coupled with reordering thus improving the overall reordering mechanism. We used the modified version of the OSM model (Durrani et al., 2013b) that additionally handles discontinuous and unaligned target MTUs3 . We borrow 4 count-based supportive features, the Gap, Open Gap, Gap-width and Deletion penalties from Durrani et al. (2011). Inst. Wt (scale) – 33.98 ±.00 23.13 –.06 31.62 –.05 28.63 –.04 34.03 +.03 15.89 +.11 23.72 –.06 Table 14: Comparison of MML filtering and weighting with baseline. The MML uses monolingual news as in-domain, and selects from all training data after alignment.The weighting uses the MML weights, optionally downscaled by 10, then exponentiated. Baselines are as Table 13. Training: During training, each bilingual sentence pair is deterministically converted to a unique sequence of operations. Please refer to Durrani et al. (2011) for a list of operations and the conversion algorithm and see Figur"
W13-2212,D10-1044,0,0.0468666,"Training with new data (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). T"
W13-2212,N13-1035,1,0.80094,"was found to be better. The modified interpolation was not possible in fr↔en as it uses to much RAM. The final experiment of the initial system development phase was to train the systems on the new data, adding newstest2011 to the tuning set (now 10,068 sentences). Table 12 reports the gains on newstest2012 due to added data, indicating very clearly that valuable new data resources became available this year. The results from the phrase-table interpolation are quite mixed, and we only used the technique 117 for the final system in en-es. An interpolation based on PRO has recently been shown (Haddow, 2013) to improve on perplexity minimisation is some cases, but the current implementation of this method is limited to 2 phrase-tables, so we did not use it in this evaluation. 2.2 Figure 1: Bilingual Sentence with Alignments sequence of operations (o1 , o2 , . . . , oJ ) and learn a Markov model over this sequence as: Modified Moore-Lewis Filtering posm (F, E, A) = p(oJ1 ) = In last year’s evaluation (Koehn and Haddow, 2012b) we had some success with modified Moore-Lewis filtering (Moore and Lewis, 2010; Axelrod et al., 2011) of the training data. This year we conducted experiments in most of the"
W13-2212,W12-3154,1,0.0809865,"ems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse features, we moved away from MERT which is known to fall 114 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114–121, c Sofia, Bulgaria, August 8-9, 2013"
W13-2212,W11-2123,1,0.674625,"do not go Generate Source Only (ja) Ich gehe ja ↓ nicht I do not go Jump Forward Ich gehe ja nicht ↓ I do not go Generate (zum, to the) . . . gehe ja nicht zum ↓ . . . not go to the Generate (haus, house) . . . ja nicht zum haus ↓ . . . go to the house Table 15: Step-wise Generation of Figure 1 LP newstest de-en fr-en es-en cs-en ru-en en-de en-fr en-es en-cs en-ru Baseline 2012 2013 23.85 26.54 30.77 31.09 34.02 30.04 22.70 25.70 31.87 24.00 17.95 20.06 28.76 30.03 33.87 29.66 15.81 18.35 23.75 18.44 The large language model was then quantized to 10 bits and compressed to 643 GB with KenLM (Heafield, 2011), loaded onto a machine with 1 TB RAM, and used as an additional feature in unconstrained French–English, Spanish–English, and Czech–English submissions. This additional language model is the only difference between our final constrained and unconstrained submissions; no additional parallel data was used. Results are shown in Table 18. Improvement from large language models is not a new result (Brants et al., 2007); the primary contribution is estimating on a single machine. +OSM 2012 2013 24.11 +.26 26.83 +.29 30.96 +.19 31.46 +.37 34.51 +.49 30.94 +.90 23.03 +.33 25.79 +.09 32.33 +.46 24.33"
W13-2212,P13-2121,1,0.843378,"reaks down the gains over the final system from Section 1 from using the operation sequence models (OSM), modified Moore-Lewis filtering (MML), fixing a bug with the sparse lexical features (Sparse-Lex Bugfix), and instance weighting (Instance Wt.), translation model combination (TM-Combine), and use of the huge language model (ClueWeb09 LM). Huge Language Models To overcome the memory limitations of SRILM, we implemented modified Kneser-Ney (Kneser and Ney, 1995; Chen and Goodman, 1998) smoothing from scratch using disk-based streaming algorithms. This open-source4 tool is described fully by Heafield et al. (2013). We used it to estimate an unpruned 5–gram language model on web pages from ClueWeb09.5 The corpus was preprocessed by removing spam (Cormack et al., 2011), selecting English documents, splitting sentences, deduplicating, tokenizing, and truecasing. Estimation on the remaining 126 billion tokens took 2.8 days on a single machine with 140 GB RAM (of which 123 GB was used at peak) and six hard drives in a RAID5 configuration. Statistics about the resulting model are shown in Table 17. 4 5 Summary Acknowledgments Thanks to Miles Osborne for preprocessing the ClueWeb09 corpus. The research leadin"
W13-2212,D07-1103,0,0.0605594,"Missing"
W13-2212,2012.amta-papers.9,1,0.770604,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,W12-3139,1,0.0637938,"anguage model trained on 126 billion tokens with a new training tool (Section 4). 1 1.1 Factored Backoff (German–English) We have consistently used factored models in past WMT systems for the German–English language pairs to include POS and morphological target sequence models. But we did not use the factored decomposition of translation options into multiple mapping steps, since this usually lead to much slower systems with usually worse results. A good place, however, for factored decomposition is the handling of rare and unknown source words which have more frequent morphological variants (Koehn and Haddow, 2012a). Here, we used only factored backoff for unknown words, giving gains in BLEU of +.12 for German–English. Initial System Development We start with systems (Haddow and Koehn, 2012) that we developed for the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The notable features of these systems are: • Moses phrase-based models with mostly default settings • training on all available parallel data, including the large UN parallel data, the FrenchEnglish 109 parallel data and the LDC Gigaword data 1.2 Tuning with k-best MIRA In preparation for training with sparse f"
W13-2212,D07-1091,1,0.809959,"Missing"
W13-2212,E03-1076,1,0.320668,"Missing"
W13-2212,2012.iwslt-papers.7,0,0.0521392,"Missing"
W13-2212,P10-2041,0,0.0792206,"Missing"
W13-2212,2001.mtsummit-papers.68,0,0.0249874,"e model, and domain adaptation techniques. We also report on utilizing a huge language model trained on 126 billion tokens. Note that while our final 2012 systems included subsampling of training data with modified Moore-Lewis filtering (Axelrod et al., 2011), we did not use such filtering at the starting point of our development. We will report on such filtering in Section 2. Moreover, our system development initially used the WMT 2012 data condition, since it took place throughout 2012, and we switched to WMT 2013 training data at a later stage. In this section, we report cased BLEU scores (Papineni et al., 2001) on newstest2011. The annual machine translation evaluation campaign for European languages organized around the ACL Workshop on Statistical Machine Translation offers the opportunity to test recent advancements in machine translation in large data condition across several diverse language pairs. Building on our own developments and external contributions to the Moses open source toolkit, we carried out extensive experiments that, by early indications, led to a strong showing in the evaluation campaign. We would like to stress especially two contributions: the use of the new operation sequence"
W13-2212,E12-1055,0,0.0108725,"ta (newstest2012 scores) 2 We adopted a number of changes that improved our baseline system by an average of +.30, see Table 10 for a breakdown. Domain Adaptation Techniques We explored two additional domain adaptation techniques: phrase table interpolation and modified Moore-Lewis filtering. method factored backoff kbest MIRA sparse features and domain indicator tuning with 25 iterations maximum phrase length 5 unpruned 4-gram LM translation table limit 100 total 2.1 Phrase Table Interpolation We experimented with phrase-table interpolation using perplexity minimisation (Foster et al., 2010; Sennrich, 2012). In particular, we used the implementation released with Sennrich (2012) and available in Moses, comparing both the naive and modified interpolation methods from that paper. For each language pair, we took the alignments created from all the data concatenated, built separate phrase tables from each of the individual corpora, and interpolated using each method. The results are shown in Table 13 Table 10: Summary of impact of changes Minor improvements that we did not adopt was avoiding reducing maximum phrase length to 5 (average +.03) and tuning with 1000-best lists (+.02). The improvements d"
W13-2212,D11-1033,0,\N,Missing
W13-2212,P02-1040,0,\N,Missing
W13-2212,P13-2071,1,\N,Missing
W13-2212,N13-1001,1,\N,Missing
W14-3309,D11-1033,0,0.150733,"Missing"
W14-3309,2013.iwslt-evaluation.3,1,0.854043,"8 10.39 20.85 19.39 30.82 19.67 10.52 +0.25 +0.55 +0.09 +0.89 +0.13 27.44 26.42 31.64 24.45 15.48 27.34 26.42 31.76 24.63 15.26 ∆ -0.10 ±0.00 +0.12 +0.18 -0.22 Table 1: Using Word Clusters in Phrase-based and OSM models – B0 = System without Clusters, +Cid = with Cluster We also trained OSM models over POS and morph tags. For the English-to-German system we added an OSM model over [pos, morph] (source:pos, target:morph) and for the Germanto-English system we added an OSM model over [morph,pos] (source:morph, target:pos), a configuration that was found to work best in our previous experiments (Birch et al., 2013). Table 2 shows gains from additionally using OSM models over POS/morph tags. Lang B0 +OSMp,m ∆ en-de de-en 20.44 27.24 20.60 27.44 +0.16 +0.20 Unsupervised Transliteration Model Pair Training OOV B0 +Tr ∆ ru-en en-ru hi-en en-hi 232K 232K 38K 38K 1356 681 503 394 24.63 19.67 14.67 11.76 25.06 19.91 15.48 12.83 +0.41 +0.24 +0.81 +1.07 Table 3: Using Unsupervised Transliteration Model – Training = Extracted Transliteration Corpus (types), OOV = Out-of-vocabulary words (tokens) B0 = System without Transliteration, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pai"
W14-3309,D08-1023,0,0.0296702,".ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram mod"
W14-3309,W13-2201,1,0.872446,"Missing"
W14-3309,buck-etal-2014-n,1,0.885994,"Missing"
W14-3309,E14-4029,1,0.915819,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,N09-1025,0,0.082017,"Missing"
W14-3309,P11-1105,1,0.910095,"Missing"
W14-3309,P05-1066,1,0.793483,"limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the tr"
W14-3309,P13-2071,1,0.866776,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,W14-3310,1,0.89194,"Missing"
W14-3309,W13-2212,1,0.838117,"s for WMT-14 Nadir Durrani Barry Haddow Philipp Koehn School of Informatics University of Edinburgh {dnadir,bhaddow,pkoehn}@inf.ed.ac.uk Kenneth Heafield Computer Science Department Stanford University heafield@cs.stanford.edu Abstract Our baseline systems are based on the setup described in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish langua"
W14-3309,D08-1089,0,0.557258,"d models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interp"
W14-3309,2014.eamt-1.17,1,0.831092,"The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and triangulated phrase-tables are presented in Durrani and Koehn (2014). Using our best Urdu-to-Hindi system, we translated the Urdu part of the multi-indic corpus to form HindiEnglish parallel data. Table 4 shows results from using the synthesized Hindi-English corpus in isolation (Syn) and on top of the baseline system (B0 + Syn). et al., 2002). But we found this system to be useless for translating the Urdu part of Indic data due to domain mismatch and huge number of OOV words (approximately 310K tokens). To reduce sparsity we synthesized additional phrase-tables using interpolation and transliteration. Interpolation: We trained two phrase translation tables p"
W14-3309,W11-2123,1,0.79041,"ntations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model"
W14-3309,C14-1041,1,0.92685,"tatistical Machine Translation (WMT). We participated in all language pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a l"
W14-3309,P13-2121,1,0.908258,"Missing"
W14-3309,P07-1019,0,0.118146,"sed for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems w"
W14-3309,E99-1010,0,0.288751,"pairs. We have improved upon our 2013 system by i) using generalized representations, specifically automatic word clusters for translations out of English, ii) using unsupervised character-based models to translate unknown words in RussianEnglish and Hindi-English pairs, iii) synthesizing Hindi data from closely-related Urdu data, and iv) building huge language on the common crawl corpus. 1 Translation Task Baseline Using Generalized Word Representations We explored the use of automatic word clusters in phrase-based models (Durrani et al., 2014a). We computed the clusters with GIZA++’s mkcls (Och, 1999) on the source and target side of the parallel training corpus. Clusters are word classes that are optimized to reduce n-gram perplexity. By generating a cluster identifier for each output word, we are able to add an n-gram model We trained our systems with the following settings: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, hierarchical lexicalized reordering (Galley and Manning, 2008), a lexicallydriven 5-gram operation sequence model (OSM) 97 Proc"
W14-3309,2005.mtsummit-papers.11,1,0.0781164,"Language Models Our unconstrained submissions use an additional language model trained on web pages from the 2012, 2013, and winter 2013 CommonCrawl.2 The additional language model is the only difference between the constrained and unconstrained submissions; we did not use additional parallel data. These language models were trained on text provided by the CommonCrawl foundation, which they converted to UTF-8 after stripping HTML. Languages were detected using the Compact Language Detection 23 and, except for Hindi where we lack tools, sentences were split with the Europarl sentence splitter (Koehn, 2005). All text was then deduplicated, minimizing the impact of boilerplate, such as social media sharing buttons. We then tokenized and truecased the text as usual. Statistics are shown in Table 5. A full description of the pipeline, including a public data release, appears in Buck et al. (2014). Transliteration: Urdu and Hindi are written in different scripts (Arabic and Devanagri respectively). We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800"
W14-3309,W12-3152,0,0.0414194,"on, +Tr = Transliterating OOVs Table 3 shows the number (types) of transliteration pairs extracted using unsupervised mining, number of OOV words (tokens) in each pair and the gains achieved by transliterating unknown words. 1.4 Synthesizing Hindi Data from Urdu Hindi and Urdu are closely related language pairs that share grammatical structure and have a large overlap in vocabulary. This provides a strong motivation to transform any Urdu-English parallel data into Hindi-English by translating the Urdu part into Hindi. We made use of the Urdu-English segment of the Indic multi-parallel corpus (Post et al., 2012) which contains roughly 87K sentence pairs. The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment. We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE1 corpus (Baker Table 2: Using POS and Morph Tags in OSM models – B0 = Baseline, +OSMp,m = POS/Morph-based OSM 1 EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data. From these we were able to sentence align 7000 sentences to build an Urdu-to-Hindi system. 98 using transliteration and t"
W14-3309,W13-2228,1,0.84762,"ne, discontinuous, swap), and one that distinguishes the discontinuous orientations to the left and right. Table 8 shows slight improvements with these models, so we used them in our baseline. Russian-English: We tried to improve wordalignments by integrating a transliteration submodel into GIZA++ word aligner. The probability of a word pair is calculated as an interpolation of the transliteration probability and translation probability stored in the t-table of the different alignment models used by the GIZA++ aligner. This interpolation is done for all iterations of all alignment models (See Sajjad et al. (2013) for details). Due to shortage of time we could only run it for Russian-to-English. The improved alignments gave a gain of +0.21 on news-test 2013 and +0.40 on news-test 2014. Threshold filtering of phrase table: We experimented with discarding some phrase table entry due to their low probability. We found that phrase translations with the phrase translation probability 100 φ(f |e)&lt;10−4 can be safely discarded with almost no change in translations. However, discarding phrase translations with the inverse phrase translation probability φ(e|f )&lt;10−4 is more risky, especially with morphologically"
W14-3309,D07-1091,1,0.922736,"n the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pa"
W14-3309,I08-2089,1,0.83166,"edings of the Ninth Workshop on Statistical Machine Translation, pages 97–104, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics over these identifiers as an additional scoring function. The inclusion of such an additional factor is trivial given the factored model implementation (Koehn and Hoang, 2007) of Moses (Koehn et al., 2007). The n-gram model is trained in the similar way as the regular language model. We trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the tuning set (Schwenk and Koehn, 2008). We also trained OSM models over cluster-ids (?). The lexically driven OSM model falls back to very small context sizes of two to three operations due to data sparsity. Learning operation sequences over cluster-ids enables us to learn richer translation and reordering patterns that can generalize better in sparse data conditions. Table 1 shows gains from adding target LM and OSM models over cluster-ids. Using word clusters was found more useful translating from English-to-*. from English Lang de cs fr ru hi 1.3 Last year, our Russian-English systems performed badly on the human evaluation. In"
W14-3309,J82-2005,0,0.208115,"Missing"
W14-3309,N07-1061,0,0.119594,"Missing"
W14-3309,E03-1076,1,0.896383,"est translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokenizer for tokenizing Hindi. The systems were tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. We used newstest 2013 for the dev experiments. For RussianEnglish pairs news-test 2012 was used for tuning and for Hindi-English pairs, we divided the newsdev 2014 into two halves, used the first half for tuning and second for dev experiments. 1.1 1.2 This paper describes the University of Edinburgh’s (UEDIN) phrase-based submissions to the translation and medical translation shared tasks o"
W14-3309,P07-1108,0,0.0994085,"Missing"
W14-3309,N04-1022,0,0.220347,"d in (Durrani et al., 2013b) that we used for the Eighth Workshop on Statistical Machine Translation (Bojar et al., 2013). The notable features of these systems are described in the following section. The experiments that we carried out for this year’s translation task are described in the following sections. (Durrani et al., 2013a) with 4 count-based supportive features, sparse domain indicator, phrase length, and count bin features (Blunsom and Osborne, 2008; Chiang et al., 2009), a distortion limit of 6, maximum phrase-length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Pruning (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and the noreordering-over-punctuation heuristic (Koehn and Haddow, 2009). We used POS and morphological tags as additional factors in phrase translation models (Koehn and Hoang, 2007) for GermanEnglish language pairs. We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser-Ney smoothed 7-gram models. We used syntactic-preordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) for German-to-English systems. We used trivia tokeni"
W14-3309,P10-2041,0,0.127106,"Missing"
W14-3309,P12-2059,0,0.0266614,". We added a transliteration component to our Urdu-to-Hindi system. An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data. We were able to extract around 2800 transliteration pairs. To learn a richer transliteration model, we additionally fed the interpolated phrase-table, as described above, to the transliteration miner. We were able to mine additional 21000 transliteration pairs and built a Urdu-Hindi character-based model from it. The transliteration module can be used to translate the 50K OOV words but previous research (Durrani et al., 2010; Nakov and Tiedemann, 2012) has shown that transliteration is useful for more than just translating OOV words when translating closely related language pairs. To fully capitalize on the large overlap in Hindi–Urdu vocabulary, we transliterated each word in the Urdu test-data into Hindi and produced a phrase-table with 100-best transliterations. The two synthesized (triangulated and transliterated) phrase-tables are then used along with the baseline Urdu-to-Hindi phrase-table in a log-linear model. Detailed results on Urdu-toHindi baseline and improvements obtained from Lang en de fr ru cs hi Lines (B) Tokens (B) 59.13 3"
W14-3316,W11-2131,0,0.0272367,"est2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. https://code.google.com/p/cld2/ 151 2.5 Post-Processing Our post-processor recases and detokenizes system output. For the English-German system, we combined both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Target-class bigram boundary: An indicator feature for the concatenation of the word class of the rightmost word in the left rule and the word class of the leftmost word in the right rule in each adjacent rule pair in a deriva"
W14-3316,buck-etal-2014-n,1,0.877365,"Missing"
W14-3316,W13-2217,1,0.814481,"corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1 These authors contributed equally. http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our system"
W14-3316,J13-1009,1,0.889149,"Missing"
W14-3316,W14-3360,1,0.882744,"f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. h"
W14-3316,W14-3311,1,0.831976,"f the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the newstest2013 data set. h"
W14-3316,P13-2121,1,0.902738,"Missing"
W14-3316,W11-2123,1,0.748178,"entences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Giga-FrEn, CzEng and Yandex corpora using the same procedure as above. Additionally, we estimated a second language model from the English Gigaword corpus. All of these language models used interpolated modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1998). 2.3.2 Unconstrained Language Model Our unconstrained En-De submission used an additional language model trained on German web text gathered by the Common Crawl"
W14-3316,P07-2045,0,0.00563235,"e of the translation rule and an indicator feature for the concatenation of the two lengths. Rule orientation features: An indicator feature for each translation rule combined with its orientation class (monotone, swap, or discontinuous). This feature also fires only for rules that occur more than 50 times in the parallel data. Again, class-based translation rules were used to extract additional features. Translation System We built our translation systems using Phrasal. 3.1 Features Our translation model has 19 dense features that were computed for all translation hypotheses: the nine Moses (Koehn et al., 2007) baseline features, the eight hierarchical lexicalized reordering model features by Galley and Manning (2008), the log count of each rule, and an indicator for unique rules. On top of that, the model uses the following additional features of Green et al. (2014a). Rule indicator features: An indicator feature for each translation rule. To combat overfitting, this feature fires only for rules that occur more than 50 times in the parallel data. Additional indicator features were constructed by mapping the words in each rule to their corresponding word classes. Alignments: An indicator feature for"
W14-3316,2005.mtsummit-papers.11,0,0.0580071,"Missing"
W14-3316,N04-1022,0,0.145424,"Missing"
W14-3316,C12-1121,0,0.113259,"The signed linear distortion δ for two rules a and b is δ = r(a)−l(b)+1, where r(x) is the rightmost source index of rule x and l(x) is the leftmost source index of rule x. Each adjacent rule pair in a derivation has an indicator feature for the signed linear distortion of this pair. Tuning We used an online, adaptive tuning algorithm (Green et al., 2013c) to learn the feature weights. The loss function is an online variant of expected BLEU (Green et al., 2014a). As a sentence-level metric, we used the extended BLEU+1 metric that smooths the unigram precision as well as the reference length (Nakov et al., 2012). For feature selection, we used L1 regularization. Each tuning epoch produces a different set of weights; we tried all of them on newstest2013, which was held out from the tuning set, then picked the weights that produced the best uncased BLEU score. 3.3 System Parameters We started off with the parameters of our systems for the WMT 2013 Translation Task (Green et al., 2013a) and optimized the L1 -regularization strength. Both systems used the following tuning parameters: a 200-best list, a learning rate of 0.02 and a mini-batch size of 20. The En-De system 152 Track En-De constrained En-De u"
W14-3316,J04-4002,0,0.384373,"Missing"
W14-3316,P13-1135,0,0.025397,"Missing"
W14-3316,D13-1049,0,0.0417255,"Missing"
W14-3316,N06-1014,0,0.0853397,"re, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points and standardized white spaces and newlines. We additionally filtered out sentences longer than 100 tokens from the parallel corpora in order to speed up model learning. 2.2 Alignment For both systems, we used the Berkeley Aligner (Liang et al., 2006) with default settings to align the parallel data. We symmetrized the alignments using the grow-diag heuristic. 2.3 Language Models Our systems used up to three language models. 2.3.1 Constrained Language Models For En-De, we used lmplz (Heafield et al., 2013) to estimate a 5-gram language model on all WMT German monolingual data and the German side of the parallel Common Crawl corpus. To query the model, we used KenLM (Heafield, 2011). For the Fr-En system, we also estimated a 5-gram language model from all the monolingual English data and the English side of the parallel Common Crawl, UN, Gi"
W14-3316,P08-1086,0,0.0164417,"23 times as large as the rest of the German monolingual corpus. Since the test data was also collected from the web, we cannot be sure that the test sentences were not in the language model. However, substantial portions of the test set are translations from other languages, which were not posted online until after 2013. 2.3.3 Word-Class Language Model We also built a word-class language model for the En-De system. We trained 512 word classes on the constrained German data using the predictive one-sided class model of Whittaker and Woodland (2001) with the parallelized clustering algorithm of Uszkoreit and Brants (2008) by Green et al. (2014a). All tokens were mapped to their word class; infrequent tokens appearing fewer than 5 times were mapped to a special cluster for unknown tokens. Finally, we estimated a 7-gram language model on the mapped corpus with SRILM (Stolcke, 2002) using Witten-Bell smoothing (Bell et al., 1990). 2.4 Tuning and Test Data For development, we tuned our systems on all 13,573 sentences contained in the newstest20082012 data sets and tested on the 3,000 sentences of the newstest2013 data set. The final system weights were chosen among all tuning iterations using performance on the ne"
W14-3316,P03-1020,0,0.266933,"ned both tasks by using a Conditional Random Field (CRF) model (Lafferty et al., 2001) to learn transformations between the raw output characters and the post-processed versions. For each test dataset, we trained a separate model on 500,000 sentences selected using the Feature Decay Algorithm for bitext selection (Biçici and Yuret, 2011). Features used include the character type of the current and surrounding characters, the token type of the current and surrounding tokens, and the position of the character within its token. The English output was recased using a language model based recaser (Lita et al., 2003). The language model was trained on the English side of the Fr-En parallel data using lmplz. 3 Target-class bigram boundary: An indicator feature for the concatenation of the word class of the rightmost word in the left rule and the word class of the leftmost word in the right rule in each adjacent rule pair in a derivation. Length features: Indicator features for the length of the source side and for the length of the target side of the translation rule and an indicator feature for the concatenation of the two lengths. Rule orientation features: An indicator feature for each translation rule"
W14-3316,J93-2004,0,0.0455718,", CzEng, and parallel CommonCrawl corpora. For parallel CommonCrawl, we concatenated the English halves for various language pairs and then deduplicated at the sentence level. In addition, our unconstrained English-German system used German text extracted from the entire 2012, 2013, and winter 2013 CommonCrawl1 corpora by Buck et al. (2014). Tables 1 and 2 show the sizes of the preprocessed corpora of parallel text and monolingual text from which our systems were built. 2.1 Pre-Processing We used Stanford CoreNLP to tokenize the English and German data according to the Penn Treebank standard (Marcus et al., 1993). The French source data was tokenized similarly to the French Treebank 1 These authors contributed equally. http://commoncrawl.org 150 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150–156, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (Abeillé et al., 2003) using the Stanford French tokenizer (Green et al., 2013b). We also lowercased the data and removed any control characters. Further, we filtered out all lines that consisted mainly of punctuation marks, removed characters that are frequently used as bullet points a"
W14-3316,de-marneffe-etal-2006-generating,1,\N,Missing
W14-3316,D08-1089,1,\N,Missing
W14-3316,P13-1031,1,\N,Missing
W17-4715,W11-2123,1,0.709077,"5 35.7† EN→DE 2016 2017 33.3 26.6 33.3 26.3 DE→EN 2016 2017 40.1 33.8 40.2 34.0 Table 2: Translation performance in BLEU with and without copied monolingual data. Statistically significant differences are marked with † (p &lt; 0.01) and ‡ (p &lt; 0.05). 5.2 Translation Performance further experiment with the outputs for each system described in section 5.2. In particular, we want to examine whether these gains are simply due to the monolingual data improving the fluency of the NMT system. In order to evaluate the fluency of each system, we train 5-gram language models for each language using KenLM (Heafield, 2011). The models are trained on the full monolingual News Crawl 2015 and 2016 datasets. This data is preprocessed as described in section 5.1, except that no subword segmentation is used. We use these language models to measure perplexity on the outputs of the baseline systems (trained using parallel and back-translated data) and the + copied systems (trained using parallel, back-translated, and copied data). The language models are also queried on the reference translations for comparison. For all language pairs except EN↔RO, we concatenate newstest2016 and newstest2017 into a single dataset to f"
W17-4715,W15-3014,0,0.0146527,"Further work used the same encoder and decoder for multi-way translation (Johnson et al., 2016). We have repurposed the idea to inject monolingual text for low-resource NMT. Their work combined multiple translation directions (e.g. French→English, German→English, and English→German) into one system. Our work combines e.g. English→English and Turkish→English into one system for the purpose of improving Turkish→English quality. They used only parallel data; our goal is to inject monolingual data. Early work on incorporating monolingual data into NMT concentrated on target-side monolingual data. Jean et al. (2015) and Gulcehre et al. (2015) used a 5-gram language model and a recurrent neural network language model (RNNLM), respectively, to re-rank NMT outputs. Gulcehre et al. (2015) also integrated a pre-trained RNNLM into NMT by concatenating hidden states. Sennrich et al. (2016b) added monolingual target data directly to NMT using null source sentences and freezing encoder parameters while training with the monolingual data. Our method is similar, although instead of using a null source sentence, we use a copy of the target sentence and train the encoder parameters on the copied sentence. Sennrich et"
W17-4715,Q17-1024,0,0.0802834,"Missing"
W17-4715,D13-1176,0,0.0599108,"original parallel data to train their NMT system. Our method is orthogonal: it could improve the initial system or be used alongside the translated data in the final system. They also considered a multitask shared encoder setup where the monolingual source data is used in a sentence reordering task. More recent approaches have used both source and target monolingual data while simultaneously training source→target and target→source NMT systems. Cheng et al. (2016) accom3 Neural Machine Translation We evaluate our approach using sequenceto-sequence neural machine translation (Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) augmented with attention (Bahdanau et al., 2015). We briefly explain these models here. Neural machine translation is an end-to-end approach to machine translation that learns to directly model p(y |x) for a source-target sentence pair (x, y). The system consists of two recurrent neural networks (RNNs): the encoder and the decoder. In our experiments, the encoder is a bidirectional RNN with gated recurrent units (GRUs) that maps the source sentence into a vector representation. The decoder is an RNN language model conditioned on the source sentence. This is aug149 Lan"
W17-4715,P16-1185,0,0.104187,"by first translating the source data into the target language using an initial machine translation system and then using this translated data and the original parallel data to train their NMT system. Our method is orthogonal: it could improve the initial system or be used alongside the translated data in the final system. They also considered a multitask shared encoder setup where the monolingual source data is used in a sentence reordering task. More recent approaches have used both source and target monolingual data while simultaneously training source→target and target→source NMT systems. Cheng et al. (2016) accom3 Neural Machine Translation We evaluate our approach using sequenceto-sequence neural machine translation (Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) augmented with attention (Bahdanau et al., 2015). We briefly explain these models here. Neural machine translation is an end-to-end approach to machine translation that learns to directly model p(y |x) for a source-target sentence pair (x, y). The system consists of two recurrent neural networks (RNNs): the encoder and the decoder. In our experiments, the encoder is a bidirectional RNN with gated recurrent un"
W17-4715,P02-1040,0,0.11857,"erplexities for each system output and the reference. Interestingly, the perplexities for the baseline and the + copied systems are similar for all language pairs. In particular, improvements in BLEU (see Table 2) do not necessarily correlate to improvements in perplexity. This indicates that the gains from the + copied system may not solely be due to fluency. We evaluate our models compared to a baseline containing parallel and back-translated data on the newstest2016 (all language pairs) and newstest2017 (EN↔TR and EN↔DE) test sets. For each model, we report case-sensitive detokenized BLEU (Papineni et al., 2002) calculated using mteval-v13a.pl. The BLEU scores for each language pair and each system are shown in Table 2. The only difference between the baseline and the + copied systems is the addition of the copied corpus during training. Note that the copied and the back-translated corpora are created using identical monolingual data, which means that in the + copied system, each sentence from the monolingual corpus occurs twice in the training data (once as part of the copied corpus and once as part of the back-translated corpus). For EN↔TR and EN↔DE, we use about twice as much monolingual as parall"
W17-4715,D14-1179,0,0.00983755,"Missing"
W17-4715,W16-2323,0,0.198546,"English→German) into one system. Our work combines e.g. English→English and Turkish→English into one system for the purpose of improving Turkish→English quality. They used only parallel data; our goal is to inject monolingual data. Early work on incorporating monolingual data into NMT concentrated on target-side monolingual data. Jean et al. (2015) and Gulcehre et al. (2015) used a 5-gram language model and a recurrent neural network language model (RNNLM), respectively, to re-rank NMT outputs. Gulcehre et al. (2015) also integrated a pre-trained RNNLM into NMT by concatenating hidden states. Sennrich et al. (2016b) added monolingual target data directly to NMT using null source sentences and freezing encoder parameters while training with the monolingual data. Our method is similar, although instead of using a null source sentence, we use a copy of the target sentence and train the encoder parameters on the copied sentence. Sennrich et al. (2016b) also created synthetic parallel data by translating target-language monolingual text into the source language. To perform this process, dubbed back-translation, they first trained an initial target→source machine translation system on the available parallel"
W17-4715,P16-1009,0,0.543127,"English→German) into one system. Our work combines e.g. English→English and Turkish→English into one system for the purpose of improving Turkish→English quality. They used only parallel data; our goal is to inject monolingual data. Early work on incorporating monolingual data into NMT concentrated on target-side monolingual data. Jean et al. (2015) and Gulcehre et al. (2015) used a 5-gram language model and a recurrent neural network language model (RNNLM), respectively, to re-rank NMT outputs. Gulcehre et al. (2015) also integrated a pre-trained RNNLM into NMT by concatenating hidden states. Sennrich et al. (2016b) added monolingual target data directly to NMT using null source sentences and freezing encoder parameters while training with the monolingual data. Our method is similar, although instead of using a null source sentence, we use a copy of the target sentence and train the encoder parameters on the copied sentence. Sennrich et al. (2016b) also created synthetic parallel data by translating target-language monolingual text into the source language. To perform this process, dubbed back-translation, they first trained an initial target→source machine translation system on the available parallel"
W17-4715,P16-1162,0,0.647568,"English→German) into one system. Our work combines e.g. English→English and Turkish→English into one system for the purpose of improving Turkish→English quality. They used only parallel data; our goal is to inject monolingual data. Early work on incorporating monolingual data into NMT concentrated on target-side monolingual data. Jean et al. (2015) and Gulcehre et al. (2015) used a 5-gram language model and a recurrent neural network language model (RNNLM), respectively, to re-rank NMT outputs. Gulcehre et al. (2015) also integrated a pre-trained RNNLM into NMT by concatenating hidden states. Sennrich et al. (2016b) added monolingual target data directly to NMT using null source sentences and freezing encoder parameters while training with the monolingual data. Our method is similar, although instead of using a null source sentence, we use a copy of the target sentence and train the encoder parameters on the copied sentence. Sennrich et al. (2016b) also created synthetic parallel data by translating target-language monolingual text into the source language. To perform this process, dubbed back-translation, they first trained an initial target→source machine translation system on the available parallel"
W17-4715,D16-1160,0,0.211585,"from the target language to the source language. The resulting back-translated data was combined with the original parallel data and used to train the final source→target NMT system. Since this back-translation method outperforms previous methods that only train the decoder (Gulcehre et al., 2015; Sennrich et al., 2016b), we use it as our baseline. In addition, our method stacks with back-translation in both the target→source and source→target systems; we can use source text to improve the back-translations and target text to improve the final outputs. In the mirror image of back-translation, Zhang and Zong (2016) added source-side monolingual data to NMT by first translating the source data into the target language using an initial machine translation system and then using this translated data and the original parallel data to train their NMT system. Our method is orthogonal: it could improve the initial system or be used alongside the translated data in the final system. They also considered a multitask shared encoder setup where the monolingual source data is used in a sentence reordering task. More recent approaches have used both source and target monolingual data while simultaneously training sou"
W17-4715,E17-3017,1,\N,Missing
W17-4739,P10-2041,0,0.0320685,"Missing"
W17-4739,buck-etal-2014-n,1,0.80914,"Missing"
W17-4739,W17-4705,0,0.0149621,"Missing"
W17-4739,D14-1179,0,0.00518473,"Missing"
W17-4739,P16-1009,1,0.524474,"ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 1 Introduction We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.1 We submitted neural machine translation systems trained with Nematus (Sennrich et al., 2017). Our setup is based on techniques described in last year’s system description (Sennrich et al., 2016a), including the use of subword models (Sennrich et al., 1 2 Novelties Here we describe the main differences to last year’s systems. We provide trained models and training commands at http://data.statmt.org/wmt17_systems/ 389 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 389–399 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 Subword Segmentation stacked architecture. Implementations of both of these architectures are available in Nematus. For completeness, we here reproduce the description of the"
W17-4739,P16-1162,1,\N,Missing
W17-4739,E17-2025,0,\N,Missing
W18-2413,W16-2316,0,0.0208392,"r RNN inputs and states, which we adopt in our experiments. Following Sennrich et al. (2016a), we also dropout entire source and target words (characters in our case) with a given probability. Model ensembling Model ensembling leads to consistent improvements for NMT (Sutskever et al., 2014; Sennrich et al., 2016a; Denkowski and Neubig, 2017). An ensemble of independent models usually outperforms an ensemble of different model checkpoints from a single training run as it results in more diverse models in the ensemble (Sennrich et al., 2017a). As an alternative method for checkpoint ensembles, Junczys-Dowmunt et al. (2016) propose exponential smoothing of network parameters averaging them over the entire training. We combine both methods and build ensembles of independently trained models with exponentially smoothed parameters. Evaluation The quality of machine transliterations is evaluated with four automatic metrics in the shared task: word accuracy, mean F-score, mean reciprocal rank, and MAPref (Chen et al., 2018). As a main evaluation metric for our experiments we use word accuracy (Acc) on the top candidate: ( N 1 X 1 if ci,1 matches any of ri,j Acc = . N 0 otherwise i=1 The closer the value to 1.0, the m"
W18-2413,W16-2378,1,0.787171,"part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply well-established techniques from NMT to machine transliteration building a strong system that achieves state-of-the-art-results. The techniques we exploit include: 2 Shared task on named entity transliteration The NEWS 2018 shared task (Chen et al., 2018) continues the tradition from the previous tasks (Xiangyu Duan et al., 2016, 2015; Zhang et al., 2012) and focuses on transliteration of personal and place names from English or into English or in both directions. 2.1 Datasets Five different data"
W18-2413,P07-1081,0,0.0609034,"Missing"
W18-2413,N16-1012,0,0.0414769,"e translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply well-established techniques from NMT to machine transliteration building a strong system that achieves state-of-the-art-results. The techniques we exploit include: 2 Shared task on named entity transliteration The NEWS 2018 shared task (Chen et al., 2018) continues the tradition from the previous tasks (Xiangyu Duan et al., 2016, 2015; Zhang et al., 2012) and focuses on transliteration of personal and place names from English or into English or in both directions. 2.1 Datasets Five different datasets have been made available for use as the t"
W18-2413,W17-3203,0,0.0202042,"rticipate in. 2.2 3.1 Regularization Randomly dropping units from the neural network during training is an effective regularization method that prevents the model from overfitting (Srivastava et al., 2014). For RNN networks, Gal and Ghahramani (2016) proposed variational dropout over RNN inputs and states, which we adopt in our experiments. Following Sennrich et al. (2016a), we also dropout entire source and target words (characters in our case) with a given probability. Model ensembling Model ensembling leads to consistent improvements for NMT (Sutskever et al., 2014; Sennrich et al., 2016a; Denkowski and Neubig, 2017). An ensemble of independent models usually outperforms an ensemble of different model checkpoints from a single training run as it results in more diverse models in the ensemble (Sennrich et al., 2017a). As an alternative method for checkpoint ensembles, Junczys-Dowmunt et al. (2016) propose exponential smoothing of network parameters averaging them over the entire training. We combine both methods and build ensembles of independently trained models with exponentially smoothed parameters. Evaluation The quality of machine transliterations is evaluated with four automatic metrics in the shared"
W18-2413,E14-4029,0,0.058845,"ty Transliteration ranked first in several tracks. We describe the shared task in Section 2, including provided data sets and evaluation metrics. In Section 3, we present the model architecture and adopted NMT techniques. The experiment details are presented in Section 4, the results are reported in Section 5, and we conclude in Section 6. Introduction Transliteration of Named Entities (NEs) is defined as the phonetic translation of names across languages (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson"
W18-2413,E17-1083,0,0.0259666,"al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply well-established techniques from NMT to machine transliteration building a strong system that achieves state-of-the-art-results. The techniques we exploit include: 2 Shared task on named entity transliteration The NEWS 2018 shared task (Chen et al., 2018) continues the tradition from the previous tasks (Xiangyu Duan et al., 2016, 2015; Zhang et al., 2012) and focuses on transliteration of personal and place names from English or into English or in both directions. 2.1 Datasets Five different datasets have been made available for use as the training and development data. The data for"
W18-2413,W16-2711,0,0.0175135,"ion metrics. In Section 3, we present the model architecture and adopted NMT techniques. The experiment details are presented in Section 4, the results are reported in Section 5, and we conclude in Section 6. Introduction Transliteration of Named Entities (NEs) is defined as the phonetic translation of names across languages (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply well-established techniques from NMT to machine transliteration building a strong system that achieves stat"
W18-2413,W17-4710,0,0.172419,"nPe PeEn English-Persian Persian-English 13,386 15,677 1000 1000 1000 1000 EnCh ChEn EnVi English-Chinese Chinese-English English-Vietnamese 41,318 32,002 3,256 1000 1000 500 1000 1000 500 EnHi EnTa EnKa EnBa EnHe HeEn English-Hindi English-Tamil English-Kannada English-Bangla English-Hebrew Hebrew-English 12,937 10,957 10,955 13,623 10,501 9,447 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 model that consists of a bidirectional multi-layer encoder and decoder, both using GRUs as their RNN variants (Sennrich et al., 2017b). It utilizes the BiDeep architecture proposed by Miceli Barone et al. (2017), which combines deep transitions with stacked RNNs. We employ the soft-attention mechanism (Bahdanau et al., 2014), and leave hard monotonic attention models (Aharoni and Goldberg, 2017) for future work. Layer normalization (Ba et al., 2016) is applied to all recurrent and feed-forward layers, except for layers followed by a softmax. We use weight tying between target and output embeddings (Press and Wolf, 2017). The model operates on word level, and no special adaptation is made to the model architecture in order to support character-level transliteration, except data preprocessing (Section"
W18-2413,E17-2025,0,0.0375279,"that consists of a bidirectional multi-layer encoder and decoder, both using GRUs as their RNN variants (Sennrich et al., 2017b). It utilizes the BiDeep architecture proposed by Miceli Barone et al. (2017), which combines deep transitions with stacked RNNs. We employ the soft-attention mechanism (Bahdanau et al., 2014), and leave hard monotonic attention models (Aharoni and Goldberg, 2017) for future work. Layer normalization (Ba et al., 2016) is applied to all recurrent and feed-forward layers, except for layers followed by a softmax. We use weight tying between target and output embeddings (Press and Wolf, 2017). The model operates on word level, and no special adaptation is made to the model architecture in order to support character-level transliteration, except data preprocessing (Section 4.1). Table 1: Official data sets in NEWS 2018 which we use in our experiments. transliteration datasets (Haizhou et al., 2004), and the VNU-HCMUS dataset (Cao et al., 2010; Ngo et al., 2015), respectively. Hindi, Tamil, Kannada, Bangla (EnHi, EnTa, EnKa, EnBa), and Hebrew (EnHe, HeEn) are provided by Microsoft Research India2 . We do not evaluate our models on the dataset from the CJK Dictionary Institute as the"
W18-2413,N16-1042,0,0.0156548,"es (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply well-established techniques from NMT to machine transliteration building a strong system that achieves state-of-the-art-results. The techniques we exploit include: 2 Shared task on named entity transliteration The NEWS 2018 shared task (Chen et al., 2018) continues the tradition from the previous tasks (Xiangyu Duan et al., 2016, 2015; Zhang et al., 2012) and focuses on transliteration of personal and place names from English or int"
W18-2413,E17-3017,0,0.0547421,"Missing"
W18-2413,W16-2309,0,0.0275624,"ked first in several tracks. We describe the shared task in Section 2, including provided data sets and evaluation metrics. In Section 3, we present the model architecture and adopted NMT techniques. The experiment details are presented in Section 4, the results are reported in Section 5, and we conclude in Section 6. Introduction Transliteration of Named Entities (NEs) is defined as the phonetic translation of names across languages (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply"
W18-2413,P16-1009,0,0.461074,"ked first in several tracks. We describe the shared task in Section 2, including provided data sets and evaluation metrics. In Section 3, we present the model architecture and adopted NMT techniques. The experiment details are presented in Section 4, the results are reported in Section 5, and we conclude in Section 6. Introduction Transliteration of Named Entities (NEs) is defined as the phonetic translation of names across languages (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply"
W18-2413,P16-1162,0,0.705986,"ked first in several tracks. We describe the shared task in Section 2, including provided data sets and evaluation metrics. In Section 3, we present the model architecture and adopted NMT techniques. The experiment details are presented in Section 4, the results are reported in Section 5, and we conclude in Section 6. Introduction Transliteration of Named Entities (NEs) is defined as the phonetic translation of names across languages (Knight and Graehl, 1998). It is an important part of a number of natural language processing tasks, and machine translation in particular (Durrani et al., 2014; Sennrich et al., 2016c). Machine transliteration can be approached as a sequence-to-sequence modeling problem (Finch et al., 2016; Ameur et al., 2017). In this work, we explore the Neural Machine Translation (NMT) approach based on an attentional RNN encoderdecoder neural network architecture (Sutskever et al., 2014), motivated by its successful application to other sequence-to-sequence tasks, such as grammatical error correction (Yuan and Briscoe, 2016), automatic post-editing (Junczys-Dowmunt and Grundkiewicz, 2016), sentence summarization (Chopra et al., 2016), or paraphrasing (Mallinson et al., 2017). We apply"
W18-2714,W18-2701,0,0.0201548,"neural machine translation. We improve the performance with an efficient mini-batching algorithm, and by fusing the softmax operation with the k-best extraction algorithm. Submissions using Amun were first, second and third fastest in the GPU efficiency track. 1 2 Improvements We describe the main enhancements to Amun since the original 2016 publication that has improved translation speed. Introduction As neural machine translation (NMT) models have become the new state-of-the-art, the challenge is to make their deployment efficient and economical. This is the challenge that this shared task (Birch et al., 2018) is shining a spotlight on. One approach is to use an off-the-shelf deeplearning toolkit to complete the shared task where the novelty comes from selecting the appropriate models and tuning parameters within the toolkit for optimal performance. We take an opposing approach by eschewing model selection and parameter tuning in favour of efficient implementation. We use and enhanced a custom inference engine, Amun (JunczysDowmunt et al., 2016), which we developed on the premise that fast deep-learning inference is an issue that deserves dedicated tools that are not compromised by competing object"
W18-2714,P18-4020,1,0.858892,"d layer, trained with an extension of the Nematus (Sennrich et al., 2017) toolkit which supports such models; multiplicative-LSTM’s suitability for use in NMT models has been previously demonstrated by Pinnis et al. (2017). As with our first submission, Amun is used as inference engine. We trained 2 systems with differing vocabulary sizes and varied the beam sizes, and chose the configuration that produced the best results for translation quality on the validation set, Table 2. GRU-based system Our first submitted system uses gated recurred units (GRU) throughout. It was trained using Marian (Junczys-Dowmunt et al., 2018), but Amun was chosen as inference engine. We experimented with varying the vocabulary size and the RNN state size before settling for a vocabulary size of 30,000 (for both source and target language) and 256 for the state size, Table 1. After further experimentation, we decided to use sentence length normalization and NVidia’s 2 State dim 512 1024 12.77 17.16 18.19 19.52 19.17 19.64 Table 1: Validation set BLEU (newstest2014) for GRU-based model RNN in the encoder and a two-layer RNN in the decoder. We use byte pair encoding (Sennrich et al., 2016) to adjust the vocabulary size. We used a var"
W18-2714,W17-4737,0,0.0453884,"Missing"
W18-2714,E17-3017,0,0.0334806,"Missing"
W18-2714,P16-1162,0,0.380597,"+ bi end for end procedure We will compare the effect of the two implementations in the Section 4. 2.2 procedure S OFTMAX(vector p) . calculate max for softmax stability max ← −∞ for all pi in p do if pi > max then max ← pi end if end for . calculate denominator sum ← 0 for all pi in p do sum ← sum + exp(pi − max) end for . calculate softmax for all pi in p do i −max) pi ← exp(psum end for end procedure Softmax and K-Best Fusion Most NMT models predict a large number of classes in their output layer, corresponding to the number of words or subword units in their target language. For example, Sennrich et al. (2016) experimented with target vocabulary sizes of 60,000 and 90,000 sub-word units. The output layer of most deep learning models consist of the following steps 1. multiplication of the weight matrix with the input vector p = wx 2. addition of a bias term to the resulting scores p=p+b 3. applying the activation function, P most commonly softmax pi = exp(pi )/ exp(pi ) procedure F IND -B EST(vector p) max ← −∞ for all pi in p do if pi > max then max ← pi best ← i end if end for return max, best end procedure 4. a search for the best (or k-best) output classes argmaxi pi Figure 1 shows the amount of"
W18-2716,D17-1300,0,0.0989181,"Missing"
W18-2716,N13-1073,0,0.041288,"2, states size 1024). This model corresponds to the University of Edinburgh submission to WMT 2017 (Sennrich et al., 2017a). 4 Optimizing for the CPU Most of our effort was concentrated on improving CPU computation in Marian. Apart from improvements from code profiling and bottleneck identification, we worked towards integrating integer-based matrix products into Marian’s computation graphs. 4.1 Shortlist A simple way to improve CPU-bound NMT efficiency is to restrict the final output matrix multiplication to a small subset of translation candidates. We use a shortlist created with fastalign (Dyer et al., 2013). For every mini-batch we restrict the output 130 vocabulary to the union of the 100 most frequent target words and the 100 most probable translations for every source word in a batch. All CPU results are computed with a shortlist. 4.2 Model Quantization and integer products Previously, Marian tensors would only work with 32-bit floating point numbers. We now support tensors with underlying types corresponding to the standard numerical types in C++. We focus on integer tensors. Some of our submissions replaced 32-bit floating-point matrix multiplication with 16-bit or 8-bit signed integers. Fo"
W18-2716,E17-3017,1,0.706471,"mprovements. The GPU-bound University of Edinburgh 10 Crichton Street Edinburgh, Scotland, EU computations in Marian are already highly optimized and we mostly concentrate on modeling aspects and beam-search hyper-parameters. The weak baselines (at 16.9 BLEU on newstest2014 at least 12 BLEU points below the stateof-the-art) could promote approaches that happily sacrifice quality for speed. We choose a quality cut-off of around 26 BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transf"
W18-2716,P18-4020,1,0.877611,"Missing"
W18-2716,D16-1139,0,0.267486,"BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source, target, and output embeddings. One model is trained until convergence for eight days on four P40 GPUs. See tables 3 and 4 for BLEU scores of an overview of BLEU scores for models trained in this work. 1 See the shared ta"
W18-2716,P16-1162,0,0.119163,"., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source, target, and output embeddings. One model is trained until convergence for eight days on four P40 GPUs. See tables 3 and 4 for BLEU scores of an overview of BLEU scores for models trained in this work. 1 See the shared task description: https://sites. google.com/site/wnmt18/shared-task 2 https://marian-nmt.github.io 3 We added smaller post-submission systems to demonstrate that our approach outperforms systems by other participants when we take part in the race to the quality bottom. 4 https://github.com/awslabs/sockeye/ tree/wnmt18/wnmt18 129 Proceedin"
W18-2716,P18-1166,0,0.172105,"er-parameters. The weak baselines (at 16.9 BLEU on newstest2014 at least 12 BLEU points below the stateof-the-art) could promote approaches that happily sacrifice quality for speed. We choose a quality cut-off of around 26 BLEU for the first test set (newstest2014) and do not spend much time on systems below that threshold.3 This threshold was chosen based on the semi-official Sockeye (Hieber et al., 2017) baseline (27.6 BLEU on newstest2014) referenced on the shared task page.4 We believe our CPU implementation of the Transformer model (Vaswani et al., 2017) and attention averaging networks (Zhang et al., 2018) to be the fastest reported so far. This is achieved by integer matrix multiplication with auto-tuning. We also show that these models respond very well to sequence-level knowledge-distillation methods (Kim and Rush, 2016). 2 Teacher-student training 2.1 State-of-the-art teacher Based on Kim and Rush (2016), we first build four strong teacher models following the procedure for the Transformer-big model (model size 1024, filter size 4096, file size 813 MiB) from Vaswani et al. (2017) for ensembling. We use 36,000 BPE joint subwords (Sennrich et al., 2016) and a joint vocabulary with tied source"
W18-2902,2000.eamt-1.5,0,0.380339,"Missing"
W18-2902,D17-1209,0,0.0331098,"Missing"
W18-2902,P16-1078,0,0.327111,"d.ac.uk Abstract Despite these successes, there is room for improvement. RNN-based NMT is sequential, whereas natural language is hierarchical; thus, RNNs may not be the most appropriate models for language. In fact, these sequential models do not fully learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). In addition, although NMT performs well on high-resource languages, it is less successful in low-resource scenarios (Koehn and Knowles, 2017). As a solution to these challenges, researchers have incorporated syntax into NMT, particularly on the source side. Notably, Eriguchi et al. (2016) introduced a tree-to-sequence (tree2seq) NMT model in which the RNN encoder was augmented with a tree long short-term memory (LSTM) network (Tai et al., 2015). This and related techniques have yielded improvements in NMT; however, injecting source syntax into NMT requires parsing the training data with an external parser, and such parsers may be unavailable for low-resource languages. Adding syntactic source information may improve low-resource NMT, but we would need a way of doing so without an external parser. We would like to mimic the improvements that come from adding source syntactic hi"
W18-2902,D16-1025,0,0.0287861,"s. For low-resource cases, the unsupervised tree2seq encoder significantly outperforms the baselines; no improvements are seen for medium-resource translation. 1 Introduction Neural machine translation (NMT) is a widely used approach to machine translation that is often trained without outside linguistic information. In NMT, sentences are typically modeled using recurrent neural networks (RNNs), so they are represented in a continuous space, alleviating the sparsity issue that afflicted many previous machine translation approaches. As a result, NMT is state-of-the-art for many language pairs (Bentivogli et al., 2016; Toral and S´anchez-Cartagena, 2017). 6 Proceedings of the Workshop on the Relevance of Linguistic Structure in Neural Architectures for NLP, pages 6–12 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics obtain the phrase nodes of the source sentence. This encoder introduces unsupervised, discrete hierarchies without modifying the maximum likelihood objective used to train NMT by leveraging straight-through Gumbel softmax (Jang et al., 2017) to sample parsing decisions. In a Gumbel tree-LSTM, the hidden state hp and memory cell cp for a given node are comput"
W18-2902,P17-1012,0,0.0214636,"external parser to parse the training and test data. The tree-LSTM encoder was improved upon by Chen et al. (2017a) and Yang et al. (2017), who added a top-down pass. Other approaches have used convolutional networks to model source syntax. Chen et al. (2017b) enriched source word representations by extracting information from the dependency tree; a convolutional encoder was then applied to the representations. Bastings et al. (2017) fed source dependency trees into a graph convolutional encoder. Inducing unsupervised or semi-supervised hierarchies in NMT is a relatively recent research area. Gehring et al. (2017a,b) introduced a fully Acknowledgements This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA865017-C-9117. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation"
W18-2902,D17-1012,0,0.0369729,"Missing"
W18-2902,P82-1020,0,0.696716,"Missing"
W18-2902,P17-1177,0,0.073644,"he root node; attention to word and/or phrase nodes is described in section 3.4. 3.3 Top-Down Encoder Pass In the bottom-up tree-LSTM encoder described in the previous section, each node is able to incorporate local information from its respective children; however, no global information is used. Thus, we introduce a top-down pass, which allows the nodes to take global information about the tree into account. We refer to models containing this topdown pass as top-down tree2seq models. Note that such a top-down pass has been shown to aid in tree-based NMT with supervised syntactic information (Chen et al., 2017a; Yang et al., 2017); here, we add it to our unsupervised hierarchies. Our top-down tree implementation is similar to the bidirectional tree-GRU of Kokkinos and Potamianos (2017). The top-down root node h↓root is defined as follows: h↓root = h↑root 3.4 Attention to Words and Phrases The standard and top-down tree2seq models take different approaches to attention. The standard (bottom-up) model attends to the intermediate phrase nodes of the tree-LSTM, in addition to the word nodes output by the leaf LSTM. This follows what was done by Eriguchi et al. (2016). We use one attention mechanism for"
W18-2902,D13-1176,0,0.0329407,"node are computed recursively based on the hidden states and memory nodes of its left and right children (hl , hr , cl , and cr ). This is done as in a standard binary treeLSTM as follows:     i σ  fl   σ          fr  =  σ  W hl + b (1)     hr o  σ  tanh g Our proposed model yields significant improvements in very low-resource NMT without requiring outside data or parsers beyond what is used in standard NMT; in addition, this model is not significantly slower to train than RNN-based models. 2 Neural Machine Translation Neural machine translation (Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014) is an end-to-end neural approach to machine translation consisting of an encoder, a decoder, and an attention mechanism (Bahdanau et al., 2015). The encoder and decoder are usually LSTMs (Hochreiter and Schmidhuber, 1997) or gated recurrent units (GRUs) (Cho et al., 2014). The encoder reads in the source sentence and creates an embedding; the attention mechanism calculates a weighted combination of the words in the source sentence. This is then fed into the decoder, which uses the source representations to generate a translation in the target language. 3 Unsupervised"
W18-2902,D17-1304,0,0.0358495,"Missing"
W18-2902,P17-4012,0,0.0324248,"source side is most helpful in very low-resource scenarios. 5.2 Implementation Unsupervised Parses Williams et al. (2017) observed that the parses resulting from Gumbel tree-LSTMs for sentence classification did not seem to fit a known formalism. An examination of the parses induced by our NMT models suggests this as well. Furthermore, the different models (tree2seq and top-down tree2seq) do not seem to learn the same parses for the same language pair. We display example parses induced by the trained systems on a sentence from the test data in Table 4. All models are implemented in OpenNMTpy (Klein et al., 2017). They use word embedding size 500, hidden layer size 1000, batch size 64, two layers in the encoder and decoder, and dropout rate 0.3 (Gal and Ghahramani, 2016). We set maximum sentence length to 50 (150 for parse2seq source). Models are trained using Adam (Kingma and Ba, 2015) with learning rate 0.001. For treebased models, we use a Gumbel temperature of 9 EN→TR tree2seq EN→TR top-down EN→RO tree2seq EN→RO top-down Example Parse ( ( ( others have ) ( ( ( dismissed him ) as ) a ) ) ( j@@ ( oke . ) ) ) ( ( ( ( others have ) dismissed ) ( him as ) ) ( ( a ( j@@ oke ) ) . ) ) ( ( ( others have )"
W18-2902,P15-1150,0,0.0563599,"e the most appropriate models for language. In fact, these sequential models do not fully learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). In addition, although NMT performs well on high-resource languages, it is less successful in low-resource scenarios (Koehn and Knowles, 2017). As a solution to these challenges, researchers have incorporated syntax into NMT, particularly on the source side. Notably, Eriguchi et al. (2016) introduced a tree-to-sequence (tree2seq) NMT model in which the RNN encoder was augmented with a tree long short-term memory (LSTM) network (Tai et al., 2015). This and related techniques have yielded improvements in NMT; however, injecting source syntax into NMT requires parsing the training data with an external parser, and such parsers may be unavailable for low-resource languages. Adding syntactic source information may improve low-resource NMT, but we would need a way of doing so without an external parser. We would like to mimic the improvements that come from adding source syntactic hierarchies to NMT without requiring syntactic annotations of the training data. Recently, there have been some proposals to induce unsupervised hierarchies base"
W18-2902,P07-2045,0,0.00404651,"o a seq2seq model (with increased maximum source sentence length to account for the parsing tags). 4.3 TR→EN 11.1 12.8 13.2 Table 2: BLEU for the baseline and the unsupervised tree2seq systems on *→EN translation. The TR↔EN and RO↔EN data is from the WMT17 and WMT16 shared tasks, respectively (Bojar et al., 2017, 2016). Development is done on newsdev2016 and evaluation on newstest2016. The TL↔EN data is from IARPA MATERIAL Program language collection release IARPA MATERIAL BASE-1B-BUILD v1.0. No monolingual data is used for training. The data is tokenized and truecased with the Moses scripts (Koehn et al., 2007). We use byte pair encoding (BPE) with 45k merge operations to split words into subwords (Sennrich et al., 2016). Notably, this means that the unsupervised tree encoder induces a binary parse tree over subwords (rather than at the word level). 4.2 TL→EN 17.9 26.1 25.3 5 5.1 Results Translation Performance Tables 2 and 3 display BLEU scores for our unsupervised tree2seq models translating into and out of English, respectively. For the lowerresource language pairs, TL↔EN and TR↔EN, the tree2seq and top-down models consistently improve over the seq2seq and parse2seq baselines. However, for the me"
W18-2902,E17-1100,0,0.0613294,"Missing"
W18-2902,W17-3204,0,0.0173575,"-Resource Neural Machine Translation Anna Currey University of Edinburgh a.currey@sms.ed.ac.uk Kenneth Heafield University of Edinburgh kheafiel@inf.ed.ac.uk Abstract Despite these successes, there is room for improvement. RNN-based NMT is sequential, whereas natural language is hierarchical; thus, RNNs may not be the most appropriate models for language. In fact, these sequential models do not fully learn syntax (Bentivogli et al., 2016; Linzen et al., 2016; Shi et al., 2016). In addition, although NMT performs well on high-resource languages, it is less successful in low-resource scenarios (Koehn and Knowles, 2017). As a solution to these challenges, researchers have incorporated syntax into NMT, particularly on the source side. Notably, Eriguchi et al. (2016) introduced a tree-to-sequence (tree2seq) NMT model in which the RNN encoder was augmented with a tree long short-term memory (LSTM) network (Tai et al., 2015). This and related techniques have yielded improvements in NMT; however, injecting source syntax into NMT requires parsing the training data with an external parser, and such parsers may be unavailable for low-resource languages. Adding syntactic source information may improve low-resource NM"
W18-2902,E17-2093,0,0.0192382,"s section, each node is able to incorporate local information from its respective children; however, no global information is used. Thus, we introduce a top-down pass, which allows the nodes to take global information about the tree into account. We refer to models containing this topdown pass as top-down tree2seq models. Note that such a top-down pass has been shown to aid in tree-based NMT with supervised syntactic information (Chen et al., 2017a; Yang et al., 2017); here, we add it to our unsupervised hierarchies. Our top-down tree implementation is similar to the bidirectional tree-GRU of Kokkinos and Potamianos (2017). The top-down root node h↓root is defined as follows: h↓root = h↑root 3.4 Attention to Words and Phrases The standard and top-down tree2seq models take different approaches to attention. The standard (bottom-up) model attends to the intermediate phrase nodes of the tree-LSTM, in addition to the word nodes output by the leaf LSTM. This follows what was done by Eriguchi et al. (2016). We use one attention mechanism for all nodes (word and phrase), making no distinction between different node types. Note that without the attention to the phrase nodes, the bottom-up tree2seq model would be almost"
W18-2902,D17-1150,0,0.0669563,"ion to word and/or phrase nodes is described in section 3.4. 3.3 Top-Down Encoder Pass In the bottom-up tree-LSTM encoder described in the previous section, each node is able to incorporate local information from its respective children; however, no global information is used. Thus, we introduce a top-down pass, which allows the nodes to take global information about the tree into account. We refer to models containing this topdown pass as top-down tree2seq models. Note that such a top-down pass has been shown to aid in tree-based NMT with supervised syntactic information (Chen et al., 2017a; Yang et al., 2017); here, we add it to our unsupervised hierarchies. Our top-down tree implementation is similar to the bidirectional tree-GRU of Kokkinos and Potamianos (2017). The top-down root node h↓root is defined as follows: h↓root = h↑root 3.4 Attention to Words and Phrases The standard and top-down tree2seq models take different approaches to attention. The standard (bottom-up) model attends to the intermediate phrase nodes of the tree-LSTM, in addition to the word nodes output by the leaf LSTM. This follows what was done by Eriguchi et al. (2016). We use one attention mechanism for all nodes (word and"
W18-2902,P17-1064,0,0.0205123,"on the validation set, and the model with the highest BLEU on the validation set is used to translate the test data. During inference, we set beam size to 12 and maximum length to 100. Baselines We compare our models to an RNN-based attentional NMT model; we refer to this model as seq2seq. Apart from the encoder, this baseline is identical to our proposed models. We train the seq2seq baseline on unparsed parallel data. For translations out of English, we also consider an upper bound that uses syntactic supervision; we dub this model parse2seq. This is based on the mixed RNN model proposed by Li et al. (2017). We parse the source sentences using the Stanford CoreNLP parser (Manning et al., 2014) and linearize the resulting parses. We parse before applying BPE, and do not add any additional structure to segmented words; thus, final parses are not necessarily binary. This is fed directly into a seq2seq model (with increased maximum source sentence length to account for the parsing tags). 4.3 TR→EN 11.1 12.8 13.2 Table 2: BLEU for the baseline and the unsupervised tree2seq systems on *→EN translation. The TR↔EN and RO↔EN data is from the WMT17 and WMT16 shared tasks, respectively (Bojar et al., 2017,"
W18-2902,Q16-1037,0,0.079271,"Missing"
W18-2902,P14-5010,0,0.0034748,"s used to translate the test data. During inference, we set beam size to 12 and maximum length to 100. Baselines We compare our models to an RNN-based attentional NMT model; we refer to this model as seq2seq. Apart from the encoder, this baseline is identical to our proposed models. We train the seq2seq baseline on unparsed parallel data. For translations out of English, we also consider an upper bound that uses syntactic supervision; we dub this model parse2seq. This is based on the mixed RNN model proposed by Li et al. (2017). We parse the source sentences using the Stanford CoreNLP parser (Manning et al., 2014) and linearize the resulting parses. We parse before applying BPE, and do not add any additional structure to segmented words; thus, final parses are not necessarily binary. This is fed directly into a seq2seq model (with increased maximum source sentence length to account for the parsing tags). 4.3 TR→EN 11.1 12.8 13.2 Table 2: BLEU for the baseline and the unsupervised tree2seq systems on *→EN translation. The TR↔EN and RO↔EN data is from the WMT17 and WMT16 shared tasks, respectively (Bojar et al., 2017, 2016). Development is done on newsdev2016 and evaluation on newstest2016. The TL↔EN dat"
W18-2902,P16-1162,0,0.0276738,"11.1 12.8 13.2 Table 2: BLEU for the baseline and the unsupervised tree2seq systems on *→EN translation. The TR↔EN and RO↔EN data is from the WMT17 and WMT16 shared tasks, respectively (Bojar et al., 2017, 2016). Development is done on newsdev2016 and evaluation on newstest2016. The TL↔EN data is from IARPA MATERIAL Program language collection release IARPA MATERIAL BASE-1B-BUILD v1.0. No monolingual data is used for training. The data is tokenized and truecased with the Moses scripts (Koehn et al., 2007). We use byte pair encoding (BPE) with 45k merge operations to split words into subwords (Sennrich et al., 2016). Notably, this means that the unsupervised tree encoder induces a binary parse tree over subwords (rather than at the word level). 4.2 TL→EN 17.9 26.1 25.3 5 5.1 Results Translation Performance Tables 2 and 3 display BLEU scores for our unsupervised tree2seq models translating into and out of English, respectively. For the lowerresource language pairs, TL↔EN and TR↔EN, the tree2seq and top-down models consistently improve over the seq2seq and parse2seq baselines. However, for the medium-resource language pair (RO↔EN), the unsupervised tree models do not improve over seq2seq, unlike the parse2"
W18-2902,D16-1159,0,0.0474577,"Missing"
W18-6412,W18-6401,1,0.759353,"airs (both directions). For these experiments, we use the same training sets and data preparation as in our system submissions, but train the deep RNNs with a working memory of 10GB, validating every 1,000 steps, and testing for convergence with a patience of 10. We use exponential smoothing and show the results on a single smoothed model. From the results in Table 7 we see that the multihead/hop extension has a small positive effect on B LEU in most language pairs. 27.6 28.2 27.7 28.1 26.9 Table 5: Results for EN↔TR systems on official WMT test sets. human evaluation from the findings paper (Bojar et al., 2018). In terms of the clustering provided by the organisers, we were in the top constrained cluster (i.e. no significant difference was observed between ours and the best constrained system) for EN→CS, DE→EN, ET→EN, FI→EN, TR→EN and EN→TR, i.e. 6/14 language pairs. Nevertheless, Table 6 still shows that our systems generally lag behind the best submitted systems. This is contrast to the 2017 shared task, where we achieved the highest scores in most of the language pairs where we submitted systems. We hypothesise that other groups have taken fuller advantage of the transformer architecture, and als"
W18-6412,P18-1008,0,0.0363778,"with the raw data. 2.3 Model Architecture For this submission we considered two types of sequence-to-sequence architectures: a transformer (Vaswani et al., 2017) and a deep RNN, specifically the BiDeep GRU encoder-decoder (Miceli Bar400 one et al., 2017). Both architectures4 are implemented in the Marian open source neural machine translation framework (Junczys-Dowmunt et al., 2018). For the transformer architecture we used the “wmt2017-transformer” setup from the Marian example collection5 . We extended the RNN with with multi-head and multi-hop attention. Multi-head attention is similar to Chen et al. (2018), with an MLP attention mechanism using a single tanh hidden layer followed by one soft-max layer for each attention heads. We further include an optional projection layer on the attended context with layer normalisation in order to avoid increasing the total size of the attended context. Let C ∈ RNs ×de be the input sentence representation produced by the encoder, where Ns is the source sentence length and de is the top-level bidirectional encoder state dimension. Let s ∈ Rdd be an internal decoder state at some step. Then for source sentence position i we compute a vector of M attention weig"
W18-6412,W17-4715,1,0.879995,"experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also applied weighting of different corpora to most language pairs, particularly DE↔EN (Section 3.5). Extensions to Back-translation For TR↔EN (Section 3.7) we used copied monolingual data (Currey et al., 2017a) and iterative back-translation. System Details 2.1 Data and Selection All our systems were constrained in the sense that they only used the supplied parallel data (including ParaCrawl) for training the systems. We also used the monolingual news crawls to create extra synthetic parallel data by back-translation, for all language pairs, and by copying monolingual data for TR↔EN. During training we generally used newsdev2016 or newstest2016 for validation, and newstest2017 for development testing (i.e. model selection), except for ZH↔EN, and ET↔EN, where we used the recent newsdev sets instead"
W18-6412,N13-1073,0,0.0722204,"Missing"
W18-6412,W16-2323,1,0.938501,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6412,P07-2045,0,0.00708596,"e of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a validation set (newstest2016) and observed the point where translation quality started to deteriorate. We used the translation plausibility score at this point as the threshold for selecting data for training the final systems. 2.2 Preprocessing For most language pairs, our preprocessing setup consisted of the Moses pipeline (Koehn et al., 2007) of normalisation, tokenisation and truecasing, followed by byte-pair encoding (BPE) (Sennrich et al., 2016c). We generally applied joint BPE, with the number of merge operations set on a per-pair basis, detailed in Section 3. Different pipelines were used for processing the two languages written in non-Latin scripts (i.e. Chinese and Russian), also explained in Section 3. For some language pairs (those including Czech, Estonian, Finnish and German) we used the preprocessed data provided by the organisers (which is preprocessed up to truecasing), whilst for the others we started with the raw d"
W18-6412,P12-3005,0,0.0282872,"id identified a significant proportion of the data as these other two Slavic languages, but on inspecting a sample, they were found nearly always to be Czech. The issue with langid is that we just give it the text, without providing any prior knowledge, when in actual fact there is a strong prior that CzEng sentences are really Czech and English, by construction 25 fi-en et-en 20 15 Bleu Language Identifier Filtering This was applied to the CS↔EN and DE↔EN corpora, based on observations that CzEng, and ParaCrawl both contain sentence pairs in the “wrong” language. For CS↔EN we applied langid (Lui and Baldwin, 2012) to both sids of the data, removing any sentences whose English side is not labelled as English, or whose Czech is not labelled as Czech, Slovak or Slovenian3 . For DE↔EN, we just applied langid to ParaCrawl and retained only those pairs where each side was identified as the ‘correct’ language by langid. This reduced the size of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a va"
W18-6412,W17-4710,1,0.858373,"ll as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also"
W18-6412,P10-2041,0,0.0719435,"Missing"
W18-6412,W17-4739,1,0.864199,"3 times the tokens on the other side, following Hassan et al. (2018). After preprocessing the corpus size was 23.6M sentences. We then applied BPE using 18,000 merge operations and we used the top 18,000 BPE segments as vocabulary. We augmented our data with backtranslated 4 The BiDeep GRU is obtainable using the -best-deep option. 5 https://github.com/marian-nmt/ marian-examples 401 6 The implementation of the multi-head and multi-hop attention architectures is available at: https://github. com/EdinburghNLP/marian-dev 7 https://marian-nmt.github.io 8 https://github.com/fxsjy/jieba ZH↔EN from Sennrich et al. (2017), which consists of 8.6M sentences for EN→ZH and 19.7M for ZH→EN. We trained using the BiDeep architecture with multi-head attention with 1 hop and 3 heads. We decoded using an ensemble of 5 L2R systems and a beam of 12 for EN→ZH and 6 L2R systems and a beam of 12 for ZH→EN. Due to time constraints, we were not able to train any of the systems to convergence. 3.2 Czech ↔ English After preprocessing, language filtering (see Sections 2.1 and 2.2), and removing any parallel sentences where neither side contains an ASCII letter, we were left with around 50M sentence pairs. We then learned a joint"
W18-6412,P16-1162,1,0.881886,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6453,W18-6475,0,0.0511142,"Missing"
W18-6453,W11-1218,0,0.0394428,"ine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled pa"
W18-6453,W18-6476,0,0.042088,"Missing"
W18-6453,W18-6477,0,0.0952092,"Missing"
W18-6453,W18-6472,0,0.0451806,"Missing"
W18-6453,W18-6478,0,0.0570614,"Missing"
W18-6453,D11-1033,0,0.165705,"investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training data and focus on types of noise occurring in web-crawled corpora. They carried out a study how noise that occurs in crawled parallel text impacts statistical and neural machine translation. There is a rich literature on data selection which aims at sub-sampling parallel data relevant for a task-specific machine translation system (Axelrod et al., 2011). van der Wees et al. (2017) find that the existing data selection methods developed for statistical machine translation are less effective for neural machine translation. This is different from our goals of handling noise since those methods tend to discard perfectly fine sentence pairs (say, about cooking recipes) that are just not relevant for the targeted domain (say, software manuals). Our task is focused on data quality that is relevant for all domains. 4 https://drive.google.com/drive/folders/ 1zZNPlAThm-Rnvxsy8rXzChC49bc0 TGO 727 Type of Noise Okay Misaligned sentences Third language B"
W18-6453,P18-4020,1,0.833453,"Missing"
W18-6453,W18-6473,0,0.0626345,"Missing"
W18-6453,W18-2709,1,0.900845,"and adherence to sentence-bysentence correspondences. The other extreme are sentence pairs extracted with fully automatic processes from indiscriminate crawling of the World Wide Web. The Shared Task on Parallel Corpus Filtering targets the second extreme, although the methods developed for this data condition should also carry over to less noisy parallel corpora. In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https:"
W18-6453,W18-6474,0,0.0714008,"Missing"
W18-6453,W18-6479,1,0.790029,"Missing"
W18-6453,2005.mtsummit-papers.11,1,0.26682,"Missing"
W18-6453,W16-2347,1,0.895759,"Missing"
W18-6453,P07-2045,1,0.0147689,"Missing"
W18-6453,W17-3209,0,0.0604803,"ed corpora, and evaluation translation quality on blind test sets using the BLEU score. For development purposes, we released configuration files and scripts that mirror the official testing procedure with a development test set. The development pack consists of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can"
W18-6453,P13-2061,0,0.0539603,"ea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very noisy 1 billion word (English token count) German–English corpus crawled from the web by the Paracrawl project. We asked participants to subselect sentence pairs that amount to (a) 10 milli"
W18-6453,W18-6480,0,0.0668994,"Missing"
W18-6453,skadins-etal-2014-billions,0,0.0402284,"Missing"
W18-6453,W18-6481,0,0.0862833,"Missing"
W18-6453,W18-6488,0,0.0644507,"Missing"
W18-6453,W18-6482,0,0.0428675,"Missing"
W18-6453,2011.eamt-1.25,0,0.413479,"Missing"
W18-6453,W18-6483,0,0.069782,"Missing"
W18-6453,2011.mtsummit-papers.47,0,0.73575,"Missing"
W18-6453,W18-6484,0,0.0645499,"Missing"
W18-6453,tiedemann-2012-parallel,0,0.0910955,"It contains news stories that were either translated from German to English or from English to German. NEWSTEST 2018 IWSLT 2017 ACQUIS IWSLT 2017 The test set from the IWSLT 2017 evaluation campaign. It consists of transcripts of talks given at the TED conference. They cover generally accessible topics in the area of technology, entertainment, and design. EMEA GLOBALVOICES KDE ate the machine translation systems trained on the subsampled data sets. Word counts are obtained with wc on untokenized text. This test set was extracted from the Acquis Communtaire corpus, which is available on OPUS7 (Tiedemann, 2012) (which was the source to create the subsequent 3 test sets). The test set consists of laws of the European Union that have to be incorporated into the national laws of the EU member countries. We only used sentences with 15 to 80 words, and removed any duplicate sentence pairs. 5 Evaluation Protocol The testing setup mirrors the development environment that we provided to the participants. 5.1 Particpants We received submissions from 17 different organizations. See Table 3 for the complete list of participants. The participant’s organizations are quite diverse, with 3 participants from Spain,"
W18-6453,W18-6485,0,0.0339437,"Missing"
W18-6453,D17-1147,0,0.150155,"Missing"
W18-6453,W18-6486,0,0.0808535,"Missing"
W18-6453,2009.mtsummit-posters.15,0,0.47964,"Missing"
W18-6453,2011.mtsummit-papers.48,0,0.14183,"of • a script to subsample corpora based on quality scores • a Moses configuration file to train and test a statistical machine translation system • Marian scripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can tran"
W18-6453,P99-1068,0,0.254476,"In setting this task, we were motivated by our ongoing efforts to create large publicly available parallel corpora from web sources and the recognition that noisy parallel data is especially a concern for neural machine translation (Khayrallah and Koehn, 2018). This paper gives an overview of the task, presents its results and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the"
W18-6453,D11-1126,0,0.109697,"ripts to train and test a neural machine translation system Most of this work was done in the context of statistical machine translation, but more recent work targets neural models. Carpuat et al. (2017) focus on identifying semantic differences in translation pairs using cross-lingual textual entailment and additional length-based features, and demonstrates that removing such sentences improves neural machine translation performance. As Rarrick et al. (2011) point out, one type of noise in parallel corpora extracted from the web are translations that have been created by machine translation. Venugopal et al. (2011) propose a method to watermark the output of machine translation systems to aid this distinction. Antonova and Misyurev (2011) report that rule-based machine translation output can be detected due to certain word choices, and statistical machine translation output can be detected due to lack of reordering. Belinkov and Bisk (2017) investigate the impact of noise on neural machine translation. They focus on creating systems that can translate the kinds of orthographic errors (typos, misspellings, etc.) that humans can comprehend. In contrast, Khayrallah and Koehn (2018) address noisy training d"
W18-6453,W18-6487,0,0.0438357,"Missing"
W18-6453,W18-6489,0,0.0366714,"Missing"
W18-6453,D17-1319,1,0.672175,"s and provides some analysis. 2 Related Work 1 http://opus.lingfil.uu.se/ http://www.paracrawl.eu/ 3 NLP4TM 2016: Shared task http://rgcl.wlv.ac.uk/nlp4tm2016/shared-task/ Although the idea of crawling the web indiscriminately for parallel data goes back to the 20th century (Resnik, 1999), work in the academic com2 726 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 726–739 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64080 3 to filter a parallel corpus; Xu and Koehn (2017) generate synthetic noisy data (inadequate and nonfluent translations) and use this data to train a classifier to identify good sentence pairs from a noisy corpus; and Cui et al. (2013) use a graph-based random walk algorithm and extract phrase pair scores to weight the phrase translation probabilities to bias towards more trustworthy ones. Task The shared task tackled the problem of filtering parallel corpora. Given a noisy parallel corpus (crawled from the web), participants developed methods to filter it to a smaller size of high quality sentence pairs. Specifically, we provided a very nois"
W19-4427,D18-1332,1,0.811303,"s with the Marian toolkit11 (Junczys-Dowmunt et al., 2018a), and generally follow the configuration proposed by JunczysDowmunt et al. (2018b). Transformer models are trained using Adam (Kingma and Ba, 2014) with a learning rate of 0.0003 and linear warm-up for the first 16k updates, followed by inverted squared decay. For the larger models, we decrease the learning rate to 0.0002 and warm-up to 8k first updates. We train with synchronous SGD (Adam) and dynamically sized mini-batches fitted into 48GB GPU RAM memory across 4 GPUs, accumulating gradients for 3 iterations before making an update (Bogoychev et al., 2018). This results in mini-batches consisting of ca. 2,700 sentences. The maximum length of a training 11 sentence is limited to 150 subword units. Strong regularization via dropout (Gal and Ghahramani, 2016) is used to dissuade the model from simply copying the input: we use a dropout probability between transformer layers of 0.3, for transformer self-attention and filters of 0.1, and for source and target words of 0.3 and 0.1 respectively. For source and target words we dropout entire embedding vectors, not just single neurons. We also use label smoothing with a weight of 0.1, and exponential av"
W19-4427,W18-0529,0,0.127845,"Missing"
W19-4427,W19-4406,0,0.408145,"vated by the problems identified in these papers but concerned by the complexity of their methods, we sought simpler and more effective approaches to both challenges. For data sparsity, we propose an unsupervised synthetic parallel data generation method exploiting confusion sets from a spellchecker to augment training data used for pre-training sequence-to-sequence models. For multi-pass decoding, we use right-to-left models in rescoring, similar to competitive neural machine translation systems. In the Building Educational Application (BEA) 2019 Shared Task on Grammatical Error Correction1 (Bryant et al., 2019), our GEC systems ranked first in the restricted and low-resource tasks.2 This confirms the effectiveness of the proposed methods in scenarios with and without readily-available large amounts of error-annotated data. The rest of the paper is organized as follows: 1 https://www.cl.cam.ac.uk/research/nl/ bea2019st/ 2 Incidentally, our restricted system also outperformed all submissions to the unrestricted task to which we did not submit. 252 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 252–263 c Florence, Italy, August 2, 2019. 2019"
W19-4427,P17-1074,0,0.139008,"ed W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The performance of participating systems was evaluated using the ERRANT scorer (Bryant et al., 2017) which reports a F0.5 over span-based corrections. 3 Related work plied their model with noisy examples synthesized from clean sentences. Junczys-Dowmunt et al. (2018b) utilized a large amount of monolingual data by pre-training decoder parameters with a language model, and Lichtarge et al. (2018, 2019), on the other hand, used a large-scale out-of-domain parallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by mea"
W19-4427,P18-1097,0,0.482717,"where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F0.5 in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-theart results of 64.16 M2 for the submitted system, and 61.30 M2 for the constrained system trained on the NUCLE and Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018"
W19-4427,W17-5037,0,0.0194999,"e correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regularization, model ensembling, and using a large-scale language model. Subsequent work highlighted two challenges in neural GEC, data sparsity and multi-pass decoding: Motivated by the problems identified in these papers but concerned by the complexity of their methods, we sought simpler and more ef"
W19-4427,N12-1067,0,0.517964,"0.4 0.49 0.65 0.58 0.75 0.73 0.53 0.38 0.47 0.37 0.26 0.25 0.53 0.44 0.71 0.61 0.28 0.21 0.6 0.57 0.59 0.56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and s"
W19-4427,W13-1703,0,0.692334,"phic errors. The shared task introduced two new annotated datasets for development and evaluation: Cambridge English Write & Improve (W&I) and the LOCNESS corpora (Bryant et al., 2019; Granger, 1998). These represent a more diverse cross-section of English language levels and domains than previous datasets. There were three tracks that varied in the amount of admissible annotated learner data for system development. In the restricted track, participants were provided with four learner corpora containing 1.2 million sentences in total: the public FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Mizumoto et al., 2012), and the mentioned W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The p"
W19-4427,E14-3013,0,0.343628,"m Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterati"
W19-4427,W09-2112,0,0.0638537,"rallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. ("
W19-4427,N18-2046,1,0.867139,"64.24 F0.5 in the restricted and low-resource tracks respectively, both on the W&I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-theart results of 64.16 M2 for the submitted system, and 61.30 M2 for the constrained system trained on the NUCLE and Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regu"
W19-4427,W11-2123,1,0.636646,"ng examples. Our final training set in the restricted setting contains 1,953,554 sentences, assembled from the cleaned Lang-8 corpus and oversampled remaining corpora: FCE and the training portion of W&I are oversampled 10 times, NUCLE 5 times. Table 3 summarizes all data sets used for training. W&I+LOCNESS Dev is used solely as a development set in both tracks. an average score from two language models: an n-gram probabilistic word-level language model estimated from target sentences, and a simplified operation sequence model built on edits between source and target sentences.8 We use KenLM (Heafield, 2011) to build 5-gram language models. The top 2 million sentence pairs with the highest scores are used as training data in place of the errorannotated ESL learner data to train models for the low-resource system. Monolingual data We use News Crawl6 — a publicly available corpus of monolingual texts extracted from online newspapers released for the WMT series of shared tasks (Bojar et al., 2018) — as our primary monolingual data source. We uniformly sampled 100 million English sentences from de-duplicated crawls in years 2007 to 2018 to produce synthetic parallel data for model pretraining. Anothe"
W19-4427,D16-1161,1,0.882952,"d Lang-8 data. 1 Multi-pass decoding: the correction process has been improved by incrementally correcting a sentence multiple times through multi-round inference using a model of one type (Ge et al., 2018a; Lichtarge et al., 2018), involving rightto-left models (Ge et al., 2018b), or by pipelining SMT and NMT-based systems (Grundkiewicz and Junczys-Dowmunt, 2018). Introduction For the past five years, machine translation methods have been the most successful approach to automated Grammatical Error Correction (GEC). Work started with statistical phrase-based machine translation (SMT) methods (Junczys-Dowmunt and Grundkiewicz, 2016; Chollampatt and Ng, 2017) while sequence-to-sequence methods adopted from neural machine translation (NMT) lagged in quality until recently (Chollampatt and Ng, 2018; JunczysDowmunt et al., 2018b). These two papers established a number of techniques for neural GEC, such as transfer learning from monolingual data, strong regularization, model ensembling, and using a large-scale language model. Subsequent work highlighted two challenges in neural GEC, data sparsity and multi-pass decoding: Motivated by the problems identified in these papers but concerned by the complexity of their methods, we"
W19-4427,N18-1055,1,0.673652,"ed to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track allowed unrestricted data. The performance of participating systems was evaluated using the ERRANT scorer (Bryant et al., 2017) which reports a F0.5 over span-based corrections. 3 Related work plied their model with noisy examples synthesized from clean sentences. Junczys-Dowmunt et al. (2018b) utilized a large amount of monolingual data by pre-training decoder parameters with a language model, and Lichtarge et al. (2018, 2019), on the other hand, used a large-scale out-of-domain parallel corpus extracted from Wikipedia revisions to pre-train their models. We also pre-train a neural sequence-to-sequence model, but we do so solely on synthetic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previo"
W19-4427,D18-1541,0,0.341544,"n explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique adva"
W19-4427,P18-1007,0,0.0309372,"ference filtering by Moore and Lewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They di"
W19-4427,D18-2012,0,0.0339514,"ewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They differ from Transformer Base by the number of hea"
W19-4427,N19-1333,0,0.397602,"Missing"
W19-4427,D17-1156,0,0.028521,"ence. embedding similarities (Mikolov et al., 2013). Corpus 4.3 FCE Train NUCLE Lang-8 W&I Train W&I+LOCNESS Dev WikEd R R R R L,R L 28,350 57,113 1,041,409 34,308 4,384 2,000,000 News Crawl L,R 100M Model pre-training and fine-tuning We generate synthetic errors from 100 million sentences sampled from the English part of the WMT News Crawl corpus (Bojar et al., 2018) and use pairs of synthetic and authentic sentences exclusively to pre-train transformer models. A pre-trained model can be used with the actual indomain error-annotated data by fine-tuning (Hinton and Salakhutdinov, 2006; Miceli Barone et al., 2017). We experimented with two fine-tuning strategies: 1. Initialising the neural network weights with the pre-trained model and starting a new training run on new data. This resets learning rate scheduling and optimizer parameters. We further refer to this procedure as re-training. 2. Continuing training the existing model with new data preserving the learning rate, optimizer parameters and historic weights for exponential smoothing. We refer to this scheme as fine-tuning. The main difference between re-training and fine-tuning is resetting the training state after pretraining. The latter strateg"
W19-4427,C12-2084,0,0.243296,"Missing"
W19-4427,P10-2041,0,0.0291478,"produce synthetic parallel data for model pretraining. Another subset of 2 million sentences was selected to augment the training data during fine-tuning. The Enchant spellchecker7 with the Aspell backend and a British English dictionary were used to generate confusion sets. 5.3 Wikipedia edits In the low-resource setting, we use a filtered subset of the WikEd corpus (Grundkiewicz and Junczys-Dowmunt, 2014). The original corpus contains 56 million automatically extracted edited sentences from Wikipedia revisions and is quite noisy. We clean the data using cross-entropy difference filtering by Moore and Lewis (2010). W&I+LOCNESS Dev is used as an in-domain seed corpus. All sentence pairs in WikEd are sorted w.r.t 5.2 Data preprocessing Following the preprocessing methods of the data provided in the shared task, we tokenize other data sets with spaCy.9 We also normalize Unicode punctuation to ASCII with a script included in the Moses SMT toolkit10 (Koehn et al., 2007). To handle the open vocabulary issue, we split tokens into 32,000 subword units trained on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Ri"
W19-4427,P15-2097,0,0.215119,"56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and should not be directly contrasted with systems trained on the NUCLE and Lang-8 corpora only. In contrast"
W19-4427,E17-2037,0,0.193977,"71 0.61 0.28 0.21 0.6 0.57 0.59 0.56 0.57 0.49 0.29 0.2 0.4 0.22 0.22 0.29 0.19 0.6 0.7 0.66 Low-resource Restricted 0.8 0.69 0.66 F0.5 0.2 W O PR EP PR O PU N N CT SP EL L V V ER ER B B: FO V RM E V RB: ER S B: VA TE N SE M O RP H N NO O U UN N :N N U O M U N :P O SS O RT OT H H ER D ET N J CO DV A A D J 0.0 Figure 2: Comparison of restricted and low-resource systems (F0.5 ) on a selection of error types from ERRANT on W&I+LOCNESS Dev. the CoNLL 2014 test set (Dahlmeier et al., 2013) calculated with the official M2Scorer (Dahlmeier and Ng, 2012). We also report results on the JFLEG test set (Napoles et al., 2017) using GLEU (Napoles et al., 2015). Following other works (Sakaguchi et al., 2017; Junczys-Dowmunt et al., 2018b), we correct spelling errors in JFLEG using Enchant before decoding. On CoNLL-2014, our best GEC system achieves 64.16 M2 , which is the highest score reported on this test set so far, including the systems trained on non-publicly available resources (Ge et al., 2018a,b). Although comparing to prior work, the improvement is impressive, our submitted system uses the public FCE corpus and the new W&I Train sets and should not be directly contrasted with systems trained on the NUCLE an"
W19-4427,P03-1021,0,0.0189038,"exts, so can be more capable of correcting errors of different types. We adapt the re-ranking technique. We first generate n-best lists using the ensemble of standard left-to-right models and the language model, then re-score sentence pairs with right-to-left models using length-normalized scores, and re-rank the hypotheses. We have experimented with different weighting strategies during re-scoring, but found that weighting all sequence-to-sequence models equally with 1.0 and grid-searching the weight of the language model again works best. Tuning all ensemble weights independently with MERT (Och, 2003) lead to overfitting to the development set. 5 5.1 Right-to-left re-ranking Experiments Datasets Error-annotated data The restricted models are trained on data provided in the shared task: the A common approach to improve the performance of NMT systems is re-ranking with right-to-left 255 FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), W&I+LOCNESS data sets (Bryant et al., 2019; Granger, 1998), and a preprocessed version of the Lang-8 Corpus of Learner English (Mizumoto et al., 2012). We clean Lang-8 using regular expressions5 to 1) filter out sentences with a low rati"
W19-4427,E17-2025,0,0.0261162,"on 10 million randomly sampled sentences from News Crawl using the default unigram-LM segmentation algorithm (Kudo, 2018) from SentencePiece (Kudo and Richardson, 2018). Model architecture We experiment with different variants of Transformer models (Vaswani et al., 2017). The “Transformer Base” architecture has 6 blocks of selfattention/feed forward sub-layers in the encoder and decoder, 8-head self-attention layers, and embeddings vector size of 512. The ReLU activation function (Nair and Hinton, 2010) is used between filters of size 2048. We tie output layer, decoder and encoder embeddings (Press and Wolf, 2017). We choose the “Transformer Big” architecture as our final models for the restricted track. They differ from Transformer Base by the number of heads in multi-head attention components (16 heads instead for 8), larger embeddings vector size of 1024 and filter size of 4096. The architecture of the language models corresponds to the structure of the decoder of the sequence-to-sequence model, either Transformer Base or Big. 5 Cleaning Lang-8 led to minor improvements during the preliminary experiments when no pre-training was used. 6 http://data.statmt.org/news-crawl/ 7 https://abiword.github.io/"
W19-4427,D17-1039,0,0.0727316,"Missing"
W19-4427,W17-5032,0,0.354327,"ial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two mode"
W19-4427,E14-1038,0,0.204522,"upervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by"
W19-4427,D10-1094,0,0.165207,"etic data. Although our unsupervised method for synthesising parallel data by means of an (inverted) spellchecker is novel, the idea of generating artificial errors has been explored in the literature before, as summarized by Felice (2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further exten"
W19-4427,I17-2062,0,0.0756667,"Missing"
W19-4427,W17-4739,1,0.866064,"tated data without and with transfer learning from the language model. Surprisingly, for the restricted system, pre-training the decoder parameters (Baseline + LM pretraining) does not yield much improvement. A major improvement is achieved, however, by pre-training of the entire neural network on the synthetic data (Re-training). The fine-tuning strategy generally leads to better results than re-training, mostly due to increased precision. Adding 2 million of synthetic sentences to the error-annotated data — resulting approximately in an 1:1 ratio of genuine and artificial training examples (Sennrich et al., 2017) — further improves the performance. Ensembling eight Transformer models with a language model and re-ranking the n-best lists with four right-to-left models leads to consistent improvements. The quality of the language model is important as using a stronger language model (LM Big) generally improves the scores. The systems with bigger models (Ensemble Big×4 + LM Big) have a higher precision and thus perform better on both datasets. Interestingly, reranking using smaller and relatively weaker rightto-left Transformer Base models is still beneficial. We have found that re-ranking works best for"
W19-4427,W16-2309,0,0.0334077,"g model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique advantages for specific error types as they decode with different contexts. Inspired by this finding, we adapt a common technique from NMT (Sennrich et al., 2016, 2017) that reranks with a right-to-left model, but without multiple rounds. We contend that multiple rounds are only necessary if the system has low recall. Many recent advances in neural GEC aim at overcoming the mentioned data sparsity problem. Ge et al. (2018a) proposed fluency-boost learning that generates additional training examples during training from an independent backward model or the forward model being trained. Xie et al. (2018) sup253 4 4.1 System overview Transformer models Our neural GEC systems are based on Transformer models (Vaswani et al., 2017) that have been recently ad"
W19-4427,N19-1406,0,0.132521,"Missing"
W19-4427,N18-1057,0,0.244346,"(2016). Previously proposed methods usually require a errorannotated corpus as a seed to generate artificial errors reflecting linguistic properties and error distributions observed in natural-error corpora (Foster and Andersen, 2009; Felice and Yuan, 2014). Artificial error generation methods spanned conditional probabilistic models for specific error types only (Rozovskaya and Roth, 2010; Rozovskaya et al., 2014; Felice and Yuan, 2014), statistical or neural MT systems trained on reversed source and target sides (Rei et al., 2017; Kasewa et al., 2018) or neural sequence transduction models (Xie et al., 2018). None of these methods is unsupervised. Other recent work focuses on improving model inference. Ge et al. (2018a) proposed correcting a sentence more than once through multi-round model inference. Lichtarge et al. (2018) introduced iterative decoding to incrementally correct a sentence with a high-precision system. The multiround correction approach has been further extended (Ge et al., 2018b) by interchanging decoding of a standard left-to-right model with a right-toleft model. The authors claim that the two models display unique advantages for specific error types as they decode with differ"
W19-4427,P11-1019,0,0.817167,"g grammatical, lexical, and orthographic errors. The shared task introduced two new annotated datasets for development and evaluation: Cambridge English Write & Improve (W&I) and the LOCNESS corpora (Bryant et al., 2019; Granger, 1998). These represent a more diverse cross-section of English language levels and domains than previous datasets. There were three tracks that varied in the amount of admissible annotated learner data for system development. In the restricted track, participants were provided with four learner corpora containing 1.2 million sentences in total: the public FCE corpus (Yannakoudakis et al., 2011), NUCLE (Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Mizumoto et al., 2012), and the mentioned W&I+LOCNESS datasets. No restriction was placed on publicly available unannotated data or NLP tools such as spellcheckers. The low-resource track was limited to the use of the W&I+LOCNESS development set. The organizers further clarified that automatically extracted parallel data, e.g. from Wikipedia, could be used only to build low-resource and unrestricted systems; it was inadmissible in the restricted track. We participated in the restricted and low-resource tracks; the third track"
W19-4427,N19-1014,0,0.240749,"Missing"
W19-5203,P17-2021,0,0.0194809,"sing a depthfirst tree traversal. We tokenize the opening parenthesis of each phrase with its phrase label. We propose two models for incorporating linearized parses into Transformer-based NMT: a multi-task model and a mixed encoder model. Figure 1 summarizes the two proposed methods; they are discussed in detail in sections 2.2 and 2.3, respectively. 2.1 3. Since neural machine translation already struggles with long sentences (Bahdanau et al., 2015), and adding the phrase nodes has the potential to make the sentences much longer, we remove part-of-speech tags from the parses (as was done by Aharoni and Goldberg, 2017). Linearized Constituency Parses Both of our proposed methods make use of linearized parses of the source sentences to inject source syntax into Transformer-based NMT. Linearizing the parses allows us to add syntactic information without modifying the Transformer architecture. Here, we describe how these parses are created. We generate and format the parsed data as follows: 4. For our multi-task model (section 2.2), we remove words from the linearized parses. We do this in order to further shorten the length of the target sequences. We do not expect that this will make the parsing task too dif"
W19-5203,D17-1209,0,0.039565,"Missing"
W19-5203,D14-1179,0,0.0859709,"Missing"
W19-5203,D16-1257,0,0.0255566,"similarly combined the standard bidirectional encoder with two additional encoders, one Related Work The performance of many RNN-based NMT paradigms has been improved by adding explicit syntactic annotations, particularly on the source side; we review some syntactic NMT models here. This paper is, along with Wu et al. (2018) and Zhang et al. (2019), among the first to add explicit syntax to Transformer-based NMT. 7.1 Syntactic NMT with Modified Encoder Linearized Parses in Neural Networks In this work, we use linearized parse trees to add syntax into the Transformer. Vinyals et al. (2015) and Choe and Charniak (2016) introduced the idea 30 mixed encoder model did only marginally better than the non-syntactic baseline. In the future, we plan on extending these techniques to incorporate target-side syntax into Transformer-based NMT. In addition, we would like to experiment with different source languages in order to find out whether adding source-side syntax has a greater effect on some source languages than others. It would also be interesting to experiment with a multi-task, multilingual NMT framework with multiple target languages. that encoded the pre-order traversal of the dependency parse of the sente"
W19-5203,P14-5010,0,0.0121335,"have not (VP been (VP elected ) ) ) . ) ) <TR> let me make a comparison . <TR> (ROOT (S (NP they ) (VP are (NP important issues ) ) . ) ) <PA> you have not been elected . <PA> you have not been elected . ... ... (a) Multi-task syntactic NMT model. The system is trained to translate (<TR>) and parse (<PA>) source sentences using the same architecture. (b) Mixed encoder syntactic NMT model. The system learns to translate directly from both parsed and unparsed source sentences into unparsed target sentences. Figure 1: Illustrations of the two proposed syntactic NMT methods. constituency parser (Manning et al., 2014) to parse the source side of the parallel corpus. This technique of parsing the parallel data instead of using gold parses is common in syntactic NMT (Eriguchi et al., 2016) and in neural parsing (Vinyals et al., 2015). For the multi-task model, it would be possible to incorporate gold parses into training as well, but we leave this for future work. to NMT that are straightforward to incorporate in practice • We empirically evaluate both methods on translation from English into 21 diverse target languages, finding that the multi-task method improves consistently over a nonsyntactic baseline 2"
W19-5203,N18-2085,0,0.013262,"cross-lingual experiments. We consider translation from English (EN) into each of the twenty remaining target languages; Table 3 contains a full list of the target languages, as well as their language families or branches. By using this data set, we are able to evaluate the usefulness of syntactic information for several relatively diverse target languages, unlike most previous work on syntactic NMT (reviewed in section 7). However, all the languages in our experiments are Indo-European or Uralic due to using Europarl. In order to facilitate comparison between the target languages, we follow Cotterell et al. (2018) by taking only the intersections of the Europarl training data. This means that the source (EN) data is identical for all experiments, and the targets are all translations of each other in the different target languages. This results in 170k parallel training sentences for each language pair. We reserve a 4.2 Results Table 4 displays BLEU scores on the test data for each target language for the proposed systems. The multi-task system outperforms the baseline for all target languages. In addition, for all but four target languages (SV, EL, SK, and ET), the multitask system is at least 1 BLEU p"
W19-5203,J93-2004,0,0.0666361,"Missing"
W19-5203,D18-1327,1,0.891895,"gle encoder and different decoders to train two tasks: parsing the source sentence and translating from source to target. Kiperwasser and Ballesteros (2018) also applied multi-task learning to syntactic NMT; they used a shared RNN decoder for translation, dependency parsing, and part-of-speech tagging and evaluated different scheduling techniques to combine the tasks. Our multi-task system builds off these two papers by training a joint NMT and parsing model using a single encoder and decoder in a Transformer framework, and further evaluates the multi-task framework on several language pairs. Currey and Heafield (2018) leveraged a multisource NMT system to learn to translate from both unparsed and parsed source sentences. Wu et al. (2018) similarly combined the standard bidirectional encoder with two additional encoders, one Related Work The performance of many RNN-based NMT paradigms has been improved by adding explicit syntactic annotations, particularly on the source side; we review some syntactic NMT models here. This paper is, along with Wu et al. (2018) and Zhang et al. (2019), among the first to add explicit syntax to Transformer-based NMT. 7.1 Syntactic NMT with Modified Encoder Linearized Parses in"
W19-5203,W17-4707,0,0.0770976,"Missing"
W19-5203,P16-1078,0,0.0627337,"<PA> you have not been elected . ... ... (a) Multi-task syntactic NMT model. The system is trained to translate (<TR>) and parse (<PA>) source sentences using the same architecture. (b) Mixed encoder syntactic NMT model. The system learns to translate directly from both parsed and unparsed source sentences into unparsed target sentences. Figure 1: Illustrations of the two proposed syntactic NMT methods. constituency parser (Manning et al., 2014) to parse the source side of the parallel corpus. This technique of parsing the parallel data instead of using gold parses is common in syntactic NMT (Eriguchi et al., 2016) and in neural parsing (Vinyals et al., 2015). For the multi-task model, it would be possible to incorporate gold parses into training as well, but we leave this for future work. to NMT that are straightforward to incorporate in practice • We empirically evaluate both methods on translation from English into 21 diverse target languages, finding that the multi-task method improves consistently over a nonsyntactic baseline 2 Transformer-Based NMT with Linearized Parses 2. We linearize the resulting parses similarly to Vinyals et al. (2015) by using a depthfirst tree traversal. We tokenize the op"
W19-5203,W18-5431,0,0.0366358,"ty of Edinburgh kheafiel@ed.ac.uk Abstract in low-resource scenarios, and adding syntax to Transformer-based NMT is currently an underexplored research area. Transformer-based NMT may in fact stand to benefit even more from explicit syntactic annotations than RNN-based NMT, particularly in lowresource settings. On the one hand, the Transformer model already learns some syntax without explicit supervision in high-resource cases. Vaswani et al. (2017) visualized a few encoder self-attentions in a trained NMT model and found that they seemed to capture syntactic structure. This was formalized by Raganato and Tiedemann (2018), who found that Transformer encoders trained on high-resource NMT tasks were able to perform reasonably well at part-of-speech tagging, chunking, and other tasks. However, for Transformers trained on low-resource NMT, the results on these tasks were not as strong. Additionally, Tran et al. (2018) found that an RNN language model did better at predicting subject-verb agreement than a Transformer language model; Tang et al. (2018) saw similar results for Transformer vs. RNN NMT models. Thus, the goal of this paper is to improve Transformer-based NMT using source-side syntactic supervision. We p"
W19-5203,E17-3017,0,0.0333409,"tence format. Inference on unparsed source sentences is slightly faster (since it does not require parsing of the source sentence) and achieves slightly higher BLEU scores, so we show results using unparsed source sentences for our experiments (sections 4.2 and 5.2). 26 3 Family Baltic Experimental Setup We evaluate our multi-task and mixed encoder models compared to a standard (non-syntactic) Transformer baseline on translation from English into 21 target languages. Sections 4.1 and 5.1 contain detailed information on the target languages and data used. All models are implemented in Sockeye (Hieber et al., 2017). For hyperparameter settings, we follow the recommendations of Vaswani et al. (2017). We preprocess our data for all experiments as follows. First, we tokenize and truecase the data using the Moses scripts (Koehn et al., 2007). We then train separate subword vocabularies (Sennrich et al., 2016) for the source and target languages, with 30k merge operations per language. We use the Stanford CoreNLP parser (Manning et al., 2014) to generate constituency parses of the source (English) sentences, and linearize and format the parses as described in section 2.1. We do not use any monolingual traini"
W19-5203,P16-1162,0,0.492306,"ilingual NMT. Table 1 gives an example of the data format. Finally, we shuffle the parsing and translation training data together and train the shared encoder and decoder on both tasks, making no further distinction between the tasks during training. Since we parse all of the training data, each source sentence appears twice: once with a target language sentence and once with a parse of the source sentence. These copies are shuffled separately. ing by Vinyals et al. (2015). 5. For our mixed encoder model (section 2.3), we convert the words in the parses into subwords using byte pair encoding (Sennrich et al., 2016). We do not allow the parse labels to be broken into subwords. Tables 1 and 2 give examples of the resulting parse formats. 2.2 Multi-Task NMT and Parsing with Shared Decoder 2.3 Our first method for incorporating source-side syntax into Transformer-based NMT adopts a multi-task framework. The main task is translating the source sentence into the target language; the secondary task is parsing the source sentence. For the parsing task, we employ the same encoderdecoder framework as for NMT, with the sequential source sentence as input and the linearized, unlexicalized parsed source sentence as"
W19-5203,P15-1150,0,0.0376924,"guage pair did not have the same issue as EN→DE: the parses generated for each sentence were different, and a manual analysis indicated that the generated EN→RO parses were reasonable. The EN→TR system generated a large amount of valid parses, but fewer than the EN→RO system; it is possible that the EN→TR system would have done better with more training data. 7 7.2 There have been several recent proposals to incorporate source-side syntax into RNN-based NMT by modifying the encoder architecture; we review some such models here. Eriguchi et al. (2016) augmented the RNN encoder with a treeLSTM (Tai et al., 2015) to read in source-side HPSG parses, and combined this with a standard RNN decoder. Similarly, Bastings et al. (2017) used a graph convolutional encoder in combination with an RNN decoder to translate from dependency parsed source sentences. Although these models improved over non-syntactic RNN-based NMT systems, they relied heavily on parsed data during both training and inference, whereas our models are able to translate unparsed data. In addition, it is not clear how to incorporate such improvements into the state-of-the-art Transformer architecture. 7.3 Linearized Parses in NMT This work f"
W19-5203,Q18-1017,0,0.0196857,"they relied heavily on parsed data during both training and inference, whereas our models are able to translate unparsed data. In addition, it is not clear how to incorporate such improvements into the state-of-the-art Transformer architecture. 7.3 Linearized Parses in NMT This work fits with another line of research that uses linearized parses to incorporate syntax into neural machine translation without requiring a specific NMT architecture. Luong et al. (2016) used a single encoder and different decoders to train two tasks: parsing the source sentence and translating from source to target. Kiperwasser and Ballesteros (2018) also applied multi-task learning to syntactic NMT; they used a shared RNN decoder for translation, dependency parsing, and part-of-speech tagging and evaluated different scheduling techniques to combine the tasks. Our multi-task system builds off these two papers by training a joint NMT and parsing model using a single encoder and decoder in a Transformer framework, and further evaluates the multi-task framework on several language pairs. Currey and Heafield (2018) leveraged a multisource NMT system to learn to translate from both unparsed and parsed source sentences. Wu et al. (2018) similar"
W19-5203,2005.mtsummit-papers.11,0,0.0283271,"wedish Greek French Italian Portuguese Romanian Spanish Bulgarian Czech Polish Slovak Slovene Estonian Finnish Hungarian Abbrev. LV LT DA NL DE SV EL FR IT PT RO ES BG CS PL SK SL ET FI HU Table 3: Target languages used in our experiments, along with their language families or branches and their abbreviations (abbrev.). random subset of 10k sentences from the original data to use as development data and an additional 10k sentences as test data; these development and test sets are not included in the training data. Small-Scale Cross-Lingual Experiments Data We use the Europarl Parallel Corpus (Koehn, 2005) as the basis for our small-scale cross-lingual experiments. We consider translation from English (EN) into each of the twenty remaining target languages; Table 3 contains a full list of the target languages, as well as their language families or branches. By using this data set, we are able to evaluate the usefulness of syntactic information for several relatively diverse target languages, unlike most previous work on syntactic NMT (reviewed in section 7). However, all the languages in our experiments are Indo-European or Uralic due to using Europarl. In order to facilitate comparison between"
W19-5203,D18-1458,0,0.128577,"Missing"
W19-5203,D18-1503,0,0.0821221,"Missing"
W19-5203,P16-5005,0,0.0612378,"Missing"
W19-5203,N19-1118,0,0.115597,"coder and decoder in a Transformer framework, and further evaluates the multi-task framework on several language pairs. Currey and Heafield (2018) leveraged a multisource NMT system to learn to translate from both unparsed and parsed source sentences. Wu et al. (2018) similarly combined the standard bidirectional encoder with two additional encoders, one Related Work The performance of many RNN-based NMT paradigms has been improved by adding explicit syntactic annotations, particularly on the source side; we review some syntactic NMT models here. This paper is, along with Wu et al. (2018) and Zhang et al. (2019), among the first to add explicit syntax to Transformer-based NMT. 7.1 Syntactic NMT with Modified Encoder Linearized Parses in Neural Networks In this work, we use linearized parse trees to add syntax into the Transformer. Vinyals et al. (2015) and Choe and Charniak (2016) introduced the idea 30 mixed encoder model did only marginally better than the non-syntactic baseline. In the future, we plan on extending these techniques to incorporate target-side syntax into Transformer-based NMT. In addition, we would like to experiment with different source languages in order to find out whether addin"
