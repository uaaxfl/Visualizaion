2020.blackboxnlp-1.3,C14-1089,0,0.0297408,"layers of contextualized language models. These observations allow us to investigate the mechanisms of neural LMs to better understand the 16 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 16–32 c Online, November 20, 2020. 2020 Association for Computational Linguistics degree to which they encode linguistic knowledge. We demonstrate how discourse-level features can be queried and analyzed from neural LMs. All of our code and parsed tree data will be available at github. (Wang et al., 2019; Zhang, 2013), examine linguistic coherence (Feng et al., 2014; Abdalla et al., 2017), and to analyze arguments (Chakrabarty et al., 2019). In this project, we extract similar RST features in the following three categories: 2 Discourse relation occurrences (Sig) We include the number of relations detected in each document. There are 18 relations in this category1 . Unfortunately, the relations adopted by open-source RST parsers are not unified. To allow for comparison against other parsers, we do not differentiate subtle differences between relations, therefore grouping very similar relations, following the approach in (Feng and Hirst, 2012). (E.g., we c"
2020.blackboxnlp-1.3,W18-5426,0,0.151734,"verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students (Mohsen and Alshahrani, 2019; Liu and Kunnan, 2016; Zhang, 2013; Powers et al., 2001), and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts. Recent work has started to evaluate encoded features from hidden representations. Among them, probing (Alain and Bengio, 2017; Adi et al., 2017) has been a popular choice. Previous work probed morphological (Belinkov et al., 2017; Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019; Hewitt and Liang, 2019). Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations. In this work, we use a probe containing self attention mechanism. We first project the variablelength embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters,"
2020.blackboxnlp-1.3,P17-1080,0,0.115944,"o quantify the extent they encode rhetorical knowledge. To verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students (Mohsen and Alshahrani, 2019; Liu and Kunnan, 2016; Zhang, 2013; Powers et al., 2001), and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts. Recent work has started to evaluate encoded features from hidden representations. Among them, probing (Alain and Bengio, 2017; Adi et al., 2017) has been a popular choice. Previous work probed morphological (Belinkov et al., 2017; Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019; Hewitt and Liang, 2019). Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations. In this work, we use a probe containing self attention mechanism. We first project the variablelength embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representatio"
2020.blackboxnlp-1.3,D18-1313,0,0.126902,"hey encode rhetorical knowledge. To verify our hypothesis, we hand-craft a set of 24 rhetorical features including those used to examine rhetorical capacities of students (Mohsen and Alshahrani, 2019; Liu and Kunnan, 2016; Zhang, 2013; Powers et al., 2001), and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts. Recent work has started to evaluate encoded features from hidden representations. Among them, probing (Alain and Bengio, 2017; Adi et al., 2017) has been a popular choice. Previous work probed morphological (Belinkov et al., 2017; Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019; Hewitt and Liang, 2019). Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations. In this work, we use a probe containing self attention mechanism. We first project the variablelength embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe r"
2020.blackboxnlp-1.3,D19-1275,0,0.0442191,"g those used to examine rhetorical capacities of students (Mohsen and Alshahrani, 2019; Liu and Kunnan, 2016; Zhang, 2013; Powers et al., 2001), and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts. Recent work has started to evaluate encoded features from hidden representations. Among them, probing (Alain and Bengio, 2017; Adi et al., 2017) has been a popular choice. Previous work probed morphological (Belinkov et al., 2017; Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019; Hewitt and Liang, 2019). Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations. In this work, we use a probe containing self attention mechanism. We first project the variablelength embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters, and enable us to better understand each model’s ability to encode rhetoric"
2020.blackboxnlp-1.3,Q17-1010,0,0.012569,"than other groups. Nevertheless, the results on all features correlate more to the Sig features. Non-contextualized word embeddings We consider two popular word embeddings here: How about averaging layers? For comparison, we also used the mean of all 12 layers for each neural LM. Figure 5 shows the probing results. Except GPT-2, other LMs show similar performances when the representations of layers are averaged. In • GloVe (Pennington et al., 2014) contains 2.2M vocabulary items and produces 300dimensional word vectors. The GloVe embedding we use is pretrained on Common Crawl. 21 • FastText (Bojanowski et al., 2017) is trained on Wikipedia 2017 + UMBC (16B tokens) including subword information, and produces 300-dimensional word vectors. Config FastText GloVe RandEmbed RandGuess Word embeddings map each token into a Ddimensional semantic space. Therefore, for a document of length L, the embedded matrix also has shape L × D. The difference from the contextualized neural LMs is that, the D-dimensional vectors of every word do not depend on their contexts. RST Feature Set EDU Sig .7215 .6911 .7142 .7166 .7365 .7077 128.9 3.1 Tree .6889 .6942 .7034 6799.0 Table 2: Comparison between RST probing losses of non-"
2020.blackboxnlp-1.3,W01-1605,0,0.33011,"dimension from D (e.g., D = 768 for BERT and 2048 for XLM) to a much smaller one, d. Then, we use self attention similar to Lin et al. (2017) to collect the information spread across the document to a condensed form: 3 Experiments 3.1 Data Most state-of-the-art rhetorical parsers are trained on either Penn Discourse Treebank (Ji and Eisenstein, 2014; Feng and Hirst, 2012) or RST-DT (Feng and Hirst, 2014; Joty et al., 2015; Surdeanu et al., 2015; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018). Although the documents contain accurate discourse annotations, RST-DT (Carlson et al., 2001) only has 385 documents. The Penn Discourse Treebank (Prasad et al., 2008) has 2,159 documents but their annotations do not follow the RST framework. So in addition to RST-DT, we extend the analysis to a 100 times larger dataset, IMDB (Maas et al., 2011). IMDB contains 50,000 movie reviews without discourse annotations. In these reviews, the authors explain and elaborate upon their opinions towards certain movies and give ratings. We removed html tags, and attempt to parse all of them (i.e., both train and test data) using a two-pass parser from Feng and Hirst (2014). We discarded 1,977 docume"
2020.blackboxnlp-1.3,N19-1419,0,0.422273,"etorical features including those used to examine rhetorical capacities of students (Mohsen and Alshahrani, 2019; Liu and Kunnan, 2016; Zhang, 2013; Powers et al., 2001), and evaluate how well neural LMs encode these rhetorical features in the representations while encoding texts. Recent work has started to evaluate encoded features from hidden representations. Among them, probing (Alain and Bengio, 2017; Adi et al., 2017) has been a popular choice. Previous work probed morphological (Belinkov et al., 2017; Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019; Hewitt and Liang, 2019). Probing involves optimizing a simple projection model from representations to features. The loss of this optimization measures the difficulty to decode features from the representations. In this work, we use a probe containing self attention mechanism. We first project the variablelength embeddings to a fixed-length latent representation per document. Then, we apply a simple diagnostic classifier to detect rhetorical features from this latent representation. This design of probe reduces the total number of parameters, and enable us to better understand each model’s a"
2020.blackboxnlp-1.3,D19-1291,0,0.0679886,"s to investigate the mechanisms of neural LMs to better understand the 16 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 16–32 c Online, November 20, 2020. 2020 Association for Computational Linguistics degree to which they encode linguistic knowledge. We demonstrate how discourse-level features can be queried and analyzed from neural LMs. All of our code and parsed tree data will be available at github. (Wang et al., 2019; Zhang, 2013), examine linguistic coherence (Feng et al., 2014; Abdalla et al., 2017), and to analyze arguments (Chakrabarty et al., 2019). In this project, we extract similar RST features in the following three categories: 2 Discourse relation occurrences (Sig) We include the number of relations detected in each document. There are 18 relations in this category1 . Unfortunately, the relations adopted by open-source RST parsers are not unified. To allow for comparison against other parsers, we do not differentiate subtle differences between relations, therefore grouping very similar relations, following the approach in (Feng and Hirst, 2012). (E.g., we consider both Topic-Shift and Topic-Drift to be a Topic-Change). Specifically"
2020.blackboxnlp-1.3,P19-1285,0,0.0668361,"Missing"
2020.blackboxnlp-1.3,P14-1002,0,0.0306433,"ficulty = E WpT A˜ − v m Our probing method contains two weight parameters, Wd and Wp . First, we embed a document with L tokens using a neural LM with D dimensions to get a raw representation matrix X ∈ RL×D . We use a projection matrix Wd ∈ RD×d to reduce the embedding dimension from D (e.g., D = 768 for BERT and 2048 for XLM) to a much smaller one, d. Then, we use self attention similar to Lin et al. (2017) to collect the information spread across the document to a condensed form: 3 Experiments 3.1 Data Most state-of-the-art rhetorical parsers are trained on either Penn Discourse Treebank (Ji and Eisenstein, 2014; Feng and Hirst, 2012) or RST-DT (Feng and Hirst, 2014; Joty et al., 2015; Surdeanu et al., 2015; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018). Although the documents contain accurate discourse annotations, RST-DT (Carlson et al., 2001) only has 385 documents. The Penn Discourse Treebank (Prasad et al., 2008) has 2,159 documents but their annotations do not follow the RST framework. So in addition to RST-DT, we extend the analysis to a 100 times larger dataset, IMDB (Maas et al., 2011). IMDB contains 50,000 movie reviews without discourse annotations. In these"
2020.blackboxnlp-1.3,J15-3002,0,0.0242348,"nd Wp . First, we embed a document with L tokens using a neural LM with D dimensions to get a raw representation matrix X ∈ RL×D . We use a projection matrix Wd ∈ RD×d to reduce the embedding dimension from D (e.g., D = 768 for BERT and 2048 for XLM) to a much smaller one, d. Then, we use self attention similar to Lin et al. (2017) to collect the information spread across the document to a condensed form: 3 Experiments 3.1 Data Most state-of-the-art rhetorical parsers are trained on either Penn Discourse Treebank (Ji and Eisenstein, 2014; Feng and Hirst, 2012) or RST-DT (Feng and Hirst, 2014; Joty et al., 2015; Surdeanu et al., 2015; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018). Although the documents contain accurate discourse annotations, RST-DT (Carlson et al., 2001) only has 385 documents. The Penn Discourse Treebank (Prasad et al., 2008) has 2,159 documents but their annotations do not follow the RST framework. So in addition to RST-DT, we extend the analysis to a 100 times larger dataset, IMDB (Maas et al., 2011). IMDB contains 50,000 movie reviews without discourse annotations. In these reviews, the authors explain and elaborate upon their opinions towards ce"
2020.blackboxnlp-1.3,N19-1199,1,0.817487,"of an EDU in an article is similar to that of a word in a sentence. • Each non-leaf node denotes a relation involving its two children. Often, one of the children is more dependent on the other, and less essential to the writer’s purpose. This child is referred to as “satellite”, while the more central child is the “nucleus”. Tree property features (Tree) We compute the depth and the Yngve depth (the number of rightbranching in the tree) (Yngve, 1960) of each tree node, and include their mean and variance as characteristic features, following previous work extracting tree linguistic features (Li et al., 2019; Zhu et al., 2019). NS-Contrast SN-Attribution I didn’t know EDU related features (EDU) We include the mean and variance of EDU lengths of each document. We hypothesize the longer EDUs indicate higher levels of redundancy in discourse, hence extracting rhetorical features require memory across longer spans. Overall, there are 24 features from three categories. We normalize them to zero mean and unit variance, and take these RST features for probing. The features are not independent of each other. Specifically, the features of each group tend to describe the same property from different aspect"
2020.blackboxnlp-1.3,W16-2812,0,0.0160675,"lying the LM. • Only 12-layer LMs are involved, to compare across various layers fairly. But our approach would be applicable to 3-layer ELMo and deeper LMs as well. Appropriate statistical controls would naturally need to be applied. • Not all documents can be analyzed. First, documents longer than 512 tokens cannot be encoded into one vector in our probing model. Second, while RST provides elegant frameworks for analyzing rhetorical structures of discourse, in practice, the RST pipeline does not guarantee a successful analysis for an arbitrary document scraped online. 5 tures of monologues (Peldszus and Stede, 2016) and, when used with other discourse features, RST can improve role-labelling in online arguments (Chakrabarty et al., 2019). Probing neural LMs is an emergent diagnostic task on those models. Previous work probed morphological (Bisazza and Tump, 2018), agreement (Giulianelli et al., 2018), and syntactic features (Hewitt and Manning, 2019). Hewitt and Liang (2019) compared different probes, and recommended linear probes with as few parameters as possible, for the purpose of reducing overfitting. Recently, Pimentel et al. (2020) argued against this choice from an information-theoretic point of"
2020.blackboxnlp-1.3,D16-1035,0,0.0203031,"with D dimensions to get a raw representation matrix X ∈ RL×D . We use a projection matrix Wd ∈ RD×d to reduce the embedding dimension from D (e.g., D = 768 for BERT and 2048 for XLM) to a much smaller one, d. Then, we use self attention similar to Lin et al. (2017) to collect the information spread across the document to a condensed form: 3 Experiments 3.1 Data Most state-of-the-art rhetorical parsers are trained on either Penn Discourse Treebank (Ji and Eisenstein, 2014; Feng and Hirst, 2012) or RST-DT (Feng and Hirst, 2014; Joty et al., 2015; Surdeanu et al., 2015; Heilman and Sagae, 2015; Li et al., 2016; Wang et al., 2017; Yu et al., 2018). Although the documents contain accurate discourse annotations, RST-DT (Carlson et al., 2001) only has 385 documents. The Penn Discourse Treebank (Prasad et al., 2008) has 2,159 documents but their annotations do not follow the RST framework. So in addition to RST-DT, we extend the analysis to a 100 times larger dataset, IMDB (Maas et al., 2011). IMDB contains 50,000 movie reviews without discourse annotations. In these reviews, the authors explain and elaborate upon their opinions towards certain movies and give ratings. We removed html tags, and attempt"
2020.blackboxnlp-1.3,D14-1162,0,0.0837197,"tures are comparably easier to probe, whereas the Sig feature groups is more challenging. However, GPT-2, XLNet, and XLM do not regard EDU or Tree features easier to probe than other groups. Nevertheless, the results on all features correlate more to the Sig features. Non-contextualized word embeddings We consider two popular word embeddings here: How about averaging layers? For comparison, we also used the mean of all 12 layers for each neural LM. Figure 5 shows the probing results. Except GPT-2, other LMs show similar performances when the representations of layers are averaged. In • GloVe (Pennington et al., 2014) contains 2.2M vocabulary items and produces 300dimensional word vectors. The GloVe embedding we use is pretrained on Common Crawl. 21 • FastText (Bojanowski et al., 2017) is trained on Wikipedia 2017 + UMBC (16B tokens) including subword information, and produces 300-dimensional word vectors. Config FastText GloVe RandEmbed RandGuess Word embeddings map each token into a Ddimensional semantic space. Therefore, for a document of length L, the embedded matrix also has shape L × D. The difference from the contextualized neural LMs is that, the D-dimensional vectors of every word do not depend on"
2020.blackboxnlp-1.3,N19-1112,0,0.314828,"ities of neural LMs. 1 Introduction In recent years, neural LMs (especially contextualized LMs) have shown profound abilities to generate texts that could be almost indistinguishable from human writings (Radford et al., 2019). Neural LMs could be used to generate concise summaries (Song et al., 2019), coherent stories (See et al., 2019), and complete documents given prompts (Keskar et al., 2019). It is natural to question their source and extent of rhetorical knowledge: What makes neural LMs articulate, and how? While some recent works query the linguistic knowledge (Hewitt and Manning, 2019; Liu et al., 2019a; Chen et al., 2019; Belinkov et al., 2017), this open question remain unanswered. We hypothesize that contextualized neural LMs encode rhetorical knowledge in their intermediate repre• The BERT-based LMs encode more rhetorical features, and in a more stable manner, than other models. • The semantics of non-contextualized embeddings also pertain to some rhetorical features, but less than most layers of contextualized language models. These observations allow us to investigate the mechanisms of neural LMs to better understand the 16 Proceedings of the Third BlackboxNLP Workshop on Analyzing an"
2020.blackboxnlp-1.3,P19-1282,0,0.0246693,"s quantitatively describing rhetorical capacities of neural language models based on unlabeled, target-domain corpus. This method may be used for selecting suitable LMs in tasks including rhetorical acts classifications, discourse modeling, and response generation. Related work Recent work has considered the interpretability of contextualized representations. For example, Jain and Wallace (2019) found attention to be uncorrelated to gradient-based feature importance, while Wiegreffe and Pinter (2019) suggested such approaches allowed too much flexibility to give convincing results. Similarly, Serrano et al. (2019) considered attention representations to be noisy indicators of feature importance. Many tasks in argument mining, similar to our task of examining neural LMs, require understanding the rhetorical aspects of discourse (Lawrence and Reed, 2019). This allows RST to be applied in relevant work. For example, RST enables understanding and analyzing argument struc23 Acknowledgement Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL. Association for Computational Linguistics. We thank the a"
2020.blackboxnlp-1.3,N19-1146,1,0.813651,"rticle is similar to that of a word in a sentence. • Each non-leaf node denotes a relation involving its two children. Often, one of the children is more dependent on the other, and less essential to the writer’s purpose. This child is referred to as “satellite”, while the more central child is the “nucleus”. Tree property features (Tree) We compute the depth and the Yngve depth (the number of rightbranching in the tree) (Yngve, 1960) of each tree node, and include their mean and variance as characteristic features, following previous work extracting tree linguistic features (Li et al., 2019; Zhu et al., 2019). NS-Contrast SN-Attribution I didn’t know EDU related features (EDU) We include the mean and variance of EDU lengths of each document. We hypothesize the longer EDUs indicate higher levels of redundancy in discourse, hence extracting rhetorical features require memory across longer spans. Overall, there are 24 features from three categories. We normalize them to zero mean and unit variance, and take these RST features for probing. The features are not independent of each other. Specifically, the features of each group tend to describe the same property from different aspects.2 but it is very"
2020.blackboxnlp-1.3,N15-3001,0,0.0540656,"Missing"
2020.clinicalnlp-1.33,D19-1678,0,0.025811,"s: equalized-odds through post-processing, which is agnostic to the underlying classifier, and word embedding debiasing which is a text-specific technique. We show that ensembling classifiers trained on structured and unstructured data, along with the aforementioned fairness algorithms, can both improve performance and mitigate unfairness relative to their constituent components. We also achieve strong results on several MIMIC-III clinical benchmark prediction tasks using a dual modality ensemble; these results may be of broader interest in clinical machine learning (Harutyunyan et al., 2019; Khadanga et al., 2019). 2 2.1 Background Combining Text and Tabular Data in Clinical Machine Learning Prior work has shown that combining unstructured text with vital sign time series data improves performance on clinical prediction tasks. Horng et al. (2017) showed that augmenting an SVM with text information in addition to vital signs data improved retrospective sepsis detection. Akbilgic et al. (2019) showed that using a text-based risk score improves performance on prediction of death after surgery for a pediatric dataset. Closest to our work, Khadanga et al. (2019) introduced a jointmodality neural network whi"
2020.emnlp-main.115,N19-1423,0,0.373104,"extual data only, and to emphasize the practicality of this model in real-world applications. Moreover, we use canonical correlation analysis (CCA; Hotelling 1992) to explore relationships between latent features learned from both structured and unstructured data. Finally, we propose an evaluation protocol to examine the usability of our model as an interpretable decision support tool. 2 2.1 Related work Transformers in the clinical domain Transformers (Vaswani et al., 2017) have gained popularity given their strong performance and parallelizability. The success of the transformer-based BERT (Devlin et al., 2019) has inspired numerous studies to apply it in various domains. For example, BioBERT was pretrained on PubMed abstracts and articles and was able to better identify biomedical entities and boundaries than base BERT (Lee et al., 2020). Alsentzer et al. (2019) further fine-tuned 1478 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1478–1489, c November 16–20, 2020. 2020 Association for Computational Linguistics BioBERT on the MIMIC-III clinical dataset (Johnson et al., 2016) and released the model as ClinicalBERT. We use these pretrained BERT-based mo"
2020.emnlp-main.115,N19-1357,0,0.0440092,"ord embeddings to learn task-specific representations spanning long documents. 2.2 Language model explainability Explainable AI is an emerging field with no standardized methodology or evaluation metrics. The definition of model explainability also varies by application; however, a generally accepted approach to language model explainability is through extractive rationales (Lei et al., 2016; Mullenbach et al., 2018; Wiegreffe and Pinter, 2019). The wide application of attention mechanisms has led to an ongoing debate over whether attention can be used as explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Jain and Wallace (2019) claimed that attention scores in recurrent neural networks (RNNs) did not correlate with other feature-importance measures, and adversarial attentions did not affect model predictions, concluding that attention was not explanation. Wiegreffe and Pinter (2019) challenged these assumptions by proposing diagnostic tests that allow for meaningful interpretation of attention, but also showed that adversarial attention distributions failed to achieve the same level of prediction performance as real model attention. We propose a clinical decision"
2020.emnlp-main.115,D16-1011,0,0.0202684,"tional Linguistics BioBERT on the MIMIC-III clinical dataset (Johnson et al., 2016) and released the model as ClinicalBERT. We use these pretrained BERT-based models as static feature extractors and build layers upon the word embeddings to learn task-specific representations spanning long documents. 2.2 Language model explainability Explainable AI is an emerging field with no standardized methodology or evaluation metrics. The definition of model explainability also varies by application; however, a generally accepted approach to language model explainability is through extractive rationales (Lei et al., 2016; Mullenbach et al., 2018; Wiegreffe and Pinter, 2019). The wide application of attention mechanisms has led to an ongoing debate over whether attention can be used as explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Jain and Wallace (2019) claimed that attention scores in recurrent neural networks (RNNs) did not correlate with other feature-importance measures, and adversarial attentions did not affect model predictions, concluding that attention was not explanation. Wiegreffe and Pinter (2019) challenged these assumptions by proposing diagnostic test"
2020.emnlp-main.115,N18-1100,0,0.161397,"BioBERT on the MIMIC-III clinical dataset (Johnson et al., 2016) and released the model as ClinicalBERT. We use these pretrained BERT-based models as static feature extractors and build layers upon the word embeddings to learn task-specific representations spanning long documents. 2.2 Language model explainability Explainable AI is an emerging field with no standardized methodology or evaluation metrics. The definition of model explainability also varies by application; however, a generally accepted approach to language model explainability is through extractive rationales (Lei et al., 2016; Mullenbach et al., 2018; Wiegreffe and Pinter, 2019). The wide application of attention mechanisms has led to an ongoing debate over whether attention can be used as explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Jain and Wallace (2019) claimed that attention scores in recurrent neural networks (RNNs) did not correlate with other feature-importance measures, and adversarial attentions did not affect model predictions, concluding that attention was not explanation. Wiegreffe and Pinter (2019) challenged these assumptions by proposing diagnostic tests that allow for meaningf"
2020.emnlp-main.115,D19-1002,0,0.0975151,"clinical dataset (Johnson et al., 2016) and released the model as ClinicalBERT. We use these pretrained BERT-based models as static feature extractors and build layers upon the word embeddings to learn task-specific representations spanning long documents. 2.2 Language model explainability Explainable AI is an emerging field with no standardized methodology or evaluation metrics. The definition of model explainability also varies by application; however, a generally accepted approach to language model explainability is through extractive rationales (Lei et al., 2016; Mullenbach et al., 2018; Wiegreffe and Pinter, 2019). The wide application of attention mechanisms has led to an ongoing debate over whether attention can be used as explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Jain and Wallace (2019) claimed that attention scores in recurrent neural networks (RNNs) did not correlate with other feature-importance measures, and adversarial attentions did not affect model predictions, concluding that attention was not explanation. Wiegreffe and Pinter (2019) challenged these assumptions by proposing diagnostic tests that allow for meaningful interpretation of attentio"
2020.emnlp-main.115,P19-1282,0,0.0212663,"d build layers upon the word embeddings to learn task-specific representations spanning long documents. 2.2 Language model explainability Explainable AI is an emerging field with no standardized methodology or evaluation metrics. The definition of model explainability also varies by application; however, a generally accepted approach to language model explainability is through extractive rationales (Lei et al., 2016; Mullenbach et al., 2018; Wiegreffe and Pinter, 2019). The wide application of attention mechanisms has led to an ongoing debate over whether attention can be used as explanation (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Jain and Wallace (2019) claimed that attention scores in recurrent neural networks (RNNs) did not correlate with other feature-importance measures, and adversarial attentions did not affect model predictions, concluding that attention was not explanation. Wiegreffe and Pinter (2019) challenged these assumptions by proposing diagnostic tests that allow for meaningful interpretation of attention, but also showed that adversarial attention distributions failed to achieve the same level of prediction performance as real model attention. We pro"
2020.emnlp-main.403,P19-1285,0,0.0638637,"Missing"
2020.emnlp-main.403,D16-1264,0,0.0476652,"s in a multi-task pre-training framework provides better results than using any single auxiliary task. Using these methods, we outperform BERTBase on the GLUE benchmark using fewer than a quarter of the training tokens. 2 1 Introduction When Devlin et al. (2018) released BERT, a transformer network (Vaswani et al., 2017) trained using a ‘masked language model’ (MLM) task and a ‘next sentence prediction’ (NSP), it redefined the NLP landscape, establishing itself as the state-of-the-art (SoTA) on many natural language understanding (NLU) benchmarks including the GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018) benchmarks. Many models inspired by BERT have since surpassed its performance. However, in contrast to the original BERT paper, many obtained better results by excluding the NSP task. Some, such as XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019), rely solely on a MLM variant, while others (Wang Related work As with most deep learning, language representations require large datasets. While there exists corpora of labelled text, the vast majority of language data exists as raw, unlabelled text. Accordingly, many language embedding methods, and all those"
2020.emnlp-main.403,N16-1162,0,0.0383258,"Missing"
2020.emnlp-main.403,2021.ccl-1.108,0,0.113277,"Missing"
2020.emnlp-main.403,W18-5446,0,0.0691755,"Missing"
2020.emnlp-main.403,D18-1009,0,0.020358,"ework provides better results than using any single auxiliary task. Using these methods, we outperform BERTBase on the GLUE benchmark using fewer than a quarter of the training tokens. 2 1 Introduction When Devlin et al. (2018) released BERT, a transformer network (Vaswani et al., 2017) trained using a ‘masked language model’ (MLM) task and a ‘next sentence prediction’ (NSP), it redefined the NLP landscape, establishing itself as the state-of-the-art (SoTA) on many natural language understanding (NLU) benchmarks including the GLUE (Wang et al., 2018), SQuAD (Rajpurkar et al., 2016), and SWAG (Zellers et al., 2018) benchmarks. Many models inspired by BERT have since surpassed its performance. However, in contrast to the original BERT paper, many obtained better results by excluding the NSP task. Some, such as XLNET (Yang et al., 2019) and RoBERTa (Liu et al., 2019), rely solely on a MLM variant, while others (Wang Related work As with most deep learning, language representations require large datasets. While there exists corpora of labelled text, the vast majority of language data exists as raw, unlabelled text. Accordingly, many language embedding methods, and all those described below, rely solely on"
2020.emnlp-main.744,C18-1139,0,0.0166452,"task and control function, respectively). Overall, we sweep the probe model hyperparameters with a unified training scheme on three tasks (probe, control task, control function). The control task (function) setting includes labels (embeddings) drawn from a uniform random sample once before all experiments. In each training, we follow the setting of (Hewitt and Liang, 2019). We save the model with the best dev loss, report the test set loss and accuracy, and average across 4 different random seeds. Data We use the Universal Dependency (Zeman et al., 2019) dataset loaded with the Flair toolkit (Akbik et al., 2018). We examine three languages: English, French, and Spanish. For the probing task, we use POS with labels provided by SpaCy3 . We use the embedding of multilingual BERT (mBERT) implemented by huggingface (Wolf et al., 2019). If a word is split into multiple word pieces, we average its representations. 4.2 The “good probes” are good for both When measuring the qualities of probes using the “selectivity” (Hewitt and Liang, 2019) or “information gain” (Pimentel et al., 2020) criterion, we show that the rules-of-thumb for training good probes largely agree. • Early stopping before 24,000 gradient s"
2020.emnlp-main.744,Q16-1037,0,0.0308221,"tropy loss” – two very strong baselines. Overall, we recommend using control mechanisms to select probes, instead of relying on merely the probing task performance. When randomization is done well, controlling the target or representations are equivalent. 2 Related work Diagnostic probes were originally intended to explain information encoded in intermediate representations (Adi et al., 2017; Alain and Bengio, 2017; Belinkov et al., 2017). Recently, various probing tasks have queried the representations of, e.g., contextualized word embeddings (Tenney et al., 2019a,b) and sentence embeddings (Linzen et al., 2016; Chen et al., 2019; Alt et al., 2020; Kassner and Sch¨utze, 2020; Maudslay et al., 2020; Chi et al., 2020). The task of probing is usually formulated as a classifier problem, with the representations as input, and the features indicating information as output. A straightforward method to train a classifier is by minimizing cross entropy, which is the approach we follow. Note that Voita and Titov (2020) derived training objectives from minimum description lengths, resulting in cross entropy loss and some variants. 3 3.1 Information theoretic probes Formulation We adopt the information theoreti"
2020.emnlp-main.744,N19-1112,0,0.0382012,"ystems? An efficient approach to reveal information encoded in internal representations uses diagnostic classifiers (Alain and Bengio, 2017). Referred to as “probes”, diagnostic classifiers are trained on pre-computed intermediate representations of neural NLP systems. The performance on tasks they are trained to predict are used to evaluate the richness of the linguistic representation in encoding the probed tasks. Such tasks include probing syntax (Hewitt and Manning, 2019; Lin et al., 2019; Tenney et al., 2019a), semantics (Yaghoobzadeh et al., 2019), discourse features (Chen et al., 2019; Liu et al., 2019; Tenney et al., 2019b), and commonsense knowledge (Petroni et al., 2019; Poerner et al., 2019). However, appropriate criteria for selecting a good probe is under debate. The traditional view that high-accuracy probes are better is challenged by Hewitt and Liang (2019), who proposed that the high accuracy could be attributed to either (1) that the representation contains rich linguistic knowledge, or (2) that the probe learns the task. To circumvent this ambiguity, they proposed to use the improvement of probing task performance against a control task (predicting random labels from the same re"
2020.emnlp-main.744,D19-1250,0,0.0421622,"Missing"
2020.emnlp-main.744,2020.acl-main.420,0,0.0747951,"dge Institute, St Michael’s Hospital {zining,frank}@cs.toronto.edu Abstract There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier – or “probe” – to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either “the representation being rich in knowledge”, or “the probe learning the task”, which Pimentel et al. (2020) challenged. We show this dichotomy is valid informationtheoretically. In addition, we find that the methods to construct and select good probes proposed by the two papers, control task (Hewitt and Liang, 2019) and control function (Pimentel et al., 2020), are equivalent – the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other. 1 Introduction Recently, neural networks have shown substantial progress in NLP tasks (Devlin et al., 2019; Radford et al., 2019). To understand and explain thei"
2020.emnlp-main.744,2020.emnlp-main.14,0,0.0366487,"Missing"
2020.emnlp-main.744,P19-1574,0,0.0613069,"Missing"
2020.lrec-1.208,N01-1016,0,0.277175,"th the primary and collateral tracks can easily be extended to these disfluencies. We obtained the timings of the primary and collateral tracks by force-alignment at the phone level with the kaldi toolkit (Povey et al., 2011) with a HMM-GMM model. Figure 2 shows the distribution of disfluencies per speaker in the dataset. The total number of disfluencies and their types vary greatly across speakers. Word-based systems can incorporate both textual and acoustic features (See Table 3). As for textual features, we used token and span features which are common in the NLP community (Shriberg, 1994; Charniak and Johnson, 2001). As for acoustic/prosodic, we use summary statistics on duration energy and F0. All statistics and pooling are done using the timing alignment of each word wi . The semantic representation and part-of-speech tags are extracted with (Honnibal and Montani, 2017). The number of syllables and phones are extracted with (Bernard et al., 2019). Table 3: List of features in the word-based prediction. Type Core features and dimension Token semantic representation 384 part-of-speech (pos): pi 19 word position 1 Span wi == wi+k , k ∈ [−15, +15]∗ 30 pi == pi+k , k ∈ [−15, +15]∗ 30 wi , wi+1 == wi+k , wi+"
2020.lrec-1.208,N15-1029,0,0.0342091,"Missing"
2020.lrec-1.208,W15-5111,1,0.897307,"Missing"
2020.lrec-1.208,N18-1007,0,0.0427907,"Missing"
2020.sigmorphon-1.26,N06-1029,0,0.489136,", mid, and low level tones. Contours for the lexical tones in both languages are shown in Figure 1. 2 Related work syllables (Chen et al., 2016) as well as in continuous speech (Ryant et al., 2014b,a). Both of these models found that Mel-frequency cepstral coefficients (MFCCs) outperformed pitch contour features, despite the fact that MFCC features do not contain pitch information. In Cantonese, support vector machines (SVMs) have been applied to classify tones in continuous speech, using pitch contours as input (Peng and Wang, 2005). Unsupervised learning of tones remains largely unexplored. Levow (2006) performed unsupervised and semi-supervised tone clustering in Mandarin, using average pitch and slope as features, and k-means and asymmetric k-lines for clustering. Graph-based community detection techniques have been applied to group n-grams of contiguous contours into clusters in Mandarin (Zhang, 2019). In recent work concurrent to ours, Fry (2020) uses adversarial autoencoders and hierarchical clustering to identify tone inventories, and evaluate their method on Mandarin, Cantonese, Fungwa, and English data. We further explore unsupervised deep neural networks for phonemic tone clustering"
2020.sigmorphon-1.26,W19-4217,0,0.0235117,"atures, despite the fact that MFCC features do not contain pitch information. In Cantonese, support vector machines (SVMs) have been applied to classify tones in continuous speech, using pitch contours as input (Peng and Wang, 2005). Unsupervised learning of tones remains largely unexplored. Levow (2006) performed unsupervised and semi-supervised tone clustering in Mandarin, using average pitch and slope as features, and k-means and asymmetric k-lines for clustering. Graph-based community detection techniques have been applied to group n-grams of contiguous contours into clusters in Mandarin (Zhang, 2019). In recent work concurrent to ours, Fry (2020) uses adversarial autoencoders and hierarchical clustering to identify tone inventories, and evaluate their method on Mandarin, Cantonese, Fungwa, and English data. We further explore unsupervised deep neural networks for phonemic tone clustering. It should be noted that our unsupervised model is not given tone labels during training, and the number of tones is assumed to be unknown, so it cannot be directly compared to supervised tone classifiers in the literature. Many low-resource languages lack sufficient transcribed data for supervised speech"
2021.acl-long.325,W18-4603,0,0.138625,"le layers. Probing results are somewhat dependent on the choice of linguistic formalism used to annotate the data, as Kulmizev et al. (2020) found for syntax, and Kuznetsov and Gurevych (2020) found for seViolations of selectional restrictions are one type of linguistic unacceptability, defined as a semantic mismatch between a verb and an argument. Sasano and Korhonen (2020) examined the geometry of word classes (e.g., words that can be a direct object of the verb ‘play’) in word vector models; they compared single-class models against discriminative models for learning word class boundaries. Chersoni et al. (2018) tested distributional semantic models on their ability to identify selectional restriction violations using stimuli from two psycholinguistic datasets. Finally, Metheniti et al. (2020) tested how much BERT relies on selectional restriction information versus other contextual information for making masked word predictions. 2.4 Psycholinguistic tests for LMs The N400 response is a negative event-related potential that occurs roughly 400ms after a stimulus in human brains, and is generally associated with the stimulus being semantically anomalous with 4216 respect to the preceding context (Kutas"
2021.acl-long.325,2020.emnlp-main.13,0,0.023937,"yntactic, and the upper layers more when the task was semantic. Several papers found that BERT’s middle layers contain the most syntactic information. Kelly et al. (2020) found that BERT’s middle layers are best at distinguishing between sentences with direct and indirect object constructions. Hewitt and Manning (2019) used a structural probe to recover syntax trees from contextual embeddings, and found the performance peaked in middle layers. Probing results are somewhat dependent on the choice of linguistic formalism used to annotate the data, as Kulmizev et al. (2020) found for syntax, and Kuznetsov and Gurevych (2020) found for seViolations of selectional restrictions are one type of linguistic unacceptability, defined as a semantic mismatch between a verb and an argument. Sasano and Korhonen (2020) examined the geometry of word classes (e.g., words that can be a direct object of the verb ‘play’) in word vector models; they compared single-class models against discriminative models for learning word class boundaries. Chersoni et al. (2018) tested distributional semantic models on their ability to identify selectional restriction violations using stimuli from two psycholinguistic datasets. Finally, Methenit"
2021.acl-long.325,2021.ccl-1.108,0,0.0681273,"Missing"
2021.acl-long.325,2020.coling-main.109,0,0.0880783,"Missing"
2021.acl-long.325,2020.coling-main.65,0,0.0858812,"Missing"
2021.acl-long.325,2020.conll-1.53,0,0.0936782,"Missing"
2021.acl-long.325,K19-1008,0,0.0274021,"ound truth. More commonly, the model is presented with a binary choice between an acceptable and unacceptable sentence: BLiMP (Warstadt et al., 2020) used templates to generate 67k such sentence pairs, covering 12 types of linguistic phenomena. Similarly, Hu et al. (2020) created syntactic tests using templates, but defined success criteria using inequalities of LM perplexities. In contrast with artificial templates, Gulordava et al. (2018) generated test cases by perturbing natural corpus data to test long-distance dependencies. Most grammaticality studies focused on syntactic phenomena, but Rabinovich et al. (2019) tested LMs’ sensitivity to semantic infelicities involving indefinite pronouns. 2.1 Probing LMs for linguistic knowledge 2.3 Tests of selectional restrictions Soon after BERT’s release, many papers invented probing techniques to discover what linguistic knowledge it contains, and how this information is distributed between layers (e.g., Rogers et al. (2021) provides a comprehensive overview). Tenney et al. (2019) used “edge probing” to determine each layer’s contribution to a task’s performance, and discovered that the middle layers contributed more when the task was syntactic, and the upper"
2021.acl-long.325,2020.acl-main.240,0,0.112646,"Which layers are sensitive to anomaly? We vary L from 0 to 12 in all three models (Figure 2b). The layer with the highest accuracy differs between models: layer 9 has the highest accuracy for BERT, 11 for RoBERTa, and 6 for XLNet. All models experience a sharp drop in the last layer, likely because the last layer is specialized for the MLM pretraining objective. Comparisons to other models. Our bestperforming model is RoBERTa, with an accuracy of 0.830. This is slightly higher the best model reported in BLiMP (GPT-2, with accuracy 0.801). We do not claim to beat the state-of-the-art on BLiMP: Salazar et al. (2020) obtains a higher accuracy of 0.865 using RoBERTa-large. Even though the main goal of this paper is not to maximize accuracy on BLiMP, our Gaussian anomaly model is competitive with other transformer-based models on this task. In Appendix A, we explore variations of the Gaussian anomaly model, such as varying the type of covariance matrix, Gaussian mixture models, and one-class SVMs (Sch¨olkopf et al., 2000). However, none of these variants offer a significant improvement over a single Gaussian model with full covariance matrix. 3.3 Lower layers are sensitive to frequency We notice that surpri"
2021.acl-long.325,2020.acl-main.337,0,0.0346727,"middle layers are best at distinguishing between sentences with direct and indirect object constructions. Hewitt and Manning (2019) used a structural probe to recover syntax trees from contextual embeddings, and found the performance peaked in middle layers. Probing results are somewhat dependent on the choice of linguistic formalism used to annotate the data, as Kulmizev et al. (2020) found for syntax, and Kuznetsov and Gurevych (2020) found for seViolations of selectional restrictions are one type of linguistic unacceptability, defined as a semantic mismatch between a verb and an argument. Sasano and Korhonen (2020) examined the geometry of word classes (e.g., words that can be a direct object of the verb ‘play’) in word vector models; they compared single-class models against discriminative models for learning word class boundaries. Chersoni et al. (2018) tested distributional semantic models on their ability to identify selectional restriction violations using stimuli from two psycholinguistic datasets. Finally, Metheniti et al. (2020) tested how much BERT relies on selectional restriction information versus other contextual information for making masked word predictions. 2.4 Psycholinguistic tests for"
2021.acl-long.325,Q19-1040,0,0.0282098,"pparent if we only had access to their softmax probability outputs. Our source code and data are available at: https://github.com/SPOClab-ca/ layerwise-anomaly. 2 Related work mantic roles. Miaschi et al. (2020) examined the layerwise performance of BERT for a suite of linguistic features, before and after fine tuning. Our work further investigates what linguistic information is contained in different layers, with a focus on anomalous inputs. 2.2 Neural grammaticality judgements Many recent probing studies used grammaticality judgement tasks to test the knowledge of specific phenomena in LMs. Warstadt et al. (2019) gathered sentences from linguistic publications, and evaluated by Matthews Correlation with the ground truth. More commonly, the model is presented with a binary choice between an acceptable and unacceptable sentence: BLiMP (Warstadt et al., 2020) used templates to generate 67k such sentence pairs, covering 12 types of linguistic phenomena. Similarly, Hu et al. (2020) created syntactic tests using templates, but defined success criteria using inequalities of LM perplexities. In contrast with artificial templates, Gulordava et al. (2018) generated test cases by perturbing natural corpus data t"
2021.acl-long.325,2020.emnlp-demos.6,0,0.032542,"· · , y n |µ For all of our experiments, we use the ‘base’ versions of pretrained language models BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and 4217 1.0 0.8 Accuracy Accuracy 0.8 0.7 0.6 0.4 BERT BERT RoBERTa 0.6 RoBERTa 0.2 XLNet XLNet 0.0 10 20 50 100 200 500 1000 2000 5000 10000 0 Training Sentences 1 2 3 4 5 6 7 8 9 10 11 12 Layer (a) (b) Figure 2: BLiMP accuracy different amounts of training data and across layers, for three LMs. About 1000 sentences are needed before a plateau is reached (mean tokens per sentence = 15.1). XLNet (Yang et al., 2019), provided by HuggingFace (Wolf et al., 2020). Each of these models have 12 contextual layers plus a 0th static layer, and each layer is 768-dimensional. We train the Gaussian model on randomly selected sentences from the British National Corpus (Leech, 1992), representative of acceptable English text from various genres. We evaluate on BLiMP (Warstadt et al., 2020), a dataset of 67k minimal sentence pairs that test acceptability judgements across a variety of syntactic and semantic phenomena. In our case, a sentence pair is considered correct if the sentence-level surprisal of the unacceptable sentence is higher than that of the accepta"
2021.cmcl-1.9,N19-1423,0,0.342792,"s. The model is initialized from pretrained weights and fine-tuned on the task data. Introduction Eye-tracking data provides precise records of where humans look during reading, with millisecond-level accuracy. This type of data has recently been leveraged for uses in natural language processing: it can improve performance on a variety of downstream tasks, such as part-of-speech tagging (Barrett et al., 2016), dependency parsing (Strzyz et al., 2019), and for cognitively-inspired evaluation methods for word embeddings (Søgaard, 2016). Meanwhile, Transformer-based language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have been applied to achieve state-of-the-art performance on many natural language tasks. The CMCL 2021 shared task aims to add to our understanding of how language models can relate to eye movement features. In this paper, we present our submission to this shared task, which achieves third place on the leaderboard. We first explore some simple baselines using token-level features, and find that these are already somewhat competitive with the final model’s performance. Next, we describe our model architecture, which is based on RoBERTa (Figure 1). We find that m"
2021.cmcl-1.9,2021.cmcl-1.7,0,0.0304026,"nd find that these are already somewhat competitive with the final model’s performance. Next, we describe our model architecture, which is based on RoBERTa (Figure 1). We find that model ensembling offers a substantial performance gain over a single model. Finally, we augment the provided training data with the publicly available Provo eye-tracking corpus and combine them using a two-stage fine-tuning procedure; this results in a moderate performance gain. Our source code is available at https://github. com/SPOClab-ca/cmcl-shared-task. 2 Task Description The shared task format is described in Hollenstein et al. (2021), which we will briefly summarize here. The task data consists of sentences derived from the ZuCo 1.0 and ZuCo 2.0 datasets; 800 sentences (15.7 tokens) were provided as training data and 191 sentences (3.5k tokens) were held out for evaluation. The objective is to predict five eye-tracking features for each token: • Number of fixations on the current word (nFix). 85 Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 85–89 Online Event, June 10, 2021. ©2021 Association for Computational Linguistics Model Median Linear regression SVR (RBF kernel) nFix 7.208 4"
2021.cmcl-1.9,2021.ccl-1.108,0,0.0915831,"Missing"
2021.cmcl-1.9,2020.acl-main.467,0,0.0215447,"ature. For the fixProp feature, we calculate the proportion of participants where IA_DW ELL_T IM E &gt; 0 for each word. Finally, we scale all five features to have the same mean and standard deviation as the task data, and verify that their distributions and pairwise scatterplots are similar (Figure 2). We use two-stage fine-tuning to combine the Provo data with the task data. In two-stage finetuning, the entire model is fine-tuned on an auxiliary task before fine-tuning on the target task – this often yields a performance improvement, especially when the target task has a small amount of data (Pruksachatkun et al., 2020). In our case, we fine-tune the RoBERTa-base model for 100 epochs on the Provo data, then fine-tune for another 150 epochs on the task data. This gave a considerable improvement on both the development and submission scores (Table 4). Our best final submission is an ensemble of 10 identical models trained this way with different random seeds. Model Ensembling We use a simple approach to ensembling: we train multiple versions of an identical model using different random seeds and make predictions on the test data. These predictions are the averaged to obtain the final submission. In our experim"
2021.cmcl-1.9,W16-2521,0,0.0212961,"r layer that predicts the 5 output features from the last layer’s embeddings. The model is initialized from pretrained weights and fine-tuned on the task data. Introduction Eye-tracking data provides precise records of where humans look during reading, with millisecond-level accuracy. This type of data has recently been leveraged for uses in natural language processing: it can improve performance on a variety of downstream tasks, such as part-of-speech tagging (Barrett et al., 2016), dependency parsing (Strzyz et al., 2019), and for cognitively-inspired evaluation methods for word embeddings (Søgaard, 2016). Meanwhile, Transformer-based language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have been applied to achieve state-of-the-art performance on many natural language tasks. The CMCL 2021 shared task aims to add to our understanding of how language models can relate to eye movement features. In this paper, we present our submission to this shared task, which achieves third place on the leaderboard. We first explore some simple baselines using token-level features, and find that these are already somewhat competitive with the final model’s performance. Next, we desc"
2021.cmcl-1.9,D19-1160,0,0.0271173,"ure 1: Our model consists of RoBERTa with a regression head on each token, which is a linear layer that predicts the 5 output features from the last layer’s embeddings. The model is initialized from pretrained weights and fine-tuned on the task data. Introduction Eye-tracking data provides precise records of where humans look during reading, with millisecond-level accuracy. This type of data has recently been leveraged for uses in natural language processing: it can improve performance on a variety of downstream tasks, such as part-of-speech tagging (Barrett et al., 2016), dependency parsing (Strzyz et al., 2019), and for cognitively-inspired evaluation methods for word embeddings (Søgaard, 2016). Meanwhile, Transformer-based language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) have been applied to achieve state-of-the-art performance on many natural language tasks. The CMCL 2021 shared task aims to add to our understanding of how language models can relate to eye movement features. In this paper, we present our submission to this shared task, which achieves third place on the leaderboard. We first explore some simple baselines using token-level features, and find that the"
2021.findings-acl.170,P19-1602,0,0.0189644,"on non-parallel datasets with two types of stylistic variation: diachronic language shift and newspaper titles versus scientific paper titles. In parallel work, John et al. (2019) proposed a disentanglement model that appends additional contentbased losses, where content is approximated by a bag-of-words representation of the text. Their approach was applied to sentiment transfer for Yelp and Amazon reviews. Other work has looked at disentangling syntax from the semantics of a text. Chen et al. (2019) proposed a VAE-based model that used parallel paraphrase corpora; this was also the focus of Bao et al. (2019) and Balasubramanian et al. (2020). All of these works are very similar in the base model architecture and the kinds of loss functions used to guide disentanglement. In the following sections, we consolidate and propose a broad categorization of these losses that we hope will guide future work in this area. We then evaluate these models on parallel style transfer datasets, with ablation studies on the PersonageNLG dataset. Note on unsupervised disentanglement: While unsupervised approaches such as the β-VAE have been very successful at disentangling factors of variation in visual data (Higgins"
2021.findings-acl.170,P19-1041,0,0.252058,"methods combine all of these different aspects of a text into a single vector embedding (Conneau et al., 2017; Reimers and Gurevych, 2019). This results in only a fuzzy measure of text similarity when it is calculated using methods such as the cosine distance between vector embeddings. Recently, some research in NLP has focused on learning disentangled representations for texts, which aim to capture the different dimensions of variation of a text in separate vector embeddings. These methods have been investigated for style transfer to obtain disentangled representations of content and style (John et al., 2019; Romanov et al., 2019; Cheng et al., 2020), and paraphrase generation for disentangling syntax and semantics (Chen et al., 2019; Balasubramanian et al., 2020). Inspired by parallel developments on style transfer and disentanglement in computer vision, many of them operate within the variational autoencoder framework, where the autoencoder is modified to now encode a text into two latent vectors: one capturing the style (the aspect of variation), and the other capturing the content. Style transfer is then achieved by combining the content vector of the input with a style vector of the target s"
2021.findings-acl.170,N19-1088,0,0.0508528,"Missing"
2021.findings-emnlp.105,N19-1423,0,0.0063133,"0.763 127 0.699 0.305 94 0.777 0.242 110 0.699 0.451 94 0.777 0.286 110 0.699 0.551 94 0.777 0.125 110 0.699 0.236 94 0.777 0.104 110 0.699 0.366 94 0.777 0.524 110 Table 1: Evaluation of topic-based and topic-free models in predicting fine-grained human moral judgments, based on both F1 score and Pearson’s correlation. Superscript minus sign under “Pearson’s r” indicates p > 0.05 (Bonferroni corrected). consider two alternative topic-free models using static and contextual embeddings, where topic information is discarded in moral sentiment inference, i.e., P (m|e, o) = P (m|e). We use BERT (Devlin et al., 2019) in the contextual embedding model to represent tweets. Similarly for the centroid model, instead of using the static embeddings of the seed words in MFD, we use BERT to embed their definitions from the online version of the Oxford English Dictionary (OED).6 Each model infers P (m|e, o) for all entities, topics, and moral dimensions. We compare these probabilities with ground-truth moral judgments Pb(m|e, o) using both F1 score and Pearson’s correlation. We consider estimates of Pb(m|e, o) and P (m|e, o) meaningful if 1) the entity appears in at least one of the tweets in topic o, and 2) there"
2021.findings-emnlp.105,2020.emnlp-main.48,0,0.0431155,"long five or six moral foundations, each organized in terms of the polarities virtue (+) and vice ( ). The computational methods using MFT tend to rely on supervised approaches to predicting the moral sentiment reflected in text (Garten et al., 2016; Lin et al., 2018; Mooijman et al., 2018; Xie et al., 2020). Other related work has characterized moral biases in language models (Schramowski et al., 2019; Jentzsch et al., 2019; Xie et al., 2019), and contributed new datasets for tasks such as automatic ethical judgment and inference of sociomoral norms (Hoover et al., 2020; Lourie et al., 2020; Forbes et al., 2020). Existing work has also studied moral sentiment change over time (Xie et al., 2019) showing how word embeddings capture hidden moral biases underlying different concepts (e.g., slavery) in history. This model uses MFD words as seeds and a hierarchical framework to capture moral change in three tiers: moral relevance, moral polarity, and fine-grained moral foundations.1 Here we go beyond this line of research by developing an unsupervised framework that automatically identifies sources of moral change toward entities in text. 3 Methodology We formulate textual source tracing of moral change as"
2021.findings-emnlp.105,D19-1472,1,0.840393,"al at another time. Similarly, moral change triggered by historical events. We apsentiment toward a more general entity like policeply our methodology to analyze the news in men could undergo a negative shift due to acts on the COVID-19 pandemic and demonstrate its racial discrimination. Existing methods for moral utility in identifying sources of moral change sentiment detection typically take an aggregate apin high-impact and real-time social events. proach and do not focus on analyzing moral sentiment of entities (Garten et al., 2016; Lin et al., 1 Introduction 2018; Mooijman et al., 2018; Xie et al., 2019). Here, we develop a methodology to identify textual From ancient Greek scholars to philosophers of sources that give rise to moral sentiment change tothe past centuries, morality has been a subject of ward an entity. Our work takes a similar approach central importance in human history (Plato and Bloom, 1968; Aristotle et al., 2009; Hume, 1739; to detecting sources of gender bias in text by locatSmith, 1759; Kant, 1785; Nietzsche, 1887). De- ing a set of documents that influence gender bias in word embeddings (Brunet et al., 2019). spite this importance, people’s morals are not static but cha"
D18-1304,P16-2003,0,0.0284094,"n category), letter fluency (i.e., naming words that start with a given letter), sentence construction, and story recall tasks. The picture description and both fluency tasks were professionally transcribed and annotated with instances of filled pauses. Previous experiments in the literature on DB have been limited to the picture description task, most likely because the other tasks are not available for all participants. 2.2 2.3 Learning a multiview embedding We apply generalized canonical correlation analysis (GCCA) to our dataset to obtain a multiview embedding. We use GCCA as described by Benton et al. 2016 to learn linear transformations Uj which project different views of our data into the embedding G. In our experiments, we consider the following views of DB: linguistic features of the picture description, category fluency and letter fluency tasks, and demographic information. 0 0 Given X ∈ Rd×N , X 0 ∈ Rd ×N , where N is the total number of data points, N 0 is the total number of data points for which all views J are available, and d and d0 are the dimensions of X and X 0 ; let Xj and Xj0 denote views j of X and X 0 . Here, j ∈ {P D, CAT, LET, DEM }, which correspond to the picture descripti"
D19-5556,W16-0301,1,0.91054,"Missing"
D19-5556,D18-1289,0,0.0386025,"Missing"
D19-5556,W18-6529,0,0.0208837,"ized as an important construct in a number of research areas, including stylistics, text readability analysis, language assessment, first and second language acquisition, and cognitive disease detection. In order to measure various dimensions of lexical richness in the datasets under comparison, we compute statistics on token/unigram, bigram, and trigram counts. Additionally, we use the Lexical Complexity Analyser (Ai and Lu, 2010) to measure various dimensions of lexical richness, such as lexical density, sophistication, and variation. Following Oraby et al. (2018), Duˇsek et al. (2019), and Jagfeld et al. (2018), we also use Shannon entropy (Manning and Schtze, 2000, 3.1.2 AphasiaBank (AphB) AphasiaBank2 (MacWhinney, 2007) is another dataset of pathological speech that consists of 1 IMDB Sentiment Extract (IMDBs) https://dementia.talkbank.org https://aphasia.talkbank.org 433 Feature subgroup p. 61ff.) as a measure of lexical diversity in the texts: H(text) = − X x ∈ text freq(x) freq(x) log2 len(text) len(text)  Lexical richness  Lexical complexity (1) Here, x stands for all unique tokens/n-grams, freq stands for the number of occurrences in the text, and len for the total number of tokens/ngrams i"
D19-5556,S13-2069,0,0.0604081,"Missing"
D19-5556,W17-2329,0,0.0908082,"Missing"
D19-5556,O12-5001,0,0.0610862,"Missing"
D19-5556,S14-2058,0,0.144954,"e individual value of lexical and syntactic features with regards to their vulnerability and importance for ML models. Syntactic and lexical feature groups are often used together without specifying their individual value. For example, recent work in text classification for AD detection revealed that a combination of lexical and syntactic features works well (Fraser et al., 2016; Noorian et al., 2017); the same is true for other cognitive disease or language impairment detection (Meteyard and Patterson, 2009; Fraser et al., 2014), as well as sentiment detection in healthy speech and language (Negi and Buitelaar, 2014; Marchand et al., 2013; Pang et al., 2002). In this paper, we focus on individual value of lexical and syntactic feature groups, as studied across medical text classification tasks, types of language, datasets and domains. As such, the main contributions of this paper are: Similarly, varying feature sets have been used for detecting aphasia from speech. Researchers have studied the importance of syntactic complexity indicators such as Yngve-depth and length of various syntactic representations for detecting aphasia (Roark et al., 2011), as well as lexical characteristics such as average frequ"
D19-5556,P11-1015,0,0.0442658,"changes to lexical features. Results are validated across three datasets representing different text-classification tasks, with different levels of lexical and syntactic complexity of both conversational and written language. 1 Introduction It is important to understand the vulnerability of linguistic features to text alteration because (1) pre-defined linguistic features are still frequently used in health text classification, e.g., detecting Alzheimers disease (AD) (Masrani et al., 2017; Zhu et al., 2018; Balagopalan et al., 2018), aphasia (Fraser et al., 2015), or sentiment from language (Maas et al., 2011); and (2) understanding the importance of syntactic and lexical information separately as well as interactively is still an open research area in linguistics (Lester et al., 2017; Blaszczak, 2019). Lexical richness and complexity relate to nuances and the intricacy of meaning in language. Numerous metrics to quantify lexical diversity, such as type-token ratio (TTR) (Richards, 1987) 431 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 431–443 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics rian et al., 2017; Zhu et al.)"
D19-5556,W18-5019,0,0.0221893,"es: Features of lexical domain have been recognized as an important construct in a number of research areas, including stylistics, text readability analysis, language assessment, first and second language acquisition, and cognitive disease detection. In order to measure various dimensions of lexical richness in the datasets under comparison, we compute statistics on token/unigram, bigram, and trigram counts. Additionally, we use the Lexical Complexity Analyser (Ai and Lu, 2010) to measure various dimensions of lexical richness, such as lexical density, sophistication, and variation. Following Oraby et al. (2018), Duˇsek et al. (2019), and Jagfeld et al. (2018), we also use Shannon entropy (Manning and Schtze, 2000, 3.1.2 AphasiaBank (AphB) AphasiaBank2 (MacWhinney, 2007) is another dataset of pathological speech that consists of 1 IMDB Sentiment Extract (IMDBs) https://dementia.talkbank.org https://aphasia.talkbank.org 433 Feature subgroup p. 61ff.) as a measure of lexical diversity in the texts: H(text) = − X x ∈ text freq(x) freq(x) log2 len(text) len(text)  Lexical richness  Lexical complexity (1) Here, x stands for all unique tokens/n-grams, freq stands for the number of occurrences in the text"
D19-5556,W14-3210,0,0.119508,"Missing"
D19-5556,W02-1011,0,0.0237421,"Missing"
D19-5556,N19-1146,1,0.319149,"Missing"
D19-5556,J11-2001,0,0.407884,"veral of these have been identified to be important for a variety of tasks in applied linguistics (Daller et al., 2003). For example, metrics related to vocabulary size, such as TTR and word-frequencies, have proven to help with early detection of mild cognitive impairment (MCI) (Aramaki et al., 2016), hence are important for early dementia diagnosis. Discourse informativeness, measured via propositional idea density, is also shown to be significantly affected in speakers with aphasia (Bryant et al., 2013). Furthermore, lexicon-based methods have proved to be successful in sentiment analysis (Taboada et al., 2011; Tang et al., 2014). Syntactic complexity is evident in language production in terms of syntactic variation and sophistication or, in other words, the range and degree of sophistication of the syntactic structures that are produced (Lu, 2011; Ortega, 2003). This construct has attracted attention in a variety of languagerelated research areas. For example, researchers have examined the developmental trends of child syntactic acquisition (e.g., (Ramer, 1977)), the role of syntactic complexity in treating syntactic deficits in agrammatical aphasia (e.g., (Melnick and Conture, 2000; Thompson et a"
D19-5556,P14-1146,0,0.193519,"en identified to be important for a variety of tasks in applied linguistics (Daller et al., 2003). For example, metrics related to vocabulary size, such as TTR and word-frequencies, have proven to help with early detection of mild cognitive impairment (MCI) (Aramaki et al., 2016), hence are important for early dementia diagnosis. Discourse informativeness, measured via propositional idea density, is also shown to be significantly affected in speakers with aphasia (Bryant et al., 2013). Furthermore, lexicon-based methods have proved to be successful in sentiment analysis (Taboada et al., 2011; Tang et al., 2014). Syntactic complexity is evident in language production in terms of syntactic variation and sophistication or, in other words, the range and degree of sophistication of the syntactic structures that are produced (Lu, 2011; Ortega, 2003). This construct has attracted attention in a variety of languagerelated research areas. For example, researchers have examined the developmental trends of child syntactic acquisition (e.g., (Ramer, 1977)), the role of syntactic complexity in treating syntactic deficits in agrammatical aphasia (e.g., (Melnick and Conture, 2000; Thompson et al., 2003)), the rela"
D19-5556,O12-1014,0,\N,Missing
D19-5556,W13-3909,1,\N,Missing
D19-5556,N15-1087,0,\N,Missing
D19-6209,D16-1128,0,0.0210794,"Missing"
D19-6209,N18-1100,0,0.0322371,"ncludes demographic information about the patient as well as the primary diagnosis. The data consist of audio files and human-generated transcripts with speaker labels. Table 1 shows the distribution of diagnoses in the dataset. Since these data are proprietary, we also use a few transcripts of staged clinical interviews from YouTube as examples. 2 and discharge diagnoses (Rajkomar et al., 2018), predicting unplanned hospital readmissions for 5k patients by encoding EMR data with a convolutional neural network (Nguyen et al., 2018), and predicting diagnosis codes along with text explanations (Mullenbach et al., 2018). Although there is some existing work on generating text from structured data (Dou et al., 2018; Lebret et al., 2016), very little work has been done in the clinical domain. Liu (2018) generated patient note templates with a language model, which was able to approximate the organization of the note, but no new information from the patient encounter was used. Du et al. (2019) introduced a system for extracting symptoms and their status (experienced or not) from clinical conversations using a multi-task learning model trained on 3,000 annotated conversations. However, their model was trained on"
D19-6209,N18-1202,0,0.00889142,"s classification, SOAP classification, and note generation. In this section we discuss each component in detail, including methods and results. See Figure 4 for a diagram of the system components. 5.1 Preprocessing and data splitting Before passing the data to our models, the text of the transcripts is lowercased, and punctuation is separated from words using WordPunctTokenizer from NLTK (Steven Bird and Loper, 2009). For the utterance type and attribute classification tasks, each word in an utterance is represented as a word embedding. In this work, we use publicly available ELMo embeddings (Peters et al., 2018) trained on PubMed abstracts, as well as word2vec embeddings trained on PubMed4 . Of the 476 annotated conversations, we randomly select 50 to use as a test set for entity extraction and attribute classification. 5.3 Entity extraction In order to understand the conversational context, it may be useful to know whether an utterance is 5.3.1 Time phrase extraction In order to determine clinical relevance, it is important to know the time and duration of events in the patient history. We use HeidelTime to identify 4 http://evexdb.org/pmresources/ vec-space-models/ 5 http://groups.inf.ed.ac.uk/ami/"
D19-6209,D14-1179,0,0.00915026,"Missing"
D19-6209,D18-2003,0,0.0186902,"audio files and human-generated transcripts with speaker labels. Table 1 shows the distribution of diagnoses in the dataset. Since these data are proprietary, we also use a few transcripts of staged clinical interviews from YouTube as examples. 2 and discharge diagnoses (Rajkomar et al., 2018), predicting unplanned hospital readmissions for 5k patients by encoding EMR data with a convolutional neural network (Nguyen et al., 2018), and predicting diagnosis codes along with text explanations (Mullenbach et al., 2018). Although there is some existing work on generating text from structured data (Dou et al., 2018; Lebret et al., 2016), very little work has been done in the clinical domain. Liu (2018) generated patient note templates with a language model, which was able to approximate the organization of the note, but no new information from the patient encounter was used. Du et al. (2019) introduced a system for extracting symptoms and their status (experienced or not) from clinical conversations using a multi-task learning model trained on 3,000 annotated conversations. However, their model was trained on a limited set of 186 symptoms and did not address other medically relevant entities. A latent D"
D19-6209,P19-1087,0,0.0879891,"t al., 2018), predicting unplanned hospital readmissions for 5k patients by encoding EMR data with a convolutional neural network (Nguyen et al., 2018), and predicting diagnosis codes along with text explanations (Mullenbach et al., 2018). Although there is some existing work on generating text from structured data (Dou et al., 2018; Lebret et al., 2016), very little work has been done in the clinical domain. Liu (2018) generated patient note templates with a language model, which was able to approximate the organization of the note, but no new information from the patient encounter was used. Du et al. (2019) introduced a system for extracting symptoms and their status (experienced or not) from clinical conversations using a multi-task learning model trained on 3,000 annotated conversations. However, their model was trained on a limited set of 186 symptoms and did not address other medically relevant entities. A latent Dirichlet allocation (LDA) model (Blei et al., 2003) is a topic modeling technique and has been applied to clinical text to extract underlying useful information. For example, Bhattacharya et al. (2017) applied LDA on structured EMR data such as age, gender, and lab results, showing"
J17-2004,J12-1001,0,0.0753295,"Missing"
J17-2004,P03-1054,0,0.0366121,"tion paid to features that are meant to be task-relevant, namely, cue words of TIBs, semantic similarity across utterances, word specificity, vocabulary richness, repetition, incomplete words, and fillers. 4.1 Lexical Features We extract 81 lexical features from the textual transcriptions of each utterance. We use Lu’s L2 Syntactic Complexity Analyzer (Lu 2010), which computes 23 features that measure the syntactic complexity of text. Lu’s Syntactic Complexity Analyzer was initially designed to measure the syntactic complexity of second-language writing and, in turn, uses the Stanford parser (Klein and Manning 2003). It includes measuring lengths of production units (i.e., clauses, sentences, T-units), the ratio of clauses to sentences, subordination (e.g., clauses per T-unit, dependent clauses per clause or T-unit), coordination (e.g., coordinate phrases per clause or T-unit), and particular structures (e.g., complex nominals per clause or T-unit, verb phrases per T-unit). We further compute the average sentence length within an utterance, in terms of the number of words, and the automated readability index, which measures word and sentence difficulty (Smith and Senter 1967), specifically, c + 0.5 w − 2"
J17-2004,W97-0710,0,0.143331,"Missing"
J17-2004,N03-1033,0,0.00695199,"e difficulty (Smith and Senter 1967), specifically, c + 0.5 w − 21.43 4.71 w s (3) where c is the number of characters, w is the number of words, and s is the number of sentences. Because these measures are derived from textual transcripts of spoken utterances, the number of characters is merely an approximation for the complexity of words. We include various part-of-speech (POS) features, such as the proportion of nouns, verbs, light verbs, adjectives, adverbs, prepositional words, demonstratives, functional words generally, and the noun-to-verb ratio, obtained using the Stanford POS tagger (Toutanova et al. 2003). We measure the mean word frequency using the SUBTL norms (Brysbaert and New 2009), and each of the Bristol norms: imageability, ageof-acquisition, and familiarity. Stadthagen-Gonzalez and Davis (2006) describe imageability as “a semantic variable that measures how easy it is for a word to elicit mental images... used to evaluate the effects of meaning on memory and word recognition” and as closely related to “concreteness,” with some exceptions (Bird, Franklin, and Howard 2001); raters of imageability were asked to indicate how easily each word elicited mental images, on a 7-point scale. Fam"
J17-2004,H05-1044,0,0.0365321,"Missing"
N19-1146,N18-1111,0,0.0660442,"Missing"
N19-1146,P17-1001,0,0.0609391,"Missing"
N19-1146,P14-5010,0,0.00339425,"), Roark et al. (2011), Fraser et al. (2015), and Hern´andez-Dom´ınguez et al. (2018). After calculating these features, we use KNN imputation to replace the undefined values (resulting from divide-by-zero, for example), and then normalize the features by their z-scores. The following are brief descriptions of these features, grouped by their natural categories. More detailed descriptions are included in the Appendix. There are 185 acoustic features (e.g., average pause time), 117 syntactic features (e.g., Yngve statistics (Yngve, 1960) of the parse tree, computed by the LexParser in CoreNLP (Manning et al., 2014)), and 31 semantic features (e.g., cosine similarity between pairs of utterances) Moreover, we use 80 part-of-speech features that relate to both syntax and semantics but are here primarily associated with the latter. Modality Division After representing each sample with a 413-dimensional vector x consisting of all available linguistic features, we divide the vector into M partitions (‘modalities’) of approximately equal sizes [x1 , x2 , ..., xM ], according to the groups mentioned above. Unless mentioned otherwise (e.g., in the ablation study shuffling 3 The version of DementiaBank dataset we"
N19-1146,D18-1304,1,0.760263,"Missing"
N19-1146,P16-1221,1,0.900512,"Missing"
N19-1146,W07-1001,0,0.0537722,"ask from the Boston Diagnostic Aphasia Examination (Becker et al., 1994). The version we have access to contains 240 speech samples labeled Control (from 98 people), 234 with AD (from 148 people), and 43 with MCI (from 19 people)3 . All participants were older than 44 years. 3.2 Linguistic Features The dataset contains narrative speech descriptions and their transcriptions. We preprocess them by extracting 413 linguistic features for each speech sample. These linguistic features are proposed by and identified as the most indicative of cognitive impairments by various previous works, including Roark et al. (2007), Chae and Nenkova (2009), Roark et al. (2011), Fraser et al. (2015), and Hern´andez-Dom´ınguez et al. (2018). After calculating these features, we use KNN imputation to replace the undefined values (resulting from divide-by-zero, for example), and then normalize the features by their z-scores. The following are brief descriptions of these features, grouped by their natural categories. More detailed descriptions are included in the Appendix. There are 185 acoustic features (e.g., average pause time), 117 syntactic features (e.g., Yngve statistics (Yngve, 1960) of the parse tree, computed by th"
N19-1146,P17-1118,0,0.0591555,"Missing"
N19-1146,N16-1143,0,0.0705267,"Missing"
N19-1146,W15-5123,1,0.871081,"Missing"
N19-1367,W06-1615,0,0.0568066,"xtraction pipeline; however, they did not consider multilingual classification directly. More generally, multilingual NLP is an active and growing area of research. Some approaches to improving classifier performance on a resourcepoor target language by leveraging a resource-rich source language include: translate the target language to the source language (or vice versa) and 3660 1 https://dementia.talkbank.org/ train a unilingual classifier (Wan, 2009); extract features from the two languages separately and then use domain adaptation techniques to train a classifier for the target language (Blitzer et al., 2006; Prettenhofer and Stein, 2010); or determine a common representation for both languages and then extract features from the combined corpus to train a multilingual classifier (Ammar et al., 2016). In the extreme case, one can also consider purely cross-lingual classification, in which the classifier is trained solely on the source language, but tested on the target language. We use a supervised domain adaptation approach, similar to that of Daum´e III (2007), by considering each language to be a different domain. In related (though not multilingual) work, Masrani et al. (2017) also used this a"
N19-1367,J92-4003,0,0.248848,".0 (2.6) 28.6 (1.4) 33 22F/11M 79.2 (6.6) 11.3 (4.0) 18.9 (3.9) participants were asked to perform the CTP task in their respective languages. In English, the image was shown on paper and speech was digitally recorded, while in the French study, the image was displayed on a tablet and speech was recorded via the tablet microphone. Features Class-Based Language Modelling In contrast to the previous work on AD classification, we measure not only which information units are mentioned, but also the order in which they are mentioned. Our approach has some similarity to class-based language models (Brown et al., 1992), in which words are first grouped into classes (or clusters), and then the language model is trained on the classes rather than the individual words. One benefit to this approach is improved generalisability (Hoidekr et al., 2006), and another is the ability of classes to span different languages (T¨ackstr¨om et al., 2012). 3 AD Table 1: Demographics of participants, where AD indicates Alzheimer’s disease, and HC indicates healthy control. The Mini Mental State Examination (MMSE) is global measure of cognitive status. 3.2 2.4 French HC Methodology 3.1 Data Data were taken from two corpora: a"
N19-1367,hoidekr-etal-2006-benefit,0,0.127725,"Missing"
N19-1367,P07-1033,0,0.657228,"Missing"
N19-1367,W11-2123,0,0.0245669,"ch language), and used to translate the full narratives to sequences of information units. As an example, the English A boy is standing on a stool and French Le garc¸on est sur un tabouret would both be mapped to the sequence B OY S TOOL. Features relating to the occurrence of each distinct information unit comprise the info feature set, described in Table 2b. Additionally, new features are derived from language models build on the sequence of information units. To this end, concept-based language models are trained for English and French in a leave-one-out fashion, using the kenlm framework (Heafield, 2011). Models up to 5-grams were constructed. For each participant, two language models are constructed for each n: one trained on the healthy control (HC) population and one trained on the AD population. The participant is left out of the model built on their associated diagnostic group. The trained language models are then applied to the held-out participant’s sequence of information units and various language model (LM) features are extracted (Table 2c). 3661 Actions STEAL , FALL , WASH , OVERFLOW, GIRL’ S AC TION , WOMAN ’ S INDIFFERENCE Actors BOY, GIRL , CHILD ( REN ), WOMAN Places KITCHEN ,"
N19-1367,N19-1199,1,0.300126,"o improve the AUC on the French dataset from 0.85 to 0.89. We also developed a new set of features for this task, using concept-based language modelling, which improved AUC from 0.80 to 0.85 in the unilingual case, and 0.88 to 0.89 in the multilingual case. Future work will involve extending the set of features involved, incorporating data from other languages, and testing whether similar techniques can be effective for detecting earlier stages of cognitive decline, such as MCI. Other work from our group has also begun to explore the use of unsupervised methods and out-of-domain data sources (Li et al., 2019). Technical challenges aside, collaborations of this nature can be difficult due to the sensitive nature of the data, and the need to respect ethical guidelines and participant consent when sharing and storing data. With this in mind, we recommend to other researchers working in similar domains to consider from the outset whether their data could eventually be shared, and to make suitable provisions in their ethics protocols and participant consent forms. We look to DementiaBank as a model for this kind of data-sharing and openness, and hope that researchers can continue to find ways to share"
N19-1367,P10-1114,0,0.0314006,"wever, they did not consider multilingual classification directly. More generally, multilingual NLP is an active and growing area of research. Some approaches to improving classifier performance on a resourcepoor target language by leveraging a resource-rich source language include: translate the target language to the source language (or vice versa) and 3660 1 https://dementia.talkbank.org/ train a unilingual classifier (Wan, 2009); extract features from the two languages separately and then use domain adaptation techniques to train a classifier for the target language (Blitzer et al., 2006; Prettenhofer and Stein, 2010); or determine a common representation for both languages and then extract features from the combined corpus to train a multilingual classifier (Ammar et al., 2016). In the extreme case, one can also consider purely cross-lingual classification, in which the classifier is trained solely on the source language, but tested on the target language. We use a supervised domain adaptation approach, similar to that of Daum´e III (2007), by considering each language to be a different domain. In related (though not multilingual) work, Masrani et al. (2017) also used this approach to adapt a dataset of A"
N19-1367,J15-4001,0,0.0348046,"Missing"
N19-1367,W16-4211,0,0.144803,"g, perceptual speed and language (B¨ackman et al., 2005; Weiner et al., 2008). Machine learning experiments using speech and language for the detection of dementia or related disorders have been conducted in many languages, including English (Roark et al., 2011; Mirheidari et al., 2016; Fraser et al., 2016; Asgari et al., 2017), French (Tr¨oger et al., 2017; K¨onig et al., 2018), German (Weiner et al., 2016), Hungarian (Szatloczki et al., 2015; Vincze et al., 2016), Spanish (Meil´an et al., 2014), Greek (Satt et al., 2013), Swedish (Lundholm Fors et al., 2018; Fraser et al., 2018a), Japanese (Shibata et al., 2016), Portuguese (Alu´ısio et al., 2016), and Mandarin Chinese (Lai et al., 2009). Most studies acknowledge that small data sets are a limitation and describe the difficulties in gathering more data, including the challenges in patient recruitment, the expense of running clinically-based studies, and the manual effort required for transcription and annotation. Here, we consider whether it could be possible to increase the amount of available data by augmenting a corpus in one language with data from another language, and thus improve predictive per3659 Proceedings of NAACL-HLT 2019, pages 3659–367"
N19-1367,K17-1033,0,0.0912277,"e (Blitzer et al., 2006; Prettenhofer and Stein, 2010); or determine a common representation for both languages and then extract features from the combined corpus to train a multilingual classifier (Ammar et al., 2016). In the extreme case, one can also consider purely cross-lingual classification, in which the classifier is trained solely on the source language, but tested on the target language. We use a supervised domain adaptation approach, similar to that of Daum´e III (2007), by considering each language to be a different domain. In related (though not multilingual) work, Masrani et al. (2017) also used this approach to adapt a dataset of AD narratives to their MCI classification task. English N Gender Age Education MMSE (/30) HC AD 241 154F/87M 64.8 (7.7) 14.2 (2.6) 29.1 (1.1) 309 189F/120M 71.4 (8.4) 12.8 (3.0) 19.8 (5.7) 25 19F/6M 75.4 (7.0) 14.0 (2.6) 28.6 (1.4) 33 22F/11M 79.2 (6.6) 11.3 (4.0) 18.9 (3.9) participants were asked to perform the CTP task in their respective languages. In English, the image was shown on paper and speech was digitally recorded, while in the French study, the image was displayed on a tablet and speech was recorded via the tablet microphone. Features"
N19-1367,N12-1052,0,0.0878325,"Missing"
N19-1367,P16-2030,0,0.0605087,"Missing"
N19-1367,P09-1027,0,0.0384355,"I), and found that classification results could be improved in both English and Swedish by incorporating multilingual topic modelling into the feature extraction pipeline; however, they did not consider multilingual classification directly. More generally, multilingual NLP is an active and growing area of research. Some approaches to improving classifier performance on a resourcepoor target language by leveraging a resource-rich source language include: translate the target language to the source language (or vice versa) and 3660 1 https://dementia.talkbank.org/ train a unilingual classifier (Wan, 2009); extract features from the two languages separately and then use domain adaptation techniques to train a classifier for the target language (Blitzer et al., 2006; Prettenhofer and Stein, 2010); or determine a common representation for both languages and then extract features from the combined corpus to train a multilingual classifier (Ammar et al., 2016). In the extreme case, one can also consider purely cross-lingual classification, in which the classifier is trained solely on the source language, but tested on the target language. We use a supervised domain adaptation approach, similar to t"
N19-1367,P16-1221,1,0.810925,"formation units”, which have been widely used in subsequent research. 2.2 NLP for AD Classification Several recent studies have used NLP and machine learning to analyse speech samples from people with dementia and other cognitive disorders. Most relevant here, are those which focus on picture description tasks in English or French. DementiaBank1 is a large database of CTP narratives from AD patients and controls, containing primarily English data. A number of recent papers report classification results on this corpus (Prud’hommeaux and Roark, 2015; Fraser et al., 2016; Al-Hameed et al., 2016; Yancheva and Rudzicz, 2016; Sirts et al., 2017). Language analysis of English-language CTP data from other sources has also been used to differentiate between different underlying pathologies in AD (Rentoumi et al., 2014), and variants of frontotemporal lobar degeneration (Pakhomov et al., 2010). In French, picture description was one of multiple tasks used to elicit speech for the classification of participants with mild cognitive impairment and AD reported by K¨onig et al. (2015) and K¨onig et al. (2018), although only acoustic processing was used. 2.3 Multi- and Cross-Lingual NLP There has been very little prior wor"
N19-1414,P16-1221,1,0.900305,"Missing"
P06-3015,P93-1008,0,0.0752744,"Missing"
P06-3015,P98-1102,0,0.0315753,"lity types, speech and video-tracked gestures chief among them, within the immersive virtual environment (Boussemart, 2004) shown in Figure 1. Its purpose is to comprehend multimodal phrases such as “put this & here & .”, for pointing gestures &, in either command-based or dialogue interaction. Figure 1: The target immersive environment. 1.1 Graphical Models and Unification Unification grammars on typed directed acyclic graphs have been explored previously in MMI, but typically extend existing mechanisms not designed for multi-dimensional input. For example, both (Holzapfel et al., 2004) and (Johnston, 1998) essentially adapt Earley’s chart parser by representing edges as sets of references to terminal input elements - unifying these as new edges are added to the agenda. In practice this has led to systems that analyze every possible subset of the input resulting in a combinatorial explosion that balloons further when considering the complexities of cross-sentential phenomena such as anaphora, and the effects of noise and uncertainty on speech and gesture tracking. We will later show the extent to which C LAVIUS reduces the size of the search space. 85 Proceedings of the COLING/ACL 2006 Student R"
P06-3015,C00-1054,0,0.0217166,"ituents, we overcome several difficulties of other systems in this domain. We also present several criteria used in this model to constrain the search process using dynamically loadable scoring functions. Some early analyses of our implementation are discussed. 1 Introduction Since the seminal work of Bolt (Bolt, 1980), the methods applied to multimodal interaction (MMI) have diverged towards unreconcilable approaches retrofitted to models not specifically amenable to the problem. For example, the representational differences between neural networks, decision trees, and finite-state machines (Johnston and Bangalore, 2000) have limited the adoption of the results using these models, and the typical reliance on the use of whole unimodal sentences defeats one of the main advantages of MMI - the ability to constrain the search using cross-modal information as early as possible. C LAVIUS is the result of an effort to combine sensing technologies for several modality types, speech and video-tracked gestures chief among them, within the immersive virtual environment (Boussemart, 2004) shown in Figure 1. Its purpose is to comprehend multimodal phrases such as “put this & here & .”, for pointing gestures &, in either c"
P06-3015,H93-1008,0,\N,Missing
P06-3015,C98-1099,0,\N,Missing
P09-1062,P99-1071,0,0.0195776,"documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. Among the extra"
P09-1062,P08-1054,1,0.852692,"their scoring to discourage this possibility. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extr"
P09-1062,W04-1013,0,0.0453026,"Missing"
P09-1062,J98-3005,0,0.0257777,"ork above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for"
P09-1062,P07-1064,0,0.235485,"entify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2 Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. I: II: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. seventeen sailors were killed. between frames are then estimated using cosine distance. All similarity scores are then normalized to the range of [0, 1], which yields similarity matrice"
P13-1093,W12-0403,0,0.256014,"unregulated nature of the interview interaction. To test the effect of verbosity, we segment the data by child age and select only the transcriptions with above-average word counts (i.e., ≥ 37 words), resulting in four groups: 12 transcriptions of 4-year-olds, 48 of 5year-olds, 39 of 6-year-olds, and 37 of 7-year-olds. This mimics the scenario in which some mini949 Figure 3: Effect of feature set choice on crossvalidation accuracy. Figure 2: Effect of threshold and classifier choice on cross-validation accuracy. Threshold τ = 0 is not present, since all data would be labeled truth. 5.5 2012; Almela et al., 2012; Fornaciari and Poesio, 2012), our results suggest that the set of syntactic features presented here perform significantly better than the LIWC feature set on our data, and across seven out of the eight experiments based on age groups and verbosity of transcriptions. Linguistic Inquiry and Word Count The Linguistic Inquiry and Word Count (LIWC) tool for generating features based on word category frequencies has been used in deception detection with adults, specifically: first-person singular pronouns (FP), exclusive words (EW), negative emotion words (NW), and motion verbs (MV) (Newman et al."
P13-1093,levy-andrew-2006-tregex,0,0.0179028,"Missing"
P13-1093,A97-1011,0,0.250734,"Missing"
P13-1093,W12-0402,0,0.0416828,"uiry and Word Count The Linguistic Inquiry and Word Count (LIWC) tool for generating features based on word category frequencies has been used in deception detection with adults, specifically: first-person singular pronouns (FP), exclusive words (EW), negative emotion words (NW), and motion verbs (MV) (Newman et al., 2003). We compare the performance of classifiers trained with our 17 syntactic features to those of classifiers trained with those LIWC-based features on the same data. To evaluate the four LIWC categories, we use the 86 words of the Pennebaker model (Little and Skillicorn, 2008; Vartapetiance and Gillam, 2012). The performance of the classifiers trained with LIWC features is shown in Table 6. The set of 17 syntactic features proposed here result in significantly higher accuracies across classifiers and experiments (µ = 0.63, σ = 0.10) than with the LIWC features used in previous work (µ = 0.58, σ = 0.09), as shown in Figure 3 (t(53) = −0.0691, p &lt; .0001). 6 Statistical analyses showed that the average sentence length (ASL), the Stajner-Mitkov measure of sentence complexity (COM), and the mean number of clauses per utterance (MCU) are the features most predictive of truth and deception (see section"
P13-1093,P09-2078,0,0.477706,"Missing"
P13-1093,W10-3023,0,0.0659677,"Missing"
P13-1093,P03-1054,0,\N,Missing
P13-1093,P12-2034,0,\N,Missing
P13-1093,stajner-mitkov-2012-diachronic,0,\N,Missing
P16-1221,D14-1162,0,0.112514,"Missing"
W11-2302,P93-1007,0,0.145939,"Missing"
W13-3909,N03-1033,0,0.0220758,"ed transcripts [8]. One strategy which combines ASR technology with manual transcripts is to use forced-alignment with manual transcripts to measure acoustic features such as rate of speech and length of pauses [9, 10]. However, for a speech analysis system to be available online or as part of an in-home continuous monitoring system, there must be no reliance on manual transcriptions at the word-level, which forced-alignment requires. A number of features can be extracted from the text transcripts. Some of our features are based on the part-of-speech (POS) tags assigned by the Stanford tagger [12]. SD patients have been observed to produce proportionally fewer nouns and more verbs and pronouns, while PNFA patients tend to produce more nouns and fewer verbs [13, 14, 15]. PNFA patients also tend to omit function words, such as determiners or auxiliaries [13, 16]. We look up the frequency of each word in the SUBTL norms, which are derived from a large corpus of subtitles from film and television [17]. We calculate the average frequency over all words as well as specically for nouns and verbs. Similarly, we calculate the average familiarity, imageability, and age of acquisition of the word"
W14-1904,W12-2907,1,0.911478,"reponderance of utterances in which human caregivers speak concurrently with the participants, as well as inordinately challenging levels of noise. The estimated signal-to-noise ratio (SNR) across utterances range from −3.42 dB to 8.14 dB, which is extremely low compared to typical SNR of 40 dB in clean speech. One cause of this low SNR is that microphones are placed in the environment, rather than on the robot (so the distance to the microphone is variable, but relatively large) and that the participant often has their back turned to the microphone, as shown in figure 1. As in previous work (Rudzicz et al., 2012), we enhance speech signals with the log-spectral amplitude estimator (LSAE) which minimizes the mean squared error of the log spectra given a model for the source speech Xk = Ak e( jωk ), where Ak is the spectral amplitude. The LSAE method is a modification of the short-time spectral amplitude estimator that finds an estimate of the spectral amplitude, Aˆk , that minimizes the distortion  2  E logAk − log Aˆk , (1) such that the log-spectral amplitude estimate is Aˆk = exp (E [ln Ak |Yk ])  Z ∞ −t  ξk 1 e = exp dt Rk , 1 + ξk 2 vk t (2) where ξk is the a priori SNR, Rk is the noisy spec"
W14-1904,P06-3015,1,0.649874,"Missing"
W14-1904,N04-4040,0,0.0111133,"d how their speech differs from that of the general population. This then can be integrated into future automatic speech recognition systems. Guinn and Habash (2012) showed, through an analysis of conversational dialogs, that repetition, incomplete words, and paraphrasing were significant indicators of Alzheimer’s disease relative but several expected measures such as filler phrases, syllables per minute, and pronoun rate were not. Indeed, pauses, fillers, formulaic speech, restarts, and speech disfluencies are all hallmarks of speech in individuals with Alzheimer’s (Davis and Maclagan, 2009; Snover et al., 2004). Effects of Alzheimer’s disease on syntax remains controversial, with some evidence that deficits in syntax or of agrammatism could be due to memory deficits in the disease (Reilly et al., 2011). Other studies has applied similar analyses to related clinical groups. Pakhomov et al. (2010) identified several different features from the audio and corresponding transcripts of 38 patients with frontotemporal lobar degeneration (FTLD). They found that pause-to-word ratio and pronounto-noun ratios were especially discriminative of FTLD variants and that length, hesitancy, and agramatism correspond"
W15-5123,W14-3210,0,0.538112,"Missing"
W15-5123,W14-3204,0,0.0625987,"Missing"
W15-5123,P03-1054,0,0.0347489,"al description of the Boston Cookie Theft picture, which typical lasts about a minute. We partition subjects between controls (CT) and those with probable AD, possible AD or MCI (or, collectively,“AD” 2 ). Considering only subjects with associated MMSE scores, the working set consists of 393 speech samples from 255 subjects (165 AD, 90 CT). 2.2. Features Three major types of features are extracted from the speech samples and their transcriptions: (1) lexicosyntactic measures, extracted from syntactic parse trees constructed with the Brown parser and POS-tagged transcriptions of the narratives [12, 13, 14, 15, 16]; (2) acoustic measures, including the standard Melfrequency cepstral coefficients (MFCCs), formant features, and measures of disruptions in vocal fold vibration regularity [17]; and (3) semantic measures, pertaining to the ability to describe concepts and objects in the Cookie Theft picture. The full list of features, along with their major type and subtype, is shown in Table 1. 2.3. Feature Analysis Two feature selection methods are used to identify the most informative features for disambiguating AD from CT. Since the MMSE score is a measure of the progression of cognitive impairment and is"
W16-0301,E09-1017,0,0.0283564,"Missing"
W16-0301,W15-1204,0,0.0306285,"Missing"
W16-0301,W14-3202,0,0.143221,"Missing"
W16-0301,W14-3204,0,0.0703373,"Missing"
W16-0301,W14-3210,0,0.51461,"Missing"
W16-0301,W15-1207,0,0.0376156,"Missing"
W17-3107,P06-4018,0,0.00666183,"e 3 shows the top 10 features from the 94 features of LIWC 2015 with the highest information gain in our data. 4.4 N -gram language models Another standard method used to extract features from text is to calculate the probability of a document within a language model. In our experiments, we use four different corpora to calculate probabilities of unigrams and bigrams. We build the first two models using the Anxiety and Control training examples, respectively. We build the third model using 100,000 unlabelled tweets from the Sentiment140 dataset (Go et al., 2009) and use the NLTK Brown corpus (Bird, 2006) for the fourth model. To generate feature vectors, we calculate the log-probability of each input sentence as unigrams and as bigrams. For each Reddit post, the associated feature subvector contains a unigram word2vec doc2vec LDA SVM Reddit Twitter 0.906 0.813 0.772 0.803 0.868 0.748 Reddit 0.900 0.797 0.846 NN Twitter 0.786 0.823 0.721 Table 4: Accuracies from feature vectors generated by models trained with Reddit data vs Twitter data. 61 5 Results 5.2 Studying collocations captures how groups of words are combined to produce meaning beyond the sum of individual component words. While N -gr"
W17-3107,W15-1204,0,0.0632219,"Missing"
W17-3107,W15-1206,0,0.0237379,"Missing"
W17-3107,W15-1205,0,0.0393326,"Missing"
W17-3107,W15-1212,0,0.237926,"Missing"
W19-1308,S15-2078,1,0.81589,"icipants from seven different countries, including Germany, the Netherlands, and the USA. The questionnaire was divided into three subscales focusing on interaction, social influence, and emotion. In general, participants from the USA showed the most positive attitudes towards robots, particularly in their openness to interacting with robots, although they were more negative than the German or Dutch on the topic of robot emotion. Social media has proven to be a rich source of data for sentiment and emotion analysis on a variety of topics, using lexicon-based and machine learning methods (e.g. Rosenthal et al. (2015); Giachanou and Crestani (2016); Mohammad et al. (2018)). However, very little work has focused on the emotions expressed towards robots. Friedman et al. (2003) analyzed 3,119 forum posts relating to the AIBO robot dog. They developed (1) Were there differences in the type or scale of emotions expressed in each of the host countries? We compare the percentages of words associated with different emotions from the tweets produced during each trip, to examine any cultural factors in the public reaction to hitchBOT. (2) What emotions were triggered when hitchBOT was destroyed? We compare the perce"
W19-1308,P18-1017,1,0.901282,"Missing"
W19-1308,S14-2033,0,0.0681897,"Missing"
W19-1308,S18-1001,1,0.899238,"Missing"
W19-1911,W16-2922,0,0.0213325,"cal terms. w2vSMS denotes the word embeddings trained on our text message data and w2vPubmed denotes the word embeddings trained on publicly available PubMed articles. Using word embeddings trained on our data performs better than the pre-trained ones. We dig deeper and report the top 5 similar words of some common medical terms in Table 6. Word embeddings trained on text messages do a much better job of capturing different spellings (e.g., “bp” and “b/p”) as well as common misspellings (e.g., “bloood” and “blod”). These results further highlight the need for context-specific word embeddings (Chiu et al., 2016). Discussion Table 4 shows that the addition of text message representations yields to improvements in Groups A, B, C, and D. The greatest improvement in in Group C. The proportion of messages which are followed by an ICU transfer three days later is much higher in Group C, which could reasonably explain the difference in performance. However, we also note that text messages in Group C tend to be longer than other data, and that nurses in Group C send more messages per visit. Across all data, the best model performance is for Group 6 Conclusion & Future Work In this work, we look at the added"
W19-1911,Q17-1010,0,0.0182825,"to ‘ls add pr pain mod, not PO. tax’. We provide examples of message header and message reply pairs in Table 2. We focus our experiments on Group A since it has the most amount of data, and on Group C since it has the most number of messages per visit and the longest messages. We use the ADT (Admission/Discharge/Transfer) code in the patients’ records to determine transfer to the ICU. A mheader is determined to have the outcome if an ICU transfer occurs within the next 3 days of the message send date (Table 1). 3 embeddings of dimension size 100, with a context window equal to 5 for training (Bojanowski et al., 2017). We explore different combinations of the text message word embeddings through concatenation, summing, and averaging. We report results using a combination of all three types. More specifically, we concatenate twenty 100dimensional word embeddings (2000 dimensions), a sum of the word embeddings (100 dimensions), and an average of the 20 words (100 dimensions), for a total of 2200-dimensional feature vector. Linguistic features: We represent each text message as a vector containing 9 linguistic features. We compute lexical features (character and word count, word density5 ), syntactic features"
W19-1911,N18-1202,0,0.0833514,"Missing"
W19-4303,P02-1040,0,0.119138,"Missing"
W19-4303,W17-2629,0,0.176924,"to obtain a loss signal for each word in the corpus. MaliGAN (Che et al., 2017) rescales the reward to control for the vanishing gradient problem faced by SeqGAN. RankGAN (Lin et al., 2017) replaces D with an adversarial ranker and minimizes pair-wise ranking loss to get better convergence, however, is more expensive than other methods due to the extra sampling from the original data. (Kusner and Hern´andez-Lobato, 2016) used the Gumbel-softmax approximation of the discrete one-hot encoded output of the G, and showed that the model learns rules of a contextfree grammar from training samples. (Rajeswar et al., 2017), the state of the art in 2017, forced the GAN to operate on continuous quantities by approximating the one-hot output tokens with a softmax distribution layer at the end of the G network. MaskGAN (Fedus et al., 2018) uses policy gradient with REINFORCE estimator (Williams, 1992) to train the model to predict a word based on its context, and show that for the specific blank-filling task, their model outperforms maximum likelihood model using the perplexity metric. LeakGAN (Guo et al., 2018) allows for long 2.2 GAN2vec In this work, we propose GAN2vec - GANs that generate real-valued word2vec-l"
W19-4303,D14-1074,0,0.035567,"to capture a significant share of the examples. For the sake of simplicity in these experiments, for the real corpus, sentences with fewer than seven words are ignored, and those with more than seven words are cut-off at the seventh word. Table 1 presents sentences generated by the original GAN2vec model. Appendix A.2 includes additional examples. While this is a small subset of randomly sampled examples, on a relatively simple dataset, the text quality appears competitive to the work of (Rajeswar et al., 2017) on this corpus. Chinese Poetry Dataset The Chinese Poetry dataset, introduced by (Zhang and Lapata, 2014) presents simple 4-line poems in Chinese with a length of 5 or 7 tokens (henceforth referred to Poem 5 and Poem 7 respectively). Following previous work by (Rajeswar et al., 2017) and (Yu et al., 2017), we treat every line as a separate data point. We modify the Poem 5 dataset to add start and end of tokens, to ensure the model captures (at least) that pattern through the corpus (given our lack of Chinese knowledge). This setup allows us to use identical architectures for both the Poem 5 and Poem 7 datasets. We also modify the GAN2vec loss function with the objective in Eq. 2, and report the r"
