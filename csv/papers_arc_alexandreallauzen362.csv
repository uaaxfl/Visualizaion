2021.jeptalnrecital-taln.7,Transport Optimal pour le Changement S{\\'e}mantique {\\`a} partir de Plongements Contextualis{\\'e}s (Optimal Transport for Semantic Change Detection using Contextualised Embeddings ),2021,-1,-1,2,1,4347,syrielle montariol,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Plusieurs m{\'e}thodes de d{\'e}tection des changements s{\'e}mantiques utilisant des plongements lexicaux contextualis{\'e}s sont apparues r{\'e}cemment. Elles permettent une analyse fine du changement d{'}usage des mots, en agr{\'e}geant les plongements contextualis{\'e}s en clusters qui refl{\`e}tent les diff{\'e}rents usages d{'}un mot. Nous proposons une nouvelle m{\'e}thode bas{\'e}e sur le transport optimal. Nous l{'}{\'e}valuons sur plusieurs corpus annot{\'e}s, montrant un gain de pr{\'e}cision par rapport aux autres m{\'e}thodes utilisant des plongements contextualis{\'e}s, et l{'}illustrons sur un corpus d{'}articles de journaux."
2021.acl-long.100,Measure and Evaluation of Semantic Divergence across Two Languages,2021,-1,-1,2,1,4347,syrielle montariol,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Languages are dynamic systems: word usage may change over time, reflecting various societal factors. However, all languages do not evolve identically: the impact of an event, the influence of a trend or thinking, can differ between communities. In this paper, we propose to track these divergences by comparing the evolution of a word and its translation across two languages. We investigate several methods of building time-varying and bilingual word embeddings, using contextualised and non-contextualised embeddings. We propose a set of scenarios to characterize semantic divergence across two languages, along with a setup to differentiate them in a bilingual corpus. We evaluate the different methods by generating a corpus of synthetic semantic change across two languages, English and French, before applying them to newspaper corpora to detect bilingual semantic divergence and provide qualitative insight for the task. We conclude that BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora; however, the performance of CBOW embeddings is very competitive and more adapted to an exploratory analysis on a large corpus."
2020.lrec-1.302,{F}lau{BERT}: Unsupervised Language Model Pre-training for {F}rench,2020,-1,-1,7,0,5778,hang le,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP."
2020.jeptalnrecital-taln.26,{F}lau{BERT} : des mod{\\`e}les de langue contextualis{\\'e}s pr{\\'e}-entra{\\^\\i}n{\\'e}s pour le fran{\\c{c}}ais ({F}lau{BERT} : Unsupervised Language Model Pre-training for {F}rench),2020,-1,-1,7,0,5778,hang le,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue pr{\'e}-entra{\^\i}n{\'e}s sont d{\'e}sormais indispensables pour obtenir des r{\'e}sultats {\`a} l{'}{\'e}tat-de-l{'}art dans de nombreuses t{\^a}ches du TALN. Tirant avantage de l{'}{\'e}norme quantit{\'e} de textes bruts disponibles, ils permettent d{'}extraire des repr{\'e}sentations continues des mots, contextualis{\'e}es au niveau de la phrase. L{'}efficacit{\'e} de ces repr{\'e}sentations pour r{\'e}soudre plusieurs t{\^a}ches de TALN a {\'e}t{\'e} d{\'e}montr{\'e}e r{\'e}cemment pour l{'}anglais. Dans cet article, nous pr{\'e}sentons et partageons FlauBERT, un ensemble de mod{\`e}les appris sur un corpus fran{\c{c}}ais h{\'e}t{\'e}rog{\`e}ne et de taille importante. Des mod{\`e}les de complexit{\'e} diff{\'e}rente sont entra{\^\i}n{\'e}s {\`a} l{'}aide du nouveau supercalculateur Jean Zay du CNRS. Nous {\'e}valuons nos mod{\`e}les de langue sur diverses t{\^a}ches en fran{\c{c}}ais (classification de textes, paraphrase, inf{\'e}rence en langage naturel, analyse syntaxique, d{\'e}sambigu{\""\i}sation automatique) et montrons qu{'}ils surpassent souvent les autres approches sur le r{\'e}f{\'e}rentiel d{'}{\'e}valuation FLUE {\'e}galement pr{\'e}sent{\'e} ici."
2020.jeptalnrecital-taln.31,{\\'E}tude des variations s{\\'e}mantiques {\\`a} travers plusieurs dimensions (Studying semantic variations through several dimensions ),2020,-1,-1,2,1,4347,syrielle montariol,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Au sein d{'}une langue, l{'}usage des mots varie selon deux axes : diachronique (dimension temporelle) et synchronique (variation selon l{'}auteur, la communaut{\'e}, la zone g{\'e}ographique... ). Dans ces travaux, nous proposons une m{\'e}thode de d{\'e}tection et d{'}interpr{\'e}tation des variations d{'}usages des mots {\`a} travers ces diff{\'e}rentes dimensions. Pour cela, nous exploitons les capacit{\'e}s d{'}une nouvelle ligne de plongements lexicaux contextualis{\'e}s, en particulier le mod{\`e}le BERT. Nous exp{\'e}rimentons sur un corpus de rapports financiers d{'}entreprises fran{\c{c}}aises, pour appr{\'e}hender les enjeux et pr{\'e}occupations propres {\`a} certaines p{\'e}riodes, acteurs et secteurs d{'}activit{\'e}s."
2020.finnlp-1.2,Variations in Word Usage for the Financial Domain,2020,-1,-1,2,1,4347,syrielle montariol,Proceedings of the Second Workshop on Financial Technology and Natural Language Processing,0,None
W19-5802,{LIMSI}-{MULTISEM} at the {IJCAI} {S}em{D}eep-5 {W}i{C} Challenge: Context Representations for Word Usage Similarity Estimation,2019,0,1,3,1,4350,aina soler,Proceedings of the 5th Workshop on Semantic Deep Learning (SemDeep-5),0,None
S19-1002,Word Usage Similarity Estimation with Sentence Representations and Automatic Substitutes,2019,26,0,3,1,4350,aina soler,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Usage similarity estimation addresses the semantic proximity of word instances in different contexts. We apply contextualized (ELMo and BERT) word and sentence embeddings to this task, and propose supervised models that leverage these representations for prediction. Our models are further assisted by lexical substitute annotations automatically assigned to word instances by context2vec, a neural model that relies on a bidirectional LSTM. We perform an extensive comparison of existing word and sentence representations on benchmark datasets addressing both graded and binary similarity.The best performing models outperform previous methods in both settings."
R19-1092,Empirical Study of Diachronic Word Embeddings for Scarce Data,2019,25,0,2,1,4347,syrielle montariol,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Word meaning change can be inferred from drifts of time-varying word embeddings. However, temporal data may be too sparse to build robust word embeddings and to discriminate significant drifts from noise. In this paper, we compare three models to learn diachronic word embeddings on scarce data: incremental updating of a Skip-Gram from Kim et al. (2014), dynamic filtering from Bamler {\&} Mandt (2017), and dynamic Bernoulli embeddings from Rudolph {\&} Blei (2018). In particular, we study the performance of different initialisation schemes and emphasise what characteristics of each model are more suitable to data scarcity, relying on the distribution of detected drifts. Finally, we regularise the loss of these models to better adapt to scarce data."
2019.jeptalnrecital-long.1,Apprentissage de plongements de mots dynamiques avec r{\\'e}gularisation de la d{\\'e}rive (Learning dynamic word embeddings with drift regularisation),2019,-1,-1,2,1,4347,syrielle montariol,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"L{'}usage, le sens et la connotation des mots peuvent changer au cours du temps. Les plongements lexicaux diachroniques permettent de mod{\'e}liser ces changements de mani{\`e}re non supervis{\'e}e. Dans cet article nous {\'e}tudions l{'}impact de plusieurs fonctions de co{\^u}t sur l{'}apprentissage de plongements dynamiques, en comparant les comportements de variantes du mod{\`e}le Dynamic Bernoulli Embeddings. Les plongements dynamiques sont estim{\'e}s sur deux corpus couvrant les m{\^e}mes deux d{\'e}cennies, le New York Times Annotated Corpus en anglais et une s{\'e}lection d{'}articles du journal Le Monde en fran{\c{c}}ais, ce qui nous permet de mettre en place un processus d{'}analyse bilingue de l{'}{\'e}volution de l{'}usage des mots."
2019.jeptalnrecital-court.16,Exploring sentence informativeness,2019,0,0,3,1,4347,syrielle montariol,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"This study is a preliminary exploration of the concept of informativeness {--}how much information a sentence gives about a word it contains{--} and its potential benefits to building quality word representations from scarce data. We propose several sentence-level classifiers to predict informativeness, and we perform a manual annotation on a set of sentences. We conclude that these two measures correspond to different notions of informativeness. However, our experiments show that using the classifiers{'} predictions to train word embeddings has an impact on embedding quality."
C18-1261,Learning with Noise-Contrastive Estimation: Easing training by learning to scale,2018,0,1,2,1,5290,matthieu labeau,Proceedings of the 27th International Conference on Computational Linguistics,0,"Noise-Contrastive Estimation (NCE) is a learning criterion that is regularly used to train neural language models in place of Maximum Likelihood Estimation, since it avoids the computational bottleneck caused by the output softmax. In this paper, we analyse and explain some of the weaknesses of this objective function, linked to the mechanism of self-normalization, by closely monitoring comparative experiments. We then explore several remedies and modifications to propose tractable and efficient NCE training strategies. In particular, we propose to make the scaling factor a trainable parameter of the model, and to use the noise distribution to initialize the output bias. These solutions, yet simple, yield stable and competitive performances in either small and large scale language modelling tasks."
2018.jeptalnrecital-court.29,Algorithmes {\\`a} base d{'}{\\'e}chantillonage pour l{'}entra{\\^\\i}nement de mod{\\`e}les de langue neuronaux (Here the title in {E}nglish),2018,-1,-1,2,1,5290,matthieu labeau,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"L{'}estimation contrastive bruit{\'e}e (NCE) et l{'}{\'e}chantillonage par importance (IS) sont des proc{\'e}dures d{'}entra{\^\i}nement bas{\'e}es sur l{'}{\'e}chantillonage, que l{'}on utilise habituellement {\`a} la place de l{'}estimation du maximum de vraisemblance (MLE) pour {\'e}viter le calcul du softmax lorsque l{'}on entra{\^\i}ne des mod{\`e}les de langue neuronaux. Dans cet article, nous cherchons {\`a} r{\'e}sumer le fonctionnement de ces algorithmes, et leur utilisation dans la litt{\'e}rature du TAL. Nous les comparons exp{\'e}rimentalement, et pr{\'e}sentons des mani{\`e}res de faciliter l{'}entra{\^\i}nement du NCE."
2018.jeptalnrecital-court.34,A comparative study of word embeddings and other features for lexical complexity detection in {F}rench,2018,0,0,3,1,4350,aina soler,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Lexical complexity detection is an important step for automatic text simplification which serves to make informed lexical substitutions. In this study, we experiment with word embeddings for measuring the complexity of French words and combine them with other features that have been shown to be well-suited for complexity prediction. Our results on a synonym ranking task show that embeddings perform better than other features in isolation, but do not outperform frequency-based systems in this language."
W17-4721,{LIMSI}@{WMT}{'}17,2017,-1,-1,4,0,23863,franck burlot,Proceedings of the Second Conference on Machine Translation,0,None
W17-4101,Character and Subword-Based Word Representation for Neural Language Modeling Prediction,2017,25,3,2,1,5290,matthieu labeau,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},0,"Most of neural language models use different kinds of embeddings for word prediction. While word embeddings can be associated to each word in the vocabulary or derived from characters as well as factored morphological decomposition, these word representations are mainly used to parametrize the input, i.e. the context of prediction. This work investigates the effect of using subword units (character and factored morphological decomposition) to build output representations for neural language modeling. We present a case study on Czech, a morphologically-rich language, experimenting with different input and output representations. When working with the full training vocabulary, despite unstable training, our experiments show that augmenting the output word representations with character-based embeddings can significantly improve the performance of the model. Moreover, reducing the size of the output look-up table, to let the character-based embeddings represent rare words, brings further improvement."
E17-2003,An experimental analysis of Noise-Contrastive Estimation: the noise distribution matters,2017,0,3,2,1,5290,matthieu labeau,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Noise Contrastive Estimation (NCE) is a learning procedure that is regularly used to train neural language models, since it avoids the computational bottleneck caused by the output softmax. In this paper, we attempt to explain some of the weaknesses of this objective function, and to draw directions for further developments. Experiments on a small task show the issues raised by an unigram noise distribution, and that a context dependent noise distribution, such as the bigram distribution, can solve these issues and provide stable and data-efficient learning."
2017.jeptalnrecital-long.3,Repr{\\'e}sentations continues d{\\'e}riv{\\'e}es des caract{\\`e}res pour un mod{\\`e}le de langue neuronal {\\`a} vocabulaire ouvert (Opening the vocabulary of neural language models with character-level word representations),2017,-1,-1,2,1,5290,matthieu labeau,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Cet article propose une architecture neuronale pour un mod{\`e}le de langue {\`a} vocabulaire ouvert. Les repr{\'e}sentations continues des mots sont calcul{\'e}es {\`a} la vol{\'e}e {\`a} partir des caract{\`e}res les composant, gr{\`a}ce {\`a} une couche convolutionnelle suivie d{'}une couche de regroupement (pooling). Cela permet au mod{\`e}le de repr{\'e}senter n{'}importe quel mot, qu{'}il fasse partie du contexte ou soit {\'e}valu{\'e} pour la pr{\'e}diction. La fonction objectif est d{\'e}riv{\'e}e de l{'}estimation contrastive bruit{\'e}e (Noise Contrastive Estimation, ou NCE), calculable dans notre cas sans vocabulaire. Nous {\'e}valuons la capacit{\'e} de notre mod{\`e}le {\`a} construire des repr{\'e}sentations continues de mots inconnus sur la t{\^a}che de traduction automatique IWSLT-2016, de l{'}Anglais vers le Tch{\`e}que, en r{\'e}-{\'e}valuant les N meilleures hypoth{\`e}ses (N-best reranking). Les r{\'e}sultats exp{\'e}rimentaux permettent des gains jusqu{'}{\`a} 0,7 point BLEU. Ils montrent aussi la difficult{\'e} d{'}utiliser des repr{\'e}sentations d{\'e}riv{\'e}es des caract{\`e}res pour la pr{\'e}diction."
2017.jeptalnrecital-court.17,Adaptation au domaine pour l{'}analyse morpho-syntaxique (Domain Adaptation for {P}o{S} tagging),2017,-1,-1,4,0,33245,eleonor bartenlian,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,Ce travail cherche {\`a} comprendre pourquoi les performances d{'}un analyseur morpho-syntaxiques chutent fortement lorsque celui-ci est utilis{\'e} sur des donn{\'e}es hors domaine. Nous montrons {\`a} l{'}aide d{'}une exp{\'e}rience jouet que ce comportement peut {\^e}tre d{\^u} {\`a} un ph{\'e}nom{\`e}ne de masquage des caract{\'e}ristiques lexicalis{\'e}es par les caract{\'e}ristiques non lexicalis{\'e}es. Nous proposons plusieurs mod{\`e}les essayant de r{\'e}duire cet effet.
W16-2304,{LIMSI}@{WMT}{'}16: Machine Translation of News,2016,-1,-1,1,1,5598,alexandre allauzen,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2314,The Karlsruhe Institute of Technology Systems for the News Translation Task in {WMT} 2016,2016,24,4,6,0.769231,5767,thanhle ha,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,15,0,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
2016.jeptalnrecital-long.16,Une m{\\'e}thode non-supervis{\\'e}e pour la segmentation morphologique et l{'}apprentissage de morphotactique {\\`a} l{'}aide de processus de {P}itman-{Y}or (An unsupervised method for joint morphological segmentation and morphotactics learning using {P}itman-{Y}or processes),2016,-1,-1,2,0,35951,kevin loser,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Cet article pr{\'e}sente un mod{\`e}le bay{\'e}sien non-param{\'e}trique pour la segmentation morphologique non supervis{\'e}e. Ce mod{\`e}le semi-markovien s{'}appuie sur des classes latentes de morph{\`e}mes afin de mod{\'e}liser les caract{\'e}ristiques morphotactiques du lexique, et son caract{\`e}re non-param{\'e}trique lui permet de s{'}adapter aux donn{\'e}es sans avoir {\`a} sp{\'e}cifier {\`a} l{'}avance l{'}inventaire des morph{\`e}mes ainsi que leurs classes. Un processus de Pitman-Yor est utilis{\'e} comme a priori sur les param{\`e}tres afin d{'}{\'e}viter une convergence vers des solutions d{\'e}g{\'e}n{\'e}r{\'e}es et inadapt{\'e}es au traitemement automatique des langues. Les r{\'e}sultats exp{\'e}rimentaux montrent la pertinence des segmentations obtenues pour le turc et l{'}anglais. Une {\'e}tude qualitative montre {\'e}galement que le mod{\`e}le inf{\`e}re une morphotactique linguistiquement pertinente, sans le recours {\`a} des connaissances expertes quant {\`a} la structure morphologique des formes de mots."
W15-3012,The {KIT}-{LIMSI} Translation System for {WMT} 2015,2015,30,1,5,0.769231,5767,thanhle ha,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015. In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists. Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost. We investigated models using a structured output layer as well as models trained with noise contrastive estimation. Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points."
W15-3016,{LIMSI}@{WMT}{'}15 : Translation Task,2015,16,5,2,0,8610,benjamin marie,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9915 translation task. We report results for French-English, Russian-English in both directions, as well as for Finnish-into-English. Our submissions use NCODE and MOSES along with continuous space translation models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: for Russian-English, we investigate a tailored normalization of Russian to translate into English, and a two-step process to translate first into simplified Russian, followed by a conversion into inflected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side."
W15-3030,{L}ist{N}et-based {MT} Rescoring,2015,21,4,3,0.0662879,5714,jan niehues,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"The log-linear combination of different features is an important component of SMT systems. It allows for the easy integartion of models into the system and is used during decoding as well as for nbest list rescoring. With the recent success of more complex models like neural network-based translation models, n-best list rescoring attracts again more attention. In this work, we present a new technique to train the log-linear model based on the ListNet algorithm. This technique scales to many features, considers the whole list and not single entries during learning and can also be applied to more complex models than a log-linear combination. Using the new learning approach, we improve the translation quality of a largescale system by 0.8 BLEU points during rescoring and generate translations which are up to 0.3 BLEU points better than other learning techniques such as MERT or MIRA."
D15-1025,Non-lexical neural architecture for fine-grained {POS} Tagging,2015,16,20,3,1,5290,matthieu labeau,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we explore a POS tagging application of neural architectures that can infer word representations from the raw character stream. It relies on two modelling stages that are jointly learnt: a convolutional network that infers a word representation directly from the character stream, followed by a prediction stage. Models are evaluated on a POS and morphological tagging task for German. Experimental results show that the convolutional network can infer meaningful word representations, while for the prediction stage, a well designed and structured strategy allows the model to outperform stateof-the-art results, without any feature engineering."
D15-1121,A Discriminative Training Procedure for Continuous Translation Models,2015,33,2,2,1,36853,quockhanh do,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines."
2015.jeptalnrecital-long.4,"Oublier ce qu{'}on sait, pour mieux apprendre ce qu{'}on ne sait pas : une {\\'e}tude sur les contraintes de type dans les mod{\\`e}les {CRF}",2015,-1,-1,2,0.5,36855,nicolas pecheux,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Quand on dispose de connaissances a priori sur les sorties possibles d{'}un probl{\`e}me d{'}{\'e}tiquetage, il semble souhaitable d{'}inclure cette information lors de l{'}apprentissage pour simplifier la t{\^a}che de mod{\'e}lisation et acc{\'e}l{\'e}rer les traitements. Pourtant, m{\^e}me lorsque ces contraintes sont correctes et utiles au d{\'e}codage, leur utilisation lors de l{'}apprentissage peut d{\'e}grader s{\'e}v{\`e}rement les performances. Dans cet article, nous {\'e}tudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entra{\^\i}ne une forme de sous-apprentissage qu{'}il est cependant possible de limiter."
2015.jeptalnrecital-long.23,Apprentissage discriminant des mod{\\`e}les continus de traduction,2015,-1,-1,2,1,36853,quockhanh do,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Alors que les r{\'e}seaux neuronaux occupent une place de plus en plus importante dans le traitement automatique des langues, les m{\'e}thodes d{'}apprentissage actuelles utilisent pour la plupart des crit{\`e}res qui sont d{\'e}corr{\'e}l{\'e}s de l{'}application. Cet article propose un nouveau cadre d{'}apprentissage discriminant pour l{'}estimation des mod{\`e}les continus de traduction. Ce cadre s{'}appuie sur la d{\'e}finition d{'}un crit{\`e}re d{'}optimisation permettant de prendre en compte d{'}une part la m{\'e}trique utilis{\'e}e pour l{'}{\'e}valuation de la traduction et d{'}autre part l{'}int{\'e}gration de ces mod{\`e}les au sein des syst{\`e}mes de traduction automatique. De plus, cette m{\'e}thode d{'}apprentissage est compar{\'e}e aux crit{\`e}res existants d{'}estimation que sont le maximum de vraisemblance et l{'}estimation contrastive bruit{\'e}e. Les exp{\'e}riences men{\'e}es sur la t{\^a}ches de traduction des s{\'e}minaires TED Talks de l{'}anglais vers le fran{\c{c}}ais montrent la pertinence d{'}un cadre discriminant d{'}apprentissage, dont les performances restent toutefois tr{\`e}s d{\'e}pendantes du choix d{'}une strat{\'e}gie d{'}initialisation idoine. Nous montrons qu{'}avec une initialisation judicieuse des gains significatifs en termes de scores BLEU peuvent {\^e}tre obtenus."
F14-1016,Cross-Lingual {POS} Tagging through Ambiguous Learning: First Experiments (Apprentissage partiellement supervis{\\'e} d{'}un {\\'e}tiqueteur morpho-syntaxique par transfert cross-lingue) [in {F}rench],2014,-1,-1,4,0.27027,168,guillaume wisniewski,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
F14-1023,Comparison of scheduling methods for the learning rate of neural network language models (Mod{\\`e}les de langue neuronaux: une comparaison de plusieurs strat{\\'e}gies d{'}apprentissage) [in {F}rench],2014,0,0,2,1,36853,quockhanh do,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
2014.iwslt-papers.6,Discriminative adaptation of continuous space translation models,2014,5,1,2,1,36853,quockhanh do,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"In this paper we explore various adaptation techniques for continuous space translation models (CSTMs). We consider the following practical situation: given a large scale, state-of-the-art SMT system containing a CSTM, the task is to adapt the CSTM to a new domain using a (relatively) small in-domain parallel corpus. Our method relies on the definition of a new discriminative loss function for the CSTM that borrows from both the max-margin and pair-wise ranking approaches. In our experiments, the baseline out-of-domain SMT system is initially trained for the WMT News translation task, and the CSTM is to be adapted to the lecture translation task as defined by IWSLT evaluation campaign. Experimental results show that an improvement of 1.5 BLEU points can be achieved with the proposed adaptation method."
2014.iwslt-evaluation.15,{LIMSI} {E}nglish-{F}rench speech translation system,2014,-1,-1,4,0,31615,natalia segal,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper documents the systems developed by LIMSI for the IWSLT 2014 speech translation task (EnglishâFrench). The main objective of this participation was twofold: adapting different components of the ASR baseline system to the peculiarities of TED talks and improving the machine translation quality on the automatic speech recognition output data. For the latter task, various techniques have been considered: punctuation and number normalization, adaptation to ASR errors, as well as the use of structured output layer neural network models for speech data."
F13-1033,A fully discriminative training framework for Statistical Machine Translation (Un cadre d{'}apprentissage int{\\'e}gralement discriminant pour la traduction statistique) [in {F}rench],2013,0,1,2,1,8590,thomas lavergne,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-3140,Joint {WMT} 2012 Submission of the {QUAERO} Project,2012,37,5,10,0.833333,3519,markus freitag,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in Bleu and 3.4 points in Ter compared to the best single system."
W12-3141,{LIMSI} @ {WMT}12,2012,23,1,3,0,40944,haison le,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model."
W12-2701,Measuring the Influence of Long Range Dependencies with Neural Network Language Models,2012,31,17,2,1,42291,hai le,Proceedings of the {NAACL}-{HLT} 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for {HLT},0,"In spite of their well known limitations, most notably their use of very local contexts, n-gram language models remain an essential component of many Natural Language Processing applications, such as Automatic Speech Recognition or Statistical Machine Translation. This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words. This study is made possible by the development of several novel Neural Network Language Model architectures, which can easily fare with such large context windows. We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufficiently high, and that efforts should be focused on improving the estimation procedures for such large models."
N12-1005,Continuous Space Translation Models with Neural Networks,2012,33,103,2,1,42291,hai le,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance."
W11-2135,{LIMSI} @ {WMT}11,2011,19,17,1,1,5598,alexandre allauzen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the French-English and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up."
W11-2142,Joint {WMT} Submission of the {QUAERO} Project,2011,25,1,9,0.833333,3519,markus freitag,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the joint QUAERO submission to the WMT 2011 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT Germanxe2x86x92English task. Each group translated the data sets with their own systems. Then RWTH system combination combines these translations to a better one. In this paper, we describe the single systems of each group. Before we present the results of the system combination, we give a short description of the RWTH Aachen system combination approach."
W11-2168,From n-gram-based to {CRF}-based Translation Models,2011,46,12,2,1,8590,thomas lavergne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure. Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. The tractability of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems."
2011.jeptalnrecital-long.37,Estimation d{'}un mod{\\`e}le de traduction {\\`a} partir d{'}alignements mot-{\\`a}-mot non-d{\\'e}terministes (Estimating a translation model from non-deterministic word-to-word alignments),2011,-1,-1,2,1,21324,nadi tomeh,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans les syst{\`e}mes de traduction statistique {\`a} base de segments, le mod{\`e}le de traduction est estim{\'e} {\`a} partir d{'}alignements mot-{\`a}-mot gr{\^a}ce {\`a} des heuristiques d{'}extraction et de valuation. Bien que ces alignements mot-{\`a}-mot soient construits par des mod{\`e}les probabilistes, les processus d{'}extraction et de valuation utilisent ces mod{\`e}les en faisant l{'}hypoth{\`e}se que ces alignements sont d{\'e}terministes. Dans cet article, nous proposons de lever cette hypoth{\`e}se en consid{\'e}rant l{'}ensemble de la matrice d{'}alignement, d{'}une paire de phrases, chaque association {\'e}tant valu{\'e}e par sa probabilit{\'e}. En comparaison avec les travaux ant{\'e}rieurs, nous montrons qu{'}en utilisant un mod{\`e}le exponentiel pour estimer de mani{\`e}re discriminante ces probabilit{\'e}s, il est possible d{'}obtenir des am{\'e}liorations significatives des performances de traduction. Ces am{\'e}liorations sont mesur{\'e}es {\`a} l{'}aide de la m{\'e}trique BLEU sur la t{\^a}che de traduction de l{'}arabe vers l{'}anglais de l{'}{\'e}valuation NIST MT{'}09, en consid{\'e}rant deux types de conditions selon la taille du corpus de donn{\'e}es parall{\`e}les utilis{\'e}es."
2011.iwslt-papers.10,How good are your phrases? Assessing phrase quality with single class classification,2011,0,8,4,1,21324,nadi tomeh,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"We present a novel translation quality informed procedure for both extraction and scoring of phrase pairs in PBSMT systems. We reformulate the extraction problem in the supervised learning framework. Our goal is twofold. First, We attempt to take the translation quality into account; and second we incorporating arbitrary features in order to circumvent alignment errors. One-Class SVMs and the Mapping Convergence algorithm permit training a single-class classifier to discriminate between useful and useless phrase pairs. Such classifier can be learned from a training corpus that comprises only useful instances. The confidence score, produced by the classifier for each phrase pairs, is employed as a selection criteria. The smoothness of these scores allow a fine control over the size of the resulting translation model. Finally, confidence scores provide a new accuracy-based feature to score phrase pairs. Experimental evaluation of the method shows accurate assessments of phrase pairs quality even for regions in the space of possible phrase pairs that are ignored by other approaches. This enhanced evaluation of phrase pairs leads to improvements in the translation performance as measured by BLEU."
2011.iwslt-evaluation.7,{LIMSI}{'}s experiments in domain adaptation for {IWSLT}11,2011,20,13,2,1,8590,thomas lavergne,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French using the in-house n-code system, which implements the n-gram based approach to Machine Translation. This framework not only allows to achieve state-of-the-art results for this language pair, but is also appealing due to its conceptual simplicity and its use of well understood statistical language models. Using this approach, we compare several ways to adapt our existing systems and resources to the TED task with mixture of language models and try to provide an analysis of the modest gains obtained by training a log linear combination of inand out-of-domain models."
2011.eamt-1.41,Discriminative Weighted Alignment Matrices For Statistical Machine Translation,2011,22,0,2,1,21324,nadi tomeh,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"In extant phrase-based statistical machine translation (SMT) systems, the translation model relies on word-to-word alignments, which serve as constraints for the subsequent heuristic extraction and scoring processes. Word alignments are usually inferred in a probabilistic framework; yet, only one single best alignment is retained, as if alignments were deterministically produced. In this paper, we explore ways to take into account the entire alignment matrix, where each alignment link is scored by its probability. By comparison with previous attempts, we use an exponential model to compute these probabilities, which enables us to achieve significant improvements on the NIST MTxe2x80x9909 Arabic-English translation task."
W10-1704,{LIMSI}{'}s Statistical Translation Systems for {WMT}{'}10,2010,19,15,1,1,5598,alexandre allauzen,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes our Statistical Machine Translation systems for the WMT10 evaluation, where LIMSI participated for two language pairs (French-English and German-English, in both directions). For German-English, we concentrated on normalizing the German side through a proper preprocessing, aimed at reducing the lexical redundancy and at splitting complex compounds. For French-English, we studied two extensions of our in-house N-code decoder: firstly, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model. For both set of experiments, we report the improvements obtained on the development and test data."
D10-1076,Training Continuous Space Language Models: Some Practical Issues,2010,23,31,2,1,42291,hai le,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms. The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues. A new initialization scheme and new training techniques are then introduced. These methods are shown to greatly reduce the training time and to significantly improve performance, both in terms of perplexity and on a large-scale translation task."
D10-1091,Assessing Phrase-Based Translation Models with Oracle Decoding,2010,30,13,2,0.27027,168,guillaume wisniewski,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Extant Statistical Machine Translation (SMT) systems are very complex softwares, which embed multiple layers of heuristics and embark very large numbers of numerical parameters. As a result, it is difficult to analyze output translations and there is a real need for tools that could help developers to better understand the various causes of errors.n n In this study, we make a step in that direction and present an attempt to evaluate the quality of the phrase-based translation model. In order to identify those translation errors that stem from deficiencies in the phrase table (PT), we propose to compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed."
2010.iwslt-evaluation.13,{LIMSI} @ {IWSLT} 2010,2010,30,0,1,1,5598,alexandre allauzen,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes LIMSI{'}s Statistical Machine Translation systems (SMT) for the IWSLT evaluation, where we participated in two tasks (Talk for English to French and BTEC for Turkish to English). For the Talk task, we studied an extension of our in-house n-code SMT system (the integration of a bilingual reordering model over generalized translation units), as well as the use of training data extracted from Wikipedia in order to adapt the target language model. For the BTEC task, we concentrated on pre-processing schemes on the Turkish side in order to reduce the morphological discrepancies with the English side. We also evaluated the use of two different continuous space language models for such a small size of training data."
2010.amta-papers.18,Refining Word Alignment with Discriminative Training,2010,-1,-1,2,1,21324,nadi tomeh,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"The quality of statistical machine translation systems depends on the quality of the word alignments that are computed during the translation model training phase. IBM alignment models, as implemented in the GIZA++ toolkit, constitute the de facto standard for performing these computations. The resulting alignments and translation models are however very noisy, and several authors have tried to improve them. In this work, we propose a simple and effective approach, which considers alignment as a series of independent binary classification problems in the alignment matrix. Through extensive feature engineering and the use of stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task."
W09-0417,{LIMSI}{`}s Statistical Translation Systems for {WMT}{`}09,2009,18,3,1,1,5598,alexandre allauzen,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus."
W08-0310,Limsi{'}s Statistical Translation Systems for {WMT}{`}08,2008,10,19,3,0.5,47820,daniel dechelotte,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper describes our statistical machine translation systems based on the Moses toolkit for the WMT08 shared task. We address the Europarl and News conditions for the following language pairs: English with French, German and Spanish. For Europarl, n-best rescoring is performed using an enhanced n-gram or a neuronal language model; for the News condition, language models incorporate extra training data. We also report unconvincing results of experiments with factored models."
allauzen-bonneau-maynard-2008-training,Training and Evaluation of {POS} Taggers on the {F}rench {MULTITAG} Corpus,2008,11,21,1,1,5598,alexandre allauzen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The explicit introduction of morphosyntactic information into statistical machine translation approaches is receiving an important focus of attention. The current freely available Part of Speech (POS) taggers for the French language are based on a limited tagset which does not account for some flectional particularities. Moreover, there is a lack of a unified framework of training and evaluation for these kinds of linguistic resources. Therefore in this paper, three standard POS taggers (Treetagger, BrillÂs tagger and the standard HMM POS tagger) are trained and evaluated in the same conditions on the French MULTITAG corpus. This POS-tagged corpus provides a tagset richer than the usual ones, including gender and number distinctions, for example. Experimental results show significant differences of performance between the taggers. According to the tagging accuracy estimated with a tagset of 300 items, taggers may be ranked as follows: Treetagger (95.7{\%}), BrillÂs tagger (94.6{\%}), HMM tagger (93.4{\%}). Examples of translation outputs illustrate how considering gender and number distinctions in the POS tagset can be relevant."
W07-0409,Combining Morphosyntactic Enriched Representation with n-best Reranking in Statistical Translation,2007,13,5,2,0,40028,helene bonneaumaynard,"Proceedings of {SSST}, {NAACL}-{HLT} 2007 / {AMTA} Workshop on Syntax and Structure in Statistical Translation",0,"The purpose of this work is to explore the integration of morphosyntactic information into the translation model itself, by enriching words with their morphosyntactic categories. We investigate word disambiguation using morphosyntactic categories, n-best hypotheses reranking, and the combination of both methods with word or morphosyntactic n-gram language model reranking. Experiments are carried out on the English-to-Spanish translation task. Using the morphosyntactic language model alone does not results in any improvement in performance. However, combining morphosyntactic word disambiguation with a word based 4-gram language model results in a relative improvement in the BLEU score of 2.3% on the development set and 1.9% on the test set."
2007.mtsummit-papers.18,A state-of-the-art statistical machine translation system based on {M}oses,2007,-1,-1,4,0,47820,daniel dechelotte,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.jeptalnrecital-poster.25,Mod{\\`e}les statistiques enrichis par la syntaxe pour la traduction automatique,2007,0,5,4,0,5770,holger schwenk,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,La traduction automatique statistique par s{\'e}quences de mots est une voie prometteuse. Nous pr{\'e}sentons dans cet article deux {\'e}volutions compl{\'e}mentaires. La premi{\`e}re permet une mod{\'e}lisation de la langue cible dans un espace continu. La seconde int{\`e}gre des cat{\'e}gories morpho-syntaxiques aux unit{\'e}s manipul{\'e}es par le mod{\`e}le de traduction. Ces deux approches sont {\'e}valu{\'e}es sur la t{\^a}che Tc-Star. Les r{\'e}sultats les plus int{\'e}ressants sont obtenus par la combinaison de ces deux m{\'e}thodes.
