1999.mtsummit-1.82,C96-1030,0,0.0992647,"f outputs (Brown and Frederking 1995). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. A recent evaluation (Hogan and Frederking 1998) has demonstrated that the MEMT architecture can indeed produce better translations than any single component MT engine. MT Summit VII Sept. 1999 Figure 1: Multi-Engine MT Architecture For the languages developed so far, the primary engines that we have produced have been EBMT and lexical-transfer MT: • EBMT (Brown 1996) uses a sentence-aligned corpus to produce translations. When such a corpus is available, fairly good quality MT for a new domain is available essentially immediately. EBMT is basically a more sophisticated version of Translation Memory, in that sub-sentential chunks of words are matched, allowing much greater coverage. Sentences that match in full are translated exactly, but sub-sentential chunks are matched with a variety of heuristics, which are reflected in the quality scores assigned to the corresponding outputs. • Lexical-transfer MT employs a very simple, very old technology: bilingual"
1999.mtsummit-1.82,1995.tmi-1.17,1,0.700542,"ed chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modeling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking 1995). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. A recent evaluation (Hogan and Frederking 1998) has demonstrated that the MEMT architecture can indeed produce better translations than any single component MT engine. MT Summit VII Sept. 1999 Figure 1: Multi-Engine MT Architecture For the languages developed so far, the primary engines that we have produced have been EBMT and lexical-transfer MT: • EBMT (Brown 1996) uses a sentence-aligned c"
1999.mtsummit-1.82,hogan-frederking-1998-evaluation,1,0.806242,"Missing"
1999.mtsummit-1.82,H93-1038,1,0.888934,"Missing"
1999.mtsummit-1.82,A94-1016,1,0.839096,"ment and broad coverage require humanintervention, but this is readily available, since there is always a trained user present, and we assume that both speakers are trying to cooperate in communicating. This system therefore provides the type of generalpurpose, human-assisted MT that we need for the Translating Telephone. We will briefly describe the most pertinent aspects of the current system here; more details are available elsewhere (Frederking et al. 1997, Frederking et al. 1999). 2.1 Multi-Engine Machine Translation Diplomat uses the Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg 1994). As shown in Figure 1 below, MEMT feeds an input text to several MT engines in parallel, with each engine employing a different MT technology. Morphological analysis, part-of-speech tagging, and possibly other text enhancements can be shared by the engines. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the q"
1999.mtsummit-1.82,C67-1009,0,0.458794,"nslation Diplomat uses the Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg 1994). As shown in Figure 1 below, MEMT feeds an input text to several MT engines in parallel, with each engine employing a different MT technology. Morphological analysis, part-of-speech tagging, and possibly other text enhancements can be shared by the engines. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay 1967, Winograd 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modeling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking 1995). These"
1999.mtsummit-1.82,W97-0409,1,0.774769,"face. We describe our initial work, which is an extension of the Diplomat wearable speech translator. 1 Introduction The Translating Telephone has been the goal of several major speech-to-speech translation projects, such as those at ATR (Moritomi et al. 1993) in Japan, the JANUS group at Carnegie Mellon University (Waibel et al. 1991, Woszczyna et al. 1994), and others in the C-STAR consortium. While there have been numerous significant accomplishments in these projects over a number of years, the ultimate goal of a useful Translating Telephone still seems remote. We in the Diplomat project (Frederking et al. 1997, Frederking et al. 1999) have therefore decided to attack the problem from a new direction, to determine whether more rapid progress can be made. Instead of gradually extending a limited-domain, fully-automatic system, we will begin with a broadcoverage, human-aided speech-to-speech translation system, and attempt to move towards full automation. The rest of Section 1 further describes our strategic view of machine translation research, and our previous work in this particular strategic direction. Section 2 presents the current state of the Diplomat system, which provides the foundation for t"
1999.mtsummit-1.82,P07-2007,0,0.0480748,"Missing"
2005.sigdial-1.14,A00-2029,0,\N,Missing
2005.sigdial-1.14,H94-1039,0,\N,Missing
2005.sigdial-1.14,P01-1066,0,\N,Missing
2007.sigdial-1.46,P99-1040,0,0.690955,"ogue, pages 256–264, c Antwerp, September 2007. 2007 Association for Computational Linguistics digm can be applied in a number of learning problems, and can pave the way towards building routinely self-improving systems. Consider for instance the problem of confidence annotation. Spoken dialog systems use confidence scores to guard against potential misunderstandings: for every utterance, a confidence score reflecting the probability that the system correctly understood the user’s utterance is computed. Confidence annotation models are traditionally built using supervised learning techniques (Litman et al, 1999; Carpenter et al, 2001; San-Segundo et al, 2001; Hazen et al, 2002; Hirchberg et al, 2004.) A corpus of dialogs (typically thousands of utterances) is manually labeled by a human annotator: each utterance is marked as either correctlyunderstood or misunderstood by the system. Supervised learning techniques are then used in conjunction with features that characterize the current utterance to train a model that can predict whether or not this utterance was misunderstood by the system. This approach suffers from the shortcomings we have outlined above: it requires a pre-existing corpus of in-dom"
2020.lrec-1.51,D18-1547,0,0.0230663,"Missing"
2020.lrec-1.51,P17-1045,0,0.0268893,"n et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychkovsky et al., 2011) or post on online communities (without multi-turn interaction) (Mohapatra, 2018; Tan et al., 2019). For NLIE, several works have crowdsourced novice IERs by giving image or paired images (Manuvinakurike et al., 2018a; Wang et al., 2018; Cheng et al., 2018). However, they did not consider i"
2020.lrec-1.51,W17-5526,0,0.0400704,"Missing"
2020.lrec-1.51,D18-1436,0,0.0196089,"ntics between visual information and language, usually with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychko"
2020.lrec-1.51,D14-1086,0,0.0133264,"e form, and includes all the low-level edit arguments. We refer to these specific type of sentences as “Imperative Low-Level Complete Image Edit Requests (ILLC-IERs)”. Collection We crowd-sourced ILLC-IERs using the Amazon Mechanical Turk (AMT) platform. We asked workers to provide a sentence that specifies an image edit and contains REFER, ATTRIBUTE, VALUE elements. For these slots, we provide (i) an image with a highlighted object, (ii) several referring expressions of the highlighted object, (iii) a randomly sampled ATTRIBUTE, and (iv) a randomly sampled VALUE. We used the RefCOCO dataset (Kazemzadeh et al., 2014) for (i) and (ii), and sampled 900 from train, 100 from dev, and 100 from test. Annotation Similar to the data collection, we also crowdsourced annotations using AMT. We choose 4 categories for ILLC-IER BIO tagging (i) ACTION (ii) REFER (iii) ATTRIBUTE (iv) VALUE. REFER , ATTRIBUTE , VALUE are slots in our ontology. ACTION is the imperative verb that corresponds to the edit action (e.g., “increase”, “decrease”, “modify”) and indicates the sign of VALUE. For example, “decrease brightness by 10” is equivalent to “change brightness by -10”. Statistics We collected 2,537 ILLC-IERs. Based on the da"
2020.lrec-1.51,J12-1006,0,0.0303099,"Missing"
2020.lrec-1.51,L18-1683,1,0.90669,"e images via a two-stage process: (i) First, experts look at the original image and the IER and interprets the high-level concepts expressed by the novice. (ii) Second, they come up with one or more low-level edit operations for these concepts and apply these edits. Can we build machines that do the same thing? This motivates the study on Natural Language Image Editing (NLIE). There are mainly two approaches towards NLIE. The two-stage approach follows the editing process of experts. Several works have developed semantic parsers for the first stage, with datasets collected from crowdsourcing (Manuvinakurike et al., 2018a), online image editing communities (Mohapatra, 2018), or spoken conversation with experts (Manuvinakurike et al., 2018b). Though these semantic parsers are able to capture the high-level intent of novices, little has been discussed about the second stage – how to infer edits with these parsers. These datasets still require expert annotations for generating edits, which is a difficult one-to-many mapping. On the other hand, the image generation approach directly generates an edited image given the original image and an IER using the end-toend adversarial learning (Goodfellow et al., 2014) fra"
2020.lrec-1.51,W18-5033,1,0.918865,"e images via a two-stage process: (i) First, experts look at the original image and the IER and interprets the high-level concepts expressed by the novice. (ii) Second, they come up with one or more low-level edit operations for these concepts and apply these edits. Can we build machines that do the same thing? This motivates the study on Natural Language Image Editing (NLIE). There are mainly two approaches towards NLIE. The two-stage approach follows the editing process of experts. Several works have developed semantic parsers for the first stage, with datasets collected from crowdsourcing (Manuvinakurike et al., 2018a), online image editing communities (Mohapatra, 2018), or spoken conversation with experts (Manuvinakurike et al., 2018b). Though these semantic parsers are able to capture the high-level intent of novices, little has been discussed about the second stage – how to infer edits with these parsers. These datasets still require expert annotations for generating edits, which is a difficult one-to-many mapping. On the other hand, the image generation approach directly generates an edited image given the original image and an IER using the end-toend adversarial learning (Goodfellow et al., 2014) fra"
2020.lrec-1.51,W18-4701,1,0.842383,"ce language. Having little or no knowledge of terminologies and techniques, novices use open-domain vocabulary to express their needs. As a result, novice IERs have certain characteristics: (i) ambiguous, (ii) abstract, and (iii) imprecise. Though experts are able to disambiguate or fill-in missing details for novice IERs, grounding opendomain vocabulary have posed difficulties for NLIE. For the two-stage approach, annotators have low or near chance level agreement on certain entities (Brixey et al., 2018), and datasets collected under different scenarios require different annotation schemas (Manuvirakurike et al., 2018). For the image-generation approach, IERs are often imprecise and includes multiple edit operations in a single request (e.g., enhance white balance and contrast) (Wang et al., 2018). If novices have some knowledge of image editing tools, machines could understand IERs more easily and will be able to perform edits more precisely. In this paper we investigate the potential of low-level language for image editing. Motivated by the difficulty of open-domain vocabulary, we propose to use dialogue to bridge novice language (open-domain vocabulary) to image editing terminologies (in-domain vocabular"
2020.lrec-1.51,P19-1644,0,0.0160898,"arning curve. 2.2. Language and Vision There are many research fields which learns to align semantics between visual information and language, usually with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, th"
2020.lrec-1.51,P19-1182,1,0.847594,"lly with an image and a natural language description. Our work falls under the category of a more recent setup, where language is related to the difference between paired images. Several works have been proposed for edited image generation using attention for alignment (Chen et al., 2018; Wang et al., 2018; Cheng et al., 2018). Other works focus more on the language side, either by determining whether a caption is true considering a pair of images (Suhr et al., 2019), or generating captions that describes the difference with latent variables (Jhamtani and Berg-Kirkpatrick, 2018) or attention (Tan et al., 2019). Our system is simpler as it grounds language at a low-level, and processes visual information with an independent segmentation module. Dialogue It is easy to imagine a dialogue for information-seeking domains (Raux et al., 2005; Bohus and Rudnicky, 2009; Dhingra et al., 2017; Wen et al., 2017; El Asri et al., 2017; Budzianowski et al., 2018) such as restaurant booking or movie booking, as these conversations exist in everyday life. Nevertheless, the same could not be said for image editing, as users interact with software tools (without language input) (Bychkovsky et al., 2011) or post on on"
2020.lrec-1.51,E17-1042,0,0.0205266,"Missing"
2021.emnlp-main.400,P08-1095,0,0.0400832,"Entangled Dialogue Multi-participant chat platforms such as Messenger and WhatsApp are common on the Internet. While being easy to communicate with others, messages often flood into a single channel, entangling chat history which is poorly organized and difficult to structure. In contrast, Slack provides a threadopening feature that allows users to manually organize their discussions. It would be ideal if we could design an algorithm to automatically organize an entangled conversation into its constituent threads. This is referred to as the task of dialogue disentanglement (Shen et al., 2006; Elsner and Charniak, 2008; Wang and Oard, 2009; Elsner and Charniak, 2011; Jiang et al., 2018; Kummerfeld et al., 2018; Zhu et al., 2020; Li et al., 2020; Yu and Joty, 2020). Code is released at self-supervised training Selection Loss Attention Loss zero shot adaption Dialogue Disentanglement Figure 1: This is the high-level flow of our proposed approach. Introduction 1 Shared Model https://github. Training data for the dialogue disentanglement task is difficult to acquire due to the need for manual annotation. Typically, the data is annotated in the reply-to links format, i.e. every utterance is linked to one precedi"
2021.emnlp-main.400,P11-1118,0,0.0195669,"rms such as Messenger and WhatsApp are common on the Internet. While being easy to communicate with others, messages often flood into a single channel, entangling chat history which is poorly organized and difficult to structure. In contrast, Slack provides a threadopening feature that allows users to manually organize their discussions. It would be ideal if we could design an algorithm to automatically organize an entangled conversation into its constituent threads. This is referred to as the task of dialogue disentanglement (Shen et al., 2006; Elsner and Charniak, 2008; Wang and Oard, 2009; Elsner and Charniak, 2011; Jiang et al., 2018; Kummerfeld et al., 2018; Zhu et al., 2020; Li et al., 2020; Yu and Joty, 2020). Code is released at self-supervised training Selection Loss Attention Loss zero shot adaption Dialogue Disentanglement Figure 1: This is the high-level flow of our proposed approach. Introduction 1 Shared Model https://github. Training data for the dialogue disentanglement task is difficult to acquire due to the need for manual annotation. Typically, the data is annotated in the reply-to links format, i.e. every utterance is linked to one preceding utterance. The effort is quadratic w.r.t the"
2021.emnlp-main.400,N18-1164,0,0.0146245,"hatsApp are common on the Internet. While being easy to communicate with others, messages often flood into a single channel, entangling chat history which is poorly organized and difficult to structure. In contrast, Slack provides a threadopening feature that allows users to manually organize their discussions. It would be ideal if we could design an algorithm to automatically organize an entangled conversation into its constituent threads. This is referred to as the task of dialogue disentanglement (Shen et al., 2006; Elsner and Charniak, 2008; Wang and Oard, 2009; Elsner and Charniak, 2011; Jiang et al., 2018; Kummerfeld et al., 2018; Zhu et al., 2020; Li et al., 2020; Yu and Joty, 2020). Code is released at self-supervised training Selection Loss Attention Loss zero shot adaption Dialogue Disentanglement Figure 1: This is the high-level flow of our proposed approach. Introduction 1 Shared Model https://github. Training data for the dialogue disentanglement task is difficult to acquire due to the need for manual annotation. Typically, the data is annotated in the reply-to links format, i.e. every utterance is linked to one preceding utterance. The effort is quadratic w.r.t the length of dialogue,"
2021.emnlp-main.400,2021.ccl-1.108,0,0.0465539,"Missing"
2021.emnlp-main.400,N09-1023,0,0.0509677,"rticipant chat platforms such as Messenger and WhatsApp are common on the Internet. While being easy to communicate with others, messages often flood into a single channel, entangling chat history which is poorly organized and difficult to structure. In contrast, Slack provides a threadopening feature that allows users to manually organize their discussions. It would be ideal if we could design an algorithm to automatically organize an entangled conversation into its constituent threads. This is referred to as the task of dialogue disentanglement (Shen et al., 2006; Elsner and Charniak, 2008; Wang and Oard, 2009; Elsner and Charniak, 2011; Jiang et al., 2018; Kummerfeld et al., 2018; Zhu et al., 2020; Li et al., 2020; Yu and Joty, 2020). Code is released at self-supervised training Selection Loss Attention Loss zero shot adaption Dialogue Disentanglement Figure 1: This is the high-level flow of our proposed approach. Introduction 1 Shared Model https://github. Training data for the dialogue disentanglement task is difficult to acquire due to the need for manual annotation. Typically, the data is annotated in the reply-to links format, i.e. every utterance is linked to one preceding utterance. The eff"
2021.emnlp-main.400,2020.emnlp-main.533,0,0.0386867,"see its role in §3.4. Then we use the output embeddings of a one layer transformer (ψ) with 8 heads to encode contextualized representations: n+1 {Vi0 }n+1 i=1 = ψ({Vi }i=1 ) (3) To determine relative importance, we use an attention module (A) to calculate attention scores: vi = MLP(Vi0 ), ∀i ∈ 1 . . . n + 1 (4) n+1 {αi }n+1 i=1 = softmax({vi }i=1 ) (5) The final predicted score is: n+1 X s = MLP( i=1 αi Vi0 ) (6) To the best of our knowledge, previous works adopt complex heuristics to prune out utterances in the Note that s should be 1 for C1 (the correct next long context (Wu et al., 2020; Wang et al., 2020; response), and otherwise 0 (row 1 of the multi-task Gu et al., 2020; Bertero et al., 2020). For example, loss table in Figure 2). This can be optimized using keeping the utterances whose speaker is the same as the binary cross-entropy loss. 4898 response sel. disentanglement shared model !&quot; (!&quot; +&apos;( ), (!# + &apos;( ), (!$ + &apos;( ), (&apos;( + &apos;( ) !# !# ;=1 ; = 1, 2 !$ &apos;(:&quot; !&quot; ü &apos;(:# *+ = - !&quot; + &apos;( , */ = - !# + &apos;( , *0 = - !$ + &apos;( , *1 = -(&apos;( + &apos;( ) 2(*+ , … , *1 ) multi-task loss k=1 k=2 &gt; 1 0 78 0 1 *+4 */4 *04 5(*64 ) &gt; = MLP ( Σ 7C *64 ) 7&quot; *14 7# 7$ 78 !$ &apos;(:&quot; zero shot 0 1 0 arg max Figure 2: Sol"
2021.emnlp-main.400,2020.emnlp-main.512,0,0.0298974,"messages often flood into a single channel, entangling chat history which is poorly organized and difficult to structure. In contrast, Slack provides a threadopening feature that allows users to manually organize their discussions. It would be ideal if we could design an algorithm to automatically organize an entangled conversation into its constituent threads. This is referred to as the task of dialogue disentanglement (Shen et al., 2006; Elsner and Charniak, 2008; Wang and Oard, 2009; Elsner and Charniak, 2011; Jiang et al., 2018; Kummerfeld et al., 2018; Zhu et al., 2020; Li et al., 2020; Yu and Joty, 2020). Code is released at self-supervised training Selection Loss Attention Loss zero shot adaption Dialogue Disentanglement Figure 1: This is the high-level flow of our proposed approach. Introduction 1 Shared Model https://github. Training data for the dialogue disentanglement task is difficult to acquire due to the need for manual annotation. Typically, the data is annotated in the reply-to links format, i.e. every utterance is linked to one preceding utterance. The effort is quadratic w.r.t the length of dialogue, partly explaining the sole existence of humanannotated large-scale dataset (Kumm"
D08-1100,W06-3407,0,0.0239294,"performance of the HMM-based segmentation algorithm since it provides a state representation that better differentiates among dialog segments which belong to dissimilar subtasks. When the Label representation which yielded a better subtask clustering result (see Section 4.2) was used, HMM-based segmentation produced a better result (5th row) especially in the map reading domain. These numbers may appear modest compared to the numbers obtained when segmenting expository text. However, predicting the boundaries of fine-grained subtasks is more difficult even with a supervised learning approach (Arguello and Rosé, 2006). Our results are comparable to Arguello and Rosé’s (2006) results. Between the two segmentation algorithms, the HMM-based algorithm performed slightly worse than TextTiling in the air travel domain but performed significantly better in the map reading domain. The HMM-based algorithm can identify more boundaries between fine-grained subtasks, which occur more often in the map reading domain. TextTiling, which relies on local lexical cohesion, is unlikely to find two significant drops in lexical similarity that are only a couple of utterances apart, and thus fails to detect boundaries of short"
D08-1100,W06-3404,1,0.819536,"ase query form while slots in the form represent search criteria. Nevertheless, a form can represent related pieces of information required to perform any domain action not just a database query action. With this more general definition of a form, a formbased dialog structure representation can be applied to various types of task-oriented domains where dialog participants have to gather pieces of information, analogous to search criteria, through dialog in order to perform domain actions that fulfill a dialog goal. Chotimongkol (2008) provided examples of these domains, for instance, meeting (Banerjee and Rudnicky, 2006) and flight simulation control (Gorman et al., 2003). In the form-based dialog structure representation, task-specific information in each dialog is organized into a three-level structure of concept, subtask and task. A concept is a word or a group of words which captures a piece of information required to perform a domain action. A subtask is a subset of a dialog which contains sufficient concepts to execute a domain action that advances a dialog toward its goal. A task is a subset of a dialog (usually the entire dialog) which contains all the subtasks that belong to the same goal. A subtask"
D08-1100,P06-1026,0,0.0394467,"Missing"
D08-1100,N04-1015,0,0.0243124,"necessary slots in each form from the concepts present in its corresponding cluster. We further simplify the problem by concentrating on the domains that have only one toplevel task (though in principle the approach can be extended to the domains that have multiple toplevel tasks). Since we utilize well-known unsupervised algorithms, only the modifications which are applied to make these algorithms suitable for inferring the structure of a spoken dialog are discussed. 960 Two unsupervised discourse segmentation algorithms are investigated: TextTiling (Hearst, 1997) and Hidden Markov Modeling (Barzilay and Lee, 2004). These algorithms only recover the sequence of subtasks but not the hierarchical structure of subtasks similar to Bangalore et al.’s (2006) chunk-based model. Nevertheless, this simplification is sufficient when a subtask is embedded at the beginning or the end of the higher-level subtask which is the case for most embedded structures we have found. Both algorithms, while performing well with expository text, require modifications when applying to a fine-grained segmentation problem of spoken dialogs. In WSJ text, the average topic length is 428 words (Beeferman et al., 1999) while in the air"
D08-1100,J97-1002,0,0.233096,"ing algorithm in DIA-MOLE (Möller, 1998) requires no additional human annotation to infer a set of domain-specific dialog acts from indomain dialogs. The motivation behind the use of an unsupervised approach is similar to ours, to reduce human effort in creating a new dialog system. 3 Form-based Dialog Structure Representation Many models have been proposed to account for the structure of a human-human conversation. Many such models focus on other aspects of a dialog such as coordinated activities, i.e. turn-taking and grounding, (Traum and Hinkelman, 1992) and regular patterns in the dialog (Carletta et al., 1997) rather than the domain-specific information communicated by participants. More complicated dialog representations (Grosz and Sidner, 1986; Litman and Allen, 1987) model several aspects of a dialog including domain-specific information. However, additional components in these models, such as beliefs and intentions, are difficult to observe directly from a conversation and, as for the current technology, may not be learnable through an unsupervised learning approach. Since the task-specific information that we would like to model will be used for configuring a dialog system, we can view this in"
D08-1100,P04-1010,0,0.0174413,"tion rather than analyzing a large amount of data. Acquiring the task-specific knowledge from a corpus of human-human dialogs is considered a knowledge acquisition process, where the target task structure has not yet been specified but will be explored from data before a dialog system is built. This is contrasted with a dialog structure recognition process (Alexandersson and Reithinger, 1997; 955 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 955–964, c Honolulu, October 2008. 2008 Association for Computational Linguistics Bangalore et al., 2006; Hardy et al., 2004), where pre-specified dialog structure components are recognized as a dialog progresses. We use an unsupervised learning approach in our knowledge acquisition process as it can freely explore the structure in the data without any influence from human supervision. Woszczyna and Waibel (1994) showed that when modeling a dialog state transition diagram from data an unsupervised approach outperformed a supervised one as it better reflects the characteristic of the data. It is also interesting to see how well a machine-learning approach can perform on the problem of taskspecific knowledge acquisiti"
D08-1100,J97-1003,0,0.254265,"ogether so that we can determine a set of necessary slots in each form from the concepts present in its corresponding cluster. We further simplify the problem by concentrating on the domains that have only one toplevel task (though in principle the approach can be extended to the domains that have multiple toplevel tasks). Since we utilize well-known unsupervised algorithms, only the modifications which are applied to make these algorithms suitable for inferring the structure of a spoken dialog are discussed. 960 Two unsupervised discourse segmentation algorithms are investigated: TextTiling (Hearst, 1997) and Hidden Markov Modeling (Barzilay and Lee, 2004). These algorithms only recover the sequence of subtasks but not the hierarchical structure of subtasks similar to Bangalore et al.’s (2006) chunk-based model. Nevertheless, this simplification is sufficient when a subtask is embedded at the beginning or the end of the higher-level subtask which is the case for most embedded structures we have found. Both algorithms, while performing well with expository text, require modifications when applying to a fine-grained segmentation problem of spoken dialogs. In WSJ text, the average topic length is"
D08-1100,J86-3001,0,\N,Missing
H05-1029,C00-1068,0,0.123483,"or handling: (1) the ability to detect the errors, (2) a set of error recovery strategies, and (3) a mechanism for engaging these strategies at the appropriate time. For some of these issues, various solutions have emerged in the community. For instance, systems generally rely on recognition confidence scores to detect potential misunderstandings (e.g. Krahmer et al., 1999; Walker et al., 2000) and use explicit and implicit confirmation strategies for recovery. The decision to engage these strategies is typically based on comparing the confidence score against manually preset thresholds (e.g. Kawahara and Komatani, 2000). For non-understandings, detection is less of a problem (systems know by definition when non-understandings occur). Strategies such as asking the user to repeat or rephrase, providing help, are usually engaged via simple heuristic rules. At the same time, a number of issues remain unsolved: can we endow systems with better error awareness by integrating existing confidence annotation schemes with correction detection mechanisms? Can we diagnose the non-understanding errors on-line? What are the tradeoffs between the 226 various non-understanding recovery strategies? Can we construct a richer"
H89-2021,H90-1045,1,0.638224,"an be carried out with comparable efficiency by voice and by keyboard. Of course, contemporary workstations make available alternate options for movement. The hand-operated mouse is one example, which might prove to be more efficient for some classes of movement. A controlled comparison of speech and mouse movement would be of great interest, but lies beyond the scope of the current study. ,~, 1 2 VOICE I KEYBOARD I 3 4 7 12 20 SCRIPTNUMBER The advantage for speech entry can be due to a number of reasons. First, it may be faster to say a number than to type it (a digit-string entry experiment [3] shows that the break-even point occurs between 3 and 5 digits). Second, when working from paper notes (a probably situation for this task in real life), users do not need to shift their attention from paper to keyboard to screen when speaking a number. They would have to do so if they were typing, particularly if they are hunt-and-peck typists. Data supporting this interpretation can be found in [3]. Figure 6: Total time for movement actions 8 2500 LU I-z Lt.I 4ooj~ z_ 350 300 ._1 25O Of course, we should not lose sight of the fact that the current implementation produces longer total task ti"
H89-2021,H89-1015,1,0.83485,"ker independence allows casual users to easily use the system and eliminates training as well as its associated problems (such as drift). Large vocabularies make it possible to create habitable languages for complex applications. Finally, a natural language processing capability allows the user to express him or herself using familiar locutions. The voice spreadsheet system The voice spreadsheet (henceforth ""vsc"") consists of the uNIx-based spreadsheet program sc interfaced to a recognizer embodying the SPHINX technology described in [4]. Additional description of v s c is available elsewhere [6], as is a description of the spreadsheet language [9]. The recognition component of the voice spreadsheet makes use of two pieces of special-purpose hardware: a signal processing unit (the USA) and a search accelerator BEAM. See [1] for fuller descriptions of these units. The recognition code is embedded in the spreadsheet program, so that the complete system runs as a single process. While the recognition technology base that makes spoken language systems possible is rapidly maturing, there is no corresponding understanding of how such 150 Table 1: Comparison of recognizer performance for on-"
H89-2021,H89-2007,0,0.021731,"Missing"
H90-1046,H90-1045,1,0.798214,"the user to intercept or edit a recognition before it is acted upon by the application. In terms of human communication, the CM performs the error repair necessitated by breakdowns in the communication cbannel (such as might be caused by a noisy telephone line or a loud interruption). It does not concern itself with the consequences of errors due to some misunderstanding on the part of the user (although it does offer an opportunity for the immediate undoing of a just-spoken utterance). Minimally, the system can pass all utterances through without intervention, though at a cost in throughput [2]. The system can also require the user to provide some acknowledgment (vocal or manual) of the correctness of an input, though again at a cost. In more complex implementations, the system can allow for editing (either by voice or by keyboard) the input and for the generation of "" u n d o "" signals for the benefit of the application. The latter facilities are available in our current interface. A more sophisticated system would be able to, by modeling the interaction and by integrating application-state information, detect utterances that might be, with high probability, incorrectly recognized"
H90-1046,H90-1027,0,0.0260263,"mplementation, these are handled separately, theoretically allowing for parallel input to the computer, allowing the use to talk to one application while typing to another one. Applications Our goal in providing an interface to individual applications was to minimize the changes that need to be made in order to incorporate speech into an application, while enforcing a common approach to our system and accordingly a coherence between applications. The diagram in Figure 3 shows the components of a voice-enabled application. Each application incorporates a frame-based parser, described elsewhere [7]. The frames produced by the parser are passed to a Mapper which translates each command to the application into suitable method invocations (see next section). Two styles of interface are possible for existing applications, either the Mapper can emulate an existing interface, generating a stream of keyboard and mouse events for each utterance that correspond to the equivalent input for those modalities or it can access functions within the application directly. A previous system [5] was implemented using the former strategy. In the present case, we chose to have the Mapper access application"
H90-1046,H89-2021,1,\N,Missing
H93-1074,H90-1046,1,0.887249,"advantage displayed by speech at the level of single input operations. The difference can actually be attributed to the additional incurred costs of non-real-time recognition and error correction. While real-time performance can he achieved, it is unlikely that error-free recognition will be available in the near future. Given these shortcomings, we might ask if speech can provide advantages to the user along dimensions other than task speed, for example by reducing the effort needed to generate an input. SYSTEM IMPLEMENTATION The Personal Information Database (PID) component of the OM system [3, 7] served as the database system in this study. Given a search request specified in some combination of first name, last name and affiliation, PID displays a window with the requested information (in this study, the information consisted of name, affiliation and all known telephone numbers). If an unknown name was entered, an error panel came up. If a query was underspecified, a choice panel containing all entries satisfying the query was shown; for example asking for ""Smith"" produced a panel showing all Smiths in the database. The existing PID was altered to incorporate a scroll window in addit"
H94-1010,H94-1022,0,\N,Missing
H94-1010,H93-1004,0,\N,Missing
H94-1010,H90-1021,0,\N,Missing
H94-1010,H90-1020,0,\N,Missing
H94-1010,H92-1003,0,\N,Missing
H94-1010,H93-1003,1,\N,Missing
I08-1035,P01-1017,0,0.0107517,"cribing them. The dataset showed regularity in sentence structure and belonged to a closed domain, making the variations in surface forms more constrained than completely free text. After sentence segmentation we arrived at a set of 3262 sentences. From this set, we selected 3000 for template extraction and kept aside 262 sentences for testing. 3.2 Preprocessing For semantic analysis, we used the ASSERT toolkit (Pradhan et al., 2004) that produces shallow semantic parses using the PropBank conventions. As a by product, it also produces syntactic parses of sentences, using the Charniak parser (Charniak, 2001). For each sentence, we maintained a part-of-speech tagged (leaves of the parse tree), parsed, baseNP2 tagged and semantic role tagged version. The baseNPs were retrieved by pruning the parse trees and not by using a separate NP chunker. The reason for having a baseNP tagged corpus will become clear as we go into the detail of our template extraction techniques. Figure 1 shows a typical output from the Charniak parser and Figure 2 shows the same tree with nodes under the baseNPs pruned. We identified the need to have a domain entity tagger for matching constituents in the sentences. 266 1 2 ht"
I08-1035,P06-2027,0,0.0191762,"erns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface forms. (Collier, 1998) proposed automatic domain template extraction for IE purposes where MUC type templates for particular types of events were constructed. The method relies on the idea from (Luhn, 1958) where statistically significant words of a corpus were extracted. Based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns. However, this method did not look at arbitrary syntactic patterns. (Filatova et al., 2006) improved the paradigm by looking at the most frequent verbs occurring in a corpus and aligning subtrees containing the verb, by using the syntactic parses as a similarity metric. However, long distance dependencies of verbs with constituents were not looked at and deep semantic analysis was not performed on the sentences to find out similar verb subcategorization frames. In contrast, in our predicate argument based approach we look into deeper semantic structures, and align sentences not only based on similar syntactic parses, but also based on the constituents’ roles with respect to the main"
I08-1035,W04-1013,0,0.0401119,"ION ] will move [ DIRECTION ] across [ LOCATION ] [ TIME ] In the current work we address the first problem of automatically extracting domain templates from human written reports. We take a two-step approach to the problem; first, we cluster report sentences based on similarity and second, we extract template(s) corresponding to each cluster by aligning the instances in the cluster. We experimented with two independent, arguably complementary techniques for clustering and aligning – a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin, 2004) based 265 approach that can extract templates containing multiple verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the"
I08-1035,H94-1031,0,0.077152,"xtract template(s) corresponding to each cluster by aligning the instances in the cluster. We experimented with two independent, arguably complementary techniques for clustering and aligning – a predicate argument based approach that extracts more general templates containing one predicate and a ROUGE (Lin, 2004) based 265 approach that can extract templates containing multiple verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract. (Yangarber et al., 2000) describe another system called ExDisco, that extracts event patterns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface"
I08-1035,2001.mtsummit-papers.68,0,0.0172638,"ucture are tagged alongside. we manually looked at the output clusters and made a judgement call whether the candidate clusters are reasonably coherent and potentially correspond to templates. The reason for the poor performance of the approach was the classical parameter estimation problem of determining a priori the number of clusters. We could not find an elegant solution for the problem without losing the motivation of an automated approach. 4.2 A ROUGE Based Approach ROUGE (Lin, 2004) is the standard automatic evaluation metric in the Summarization community. It is derived from the BLEU (Papineni et al., 2001) score which is the evaluation metric used in the Machine Translation community. The underlying idea in the metric is comparing the candidate and the reference sentences (or summaries) based on their token co-occurrence statistics. For example, a unigram based measure would compare the vocabulary overlap between the candidate and reference sentences. Thus, intuitively, we may use the ROUGE score as a measure for clustering the sentences. Amongst the various ROUGE statistics, the most appealing is Weighted Longest Common Subsequence(WLCS). WLCS favors contiguous LCS which corresponds to the int"
I08-1035,N04-1030,0,0.0231612,"ext, we used a corpus from the domain of weather forecasts (Reiter et al., 2005). This is a freely available parallel corpus1 consisting of weather data and human written forecasts describing them. The dataset showed regularity in sentence structure and belonged to a closed domain, making the variations in surface forms more constrained than completely free text. After sentence segmentation we arrived at a set of 3262 sentences. From this set, we selected 3000 for template extraction and kept aside 262 sentences for testing. 3.2 Preprocessing For semantic analysis, we used the ASSERT toolkit (Pradhan et al., 2004) that produces shallow semantic parses using the PropBank conventions. As a by product, it also produces syntactic parses of sentences, using the Charniak parser (Charniak, 2001). For each sentence, we maintained a part-of-speech tagged (leaves of the parse tree), parsed, baseNP2 tagged and semantic role tagged version. The baseNPs were retrieved by pruning the parse trees and not by using a separate NP chunker. The reason for having a baseNP tagged corpus will become clear as we go into the detail of our template extraction techniques. Figure 1 shows a typical output from the Charniak parser"
I08-1035,P03-1002,0,0.0317515,"h we look into deeper semantic structures, and align sentences not only based on similar syntactic parses, but also based on the constituents’ roles with respect to the main predicate. Also, they relied on typical Named Entities (NEs) like location, organization, person etc. and included another entity that they termed as NUMBER. However, for specific domains like weather forecasts, medical reports or student reports, more varied domain entities form slots in templates, as we observe in our data; hence, existence of a module handling domain specific entities become essential for such a task. (Surdeanu et al., 2003) identify arguments for predicates in a sentence and emphasize how semantic role information may assist in IE related tasks, but their primary focus remained on the extraction of PropBank (Kingsbury et al., 2002) type semantic roles. To our knowledge, the ROUGE metric has not been used for automatic extraction of templates. 3 The Data 3.1 Data Description Since our focus is on creating summary items from events or structured data rather than from text, we used a corpus from the domain of weather forecasts (Reiter et al., 2005). This is a freely available parallel corpus1 consisting of weather"
I08-1035,C00-2136,0,0.0217882,"e verbs. As we will see below, both approaches show promise. 2 Related Work There has been instances of template based summarization in popular Information Extraction (IE) evaluations like MUC (Marsh & Perzanowski, 1998; Onyshkevych, 1994) and ACE (ACE, 2007) where hand engineered slots were to be filled for events in text; but the focus lay on template filling rather than their creation. (Riloff, 1996) describes an interesting work on the generation of extraction patterns from untagged text, but the analysis is syntactic and the patterns do not resemble the templates that we aim to extract. (Yangarber et al., 2000) describe another system called ExDisco, that extracts event patterns from un-annotated text starting from seed patterns. Once again, the text analysis is not deep and the patterns extracted are not sentence surface forms. (Collier, 1998) proposed automatic domain template extraction for IE purposes where MUC type templates for particular types of events were constructed. The method relies on the idea from (Luhn, 1958) where statistically significant words of a corpus were extracted. Based on these words, sentences containing them were chosen and aligned using subject-object-verb patterns. How"
I08-1035,P02-1040,0,\N,Missing
I08-1035,M98-1002,0,\N,Missing
L16-1499,E12-2021,0,0.0418682,"Missing"
N06-4003,J97-1003,0,\N,Missing
N06-4003,W06-3404,1,\N,Missing
N07-2019,N04-4034,0,\N,Missing
N15-1064,N10-1138,0,0.0528064,"be expressed on the basis of semantic frames, which encompass three major components: frame (F), frame elements (FE), and lexical units (LU). For example, the frame “food” contains words referring to items of food. A descriptor frame element within the food frame indicates the characteristic of the food. For example, the phrase “low fat milk” should be analyzed with “milk” evoking the food frame and “low fat” filling the descriptor FE of that frame. In our approach, we parse all ASR-decoded utterances in our corpus using SEMAFOR5 , a stateof-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates, where the LUs that correspond to the frames are extracted for slot filling. For example, Figure 2 shows an example of an ASR-decoded text output parsed by SEMAFOR. SEMAFOR generates three frames (capability, expensiveness, and locale by use) for the utterance, which we consider as slot candidates for training the SLU model. Note that for each slot candidate, SEMAFOR also includes the corresponding lexical unit (can i, cheap, 5 http://www.ark.cs.cmu.edu/SEMAFOR/ 621 Independent Semantic Decoder With ou"
N15-1064,W08-1301,0,0.00761516,"Missing"
N15-1064,J06-2003,0,0.0227167,"walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround time for domain-specific dialogue applications is also critical (Chen and Rudnicky, 2014). Prior work shows that it is possible to use the frame-semantics theory to automatically in1 http://www.windowsphone.com/en-us/how-to/ wp8/cortana 2 http://www.google.com/landing/now 3 http://www.apple."
N15-1064,D11-1049,0,0.0360983,"ment perspective, empowering the system with a large knowledge base is of crucial significance to modern spoken dialogue systems. On this end, our work clearly aligns with recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et a"
N15-1064,D12-1093,0,0.00457124,"recent studies on leveraging semantic knowledge graphs for SLU modeling (Heck et al., 2013; Hakkani-T¨ur et al., 2013; Hakkani-T¨ur et al., 2014; El-Kahky et al., 2014; Chen et al., 2014a). While leveraging external knowledge is the trend, efficient inference algorithms, such as random walk, are still less-studied for direct inference on knowledge graphs of the spoken contents. In the natural language processing literature, Lao et al. (2011) used a random walk algorithm to construct inference rules on large entity-based knowledge bases, and leveraged syntactic information for reading the Web (Lao et al., 2012). Even though this work has important contributions, the proposed algorithm cannot learn mutually-recursive relations, and does not to consider lexical items— in fact, more and more studies show that, in addition to semantic knowledge graphs, lexical knowledge graphs (Inkpen and Hirst, 2006; Song et al., 2011; Li et al., 2013b) that model surface-level natural language realization, multiword expressions, and context (Li et al., 2013a), are also critical for short text understanding (Song et al., 2011; Wang et al., 2014). From the engineering perspective, quick and easy development turnaround t"
N15-1064,P14-2050,0,0.0784672,"ings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argument semantics, and associated lexical units for Eng"
N15-1064,N13-1090,0,0.154004,"from automatic speech recognition (ASR) decodings of the raw audio conversation files, and then adapt the FrameNet-style frames to the semantic slots in the target semantic space, so that they can be used practically in the SDSs. Chen et al. formulated the semantic mapping and adaptation problem as a ranking problem to differentiate generic semantic concepts from target semantic space for task-oriented dialogue systems. This paper improves the adaptation process by leveraging distributed word embeddings associated with typed syntactic dependencies between words to infer inter-slot relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014). The proposed framework is shown in Figure 1. In the remainder of the section, we first introduce framesemantic parsing to obtain slot candidates. With slot candidates, then we train the independent semantic decoders. The adaptation process, which is the main focus of this paper, is performed to decide outputted slots. Finally we can build an SLU model based on the learned semantic decoders and induced slots. 3.1 Probabilistic Semantic Parsing FrameNet is a linguistically-principled semantic resource that offers annotations of predicate-argume"
N15-1064,P13-1045,0,0.00828976,"Missing"
N15-1064,P14-2105,0,0.060405,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bagof-words contexts with dependency-based contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each word w is associated with a word vector vw ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both words and contexts such that the dot product vw · vc associated with “good” word-context pairs belonging to the training data D is maximized, leading to the objective function: X 1 arg max log , (5) vw ,vc 1 + exp(−vc · vw ) (w,c)∈D which can be trained using stochastic-gradient updates (Levy and Goldberg, 2"
N15-1064,P98-1013,0,\N,Missing
N15-1064,C98-1013,0,\N,Missing
N15-1064,J14-1002,0,\N,Missing
P09-2023,P99-1040,0,0.374807,"errors in barge-in utterances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results 2 Implicitly Supervised Estimation of ASR"
P09-2023,A00-2028,0,0.474089,"terances. Experimental results showed that the estimated ASR accuracy improved prediction performance. Since this ASR accuracy and the barge-in rate are obtainable at runtime, they improve prediction performance without the need for manual labeling. 1 Introduction The automatic speech recognition (ASR) result is the most important input information for spoken dialogue systems, and therefore, its errors are critical problems. Many researchers have tackled this problem by developing ASR confidence measures based on utterance-level information and dialogue-level information (Litman et al., 1999; Walker et al., 2000). Especially in systems deployed for the general public such as those of (Komatani et al., 2005) and (Raux et al., 2006), the systems need to correctly detect interpretation errors caused by various utterances made by various kinds of users including novices. Furthermore, since some users access such systems repeatedly (Komatani et al., 2007), error detection by using individual user models would be a promising way of improving performance. In another aspect in dialogue systems, certain dialogue patterns indicate that ASR results 2 Implicitly Supervised Estimation of ASR Accuracy 2.1 Predictin"
P09-2023,2007.sigdial-1.46,1,\N,Missing
P15-1047,J80-3005,0,0.713369,"Missing"
P15-1047,P98-1013,0,0.0995053,"that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techniques, including 1) modeling the noisy data, 2) modeling hidden semantics, and 3) modeling the 485 (capability, expensiveness, and locale by use) are generated for the utterance, which we consider as slot candidates for a domain-specific dialogue system (Baker et al., 1998). Then we build a slot matrix Fs with binary values based on the induced slots, which also denotes the slot features for the utterances (right part of the matrix in Figure 1(b)). To build the feature model MF , we concatenate two matrices: can i have a cheap restaurant Frame: expensiveness FT LU: cheap Frame: capability Frame: locale by use FT LU: can FE Filler: i FT/FE LU: restaurant Figure 2: An example of probabilistic framesemantic parsing on ASR output. FT: frame target. FE: frame element. LU: lexical unit. long-range dependencies between observations, in this work we apply an MF approach"
P15-1047,N15-1064,1,0.818583,"l., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits brought by MF techn"
P15-1047,N10-1138,0,0.0189836,"F by integrating a feature model and a knowledge graph propagation model below. p(Mu,x = 1 |θu,x ) = σ(θu,x ) = 4.1 Feature Model First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the"
P15-1047,W09-0505,0,0.0287041,"First, we build a word pattern matrix Fw with binary values based on observations, where each row represents an utterance and each column refers to an observed unigram. In other words, Fw carries the basic word vectors for the utterances, which is illustrated as the left part of the matrix in Figure 1(b). To induce the semantic elements, we parse all ASR-decoded utterances in our corpus using SEMAFOR2 , a state-of-the-art semantic parser for frame-semantic parsing (Das et al., 2010; Das et al., 2013), and extract all frames from semantic parsing results as slot candidates (Chen et al., 2013b; Dinarelli et al., 2009). Figure 2 shows an example of an ASR-decoded output parsed by SEMAFOR. Three FrameNet-defined frames 2 (2) Knowledge Graph Propagation Model Since SEMAFOR was trained on FrameNet annotation, which has a more generic frame-semantic context, not all the frames from the parsing results can be used as the actual slots in the domainspecific dialogue systems. For instance, in Figure 2, we see that the frames “expensiveness” and “locale by use” are essentially the key slots for the purpose of understanding in the restaurant query domain, whereas the “capability” frame does not convey particularly va"
P15-1047,P93-1008,0,0.0867577,"Missing"
P15-1047,N13-1008,0,0.0514169,"tic slots per utterance. 4.4.2 det Figure 4: The dependency parsing result. ln p(Mu |θ) − λθ , where Mu is the vector corresponding to the utterance u from Mu,x in (1), because we assume that each utterance is independent of others. To avoid treating unobserved facts as designed negative facts, we consider our positive-only data as implicit feedback. Bayesian Personalized Ranking (BPR) is an optimization criterion that learns from implicit feedback for MF, which uses a variant of the ranking: giving observed true facts higher scores than unobserved (true or false) facts (Rendle et al., 2009). Riedel et al. (2013) also showed that BPR learns the implicit relations for improving the relation extraction task. dobj can i have a cheap restaurant u∈U = arg max u∈U nsubj u∈U = arg max 4.4.1 ccomp Relation Weight Estimation For the edges in the knowledge graphs, we model the relations between two connected nodes xi and xj as rˆ(xi , xj ), where x is either a slot s or a word pattern w. Since the weights are measured based on the relations between nodes regardless of the directions, we combine the scores of two directional dependencies: Optimization To maximize the objective in (6), we employ a stochastic grad"
P15-1047,J92-1004,0,0.103938,"ed MF approaches produce better SLU models that are able to predict semantic slots and word patterns taking into account their relations and domain-specificity in a joint manner. 1 Introduction A key component of a spoken dialogue system (SDS) is the spoken language understanding (SLU) module—it parses the users’ utterances into semantic representations; for example, the utterance “find a cheap restaurant” can be parsed into (price=cheap, target=restaurant) (Pieraccini et al., 1992). To design the SLU module of a SDS, most previous studies relied on predefined slots1 for training the decoder (Seneff, 1992; Dowding 1 A slot is defined as a basic semantic unit in SLU, such as “price” and “target” in the example. 483 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 483–494, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a knowledge graph propagation based model, fusing both a word-based lexical knowledge graph and a slot-based semantic graph. In fact, as it is shown in the Netflix challenge, MF is credited as the most useful technique for reco"
P15-1047,P13-1045,0,0.0113493,"Missing"
P15-1047,P14-2050,0,0.239037,"n Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matrix Factorization Approach Considering the benefits"
P15-1047,N13-1090,0,0.0868124,"ugh frame-semantic parsing (the yellow block in Figure 1(a)) (Chen et al., 2013b). Section 4.1 explains the detail of the feature model. In order to consider the additional inter-word and inter-slot relations, we propose a knowledge graph propagation model based on two knowledge graphs, which includes a word relation model (blue block) and a slot relation model (pink block), described in Section 4.2. The method of automatic knowledge graph construction is introduced in Section 5, where we leverage distributed word embeddings associated with typed syntactic dependencies to model the relations (Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014; Chen et al., 2015). Finally, we train the SLU model by learning latent feature vectors for utterances and slot candidates through MF techniques. Combining with a knowledge graph propagation model based on word/slot relations, the trained SLU model estimates the probability that each semantic slot occurs in the testing utterance, and how likely each slot is domain-specific simultaneously. In other words, the SLU model is able to transform the testing utterances into domain-specific semantic representations without human involvement. 4 The Matri"
P15-1047,P14-2105,0,0.021105,"ns the word participates in for training embeddings, where the embeddings are less topical but offer more functional similarity compared to original embeddings (Levy and Goldberg, 2014). Table 1 shows the extracted dependency-based contexts for each target word from the example in Figure 4, where headwords and their dependents can form the contexts by following the arc on a word in the dependency tree, and −1 denotes the directionality of the dependency. After replacing original bag-of-words contexts with dependencybased contexts, we can train dependency-based embeddings for all target words (Yih et al., 2014; Bordes et al., 2011; Bordes et al., 2013). For training dependency-based word embeddings, each target x is associated with a vector vx ∈ Rd and each context c is represented as a context vector vc ∈ Rd , where d is the embedding dimensionality. We learn vector representations for both targets and contexts such that the dot product vx · vc associated with “good” targetcontext pairs belonging to the training data D is maximized, leading to the objective function: arg max Target Word restaurant cheap locale by use expansiveness Experiments Experimental Setup In this experiment, we used the Camb"
P15-1047,C98-1013,0,\N,Missing
P15-1047,J14-1002,0,\N,Missing
P15-1047,H93-1008,0,\N,Missing
P15-1047,P14-1004,0,\N,Missing
W00-0306,W98-1425,0,\N,Missing
W00-0306,J98-3006,0,\N,Missing
W00-0306,H89-2017,0,\N,Missing
W02-0711,A94-1016,1,\N,Missing
W02-0711,C96-1030,1,\N,Missing
W02-0711,frederking-etal-2002-field,1,\N,Missing
W06-3404,P03-1071,0,0.0283517,"tanding to assist meeting participants in various tasks during and after meetings. One such task is the retrieval of information from previous meetings, which is typically a difficult and time consuming task for the human Meeting analysis is a quickly growing field of study. In recent years, research has focussed on automatic speech recognition in meetings (Stolcke et al., 2004; Metze et al., 2004; Hain et al., 2005), activity recognition (Rybski and Veloso, 2004), automatic meeting summarization (Murray et al., 2005), meeting phase detection (Banerjee and Rudnicky, 2004) and topic detection (Galley et al., 2003). Relatively little research has been performed on automatically detecting the roles that meeting participants play as they participate in meetings. These roles can be functional (e.g. the facilitator who runs the meeting, and the scribe who is the designated note taker at the meeting), discourse based (e.g. the presenter, and the discussion participant), and expertise related (e.g. the hardware acquisition expert and the speech recognition research expert). Some roles are tightly scoped, relevant to just one meeting or even a part of a meeting. For example, a person can be the facilitator of"
W07-0305,E03-2001,0,0.0490602,"Missing"
W07-0305,2005.sigdial-1.14,1,0.570476,"Missing"
W07-0305,N07-4008,0,0.0431217,"Missing"
W07-0305,H94-1039,0,0.0825901,"Missing"
W07-0305,W03-2123,0,\N,Missing
W07-0305,N07-2003,1,\N,Missing
W08-0805,H92-1073,0,0.449187,"Missing"
W09-2813,I08-1035,1,0.241935,"cular history of a session will affect what is considered to be briefing-worthy. Figure 2: The category tree showing the information types that we expect in a briefing. events are the task creation and task completion actions logged by various cognitive assistants in the system (so-called specialists). As part of the design phase for the template-based generation component, we identified a set of templates, based on the actual briefings written by users in a separate experiment. Ideally, we would like to adopt a corpus-based approach to automatically extract the templates in the domain, like (Kumar et al., 2008), but since the sample briefings available to us were very few, the application of such corpusbased techniques was not necessary. Based on this set of templates we identified the patterns that needed to be extracted from the event logs in order to populate the templates. A ranking model was also designed for ordering instantiations of this set of templates and to recommend the top 4 most relevant ones for a given session. The overall data flow for BA during a session (runtime) is shown in Figure 1. The various specialist modules generate task related events that are logged in a database. The a"
W09-2813,W09-2813,1,0.0523105,"in that experiment chose to include in their reports corresponding to nine categories shown in Figure 2. We found that information can be conveyed at different levels of granularity (for example, qualitatively or quantitatively). The appropriate choice of granularity for 3 System Overview Figure 1: Briefing Assistant Data Flow. The Briefing Assistant Model: We treat the task of briefing generation in the current domain1 as non-textual event-based summarization. The 1 More details about the domain and the interaction of BA with the larger system are mentioned in a longer version of the paper (Kumar et al., 2009) 68 is external to our system; it took into account the template categories we earlier identified. Figure 4 shows a sample briefing email stimulus. The mapping from the sample email in the figure to the categories is as follows: “expected attendance” - Property-Session; “how many sessions have been rescheduled”, “how many still need to be rescheduled”, “any problems you see as you try to reschedule” - Session-Reschedule; “status of food service (I am worried about the keynote lunch)” - Catering Vendors. Training: Eleven expert users5 were asked to provide training by using the system then gene"
W09-2813,P06-1047,0,0.32611,"ing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrob"
W09-2813,N04-1015,0,0.058674,"eration and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a v"
W09-2813,N07-1021,0,0.149906,"n and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for"
W09-2813,W05-1603,0,0.207137,"orming and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs of such models using a voting scheme gives the best results, although our approach is distinguishable from collaborative filtering techniques used for driving recommendation systems (Hofmann, 2004). We believe this is due to the fact that the individual sessions from which ranking models are learned, although they range over the same collection of component t"
W09-2813,W08-1124,0,0.156171,"t al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefi"
W09-2813,W03-0502,0,0.377088,"es draft reports for another set of users. This system, the Briefing Assistant(BA), is part of a set of learning-based cognitive assistants each of which observes users and learns to assist users in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pag"
W09-2813,J98-3005,0,0.334522,"Agarwal and Alexander I. Rudnicky Language Technologies Institute Carnegie Mellon University, Pittsburgh, USA mohitkum,dipanjan,sachina,air@cs.cmu.edu Abstract Report generation from non-textual sources has been previously explored in the Natural Language Generation (NLG) community in a variety of domains, based on, for example, a database of events. However, a purely generative approach is not suitable in our circumstances, as we want to summarize a variety of tasks that the user is performing and present a summary tailored to a target audience, a desirable characteristic of good briefings (Radev and McKeown, 1998). Thus we approach the problem by applying learning techniques combined with a template-based generation system to instantiate the briefing-worthy report items. The task of instantiating the briefing-worthy items is similar to the task of Content Selection (Duboue, 2004) in the Generation pipeline however our approach minimizes linguistic involvement. Our choice of a template-based generative system was motivated by recent discussions in the NLG community (van Deemter et al., 2005) about the practicality and effectiveness of this approach. The plan of the paper is as follows. We describe relev"
W09-2813,W03-1016,0,0.169791,"9 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sports (Oh and Shrobe, 2008) etc. However, in our case we want to summarize a variety of tasks that the user is performing and present a summary to an intended audience (as defined by a report request). Recent advances in NLG research use statistical approaches at various stages of processing in the generation pipeline like content selection (Duboue and McKeown, 2003; Barzilay and Lee, 2004), probabilistic generation rules (Belz, 2007). Our proposed approach differs from these in that we apply machine learning after generation of all the templates, as a post-processing step, to rank them for inclusion in the final briefing. We could have used a general purpose template-based generation framework like TG/2 (Busemann, 2005), but since the number of templates and their corresponding aggregators is limited, we chose an approach based on string manipulation. We found in our work that an approach based on modeling individual users and then combining the outputs"
W09-2813,J05-1002,0,0.411147,"Missing"
W09-2813,W04-1017,0,0.113097,"is part of a set of learning-based cognitive assistants each of which observes users and learns to assist users in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events"
W09-2813,P06-3007,0,0.230527,"in performing their tasks faster and more accurately. The difference between this work from most previous efforts, primarily based on textextraction approaches is the emphasis on learning to summarize event patterns. This work also differs in its emphasis on learning from user behavior in the context of a task. 2 Related Work Event based summarization has been studied in the summarization community. (Daniel et al., 2003) described identification of sub-events in multiple documents. (Filatova and Hatzivassiloglou, 2004) mentioned the use of event-based features in extractive summarization and (Wu, 2006; Li et al., 2006) describe similar work based on events occurring in text. However, unlike the case at hand, all the work on event-based summarization used text as source material. Non-textual summarization has also been explored in the Natural Language Generation (NLG) community within the broad task of generating 67 Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 67–71, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP reports based on database of events in specific domains such as medical (Portet et al., 2009), weather (Belz, 2007), sp"
W09-2813,W00-0410,0,\N,Missing
W09-3910,W06-1643,0,0.65223,"213 air@cs.cmu.edu Satanjeev Banerjee Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 banerjee@cs.cmu.edu pating in the ongoing discussion. Our goal is to make note-taking easier by automatically extracting noteworthy items from spoken interactions in real time, and proposing them to the humans for inclusion in their notes. Judging which pieces of information in a meeting are noteworthy is a very subjective task. The subjectivity of this task is likely to be more acute than even that of meeting summarization, where low inter-annotator agreement is typical e.g. (Galley, 2006), (Liu & Liu, 2008), (Penn & Zhu, 2008), etc – whether a piece of information should be included in a participant’s notes depends not only on its importance, but also on factors such as the participant’s need to remember, his perceived likelihood of forgetting, etc. To investigate whether it is feasible even for a human to predict what someone else might find noteworthy in a meeting, we conducted a Wizard of Oz-based user study where a human suggested notes (with restriction) to meeting participants during the meeting. We concluded from this study (presented in section 2) that this task appear"
W09-3910,N07-2019,1,0.895315,"Missing"
W09-3910,W04-1013,0,0.00440708,"o not test on separate data, nor do we perform any tuning. Using the 3-level annotation described above, we train a 3-way classifier to label each utterance with one of the multilevel noteworthiness labels. In addition, we use the two 2-way merged-label annotations – “definitely show” vs. others and “don’t show” vs. others – to train two more 2way classifiers. In each of these classification Don’t show 0.70 0.93 0.80 Table 2 Inter-Annotator Agreement using Accuracy Etc. 4.3 Automatic Label Prediction Inter-Annotator Rouge Scores Annotations can also be evaluated by computing the ROUGE metric (Lin, 2004). ROUGE, a popular metric for summarization tasks, compares two summaries by computing precision, recall and f-measure over ngrams that overlap between 75 occur in the notes of previous meetings. Finally, we compute the degree of overlap between this utterance and other utterances in the meeting. The motivation for this last feature is to find utterances that are repeats (or near-repeats) of other utterances – repetition may correlate with importance. Other features: In addition to the ngram and ngram overlap features, we also include term frequency – inverse document frequency (tf-idf) featur"
W09-3910,P08-1054,0,0.522488,"ee Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213 banerjee@cs.cmu.edu pating in the ongoing discussion. Our goal is to make note-taking easier by automatically extracting noteworthy items from spoken interactions in real time, and proposing them to the humans for inclusion in their notes. Judging which pieces of information in a meeting are noteworthy is a very subjective task. The subjectivity of this task is likely to be more acute than even that of meeting summarization, where low inter-annotator agreement is typical e.g. (Galley, 2006), (Liu & Liu, 2008), (Penn & Zhu, 2008), etc – whether a piece of information should be included in a participant’s notes depends not only on its importance, but also on factors such as the participant’s need to remember, his perceived likelihood of forgetting, etc. To investigate whether it is feasible even for a human to predict what someone else might find noteworthy in a meeting, we conducted a Wizard of Oz-based user study where a human suggested notes (with restriction) to meeting participants during the meeting. We concluded from this study (presented in section 2) that this task appears to be feasible for humans. Assuming f"
W09-3910,P08-2051,0,\N,Missing
W10-0716,W09-3910,1,0.677967,"be infeasible as the exact value for a given utterance is difficult to ascertain. 3-Class Formulation One way to alleviate this problem is to redefine the task as a 3-class labeling problem. Annotators can be asked to label utterances as either “important”, “unimportant” or “in-between”. Although this formulation creates two decision boundaries, instead of the single one in the 2-class formulation, the expectation is that a large number of utterances with middling importance will simply be assigned to the “in between” class, thus reducing the amount of noise in the data. Indeed we have shown (Banerjee and Rudnicky, 2009) that in-house annotators achieve high inter-annotator agreement when provided with the 3-class formulation. Another way to alleviate the problem of low agreement is to obtain annotations from many annotators, and identify the utterances that a majority of the annotators appear to agree on; such utterances may be considered as good examples of their class. Using multiple annotators is typically not feasible due to cost. In this paper we investigate using MTurk to create 3-class-based summarization annotations from multiple annotators per meeting, and to combine and filter these annotations to"
W10-0716,W06-1643,0,0.00962458,"of the audio, or they are asked to label each transcribed utterance as either “in summary” or “out of summary”. The latter annotation is particularly useful for training and evaluating extractive summarization systems—systems that create summaries by selecting a subset of the utterances. Due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task. Liu and Liu (2008) reported Kappa agreement scores of between 0.11 and 0.35 across 6 annotators, Penn and Zhu (2008) reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and Galley (2006) reported 0.32 on meeting data. Such low levels of agreement imply that the resulting training data is likely to contain a great deal of “noise”—utterances labeled “in summary” or “out of summary”, when in fact they are not good examples of those classes. Disagreements arise due to the fact that utterance importance is a spectrum. While some utterances are clearly important or unimportant, there are many utterances that lie between these extremes. In order to label utterances as either “insummary” or not, annotators must choose an arbitrary threshold at which to make this decision. Simply aski"
W10-0716,P08-2051,0,0.0295886,"lness of a summary is clear, and audio summarization is an active area of research. Within this field, two kinds of human annotations are generally created— annotators are either asked to write a short summary of the audio, or they are asked to label each transcribed utterance as either “in summary” or “out of summary”. The latter annotation is particularly useful for training and evaluating extractive summarization systems—systems that create summaries by selecting a subset of the utterances. Due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task. Liu and Liu (2008) reported Kappa agreement scores of between 0.11 and 0.35 across 6 annotators, Penn and Zhu (2008) reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and Galley (2006) reported 0.32 on meeting data. Such low levels of agreement imply that the resulting training data is likely to contain a great deal of “noise”—utterances labeled “in summary” or “out of summary”, when in fact they are not good examples of those classes. Disagreements arise due to the fact that utterance importance is a spectrum. While some utterances are clearly important or unimportant, the"
W10-0716,N10-1024,0,0.142823,"en established. We find that MTurk can be used to produce highquality transcription and describe two techniques for doing so (voting and corrective). We also show that using a similar approach, high quality annotations useful for summarization systems can also be produced. In both cases, accuracy is comparable to that obtained using trained personnel. 1 2 Introduction Recently, Amazon’s Mechanical Turk (MTurk) has been shown to produce useful transcriptions of speech data; Gruenstein et al. (2009) have successfully used MTurk to correct the transcription output from a speech recognizer, while Novotney and Callison-Burch (2010) used MTurk for transcribing a corpus of conversational speech. These studies suggest that transcription, formerly considered to be an exacting task requiring at least some training, could be carried out by casual workers. However, only fairly simple transcription tasks were studied. We propose to assess the suitability of MTurk for processing more challenging material, specifically recordings of meeting speech. Spontaneous speech can be difficult to transcribe because it may contain false starts, disfluencies, mispronunciations and other defects. Similarly for annotation, meeting content may"
W10-0716,P08-1054,0,0.0825696,"ield, two kinds of human annotations are generally created— annotators are either asked to write a short summary of the audio, or they are asked to label each transcribed utterance as either “in summary” or “out of summary”. The latter annotation is particularly useful for training and evaluating extractive summarization systems—systems that create summaries by selecting a subset of the utterances. Due to the subjectivity involved, we find very low inter-annotator agreement for this labeling task. Liu and Liu (2008) reported Kappa agreement scores of between 0.11 and 0.35 across 6 annotators, Penn and Zhu (2008) reported 0.38 on telephone conversation and 0.37 on lecture speech, using 3 annotators, and Galley (2006) reported 0.32 on meeting data. Such low levels of agreement imply that the resulting training data is likely to contain a great deal of “noise”—utterances labeled “in summary” or “out of summary”, when in fact they are not good examples of those classes. Disagreements arise due to the fact that utterance importance is a spectrum. While some utterances are clearly important or unimportant, there are many utterances that lie between these extremes. In order to label utterances as either “in"
W10-4318,D08-1040,0,0.0454809,"Missing"
W10-4328,W09-3915,0,\N,Missing
W12-1613,2005.sigdial-1.14,1,0.823593,"Missing"
W12-1613,eberhard-etal-2010-indiana,0,0.0202597,"a taxonomy of 15 primitive categories in a concrete “action” framework. In contrast, (Daniel and Denis, 1998) suggested a five-way categorization based on cognitive properties of instructions. Index Terms: Robot Navigation, Spoken Instructions 1 Introduction Generating and interpreting instructions is a topic of enduring interest. Cognitive psychologists have examined how people perceive spatial entities and structure route instructions (Daniel and Denis, 1998; Allen, 1997). Linguists and others have investigated how people articulate route instructions in conversation with people or agents (Eberhard et al., 2010; Gargett et al., 2010; Stoia et al., 2008; Marge and Rudnicky, 2010). Artificial intelligence researchers have shown that under supervised conditions autonomous agents can learn to interpret route instructions (Kollar et al., 2010; MacMahon et al., 2006; Matuszek et al., 2010; Bugmann et al., 2004; Chen and Mooney, 2010). While the subject has been approached from different perspectives, it has been generally held that the language Instructions vary greatly and can include superfluous detail. (Denis et al., 1999) found that when people were asked to read and assess a set of instructions some"
W12-1613,gargett-etal-2010-give,0,0.0141749,"tive categories in a concrete “action” framework. In contrast, (Daniel and Denis, 1998) suggested a five-way categorization based on cognitive properties of instructions. Index Terms: Robot Navigation, Spoken Instructions 1 Introduction Generating and interpreting instructions is a topic of enduring interest. Cognitive psychologists have examined how people perceive spatial entities and structure route instructions (Daniel and Denis, 1998; Allen, 1997). Linguists and others have investigated how people articulate route instructions in conversation with people or agents (Eberhard et al., 2010; Gargett et al., 2010; Stoia et al., 2008; Marge and Rudnicky, 2010). Artificial intelligence researchers have shown that under supervised conditions autonomous agents can learn to interpret route instructions (Kollar et al., 2010; MacMahon et al., 2006; Matuszek et al., 2010; Bugmann et al., 2004; Chen and Mooney, 2010). While the subject has been approached from different perspectives, it has been generally held that the language Instructions vary greatly and can include superfluous detail. (Denis et al., 1999) found that when people were asked to read and assess a set of instructions some of the instructions we"
W12-1613,W10-4328,1,0.927628,"mework. In contrast, (Daniel and Denis, 1998) suggested a five-way categorization based on cognitive properties of instructions. Index Terms: Robot Navigation, Spoken Instructions 1 Introduction Generating and interpreting instructions is a topic of enduring interest. Cognitive psychologists have examined how people perceive spatial entities and structure route instructions (Daniel and Denis, 1998; Allen, 1997). Linguists and others have investigated how people articulate route instructions in conversation with people or agents (Eberhard et al., 2010; Gargett et al., 2010; Stoia et al., 2008; Marge and Rudnicky, 2010). Artificial intelligence researchers have shown that under supervised conditions autonomous agents can learn to interpret route instructions (Kollar et al., 2010; MacMahon et al., 2006; Matuszek et al., 2010; Bugmann et al., 2004; Chen and Mooney, 2010). While the subject has been approached from different perspectives, it has been generally held that the language Instructions vary greatly and can include superfluous detail. (Denis et al., 1999) found that when people were asked to read and assess a set of instructions some of the instructions were deemed unnecessary and could be discarded. T"
W12-1613,stoia-etal-2008-scare,0,0.0186293,"oncrete “action” framework. In contrast, (Daniel and Denis, 1998) suggested a five-way categorization based on cognitive properties of instructions. Index Terms: Robot Navigation, Spoken Instructions 1 Introduction Generating and interpreting instructions is a topic of enduring interest. Cognitive psychologists have examined how people perceive spatial entities and structure route instructions (Daniel and Denis, 1998; Allen, 1997). Linguists and others have investigated how people articulate route instructions in conversation with people or agents (Eberhard et al., 2010; Gargett et al., 2010; Stoia et al., 2008; Marge and Rudnicky, 2010). Artificial intelligence researchers have shown that under supervised conditions autonomous agents can learn to interpret route instructions (Kollar et al., 2010; MacMahon et al., 2006; Matuszek et al., 2010; Bugmann et al., 2004; Chen and Mooney, 2010). While the subject has been approached from different perspectives, it has been generally held that the language Instructions vary greatly and can include superfluous detail. (Denis et al., 1999) found that when people were asked to read and assess a set of instructions some of the instructions were deemed unnecessar"
W12-1613,N03-1033,0,0.0062814,"Missing"
W13-4038,W12-1608,0,0.10766,"rser to segment an utterance into meaningful chunks. The domain knowledge helps in labeling the tokens/phrases with concepts. The parser uses the labeled tokens and the chunked form of the utterance, to classify the utterance into one of the tasks. Open domain resources such as world-wideweb, had been used in the context of speech recognition. (Misu and Kawahara, 2006) and (Creutz et al., 2009) used web-texts to improve the language models for speech recognition in a target domain. They have used a dialog corpus in order to query relevant web-texts to build the target domain models. Although (Araki, 2012) did not conduct empirical experiments, yet they have presented an interesting architecture that exploits an open-domain resource like Freebase.com to build spoken dialog systems. Table 1: Tasks and Concepts in Grammar Tasks Imperative Advisory Instructions Grounding Instructions Concepts Locations Adjectives-of-Locations Pathways LiftingDevice Spatial Relations Numbers Ordinals In this work, we have framed the task prediction problem as a classification problem. We use the user’s utterances to extract lexical semantic features and classify it into being one of the many tasks the system was de"
W13-4038,E09-1012,0,0.0698877,"Missing"
W13-4038,E09-1019,0,0.0122441,"ring the interactions and represent the domain knowledge in-terms of concepts and their instances. Table 1 illustrates the tasks and the concepts used in a navigation domain grammar. The linguistic constructions help the parser to segment an utterance into meaningful chunks. The domain knowledge helps in labeling the tokens/phrases with concepts. The parser uses the labeled tokens and the chunked form of the utterance, to classify the utterance into one of the tasks. Open domain resources such as world-wideweb, had been used in the context of speech recognition. (Misu and Kawahara, 2006) and (Creutz et al., 2009) used web-texts to improve the language models for speech recognition in a target domain. They have used a dialog corpus in order to query relevant web-texts to build the target domain models. Although (Araki, 2012) did not conduct empirical experiments, yet they have presented an interesting architecture that exploits an open-domain resource like Freebase.com to build spoken dialog systems. Table 1: Tasks and Concepts in Grammar Tasks Imperative Advisory Instructions Grounding Instructions Concepts Locations Adjectives-of-Locations Pathways LiftingDevice Spatial Relations Numbers Ordinals In"
W13-4038,2005.sigdial-1.14,1,0.831627,"Missing"
W13-4038,W07-0305,1,0.770948,"annotated task labels and manual transcriptions for the utterances. We filtered out the non-task utterances such as “yes”, “no” and other clarifications from the Roomline corpus. We obtained the ASR output for the Navagati corpus by running the test utterances through PocketSphinx (Huggins-Daines et al., 2006). The Roomline corpus already had the ASR output for the utterances. Table 2 illustrates some of the statistics for each corpus. Our baseline model for the task detection is the Phoenix (Ward, 1991) parser output, which is the default method used in the Ravenclaw/Olympus dialog systems (Bohus et al., 2007). For the Navagati Corpus we have obtained the parser output us• gra: No weighting for term-concept pairs in the Grammar, i.e., P = 1, for all concepts ci of t, P = 0 otherwise. • fb: Weighting using normalized Freebase.com relevance score, i.e., P = f bscore(t, ci ) − f bscore(t, cmin ) f bscore(t, cmax ) − f bscore(t, cmin ) (1) • nell: Weighting for the NELL term-concept pairs using the probability for a belief i.e., P = nellprob(t, ci ) (2) , for all concepts ci of t, P = 0 otherwise. 1 245 Originally has 10356 utts; filtered out non-task utts. • wnpath: Weighting for the term-concept pair"
W13-4038,E09-1066,0,0.0593304,"Missing"
W13-4038,W12-1613,1,0.927851,"d the task prediction problem as a classification problem. We use the user’s utterances to extract lexical semantic features and classify it into being one of the many tasks the system was designed to perform. We harness the power of semantic knowledge bases by bootstraping an utterance with semantic concepts related to the tokens in the utterance. The semantic distance/similarity between concepts in the knowledge base is incorporated into the model using a kernel. We show that our approach improves the task prediction accuracy over a grammar-based approach on two spoken corpora (1) Navagati (Pappu and Rudnicky, 2012): a corpus of spoken route instructions, and (2) Roomline (Bohus, 2003): a corpus of spoken dialog sessions in roomreservation domain. Examples GoToPlace, Turn, etc You_Will_See_Location You_are_at_Location Examples buildings, other landmarks large, open, black, small etc. hallway, corridor, bridge, etc. elevator, staircase, etc. behind, above, on left, etc. turn-angles, distance, etc. first, second, etc. floor numbers The dialog agent uses the root node of a parser output as the task. Figure 1 illustrates a semantic parser output for a fictitious utterance in the navigation domain. The dialog"
W13-4038,J99-3003,0,\N,Missing
W13-4038,N04-3012,0,\N,Missing
W14-0211,P99-1024,0,0.228499,"Missing"
W14-0211,W07-0305,1,\N,Missing
W14-0211,W13-4037,0,\N,Missing
W14-4326,N03-1005,0,0.0453294,"hrases in your research? Tell me some well-known people in your research area How would you describe this talk in a sentence, say a tweet. Give keywords for this talk in your own words. Do you know anyone who might be interested in this talk? Knowledge Acquisition Strategies help towards better task performance. We posit three different circumstances that can trigger knowledge acquisition behavior: (1) initiated by expert users of the system [Holzapfel et al., 2008, Spexard et al., 2006, L¨utkebohle et al., 2009, Rudnicky et al., 2010], (2) triggered by “misunderstanding” of the user’s input [Chung et al., 2003, Filisko and Seneff, 2005, Prasad et al., 2012, Pappu et al., 2014], or (3) triggered by the system. They are described below: Q UERY D RIVEN. The system prompts a user with an open-ended question akin to “how-may-Ihelp-you” to learn what “values” of a slot are of interest to the user. This strategy does not ground user about system’s knowledge limitations. However, it allows the system to acquire information (slot-value pairs) from user’s input. The system can choose to respond to the input or ignore the input depending on its knowledge about the slotvalue pairs in the input. Table 1 shows s"
W14-4326,W13-4038,1,\N,Missing
W14-4326,W12-1608,0,\N,Missing
W14-4326,2012.iwslt-papers.1,0,\N,Missing
W14-4414,P05-1045,0,0.00457601,"tage generation for surface realization. In preprocessing, we perform sentence segmentation for each email, and then manually annotate each sentence with a structure element, which is used to create a structural label sequence for each email and then to model sender style and topic structure for email organization (1st stage in the figure). The defined structural labels include greeting, inform, request, suggestion, question, answer, regard, acknowledgement, sorry, and signature. We also annotate content slots, including general classes automatically created by named entity recognition (NER) (Finkel et al., 2005) and hand-crafted topic classes, to model text content for surface realization (2nd stage in the figure). The content slots include person, organization, location, time, money, percent, and date (general classes), and meeting, issue, and discussion (topic classes). Introduction This paper focuses on synthesizing emails that reflect sender style and the intent of the communication. Such a process might be used for the generation of common messages (for example a request for a meeting without direct intervention from the sender). It can also be used in situations where naturalistic emails are ne"
W14-4414,W14-4425,1,\N,Missing
W14-4425,N04-1015,0,0.0202694,"le and are more spontaneous. Lampert et al. (2009) segmented email messages into zones, including sender zones, quoted conversation zones, and boilerplate zones. This paper only models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characteristics in the emails, both of which may require higher"
W14-4425,P05-1045,0,0.00994748,"d: to have or show respect or concern for, usually at the end of an email. 8. acknowledgement: to show or express appreciation or gratitude. 9. sorry: express regret, compunction, sympathy, pity, etc. 10. signature: a sender’s name usually at the end of the email. We perform sentence segmentation using punctuation and line-breaks and then manually tag each sentence with a structure label. We exclude the header of emails for labeling. Figure 2 shows an example email with structural labels. 3.2.1 General Class We use existing named entity recognition (NER) tools for identifying general classes. Finkel et al. (2005) used CRF to label sequences of words in text that are names of things, such as person, organization, etc. There are three models trained on different data, which are a 4-class model trained for CoNLL1 , a 7-class model trained for MUC, and a 3-class model trained on both data sets for the intersection of those class sets below. • 4-class: location, person, organization, misc • 7-class: location, person, organization, time, money, percent, date Considering that 3-class model performs higher accuracy and 7-class model provides better coverage, we take the union of outputs produced by 3class and"
W14-4425,W12-1508,0,0.0172628,"nly models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characteristics in the emails, both of which may require higher cohesion to be acceptable. This paper describes a two-stage process for stochastic generation of email, in which the first stage structures the emails according to sende"
W14-4425,D09-1096,0,0.0682054,"Missing"
W14-4425,P09-1024,0,0.0279537,"messages differ from them, which reflect senders’ style and are more spontaneous. Lampert et al. (2009) segmented email messages into zones, including sender zones, quoted conversation zones, and boilerplate zones. This paper only models the text in the sender zone, new content from the current sender. In the present work, we investigate the use of stochastic techniques for generation of a different class of communications and whether global structures can be convincingly created in the email domain. A lot of NLG systems are applied in dialogue systems, some of which focus on topic modeling (Sauper and Barzilay, 2009; Barzilay and Lapata, 2008; Barzilay and Lee, 2004), proposing algorithms to balance local fit of information and global coherence. However, they seldom consider to model the speaker’s characteristics. Gill et al. (2012) considered sentiment such as openness and neuroticism to specify characters for dialogue generation. In stead of modeling authors’ attitudes, this paper proposes the first approach of synthesizing emails by modeling their writing patterns. Specifically we investigate whether stochastic techniques can be used to acceptably model longer texts and individual speaker characterist"
W14-4425,J08-1001,0,\N,Missing
W15-4604,W09-3915,0,0.0309854,"similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008). The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011). Others have also developed recovery strategies for situated dialogue. Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the grounding process. A similar study focused on resolving misunderstandings in the humanrobot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009). A body of work on referring expression generation uses object attributes to generate descriptions of referents (e.g., (Guhe and Bard, 2008; Garoufi and Koller, 2014)). Viethen and Dale (2006) compared human-authored referring expressions of objects to existing natural language generation algorithms and found them to have very different content. Crowdsourcing has been shown to provide useful dialogue data: Manuvinakurike and DeVault (2015) used the technique to collect gameplaying conversations. Wang et al. (2012) and Mitchell et al. (2014) have used crowdsourced data for training, while othe"
W15-4604,C92-3134,0,0.526302,". 2015 Association for Computational Linguistics We determined the most common recovery strategies for referential ambiguity and impossible-toexecute problems. Several patterns emerged that suggest ways that people expect agents to recover. Ultimately we intend for dialogue systems to use such strategies in physically situated contexts. 2 Related Work Researchers have long observed miscommunication and recovery in human-human dialogue corpora. The HCRC MapTask had a direction giverdirection follower pair navigate two dimensional schematics with slightly different maps (Anderson et al., 1991). Carletta (1992) proposed several recovery strategies following an analysis of this corpus. The SCARE corpus collected human-human dialogues in a similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008). The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011). Others have also developed recovery strategies for situated dialogue. Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the ground"
W15-4604,W14-5003,0,0.0306641,"robot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009). A body of work on referring expression generation uses object attributes to generate descriptions of referents (e.g., (Guhe and Bard, 2008; Garoufi and Koller, 2014)). Viethen and Dale (2006) compared human-authored referring expressions of objects to existing natural language generation algorithms and found them to have very different content. Crowdsourcing has been shown to provide useful dialogue data: Manuvinakurike and DeVault (2015) used the technique to collect gameplaying conversations. Wang et al. (2012) and Mitchell et al. (2014) have used crowdsourced data for training, while others have used it in real time systems (Lasecki et al., 2013; Huang et al., 2014). 3 Figure 1: An example trial where the operator’s command was “Move to the table”. In red is the robot (centered) pointed toward the back wall. Participants would listen to the operator’s command and enter a response into a text box. ronment then formulate the robot’s response to an operator’s request. Participants listened to an operator’s verbal command then typed in a response. Scenes displayed one of three situations: referential ambiguity (more than one pos"
W15-4604,P05-1030,0,0.465104,"along a supposed axis with the robot. 20 Table 2: Ambiguous scene referent description space. Number of scenes was out of 25 total. We relate the current terms to general types defined by Carlson and Hill (2009). properties in visual search tasks when they are highly distinguishable. • Hypothesis 4: In cases of referential ambiguity where two candidate referents are present, responses will confirm one referent in the form of a yes-no question more than presenting a list. Results from an analysis of task-oriented dialogue suggests that people are efficient when asking clarification questions (Rieser and Moore, 2005). Additionally, Clark’s least effort principle (Clark, 1996) suggests that clarifying one referent using a yes-no confirmation would require less effort than presenting a list in two ways: producing a shorter question and constraining the range of responses to expect. • Hypothesis 5: For impossible-to-execute instructions, responses will most commonly be ways for the robot to proactively work with the operator’s instruction, in an effort to get the conversation back on track. The other possible technique, to simply declare that the problem is not possible, will be less common. This is because"
W15-4604,stoia-etal-2008-scare,0,0.18769,"ely we intend for dialogue systems to use such strategies in physically situated contexts. 2 Related Work Researchers have long observed miscommunication and recovery in human-human dialogue corpora. The HCRC MapTask had a direction giverdirection follower pair navigate two dimensional schematics with slightly different maps (Anderson et al., 1991). Carletta (1992) proposed several recovery strategies following an analysis of this corpus. The SCARE corpus collected human-human dialogues in a similar scenario where the direction follower was situated in a three-dimensional virtual environment (Stoia et al., 2008). The current study follows up an initial proposal set of recovery strategies for physically situated domains (Marge and Rudnicky, 2011). Others have also developed recovery strategies for situated dialogue. Kruijff et al. (2006) developed a framework for a robot mapping an environment that employed conversational strategies as part of the grounding process. A similar study focused on resolving misunderstandings in the humanrobot domain using the Wizard-of-Oz methodology (Koulouri and Lauria, 2009). A body of work on referring expression generation uses object attributes to generate descriptio"
W15-4604,W06-1410,0,0.45068,"s, 2007; Skantze, 2007)). Situated grounding problems are associated with the main joint activities; to resolve them we believe that the recovery model must be extended to include planning and environment information. Flexible recovery strategies make this possible by enabling dialogue partners to coordinate their joint activities and accomplish tasks. We cast the problem space as one where the agent aims to select the most efficient recovery strategy that would resolve a user’s intended referent. We expect that this efficiency is tied to the cognitive load it takes to produce clarifications. Viethen and Dale (2006) suggest a similar prediction in their study comparing human and automatically generated referring expressions of objects and their properties. We sought to answer the following questions in this work: • How good are people at detecting situated grounding problems? • How do people organize recovery strategies? • When resolving ambiguity, which properties do people use to differentiate referents? • When resolving impossible-to-execute instructions, do people use active or passive ways to get the conversation back on track? We describe an empirical study that crowdsourced human-authored recovery"
W16-3608,W15-4652,1,0.642075,"t al., 2014)) and asking the user to explain missing words. (Schmidt et al., 2015). In this paper, we propose a set of strategies that actively deal with both user engagement and system response appropriateness. 2 3 Related Work Many experiments have shown that an agent reacting to a user’s behavior or internal state leads to better user experience. In an in-car navigation setting, a system that reacts to the user’s cognitive load was shown to have better user experience (Kousidis et al., 2014). In a direction giving setting, a system that reacts to user’s attention was shown to be preferred (Yu et al., 2015a). In a tutoring setting, a system that reacts to the user’s disengagement resulted in better learning gain (ForbesRiley and Litman, 2012). In task-oriented systems users have a concrete reason to interact with the system. However, in a non-task-oriented setting, user engagement is the sole reason for the user to stay in the conversation, making it an ideal situation for engagement study. In this paper, we focus on making the system reactive to user engagement in real time in an everyday chatting setting. In human-human conversations, engagement has been studied extensively. Engagement is con"
W16-3608,P12-3007,0,0.019759,"Missing"
W16-3608,W12-1630,0,0.0338049,"Missing"
W16-3608,C14-1088,0,0.0189675,"xpert in a Wizard-of-Oz setting, it is the first step towards a fully automated engagement reactive system. Previously very little research addressed reactive systems due to the difficulty of modeling the users and the lack of audiovisual data. We also make the audiovisual data along with the annotations available. ily, making users have the interest to continue is critical. A lot of conversational strategies have been proposed in previous work to avoid generating incoherent utterances in non-task-oriented conversations, such as introducing topics, (e.g. “Let’s talk about favorite foods!” in (Higashinaka et al., 2014)) and asking the user to explain missing words. (Schmidt et al., 2015). In this paper, we propose a set of strategies that actively deal with both user engagement and system response appropriateness. 2 3 Related Work Many experiments have shown that an agent reacting to a user’s behavior or internal state leads to better user experience. In an in-car navigation setting, a system that reacts to the user’s cognitive load was shown to have better user experience (Kousidis et al., 2014). In a direction giving setting, a system that reacts to user’s attention was shown to be preferred (Yu et al., 2"
W16-3608,D11-1054,0,0.116176,"Missing"
W16-3649,P12-3007,0,0.00946686,"ction Non-task-oriented conversational systems do not have a stated goal to work towards. Nevertheless, they are useful for many purposes, such as keeping elderly people company and helping second language learners improve conversation and communication skills. More importantly, they can be combined with task-oriented systems to act as a transition smoother or a rapport builder for complex tasks that require user cooperation. There are a variety of methods to generate responses for nontask-oriented systems, such as machine translation (Ritter et al., 2011), retrieval-based response selection (Banchs and Li, 2012), and sequence-tosequence recurrent neural network (Vinyals and Le, 2015). However, these systems still produce utterances that are incoherent or inappropriate from time to time. To tackle this problem, we propose a set of conversational strategies, such as switching topics, to avoid possible inappropriate responses (breakdowns). After we have a set of strategies, which strategy to perform according to Reinforcement learning was introduced to the dialog community two decades ago (Biermann and Long, 1996) and has mainly been used in task-oriented systems (Singh et al., 1999). Researchers have p"
W16-3649,C14-1088,0,0.0167215,"omputable. Perplexity of the language model is an automatically computable metric but is hard to interpret (Vinyals and Le, 2015). In this paper, we propose three metrics: turn-level appropriateness, conversational depth and information gain, which access both the local and the global conversation quality of a non-task-oriented conversation. Information Related Work Many generic conversational strategies have been proposed in previous work to avoid generating incoherent utterances in non-task-oriented conversations, such as introducing new topics (e.g. “Let’s talk about favorite foods!” ) in (Higashinaka et al., 2014), asking the user to explain missing words (e.g. “What is SIGDIAL?”) (Maria Schmidt and Waibel, 2015). In this paper, we propose a set of generic strategies that are inspired by previous work, and test their usability on human users. No researcher has investigated thoroughly on which strategy to use in different conversational contexts. Compared to task-oriented dialog systems, non1 www.cmuticktock.org 405 when the posterior probability of the generated response is higher than a certain threshold. gain is automatically quantifiable. We use supervised machine learning methods to built automatic"
W16-3649,P98-2219,0,0.189651,"ign a dialog system that takes actions to maximize some measure of system reward, such as task completion rate or dialog length. The difficulty of such modeling lies in the state representation. Representing the dialog by the entire history is often neither feasible nor 404 Proceedings of the SIGDIAL 2016 Conference, pages 404–412, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics conceptually useful, and the so-called belief state approach is not possible, since we do not even know what features are required to represent the belief state. Previous work (Walker et al., 1998) has largely dealt with this issue by imposing prior limitations on the features used to represent the approximate state. In this paper, instead of focusing on task-oriented systems, we apply reinforcement learning to design a policy to select designed conversation strategies in a non-task-oriented dialog systems. Unlike task-oriented dialog systems, non-task-oriented systems have no specific goal that guides the interaction. Consequently, evaluation metrics that are traditionally used for reward design, such as task completion rate, are no longer appropriate. The state design in reinforcement"
W16-3649,W10-4339,0,0.0220514,"e more varied conversation history, which are thus harder to formulate as a mathematical problem. In this work, we propose a method to use statistical findings in conversational study to constrain the dialog history space and to use reinforcement learning for statistical policy learning in a non-task-oriented conversation setting. To date, reinforcement learning is mainly used for learning dialogue policies for slot-filling taskoriented applications such as bus information search (Lee and Eskenazi, 2012), restaurant recommendations (Jurˇc´ıcˇ ek et al., 2012), and sightseeing recommendations (Misu et al., 2010). Reinforcement learning is also used for some more complex systems, such as learning negotiation policies (Georgila and Traum, 2011) and tutoring (Chi et al., 2011). Reinforcement learning is also used in question-answering systems (Misu et al., 2012). Question-answering systems are very similar to non-task-oriented systems except that they do not consider dialog context in generating responses. They have pre-existing questions that the user is expected to go through, which limits the content space of the dialog. Reinforcement learning has also been applied to a non-task-oriented system for d"
W16-3649,W12-1611,0,0.0134265,"ing for statistical policy learning in a non-task-oriented conversation setting. To date, reinforcement learning is mainly used for learning dialogue policies for slot-filling taskoriented applications such as bus information search (Lee and Eskenazi, 2012), restaurant recommendations (Jurˇc´ıcˇ ek et al., 2012), and sightseeing recommendations (Misu et al., 2010). Reinforcement learning is also used for some more complex systems, such as learning negotiation policies (Georgila and Traum, 2011) and tutoring (Chi et al., 2011). Reinforcement learning is also used in question-answering systems (Misu et al., 2012). Question-answering systems are very similar to non-task-oriented systems except that they do not consider dialog context in generating responses. They have pre-existing questions that the user is expected to go through, which limits the content space of the dialog. Reinforcement learning has also been applied to a non-task-oriented system for deciding which sub-system to choose to generate a system utterance (Shibata et al., 2014). In this paper, we used reinforcement learning to learn a policy to sequentially decide which conversational strategy to use to avoid possible system breakdowns. T"
W16-3649,D11-1054,0,0.00767479,"he local and global quality of the conversation. 1 Introduction Non-task-oriented conversational systems do not have a stated goal to work towards. Nevertheless, they are useful for many purposes, such as keeping elderly people company and helping second language learners improve conversation and communication skills. More importantly, they can be combined with task-oriented systems to act as a transition smoother or a rapport builder for complex tasks that require user cooperation. There are a variety of methods to generate responses for nontask-oriented systems, such as machine translation (Ritter et al., 2011), retrieval-based response selection (Banchs and Li, 2012), and sequence-tosequence recurrent neural network (Vinyals and Le, 2015). However, these systems still produce utterances that are incoherent or inappropriate from time to time. To tackle this problem, we propose a set of conversational strategies, such as switching topics, to avoid possible inappropriate responses (breakdowns). After we have a set of strategies, which strategy to perform according to Reinforcement learning was introduced to the dialog community two decades ago (Biermann and Long, 1996) and has mainly been used in task"
W97-0409,C96-1030,0,0.132693,"OMAT (showing how it is well-suited for interactive user correction), describe our approach to rapid-deployment speech recognition and then discuss our approach to interactive user correction of errors in the overall system. 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-"
W97-0409,1995.tmi-1.17,1,0.768974,"chart data structure (Kay, 1967; Winograd, 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modelling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking, 1995; Frederking, 1994). These selection techniques attempt to produce the best overall result, taking the probability of transitions between segments into account as well as modifying the quality scores of individual segments. Differences in the development times and costs of different .technologies can be exploited to enable MT systems to be rapidly deployed for new languages (Frederking and Brown, 1996). If parallel corpora are available for a new language pair, the E B M T engine can provide translations for a new language in a m a t t e r of hours. Knowledgebases for lexical-transfer M T can"
W97-0409,1991.mtsummit-papers.3,0,0.129582,"Missing"
W97-0409,A94-1016,1,0.942357,"T system that performs initial translations at a useful level of quality between a new language and English within a matter of days or weeks, with continual, graceful improvement to a good level of quality over a period of months. The speech understanding component used is the SPHINX II HMM-based speaker-independent continuous speech recognition system (Huang el al., 1992; Ravishankar, 1996), with techniques for rapidly developing acoustic and language models for new languages (Rudnicky, 1995). The machine translation (MT) technology is the MultiEngine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994), described further below. The speech synthes!s component is a newly-developed concatenative system (Lenzo, 1997) based on variable-sized compositional units. This use of subword concatenation is especially important, since it is the only currently available method for rapidly bringing up synthesis for a new language. DIPLOMAT thus involves research in MT, speech understanding and synthesis, interface design, as well as wearable computer systems. While beginning our investigations into new semi-automatic techniques for both speech and MT knowledge-base development, we have already produced an"
W97-0409,C67-1009,0,0.681711,"such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technologies. As shown in Figure 1, M E M T feeds an input text to several MT engines in parallel, with each engine employing a different MT technology 1. Each engine attempts to translate the entire input text, segmenting each sentence in whatever manner is most appropriate for its technology, and putting the resulting translated output segments into a shared chart data structure (Kay, 1967; Winograd, 1983) after giving each segment a score indicating the engine&apos;s internal assessment of the quality of the output segment. These output (target language) segments are indexed in the chart based on the positions of the corresponding input (source language) segments. Thus the chart contains multiple, possibly overlapping, alternative translations. Since the scores produced by the engines are estimates of variable accuracy, we use statistical language modelling techniques adapted from speech recognition research to select the best overall set of outputs (Brown and Frederking, 1995; Fre"
W97-0409,A83-1029,0,0.0247733,"ion Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-line corpora, such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technologies. As show"
W97-0409,1991.mtsummit-papers.9,0,0.0818523,"Missing"
W97-0409,H93-1038,1,0.938628,". 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-line corpora, such as electronic dictionaries or online bilingual parallel texts. The Multi-Engine Machine Translation (MEMT) architecture (Frederking and Nirenburg, 1994) makes it possible to exploit the differences between MT technol"
W97-0409,1996.amta-1.35,1,0.879027,"re used in DIPLOMAT (showing how it is well-suited for interactive user correction), describe our approach to rapid-deployment speech recognition and then discuss our approach to interactive user correction of errors in the overall system. 2 Multi-Engine Translation Machine Different MT technologies exhibit different strengths and weaknesses. Technologies such as Knowledge-Based MT (KBMT) can provide highquality, fully-automated translations in narrow, well-defined domains (Mitamura el al., 1991; Farwell and Wilks, 1991). Other technologies such as lexical-transfer MT (Nirenburg et al., 1995; Frederking and Brown, 1996; MacDonald, 1963), and Example-Based MT (EBMT) (Brown, 1996; Na61 gao, 1984: Sato and Nagao, 1990) provide lowerquality general-purpose translations, unless they are incorporated into human-assisted MT systems (Frederking et al., 1993; Melby, 1983), but can be used in non-domain-restricted translation applications. Moreover, these technologies differ not just in the quality of their translations, and level of domain-dependence, but also along other dimensions, such as types of errors they make, required development time, cost of development, and ability to easily make use of any available on-"
