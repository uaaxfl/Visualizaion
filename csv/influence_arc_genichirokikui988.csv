1999.mtsummit-1.24,P99-1001,0,0.0196818,"Missing"
1999.mtsummit-1.24,1998.amta-tutorials.5,0,0.080672,"Missing"
2005.iwslt-1.2,W02-0706,0,0.0227942,"y only the singlebest outcome of speech recognition is used in the machine translation component. Due to the inevitable errors of speech recognition, speech translation cannot achieve the same level of translation performance as that achieved by perfect text input. To overcome the weakness in speech translation, several architectures have been proposed so far. [1] proposed a coupling structure to combine automatic speech recognition and statistical machine translation. [2] used a unified structure where the maximum entropy approach is proposed to build entire speech translation system models. [3] and [4] implemented the integrated structure by means of finite-state network, and a comparison with the cascaded structure was made. Strictly speaking, this approach does not use the statistical speech translation structure [1]. In this work we used the speech recognition word lattice as the output of speech recognition and the input of machine translation. While in this structure the speech recognition component and the machine translation component are sequentially connected, more hypotheses are stored in the word lattice than the single-best structure; complementary information, such as t"
2005.iwslt-1.2,J93-2003,0,0.0243623,"eech recognition word lattice as the output of speech recognition and the input of machine translation. While in this structure the speech recognition component and the machine translation component are sequentially connected, more hypotheses are stored in the word lattice than the single-best structure; complementary information, such as the acoustic model and language model scores instantiated by posterior probability, was used in the machine translation component to enhance translation performance. In the field of statistical machine translation, the famous, early models are the IBM models [5], using Bayes rule to convert P (e|f ) into P (e)P (f |e). The IBM Models 1 through 5 introduced various models for P (f |e) with increasing complexity. Another popular model is called an “HMM” model [6], which adds alignment probability in the basis of IBM Model 1. Recently a direct modeling of P (e|f ) in the maximum entropy framework, the log-linear model, has been proved effective [7]. This model can integrate a number of features log-linearly. Hence, we use the statistical log-linear model as the translation model in this work. We implemented a new lattice translation decoding algorithm s"
2005.iwslt-1.2,P03-1021,0,0.0071323,"ntiated by posterior probability, was used in the machine translation component to enhance translation performance. In the field of statistical machine translation, the famous, early models are the IBM models [5], using Bayes rule to convert P (e|f ) into P (e)P (f |e). The IBM Models 1 through 5 introduced various models for P (f |e) with increasing complexity. Another popular model is called an “HMM” model [6], which adds alignment probability in the basis of IBM Model 1. Recently a direct modeling of P (e|f ) in the maximum entropy framework, the log-linear model, has been proved effective [7]. This model can integrate a number of features log-linearly. Hence, we use the statistical log-linear model as the translation model in this work. We implemented a new lattice translation decoding algorithm specialized for speech translation. In the decoding we used a two-pass search strategy, graph-based plus A*. For the first graph search, we integrated features from IBM Model 1 into the log-linear model while IBM Model 4’s features were integrated in the second A* search. We invented a new method to minimize the size of the raw lattice generated by the speech recognizer to reduce the decod"
2005.iwslt-1.2,W02-1021,0,0.033555,"nded by IBM model 4 [5] and the acoustic feature is replaced by the posterior probability. 3. Word lattice translation – WLT Word lattice translation is much more complicated than text translation. In contrast to text translation where a single source sentence is known, there is no single source sentence for word lattice translation but a lattice containing multiple hypotheses. Which hypothesis is the best one to be translated is unknown before the decoding is completed. We use the graph+A* decoding approach for the word lattice translation. This approach has been used for text translation by [8]. We extend the approach to speech translation in this work. We adopted this approach because it can keep more hypotheses in a compact structure. The graph+A* decoding is a two-pass decoding. The first pass uses a simple model to generate a word graph to save the most likely hypotheses. It amounts to converting a source language word lattice (SWL) into a target language word graph (TWG). Edges in the SWL are aligned to some edges in the TWG. The second pass uses a complicated model to output the best hypothesis by traversing the target word graph. We describe the two-pass WLT algorithm in the"
2005.iwslt-1.2,2004.iwslt-evaluation.1,0,0.0552724,"Missing"
2005.iwslt-1.3,C04-1168,1,0.887071,"d, a pseudo-lattice decoding algorithm for translating word lattice, can dramatically reduce computation cost incurred by the first approach. We found in the experiments that both of these approaches could improve speech translation significantly. 1. Introduction At least two components are involved in speech to speech translation: automatic speech recognizer and machine translation. Unlike plain text translation, the performance of speech translation may be degraded due to the speech recognition errors. Several approaches have been proposed to compensate for the loss of recognition accuracy. [1] proposed N -best recognition hypothesis translation, which translates all the top N hypotheses and then outputs the highest scored translations by ways of weighing all the translations using a log-linear model. [2] used word lattices to improve translations. [3] used finite state transducers (FST) to convey the features from acoustic analysis and source target translation models. All these approaches realized an integration between speech recognition modules and machine translation modules so that information from speech recognition, such as acoustic model score and language model score, can"
2005.iwslt-1.3,W02-1018,0,0.0168812,"te state transducers (FST) to convey the features from acoustic analysis and source target translation models. All these approaches realized an integration between speech recognition modules and machine translation modules so that information from speech recognition, such as acoustic model score and language model score, can be exploited in the translation module to achieve the maximum performance over the single-best translation. In the field of machine translation, the phrase-based statistical machine translation approach is widely accepted at present. The related literature can be found in [4] [5]. But previously, word-based statistical machine translation, pioneered by IBM Models 1 to 5 [6], were used widely. In the evaluation, we used both the word-based and phrase-based systems. However, the purpose of this work is not to compare performance of word-based with phrase-based translation. We used two system for different translations. The phrase-based SMT is used in recognized text utterance X ASR J N 1 target translation E SMT NxK 1 best translation E Rescore Figure 1: N-best hypothesis translation Chinese-English translation while the word-based SMT is used in Japanese-English tr"
2005.iwslt-1.3,N03-1017,0,0.0432809,"tate transducers (FST) to convey the features from acoustic analysis and source target translation models. All these approaches realized an integration between speech recognition modules and machine translation modules so that information from speech recognition, such as acoustic model score and language model score, can be exploited in the translation module to achieve the maximum performance over the single-best translation. In the field of machine translation, the phrase-based statistical machine translation approach is widely accepted at present. The related literature can be found in [4] [5]. But previously, word-based statistical machine translation, pioneered by IBM Models 1 to 5 [6], were used widely. In the evaluation, we used both the word-based and phrase-based systems. However, the purpose of this work is not to compare performance of word-based with phrase-based translation. We used two system for different translations. The phrase-based SMT is used in recognized text utterance X ASR J N 1 target translation E SMT NxK 1 best translation E Rescore Figure 1: N-best hypothesis translation Chinese-English translation while the word-based SMT is used in Japanese-English transl"
2005.iwslt-1.3,J93-2003,0,0.00587759,"on models. All these approaches realized an integration between speech recognition modules and machine translation modules so that information from speech recognition, such as acoustic model score and language model score, can be exploited in the translation module to achieve the maximum performance over the single-best translation. In the field of machine translation, the phrase-based statistical machine translation approach is widely accepted at present. The related literature can be found in [4] [5]. But previously, word-based statistical machine translation, pioneered by IBM Models 1 to 5 [6], were used widely. In the evaluation, we used both the word-based and phrase-based systems. However, the purpose of this work is not to compare performance of word-based with phrase-based translation. We used two system for different translations. The phrase-based SMT is used in recognized text utterance X ASR J N 1 target translation E SMT NxK 1 best translation E Rescore Figure 1: N-best hypothesis translation Chinese-English translation while the word-based SMT is used in Japanese-English translation. In this paper we describe two speech translation structures. The first is a direct N-best"
2005.iwslt-1.3,W02-1021,0,0.0261455,"Missing"
2005.iwslt-1.3,2005.iwslt-1.2,1,0.872223,"Missing"
C04-1168,J93-2003,0,0.00536183,"where Pam (X|J) is the acoustic model likelihood of the observations given the recognized sentence J; Plm (J), the source language model probability; and P (X), the probability of all acoustic observations. In the experiment we generated a set of N best hypotheses, J1N = {J1 , J2 , · · · , JN } 1 and each Ji is determined by Ji = arg max Pam (X|J)Plm (J) J∈Ωi where Ωi is the set of all possible source sentences excluding all higher ranked J k ’s, 1 ≤ k ≤ i − 1. The conversion from J to E in Fig. 1 is the machine translation process. According to the statistical machine translation formalism (Brown et al., 1993), the translation process b such that is to search for the best sentence E b = arg max P (E|J) = arg max P (J|E)P (E) E E E where P (J|E) is a translation model characterizing the correspondence between E and J; P (E), the English language model probability. In the IBM model 4, the translation model P (J|E) is further decomposed into four submodels: • Lexicon Model – t(j|e): probability of a word j in the Japanese language being translated into a word e in the English language. 1 Hereafter, J1 is called the single-best hypothesis of speech recognition; J1N , the N -best hypotheses. • Fertility"
C04-1168,W02-0706,0,0.0559183,"Missing"
C04-1168,W02-1018,0,0.0201343,"nvestigate the effect of different features for improving speech translation. In addition to the above seven features, the following features are also incorporated. • Part-of-speech language models: English part-of-speech language models were used. POS dependence of a translated English sentence is an effective constraint in pruning English sentence candidates. In our experiments 81 part-of-speech tags and a 5gram POS language model were used. • Length model P (l|E, J): l is the length (number of words) of a translated English sentence. • Jump weight: Jump width for adjacent cepts in Model 4 (Marcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 20"
C04-1168,niessen-etal-2000-evaluation,0,0.0383394,"weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we used M (=12) different features. In section 3, we review Powell’s algorithm (Press et al., 2000) as our tool to optimize model parameters, λM 1 , based on different objective translation metrics. 3 • NIST : An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor which again penalizes short translation sentences. Parameter Optimization Based on Translation Metrics • mWER (Niessen et al., 2000): Multiple reference word error rate, which computes the edit distance (minimum number of insertions, deletions, and substitutions) between test and reference sentences. The denominator in Eq. 1 can be ignored since the normalization is applied equally to every hypothesis. Hence, the choice of the best translab out of all possible translations, E, is tion, E, independent of the denominator, b = arg max E E M X λi logPi (X, E) • mPER: Multiple reference position independent word error rate, which computes the edit distance without considering the word order. (2) i=1 where we write features, fi"
C04-1168,J03-1002,0,0.0259349,"ses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translations from the bilingual sentences (Watanabe and Sumita, 2003). The decoder used the IBM Model 4 with a trigram language model and a 5-gram part-of-speech language model. The training of IBM model 4 was implemented by the GIZA++ package (Och and Ney, 2003). 4.2 Model Training In order to quantify translation improvement by features from speech recognition and machine translation respectively, we built four log-linear models by adding features successively. The four models are: • Standard translation model(stm): Only features from the IBM model 4 (M =5) described in section 2 were used in the loglinear models. We did not perform parameter optimization on this model. It is equivalent to setting all the λM 1 to 1. This model was the standard model used in most statistical machine translation system. It is referred to as the baseline model. • Optim"
C04-1168,P03-1021,0,0.490288,"nslation E th utterance is produced by the (Eq. 2), where E ∈ Cl . Let R = {R1 , · · · , RL } be the set of translation references for all utterances. Human translators paraphrased 16 reference sentences for The BLEU score and NIST score are calculated by the tool downloadable 2 . Because the objective function in the model (Eq. 3) is not smoothed function, we used Powell’s search method to find a solution. The Powell’s algorithm used in this work is similar as the one from (Press et al., 2000) but we modified the line optimization codes, a subroutine of Powell’s algorithm, with reference to (Och, 2003). Finding a global optimum is usually difficult in a high dimensional vector space. To make sure that we had found a good local optimum, we restarted the algorithm by using various initializations and used the best local optimum as the final solution. 4 Experiments 4.1 Corpus & System The data used in this study was the Basic Travel Expression Corpus (BTEC) (Kikui et al., 2003), consisting of commonly used sentences listed in travel guidebooks and tour conversations. The corpus were designed for developing multiple language speech-to-speech translation systems. It contains four different langu"
C04-1168,P02-1040,0,0.110685,"rcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 2002): A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we used M (=12) different features. In section 3, we review Powell’s algorithm (Press et al., 2000) as our tool to optimize model parameters, λM 1 , based on different objective translation metrics. 3 • NIST : An arithmetic mean of the n-gram matches between test and reference sentences multiplied by a length factor which again penalizes short translation sentences. Parameter Optimization Based on Translation Metrics • mW"
C04-1168,W02-1021,0,0.0119337,"d test corpus #01 were used for training, development and test respectively. The statistics of corpus is shown in table 1. The speech recognition engine used in the experiments was an HMM-based, large vocabulary continuous speech recognizer. The acoustic HMMs were triphone models with 2,100 states in total, using 25 dimensional, short-time spectrum features. In the first and second pass of decoding, a multiclass word bigram of a lexicon of 37,000 words plus 10,000 compound words was used. A word trigram was used in rescoring the results. The machine translation system is a graphbased decoder (Ueffing et al., 2002). The first pass of the decoder generates a word-graph, a compact representation of alternative translation candidates, using a beam search based on the scores of the lexicon and language models. In the second pass an A* search traverses the graph. The edges of the word-graph, or the phrase translation candidates, are generated by the list of word translations obtained from the inverted lexicon model. The phrase translations extracted from the Viterbi alignments of the training corpus also constitute the edges. Similarly, the edges are also created from dynamically extracted phrase translation"
C04-1168,2003.mtsummit-papers.54,1,0.913845,"peech language models: English part-of-speech language models were used. POS dependence of a translated English sentence is an effective constraint in pruning English sentence candidates. In our experiments 81 part-of-speech tags and a 5gram POS language model were used. • Length model P (l|E, J): l is the length (number of words) of a translated English sentence. • Jump weight: Jump width for adjacent cepts in Model 4 (Marcu and Wong, 2002). • Example matching score: The translated English sentence is matched with phrase translation examples. A score is derived based on the count of matches (Watanabe and Sumita, 2003). • Dynamic example matching score: Similar to the example matching score but phrases were extracted dynamically from sentence examples (Watanabe and Sumita, 2003). each utterance, i.e., Rl contains 16 reference candidates for the l-th utterance. b R) is a translation “distortion” or an D(E, objective translation assessment. The following four metrics were used specifically in this study: • BLEU (Papineni et al., 2002): A weighted geometric mean of the n-gram matches between test and reference sentences multiplied by a brevity penalty that penalizes short translation sentences. Altogether, we"
C10-2046,N04-1015,0,0.242711,"support vector machines (SVMs) and creates a structured summary by heuristic rules that assign the mapped utterances to appropriate summary sections. Our work shares the same motivation as theirs in that we want to make it easier for quality analysts to analyze the massive number of calls. However, we tackle the problem differently in that we propose a new modeling of utterance sequences for extractive summarization that makes it unnecessary to create heuristics rules by hand and facilitates the porting of a summarization system. HMMs have been successfully applied to automatic summarization (Barzilay and Lee, 2004). In their work, an HMM was used to model the transition of content topics. The Viterbi decoding (Rabiner, 1990) was performed to find content topics that should be incorporated into a summary. Their approach is similar to ours in that HMMs are utilized to model topic sequences, but they did not use data of multiple domains in creating their model. In addition, their method requires training data (original articles with their reference summaries) in order to find which content topics should be included in a summary, whereas our method requires only the raw sequences with their domain labels. 3"
C10-2046,J99-3003,0,0.0542732,"2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999). Initially, the primary aim of such research was to transfer calls from answering agents to operators as quickly as possible in the case of problematic situations. However, real-time processing of calls requires a tremendous engineering effort, especially when customer satisfaction is at stake, which led to recent work on the offline processing of calls, such as call mining (Takeuchi et al., 2007) and call summarization (Byrd et al., 2008). The work most related to ours is (Byrd et al., 2008), which maps operator/caller utterances to an ontology in the automotive domain by using support vecto"
C10-2046,W02-0401,0,0.0223443,"for understanding operator/caller utterances, which makes it difficult to port one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-C"
C10-2046,W09-1802,0,0.131295,"one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summarization of multi-domain data. 400 Coling 2010: Poster Volume, pages 400–408, Beijing, August 2010 In the past few decades, contact center dialogues have been an active research focus (Gorin et al., 1997; Chu-Carroll and Carpenter, 1999). Initially, the primary aim of such research was to transfer c"
C10-2046,N03-1020,0,0.0693467,"summaries within the character lengths of our systems’ summaries. We used scenario texts (See Fig. 5) as reference data; that is, a dialogue dealing with a certain task is evaluated using the scenario text for that task. As an evaluation criterion, we used the F-measure (F1) to evaluate the retrieval accuracy on the basis of the recall and precision of retrieved content words. We used the scenarios as references because they contain the basic content exchanged between an operator and a caller, the retrieval accuracy of which should be important for quality analysts. We could have used ROUGE (Lin and Hovy, 2003), but we did not because ROUGE does not correlate well with human judgments in conversational data (Liu and Liu, 2008). Another benefit of using the F-measure is that summaries of varying lengths can be compared. 5.5 Results Table 4 shows the evaluation results for the proposed systems and the baselines. It can be seen that concat3 shows the best performance in Fmeasure among all systems, having a statistically better performance over all systems except for concat2. The CSHMMs with concatenated training were all better than ergodic0–3. Here, the performance (and output) of ergodic0–3 was exact"
C10-2046,P08-2051,0,0.0811624,"a; that is, a dialogue dealing with a certain task is evaluated using the scenario text for that task. As an evaluation criterion, we used the F-measure (F1) to evaluate the retrieval accuracy on the basis of the recall and precision of retrieved content words. We used the scenarios as references because they contain the basic content exchanged between an operator and a caller, the retrieval accuracy of which should be important for quality analysts. We could have used ROUGE (Lin and Hovy, 2003), but we did not because ROUGE does not correlate well with human judgments in conversational data (Liu and Liu, 2008). Another benefit of using the F-measure is that summaries of varying lengths can be compared. 5.5 Results Table 4 shows the evaluation results for the proposed systems and the baselines. It can be seen that concat3 shows the best performance in Fmeasure among all systems, having a statistically better performance over all systems except for concat2. The CSHMMs with concatenated training were all better than ergodic0–3. Here, the performance (and output) of ergodic0–3 was exactly the same. This happened because of the broad distributions in their common states; no paths went through the common"
C10-2046,W01-0100,0,0.158865,"alls, automatic summarization has been utilized and shown to successfully reduce costs (Byrd et al., 2008). However, one of the problems in current call summarization is that a domain ontology is required for understanding operator/caller utterances, which makes it difficult to port one summarization system from domain to domain. This paper describes a novel automatic summarization method for contact center dialogues without the costly process of creating domain on2 Related Work There is an abundance of research in automatic summarization. It has been successfully applied to single documents (Mani, 2001) as well as to multiple documents (Radev et al., 2004), and various summarization methods, such as the conventional LEAD method, machine-learning based sentence selection (Kupiec et al., 1995; Osborne, 2002), and integer linear programming (ILP) based sentence extraction (Gillick and Favre, 2009), have been proposed. Recent years have seen work on summarizing broadcast news speech (Hori and Furui, 2003), multi-party meetings (Murray et al., 2005), and contact center dialogues (Byrd et al., 2008). However, despite the large amount of previous work, little work has tackled the automatic summariz"
C10-2046,W09-3917,1,\N,Missing
C10-2047,P08-1004,0,0.472088,"X, Y , R] from a text involves two tasks. One is detecting semantically related pairs from named entity pairs that co-occur in a text and the other is determining the relation between a detected pair. For the former task, various supervised learning methods (Culotta and Sorensen, 2004; Zelenko et al., 2003; Hirano et al., 2007) and bootstrapping methods (Brin, 1998; Pantel and Pennacchiotti, 2006) have been explored to date. In contrast, for the latter task, 409 Coling 2010: Poster Volume, pages 409–417, Beijing, August 2010 only a few methods have been proposed so far (Hasegawa et al., 2004; Banko and Etzioni, 2008; Zhu et al., 2009). We therefore addressed the problem of how to determine relations between a given pair. We used a three-step approach to address this problem. The first step is to recognize an expression that shows explicit relations between a given named entity pair in a text. If no such expression is recognized, the second step is to estimate the relationship that exists between a given named entity pair that has an implicit relation. The last step is to identify synonyms of the relations that are recognized or estimated in the above steps. In this paper, we focus on the first step. The"
C10-2047,W04-3239,0,0.0315531,"ed probability, we ranked candidates and appended the rank “Crank ” and the probability score “Cprob ” to the feature tree as a child of the candidate node (See Figure 3, 5, or 6). For example, if the past relations Rm were “dating” and “engagement” and candidates were “marriage”, “meeting’, “eating”, or “drinking”, the candidates probabilities were calculated and ranked as “marriage” (Cprob :0.15, Crank :1), “meeting” (Cprob :0.08, Crank :2), etc. 3.3 Classification Algorithms Several structure-based learning algorithms have been proposed so far (Collins and Duffy, 2002; Suzuki et al., 2003; Kudo and Matsumoto, 2004). The experiments tested Kudo and Matsumoto’s boosting-based algorithm using sub-trees as features, which is implemented as a BACT system. Given a set of training examples each of which is represented as a tree labeling whether the candidate is the relation expression of a given pair or not, the BACT system learns that a set of rules is effective in classifying. Then, given a test instance, the BACT system classifies using a set of learned rules. 4 Experiments We conducted experiments using texts from Japanese newspaper articles and weblog texts to test the proposed method for both intra- and"
C10-2047,2002.tmi-papers.15,0,0.0125608,"POS:Noun others: others: Miyaji Miyaji21 21 others: others: Chumoku Chumoku11 11 Inh:1 Inh:1 C :1 Crank :1 rank Figure 4: Salient referent list tree C :0.23 Cprob :0.23 prob Figure 3: Intra-sentential feature tree Contextual Feature To recognize the inter-sentential relation expressions for a given pair, we assume that there is a discriminative contextual structure that consists of given entities and their relation expression. Here, we use a Salient Referent List (SRL) to obtain contextual structure. The SRL is an empirical sorting rule proposed to identify the antecedent of (zero) pronouns (Nariyama, 2002), and Hirano (2007) proposed a way of applying SRL to relation detection. In this work, we use this way to apply SRL to recognize inter-sentential relation expressions. We applied SRL to each candidate as follows. First, from among given entities and the candidate, we choose the one appearing last in the text as the root of the tree. We then append noun phrases, from the chosen one to the beginning of the text, to the tree depending on case markers, “wa” (topicalised subject), “ga” (subject), “ni” (indirect object),“wo” (object), and “others”, with the following rules. If there are nodes of th"
C10-2047,P06-1015,0,0.185643,"Democratic Party, member] extracted from the same text is an implicit relation because there is no expression showing the relation (e.g. member) between “Yukio Hatoyama” and “the Democratic Party” in the text. Extracting triples [X, Y , R] from a text involves two tasks. One is detecting semantically related pairs from named entity pairs that co-occur in a text and the other is determining the relation between a detected pair. For the former task, various supervised learning methods (Culotta and Sorensen, 2004; Zelenko et al., 2003; Hirano et al., 2007) and bootstrapping methods (Brin, 1998; Pantel and Pennacchiotti, 2006) have been explored to date. In contrast, for the latter task, 409 Coling 2010: Poster Volume, pages 409–417, Beijing, August 2010 only a few methods have been proposed so far (Hasegawa et al., 2004; Banko and Etzioni, 2008; Zhu et al., 2009). We therefore addressed the problem of how to determine relations between a given pair. We used a three-step approach to address this problem. The first step is to recognize an expression that shows explicit relations between a given named entity pair in a text. If no such expression is recognized, the second step is to estimate the relationship that exis"
C10-2047,P04-1054,0,0.718166,"atic Party, is Kunio Hatoyama’s brother” is an explicit relation. In contrast, the triple [Yukio Hatoyama, the Democratic Party, member] extracted from the same text is an implicit relation because there is no expression showing the relation (e.g. member) between “Yukio Hatoyama” and “the Democratic Party” in the text. Extracting triples [X, Y , R] from a text involves two tasks. One is detecting semantically related pairs from named entity pairs that co-occur in a text and the other is determining the relation between a detected pair. For the former task, various supervised learning methods (Culotta and Sorensen, 2004; Zelenko et al., 2003; Hirano et al., 2007) and bootstrapping methods (Brin, 1998; Pantel and Pennacchiotti, 2006) have been explored to date. In contrast, for the latter task, 409 Coling 2010: Poster Volume, pages 409–417, Beijing, August 2010 only a few methods have been proposed so far (Hasegawa et al., 2004; Banko and Etzioni, 2008; Zhu et al., 2009). We therefore addressed the problem of how to determine relations between a given pair. We used a three-step approach to address this problem. The first step is to recognize an expression that shows explicit relations between a given named en"
C10-2047,P03-1005,0,0.0264684,"} Using this calculated probability, we ranked candidates and appended the rank “Crank ” and the probability score “Cprob ” to the feature tree as a child of the candidate node (See Figure 3, 5, or 6). For example, if the past relations Rm were “dating” and “engagement” and candidates were “marriage”, “meeting’, “eating”, or “drinking”, the candidates probabilities were calculated and ranked as “marriage” (Cprob :0.15, Crank :1), “meeting” (Cprob :0.08, Crank :2), etc. 3.3 Classification Algorithms Several structure-based learning algorithms have been proposed so far (Collins and Duffy, 2002; Suzuki et al., 2003; Kudo and Matsumoto, 2004). The experiments tested Kudo and Matsumoto’s boosting-based algorithm using sub-trees as features, which is implemented as a BACT system. Given a set of training examples each of which is represented as a tree labeling whether the candidate is the relation expression of a given pair or not, the BACT system learns that a set of rules is effective in classifying. Then, given a test instance, the BACT system classifies using a set of learned rules. 4 Experiments We conducted experiments using texts from Japanese newspaper articles and weblog texts to test the proposed"
C10-2047,P98-1068,0,0.0544265,"t-dependent FeaturesTM ++Context-dependent FeaturesCM P recision 70.1 ％ (579/825) 77.1 ％ (719/932) 75.2 ％ (794/1,055) 74.3 ％ (732/985) Recall 28.1 ％ (579/2,058) 34.9 ％ (719/2,058) 38.5 ％ (794/2,058) 35.5 ％ (732/2,058) F 0.401 0.480 0.510 0.481 Table 4: Experimental result of inter-sentential were given, but only 2,058 of them had relation expressions. We conducted five-fold cross-validation over 17,228 entity pairs so that sets of pairs from a single text were not divided into the training and test sets. In the experiments, all features were automatically acquired using a Japanese POS tagger (Fuchi and Takagi, 1998) and dependency parser (Imamura et al., 2007). 4.2 Results Tables 3 and 4 show the performance of several methods for intra-sentential and inter-sentential. P recision is defined as the percentage of correct relation expressions out of recognized ones. Recall is the percentage of correct relation expressions from among the manually annotated ones. The F measure is the harmonic mean of precision and recall. A comparison with the Conventional Features and Inherent Features method for intra/inter-sentential tasks indicates that the proposed method using inherent features of relational words impro"
C10-2047,P06-1028,0,0.0204841,"ort relations in order of time and count pairs of present and previous relations. For example, if we extract “dating” occurring for an entity pair on January 10, 1998, “engagement” occurring on February 15, 2001, and “marriage” occurring on December 24, 2001, the pairs hdating, engagementi, hdating, marriagei, and hengagement, marriagei are counted. The counted score is then summed up by the pair of entity class and the trigger model is calculated by the following formula. Count(rm , rn ) rn Count(rm , rn ) PT (rn |rm ) = ∑ For the evaluation, we extracted triples by named entity recognition (Suzuki et al., 2006), relation detection (Hirano et al., 2007), and the proposed method using the inherent features of relational words described in Section 3.2. A total of 10,463,232 triples were extracted from 8,320,042 newspaper articles and weblog texts with time stamps made between January 1, 1991 and June 30, 2006. As examples of the calculated relation trigger model, Table 1 shows the top three probability relations rn of several relations rm between Japanese standard named entity classes defined in the IREX workshop2 . For instance, the relation “fellow” has the highest probability of being changed from t"
C10-2047,P04-1053,0,0.293643,"t. Extracting triples [X, Y , R] from a text involves two tasks. One is detecting semantically related pairs from named entity pairs that co-occur in a text and the other is determining the relation between a detected pair. For the former task, various supervised learning methods (Culotta and Sorensen, 2004; Zelenko et al., 2003; Hirano et al., 2007) and bootstrapping methods (Brin, 1998; Pantel and Pennacchiotti, 2006) have been explored to date. In contrast, for the latter task, 409 Coling 2010: Poster Volume, pages 409–417, Beijing, August 2010 only a few methods have been proposed so far (Hasegawa et al., 2004; Banko and Etzioni, 2008; Zhu et al., 2009). We therefore addressed the problem of how to determine relations between a given pair. We used a three-step approach to address this problem. The first step is to recognize an expression that shows explicit relations between a given named entity pair in a text. If no such expression is recognized, the second step is to estimate the relationship that exists between a given named entity pair that has an implicit relation. The last step is to identify synonyms of the relations that are recognized or estimated in the above steps. In this paper, we focu"
C10-2047,P07-2040,1,0.907826,"licit relation. In contrast, the triple [Yukio Hatoyama, the Democratic Party, member] extracted from the same text is an implicit relation because there is no expression showing the relation (e.g. member) between “Yukio Hatoyama” and “the Democratic Party” in the text. Extracting triples [X, Y , R] from a text involves two tasks. One is detecting semantically related pairs from named entity pairs that co-occur in a text and the other is determining the relation between a detected pair. For the former task, various supervised learning methods (Culotta and Sorensen, 2004; Zelenko et al., 2003; Hirano et al., 2007) and bootstrapping methods (Brin, 1998; Pantel and Pennacchiotti, 2006) have been explored to date. In contrast, for the latter task, 409 Coling 2010: Poster Volume, pages 409–417, Beijing, August 2010 only a few methods have been proposed so far (Hasegawa et al., 2004; Banko and Etzioni, 2008; Zhu et al., 2009). We therefore addressed the problem of how to determine relations between a given pair. We used a three-step approach to address this problem. The first step is to recognize an expression that shows explicit relations between a given named entity pair in a text. If no such expression i"
C10-2047,P07-2057,1,0.840516,"uresCM P recision 70.1 ％ (579/825) 77.1 ％ (719/932) 75.2 ％ (794/1,055) 74.3 ％ (732/985) Recall 28.1 ％ (579/2,058) 34.9 ％ (719/2,058) 38.5 ％ (794/2,058) 35.5 ％ (732/2,058) F 0.401 0.480 0.510 0.481 Table 4: Experimental result of inter-sentential were given, but only 2,058 of them had relation expressions. We conducted five-fold cross-validation over 17,228 entity pairs so that sets of pairs from a single text were not divided into the training and test sets. In the experiments, all features were automatically acquired using a Japanese POS tagger (Fuchi and Takagi, 1998) and dependency parser (Imamura et al., 2007). 4.2 Results Tables 3 and 4 show the performance of several methods for intra-sentential and inter-sentential. P recision is defined as the percentage of correct relation expressions out of recognized ones. Recall is the percentage of correct relation expressions from among the manually annotated ones. The F measure is the harmonic mean of precision and recall. A comparison with the Conventional Features and Inherent Features method for intra/inter-sentential tasks indicates that the proposed method using inherent features of relational words improved intra-sentential tasks F by 0.06 points a"
C10-2047,C98-1065,0,\N,Missing
C10-2105,P04-1051,0,0.330236,"our model uses the unit of sentences. This problem can be formulated as the Traveling Salesman Problem and its variants. Banko et al. (2000) uses beam search to identify approximate solutions. Deshpande et al. (2007) uses ILP and a randomized algorithm to find the optimal solution. 2.2 Sentence Ordering It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of 911                   s11 s12 s13 .. .  s31 s32 s33 Figure 1: Graph representation of summarization. words (Barzilay and Lapata, 2005). Some approaches use machine learning to integrate these features (Soricut and Marcu, 2006; Elsner et al., 2007). Generally speaking, these methods score the discourse coherence of a fixed set of sentences. These methods are separated from the extraction step so they may fail if the set includes sentences that are impossible to order naturally. As mentioned above, there is a preceding work that attempted"
C10-2105,P00-1041,0,0.0120641,"and Hatzivassiloglou, 2004), stack decoding (Yih et al., 2007; Takamura and Okumura, 2009) and Integer Linear Programming (Clarke and Lapata, 2007; McDonald, 2007; Gillick and Favre, 2009; Martins and Smith, 2009). Gillick and Favre (2009) and Takamura and Okumura (2009) formulate summarization as a Maximum Coverage Problem. We also use this formulation. While these methods focus on extracting a set of sentences from the source document set, our method performs extraction and ordering simultaneously. Some studies attempt to generate a single sentence (i.e. headline) from the source document (Banko et al., 2000; Deshpande et al., 2007). While they extract and order words from the source document as a unit, our model uses the unit of sentences. This problem can be formulated as the Traveling Salesman Problem and its variants. Banko et al. (2000) uses beam search to identify approximate solutions. Deshpande et al. (2007) uses ILP and a randomized algorithm to find the optimal solution. 2.2 Sentence Ordering It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate"
C10-2105,P05-1018,0,0.00958355,"tions. Deshpande et al. (2007) uses ILP and a randomized algorithm to find the optimal solution. 2.2 Sentence Ordering It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of 911                   s11 s12 s13 .. .  s31 s32 s33 Figure 1: Graph representation of summarization. words (Barzilay and Lapata, 2005). Some approaches use machine learning to integrate these features (Soricut and Marcu, 2006; Elsner et al., 2007). Generally speaking, these methods score the discourse coherence of a fixed set of sentences. These methods are separated from the extraction step so they may fail if the set includes sentences that are impossible to order naturally. As mentioned above, there is a preceding work that attempted to perform sentence extraction and ordering simultaneously (Nishikawa et al., 2010). Differences between this paper and that work are as follows: • This work adopts ILP solver as a decoder. I"
C10-2105,E06-1039,0,0.310117,"nd Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some summarization systems that target reviews, opinion summarizers, extract particular information, opinion, from the input sentences and leverage them to select important sentences (Carenini et al., 2006; Lerman et al., 2009). In this paper, since we aim to summarize reviews, the objective function is defined through opinion as the concept that the reviews contain. We explain our detailed objective function in Section 3. We describe features of above existing summarizers in Section 4 and compare our method to them as baselines. Decoding Method The algorithms proposed for argmax operation include the greedy method (Filatova and Hatzivassiloglou, 2004), stack decoding (Yih et al., 2007; Takamura and Okumura, 2009) and Integer Linear Programming (Clarke and Lapata, 2007; McDonald, 2007; Gillick"
C10-2105,D07-1001,0,0.0177836,"select important sentences (Carenini et al., 2006; Lerman et al., 2009). In this paper, since we aim to summarize reviews, the objective function is defined through opinion as the concept that the reviews contain. We explain our detailed objective function in Section 3. We describe features of above existing summarizers in Section 4 and compare our method to them as baselines. Decoding Method The algorithms proposed for argmax operation include the greedy method (Filatova and Hatzivassiloglou, 2004), stack decoding (Yih et al., 2007; Takamura and Okumura, 2009) and Integer Linear Programming (Clarke and Lapata, 2007; McDonald, 2007; Gillick and Favre, 2009; Martins and Smith, 2009). Gillick and Favre (2009) and Takamura and Okumura (2009) formulate summarization as a Maximum Coverage Problem. We also use this formulation. While these methods focus on extracting a set of sentences from the source document set, our method performs extraction and ordering simultaneously. Some studies attempt to generate a single sentence (i.e. headline) from the source document (Banko et al., 2000; Deshpande et al., 2007). While they extract and order words from the source document as a unit, our model uses the unit of sent"
C10-2105,W02-1001,0,0.00276367,"s work adopts ILP solver as a decoder. ILP solver allows the summarizer to search for the optimal solution much more rapidly than beam search (Deshpande et al., 2007), which was adopted by the prior work. To permit ILP solver incorporation, we propose in this paper a totally new ILP formulation. The formulation can be widely used for text summarization and generation. • Moreover, to learn better discourse coherence, we adopt the Passive-Aggressive algorithm (Crammer et al., 2006) and use Kendall’s tau (Lapata, 2006) as the loss function. In contrast, the above work adopts Averaged Perceptron (Collins, 2002) and has no explicit loss function. These advances make this work very different from that work. 3 Our Method 3.1 The Model We consider a summary as a sequence of sentences. As an example, document set D = {d1 , d2 , d3 } is given to a summarizer. We define d as a single document. Document d1 , which consists of four sentences, is describe by d1 = {s11 , s12 , s13 , s14 }. Documents d2 and d3 consist of five sentences and three sentences (i.e. d2 = {s21 , s22 , s23 , s24 , s25 }, d3 = e1 1 0 0 e2 0 1 0 e3 0 0 0 ... .. 0 0 0 0 0 0 e6 1 0 0 e7 0 0 0 e8 0 0 1 0 0 0 0 1 0 0 0 1 . 0 1 0 Table 2: Se"
C10-2105,N07-1056,0,0.266416,"u, 2004), stack decoding (Yih et al., 2007; Takamura and Okumura, 2009) and Integer Linear Programming (Clarke and Lapata, 2007; McDonald, 2007; Gillick and Favre, 2009; Martins and Smith, 2009). Gillick and Favre (2009) and Takamura and Okumura (2009) formulate summarization as a Maximum Coverage Problem. We also use this formulation. While these methods focus on extracting a set of sentences from the source document set, our method performs extraction and ordering simultaneously. Some studies attempt to generate a single sentence (i.e. headline) from the source document (Banko et al., 2000; Deshpande et al., 2007). While they extract and order words from the source document as a unit, our model uses the unit of sentences. This problem can be formulated as the Traveling Salesman Problem and its variants. Banko et al. (2000) uses beam search to identify approximate solutions. Deshpande et al. (2007) uses ILP and a randomized algorithm to find the optimal solution. 2.2 Sentence Ordering It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate order include publicatio"
C10-2105,N07-1055,0,0.0215748,"Missing"
C10-2105,C04-1057,0,0.106398,"e other defines a weight for a sub-sentence, concept, that the summary contains. McDonald (2007) and Martins and Smith (2009) directly weight sentences and use MMR to avoid redundancy (Carbonell and Goldstein, 1998). In contrast to their approaches, we set weights on concepts, not sentences. Gillick and Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some summarization systems that target reviews, opinion summarizers, extract particular information, opinion, from the input sentences and leverage them to select important sentences (Carenini et al., 2006; Lerman et al., 2009). In this paper, since we aim to summarize reviews, the objective function is defined through opinion as the concept that the reviews contain. We explain our detailed objective function in Section 3. We describe features of above existing summarizers in Section 4 and compare our method to t"
C10-2105,W09-1802,0,0.122583,"en sentences (Carbonell and Goldstein, 1998) and centroid (Radev et al., 2004); some approaches directly learn function L from references (Kupiec et al., 1995; Hirao et al., 2002). There are two approaches to defining the score of the summary. One defines the weight on each sentence forming the summary. The other defines a weight for a sub-sentence, concept, that the summary contains. McDonald (2007) and Martins and Smith (2009) directly weight sentences and use MMR to avoid redundancy (Carbonell and Goldstein, 1998). In contrast to their approaches, we set weights on concepts, not sentences. Gillick and Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some summarization systems that target reviews, opinion summarizers, extract particular information, opinion, from the input sentences and leverage them to select important sentences (Carenini et al."
C10-2105,C02-1053,0,0.087636,"ndicates the length of S, K is the maximum size of the summary. That is, most summarization algorithms search for, or decode, the set of sentences Sˆ that maximizes function L under the given maximum size of the summary K. Thus most studies focus on the design of function L and efficient search algorithms (i.e. argmax operation in Eq.1). Objective Function Many useful L functions have been proposed including the cosine similarity of given sentences (Carbonell and Goldstein, 1998) and centroid (Radev et al., 2004); some approaches directly learn function L from references (Kupiec et al., 1995; Hirao et al., 2002). There are two approaches to defining the score of the summary. One defines the weight on each sentence forming the summary. The other defines a weight for a sub-sentence, concept, that the summary contains. McDonald (2007) and Martins and Smith (2009) directly weight sentences and use MMR to avoid redundancy (Carbonell and Goldstein, 1998). In contrast to their approaches, we set weights on concepts, not sentences. Gillick and Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a w"
C10-2105,P03-1069,0,0.0737846,"nt as a unit, our model uses the unit of sentences. This problem can be formulated as the Traveling Salesman Problem and its variants. Banko et al. (2000) uses beam search to identify approximate solutions. Deshpande et al. (2007) uses ILP and a randomized algorithm to find the optimal solution. 2.2 Sentence Ordering It is known that the readability of a collection of sentences, a summary, can be greatly improved by appropriately ordering them (Barzilay et al., 2002). Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of 911                   s11 s12 s13 .. .  s31 s32 s33 Figure 1: Graph representation of summarization. words (Barzilay and Lapata, 2005). Some approaches use machine learning to integrate these features (Soricut and Marcu, 2006; Elsner et al., 2007). Generally speaking, these methods score the discourse coherence of a fixed set of sentences. These methods are separated from the extraction step so they may fail if the set includes sentences that are impossible to order naturally. As mentioned above, there is a precedi"
C10-2105,J06-4002,0,0.240568,"(Nishikawa et al., 2010). Differences between this paper and that work are as follows: • This work adopts ILP solver as a decoder. ILP solver allows the summarizer to search for the optimal solution much more rapidly than beam search (Deshpande et al., 2007), which was adopted by the prior work. To permit ILP solver incorporation, we propose in this paper a totally new ILP formulation. The formulation can be widely used for text summarization and generation. • Moreover, to learn better discourse coherence, we adopt the Passive-Aggressive algorithm (Crammer et al., 2006) and use Kendall’s tau (Lapata, 2006) as the loss function. In contrast, the above work adopts Averaged Perceptron (Collins, 2002) and has no explicit loss function. These advances make this work very different from that work. 3 Our Method 3.1 The Model We consider a summary as a sequence of sentences. As an example, document set D = {d1 , d2 , d3 } is given to a summarizer. We define d as a single document. Document d1 , which consists of four sentences, is describe by d1 = {s11 , s12 , s13 , s14 }. Documents d2 and d3 consist of five sentences and three sentences (i.e. d2 = {s21 , s22 , s23 , s24 , s25 }, d3 = e1 1 0 0 e2 0 1 0"
C10-2105,E09-1059,0,0.385155,"d that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some summarization systems that target reviews, opinion summarizers, extract particular information, opinion, from the input sentences and leverage them to select important sentences (Carenini et al., 2006; Lerman et al., 2009). In this paper, since we aim to summarize reviews, the objective function is defined through opinion as the concept that the reviews contain. We explain our detailed objective function in Section 3. We describe features of above existing summarizers in Section 4 and compare our method to them as baselines. Decoding Method The algorithms proposed for argmax operation include the greedy method (Filatova and Hatzivassiloglou, 2004), stack decoding (Yih et al., 2007; Takamura and Okumura, 2009) and Integer Linear Programming (Clarke and Lapata, 2007; McDonald, 2007; Gillick and Favre, 2009; Marti"
C10-2105,W04-1013,0,0.0457808,"prone to lock onto R-2 0.158 0.205 0.231 0.384 R-SU4 0.202 0.247 0.251 0.392 R-SU9 0.186 0.227 0.230 0.358 R-2 0.251 0.260 0.285 0.358 R-SU4 0.281 0.296 0.303 0.370 R-SU9 0.258 0.273 0.273 0.335 Table 3: Automatic ROUGE evaluation. (Carenini et al., 2006) (Lerman et al., 2009) Our Method Human # of Sentences 3.79 6.28 7.88 5.83 Table 4: Average number of sentences in the summary. local solutions, the summarizer can reach the optimal solution by changing the starting sentences and repeating the process. In this experiment, we used 100 randomly selected starting points. 4.2 ROUGE We used ROUGE (Lin, 2004) for evaluating the content of summaries. We chose ROUGE-2, ROUGE-SU4 and ROUGE-SU9. We prepared four reference summaries for each document set. The results of these experiments are shown in Table 3. ROUGE scores increase in the order of (Carenini et al., 2006), (Lerman et al., 2009) and our method, but no method could match the performance of Human. Our method significantly outperformed Lerman et al. (2009)’s method over ROUGE-2 according to the Wilcoxon signed-rank test, while it shows no advantage over ROUGESU4 and ROUGE-SU9. Although our weighting of the set of sentences is relatively naiv"
C10-2105,W09-1801,0,0.105789,"n the design of function L and efficient search algorithms (i.e. argmax operation in Eq.1). Objective Function Many useful L functions have been proposed including the cosine similarity of given sentences (Carbonell and Goldstein, 1998) and centroid (Radev et al., 2004); some approaches directly learn function L from references (Kupiec et al., 1995; Hirao et al., 2002). There are two approaches to defining the score of the summary. One defines the weight on each sentence forming the summary. The other defines a weight for a sub-sentence, concept, that the summary contains. McDonald (2007) and Martins and Smith (2009) directly weight sentences and use MMR to avoid redundancy (Carbonell and Goldstein, 1998). In contrast to their approaches, we set weights on concepts, not sentences. Gillick and Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some su"
C10-2105,P10-2060,1,0.929242,"ted by Barzilay et al. (2002). To make summaries coherent, the extracted sentences must be appropriately ordered. However, most summarization systems delink sentence extraction from sentence ordering, so a sentence can be extracted that can never be ordered naturally with the other extracted sentences. Moreover, due to recent advances in decoding techniques for text summarization, the summarizers tend to select shorter sentences to optimize summary content. It aggravates this problem. Although a preceding work tackles this problem by performing sentence extraction and ordering simultaneously (Nishikawa et al., 2010), they adopt beam search and dynamic programming to search for the optimal solution, so their proposed method may fail to locate it. To overcome this weakness, this paper proposes a novel Integer Linear Programming (ILP) formulation for searching for the optimal solution efficiently. We formulate the multidocument summarization task as an ILP problem that tries to optimize the content and coherence of the summary by extracting and ordering sentences simultaneously. We apply our method to opinion summarization and show that it outperforms state-ofthe-art opinion summarizers in terms of ROUGE ev"
C10-2105,P06-2103,0,0.0377153,"Missing"
C10-2105,E09-1089,0,0.139765,"(2007) and Martins and Smith (2009) directly weight sentences and use MMR to avoid redundancy (Carbonell and Goldstein, 1998). In contrast to their approaches, we set weights on concepts, not sentences. Gillick and Favre (2009) reported that the concept-based model achieves better performance and scalability than the sentence-based model when it is formulated as ILP. There is a wide range of choice with regard to the unit of the concept. Concepts include words and the relationship between named entities (Filatova and Hatzivassiloglou, 2004), bigrams (Gillick and Favre, 2009), and word stems (Takamura and Okumura, 2009). Some summarization systems that target reviews, opinion summarizers, extract particular information, opinion, from the input sentences and leverage them to select important sentences (Carenini et al., 2006; Lerman et al., 2009). In this paper, since we aim to summarize reviews, the objective function is defined through opinion as the concept that the reviews contain. We explain our detailed objective function in Section 3. We describe features of above existing summarizers in Section 4 and compare our method to them as baselines. Decoding Method The algorithms proposed for argmax operation i"
C10-2105,J08-1001,0,\N,Missing
C92-1009,C92-3164,1,0.824962,"eses slot of node VI' are the hypothetical nodes VPl, VP2, and vt'3, corresponding to hypothet ical trees T I , T2, 'I'3 respectively. If expansion failure occur in T I and '1'2, VP1 ;~nd V I'2 are removed from the hypothesis slot. Then, Vl'3 is unitied with VP, because there is only one hy pothesis node left in the slot VP node. If there is no hypothesis node dominating the failed expansion node, the entire generation l)r~)cess fails. 37 I'ROC. OFCOLING-92, NANTF.S, AUr3. 23-28, 1992 perform adjunct operation [9]. The algorithm is implemented in SL-Trans, a spoken language translation system [8]. unify Acknowledgments ....... Nh VP1 VP2 The author would like to thank M0zk for helpful comments on this paper would like to thank Akira Kurematsu, Morimoto and other members of ATR constant help and fruitful discussions. VP3 Seligma~t and also Tsuyoshi for their References Figure 8: an illustration of generation ambiguity resolution 3.7 Postprocess Expansion halts when no node is selected in the expanding node selection step. This does not necessarily mean the agenda is empty, because there m~y be some nodes without instantiated smnantic structure. ltow do such semantically empty nodes ari"
C92-1009,C88-2147,0,\N,Missing
C92-1009,P91-1041,0,\N,Missing
C92-1009,1991.iwpt-1.19,0,\N,Missing
C92-1009,P89-1002,0,\N,Missing
C92-3164,P91-1041,0,0.0921449,"Missing"
C92-3164,C92-1030,1,0.883308,"Missing"
C92-3164,C92-1009,1,0.823517,"Missing"
C92-3164,C88-2118,0,\N,Missing
C92-3164,1991.iwpt-1.19,0,\N,Missing
C94-1058,C90-2038,0,0.0722435,"Missing"
C94-1058,P91-1025,0,\N,Missing
C98-1106,P91-1034,0,0.549896,"quires much cost and that they are usually domain-dependent 1 The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This r e s e a r c h w a s done when the author w a s a t Center for the S t u d y of Language and Information(CSLI), Stanford University. 1In fact, this is partly shown by the fact that many MT s y s t e m s have substitutable d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associat"
C98-1106,J94-4003,0,0.710166,"that they are usually domain-dependent 1 The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This r e s e a r c h w a s done when the author w a s a t Center for the S t u d y of Language and Information(CSLI), Stanford University. 1In fact, this is partly shown by the fact that many MT s y s t e m s have substitutable d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associative relations between t"
C98-1106,C96-2098,0,0.42656,"in-dependent 1 The second type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * This r e s e a r c h w a s done when the author w a s a t Center for the S t u d y of Language and Information(CSLI), Stanford University. 1In fact, this is partly shown by the fact that many MT s y s t e m s have substitutable d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associative relations between two words which do not co-occur"
C98-1106,C88-1016,0,\N,Missing
C98-1106,J90-2002,0,\N,Missing
I11-1103,R09-1052,1,0.850541,"t Vector Machines with various features. We think considering constraints among relations might contribute to improve the performance of identifying relations. Therefore, we realize it with a Markov logic network. Relation identiﬁcation is considered as a problem to ﬁnd labeled edges between pairs of nodes, where a node is an answer in a thread. Structured output learning is a method to predict such a structure (Tsochantaridis et al., 2004; Crammer et al., 2006). Morita et al. proposed a model based on structured output learning to identify agreement and disagreement relations in a discourse (Morita et al., 2009). Yang et al. used structured Support Vector Machines to extract contexts and answers for questions in threads of online forums (Yang et al., 2009). tive relations”, respectively. We consider that these relations might be useful for identiﬁcation of logical relations and that identiﬁcation of these relations is easier than that of logical relations. Thus, we incorporate identiﬁcation of these superrelations into our model. We brieﬂy describe our related work in section two. Then, we show the logical relations between answers in section three and present our model with global constraints using"
I11-1103,W00-1009,0,0.0148102,"her answers or questions to the original questioner to ask for further details. Figure 1: Example of a QA thread Here, since the relation between (a1) and (a2) is “equivalence” and the relation between (a1) and (a4) is “contradiction”, we expect that the relation between (a2) and (a4) will be the same as the one between (a1) and (a4). This type of constraint is what we incorporate into the model. 3.2 Relations for Answer-Answer Pairs We deﬁne the logical relations for answer-answer pairs according to Radev’s work that deﬁnes 24 types of relations between texts for multidocument summarization (Radev, 2000). Table 1 shows the relations we consider. 4 Relation Identiﬁcation Model with Global Constraints We propose a joint identiﬁcation model of logical relations between answers in a thread. We consider that there are some constraints between logical relations. However, since not all relations satisfy a same constraint, we group logical relations into two types of super-relations on the basis of two kinds of commonality; transitivity and semantic similarity. To incorporate constraints between relations, we try to identify relations for all pairs in a thread jointly. For these purposes, we take an"
I11-1103,P04-1085,0,0.0175292,"identiﬁcation of discourse relations in meetings or dialogs was tackled by some researchers. Hillard et al. demonstrated that automatic identiﬁcation of agreement and disagreement is feasible by using various textual, durational, and acoustic features (Hillard et al., 2003). Galley et al. described a statistical approach for modeling agreements and disagreements in conversational interaction, and classiﬁed utterances as agreement or disagreement by using the adjacency pairs and features that represent various pragmatic inﬂuences of previous agreements or disagreements to the target utterance (Galley et al., 2004). Jimbo et al. proposed a model of relation identiﬁcation for Community-based Question Answering services (Jimbo et al., 2010). Their model identiﬁed relations using Support Vector Machines with various features. We think considering constraints among relations might contribute to improve the performance of identifying relations. Therefore, we realize it with a Markov logic network. Relation identiﬁcation is considered as a problem to ﬁnd labeled edges between pairs of nodes, where a node is an answer in a thread. Structured output learning is a method to predict such a structure (Tsochantarid"
I11-1103,N03-2012,0,0.0136321,"November 8 – 13, 2011. 2011 AFNLP similar questions, factuality and so on, and their question type taxonomy is based on the expected answer. Achananuparp et al. proposed a model to extract a diverse set of answers (Achananuparp et al., 2010). Their approach is based on a graph whose edges have weight about similarity and redundancy. Meanwhile, identiﬁcation of discourse relations in meetings or dialogs was tackled by some researchers. Hillard et al. demonstrated that automatic identiﬁcation of agreement and disagreement is feasible by using various textual, durational, and acoustic features (Hillard et al., 2003). Galley et al. described a statistical approach for modeling agreements and disagreements in conversational interaction, and classiﬁed utterances as agreement or disagreement by using the adjacency pairs and features that represent various pragmatic inﬂuences of previous agreements or disagreements to the target utterance (Galley et al., 2004). Jimbo et al. proposed a model of relation identiﬁcation for Community-based Question Answering services (Jimbo et al., 2010). Their model identiﬁed relations using Support Vector Machines with various features. We think considering constraints among re"
I11-1103,P08-1082,0,0.0261559,"ve and conclude our paper in section six. 2 Related Work The growing popularity of Community-based Question Answering services has prompted many researchers to investigate their characteristics and to propose models for applications using them. Question search and ranking answers are an important application because there are many threads in these services. Jeon et al. discussed a practical method for ﬁnding existing question and answer pairs in response to a newly submitted question (Jeon et al., 2005). Surdeanu et al. proposed an approach for ranking the answers retrieved by Yahoo! Answers (Surdeanu et al., 2008). Wang et al. proposed the ranking model for answers (Wang et al., 2009). Wang et al. proposed a model based on a deep belief network for the semantic relevance of question-answer pairs (Wang et al., 2010). The user’s qualiﬁcations affect the quality of his or her answer. For example, an IT expert may provide a good answer to a question about computers. Jurczyk and Agichtein proposed a model to estimate the authority of users as a means of identifying better answers (Jurczyk and Agichtein, 2007). Pal and Konstan proposed the expert identiﬁcation model (Pal and Konstan, 2010) . Each user has a"
I11-1103,I05-1038,1,0.870193,"re sentence. The question types whose answers are nouns require a speciﬁc class of named entity for the answer, e.g. the class of named entity, PERSON for the question type, Person. We call this class focused NE class. Table 7 shows the correspondence between a question type and a named entity class. Question. I’m planning a journey to Hokkaido. Can you suggest some good sightseeing places? Figure 2: Example of a question sential question is the latter sentence. Therefore, we extract the core sentence and the questionfocus and estimate the question type, on the basis of Tamura et al.’s model (Tamura et al., 2005). The core sentence is the most important sentence, that is, the one requiring an answer in the question. Usually, the core sentence is an interrogative one such as “Where is the most famous place in Hokkaido?”. But questioners sometimes ask questions without an interrogative form, such as “Please tell me some good sightseeing places Table 7: Question type and focused NE class Question type Person Product Facility Location Time Number Focused NE class PERSON ARTIFACT LOCATION, ORGANIZATION LOCATION DATE, TIME MONEY, PERCENT For n-gram features, we also consider unigram and bigram for the ﬁrst"
I11-1103,P10-1125,0,0.0156636,"models for applications using them. Question search and ranking answers are an important application because there are many threads in these services. Jeon et al. discussed a practical method for ﬁnding existing question and answer pairs in response to a newly submitted question (Jeon et al., 2005). Surdeanu et al. proposed an approach for ranking the answers retrieved by Yahoo! Answers (Surdeanu et al., 2008). Wang et al. proposed the ranking model for answers (Wang et al., 2009). Wang et al. proposed a model based on a deep belief network for the semantic relevance of question-answer pairs (Wang et al., 2010). The user’s qualiﬁcations affect the quality of his or her answer. For example, an IT expert may provide a good answer to a question about computers. Jurczyk and Agichtein proposed a model to estimate the authority of users as a means of identifying better answers (Jurczyk and Agichtein, 2007). Pal and Konstan proposed the expert identiﬁcation model (Pal and Konstan, 2010) . Each user has a background. If a user is an amateur in some ﬁeld, he or she cannot understand a difﬁcult question of the ﬁeld. For a user-oriented question ranking, Chen and Kao proposed a model to classify a question as"
I11-1103,D09-1054,0,0.0679126,"Missing"
I11-1103,C08-1063,0,0.163693,"er. Also, when a question tends to have various answers, e.g. the questioner asks for opinions (e.g. “What are your song recommendations?”), it is insufﬁcient to read only the best answer. Generally, as the only one best answer is chosen from the answers, a user may miss other beneﬁcial answers. When a user checks these services with a mobile device, its small display is inefﬁcient to browse all answers. To alleviate these problems, it would be useful to get an overview of answers in a thread, such as by identifying the relations between the answers or by summarizing them (Jimbo et al., 2010; Liu et al., 2008). The purpose of this study is to identify logical relations between answers with a high degree of accuracy, as a basis of these methods. We propose an identiﬁcation model with global constraints on logical relations between answers. Among the relations, there are some constraints like a transitive law. To this end, it is necessary to identify relations in a thread at once, and identiﬁed relations need to satisfy as many of these constraints as possible. Our model is based on a Markov logic network and incorporates these constraints as formulas of ﬁrst order logic. Also, we group logical relat"
kageura-kikui-2006-self,P98-1104,1,\N,Missing
kageura-kikui-2006-self,P98-1105,1,\N,Missing
kageura-kikui-2006-self,C98-1101,1,\N,Missing
kageura-kikui-2006-self,takezawa-etal-2002-toward,0,\N,Missing
N06-2049,I05-3017,0,0.0630146,"-based IOB tagging is downgraded into a character-based one. Taking the same example mentioned above, “ (whole) (Beijing city)” is labeled as ” (whole)/O (Beijing)/B (city)/I” in the subword-based tagging, where ” (Beijing)/B” is labeled as one unit. We will give a detailed description of this approach in Section 2. ∗ Now the second author is affiliated with NTT. In addition, we found a clear weakness with the IOB tagging approach: It yields a very low in-vocabulary (IV) rate (R-iv) in return for a higher out-of-vocabulary (OOV) rate (R-oov). In the results of the closed test in Bakeoff 2005 (Emerson, 2005), the work of (Tseng et al., 2005), using conditional random fields (CRF) for the IOB tagging, yielded very high R-oovs in all of the four corpora used, but the R-iv rates were lower. While OOV recognition is very important in word segmentation, a higher IV rate is also desired. In this work we propose a confidence measure approach to lessen the weakness. By this approach we can change R-oovs and R-ivs and find an optimal tradeoff. This approach will be described in Section 2.2. In the followings, we illustrate our word segmentation process in Section 2, where the subword-based tagging is impl"
N06-2049,C04-1081,0,0.577246,"agging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chine"
N06-2049,I05-3027,0,0.24431,"easure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If onl"
N06-2049,W03-1728,0,0.348609,"n: a subword-based tagging and a confidence measure approach. We found the former achieved better performance than the existing character-based tagging, and the latter improved segmentation further by combining the former with a dictionary-based segmentation. In addition, the latter can be used to balance out-of-vocabulary rates and in-vocabulary rates. By these techniques we achieved higher F-scores in CITYU, PKU and MSR corpora than the best results from Sighan Bakeoff 2005. 1 Introduction The character-based “IOB” tagging approach has been widely used in Chinese word segmentation recently (Xue and Shen, 2003; Peng and McCallum, 2004; Tseng et al., 2005). Under the scheme, each character of a word is labeled as ‘B’ if it is the first character of a multiple-character word, or ‘O’ if the character functions as an independent word, or ‘I’ otherwise.” For example, ” (whole) (Beijing city)” is labeled as ” (whole)/O (north)/B (capital)/I (city)/I”. We found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in"
O07-5005,2006.iwslt-plenaries.1,0,0.0390049,"Missing"
O07-5005,takezawa-etal-2002-toward,1,0.79505,"Missing"
O07-5005,takezawa-kikui-2004-comparative,1,0.893781,"Missing"
P06-2123,C04-1067,0,0.0221711,"and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used hybrid HMM models to integrate word level and character level information seamlessly. We used confidence measure to determine a better balance between R-oov and R-iv. The idea of using the confidence measure has appeared in (Peng and McCallum, 2004), where it was used to recognize the OOVs. In this work we used it more than that. By way of the confidence measure we combined results from the dictionary-based and the IOBtagging-based and as a result, we could achieve the optimal performance. Our main contribution is to extend the IOB tagging approach from being a character-based to a subwo"
P06-2123,C04-1081,0,0.160179,"ce measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This problem has been observed by some participants in the Bakeoff 2005 (Asahara et al., 2005), where they applied the IOB tagging to recognize OOVs, and added the OOVs to the lexicon used in the HMMbased or CRF-based approaches. (Nakagawa, 2004) used"
P06-2123,W03-1728,0,0.537024,"88, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”. Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging. For new test data, word boundaries are determined based on the results of tagging. We proposed a subword-based tagging for Chinese"
P06-2123,I05-3018,0,0.360556,"iro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled as “全/O 北/B 京/I 市/I”. Thus, the training data in word sequences are turned into IOB-labeled data in character sequences, which are then used as the training data for tagging. For new test data, word boundaries are determined based on the results of tagging. We proposed a subword-based tagging for Chinese word segmentation to improve the"
P06-2123,I05-3017,0,0.717137,"d Tagging for Confidence-dependent Chinese Word Segmentation Ruiqiang Zhang1,2 and Genichiro Kikui∗ and Eiichiro Sumita1,2 1 National Institute of Information and Communications Technology 2 ATR Spoken Language Communication Research Laboratories 2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan {ruiqiang.zhang,eiichiro.sumita}@atr.jp Abstract (SVM) (Kudo and Matsumoto, 2001), conditional random fields (CRF) (Peng and McCallum, 2004), and minimum error rate training (Gao et al., 2004). By analyzing the top results in the first and second Bakeoffs, (Sproat and Emerson, 2003) and (Emerson, 2005), we found the top results were produced by direct or indirect use of so-called “IOB” tagging, which converts the problem of word segmentation into one of character tagging so that part-of-speech tagging approaches can be used for word segmentation. This approach was also called “LMR” (Xue and Shen, 2003) or “BIES” (Asahara et al., 2005) tagging. Under the scheme, each character of a word is labeled as ”B” if it is the first character of a multiple-character word, or ”I” otherwise, and ”O” if the character functioned as an independent word. For example, “全(whole) 北京市(Beijing city)” is labeled"
P06-2123,P04-1059,0,0.0675735,"Missing"
P06-2123,W03-1730,0,0.0709253,"ts of a dictionary-based and a subword-tagging-based segmentation. This approach can produce an ideal tradeoff between the in-vocaulary rate and out-of-vocabulary rate. Our techniques were evaluated using the test data from Sighan Bakeoff 2005. We achieved higher F-scores than the best results in three of the four corpora: PKU(0.951), CITYU(0.950) and MSR(0.971). 1 Introduction Many approaches have been proposed in Chinese word segmentation in the past decades. Segmentation performance has been improved significantly, from the earliest maximal match (dictionary-based) approaches to HMM-based (Zhang et al., 2003) approaches and recent state-of-the-art machine learning approaches such as maximum entropy (MaxEnt) (Xue and Shen, 2003), support vector machine ∗ Now the second author is affiliated with NTT. While the IOB tagging approach has been widely used in Chinese word segmentation, we found that so far all the existing implementations were using character-based IOB tagging. In this work we propose a subword-based IOB tagging, which assigns tags to a pre-defined lexicon subset consisting of the most frequent multiple-character words in addition to single Chinese characters. If only Chinese characters"
P06-2123,N06-2049,1,0.825375,"43/0.948/0.948 0.951/0.961/0.962 R-oov 0.674/0.641/0.606 0.705/0.597/0.667 0.672/0.662/0.660 0.671/0.674/0.631 R-iv 0.950/0.964/0.969 0.950/0.977/0.968 0.958/0.966/0.966 0.951/0.967/0.970 Table 5: Effects of combination using the confidence measure. Here we used α = 0.8 and confidence threshold t = 0.7. The separator “/” divides the results of s1, s2, and s3. no change on F-score for AS corpus, but a better recall rate was found. Our results are better than the best one of Bakeoff 2005 in PKU, CITYU and MSR corpora. Detailed descriptions about subword tagging by CRF can be found in our paper (Zhang et al., 2006). 4 Discussion and Related works The IOB tagging approach adopted in this work is not a new idea. It was first implemented in Chinese word segmentation by (Xue and Shen, 2003) using the maximum entropy methods. Later, (Peng and McCallum, 2004) implemented the idea using the CRF-based approach, which yielded better results than the maximum entropy approach because it could solve the label bias problem (Lafferty et al., 2001). However, as we mentioned before, this approach does not take advantage of the prior knowledge of in-vocabulary words; It produced a higher R-oov but a lower R-iv. This pro"
P06-2123,W03-1719,0,\N,Missing
P06-2123,I05-3027,0,\N,Missing
P06-2123,N01-1025,0,\N,Missing
P06-2123,W03-1726,0,\N,Missing
P07-2040,P04-1054,0,0.359425,"Missing"
P07-2040,P83-1007,0,0.0250128,"“Tokyo12 ”) is the same as the feature of the pair with no semantic relation (“Ken11 ” and “New York14 ”). (S-1) Ken11 -wa Tokyo12 -de, Tom13 -wa New York14 -de umareta15 . (Ken11 was born15 in Tokyo12 , Tom13 in New York14 .) To solve the above problems, we propose a supervised learning method using contextual features. The rest of this paper is organized as follows. Section 2 describes the proposed method. We report the results of our experiments in Section 3 and conclude the paper in Section 4. 2 Relation Detection The proposed method employs contextual features based on centering theory (Grosz et al., 1983) as well as conventional syntactic and word-based features. These features are organized as a tree structure and are fed into a boosting-based classification algorithm. The method consists of three parts: preprocessing (POS tagging, NE tagging, and parsing), 1 The numbers show correspondences of words between Japanese and English. Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157–160, c Prague, June 2007. 2007 Association for Computational Linguistics feature extraction (contextual, syntactic, and wordbased features), and classification. In this section, we describe the underlyin"
P07-2040,P04-3022,0,0.0409885,"Missing"
P07-2040,P86-1031,0,0.179603,"re, when a pair of NEs with a semantic relation appears in a parallel sentence arise from predication ellipsis, the antecedent NE is contextually easily referred to in the phrase with the following NE. In the example of “(S-1)”, the pair “Ken11 ” and “Tokyo12 ” have a semantic relation “umareta15 (was born)”. Meanwhile, the pair “Ken11 ” and “New York14 ” has no semantic relation. Therefore, using whether the antecedent NE is referred to in the context with the following NE as features of a given pair of NEs would improve relation detection performance. In this paper, we use centering theory (Kameyama, 1986) to determine how easily a noun phrase can be referred to in the following context. 2.2 Centering Theory Centering theory is an empirical sorting rule used to identify the antecedents of (zero) pronouns. When there is a (zero) pronoun in the text, noun phrases that are in the previous context of the pronoun are sorted in order of likelihood of being the antecedent. The sorting algorithm has two steps. First, from the beginning of the text until the pronoun appears, noun 158 Priority wa ga ni o Ken22 Osaka23 others asu21, Naomi25 Figure 1: Information Stacked According to Centering Theory phras"
P07-2040,W04-3239,0,0.100028,"Missing"
P07-2040,P03-1005,0,\N,Missing
P07-2057,A00-2018,0,0.0337142,"tructurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with the growth 225 of Web 2.0. Such documents do not use controlled written language and contain fillers and emoticons. This implies that analyzing such documents is difficult for conventional parsers. This paper presents a new method of Japanese dependency parsing that utilizes sequential labeling based"
P07-2057,W02-2016,0,0.40304,"ifficult to analyze using conventional parsers. This paper presents dependency parsing whose goal is to analyze Japanese semi-spoken expressions. One characteristic of our method is that it can parse selfdependent (independent) segments using sequential labeling. 1 Introduction Dependency parsing is a way of structurally analyzing a sentence from the viewpoint of modification. In Japanese, relationships of modification between phrasal units called bunsetsu segments are analyzed. A number of studies have focused on parsing of Japanese as well as of other languages. Popular parsers are CaboCha (Kudo and Matsumoto, 2002) and KNP (Kurohashi and Nagao, 1994), which were developed to analyze formal written language expressions such as that in newspaper articles. Generally, the syntactic structure of a sentence is represented as a tree, and parsing is carried out by maximizing the likelihood of the tree (Charniak, 2000; Uchimoto et al., 1999). Units that do not modify any other units, such as fillers, are difficult to place in the tree structure. Conventional parsers have forced such independent units to modify other units. Documents published by end users (e.g., blogs) are increasing on the Internet along with t"
P07-2057,J94-4001,0,\N,Missing
P07-2057,E99-1026,0,\N,Missing
P10-2060,N04-1015,0,0.0308575,"e 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such a way that the ordered"
P10-2060,E06-1039,0,0.119465,"chasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics once for scoring. In addition, the aspects are clustered and similar aspects (e.g. air, ambience) are treated as the same aspect (e.g. atmosphere). In this paper we define f (e) as the frequency of e in the target documents."
P10-2060,W08-1106,0,0.017137,"uide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics once for scoring. In addition, the aspects are clustered and similar aspects (e.g. air, ambience) are treated as the same aspect (e.g. atmosphere). In this paper we define f (e) as the frequency of e in the target documents. Sentiments are extracted us"
P10-2060,W02-1001,0,0.00801288,"s still computationally hard, only the top b hypotheses are expanded. Note that our method learns w from texts automatically annotated by a POS tagger and a named entity tagger. Thus manual annotation isn’t required. (4) i=0 Therefore, the score of sequence S is w&gt; Φ(S). Given a training set, if a trained parameter w assigns a score w&gt; Φ(S + ) to an correct order S + that is higher than a score w&gt; Φ(S − ) to an incorrect order S − , it is expected that the trained parameter will give higher score to naturally ordered sentences than to unnaturally ordered sentences. We use Averaged Perceptron (Collins, 2002) to find w. Averaged Perceptron requires an argmax operation for parameter estimation. Since we attempt to order a set of sentences, the operation is regarded as solving the Traveling Salesman Problem; that is, we locate the path that offers maximum score through all n sentences as s0 and sn+1 are starting and ending points, respectively. Thus the operation is NP-hard and it is difficult to find the global optimal solution. To alleviate this, we find an approximate solution by adopting the dynamic programming technique of the Held and Karp Algorithm (Held and Karp, 1962) and beam search. We sh"
P10-2060,P03-1069,0,0.621509,"le high. Figure 1: A typical summary. are helpful, such representations necessitate some simplifications of information to presentation. In contrast, text can present complex information that can’t readily be visualized, so in this paper we focus on producing textual summaries. One crucial weakness of existing text-oriented summarizers is the poor readability of their results. Good readability is essential because readability strongly affects text comprehension (Barzilay et al., 2002). To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). Barzilay et al. (2002) proposed an algorithm for ordering sentences according to the dates of the publications from which the sentences were extracted. Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. Both methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such"
P10-2060,E09-1059,0,0.145359,"tivities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics once for scoring. In addition, the aspects are clustered and similar aspects (e.g. air, ambience) are treated as the same aspect (e.g. atmosphere). In this paper we define f (e) as the frequency of e in the target documents. Sentiments are extracted using a sentiment lexic"
P10-2060,N09-2029,0,0.0206694,"uct improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics once for scoring. In addition, the aspects are clustered and similar aspects (e.g. air, ambience) are treated as the same aspect (e.g. atmosphere). In this paper we define f (e) as the frequency of e in the target documents. Sentiments are extracted using a sentiment lexicon and pattern matched from"
P10-2060,W04-1013,0,0.0879433,"methods delink sentence extraction from sentence ordering, so a sentence can be extracted that cannot be ordered naturally with the other extracted sentences. To solve this problem, we propose an algorithm that chooses sentences and orders them simultaneously in such a way that the ordered sentences maximize the scores of informativeness and readability. Our algorithm efficiently searches for the best sequence of sentences by using dynamic programming and beam search. We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGE score (Lin, 2004) and subjective readability measures. As far as we know, this is the first work to 1 Introduction The Web holds a massive number of reviews describing the sentiments of customers about products and services. These reviews can help the user reach purchasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of informati"
P10-2060,P06-1028,0,0.06655,"Missing"
P10-2060,P08-1036,0,0.0848658,"of customers about products and services. These reviews can help the user reach purchasing decisions and guide companies’ business activities such as product improvements. It is, however, almost impossible to read all reviews given their sheer number. These reviews are best utilized by the development of automatic text summarization, particularly sentiment summarization. It enables us to efficiently grasp the key bits of information. Sentiment summarizers are divided into two categories in terms of output style. One outputs lists of sentences (Hu and Liu, 2004; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008), the other outputs texts consisting of ordered sentences (Carenini et al., 2006; Carenini and Cheung, 2008; Lerman et al., 2009; Lerman and McDonald, 2009). Our work lies in the latter category, and a typical summary is shown in Figure 1. Although visual representations such as bar or rader charts 325 Proceedings of the ACL 2010 Conference Short Papers, pages 325–330, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics once for scoring. In addition, the aspects are clustered and similar aspects (e.g. air, ambience) are treated as the same aspect (e.g. atmosphere"
P10-2060,P07-2057,1,\N,Missing
P10-2060,J08-1001,0,\N,Missing
P11-2128,P98-1068,0,0.642164,"Missing"
P11-2128,D08-1106,0,0.0463252,"Missing"
P11-2128,D10-1022,0,0.0257846,"ods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accurately select negative examples. Our method uses topic information to efficiently solve both the problem of extracting global information and the problem of selecting negative examples. 6 Conclusion We proposed an approach to set expansion that uses topic information in three modules and showed that it can improve expansion accuracy. The remaining problem is that the grain size of topic models is not always the same as the target domain. To resolve this problem, we will"
P11-2128,P09-1113,0,0.0233032,"s. In Section 2, we illustrate discriminative bootstrapping algorithms and describe their problems. Our proposal is described in Section 3 and experimental results are shown in Section 4. Related works are described in Section 5. Finally, Section 6 provides our conclusion and describes future works. 2 Problems of the previous Discriminative Bootstrapping method Some previous works introduced discriminative methods based on the logistic sigmoid classifier, which can utilize arbitrary features for the relation extraction task instead of a scoring function such as Espresso (Bellare et al., 2006; Mintz et al., 2009). Bellare et al. reported that the discriminative approach achieves better accuracy than Espresso when the number of extracted pairs is increased because 726 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726–731, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics multiple features are used to support the evidence. However, three problems exist in their methods. First, they use only local context features. The discriminative approach is useful for using arbitrary features, however, they did not identi"
P11-2128,P08-1003,0,0.133341,"Missing"
P11-2128,P06-1015,0,0.17644,"1 Introduction The task of this paper is entity set expansion in which the lexicons are expanded from just a few seed entities (Pantel et al., 2009). For example, the user inputs a few words “Apple”, “Google” and “IBM” , and the system outputs “Microsoft”, “Facebook” and “Intel”. Many set expansion algorithms are based on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on ∗ Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate"
P11-2128,P10-1044,0,0.0402157,"(Japanese musicians) toritomento (treatment), keana (pore), hoshitsu (moisture retention) Table 2: The characteristic words belonging to three topics, zh , zl and ze . zh is the nearest topic and zl is the farthest topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating “drifted entities” extracted by the baseline system. are clustering methods based on probabilistic measures. By contrast, Pas¸ca and Durme (2008) proposed clustering methods that are effective in terms of extraction, even though their clustering target is only the surrounding context. Ritter and Etzioni (2010) proposed a generative approach to use extended LDA to model selectional preferences. Although their approach is similar to ours, our approach is discriminative and so can treat arbitrary features; it is applicable to bootstrapping methods. The accurate selection of negative examples is a major problem for positive and unlabeled learning methods or general bootstrapping methods and some previous works have attempted to reach a solution (Liu et al., 2002; Li et al., 2010). However, their methods are hard to apply to the Bootstrapping algorithms because the positive seed set is too small to accu"
P11-2128,P06-1028,0,0.117094,"up as many negative documents as there are positive documents with each selected negative topic being equally represented. 3.4 Candidate Pruning Previous works discriminate all candidates for extracting new entities. Our basic system can constrain 1 z is a random variable whose sample space is represented as a discrete variable, not explicit words. 728 4.1 Experimental Settings We use 30M Japanese blog articles crawled in May 2008. The documents were tokenized by JTAG (Fuchi and Takagi, 1998), chunked, and labeled with IREX 8 Named Entity types by CRFs using Minimum Classification Error rate (Suzuki et al., 2006), and transformed into features. The context features were defined using the template “(head) entity (mid.) attribute (tail)”. The words included in each part were used as surface, part-of-speech and Named Entity label features added position information. Maximum word number of each part was set at 2 words. The features have to appear in both the positive and negative training data at least 5 times. In the experiments, we used three domains, car (“CAR”), broadcast program (“PRG”) and sports organization (“SPT”). The adjustment numbers for basic settings are Ns = 10, Na = 10, Nn = 100. After ru"
P11-2128,W02-1028,0,0.144401,"on bootstrapping algorithms, which iteratively acquire new entities. These algorithms suffer from the general problem of “semantic drift”. Semantic drift moves the extraction criteria away from the initial criteria demanded by the user and so reduces the accuracy of extraction. Pantel and Pennacchiotti (2006) proposed Espresso, a relation extraction method based on the co-training bootstrapping algorithm with entities and attributes. Espresso alleviates semanticdrift by a sophisticated scoring system based on ∗ Presently with Okayama Prefectural University pointwise mutual information (PMI). Thelen and Riloff (2002), Ghahramani and Heller (2005) and Sarmento et al. (2007) also proposed original score functions with the goal of reducing semantic-drift. Our purpose is also to reduce semantic drift. For achieving this goal, we use a discriminative method instead of a scoring function and incorporate topic information into it. Topic information means the genre of each document as estimated by statistical topic models. In this paper, we effectively utilize topic information in three modules: the first generates the features of the discriminative models; the second selects negative examples; the third prunes i"
P11-2128,C98-1065,0,\N,Missing
P11-2128,D09-1098,0,\N,Missing
P98-1110,P91-1034,0,0.451727,"econd type, called the statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * T h i s research was d o n e w h e n t h e a u t h o r was at C e n t e r for t h e S t u d y of L a n g u a g e a n d I n f o r m a t i o n ( C S L I ) , Stanford University. 1In fact, this is p a r t l y s h o w n by t h e fact t h a t m a n y M T s y s t e m s have s u b s t i t u t a b l e d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associat"
P98-1110,J94-4003,0,0.168895,"e statistics-based approach, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * T h i s research was d o n e w h e n t h e a u t h o r was at C e n t e r for t h e S t u d y of L a n g u a g e a n d I n f o r m a t i o n ( C S L I ) , Stanford University. 1In fact, this is p a r t l y s h o w n by t h e fact t h a t m a n y M T s y s t e m s have s u b s t i t u t a b l e d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associative relations between t"
P98-1110,C96-2098,0,0.396583,"h, learns disambiguation knowledge from large corpora. Brown et al. presented an algorithm that * T h i s research was d o n e w h e n t h e a u t h o r was at C e n t e r for t h e S t u d y of L a n g u a g e a n d I n f o r m a t i o n ( C S L I ) , Stanford University. 1In fact, this is p a r t l y s h o w n by t h e fact t h a t m a n y M T s y s t e m s have s u b s t i t u t a b l e d o m a i n - d e p e n d e n t (or &quot;user&quot; ) dictionaries . 670 relies on translation probabilities estimated from large bilingual corpora (Brown et al., 1990)(Brown et al., 1991). Dagan and Itai (1994) and Tanaka and Iwasaki (1996) proposed algorithms for selecting target words by using word co-occurrence statistics in the target language corpora. The latter algorithms using mono-lingual corpora are particularly important because, at present, we cannot always get a sufficient amount of bilingual or parallel corpora. Our method is closely related to (Tanaka and Iwasaki, 1996) from the viewpoint that they both rely on mono-lingual corpora only and do not require any syntactic analysis. The difference is that our method uses &quot;coherence scores&quot;, which can capture associative relations between two words which do not co-occur"
P98-1110,C88-1016,0,\N,Missing
P98-1110,J90-2002,0,\N,Missing
takezawa-kikui-2004-comparative,W01-1401,0,\N,Missing
takezawa-kikui-2004-comparative,1999.mtsummit-1.34,0,\N,Missing
takezawa-kikui-2004-comparative,takezawa-etal-2002-toward,1,\N,Missing
takezawa-kikui-2004-comparative,hoge-2002-project,0,\N,Missing
W02-0704,1994.amta-1.26,0,\N,Missing
W02-0704,W95-0114,0,\N,Missing
W02-0704,C94-1032,0,\N,Missing
W02-0704,J90-2002,0,\N,Missing
W04-1708,2002.tmi-tutorials.2,0,0.0357521,"Missing"
W04-1708,P02-1040,0,0.102728,"t examination designers to make the questions and the alternative “detractor” answers. In this paper, we propose a method for the automatic measurement of English language proficiency by applying automatic evaluation techniques. The proposed method selects adequate test sentences from an existing corpus. Then, it automatically evaluates the translations of test sentences done by users. The core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of Machine Translation (MT) technology (Su et al., 1992; Papineni et al., 2002; NIST, 2002). In the proposed method, we apply these MT evaluation technologies to the measurement of human English language proficiency. The proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking. It does not measure elementary capabilities including vocabulary or grammar. This method also proposes a test sentence selection scheme to enable efficient testing. Section 2 describes several automatic evaluation methods applied to the proposed method. Section 3 introduces the proposed evaluation scheme. Section 4 show"
W04-1708,shimohata-sumita-2002-automatic,1,0.827135,"Missing"
W04-1708,C92-2067,0,0.27597,"r costs for expert examination designers to make the questions and the alternative “detractor” answers. In this paper, we propose a method for the automatic measurement of English language proficiency by applying automatic evaluation techniques. The proposed method selects adequate test sentences from an existing corpus. Then, it automatically evaluates the translations of test sentences done by users. The core technology of the proposed method, i.e., the automatic evaluation of translations, was developed in research aiming at the efficient development of Machine Translation (MT) technology (Su et al., 1992; Papineni et al., 2002; NIST, 2002). In the proposed method, we apply these MT evaluation technologies to the measurement of human English language proficiency. The proposed method focuses on measuring the communicative skill of structuring sentences, which is indispensable for writing and speaking. It does not measure elementary capabilities including vocabulary or grammar. This method also proposes a test sentence selection scheme to enable efficient testing. Section 2 describes several automatic evaluation methods applied to the proposed method. Section 3 introduces the proposed evaluation"
W04-1708,1999.mtsummit-1.44,1,0.732327,"automatic evaluation methods of translation. These methods were proposed to evaluate MT output, but they are applicable to translation by humans. All of these methods are based on the same idea, that is, to compare the target translation for evaluation with high-quality reference translations that are usually done by skilled translators. Therefore, these methods require a corpus of high-quality human reference translations. We call these translations as “references”. 2.1 DP-based Method The DP score between a translation output and references can be calculated by DP matching (Su et al., 1992; Takezawa et al., 1999). First, we define the DP score between sentence (i.e., word array) Wa and sentence Wb by the following formula. T −S−I −D (1) T where T is the total number of words in Wa , S is the number of substitution words for comparing Wa to Wb , I is the number of inserted words for comparing Wa to Wb , and D is the number of deleted words for comparing Wa to Wb . Using Equation 1, (Si (j)), that is, the test sentence unit DP-score of the translation of test sentence j done by subject i, can be calculated by the following formula. SDP (Wa , Wb ) = SDPi (j) = max n k=1 to Nref o SDP (Wref (k) (j), Wsub("
W04-1708,takezawa-etal-2002-toward,1,0.786432,"2 shows the flow of measuring English proficiency. In the parameter-estimation phase, for each subject, we first calculate the test set unit automatic score by using Equation 3, 6 or 7. Next, we apply regression analysis using the automatic scores and subjects’ TOEIC scores. In the testing phase, we calculate a user’s TOEIC score using the automatic score of the user and the regression line calculated in the parameter-estimation phase. 4 Experiments 4.1 Experimental Conditions 4.1.1 Test sets For the experiments, we employ two different test sets. One is BTEC (Basic Travel Expression Corpus) (Takezawa et al., 2002) and the other is SLTA1 (Takezawa, 1999). Both BTEC and SLTA1 are parts of bilingual corpora that have been collected for research on speech translation systems. However, they have different features. A detailed analysis of these corpora was done by Kikui et al. (2003). Here, we briefly explain these test sets. In this study, we use the Japanese side as a test set and the English side as a reference for automatic evaluation. BTEC BTEC was designed to cover expressions for every potential subject in travel conversation. This test set was collected by investigating “phrasebooks” that contain Jap"
W04-3244,A00-2004,0,0.0205453,"logy, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Yates and Ribeiro-Neto, 1999; Sch¨utze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved. While segmentation algorithms, such as T EXT T ILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs. However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product. In this paper, we propose an optimal distance function that is parameterized by a global metric matrix. This metric is optimal in the sense of global quadratic minimization, and can be learned from the given clusters in the training data. These clusters are often attributable wi"
W04-3244,P94-1002,0,0.0713237,"e leaf comparison is still atomic), Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Yates and Ribeiro-Neto, 1999; Sch¨utze, 1992) mainly due to a lack of scalable segmentation algorithms and the huge amount of data involved. While segmentation algorithms, such as T EXT T ILING (Hearst, 1994) and its recent successors using the inter-paragraph similarity matrix (Choi, 2000), all themselves use nonstructural cosine similarity as a measure of semantic proximity between paragraphs. However, the distance function so far has been largely defined and used ad hoc, usually by a tf.idf weighting scheme (Salton and Yang, 1973) and a simple cosine similarity, equivalently, an Euclidean dot product. In this paper, we propose an optimal distance function that is parameterized by a global metric matrix. This metric is optimal in the sense of global quadratic minimization, and can be learned fro"
W04-3244,suyaga-etal-2002-proposal,1,0.894632,"Missing"
W04-3244,P03-1005,0,0.0315214,"mparing these expressions based on semantic proximity is a fundamental task and has many applications. Generally, two basic approaches exist to compare two expressions: (a) structural and (b) nonstructural. Structural approaches make use of syntactic parsing or dependency analysis to make a rigorous comparison; nonstructural approaches use vector representation and provide a rough but fast comparison that is required for search/retrieval from a vast amount of corpora. While structural approaches have recently become available in a kernel-based sophisticated treatment (Collins and Duffy, 2001; Suzuki et al., 2003), here we concentrate on nonstructural comparison. This is not only because nonstructural comparison constitutes an integral part in structural methods (that is, even in hierarchical methods the leaf comparison is still atomic), Kenji Kita Center for Advanced Information Technology, Tokushima University Minamijosanjima 2-1 Tokushima 770-8506, Japan kita@is.tokushima-u.ac.jp but because it is frequently embedded in many applications where structural parsings are not available or computationally too expensive. For example, information retrieval has long used the ‘bag of words’ approach (Baeza-Ya"
W10-3709,I08-2094,1,0.883832,"Missing"
W10-3709,W04-0405,0,\N,Missing
W99-0905,P95-1026,0,0.116351,"er related research. Fung et al. ((Fung and K., 1997),(Fung and Yee, 1998)) presented interesting results on bilingual lexicon construction from ""comparable corpora"", which is non-parallel bilingual corpora in the same domain. Since their algorithm does not resolve word-sense ambiguity in the source language, it would be interesting to combine unsupervised disambiguation in the same way as we did. Although we employed the distributional clustering algorithm for resolving word sense ambiguity, different algorithms are also applicable. Among them, the unsupervised algorithm using decisiontrees (Yarowsky, 1995) has achieved promising performance. An interesting approach is to use the output of our sense-translation linking process as the ""seeds"" required by that algorithm. Discussion Although our method produced higher accuracy than the previous method, we cannot tell whether or not the difference is quantitatively significant. Further experiments with more data might be required. From a qualitative view point, the proposed method successfully learned useful knowledge for choosing the correct target word. An example is shown in Table 4. One advantage of the proposed method is that it is applicable t"
W99-0905,P91-1034,0,0.286031,"ss (i.e., cluster) with a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments. 1 Introduction Choosing the correct translation of a content word in context, referred to as ""translation disambiguation (of content word)"", is a key task in machine translation. It is also crucial in cross-language text processing including cross-language information retrieval and abstraction. Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted. Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful 2 Overview of the method Figure 1 shows an overview of the entire method. Components inside the dotted line"
W99-0905,1997.tmi-1.13,0,0.196995,"h a target word that is most relevant to the usage. This paper also shows preliminary results of translation experiments. 1 Introduction Choosing the correct translation of a content word in context, referred to as ""translation disambiguation (of content word)"", is a key task in machine translation. It is also crucial in cross-language text processing including cross-language information retrieval and abstraction. Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted. Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful 2 Overview of the method Figure 1 shows an overview of the entire method. Components inside the dotted line on the left rep"
W99-0905,J94-4003,0,0.450681,"translation of a content word in context, referred to as ""translation disambiguation (of content word)"", is a key task in machine translation. It is also crucial in cross-language text processing including cross-language information retrieval and abstraction. Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted. Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful 2 Overview of the method Figure 1 shows an overview of the entire method. Components inside the dotted line on the left represent word-sense disambiguation (WSD) in the source language. There are two sub-processes: distributional clustering and categorizing. The former automatically identi"
W99-0905,W97-0119,0,0.0605897,"Missing"
W99-0905,P98-1069,0,0.0696079,"rnatives. If all the translation alternatives in the bilingual dictionary were judged to be correct, we then excluded them in calculating the success-rate. The success rates of the proposed method and the previous algorithm are shown in Table 3. 5.3 Related Work Since we have referred to previous work in the area of statistical target word selection in Section 1 ((Brown et hi., 1990), (Brown et al., 1991), (Brown, 1997), (Yamabana et hi., 1998), (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998)), this section focuses on other related research. Fung et al. ((Fung and K., 1997),(Fung and Yee, 1998)) presented interesting results on bilingual lexicon construction from ""comparable corpora"", which is non-parallel bilingual corpora in the same domain. Since their algorithm does not resolve word-sense ambiguity in the source language, it would be interesting to combine unsupervised disambiguation in the same way as we did. Although we employed the distributional clustering algorithm for resolving word sense ambiguity, different algorithms are also applicable. Among them, the unsupervised algorithm using decisiontrees (Yarowsky, 1995) has achieved promising performance. An interesting approac"
W99-0905,P98-1110,1,0.622485,"arget word selection. The method presented in this paper solves this problem by choosing the target word that corresponds to the usage identified in the source language corpora. This method is totally unsupervised in the sense that it acquires disambiguation information from non-parallel bilingual corpora (preferably in the same domain) free from tagging. It combines two unsupervised disambiguation algorithms: one is the word sense disambiguation algorithm based on distributional clustering(Schuetze, 1997) and the other is the translation disambiguation algorithm using target language corpora(Kikui, 1998). For the given word in context, the former algorithm identifies its usage as one of several predefined usage classes derived by clustering a large amount of usages in the source language corpus. The latter algorithm is responsible for associating each usage class (i.e., cluster) with a target word that best expresses the usage. The following sections are organized as follows. In Section 2, we overview the entire method. The following two sections (i.e., Section 3 and 4) then introduce the two major components of the method including the two unsupervised disambiguation algorithms. Section 5 an"
W99-0905,C96-2098,0,0.177163,"word in context, referred to as ""translation disambiguation (of content word)"", is a key task in machine translation. It is also crucial in cross-language text processing including cross-language information retrieval and abstraction. Due to the recent availability of large text corpora, various statistical approaches have been tried including using 1) parallel corpora (Brown et al., 1990), (Brown et al., 1991), (Brown, 1997), 2) non-parallel bilingual corpora tagged with topic area (Yamabana et al., 1998) and 3) un-tagged mono-language corpora in the target language (Dagan and Itai, 1994), (Tanaka and Iwasaki, 1996), (Kikui, 1998). A problem with the first two approaches is that it is not easy to obtain sufficiently large parallel or manually tagged corpora for the pair of languages targeted. Although the third approach eases the problem of preparing corpora, it suffers from a lack of useful 2 Overview of the method Figure 1 shows an overview of the entire method. Components inside the dotted line on the left represent word-sense disambiguation (WSD) in the source language. There are two sub-processes: distributional clustering and categorizing. The former automatically identifies different usages (or se"
W99-0905,C98-1066,0,\N,Missing
W99-0905,C88-1016,0,\N,Missing
W99-0905,C98-1106,1,\N,Missing
