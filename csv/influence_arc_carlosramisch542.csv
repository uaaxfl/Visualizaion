2017.jeptalnrecital-court.1,F12-2024,1,0.858093,"Missing"
2017.jeptalnrecital-court.1,C86-1001,0,0.547911,"Missing"
2017.jeptalnrecital-court.1,2010.jeptalnrecital-invite.1,0,0.108359,"Missing"
2017.jeptalnrecital-court.1,L16-1262,0,0.0611543,"Missing"
2017.jeptalnrecital-court.1,W17-1704,1,0.869636,"Missing"
2020.coling-main.296,bouamor-etal-2012-identifying,0,0.0316025,"iring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this"
2020.coling-main.296,N10-1029,0,0.0376178,"e-of-the-art systems based on complex architectures, including advanced machine/deep learning techniques, and requiring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-"
2020.coling-main.296,J17-4005,1,0.912976,"Missing"
2020.coling-main.296,N19-1423,0,0.00895363,"WE-based F-score. It outperformed 6 other open track systems, notably those using complex neural architectures and contextual word embeddings. It scored best (F=0.65), across both tracks in Italian, and second, with less than 0.01 point F-score difference behind the best (open track) system in Polish, Portuguese and Swedish (global F=0.82, F=0.73 and F=0.71). Also for phenomenon-specific measures Seen2Seen scored second across both tracks on both discontinuous and seen VMWEs. The only (open) system which outperformed Seen2Seen is a deep learning system using a generic multilingual BERT model (Devlin et al., 2019) tuned for joined parsing and VMWE identification. It scored a bit less than 0.04 F-measure point higher in the general ranking. Together with Seen2Seen, we submitted another system, Seen2Unseen, which relies on the former for seen VMWEs and adds discovery methods to cover unseen VMWE (Pasquer et al., 2020b). 6 Interpretability and Generalization Our method proves encouraging. Not only does it outperform state-of-the-art systems, even those in the open track, but also it is interpretable. First, it is straightforward to identify the filters responsible for errors made by the system, enabling i"
2020.coling-main.296,J09-1005,0,0.119582,"Missing"
2020.coling-main.296,2020.lrec-1.497,0,0.051097,"Missing"
2020.coling-main.296,W18-4932,1,0.834419,"n illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNO"
2020.coling-main.296,C18-1219,1,0.745152,"n illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNO"
2020.coling-main.296,2020.mwe-1.16,1,0.896122,"hole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaDET porteNOUN aAUX résolumentADV étéAUX ferméeVERB aux initiatives. ‘The door was firmly closed on initiatives.’ (VID) (7) Il fermeVERB laDET porteNOUN.sing à une loi. ‘He"
2020.coling-main.296,2013.mtsummit-wmwumttt.8,0,0.0133367,"ed on complex architectures, including advanced machine/deep learning techniques, and requiring up to millions of parameters. We also expect the method to generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System ("
2020.coling-main.296,N19-1275,0,0.0479994,"Missing"
2020.coling-main.296,W19-5110,1,0.801388,"if a VMWE has previously been observed in a training corpus or in a lexicon, it can re-appear in morphosyntactically diverse forms. Examples (1–2) show two occurrences of a VMWE with variation in the components’ inflection (cutting vs. cut), word order, presence of discontinuities (were), and syntactic relations (obj vs. nsubj). (1) Some companies were cutting cornersobj to save costs. (2) The field would look uneven if cornersnsubj were cut. However, unrestricted variability is not a reasonable assumption either, since it may lead to literal or coincidental occurrences of VMWEs’ components (Savary et al., 2019b), as in (3) and (4), respectively.2 (3) Start with cutting one corner of the disinfectant bag. :::::: :::::: (4) If you ::: cut along these lines, you’ll get two acute ::::::: corners. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 Henceforth, the lexicalized components of a MWE, i.e. those always realized by the same lexemes, appear in bold. 2 Henceforth, literal and coincidental occurrences are highlighted with wavy underlining, following Savary et al. (2019b). 3333 Proceedings of the 28"
2020.coling-main.296,L18-1047,0,0.0242849,"generalize over many languages since the properties it exploits proved generic by the state-of-the-art studies. Details of this method and an illustration of the whole set of filters are exposed in the next section. Extraction and filtering techniques have been employed in the past (Constant et al., 2017, Sec. 3.2.1). Dictionary lookup was performed using lemmas, POS and distance filters prior to machine translation (Carpuat and Diab, 2010; Ramisch et al., 2013). In bilingual MWE lexicon discovery, filters can be applied before (Bouamor et al., 2012) or after extraction (Caseli et al., 2010; Semmar, 2018). Filters can be implemented using finite-state machines (Silberztein, 1997; Savary, 2009) and parameterized using classifiers (Pasquer et al., 2018b; Pasquer et al., 2020a). Our originality lies in (a) proposing a highly interpretable method based on on/off filters, (b) evaluating it on several categories of seen VMWEs in 11 languages, and (c) outperforming less interpretable state-of-the-art machine learning models. 4 Filter-Based Seen2020 System (5) LaDET lumièreNOUN.sing seraAUX faiteVERB sur ce drame. ‘The light will be done on this drama’⇒‘Light will be shed on this drama.’ (VID) (6) LaD"
2020.coling-main.296,W18-4930,0,0.0409915,"Missing"
2020.coling-main.296,W18-4931,0,0.061361,"Missing"
2020.coling-main.298,P11-2123,0,0.0126922,"SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al., 2018), such as a threeway classification of adjectives (Boleda et al., 2012) or animate vs. inanimate nouns (Øvrelid, 2006). We understand supersenses as general coarse semantic distinctions. Our"
2020.coling-main.298,2020.lrec-1.724,1,0.874407,"Missing"
2020.coling-main.298,Q17-1010,0,0.0487418,"ngs have become commonplace in NLP, as they naturally represent input (words) in state-of-the-art neural models. Although they can be randomly initialised and learned, unsupervised pre-training on raw corpora is common (Turian et al., 2010). Embeddings can be pre-trained as by-products of predictive neural language models (Mikolov et al., 2013), by factorisation of the co-occurrence matrix (Landauer and Dumais, 1997; Pennington et al., 2014), etc. Sub-lexical units (character n-grams) address linguistic variability, e.g., due to rich morphology, non-standard text, and out-of-vocabulary forms (Bojanowski et al., 2017). Most of the models prior to 2018 are static, assuming a single vector per word. These models suffer from meaning conflation, i.e., a single vector is created for ambiguous units, ignoring polysemous and multi-facet words. Advances in neural networks triggered the development of contextual embeddings, with representations conditioned on the surrounding words. They can be obtained using stacked recurrent layers as in ELMo (Peters et al., 2018), or attention-based transformers as in BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018). In addition to their outstanding performances, these"
2020.coling-main.298,S12-1023,0,0.0459254,"Missing"
2020.coling-main.298,F12-2024,0,0.0332991,"Missing"
2020.coling-main.298,W03-1022,0,0.180902,"performing WSD in languages other than English (Moro et al., 2014). Supervised WSD relies on sense-annotated corpora specifying which of the senses in the inventory are employed in context (Pasini and Camacho-Collados, 2020), e.g., SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream"
2020.coling-main.298,P17-2094,0,0.0229623,"nymy, synonymy). For many years, the English Wordnet has been the basis of 3358 sense-annotated corpora (Landes et al., 1998) and WSD research (Navigli, 2009). Babelnet (Navigli and Ponzetto, 2012) is a semi-automatic multilingual lexicon similar to Wordnet and also quite popular for performing WSD in languages other than English (Moro et al., 2014). Supervised WSD relies on sense-annotated corpora specifying which of the senses in the inventory are employed in context (Pasini and Camacho-Collados, 2020), e.g., SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we em"
2020.coling-main.298,N15-1184,0,0.0730725,"Missing"
2020.coling-main.298,D15-1208,0,0.0255752,") and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al., 2018), such as a threeway classification of adjectives (Boleda et al., 2012) or animate vs. inanimate nouns (Øvrelid, 2006). We understand supersenses as general coarse semantic distinctions. Our set of six semantic labels is related to Wordnet supers"
2020.coling-main.298,P16-1191,0,0.194906,"which a lexical unit is associated to several vectors (as in contextual models), but with some generalisation across occurrences (as in static models). Unsupervised sense (multi-prototype) embeddings can be obtained by adapting the objective of the learning procedure (Neelakantan et al., 2014), or with word sense induction methods based on clustering, e.g., Panchenko et al. (2017). For interpretability, resources such as Wordnet can be used to semantically enhance static embeddings (Faruqui et al., 2015) or to learn representations for Wordnet synsets (Rothe and Sch¨utze, 2015), supersenses (Flekova and Gurevych, 2016), or Babelnet senses (Camacho-Collados et al., 2016). Contextual models such as BERT can be enriched with supersenses, predicted jointly with masked words during training, with observed improvements in tasks requiring lexical semantics (Levine et al., 2020). Interpretable semantic representations One of the most popular sense inventories in NLP is Wordnet (Miller et al., 1990), in which words are grouped into synsets and linked to each other via lexical-semantic relations (e.g., hypernymy, synonymy). For many years, the English Wordnet has been the basis of 3358 sense-annotated corpora (Landes"
2020.coling-main.298,C18-1001,0,0.0144746,"al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al., 2018), such as a threeway classification of adjectives (Boleda et al., 2012) or animate vs. inanimate nouns (Øvrelid, 2006). We understand supersenses as general coarse semantic distinctions. Our set of six semantic labels is related to Wordnet supersenses, but there is not a 1:1 relation between our supersenses and Wordnet’s ones. Weakly supervised semantic classification Many models have been proposed to induce lexical semantics from raw corpora without supervision, e.g., (Lin, 1998), usually performing unsupervised WSD as a by-product. Most methods rely on distributional clustering algorithms, e"
2020.coling-main.298,S17-1025,0,0.0193502,"ted corpora, such as Semcor (Flekova and Gurevych, 2016) or sense inventories such as Wordnet (Levine et al., 2020). Our embedding learning procedure is not fully unsupervised, but uses weak supervision to bootstrap semantic classes from corpora. Typical or non-ambiguous words can be used to produce sense-annotated data, which in turn enable training classifiers for inducing lexical knowledge. This has been proposed in several studies, e.g., Mihalcea (2003), especially for polysemy pattern detection (Boleda et al., 2012), and adapted to semantic frame induction using predicate-argument pairs (Jauhar and Hovy, 2017). The method of Thelen and Riloff (2002) is similar to ours. They learn representations for six coarse supersenses using pattern-based bootstrapping based on a small list of seed words. The features used to learn senses are based on lexical patterns, syntactic co-occurrence, web queries, etc. (Qadir and Riloff, 2012). Instead of focusing on the features, our approach is more in line with current neural methods, with features learned from the data jointly with the supersense classifiers. 3 Contextual and Lexical Signatures The heart of SLICE consists in a series of binary classifiers, one per s"
2020.coling-main.298,P19-1356,0,0.215452,"c embedding per lexical unit, contextual This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3357 Proceedings of the 28th International Conference on Computational Linguistics, pages 3357–3370 Barcelona, Spain (Online), December 8-13, 2020 models provide a fine-grained distinct representation for each occurrence. Both models, but especially the latter, are increasingly complex and opaque (Rogers et al., 2020), requiring advanced techniques to help humans understand their strengths and limitations (Jawahar et al., 2019; Serrano and Smith, 2019). Given this landscape, we introduce SLICE: an alternative semantic model which constitutes a trade-off between static, interpretable symbolic senses and contextual word embeddings. We propose a weakly supervised technique to build dense low-dimensional embeddings whose dimensions represent coarsegrained semantic classes i.e., supersenses such as ANIMATE ENTITY and NATURAL OBJECT (Sec. 3). Our lightweight model embeds both lexical units and their contexts into the same semantic space. Thus, words and their contexts are represented as two compact vectors of directly in"
2020.coling-main.298,2020.lrec-1.302,0,0.0592296,"Missing"
2020.coling-main.298,2020.acl-main.423,0,0.16561,"dure (Neelakantan et al., 2014), or with word sense induction methods based on clustering, e.g., Panchenko et al. (2017). For interpretability, resources such as Wordnet can be used to semantically enhance static embeddings (Faruqui et al., 2015) or to learn representations for Wordnet synsets (Rothe and Sch¨utze, 2015), supersenses (Flekova and Gurevych, 2016), or Babelnet senses (Camacho-Collados et al., 2016). Contextual models such as BERT can be enriched with supersenses, predicted jointly with masked words during training, with observed improvements in tasks requiring lexical semantics (Levine et al., 2020). Interpretable semantic representations One of the most popular sense inventories in NLP is Wordnet (Miller et al., 1990), in which words are grouped into synsets and linked to each other via lexical-semantic relations (e.g., hypernymy, synonymy). For many years, the English Wordnet has been the basis of 3358 sense-annotated corpora (Landes et al., 1998) and WSD research (Navigli, 2009). Babelnet (Navigli and Ponzetto, 2012) is a semi-automatic multilingual lexicon similar to Wordnet and also quite popular for performing WSD in languages other than English (Moro et al., 2014). Supervised WSD"
2020.coling-main.298,P98-2127,0,0.339672,"lar, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al., 2018), such as a threeway classification of adjectives (Boleda et al., 2012) or animate vs. inanimate nouns (Øvrelid, 2006). We understand supersenses as general coarse semantic distinctions. Our set of six semantic labels is related to Wordnet supersenses, but there is not a 1:1 relation between our supersenses and Wordnet’s ones. Weakly supervised semantic classification Many models have been proposed to induce lexical semantics from raw corpora without supervision, e.g., (Lin, 1998), usually performing unsupervised WSD as a by-product. Most methods rely on distributional clustering algorithms, e.g., (Biemann and Riedl, 2013). While automatically induced word senses are hard to interpret, they may be automatically labeled, for example, using hypernym-induction patterns (Ustalov et al., 2019). There have been several proposals to integrate interpretable representations such as supersenses with continuous (unsupervised) representations, but they often rely on annotated corpora, such as Semcor (Flekova and Gurevych, 2016) or sense inventories such as Wordnet (Levine et al.,"
2020.coling-main.298,Q14-1019,0,0.0404694,"cal semantics (Levine et al., 2020). Interpretable semantic representations One of the most popular sense inventories in NLP is Wordnet (Miller et al., 1990), in which words are grouped into synsets and linked to each other via lexical-semantic relations (e.g., hypernymy, synonymy). For many years, the English Wordnet has been the basis of 3358 sense-annotated corpora (Landes et al., 1998) and WSD research (Navigli, 2009). Babelnet (Navigli and Ponzetto, 2012) is a semi-automatic multilingual lexicon similar to Wordnet and also quite popular for performing WSD in languages other than English (Moro et al., 2014). Supervised WSD relies on sense-annotated corpora specifying which of the senses in the inventory are employed in context (Pasini and Camacho-Collados, 2020), e.g., SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces"
2020.coling-main.298,D14-1113,0,0.0358788,"t senses and are modelled with a custom embeddings. On the downside, they are computationally heavy and opaque (Rogers et al., 2020), requiring sophisticated techniques such as probing to interpret predictions (Jawahar et al., 2019). Particularly relevant to our work are sense embeddings (Camacho-Collados and Pilehvar, 2018), in which a lexical unit is associated to several vectors (as in contextual models), but with some generalisation across occurrences (as in static models). Unsupervised sense (multi-prototype) embeddings can be obtained by adapting the objective of the learning procedure (Neelakantan et al., 2014), or with word sense induction methods based on clustering, e.g., Panchenko et al. (2017). For interpretability, resources such as Wordnet can be used to semantically enhance static embeddings (Faruqui et al., 2015) or to learn representations for Wordnet synsets (Rothe and Sch¨utze, 2015), supersenses (Flekova and Gurevych, 2016), or Babelnet senses (Camacho-Collados et al., 2016). Contextual models such as BERT can be enriched with supersenses, predicted jointly with masked words during training, with observed improvements in tasks requiring lexical semantics (Levine et al., 2020). Interpret"
2020.coling-main.298,L16-1262,0,0.0179717,"Missing"
2020.coling-main.298,E06-3008,0,0.0699912,"tive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al., 2018), such as a threeway classification of adjectives (Boleda et al., 2012) or animate vs. inanimate nouns (Øvrelid, 2006). We understand supersenses as general coarse semantic distinctions. Our set of six semantic labels is related to Wordnet supersenses, but there is not a 1:1 relation between our supersenses and Wordnet’s ones. Weakly supervised semantic classification Many models have been proposed to induce lexical semantics from raw corpora without supervision, e.g., (Lin, 1998), usually performing unsupervised WSD as a by-product. Most methods rely on distributional clustering algorithms, e.g., (Biemann and Riedl, 2013). While automatically induced word senses are hard to interpret, they may be automatical"
2020.coling-main.298,E17-1009,0,0.0177933,"ly heavy and opaque (Rogers et al., 2020), requiring sophisticated techniques such as probing to interpret predictions (Jawahar et al., 2019). Particularly relevant to our work are sense embeddings (Camacho-Collados and Pilehvar, 2018), in which a lexical unit is associated to several vectors (as in contextual models), but with some generalisation across occurrences (as in static models). Unsupervised sense (multi-prototype) embeddings can be obtained by adapting the objective of the learning procedure (Neelakantan et al., 2014), or with word sense induction methods based on clustering, e.g., Panchenko et al. (2017). For interpretability, resources such as Wordnet can be used to semantically enhance static embeddings (Faruqui et al., 2015) or to learn representations for Wordnet synsets (Rothe and Sch¨utze, 2015), supersenses (Flekova and Gurevych, 2016), or Babelnet senses (Camacho-Collados et al., 2016). Contextual models such as BERT can be enriched with supersenses, predicted jointly with masked words during training, with observed improvements in tasks requiring lexical semantics (Levine et al., 2020). Interpretable semantic representations One of the most popular sense inventories in NLP is Wordnet"
2020.coling-main.298,2020.lrec-1.706,0,0.019867,"r et al., 1990), in which words are grouped into synsets and linked to each other via lexical-semantic relations (e.g., hypernymy, synonymy). For many years, the English Wordnet has been the basis of 3358 sense-annotated corpora (Landes et al., 1998) and WSD research (Navigli, 2009). Babelnet (Navigli and Ponzetto, 2012) is a semi-automatic multilingual lexicon similar to Wordnet and also quite popular for performing WSD in languages other than English (Moro et al., 2014). Supervised WSD relies on sense-annotated corpora specifying which of the senses in the inventory are employed in context (Pasini and Camacho-Collados, 2020), e.g., SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency pa"
2020.coling-main.298,D14-1162,0,0.0834123,"e being constantly improved, the main principle is stable across models: vectors represent a word’s usage (and meaning) based on its distributional context (Harris, 1954). Embeddings have become commonplace in NLP, as they naturally represent input (words) in state-of-the-art neural models. Although they can be randomly initialised and learned, unsupervised pre-training on raw corpora is common (Turian et al., 2010). Embeddings can be pre-trained as by-products of predictive neural language models (Mikolov et al., 2013), by factorisation of the co-occurrence matrix (Landauer and Dumais, 1997; Pennington et al., 2014), etc. Sub-lexical units (character n-grams) address linguistic variability, e.g., due to rich morphology, non-standard text, and out-of-vocabulary forms (Bojanowski et al., 2017). Most of the models prior to 2018 are static, assuming a single vector per word. These models suffer from meaning conflation, i.e., a single vector is created for ambiguous units, ignoring polysemous and multi-facet words. Advances in neural networks triggered the development of contextual embeddings, with representations conditioned on the surrounding words. They can be obtained using stacked recurrent layers as in"
2020.coling-main.298,N18-1202,0,0.0419497,"Sub-lexical units (character n-grams) address linguistic variability, e.g., due to rich morphology, non-standard text, and out-of-vocabulary forms (Bojanowski et al., 2017). Most of the models prior to 2018 are static, assuming a single vector per word. These models suffer from meaning conflation, i.e., a single vector is created for ambiguous units, ignoring polysemous and multi-facet words. Advances in neural networks triggered the development of contextual embeddings, with representations conditioned on the surrounding words. They can be obtained using stacked recurrent layers as in ELMo (Peters et al., 2018), or attention-based transformers as in BERT (Devlin et al., 2018) and GPT-2 (Radford et al., 2018). In addition to their outstanding performances, these models address meaning conflation: contexts correspond to (slightly) different senses and are modelled with a custom embeddings. On the downside, they are computationally heavy and opaque (Rogers et al., 2020), requiring sophisticated techniques such as probing to interpret predictions (Jawahar et al., 2019). Particularly relevant to our work are sense embeddings (Camacho-Collados and Pilehvar, 2018), in which a lexical unit is associated to"
2020.coling-main.298,S12-1028,0,0.0279669,"data, which in turn enable training classifiers for inducing lexical knowledge. This has been proposed in several studies, e.g., Mihalcea (2003), especially for polysemy pattern detection (Boleda et al., 2012), and adapted to semantic frame induction using predicate-argument pairs (Jauhar and Hovy, 2017). The method of Thelen and Riloff (2002) is similar to ours. They learn representations for six coarse supersenses using pattern-based bootstrapping based on a small list of seed words. The features used to learn senses are based on lexical patterns, syntactic co-occurrence, web queries, etc. (Qadir and Riloff, 2012). Instead of focusing on the features, our approach is more in line with current neural methods, with features learned from the data jointly with the supersense classifiers. 3 Contextual and Lexical Signatures The heart of SLICE consists in a series of binary classifiers, one per supersense. Each classifier takes as input a context C and produces a score that indicates how likely C could be associated to a given supersense si . This score, noted csi (C), is called a context score. A context C can be associated to a d-dimensional vector, called its signature CS(C) = (cs1 (C), . . . , csd (C))T"
2020.coling-main.298,2020.tacl-1.54,0,0.0458401,"each context corresponds to a different sense (Yarowsky, 1993). In short, while static models create one generic embedding per lexical unit, contextual This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3357 Proceedings of the 28th International Conference on Computational Linguistics, pages 3357–3370 Barcelona, Spain (Online), December 8-13, 2020 models provide a fine-grained distinct representation for each occurrence. Both models, but especially the latter, are increasingly complex and opaque (Rogers et al., 2020), requiring advanced techniques to help humans understand their strengths and limitations (Jawahar et al., 2019; Serrano and Smith, 2019). Given this landscape, we introduce SLICE: an alternative semantic model which constitutes a trade-off between static, interpretable symbolic senses and contextual word embeddings. We propose a weakly supervised technique to build dense low-dimensional embeddings whose dimensions represent coarsegrained semantic classes i.e., supersenses such as ANIMATE ENTITY and NATURAL OBJECT (Sec. 3). Our lightweight model embeds both lexical units and their contexts int"
2020.coling-main.298,P15-1173,0,0.051392,"Missing"
2020.coling-main.298,S16-1084,0,0.0224397,"ther than English (Moro et al., 2014). Supervised WSD relies on sense-annotated corpora specifying which of the senses in the inventory are employed in context (Pasini and Camacho-Collados, 2020), e.g., SemCor for English Wordnet (Landes et al., 1998) and Eurosense for Babelnet (Delli Bovi et al., 2017). The fine granularity of sense inventories is often criticised as unrealistic (Navigli, 2009). One alternative is to represent senses using top-level synsets in Wordnet’s taxonomy (e.g., A NIMAL, E VENT), referred to as supersenses, reached via hypernymy relations (Ciaramita and Johnson, 2003; Schneider et al., 2016). This reduces the number of labels at the expense of missing potentially relevant distinctions, often with positive impact on downstream applications such as dependency parsing (Agirre et al., 2011) and personality profiling (Flekova and Gurevych, 2015).1 In our evaluation, we employ the FrSemCor corpus, a French corpus in which nouns are annotated using Wordnet supersenses as semantic tags (Barque et al., 2020). Although the set of 25 Wordnet top-level categories is quite popular, alternative representations with even coarser granularity can be useful for downstream applications (Jahan et al"
2020.coling-main.298,P19-1282,0,0.0279069,"l unit, contextual This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 3357 Proceedings of the 28th International Conference on Computational Linguistics, pages 3357–3370 Barcelona, Spain (Online), December 8-13, 2020 models provide a fine-grained distinct representation for each occurrence. Both models, but especially the latter, are increasingly complex and opaque (Rogers et al., 2020), requiring advanced techniques to help humans understand their strengths and limitations (Jawahar et al., 2019; Serrano and Smith, 2019). Given this landscape, we introduce SLICE: an alternative semantic model which constitutes a trade-off between static, interpretable symbolic senses and contextual word embeddings. We propose a weakly supervised technique to build dense low-dimensional embeddings whose dimensions represent coarsegrained semantic classes i.e., supersenses such as ANIMATE ENTITY and NATURAL OBJECT (Sec. 3). Our lightweight model embeds both lexical units and their contexts into the same semantic space. Thus, words and their contexts are represented as two compact vectors of directly interpretable scores, one pe"
2020.coling-main.298,W02-1028,0,0.32498,"nd Gurevych, 2016) or sense inventories such as Wordnet (Levine et al., 2020). Our embedding learning procedure is not fully unsupervised, but uses weak supervision to bootstrap semantic classes from corpora. Typical or non-ambiguous words can be used to produce sense-annotated data, which in turn enable training classifiers for inducing lexical knowledge. This has been proposed in several studies, e.g., Mihalcea (2003), especially for polysemy pattern detection (Boleda et al., 2012), and adapted to semantic frame induction using predicate-argument pairs (Jauhar and Hovy, 2017). The method of Thelen and Riloff (2002) is similar to ours. They learn representations for six coarse supersenses using pattern-based bootstrapping based on a small list of seed words. The features used to learn senses are based on lexical patterns, syntactic co-occurrence, web queries, etc. (Qadir and Riloff, 2012). Instead of focusing on the features, our approach is more in line with current neural methods, with features learned from the data jointly with the supersense classifiers. 3 Contextual and Lexical Signatures The heart of SLICE consists in a series of binary classifiers, one per supersense. Each classifier takes as inpu"
2020.coling-main.298,P10-1040,0,0.110797,"entations is enormous, ranging from traditional models such as LSA (Landauer and Dumais, 1997) to sophisticated deep contextualised embeddings such as BERT (Devlin et al., 2018). Although techniques are being constantly improved, the main principle is stable across models: vectors represent a word’s usage (and meaning) based on its distributional context (Harris, 1954). Embeddings have become commonplace in NLP, as they naturally represent input (words) in state-of-the-art neural models. Although they can be randomly initialised and learned, unsupervised pre-training on raw corpora is common (Turian et al., 2010). Embeddings can be pre-trained as by-products of predictive neural language models (Mikolov et al., 2013), by factorisation of the co-occurrence matrix (Landauer and Dumais, 1997; Pennington et al., 2014), etc. Sub-lexical units (character n-grams) address linguistic variability, e.g., due to rich morphology, non-standard text, and out-of-vocabulary forms (Bojanowski et al., 2017). Most of the models prior to 2018 are static, assuming a single vector per word. These models suffer from meaning conflation, i.e., a single vector is created for ambiguous units, ignoring polysemous and multi-facet"
2020.coling-main.298,J19-3002,0,0.0113555,"nctions. Our set of six semantic labels is related to Wordnet supersenses, but there is not a 1:1 relation between our supersenses and Wordnet’s ones. Weakly supervised semantic classification Many models have been proposed to induce lexical semantics from raw corpora without supervision, e.g., (Lin, 1998), usually performing unsupervised WSD as a by-product. Most methods rely on distributional clustering algorithms, e.g., (Biemann and Riedl, 2013). While automatically induced word senses are hard to interpret, they may be automatically labeled, for example, using hypernym-induction patterns (Ustalov et al., 2019). There have been several proposals to integrate interpretable representations such as supersenses with continuous (unsupervised) representations, but they often rely on annotated corpora, such as Semcor (Flekova and Gurevych, 2016) or sense inventories such as Wordnet (Levine et al., 2020). Our embedding learning procedure is not fully unsupervised, but uses weak supervision to bootstrap semantic classes from corpora. Typical or non-ambiguous words can be used to produce sense-annotated data, which in turn enable training classifiers for inducing lexical knowledge. This has been proposed in s"
2020.multilingualbio-1.4,P84-1044,0,0.319114,"Missing"
2020.multilingualbio-1.4,P02-1040,0,0.122226,"max Mm bl (e, [e1 , ..., en ], l) = mean J max MM bl (e, [e1 , ..., en ], l) = max J max Definition of Evaluation Metrics 3.4.1. Coverage Metric To estimate the coverage of Wikipedia on a biomedical ontology we use the following metric: |{e ∈ E1 |Ll (e) 6= ∅}| |{e0 ∈ E2 |Ll (e0 ) 6= ∅}| where E1 and E2 are sets of entities. 3.4.2. Jaro Similarity and n-ary Jaro In order to evaluate the quality of the translations, we follow Silva et al. (2015) choosing the Jaro similarity, which is a type of edit distance. We made this choice as we are looking at entities. Whereas other measures such as BLEU (Papineni et al., 2002) are widely used for translation tasks, they have been designed for full sentences instead of relatively short ontology labels. The Jaro Similarity is defined as: 1 J(s, s ) = 3  m m m−t + 0 + |s ||s | m  s, s0 ∈ {a, ..., z}∗ with s and s0 two strings, |s |the length of s, t is half the number of transpositions, m the number of matching characters. Two characters from s and s0 are matching if 0 |) they are the same and not further than max(|s|,|s − 1. The 2 Jaro Similarity ranges between 0 and 1, where the score is 1 when the two strings are the same. However, since one Orphanet entity may h"
2020.multilingualbio-1.4,W14-6201,0,0.111783,"second-order links and comparing this quality with the quality obtained by machine translation tools. This paper is part of a long-term project whose goal is to work on multilingual disease extraction from news with strategies based on dictionary expansion. Consequently, we need a multilingual vocabulary with diseases which are normalized with respect to an ontology. Thus, we focus on one kind of biomedical ontologies, that is, ontologies about diseases. 2. Resources and Related Work There has already been some work trying to use opensource knowledge bases to translate biomedical ontologies. Bretschneider et al. (2014) obtain a German-English medical dictionary using DBPedia. The goal is to perform information extraction from a German biomedical corpus. They could not directly use the RadLex ontology (Langlotz, 2006) as it is only available in English. So, they first extract term candidates in their German corpus. Then, they try to match the candidates with the pairs in their GermanEnglish dictionary. If a candidate is in the dictionary, they 21 Figure 1: Example of first-order link (left) and second-order link (right) use the translation to match with the RadLex ontology. Finally, this term candidate along"
2020.mwe-1.14,calzolari-etal-2002-towards,0,0.0556036,"ary discovery methods. We released annotated and raw corpora in 14 languages, and this semi-supervised challenge attracted 7 teams who submitted 9 system results. This paper describes the effort of corpus creation, the task design, and the results obtained by the participating systems, especially their performance on unseen expressions. 1 Introduction Multiword expressions (MWEs) such as to throw someone under the bus ‘to cause one’s suffering to gain personal advantage’ are idiosyncratic word combinations which need to be identiﬁed prior to further semantic processing (Baldwin and Kim, 2010; Calzolari et al., 2002). The task of MWE identiﬁcation, that is, automatically locating instances of MWEs in running text (Constant et al., 2017) has received growing attention in the last 4 years. Progress on this task was especially motivated by shared tasks such as DiMSUM (Schneider et al., 2016), and two editions of the PARSEME shared tasks, edition 1.0 in 2017 (Savary et al., 2017), and edition 1.1 in 2018 (Ramisch et al., 2018). Previous editions of the PARSEME shared task focused on the identiﬁcation of verbal MWEs (VMWEs), because of their challenging traits: complex structure, discontinuities, variability,"
2020.mwe-1.14,J17-4005,1,0.888334,"Missing"
2020.mwe-1.14,W17-4418,0,0.065602,"Missing"
2020.mwe-1.16,J17-4005,1,0.866711,"Missing"
2020.mwe-1.16,2020.coling-main.296,1,0.778798,"commons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1-3367 2 VMWEs are represented as multisets (i.e. bags of elements with repetition allowed), since the same lemma and/or POS can occur twice, as in appeler un chat un chat ‘to call a cat a cat’⇒‘to call a spade a spade’. 124 Joint Workshop on Multiword Expressions and Electronic Lexicons, pages 124–129 Barcelona, Spain (Online), December 13, 2020. Seen2Seen in a nutshell Seen2Seen is a VMWE identification system dedicated to only those VMWEs which have been previously seen in the training data. Its detailed description is provided in Pasquer et al. (2020), but a brief overview is included here to make the current paper self-contained. Seen2Seen extracts lemma combinations of VMWEs seen in Train, looking for the same combinations (within one sentence) in Test, with an expected high recall. To improve precision, up to eight independent criteria can be used: (1) component lemmas should be disambiguated by their POS, (2) components should appear in specific orders (e.g. the determiner before the noun), (3) the order of “gap” words possibly occurring between components is also considered, (4) components should not be too far from each other in a se"
2020.mwe-1.16,W19-5110,1,0.598673,"seen F-score (i.e. not only for verb-noun constructions) was obtained for Hindi (42.66) and it reached 25.36 in French, which was our main focus. Despite the lower global MWE-based F1score of Seen2Unseen (63.02) compared to Seen2Seen (66.23), we describe the former (Sec. 2), analyse its interesting negative results (Sec. 3), and conclude with ideas for future work (Sec. 4). 2 System Description While describing the architecture of our system, we use the notions of a VMWE token (its occurrence in running text) and a VMWE type (abstraction over all occurrences of a given VMWE), as introduced by Savary et al. (2019b). We represent VMWE types as multisets of lemmas and POS.2 Our system uses a mixture of discovery and identification methods, as defined by Constant et al. (2017). Namely, VMWE discovery consists in generating lists of MWE types out of context, while VMWE identification marks VMWE tokens in running text. The system is freely available online (https://gitlab.com/ cpasquer/st_2020). This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1-3367 2 VMWEs are represented as m"
2021.case-1.21,K19-1063,0,0.0188642,"o be minimized whereas F1 is a score that we would like to maximize, the final loss is 1 − F 1. 3.2.2 Another recommendation to work with Transformers-based models and small data made by Mosbach et al. (2021) is to use smaller learning rates but compensating with more epochs. We have taken this into account during the hyper-parameter search. Ruder (2021) recommend using behavioral finetuning to reduce fine-tuning instabilities. It is supposed to be especially helpful to have a better initialization of the final prediction layer. It has also already been used on named entity recognition tasks (Broscheit, 2019) and has shown that it has improved results for a task with a very small training dataset. Thus, to do so, we need a task with the same number of classes, but much larger training datasets. As we did not find such a task, we decided to fine-tune our model on at least the different languages we are working with, English, Spanish and Portuguese. We used named entity recognition datasets and kept only three classes in common in all the datasets: person, organization, and location. These three types of entities can be found in the shared task. To perform this test, the training has been done like"
2021.case-1.21,N19-1423,0,0.195325,", task 1 is composed of the four aforementioned tasks and adds another difficulty: multilinguality. This year’s data is available in English, Spanish, and Portuguese. Thus, it is important to note that there is much more data in English than in the other languages. For the document classification sub-task, to test multilingual capabilities, Hindi is available on the testing set only. We have mainly focused on the last sub-task (event information extraction), but we have also submitted results for the first and second sub-tasks (document and sentence classification). We used multilingual BERT (Devlin et al., 2019), henceforth M-BERT, which is a model known to obtain near state-of-the-art results on many tasks. It is also supposed to work well for zero-or-few-shot learning on different languages (Pires et al., 2019). We will see the results on these sub-tasks, especially for sub-task 4 where the training set available for Spanish and Portuguese is small. Thus, one of the issues with transformer-based models such as M-BERT is the instability on small datasets (Dodge et al., 2020; Ruder, 2021). The instability issue is the fact that by changing some random seeds before the learning phase but using the sam"
2021.case-1.21,D19-5709,0,0.0541724,"protested, attack, ...), Thus, not all the snippets contain all the classes, and they can contain several times the same classes. Each information can be composed of one or several adjacent words. Each snippet contains information related to one and only one event. As the data is already separated into groups of sentences related to the same event, our approach consists of considering a task of named entity recognition with the aforementioned classes. Multilingual BERT has already been used for multilingual named entity recognition and showed great results compared to state-of-the-art models (Hakala and Pyysalo, 2019). 3 162 3.1 System overview Sub task 1 and 2 For sub-tasks 1 and 2, we approach these tasks as binary sequence classification, as the goal is to predict whether or not a document (sub-task 1) or sentence (sub-task 2) contains relevant information about a protest event. Thus the size of the output of the dense layer is 2. We then perform an argmax on these values to predict a class. We use the base parameters in HuggingFace’s ’transformers’ library. The loss is a cross-entropy, the learning rate is handled by an AdamW optimizer (Loshchilov and Hutter, 2019) and the activation function is a gelu"
2021.case-1.21,2021.case-1.11,0,0.0833654,"Missing"
2021.case-1.21,2021.case-1.1,0,0.079858,"Missing"
2021.case-1.21,2020.emnlp-demos.6,0,0.095261,"Missing"
2021.case-1.21,P19-1493,0,0.0708321,"e is much more data in English than in the other languages. For the document classification sub-task, to test multilingual capabilities, Hindi is available on the testing set only. We have mainly focused on the last sub-task (event information extraction), but we have also submitted results for the first and second sub-tasks (document and sentence classification). We used multilingual BERT (Devlin et al., 2019), henceforth M-BERT, which is a model known to obtain near state-of-the-art results on many tasks. It is also supposed to work well for zero-or-few-shot learning on different languages (Pires et al., 2019). We will see the results on these sub-tasks, especially for sub-task 4 where the training set available for Spanish and Portuguese is small. Thus, one of the issues with transformer-based models such as M-BERT is the instability on small datasets (Dodge et al., 2020; Ruder, 2021). The instability issue is the fact that by changing some random seeds before the learning phase but using the same architecture, data and hyper-parameters the results can have a great variance. We will look at some solutions to mitigate this issue, and how this issue is impacting our results for sub-task 4.1 2 Tasks"
2021.case-1.21,santos-etal-2006-harem,0,0.0344297,"Missing"
2021.case-1.21,W02-2024,0,0.0933341,"Missing"
C10-2120,1999.tc-1.8,0,0.307592,"Missing"
C10-2120,J03-3005,0,0.0982174,"Missing"
C10-2120,J03-3001,0,0.204183,"Missing"
C10-2120,2005.mtsummit-papers.11,0,0.0656565,"Missing"
C10-2120,W06-1208,0,0.260743,"Missing"
C10-2120,ramisch-etal-2010-mwetoolkit,1,0.585795,"ate techniques for the combination of n-gram counts from heterogeneous sources. Therefore, we will use the insights about the vocabulary differences presented in the previous section. In this evaluation, we measure the impact of the suggested techniques in the identification of noun–noun compounds in corpora. Noun compounds are very frequent in general-purpose and specialised texts (e.g. bus stop, European Union and gene activation). We extract them automatically from ep and from genia using a standard method based on POS patterns and association measures (Evert and Krenn, 2005; Pecina, 2008; Ramisch et al., 2010). 5.1 Experimental setup The evaluation task consists of, given a corpus of N words, extract all occurrences of adjacent pairs of nouns8 and then rank them using a standard statistical measure that estimates the association strength between the two nouns. Analogously to the formalism adopted in section 4.2, we assume that, for each corpus, we generate a set NN containing n-grams v1...n ∈ NN 9 for which we obtain n-gram counts from four sources. The elements in NN are generated by comparing the POS pattern noun–noun against all the bigrams in the corpus and keeping only those pairs of adjacent"
C10-2120,D07-1110,1,0.9292,"Missing"
C10-2120,J01-1001,0,0.12675,"percase words at the beginning of sentences were lowercased; • other words were not modified. 3 Genia contains manual POS tag annotation. Europarl was tagged using the TreeTagger (www.ims.uni-stuttgart.de/projekte/ corplex/TreeTagger). 1043 This lowercasing algorithm helps to deal with the massive use of abbreviations, acronyms, named entities, and formulae found in specialised corpora, such as those containing biomedical (and other specialised) scientific articles. For calculating arbitrary-sized n-grams in large textual corpora efficiently, we implemented a structure based on suffix arrays (Yamamoto and Church, 2001). While suffix trees are often used in LM tools, where n-grams have a fixed size, they are not fit for arbitrary length n-gram searches and can consume quite large amounts of memory to store all the node pointers. Suffix arrays, on the other hand, allow for arbitrary length n-grams to be counted in a time that is proportional to log(N), where N is the number of words (which is equivalent to the number of suffixes) in the corpus. Suffix arrays use a constant amount of memory proportional to N. In our implementation, where every word and every word position in the corpus are encoded as a 4-byte"
C10-3015,ramisch-etal-2010-mwetoolkit,1,0.700251,"rds can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pages 57–60, Beijing, August 2010 c(w1 . . . wn ) N n × c(w1 . . . wn ) dice = ∑ni=1 c(wi ) c(w1 . . . wn ) pmi = log2 E(w1 . . . wn ) c(w1 . . . wn ) − E(w1 . . . wn ) p t-score = c(w1 . . . wn ) mle candidate = fEP fgoogle class status quo 137 US navy 4 International Cooperation 2 Cooperation Agreement 188 Panama Canal 2 security institution 5 lending institution 4 human ri"
C10-3015,C10-2120,1,0.605893,"rds can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pages 57–60, Beijing, August 2010 c(w1 . . . wn ) N n × c(w1 . . . wn ) dice = ∑ni=1 c(wi ) c(w1 . . . wn ) pmi = log2 E(w1 . . . wn ) c(w1 . . . wn ) − E(w1 . . . wn ) p t-score = c(w1 . . . wn ) mle candidate = fEP fgoogle class status quo 137 US navy 4 International Cooperation 2 Cooperation Agreement 188 Panama Canal 2 security institution 5 lending institution 4 human ri"
C10-3015,J93-1007,0,0.652141,"t of selected examples, comparing it with related work on MWE extraction. 1 MWEs in a nutshell One of the factors that makes Natural Language Processing (NLP) a challenging area is the fact that some linguistic phenomena are not entirely compositional or predictable. For instance, why do we prefer to say full moon instead of total moon or entire moon if all these words can be considered synonyms to transmit the idea of completeness? This is an example of a collocation, i.e. a sequence of words that tend to occur together and whose interpretation generally crosses the boundaries between words (Smadja, 1993). More generally, collocations are a frequent type of multiword expression (MWE), a sequence of words that presents some lexical, syntactic, semantic, pragmatic or statistical idiosyncrasies (Sag et al., 2002). The definition of MWE also includes a wide range of constructions like phrasal verbs (go 2 1 The first version of the toolkit was presented in (Ramisch et al., 2010b), where we described a language- and type-independent methodology. Inside the black box MWE identification is composed of two phases: first, we automatically generate a list of candi57 Coling 2010: Demonstration Volume, pag"
C18-1219,F12-2024,0,0.0517206,"inflection of the noun. Still, modification of the noun and passivization is exhibited by the former (8)–(9) but not by the latter (20). 4 Corpus We study VMWE variability on a corpus of French texts annotated with VMWEs for the PARSEME shared task.3 In addition to VMWE annotation (based on universal guidelines), the corpus contains POS, lemmas, morphological features and dependency structures, which we use in our experiments. The original release of the VMWE-annotated corpus contained two sub-corpora: the French part of the Universal Dependencies v1.4 corpus (Nivre et al., 2016) and Sequoia (Candito and Seddah, 2012). 3 http://multiword.sf.net/sharedtask2017, corpus at http://hdl.handle.net/11372/LRT-2282 2585 Corpus TrC TeC # Sentences 17,880 1,667 All POS patterns # Tokens # VMWEs 450,221 1,584 35,784 291 # occ. 4,462 500 All Verb-(Det-)Noun # VMWEs # occ. 854 2098 177 283 Verb-(Det-)Noun variants # VMWEs # occ. n/a n/a 86 132 Table 1: Number of different VMWE types (# VMWEs) and their token occurrences (# occ.) per POS and variability class in the training corpus (TrC) and in the test corpus (TeC). The tagsets and dependency trees used in both sub-corpora were incompatible. We homogenized them by repla"
C18-1219,P16-1016,0,0.0276659,"Missing"
C18-1219,J17-4005,1,0.755623,"Missing"
C18-1219,J09-1005,0,0.676032,"ocessors combined with rich finite-state patterns (Krstev et al., 2014) or unification meta-grammars (Jacquemin, 2001). Rule-based methods were often combined with statistical measures in the domain of multiword term extraction (Savary and Jacquemin, 2003), and explicitly addressed variation. But they can hardly distinguish literal from idiomatic readings, weakly cover discontinuous MWEs, and cannot generalize to MWEs absent from the lexicon and to new, initially uncovered, variation patterns. Sense disambiguation methods often focus on ambiguous known MWEs and neglect variant identification (Fazly et al., 2009). Sequence taggers learn identification models from annotated data based on regularities in sequences of tokens (Constant et al., 2013; Schneider et al., 2014). They are well suited for continuous seen MWEs and neutralize inflection when lemmas are available. They cope, however, badly with syntactic variation whenever it leads to discontinuities. MWE-aware parsers identify MWEs as a by-product of parsing a sentence (Green et al., 2013; Constant and Nivre, 2016). They often can deal with both morphological and syntactic variants, even though they are usually less accurate for highly discontinuo"
C18-1219,N15-1065,0,0.0281204,"ure, POS tags and lemmas with at least one annotated MWE in the training set. They find out that most systems perform better when the proportion of seen MWEs is high. This result suggests that MWE variant identification remains a hard problem and deserves being addressed explicitly by dedicated methods. 3 VMWE variation in French VMWEs are known to have specific lexical and morpho-syntactic variability profiles: they are more or less variable, but usually not as variable as regular phrases with the same syntactic structure. Therefore, methods used for detecting paraphrases of regular phrases (Fujita and Isabelle, 2015) cannot be straightforwardly applied to VMWEs. Different aspects of variability may be considered. Firstly, MWE-hood is, by nature, a lexical phenomenon, that is, a particular idiomatic reading is available only in presence of a combination of particular lexical units. Replacing one of them by a semantically close lexeme usually leads to the loss of idiomatic reading, e.g. (6) is an idiom but (7) can 2584 only be understood literally. Lexical variability is admitted by some VMWEs, but usually with a very restricted list of equivalents only, as in (11) and (12). In this work, unlike Fazly et al"
C18-1219,J13-1009,0,0.0728331,"Missing"
C18-1219,W17-1715,0,0.051104,"Missing"
C18-1219,L16-1262,0,0.0229078,"Missing"
C18-1219,N18-2068,1,0.909561,"both the literal and the idiomatic meaning can sometimes be preserved under syntactic variation as in (6) vs. (9). Secondly, some types of syntactic variation (e.g. passivization) tend to be exhibited less frequently by VMWEs than by non-VMWEs of the same syntactic structures (Fazly et al., 2009). Thirdly, some types of syntactic dependencies can be specific to some VMWEs, e.g. (18) involves a compulsory though non-lexicalized adjectival modifier of the noun. This also means that this VMWE admits insertions of external elements between its lexicalized components. Finally, as previously shown (Pasquer et al., 2018), syntactic features are particularly strong indicators for linguistically motivated similarity and flexibility measures of (French) VWMEs. The main challenge in modeling the variability profiles of VMWEs lies in the fact that VMWEs of the same syntactic structure may behave differently. For instance, the VMWEs in (6) and (19) both admit interrogation, insertions and inflection of the verb, and prohibit inflection of the noun. Still, modification of the noun and passivization is exhibited by the former (8)–(9) but not by the latter (20). 4 Corpus We study VMWE variability on a corpus of French"
C18-1219,Q14-1016,0,0.0776456,"Missing"
C18-1219,S16-1084,0,0.0159246,"f words exhibiting unexpected lexical, morphological, syntactic, semantic, pragmatic and/or statistical behavior (Baldwin and Kim, 2010). Most prominently, they are semantically non-compositional, that is, their meaning cannot be deduced from the meanings of their components and from their syntactic structure in a way deemed regular. For this reason, the presence of MWEs in texts calls for dedicated treatment, whose prerequisites include their automatic identification. The goal of MWE identification is, given some input running text, to identify all MWEs’ lexicalized components present in it (Schneider et al., 2016; Savary et al., 2017).1 Such systems face three main This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 A lexicalized component of a MWE, or component for short – marked in bold in examples – is one which is always realized by the same lexeme. For instance, the noun decision is lexicalized in to make a decision but the determiner a is not, because it can be replaced by other determiners (e.g. to make this/any/the decision). 2582 Proceedings of the 27th International Conference on Computational L"
C18-1219,I13-1024,0,0.0610946,"Missing"
C18-1219,C16-1042,1,0.868377,"d 1.b) or false positives (Fig.1.d). False negatives include occurrences without direct connection like complex determiners (Fig.1.f). The baseline is also sensitive to syntactic annotation errors, as in Fig.1.e (rˆole ‘role’ rather than cl´e ‘key’ should be the head of rˆole cl´e ‘key role’). Despite its simplicity, the baseline is already very strong, as shown in the first line of Tab. 2. It extracts 158 Verb-(Det-)Noun candidates, which – compared to the 132 STVs to be extracted – yield the F-score of 0.88. These results notably confirm previous observations that literal readings are rare (Waszczuk et al., 2016; Savary and Cordeiro, 2018). obj obl acl acl:relcl obj (a) Il joue son propre rôle (b) Chacun a son rôle à jouer (c) Elle jouera dans le film où elle tiendra le rôle de Dorothy ::::: ::: He plays his own role Everyone has a role to play She will-play in the movie where she will-hold the role of Dorothy obj compound obl obj nmod (d) Comme pour son rôle dans Aïda , l’ acteur joue avec son poids (e) Le tact joua un rôle clé (f) Il joue un tas de rôles ::: :::: As with his role in Aida , the actor plays with his weight The tact played a role key He plays a pile of roles Figure 1: True positive (a"
D07-1110,W02-2001,1,0.844682,"Missing"
D07-1110,baldwin-etal-2004-road,0,0.0343885,"Missing"
D07-1110,A97-1052,0,0.0539433,"Missing"
D07-1110,C02-2025,0,0.0124496,"wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model predicted for each occurrence of the head words a most plausible lexical type in that context. Only those predictions that occurred 5 times or more were taken into consideration for the generation of the new lexical entries. As a result, we obtained 21 new lexical entries. These new lexical entries were later merged into the ERG lexicon. To evaluate the grammar performance with and without these new lexical entries, we 1. parsed the sub-corpus with/without new lexical entries and compared the grammar coverage; 2. inspected the parse"
D07-1110,pearce-2002-comparative,0,0.705957,"ring Aline Villavicencio♣♠ , Valia Kordoni♦ , Yi Zhang♦ , Marco Idiart♥ and Carlos Ramisch♣ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♠ Department of Computer Sciences, Bath University (UK) ♦ Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br Abstract Another difficulty for work on MWE identification is that of the evaluation of the results obtained (Pearce, 2002; Evert and Krenn, 2005), starting from the lack of consensus about a precise definition for MWEs (Villavicencio et al., 2005). This paper focuses on the evaluation of methods for the automatic acquisition of Multiword Expressions (MWEs) for robust grammar engineering. First we investigate the hypothesis that MWEs can be detected by the distinct statistical properties of their component words, regardless of their type, comparing 3 statistical measures: mutual information (MI), χ2 and permutation entropy (PE). Our overall conclusion is that at least two measures, MI and PE, seem to differentiat"
D07-1110,zhang-kordoni-2006-automated,1,0.854157,"candidates are used in 2 The combination of the “word with space” approach of Zhang et al. (2006) with the constructional approach we propose here is an interesting topic that we want to investigate in future research. 1040 this experiment. We used simple heuristics in order to extract the head words from these MWEs: • the n-grams are POS-tagged with an automatic tagger; • finite verbs in the n-grams are extracted as head words; • nouns are also extracted if there is no verb in the n-gram. Occasionally, the tagger errors might introduce wrong head words. However, the lexical type predictor of Zhang and Kordoni (2006) that we used in our experiments did not generate interesting new entries for them in the subsequent steps, and they were thus discarded, as discussed below. With the 30 MWE candidates, we extracted a sub-corpus from the BNC with 674 sentences which included at least one of these MWEs. The lexical acquisition technique described in Zhang and Kordoni (2006) was used with this subcorpus in order to acquire new lexical entries for the head words. The lexical acquisition model was trained with the Redwoods treebank (Oepen et al., 2002), following Zhang et al. (2006). The lexical prediction model p"
D07-1110,W06-1206,1,0.248015,"se corpora are probably compensated by their size. Finally, we show a qualitative evaluation of the results of automatically adding extracted MWEs to existing linguistic resources. We argue that such a process improves qualitatively, if a more compositional approach to grammar/lexicon automated extension is adopted. 1 Introduction The task of automatically identifying Multiword Expressions (MWEs) like phrasal verbs (break down) and compound nouns (coffee machine) using statistical measures has been the focus of considerable investigative effort, (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Given the heterogeneousness of the different phenomena that are considered to be MWEs, there is no consensus about which method is best suited for which type of MWE, and if there is a single method that can be successfully used for any kind of MWE. In this paper we investigate some of the issues involved in the evaluation of automatically extracted MWEs, from their extraction to their subsequent use in an NLP task. In order to do that, we present a discussion of different statistical measures, and the influence that the size and quality of different data sources have. We then perform a comp"
D07-1110,P04-1057,0,\N,Missing
D07-1110,W06-1208,0,\N,Missing
D14-1047,W02-0908,0,0.508083,"2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of cont"
D14-1047,P10-2017,0,0.0803912,"like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c October 25-29, 2014, Doha, Qatar. 2014"
D14-1047,ferret-2010-testing,0,0.071418,"an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the ta"
D14-1047,P13-1055,0,0.551824,"emantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measures (Lin, 1998; Curran and Moens, 2002; Ferret, 2010), identifying and demoting bad neighbors (Ferret, 2013), or using more relevant contexts (Broda et al., 2009; Biemann and Riedl, 2013). For the latter in particular, as words vary in their collocational tendencies, it is difficult to determine how informative a given context is. To remove uninformative and noisy contexts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an inf"
D14-1047,W05-0604,0,0.923337,"ts, filters have often been applied like pointwise mutual information (PMI), lexicographer’s mutual information (LMI) (Biemann and Riedl, 2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c Oct"
D14-1047,P98-2127,0,0.89512,"nclusions and discussion of future work. Much attention has been given to the impact of informativeness and similarity measures on distributional thesauri. We investigate the effects of context filters on thesaurus quality and propose the use of cooccurrence frequency as a simple and inexpensive criterion. For evaluation, we measure thesaurus agreement with WordNet and performance in answering TOEFL-like questions. Results illustrate the sensitivity of distributional thesauri to filters. 1 Introduction Large-scale distributional thesauri created automatically from corpora (Grefenstette, 1994; Lin, 1998; Weeds et al., 2004; Ferret, 2012) are an inexpensive and fast alternative for representing semantic relatedness between words, when manually constructed resources like WordNet (Fellbaum, 1998) are unavailable or lack coverage. To construct a distributional thesaurus, the (collocational or syntactic) contexts in which a target word occurs are used as the basis for calculating its similarity with other words. That is, two words are similar if they share a large proportion of contexts. Much attention has been devoted to refining thesaurus quality, improving informativeness and similarity measur"
D14-1047,W03-1810,0,0.648621,"2 Related Work In a nutshell, the standard approach to build a distributional thesaurus consists of: (i) the extraction of contexts for the target words from corpora, (ii) the application of an informativeness measure to represent these contexts and (iii) the application of a similarity measure to compare sets of contexts. The contexts in which a target word appears can be extracted in terms of a window of cooccurring (content) words surrounding the target (Freitag et al., 2005; Ferret, 2012; Erk and Pado, 2010) or in terms of the syntactic dependencies in which the target appears (Lin, 1998; McCarthy et al., 2003; Weeds et al., 2004). The informativeness of each context is calculated using measures like PMI, and t-test while the similarity between contexts is calculated using measures like Lin’s (1998), cosine, Jensen-Shannon divergence, Dice or Jaccard. Evaluation of the quality of distributional thesauri is a well know problem in the area (Lin, 419 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419–424, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 1998; Curran and Moens, 2002). For instance, for intrinsic ev"
D14-1047,Q13-1029,0,0.017365,"endencies generated by RASP (Briscoe et al., 2006), using nouns (heads of NPs) which have subject and direct object relations with the target verb. Thus, each target verb is represented by a set of triples containing (i) the verb itself, (ii) a context noun and (iii) a syntactic relation (object, subject). The thesauri were constructed using Lin’s (1998) method. Lin’s version of the distributional hypothesis states that two words (verbs v1 and v2 in our case) are similar if they share a large proportion of contexts weighted by their information content, assessed with PMI (Bansal et al., 2012; Turney, 2013). In the literature, little attention is paid to context filters. To investigate their impact, we compare two kinds of filters, and before calculating similarity using Lin’s measure, we apply them to remove 4 Results Figure 1 shows average WordNet similarities for thesauri built filtering by frequency threshold th and by p most frequent contexts. Table 1 summarizes the parametrization leading to the best WordNet similarity for each kind of filter. In all cases we show the results obtained for different frequency ranges2 as well as the results when averaging over all verbs. 1 Even though larger"
D14-1047,W09-0211,0,0.0811251,"Missing"
D14-1047,C04-1146,0,0.0807586,"Missing"
D14-1047,N12-1095,0,\N,Missing
D14-1047,P06-4020,0,\N,Missing
D14-1047,C98-2122,0,\N,Missing
F12-3011,W11-0815,0,0.0208236,"Missing"
F12-3011,W11-0822,1,0.895697,"Missing"
F12-3011,calzolari-etal-2002-towards,0,0.10343,"Missing"
F12-3011,N10-1029,0,0.0407676,"Missing"
F12-3011,W11-0823,0,0.0202915,"Missing"
F12-3011,J90-1003,0,0.200534,"Missing"
F12-3011,W11-0809,0,0.035056,"Missing"
F12-3011,W07-1104,0,0.0716465,"Missing"
F12-3011,W03-1806,0,0.0818894,"Missing"
F12-3011,W11-0812,1,0.882275,"Missing"
F12-3011,W11-0805,0,0.0632403,"Missing"
F12-3011,W10-3702,0,0.060955,"Missing"
F12-3011,C86-1001,0,0.395582,"Missing"
F12-3011,J03-3005,0,0.0652057,"Missing"
F12-3011,pearce-2002-comparative,0,0.0352161,"Missing"
F12-3011,P05-2003,0,0.0697964,"Missing"
F12-3011,W12-3301,1,0.840146,"Missing"
F12-3011,C10-3015,1,0.870808,"Missing"
F12-3011,ramisch-etal-2010-mwetoolkit,1,0.886517,"Missing"
F12-3011,W08-2107,1,0.899782,"Missing"
F12-3011,W01-0513,0,0.104419,"Missing"
F12-3011,J93-1007,0,0.513121,"Missing"
F12-3011,spina-2010-dictionary,0,0.0391338,"Missing"
F12-3011,P11-3003,0,0.0315121,"Missing"
F12-3011,E93-1070,0,0.392067,"Missing"
J17-4005,E89-1001,0,0.360581,"lable during parsing. Some studies have shown the great interest of performing MWE identification during syntactic parsing. We henceforth also use the term “joint” to refer to such approaches. Joint Grammar-Based Approaches. In a grammar-based parser, MWE identification is often integrated in the grammar. MWEs are generally found in a lexical resource, and parsers embody mechanisms to link MWE entries to grammar rules, as in Abeillé (1995) for TAG, Attia (2006) for LFG, and Copestake et al. (2002) and Villavicencio et al. (2007) for HPSG. For instance, in the TAG paradigm, Abeillé and Schabes (1989) link MWE lexical entries to tree rules that are anchored by multiple components present in the lexical entry. This mechanism has been integrated in the XTAG project that aims to construct a lexicalized TAG for English (XTAG 2001). In practice, rule-based parsers can also use MWE identification as a cue to locally select the best syntactic analysis: for instance, Wehrli (2014) applies heuristics favoring MWE analyses. Where statistical grammar-based parsers are trained from a reference treebank, MWEs must be annotated within the treebank. Typically, each MWE is annotated with a specific subtre"
J17-4005,W03-1812,0,0.035785,"s consider the items cat, dog, hot dog, and sandwich. We would expect that dog is similar to cat, dog is not similar to hot dog, and hot dog is similar to sandwich. Semantic similarity methods differ mainly in how they represent word and MWE senses, how they combine senses, and how they measure similarity. Word and MWE senses can be modeled using entries of semantic lexicons like WordNet synsets (McCarthy, Venkatapathy, and Joshi 2007). However, most discovery methods use distributional models (or word embeddings) instead, where senses are represented as vectors of co-occurring context words (Baldwin et al. 2003; Korkontzelos 2011). The creation of such vectors in distributional models has several parameters that affect the performance of MWE discovery, such as the number of vector dimensions and the type of context window (Cordeiro et al. 2016). The evaluation of discovery methods based on distributional similarity can use dedicated test sets (Reddy, McCarthy, and Manandhar 2011; Farahmand and Henderson 2016) or use handbuilt resources such as WordNet (Baldwin et al. 2003). Because methods based on distributional semantics use contextual information to represent meaning, they are closely related to"
J17-4005,2013.mtsummit-wmwumttt.5,1,0.923746,"BMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011; Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and Bond 2011). Discontiguous or variable MWEs represent an important source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT. 5.3 Evaluation of MWE-Aware MT The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012; Ramisch, Besacier, and"
J17-4005,E06-1042,0,0.0535441,"res. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard features. Sporleder and Li (2009) propose a completely unsupervised method based on lexical chains and text cohesion graphs. Their classifier considers an expression as literal if its presence in the sentence does not have a negative impact on cohesion, defined as the similarity between co-occurring words. For instance, play with fire reinforces cohesion"
J17-4005,H91-1060,0,0.0613814,"Survey of a parser generating the n-best parses (including MWE identification) and showed significant improvement in MWE identification accuracy. 4.3 Evaluation of MWE-Aware Parsing Evaluating a syntactic parser generally consists of comparing the output to reference (gold standard) parses from a manually labeled treebank. In the case of constituency parsing, a constituent is treated as correct if there exists a constituent in the gold standard parse with the same labels and starting and ending points. These parsers are traditionally evaluated through precision, recall, and F-measure metrics (Black et al. 1991; Sekine and Collins 1997). In standard dependency parsing with the single-head constraint,23 the number of dependencies produced by the parser should be equal to the number of total dependencies in the gold-standard parse tree. Common metrics to evaluate these parsers include the percentage of tokens with correct head, called unlabeled attachment score (UAS), and the percentage of tokens with correct head and dependency label, called labeled attachment score (LAS) (Buchholz and Marsi 2006; Nilsson, Riedel, and Yuret 2007). The evaluation of identification and discovery has been discussed in p"
J17-4005,W06-1620,0,0.021192,"Missing"
J17-4005,bonial-etal-2014-propbank,0,0.0382152,"Missing"
J17-4005,W12-5108,0,0.0604679,"Missing"
J17-4005,bouamor-etal-2012-identifying,0,0.0607539,"Missing"
J17-4005,D09-1049,0,0.0657262,"Missing"
J17-4005,W05-1501,0,0.0707877,"Missing"
J17-4005,C96-2182,0,0.467758,"Missing"
J17-4005,P98-1030,0,0.271352,"search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way → by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007; Korkontzelos and Manandhar 2010; Constant, Sigogne, and Watrin 2012; de Lhoneux 2015) and rule-based parsers (Brun 1998; Mamede et al. 2012). This orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013). Limitations. The pre-identification approach suffers from limitations. First, in this scenar"
J17-4005,W06-2920,0,0.0923239,"lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010; Ghoneim and Diab 2013). The lookup relies on a maximum forwar"
J17-4005,W13-1003,0,0.0193097,"e-to-many alignments to solve translation asymmetries (Melamed 1997; Carpuat and Diab 2010; Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is used to retrain the system in order to build a new phrase table. As a consequence, the phrase table takes into account MWEs and their translations. Alternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010; Tan and Pal 2014). Modifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b; Kordoni and Simova 2012; Pal, Naskar,"
J17-4005,calzolari-etal-2002-towards,0,0.0359057,"n aim of this survey is to shed light on how MWEs are handled in NLP applications. More particularly, it tries to clarify the nature of interactions between MWE processing and downstream applications such as MWE-aware parsing and MT. There is no shortage of proposed approaches for MWE processing and MWE-aware NLP applications. In fact, it is the emergence of approaches in the absence of guiding principles that motivates this article. There have been other surveys and reviews about MWEs with different scopes. Some concentrate primarily on their linguistic characteristics (Mel’ˇcuk et al. 1999; Calzolari et al. 2002; Sag et al. 2002; Wray 2002). Although this is a valid area of linguistic research, it is not of primary interest to researchers who are addressing the design of computational solutions to the spectrum of problems that MWEs bring into focus. Others are bibliographical reviews/state-of-the-art overviews done in the context of Ph.D. theses (Evert 2005; Pecina 2008) or book chapters (Manning and Schütze 1999; McKeown and Radev 1999; Baldwin and Kim 2010; Seretan 2011; Ramisch 2015), with a narrow scope focusing only on a specific part of MWE processing. In these studies, the subject area is rele"
J17-4005,W11-3806,0,0.348073,"lve word reordering: John gave it up → John gave_up it. In addition, when MWE components are concatenated into a single token, their internal syntactic structure is lost, whereas it may be required for the semantic processing of semi-compositional MWEs. However, this can be performed a posteriori, for instance, by applying simple rules based on POS patterns (Candito and Constant 2014). Then, the retokenization increases data sparsity that negatively affects parsing performance, because the vocabulary size increases whereas the total amount of training ˙ data is the same. Eryigit, ˘ Ilbay, and Can (2011) showed that the concatenation operation of different MWE categories has different impacts on parsing performance for Turkish. Whereas retokenization of multiword named entities and numerical expressions 865 Computational Linguistics Volume 43, Number 4 improved dependency parsing performance, retokenization of light-verb constructions harmed it. Another disadvantage is that pre-identification is deterministic, so the syntactic parser cannot recover from MWE identification errors. A sentence like He walked by and large tractors passed him cannot be analyzed correctly if by and large is pre-ana"
J17-4005,W16-1809,0,0.299487,"syntactic permutations by reordering words inside the MWE, combining frequencies using an entropy measure. Artificially generated variants can be transformed into features for supervised discovery methods, as we will see in Section 2.2.4 (Lapata and Lascarides 2003; Ramisch et al. 2008a). Methods based on variant generation and/or lookup were used to discover several MWE categories, such as English verb-particle constructions (McCarthy, Keller, and Carroll 2003; Ramisch et al. 2008b), English verb-noun idioms (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007), English noun compounds (Farahmand and Henderson 2016), and German noun-verb and noun-PP idioms (Weller and Heid 2010). Such methods often require external lexicons or grammars describing possible variants, like synonym lists or local reorderings (e.g., Noun1 Noun2 → Noun2 of Noun1 ). Synonyms or related words in substitution methods can come from thesauri like a WordNet and VerbNet (Pearce 2001; Ramisch et al. 2008b). Related words can be found in automatically compiled thesauri built using distributional vectors (Riedl and Biemann 2015; Farahmand and Henderson 2016). When compared with association measures, most of these methods are hard to gen"
J17-4005,J09-1005,0,0.306126,"Missing"
J17-4005,E06-1043,0,0.235234,"lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-distance discontiguity. Constraints encoded in the lexicon, such as obligatory or forbidden transformations, can be projected on text to disambiguate idiomatic constructions. Hashimoto, Sato, and Utsuro (2006) encode in a lexicon detailed properties of 100 Japanese verb-noun ambiguous idioms such as voice, adnominal modifications, modality, and selectional restrictions. Then, they only classify as idioms those occurrences that match the constraints in a dependency-parsed test set. More recent approaches to rule-based identification use dictionaries containing canonical MWE forms with no additional constraints. They consist of two stages: (1) POS tagging and lemmatizing the text and (2) performing dictionary lookup (Carpuat and Diab 2010; Ghoneim and Diab 2013). The lookup relies on a maximum forwar"
J17-4005,N09-1037,0,0.183694,"introduced by preposition de (of, from). Discussion. Joint approaches are of great interest for MWEs having syntactic vari˙ ability. In particular, Eryigit, ˘ Ilbay, and Can (2011) and Vincze, Zsibrita, and Nagy (2013) state that a joint approach using a dependency parser is very successful for the identification of light-verb constructions in Turkish and in Hungarian, respectively. Nonetheless, such approaches have the inconvenience of complicating the parsing stage through an increase in the size of the label sets. For instance, the literature shows 21 Also of note is the work of Finkel and Manning (2009), which is limited to named entity recognition and constituent parsing: They jointly performed both tasks using a parser based on conditional random fields, combining features specific to both tasks. The experimental results showed that the accuracy of both tasks increased. 867 Computational Linguistics Volume 43, Number 4 mixed results for non-compositional open compounds for which a pre-identification approach is sometimes more accurate than a joint one (Constant and Nivre 2016). The right balance therefore has to be found. An interesting way to deal with this issue is to combine the before"
J17-4005,W11-0805,0,0.163378,"Missing"
J17-4005,S12-1017,0,0.0264325,"zaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses a clustering strategy that tries to maximize transitive similarity with the seed set of literal or non-literal sentences using standard feat"
J17-4005,W10-1734,0,0.0311993,"word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German neologism Helikoptereltern (helicopter parents). They are usually translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning. Splitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010; Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics (Weller et al. 2014) or by using a morphosyntactic tagger and parser (Cap et al. 2014). Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones (Weller et al. 2014). A postprocessing step is required to merge components back into compound"
J17-4005,N06-2011,0,0.0164964,"Missing"
J17-4005,I13-1168,0,0.0605546,"Missing"
J17-4005,D11-1067,0,0.084647,"Missing"
J17-4005,J13-1009,0,0.0702486,"Missing"
J17-4005,D08-1104,0,0.0324534,"instead focus on detecting which of these are true MWEs. We discuss both supervised and unsupervised classifiers. Uchiyama, Baldwin, and Ishizaki (2005) tackle the problem of identifying Japanese verb compounds. Sense labels correspond to the meaning added by the second verb (aspectual, spatial, adverbial) with respect to the first verb. Their support vector machine guesses the possible semantic classes of a given verb combination, using the semantic classes of other co-occurring verbs as features. Then, in a second step, identification proper is done simply by taking the most frequent sense. Hashimoto and Kawahara (2008) propose a supervised disambiguation system able to distinguish literal from idiomatic uses of Japanese idioms. Fothergill and Baldwin (2012) perform an extended evaluation using the same data set and methodology, including new features, a feature ablation study, and cross-idiom tests. Similar approaches based on support vector machines and surface-level features have also been proposed for English light-verb constructions and verb-particle constructions (Tu 2012). Birke and Sarkar (2006) present a nearly unsupervised system capable of distinguishing literal from non-literal verb uses. It uses"
J17-4005,P06-2046,0,0.0188686,"Missing"
J17-4005,W11-0814,0,0.0259244,"Missing"
J17-4005,W10-1761,0,0.0300946,"Missing"
J17-4005,W09-2905,0,0.0632833,"Missing"
J17-4005,L16-1629,0,0.0216352,"Missing"
J17-4005,N15-2005,1,0.837894,"Missing"
J17-4005,D13-1176,0,0.0140378,"Chiang 2007; Hoang and Koehn 2010), or linguistic annotation layers in factorbased SMT (Koehn and Hoang 2007)). Phrase-based SMT and its variants build phrase tables—that is, a list of source fragments (words, phrases, subtrees), their translations, and their translation probabilities 870 Constant et al. MWE Processing: A Survey that take into account word sequences, not only simple words. In principle, therefore, such systems can naturally handle contiguous MWEs. Whether they can handle them correctly in all cases is, of course, a separate question. More recently, neural machine translation (Kalchbrenner and Blunsom 2013; Cho et al. 2014) proposes alternative methods to compute translation probabilities, by using recurrent neural networks to model the translation task. Most neural translation systems use an encoder–decoder architecture. The input sentence is encoded into a fixedlength or variable–length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and"
J17-4005,W06-1203,0,0.045459,"ndicates any item whose part of speech is not prefixed by V; [lemma!=there] indicates any item whose lemma is not the string there. Every lexical item occurs once by default. In case of potential multiple occurrences, it is followed by a repeat feature (between curly brackets) indicating the number of times it can occur as a range (here, [pos!∼/V.*/] can occur between 0 and 5 times). 19 http://intex.univ-fcomte.fr, http://macaon.lif.univ-mrs.fr, http://www.nooj4nlp.net, http://alpage.inria.fr/~sagot/sxpipe.html, http://www-igm.univ-mlv.fr/~unitex/. 858 Constant et al. MWE Processing: A Survey Katz and Giesbrecht (2006), detecting idiomatic verb-noun expressions in German, assume that the context of an idiomatic MWE differs from the contexts of its literal uses. Given two distributional vectors representing literal and idiomatic instances, a test instance is classified according to its similarity to the respective vectors. Cook, Fazly, and Stevenson (2007) propose a similar method based on canonical forms learned automatically from large corpora. Once a canonical form is recognized, distributional vectors for canonical and non-canonical forms are learned and then an instance is classified as idiomatic if it"
J17-4005,J03-3005,0,0.0173438,"s take into account their contingency table. Examples of such measures are χ2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher’s exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package.13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003; Ramisch et al. 2008a). Although association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursive"
J17-4005,2010.eamt-1.27,0,0.0204193,"Missing"
J17-4005,D07-1091,0,0.0396832,"Missing"
J17-4005,N03-1017,0,0.058843,"Missing"
J17-4005,kordoni-simova-2014-multiword,0,0.0556978,"Missing"
J17-4005,N10-1089,0,0.0109352,"ation measures. 4.2.2 Identification Before Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a single token (e.g., by the way → by_the_way). MWE identification prior to parsing has been implemented both in statistical (Cafferkey, Hogan, and van Genabith 2007; Korkontzelos and Manandhar 2010; Constant, Sigogne, and Watrin 2012; de Lhoneux 2015) and rule-based parsers (Brun 1998; Mamede et al. 2012). This orchestration type has the advantage of simplicity and empirical efficiency. For instance, Cafferkey, Hogan, and van Genabith (2007) show that pre-identifying multiword named entities and prepropositional MWEs improves parsing accuracy in the constituent framework. The best system of the track on MWE-aware dependency parsing in the SPMRL 2013 shared task (Seddah et al. 2013) was the only one that included deterministic pre-identification (Constant, Candito, and Seddah 2013). Limi"
J17-4005,2005.mtsummit-posters.11,0,0.058389,"ble takes into account MWEs and their translations. Alternatively, bilingual dictionaries of MWEs are added as additional training data to the parallel corpus (Babych and Hartley 2010; Tan and Pal 2014). Modifying the phrase table. Usually, a bilingual list of MWEs and their equivalents is dynamically extracted from the simple word alignment using specific MWE discovery tools (Bouamor, Semmar, and Zweigenbaum 2012b; Kordoni and Simova 2012; Pal, Naskar, and Bandyopadhyay 2013). Then, the phrase table is completed with the bilingual lists of MWEs and the probabilities are modified accordingly (Lambert and Banchs 2005) or added into a new phrase table with the probability set to 1 (Ren et al. 2009). An alternate strategy consists of adding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Jussà, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word tr"
J17-4005,E03-1073,0,0.14049,"Missing"
J17-4005,C14-1177,0,0.156618,"Missing"
J17-4005,D13-1116,0,0.0259179,"Missing"
J17-4005,W16-1810,0,0.157295,"Missing"
J17-4005,L16-1364,1,0.862298,"Missing"
J17-4005,J93-2004,0,0.0584036,"Missing"
J17-4005,W10-3713,0,0.193741,"lustration, Baldwin (2005) proposes different morphosyntactic and syntactic patterns to extract English verb-particle constructions with valence information from raw text. In particular, he shows the effect of using the outputs of a POS tagger, a chunker, a chunk grammar, or a parser, either individually or combined via a classifier. It experimentally appears that the ensemble method significantly outperforms the individual performances. As for individual scores, the use of shallow syntactic information like chunks tends to be prevalent. It is also possible to use pattern-free approaches like Martens and Vandeghinste (2010) and Sangati and van Cranenburgh (2015), who propose discovery methods not dedicated to a specific MWE category but based on recurring tree fragments and association measures. 4.2.2 Identification Before Parsing. When MWE identification is performed before parsing, the search space of the parsing algorithm is reduced. Hence, the main advantage of this type of orchestration is that the parsing process becomes less complex. The parser takes as input a sequence of partially analyzed linguistic units. This can be seen as a retokenization process, where the pre-identified MWE is merged into a singl"
J17-4005,D10-1004,0,0.0384566,"Missing"
J17-4005,W03-1810,0,0.0220181,"Missing"
J17-4005,D07-1039,0,0.0853842,"Missing"
J17-4005,H05-1066,0,0.0219995,"Missing"
J17-4005,W97-0311,0,0.0813059,"hrase (Carpuat and Diab 2010). Several observed approaches are: (1) changing the training data dynamically (word alignment or the parallel corpus) to take into account MWEs and then retraining the system; (2) modifying the phrase table directly by including information about MWEs and their translations. In both strategies, the use of MWE identification and discovery tools is essential to improve the quality of the translation. Modifying training data. A frequent strategy completes simple word alignment with many-to-many, many-to-one, or one-to-many alignments to solve translation asymmetries (Melamed 1997; Carpuat and Diab 2010; Okita 2012). Word alignment completion is based on simple word alignment and on MWE identification tools, designed for specific MWE categories (Tan and Pal [2014] for multiword named entities; Bouamor, Semmar, and Zweigenbaum [2012b] and Okita and Way [2011] for terms; Ramisch, Villavicencio, and Boitet [2010] for general MWEs). Alternatively, MWE identification and alignment is performed using bilingual lexical resources, with translation alongside an n-gram language model to help with disambiguation (Bungum et al. 2013). The resulting many-to-many word alignment is u"
J17-4005,P12-1082,0,0.0585527,"Missing"
J17-4005,2011.freeopmt-1.4,1,0.821234,"decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language"
J17-4005,2010.amta-srw.2,0,0.00843731,"dding new features in the phrase table, such as the number of MWEs present in the bilingual aligned phrases (Carpuat and Diab 2010) or the property that the parallel phrase contains a bilingual MWE (Ren et al. 2009). In this way, the translation quality is improved for certain specific MWE categories or languages (Costa-Jussà, Daudaravicius, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntac"
J17-4005,W14-0803,0,0.0495089,"Missing"
J17-4005,P15-1108,1,0.945817,"key role in syntax may be ambiguous (e.g., up to). For instance, in John looked up to the sky, the sequence up to should not be identified as a multiword preposition. If so, it would prevent the right analysis: (John) ((looked) (up to the sky)) instead of (John) ((looked) (up) (to the sky)). Conversely, combining MWE identification and parsing can help resolve such ambiguities, yielding both better identification and parsing models. Multiword function words such as complex prepositions, conjunctions, and adverbials (up to, now that, by the way) can be disambiguated by their syntactic context (Nasr et al. 2015). For example, the sequence de la in French can be either a compositional sequence (preposition de + determiner la), or a complex partitive determiner, as shown in the following examples and their corresponding syntactic analyses in Figure 5: la voiture (1) Je parle de I talk about the car (2) Je mange de la soupe I eat some soup MWE-aware parsing is a natural way to solve this ambiguity. The intransitive verb parle (talk) selects the preposition de (about), whereas mange (eat) requires a noun phrase as its object. Furthermore, one of the main challenges of parsing in general is attachment amb"
J17-4005,J14-2001,0,0.0292739,"Missing"
J17-4005,P00-1056,0,0.382568,"Missing"
J17-4005,2004.tmi-1.2,0,0.198054,"account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016). Rule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation equivalents. Finally, example-based machine translation (EBMT) is based mainly on examples in the form of large translation memories (large collections of source/target sentence pairs) but also uses rules to acquire new linguistic knowledge dynamically. EBMT is based on a translation by analogy approach, where at run time translations are obtained by looking up and using examples stored in translation memories. The translation process is organized in three stages: (i) matching of input sentences with translations previously stored, (ii) retrieval of"
J17-4005,W04-0409,0,0.240092,"r verbal inflection paradigm. The inflection process may be based on finite-state transducers as in Silberztein (1997), possibly augmented with a unification mechanism for handling agreement between the MWE components (Savary 2009). These approaches are extremely precise, but costly. The manual assignment of inflection rules may be eased by tools like Leximir for predicting inflection classes (Krstev et al. 2013). Another approach comprises two processing stages: morphological analysis of simple words followed by a composition of regular rules to identify MWEs, as in Oflazer, Çetinoglu, ˘ and Say (2004) for Turkish. Breidt, Segond, and Valetto (1996) design regular rules that handle morphological variations and restrictions like the French idiom perdre ADV* :la :tˆete (lit. lose ADV* :the :head, ’to lose one’s mind’),16 lexical and structural variations (birth date = date of birth). Copestake et al. (2002) design an MWE lexicon for English based on typed feature structures that may rely on analysis of internal words of MWE. Silberztein (1997) also proposes the use of local grammars in the form of equivalence graphs. These approaches are very efficient in dealing with variability and short-di"
J17-4005,W13-2814,0,0.0239032,"Missing"
J17-4005,P02-1040,0,0.118219,"source of translation errors. These methods have the advantage of handling discontiguous or variable MWEs with the help of rules for RBMT or by completing word alignments dynamically in SMT. 5.3 Evaluation of MWE-Aware MT The evaluation of MWEs translation quality remains an open challenge, whatever MT paradigm is adopted (Monti et al. 2012; Ramisch, Besacier, and Kobzar 2013; Barreiro et al. 2014), because of a lack of shared assessment methodologies, benchmarking resources, and annotation guidelines. With reference to the assessment methodologies, automatic evaluation metrics such as BLEU (Papineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015)"
J17-4005,pearce-2002-comparative,0,0.0450507,"s by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999).14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009). Association measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002; Evert 2005; Pecina 2008; Ramisch, De Araujo, and Villavicencio 2012). 2.2.2 Substitution and Insertion. A French kiss cannot be referred to as a kiss that is French, a kiss from France, or a French smack, unlike non-MWE combinations like French painter and passionate kiss. Because of their non-compositionality, MWEs exhibit nonsubstitutability, that is, limited morphosyntactic and semantic variability. Thus, the replacement or modification of individual words of an MWE often results in unpredictable meaning shifts or invalid combinations. This property is the basis of discovery methods based"
J17-4005,P15-1031,0,0.0418764,"Missing"
J17-4005,racz-etal-2014-4fx,0,0.0382637,"Missing"
J17-4005,2013.mtsummit-wmwumttt.8,0,0.0747715,"Missing"
J17-4005,W12-3301,1,0.804561,"Missing"
J17-4005,ramisch-etal-2010-mwetoolkit,1,0.876983,"Missing"
J17-4005,W08-2107,1,0.876823,"or a French smack, unlike non-MWE combinations like French painter and passionate kiss. Because of their non-compositionality, MWEs exhibit nonsubstitutability, that is, limited morphosyntactic and semantic variability. Thus, the replacement or modification of individual words of an MWE often results in unpredictable meaning shifts or invalid combinations. This property is the basis of discovery methods based on substitution and insertion (including permutation, syntactic alternations, etc.). 12 For more details on association measures, see http://www.collocations.de, Evert (2005), and Pecina (2008). 13 http://search.cpan.org/dist/Text-NSP/. 14 http://research.variancia.com/multiwords2/. 850 Constant et al. MWE Processing: A Survey Pearce’s (2001) early synonym substitution method replaces parts of the MWE by synonyms obtained from WordNet, and then obtains frequencies for the artificially generated MWE variants from external sources. Instead of using variant frequencies directly, it is possible to estimate an MWE candidate’s frequency using a weighted sum of variant corpus frequencies (Lapata and Lascarides 2003) or Web-based frequencies (Keller and Lapata 2003). A similar approach is u"
J17-4005,W95-0107,0,0.118037,"ing the insertion of variable components. The overall impact of discontiguity is language-dependent. For example, separable verb-particle constructions, frequent in Germanic languages, are almost non-existent in Romance languages. 855 Computational Linguistics Volume 43, Number 4 Overlaps. A discontiguous MWE can have other nested MWEs in between its components, like dirty word as the direct object of look up. This is especially problematic for systems that use IOB encoding, as exemplified in row 3, which addresses the segmentation problem with tags B, for begin, I for inside, O, for outside (Ramshaw and Marcus 1995). Often, nesting demands multi-level tags, otherwise different segments could be mixed up. For instance, word and up would have subsequent I tags, although they are not part of the same MWE. In the example, outer MWEs use capital IOB tags and inner MWEs use lowercase iob tags, following the tagging scheme proposed by Schneider et al. (2014a). Nesting is a particular case of overlap, whereby MWEs can also share tokens in a sentence. For instance, the verb-particle construction to let out can be contained in the idiom to let the cat out of the bag. If the MWE tagger cannot output more than one M"
J17-4005,I11-1024,0,0.0169712,"Missing"
J17-4005,W09-2907,0,0.0591534,"MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire (to make) combined with the compound faux_pas (blunder), which had been merged due to high association in a previous pass (Seretan 2011). The LocalMaxs algorithm finds optimal MWE boundaries by recursively including left and right context words, stopping when the association decreases (da Silva et al. 1999).14 A similar approach, using a lexical tightness measure, was proposed to segment Chinese MWEs (Ren et al. 2009). Association measures can be adapted according to the morphosyntactic nature of lexical elements. Hoang, Kim, and Kan (2009) propose new measures where very frequent words such as prepositions are weighted differently from regular tokens. Comparisons between different association measures have been published, but to date no single best measure has been identified (Pearce 2002; Evert 2005; Pecina 2008; Ramisch, De Araujo, and Villavicencio 2012). 2.2.2 Substitution and Insertion. A French kiss cannot be referred to as a kiss that is French, a kiss from France, or a French smack, unlike non-MWE"
J17-4005,C92-2065,0,0.422926,"s comprises two phases: an analysis phase, generating a set of possible syntactic trees, followed by a disambiguation phase based on heuristics (Boullier and Sagot 2005; Wehrli 2014) or statistical models (Riezler et al. 2002; Villemonte De La Clergerie 2013). In the second (mainstream) strategy, the grammar is accompanied by a statistical model. For instance, parsers based on generative-models assign probabilities to rules of an underlying grammatical formalism, as in probabilistic context-free grammars (PCFGs) (Charniak and Johnson 2005), tree-substitution grammars (Green et al. 2011), TAG (Resnik 1992), and LFG (Cahill 2004). The parsing algorithms generally rely on dynamic programming. They usually include one pass, but two-pass processes also exist. For instance, Charniak and Johnson (2005) successfully propose applying a discriminative reranker to the n-best parses produced by a generative PCFG-based parser. Grammarless parsing is performed without any underlying grammar and is based on discriminative approaches. It uses machine learning techniques only, mainly (not exclusively) in the dependency framework. The different parsing algorithms vary from local search approaches, such as trans"
J17-4005,D15-1290,0,0.181527,"h verb-noun idioms (Fazly and Stevenson 2006; Cook, Fazly, and Stevenson 2007), English noun compounds (Farahmand and Henderson 2016), and German noun-verb and noun-PP idioms (Weller and Heid 2010). Such methods often require external lexicons or grammars describing possible variants, like synonym lists or local reorderings (e.g., Noun1 Noun2 → Noun2 of Noun1 ). Synonyms or related words in substitution methods can come from thesauri like a WordNet and VerbNet (Pearce 2001; Ramisch et al. 2008b). Related words can be found in automatically compiled thesauri built using distributional vectors (Riedl and Biemann 2015; Farahmand and Henderson 2016). When compared with association measures, most of these methods are hard to generalize, as they model specific limitations that depend on the language and MWE category. 2.2.3 Semantic Similarity. Models based on semantics account for the fact that many MWE categories are partly or fully non-compositional. Because the meaning of the parts does not add up to the meaning of the whole, there should be little similarity between the computational-semantic representation of MWEs and of words that constitute them. For instance, let us consider the items cat, dog, hot do"
J17-4005,W16-1816,0,0.155422,"Missing"
J17-4005,P02-1035,0,0.175342,"Missing"
J17-4005,W15-0908,1,0.841812,"Missing"
J17-4005,L16-1368,0,0.056784,"Missing"
J17-4005,N15-1099,0,0.0118336,"pineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015). Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation. Another important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary g"
J17-4005,W15-0909,0,0.0305586,"pineni et al. 2002) do not specifically take MWE translation quality into account. For instance, BLEU is based on shared words between the candidate and the reference translation, and gives only a very general indication about quality. Thus, it cannot be considered as a suitable metric for the kind of more differentiated analysis required to identify specific gaps in the coverage of the system, as is needed for MWEs. There have been a few attempts to adapt automatic evaluation metrics towards a more fine-grained MT error analysis (Babych and Hartley 2010; Stymne, Cancedda, and Ahrenberg 2013; Salehi et al. 2015). Extrinsic evaluations in MT have also been performed, mainly for SMT. For instance, Carpuat and Diab (2010) conducted a pilot study for a task-oriented evaluation of MWE translation in SMT, whereas Bouamor, Semmar, and Zweigenbaum (2012a) consider SMT as an extrinsic evaluation of the usefulness of automatically discovered MWEs and explore strategies for integrating them in a SMT system, aiming at a more thorough error analysis of MWE translation. Another important drawback in this field is represented by the fact that parallel corpora annotated with MWEs, which are important and necessary g"
J17-4005,W14-0806,0,0.0203644,"Missing"
J17-4005,W15-0902,0,0.0491953,"Missing"
J17-4005,Q14-1016,0,0.0745576,"Missing"
J17-4005,S16-1084,0,0.39778,"al. 2012). For example, whereas the syntactic structure within light-verb constructions such as to make a decision is annotated with special MWE relations in the Prague Treebank, they are annotated as regular verb–object pairs in the Penn Treebank. Although, at the time of writing this survey, there is still not a universal standard for MWE annotation, one of the main goals of the PARSEME network is to develop annotation guidelines for MWE representation in both constituency and dependency treebanks. For an up-to-date status of the current annotations for different languages, see Rosén et al. (2015, 2016). Appendix A provides a complementary list of resources and tools for MWE processing. Identification relies on lexical resources that can be either the fruit of discovery or hand-built. Both parsing and MT rely on lexical resources as well, either through a separate identification step or by using them internally. For example, MWE lexicons are important for MT within preprocessing, postprocessing, and translation phases of different paradigms: They are mainly used to delimit MWEs, replacing them by either a single token, a sense identifier, or by a translation equivalent before alignment takes"
J17-4005,schneider-etal-2014-comprehensive,0,0.0968038,"Missing"
J17-4005,W14-0821,0,0.0606462,"Missing"
J17-4005,W13-4917,0,0.0380886,"Missing"
J17-4005,W16-2209,0,0.016354,"fixedlength or variable–length vector and then one or more decoders use this representation to obtain the target sentence. The probability of the translation of one word is computed on the basis of the translation probabilities of previous words. An attention model is frequently used to represent larger contexts for the translated words and sentences. Indeed, attention models represent source word and larger-context words (using a dot product of vectors or multilayer perceptrons) to generate a target word. Few neural machine translation systems take into account fine linguistic descriptions (Sennrich and Haddow 2016). Neural machine translation obtains impressive improvements of the evaluation scores such as BLEU (Wu et al. 2016). Rule-based machine translation (RBMT) uses large lexicons and explicit rules describing the syntactic and semantic constraints on both the source and the target language. Transfer rules are used to map source language structures to target language ones and to identify the right translation. These rules are based on formal grammars or intermediate language-independent structures (such as minimal recursion semantics [Oepen et al. 2004]) capable of generating correct translation eq"
J17-4005,W13-1021,0,0.0692869,"nd Baldwin (2006), whose work’s main purpose is to acquire new tagged lexical items for two head-driven phrase structure grammars (ERG for English and JACY for Japanese), propose a supertagging approach based on conditional random fields to assign lexical types to the tokens of an input sequence using a pseudolikelihood method that accommodates large tag sets. The proposed approach only enables the identification of contiguous MWEs. This work is very close to methods for joint (contiguous) MWE identification and POS tagging based on linear conditional random fields (Constant and Sigogne 2011; Shigeto et al. 2013). Their tagging scheme concatenates lexical segmentation information (B and I tags for the IOB tag set) with the POS tag of the lexical unit to which the current token belongs. Constant and Sigogne (2011) trained and evaluated their models on the French Treebank, and Shigeto et al. (2013) worked on a modified version of the Penn Treebank onto which complex function words from Wiktionary were projected. Some MWE taggers concentrate on the identification task only, using an IOB-like annotation scheme (Vincze, Nagy, and Berend 2011; Constant, Sigogne, and Watrin 2012; Schneider et al. 2014a). Vin"
J17-4005,W09-2906,0,0.0284012,"llel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word. Monolingual discovery. For monolingual discovery we consider the possibility of using translation asymmetries identified in parallel corpora to compile lists of potential MWE candidates in one specific language without using precomputed word alignments. For example, Sinha (2009) discover Hindi by compiling a list of Hindi light verbs and then looking at mismatches (indicating the use of a verb in a more idiomatic sense) in meaning in the corresponding English counterpart, given a list of literal translations of Hindi light verbs into English. This approach has also been extended to comparable corpora (texts from the same domain, genre, or type that are not in a translation relation). Morin and Daille (2010) collect bilingual terminology from comparable corpora with the help of a bilingual dictionary. This method applies a compositional word-for-word translation for a"
J17-4005,J93-1007,0,0.0567457,"ng evidence to reject the independence null hypothesis, that is, the candidate words are not independent and probably form an MWE. More sophisticated test statistics for two-word MWE candidates take into account their contingency table. Examples of such measures are χ2 and the more robust likelihood ratio (Dunning 1993). Pedersen (1996) suggests using Fisher’s exact test in automatic MWE discovery, and this measure is implemented among others in the Text:NSP package.13 Another measure for MWE discovery is the average and standard deviation of the distance between words, implemented in Xtract (Smadja 1993). Because these measures are based on frequency counts, there have been some studies to use Web hits as an alternative to corpus counts, in order to avoid low-frequency estimates (Keller and Lapata 2003; Ramisch et al. 2008a). Although association measures work quite well for two-word expressions, they are hard to generalize to arbitrary n-word MWE candidates. One simple approach is to merge two-word MWEs as single tokens and then apply the measure recursively. For instance, in French, the MWE faire un faux pas (lit. to make a false step, ’to make a blunder’) can be modeled as the verb faire ("
J17-4005,E09-1086,0,0.0174536,"Missing"
J17-4005,W04-0401,0,0.137462,"Missing"
J17-4005,J13-4009,0,0.0958839,"Missing"
J17-4005,W14-3323,0,0.128234,"tive, that is, new multiword named entities and terms are constantly being created, so it is difficult to have complete and updated lexical resources for use during the translation process. For term identification, bilingual or multilingual term glossaries might be applied to transform terms into an intermediate representation (words concatenated by underscore). But when these resources are missing, for new domains or for under-resourced languages, multiword named entities and multiword terms can be annotated as a single token with the help of specific techniques for named entity recognition (Tan and Pal 2014), or term extraction (Bouamor, Semmar, and Zweigenbaum 2012b) designed for monolingual, parallel, or comparable data (Morin and Daille 2010). Closed compounds, obtained by concatenating several lexemes with any parts of speech, are typical of Germanic languages and represent another difficult task for MT. This category of expressions can be lexicalized, that is, they belong to the lexicon of a language as a single meaning unit, such as the German word Schwiegereltern (parentsin-law) or non-lexicalized, that is, the individual words keep their meanings when combined, for instance, the German ne"
J17-4005,J14-2007,0,0.0636085,"lingual MWE lists, as presented in Section 2. Bouamor, Semmar, and Zweigenbaum (2012b) use an association measure to find the translations of each MWE in the target language counterpart without exploiting the alignment. Parsed bilingual data has also been used to filter word-aligned MWE candidates. Thus, Zarrieß and Kuhn (2009) propose a method for detecting verb-object 873 Computational Linguistics Volume 43, Number 4 MWEs in both source and target languages that are dependency-parsed, only retaining MWEs whose words are bilingually aligned and monolingually linked by syntactic dependencies. Tsvetkov and Wintner (2014) proposed supervised classifiers to distinguish MWEs from non-MWEs, using linguistically motivated features such as literal translatability derived from simple word alignments in parallel corpora. Here, a check is carried out to assess whether the MWE candidate could be literally translated from a bilingual dictionary. Similarly, Rondon, Caseli, and Ramisch (2015) model translatability as the probability of translating the words of the MWE from Portuguese into English and then back to the same Portuguese word. Monolingual discovery. For monolingual discovery we consider the possibility of usin"
J17-4005,W14-0817,0,0.0365037,"Missing"
J17-4005,D07-1110,1,0.793142,"Missing"
J17-4005,W13-5706,0,0.0378745,"Missing"
J17-4005,R11-1040,0,0.208692,"Missing"
J17-4005,vincze-2012-light,0,0.323696,". Progressively more refined information can approach the level of expressiveness found in treebanks. Examples of annotated corpora with MWE tags include Wiki50 (Vincze, Nagy, and Berend 2011), STREUSLE (Schneider et al. 2014b), and the PARSEME shared task corpora (Savary et al. 2017). Two or more corpora can also be set in correspondence. For example, parallel corpora in different languages include sentence-level alignment and are used to detect manyto-many, one-to-many, or many-to-one translations. An example of MWE-annotated parallel corpus is the English–Hungarian SzegedParallelFX corpus (Vincze 2012). Finally, treebanks are special corpora that include syntactic relations between nodes over text segments and are arguably the most valuable resources for data-driven parsing 845 Computational Linguistics Volume 43, Number 4 systems and syntax-aware MT systems. In the literature, there exist different opinions on whether syntactically regular but semantically idiomatic MWEs should be identified in syntactic treebanks. Although the Penn Treebank designers prefer not to annotate verbal MWEs (Marcus, Marcinkiewicz, and Santorini 1993), these are annotated in the Prague Treebank (Bejˇcek et al. 2"
J17-4005,I13-1024,0,0.24777,"Missing"
J17-4005,S13-1038,0,0.06584,"Missing"
J17-4005,W14-0804,0,0.196566,"hashi 1975), combinatory categorial grammar (Steedman 1987), lexical functional grammar (LFG) (Kaplan 1989), and headdriven phrase-structure grammar (HPSG) (Pollard and Sag 1994). Grammars may also be composed of sets of finite-state rules that are incrementally applied (Joshi and Hopeli 1996; Ait-Mokhtar, Chanod, and Roux 2002). Two different strategies are generally used to handle ambiguity. In the first strategy, the process comprises two phases: an analysis phase, generating a set of possible syntactic trees, followed by a disambiguation phase based on heuristics (Boullier and Sagot 2005; Wehrli 2014) or statistical models (Riezler et al. 2002; Villemonte De La Clergerie 2013). In the second (mainstream) strategy, the grammar is accompanied by a statistical model. For instance, parsers based on generative-models assign probabilities to rules of an underlying grammatical formalism, as in probabilistic context-free grammars (PCFGs) (Charniak and Johnson 2005), tree-substitution grammars (Green et al. 2011), TAG (Resnik 1992), and LFG (Cahill 2004). The parsing algorithms generally rely on dynamic programming. They usually include one pass, but two-pass processes also exist. For instance, Cha"
J17-4005,W10-3705,0,0.104896,"Missing"
J17-4005,2009.eamt-1.18,0,0.0171667,"es and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using parsing output or some linguistic patterns. Several RBMT systems identify MWEs and generate translations on the basis of formal representations of natural language texts such as parse trees (Wehrli et al. 2009) or intermediate representation languages like minimal recursion semantics (Oepen et al. 2004), a semantico-syntactic abstraction language (Monti et al. 2011; Barreiro et al. 2013). Transfer rules handle MWE variability and discontiguity (Forcada et al. 2011) and are manually defined or automatically learned from parallel corpora (Haugereid and B"
J17-4005,2011.mtsummit-papers.45,0,0.0143103,"us, and Banchs 2010). The modified phrase table contains, indeed, the correct translations of MWEs, thus avoiding an incorrect wordfor-word translation during the decoding phase and helping disambiguation. More complex models are proposed in syntax-based SMT (Na et al. 2010) or in hierarchical SMT (Chiang 2007). These approaches use grammars to handle discontiguous components and find their translation directly: parsing improves the translation process 876 Constant et al. MWE Processing: A Survey (according to BLEU and METEOR scores) by providing trees and transfer rules based on parsed data (Wei and Xu 2011). MWE-aware strategies in EBMT and RBMT. EBMT (Gangadharaiah, Brown, and Carbonell 2006) or RBMT strategies (Anastasiou 2008; Forcada et al. 2011; Monti et al. 2011) dynamically apply rules to handle MWE translations. Some rules are identified from the syntactic tree alignments (Segura and Prince 2011) and integrated into an EBMT system to handle discontiguous MWEs. RBMT systems use large lexicons to handle contiguous MWEs and apply the correct translation strategy: a simple word-for-word translation strategy or a compositional rule (Wehrli et al. 2009). Discontiguous MWEs are identified using"
J17-4005,W14-5709,0,0.135338,"ally translated into several target language words. Their meaning might be more or less compositional. MT systems fail to correctly translate these compounds because of their low frequencies and their variability. Moreover, non-compositional compounds have unpredictable meaning. Splitting strategies can be applied to cut the compounds into subsequent words to improve translation quality (Fritzinger and Fraser 2010; Stymne, Cancedda, and Ahrenberg 2013). Splitting is done by identifying component words in the corpus or by prefix and suffix identification together with distributional semantics (Weller et al. 2014) or by using a morphosyntactic tagger and parser (Cap et al. 2014). Oversplitting can also be a problem: Splitting non-compositional compounds may generate erroneous translations. Some methods aim to distinguish between compositional and non-compositional compounds and split only the compositional ones (Weller et al. 2014). A postprocessing step is required to merge components back into compounds once a translation is generated using a system trained on split compounds. Some methods replace the compounds by paraphrases (Ullman and Nivre 2014) before translating them. 875 Computational Linguist"
J17-4005,weller-heid-2010-extraction,0,0.104568,"Missing"
J17-4005,1983.tc-1.13,0,0.773002,"Missing"
J17-4005,D15-1201,0,0.0278442,"Missing"
J17-4005,W09-2904,0,0.0667944,"Missing"
J17-4005,W04-2407,0,\N,Missing
J17-4005,J90-1003,0,\N,Missing
J17-4005,copestake-etal-2002-multiword,0,\N,Missing
J17-4005,W04-0412,0,\N,Missing
J17-4005,W07-1106,0,\N,Missing
J17-4005,P05-1038,0,\N,Missing
J17-4005,C98-1030,0,\N,Missing
J17-4005,P11-1070,0,\N,Missing
J17-4005,P05-1022,0,\N,Missing
J17-4005,P15-1033,0,\N,Missing
J17-4005,D11-1077,0,\N,Missing
J17-4005,W13-4905,0,\N,Missing
J17-4005,P14-1070,0,\N,Missing
J17-4005,N10-1029,0,\N,Missing
J17-4005,D14-1179,0,\N,Missing
J17-4005,W11-0809,0,\N,Missing
J17-4005,2010.eamt-1.17,0,\N,Missing
J17-4005,W09-2903,0,\N,Missing
J17-4005,E14-1061,0,\N,Missing
J17-4005,P12-1022,0,\N,Missing
J17-4005,C12-1015,0,\N,Missing
J17-4005,W15-0903,0,\N,Missing
J17-4005,D14-1082,0,\N,Missing
J17-4005,barreiro-etal-2014-linguistic,1,\N,Missing
J17-4005,vincze-etal-2010-hungarian,0,\N,Missing
J17-4005,J07-2003,0,\N,Missing
J17-4005,N16-1127,0,\N,Missing
J17-4005,P16-1016,0,\N,Missing
J17-4005,P16-1187,1,\N,Missing
J17-4005,D07-1096,0,\N,Missing
J17-4005,W17-1704,1,\N,Missing
J17-4005,D14-1108,0,\N,Missing
J19-1001,N09-1003,0,0.063104,"Missing"
J19-1001,J08-4004,0,0.212898,"ositional or fully idiomatic) for all scores. These findings can be partly explained by end-of-scale effects, that result in greater variability for the intermediate scores in the Likert scale (from 1 to 4) that correspond to the partly compositional cases. Hence, we expect that it will be easier to predict the compositionality of idiomatic/compositional compounds than of partly compositional ones. Inter-Annotator Agreement (α). To measure inter-annotator agreement of multiple participants, taking into account the distance between the ordinal ratings of the Likert scale, we adopt the α score (Artstein and Poesio 2008). The α score is more appropriate for ordinal data than traditional agreement scores for categorical data, such as Cohen’s and Fleiss’ κ (Cohen 1960; Fleiss and Cohen 1973). However, due to the use of crowdsourcing, most participants rated only a small number of compounds with very limited chance of overlap among them: the average number of answers per participant is 13.6 for EN-comp90 , 10.2 for EN-compExt , 33.7 for FR-comp, and 53.5 for PT-comp. Because the 11 Participants with negative correlation with the mean, and answers farther than ±1.5 from the mean. 12 Only FR-comp is shown as the o"
J19-1001,W03-1809,0,0.184601,"Missing"
J19-1001,P14-1023,0,0.105379,"Missing"
J19-1001,J10-4006,0,0.309322,"2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrases, sentences, and even whole documents, using simple additive and multiplicative vector operations (Mitchell and Lapata 2010; Reddy, McCarthy, and Manandhar 2011; Mikolov et al. 2013; Salehi, Cook, and Baldwin 2015), syntax-based lexical functions (Socher et al. 2012), or matrix and tensor operations (Baroni and Lenci 2010; Bride, Van de Cruys, and Asher 2015). However, it is not clear to what extent this approach is adequate in the case of idiomatic multiword expressions (MWEs). MWEs fall into a wide spectrum of compositionality; that is, some MWEs are more compositional (e.g., olive oil) while others are more idiomatic (Sag et al. 2002; Baldwin and Kim 2010). In the latter case, the meaning of the MWE may not be straightforwardly related to the meanings of its parts, creating a challenge for the principle of compositionality (e.g., snake oil as a product of questionable benefit, not necessarily an oil and cer"
J19-1001,boos-etal-2014-identification,1,0.883664,"Missing"
J19-1001,P15-1028,0,0.036196,"Missing"
J19-1001,P15-2001,0,0.0314647,"Missing"
J19-1001,W15-0903,0,0.108306,"n that MWEs are frequent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compo"
J19-1001,N10-1029,0,0.0596242,"Missing"
J19-1001,J90-1003,0,0.608336,"hat are more frequent tend to be assigned higher compositionality scores. However, frequency alone is not enough to predict compositionality, and further investigation is needed to determine if compositionality and frequency are also correlated with other factors. 14 r2arith and r2geom are .91 and .96 in PT-comp, .90 and .96 in EN-comp90 , and .92 and .95 in EN-compExt . 13 Computational Linguistics Volume 45, Number 1 We also analyzed the correlation between compositionality and conventionalization to determine if more idiomatic compounds correspond to more conventionalized ones. We use PMI (Church and Hanks 1990) as a measure of conventionalization, as it indicates the strength of association between the components (Farahmand, Smith, and Nivre 2015). We found no statistically significant correlation between compositionality and PMI. 4. Compositionality Prediction Framework We propose a compositionality prediction framework15 including the following elements: a DSM, created from corpora using existing state-of-the-art models that generate corpus-derived vectors16 for compounds w1 w2 and for their components w1 and w2 ; a composition function; and a set of predicted compositionality scores (pc). The fra"
J19-1001,J17-4005,1,0.911229,"Missing"
J19-1001,P16-1187,1,0.856526,"Missing"
J19-1001,L16-1194,1,0.854555,"tion (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics Volume 45, Number 1 2.1 Distributional Semantic Models Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. These vectors are built assuming the distributional hypothesis, whose central idea is that the meaning of a word can be learned based on the contexts where it appears—or, as popularized by Firth (1957), “you shall know a word by the company it keeps.” Formally, a DSM attem"
J19-1001,P02-1030,0,0.120372,"e characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task and model (Lapesa and Evert 2014). Because there is no consensus about a single optimal model that works for all tasks, we compare a variety of models (Section 5) to determine which are best suited for our compositionality prediction framework. 2.2 Compositionality Prediction Before adopting the principle of com"
J19-1001,W15-0904,0,0.275318,"Missing"
J19-1001,J09-1005,0,0.0990759,"Missing"
J19-1001,P13-1055,0,0.0187504,"solve the prediction task. The weight parameters that connect the unity representing wi with the d-dimensional hidden layer are taken as its vector representation v(wi ). There are a number of factors that may influence the ability of a DSM to accurately learn a semantic representation. These include characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task an"
J19-1001,W11-0805,0,0.0272043,"er MWE categories by addressing their variability in future work. Furthermore, to determine to what extent these approaches are also adequate cross-lingually, we evaluate them in three languages: English, French, and Portuguese. Given that MWEs are frequent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a com"
J19-1001,W05-0604,0,0.0251484,"is’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrases, sentences, and even whole documents, using simple additive and multiplicative vector operations (Mitchell and Lapata 2010; Reddy, McCarthy, and Manandhar 2011; Mikolov et al. 2013; Salehi, Cook, and Baldwin 2015), syntax-based lexical functions (Socher et al. 2012), or matrix and tensor operations (Baro"
J19-1001,W11-0115,0,0.0207817,", McCarthy, and Manandhar 2011; Schulte ¨ im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy, and Manandhar 2011). For example, in flea market, it is the head (market) that has a clear contribution to the overall meaning, whereas in couch potato it is the modifier (couch). The additive model can be generalized to use a matrix of multiplicative coefficients, which can be estimated through linear regression (Guevara 2011). This model can be 4 The task of determining whether a phrase is compositional is closely related to MWE discovery (Constant et al. 2017), which aims to automatically extract MWE lists from corpora. 5 Computational Linguistics Volume 45, Number 1 further modified to learn polynomial projections of higher degree, with quadratic projections yielding particularly promising results (Yazdani, Farahmand, and Henderson 2015). These models come with the caveat of being supervised, requiring some amount of pre-annotated data in the target language. Because of these requirements, our study focuses on u"
J19-1001,E17-1006,0,0.0188226,"ments, our study focuses on unsupervised compositionality prediction methods only, based exclusively on automatically POS-tagged and lemmatized monolingual corpora. Alternatives to the additive model include the multiplicative model and its variants (Mitchell and Lapata 2008). However, results suggest that this representation is inferior to the one obtained through the additive model (Reddy, McCarthy, and Manandhar 2011; Salehi, Cook, and Baldwin 2015). Recent work on predicting intracompound semantics also supports that additive models tend to yield better results than multiplicative models (Hartung et al. 2017). The third ingredient is the measure of similarity between the compositionally constructed vector and its actual corpus-based representation. Cosine similarity is the most commonly used measure for compositionality prediction in the literature (Schone ¨ and Jurafsky 2001; Reddy, McCarthy, and Manandhar 2011; Schulte im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). Alternatively, one can calculate the overlap between the distributional neighbors of the whole phrase and those of the component words (McCarthy, Keller, and Carroll 2003), or the number of single-word distributio"
J19-1001,S13-2025,0,0.276868,"Missing"
J19-1001,W10-1810,0,0.0249222,"Missing"
J19-1001,N15-2005,0,0.060798,"Missing"
J19-1001,W14-1503,0,0.12443,"0 dimensions. • W ORD F ORM: One of the four word-form and stopword removal variants used to represent a corpus, in Section 5.1: surface+ , surface, lemma, and lemmaPoS . They represent different levels of specificity in the informational content of the tokens, and may have a language-dependent impact on the performance of compositionality prediction. 30 https://github.com/alexandres/lexvec 31 This is in line with the authors’ threshold suggestions (Salle, Villavicencio, and Idiart 2016). 32 Common window sizes are between 1+1 and 10+10, but a few works adopt larger sizes like 16+16 or 20+20 (Kiela and Clark 2014; Lapesa and Evert 2014). 18 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds 5.3 Evaluation Metrics To evaluate a compositionality prediction configuration, we calculate Spearman’s ρ rank correlation between the predicted compositionality scores (pc)s and the human compositionality scores (hc)s for the compounds that appear in the evaluation data set. We mostly use the rank correlation instead of linear correlation (Pearson) because we are interested in the framework’s ability to order compounds from least to most compositional, regardless of the actual predicted"
J19-1001,N16-1039,0,0.0487032,"Missing"
J19-1001,S14-1021,0,0.246147,"nguistics Volume 45, Number 1 • Farahmand, Smith, and Nivre (2015) collected judgments for 1,042 English noun–noun compounds. Each compound has binary judgments regarding non-compositionality and conventionalization given by four expert annotators (both native and non-native speakers). A hard threshold is applied so that compounds are considered as noncompositional if at least two annotators say so (Yazdani, Farahmand, and Henderson 2015), and the total compositionality score is given by the sum of the four binary judgments. This data set will be referred to as Farahmand in our experiments. • Kruszewski and Baroni (2014) built the Norwegian Blue Parrot data set, containing judgments for modifier-head phrases in English. The judgments consider whether the phrase is (1) an instance of the concept denoted by the head (e.g., dead parrot and parrot) and (2) a member of the more general concept that includes the head (e.g., dead parrot and pet), along with typicality ratings, with 5,849 judgments in total. • Roller, Schulte im Walde, and Scheible (2013) collected judgments for a set of 244 German noun–noun compounds, each compound with an average of around 30 judgments on a compositionality scale from 1 to 7, obtai"
J19-1001,Q14-1041,0,0.142602,"meters that connect the unity representing wi with the d-dimensional hidden layer are taken as its vector representation v(wi ). There are a number of factors that may influence the ability of a DSM to accurately learn a semantic representation. These include characteristics of the training corpus such 3 After dimensionality reduction, nowadays word vectors are often called word embeddings. 4 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds as size (Mikolov, Yih, and Zweig 2013) as well as frequency thresholds and filters (Ferret 2013; Padro´ et al. 2014b), genre (Lapesa and Evert 2014), preprocessing (Pado´ and Lapata 2003, 2007), and type of context (window vs. syntactic dependencies) (Agirre et al. 2009; Lapesa and Evert 2017). Characteristics of the model include the choice of association and similarity measures (Curran and Moens 2002), dimensionality reduction strategies (Van de Cruys et al. 2012), and the use of subsampling and negative sampling techniques (Mikolov, Yih, and Zweig 2013). However, the particular impact of these factors on the quality of the resulting DSM may be heterogeneous and depends on the task and model (Lapesa and Evert 2014). Because there is no"
J19-1001,E17-2063,0,0.0459986,"Missing"
J19-1001,Q15-1016,0,0.120596,"Missing"
J19-1001,P98-2127,0,0.361882,"processing (NLP), this is an attractive way of linearly deriving the meaning of larger units from their components, performing the semantic interpretation of any text. For representing the meaning of individual words and their combinations in computational systems, distributional semantic models (DSMs) have been widely used. DSMs are based on Harris’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to"
J19-1001,P99-1041,0,0.435749,"Missing"
J19-1001,W03-1810,0,0.420844,"Missing"
J19-1001,N13-1090,0,0.708849,"y of linearly deriving the meaning of larger units from their components, performing the semantic interpretation of any text. For representing the meaning of individual words and their combinations in computational systems, distributional semantic models (DSMs) have been widely used. DSMs are based on Harris’ distributional hypothesis that the meaning of a word can be inferred from the context in which it occurs (Harris 1954; Firth 1957). In DSMs, words are usually represented as vectors that, to some extent, capture cooccurrence patterns in corpora (Lin 1998; Landauer, Foltz, and Laham 1998; Mikolov et al. 2013; Baroni, Dinu, and Kruszewski 2014). Evaluation of DSMs has focused on obtaining accurate semantic representations for words, and state-of-the-art models are already capable of obtaining a high level of agreement with human judgments for predicting synonymy or similarity between words (Freitag et al. 2005; Camacho-Collados, Pilehvar, and Navigli 2015; Lapesa and Evert 2017) and for modeling syntactic and semantic analogies between word pairs (Mikolov, Yih, and Zweig 2013). These representations for individual words can also be combined to create representations for larger units such as phrase"
J19-1001,P08-1028,0,0.859792,"he compositional meaning of a phrase is calculated from the meanings of its parts. Third, we need the compositionality measure itself, which estimates the similarity between the compositionally constructed meaning of a phrase and its observed meaning, derived from corpora. There are a number of alternatives for each of the ingredients, and throughout this article we call a specific choice of the three ingredients a compositionality prediction configuration. Regarding the second ingredient, that is, the mathematical model of compositional meaning, the most natural choice is the additive model (Mitchell and Lapata 2008). In the additive model, the compositional meaning of a phrase P w1 w2 . . . wn is calculated as a linear combination of the word vectors of its components: i βi v(wi ), where v(wi ) is a d-dimensional vector for each word wi , and the βi coefficients assign different weights to the representation of each word (Reddy, McCarthy, and Manandhar 2011; Schulte ¨ im Walde, Muller, and Roller 2013; Salehi, Cook, and Baldwin 2015). These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy"
J19-1001,nivre-etal-2006-maltparser,0,0.0915977,"Missing"
J19-1001,P03-1017,0,0.224052,"Missing"
J19-1001,J07-2002,0,0.190255,"Missing"
J19-1001,padro-etal-2014-comparing,1,0.907159,"Missing"
J19-1001,D14-1047,1,0.898559,"Missing"
J19-1001,D14-1162,0,0.0803558,"Missing"
J19-1001,P16-2026,1,0.883726,"de only a brief introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,W16-1804,1,0.852714,"de only a brief introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,I11-1024,0,0.139794,"Missing"
J19-1001,W09-2907,0,0.0942899,"Missing"
J19-1001,D15-1290,0,0.0208105,"Missing"
J19-1001,W14-0818,0,0.0654706,"Missing"
J19-1001,W13-1005,0,0.257755,"Missing"
J19-1001,E14-1050,0,0.103733,"Missing"
J19-1001,N15-1099,0,0.258868,"requent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compound vector representa"
J19-1001,W15-0909,0,0.117951,"requent in languages (Sag et al. 2002), identifying idiomaticity and producing accurate semantic representations for compositional and idiomatic cases is of relevance to NLP tasks and applications that involve some form of semantic processing, including semantic parsing (Hwang et al. 2010; Jagfeld and van der Plas 2015), word sense disambiguation (Finlayson and Kulkarni 2011; Schneider et al. 2016), 1 Attributed to Frege (1892/1960). 2 Cordeiro et al. Unsupervised Compositionality Prediction of Nominal Compounds and machine translation (Ren et al. 2009; Carpuat and Diab 2010; Cap et al. 2015; Salehi et al. 2015). Moreover, the evaluation of DSMs on tasks involving MWEs, such as compositionality prediction, has the potential to drive their development towards new directions. The main hypothesis of our work is that, if the meaning of a compositional nominal compound can be derived from a combination of its parts, this translates in DSMs as similar vectors for a compositional nominal compound and for the combination of the vectors of its parts using some vector operation, that we refer to as composition function. Conversely we can use the lack of similarity between the nominal compound vector representa"
J19-1001,P16-2068,1,0.899605,"Missing"
J19-1001,S16-1084,0,0.507599,"introduction here, underlining their most relevant characteristics to our framework (Section 2.1). Then, we define compositionality prediction and discuss existing approaches, focusing on distributional techniques for multiword expressions (Section 2.2). Our framework is evaluated on nominal compounds, and we discuss their relevant properties (Section 2.3) along with existing data sets for evaluating compositionality prediction (Section 2.4). 2 This article significantly extends and updates previous publications: 1. We consolidate the description of the data sets introduced in Ramisch et al. (2016) and Ramisch, Cordeiro, and Villavicencio (2016) by adding details about data collection, filtering, and results of a thorough analysis studying the correlation between compositionality and related variables. 2. We extend the compositionality prediction framework described in Cordeiro, Ramisch, and Villavicencio (2016) by adding and evaluating new composition functions and DSMs. 3. We extend the evaluation reported in Cordeiro et al. (2016) not only by adding Portuguese, but also by evaluating additional parameters: corpus size, composition functions, and new DSMs. 3 Computational Linguistics"
J19-1001,W01-0513,0,0.0987547,"Missing"
J19-1001,L16-1362,0,0.115467,"ether the phrase is (1) an instance of the concept denoted by the head (e.g., dead parrot and parrot) and (2) a member of the more general concept that includes the head (e.g., dead parrot and pet), along with typicality ratings, with 5,849 judgments in total. • Roller, Schulte im Walde, and Scheible (2013) collected judgments for a set of 244 German noun–noun compounds, each compound with an average of around 30 judgments on a compositionality scale from 1 to 7, obtained through crowdsourcing. The resource was later enriched with feature norms (Roller and Schulte im Walde 2014). • Schulte im Walde et al. (2016) collected judgments for a set of 868 German noun–noun compounds, including human judgments of compositionality on a scale of 1 to 6. Compounds are judged by multiple annotators, and the final compositionality score is the average across annotators. The data set is also annotated for in-corpus frequency, productivity, and ambiguity, and a subset of 180 compounds has been selected for balancing these variables. The annotations were performed by the authors, linguists, and through crowdsourcing. For the balanced subset of 180 compounds, compositionality annotations were performed by experts only"
J19-1001,S13-1038,0,0.0425081,"Missing"
J19-1001,D12-1110,0,0.265383,"Missing"
J19-1001,J13-4009,0,0.0755415,"Missing"
J19-1001,C12-1165,0,0.0732893,"Missing"
J19-1001,D15-1201,0,0.116679,"Missing"
L16-1194,P13-4006,0,0.0285428,"; Ramisch, 2015). Meaning composition for MWEs requires accurate meaning representation of single words. To date, many models have been proposed for representing the lexical semantics of single words. We focus on distributional models, based on Harris’ distributional hypothesis. DSMs have 1221 1 http://mwetoolkit.sf.net been around for a while (Landauer and Dumais, 1997). However, the recent enthusiasm about neural networks and word embeddings has made DSMs more accurate and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the"
L16-1194,W15-0904,0,0.493606,"the mwetoolkit that estimates semantic compositionality scores for multiword expressions (MWEs) based on word embeddings. First, we describe our implementation of vector-space operations working on distributional vectors. The compositionality score is based on the cosine distance between the MWE vector and the composition of the vectors of its member words. Our generic system can handle several types of word embeddings and MWE lists, and may combine individual word representations using several composition techniques. We evaluate our implementation on a dataset of 1042 English noun compounds (Farahmand et al., 2015), comparing different configurations of the underlying word embeddings and word-composition models. We show that our vector-based scores model non-compositionality better than standard association measures such as log-likelihood. Keywords: Lexical semantics, multiword expressions, compositionality, word embeddings. 1. Introduction Multiword expressions (MWEs) are often defined as word combinations “whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components” (Choueka, 1988). A broader definition of MWEs considers that, beyond"
L16-1194,W13-1017,0,0.0271844,"on measures used in the literature to compare the model predictions with human judgments. • Spearman’s Rho (ρ): measures correlation between the ranks provided by the model predictions and by human judgments. • Normalized Discounted Cumulative Gain (NDCG): a precision measure that penalizes more intensely wrong predictions at the top of the ranking. • Best F-score (F1 ): the highest F-score considering the first k predictions, for all values of k. • Precision at k (P@k): precision for the top k predictions. • Average precision (AP): average of precision calculated at each relevant prediction (Gurrutxaga and Alegria, 2013). Our dataset contains four binary judgments per compound. For ρ, we use the sum of the binary judgments to rank the compounds. For NDCG, F1 , P@k and AP, a compound is considered relevant (i.e. conventional or noncompositional) if at least two judges consider it relevant (Yazdani et al., 2015). 5. Results Table 1 presents the results when evaluating the predictive ability of different models concerning conventionality. Except for the baseline, all of the other models use a 5 Since our model predicts compositionality, and the human judges annotated non-compositionality, we inverted the model p"
L16-1194,Q15-1016,0,0.077608,"tional models, as well as different word combination weights. We train an instance of each of these distributional semantic models: minimantics, word2vec (cbow) and GloVe. For training, we feed an MWE-annotated corpus where MWEs are joined as a single token as in bounty_hunter), as performed by Ferret (2014). We fix the following parameters: • Corpus: UKWaC, containing 2G words of English texts crawled from the web (Baroni et al., 2009); • Context window: lemma of each content word 8 words to the left/right of the target;   • Context weight decay: linear, that is, 88 , 78 , 68 , . . . , 18 (Levy et al., 2015); • Dimensions per embedding: 250. We train an additional model, minimanticsB , with a window of size 1 and dimension of 500, to verify the impact of the parameters. We compare our model for compositionality prediction with a simple baseline that uses the loglikelihood (LL) association score. LL compares an MWE frequency with the frequency of each component word. We implemented several evaluation measures used in the literature to compare the model predictions with human judgments. • Spearman’s Rho (ρ): measures correlation between the ranks provided by the model predictions and by human judgm"
L16-1194,D14-1162,0,0.102269,"ntation of single words. To date, many models have been proposed for representing the lexical semantics of single words. We focus on distributional models, based on Harris’ distributional hypothesis. DSMs have 1221 1 http://mwetoolkit.sf.net been around for a while (Landauer and Dumais, 1997). However, the recent enthusiasm about neural networks and word embeddings has made DSMs more accurate and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination wit"
L16-1194,I11-1024,0,0.0490926,"and faster to build using very large corpora. Many tools are nowadays available for building word embeddings, like Dissect (Dinu et al., 2013), minimantics2 , word2vec3 (Mikolov et al., 2013) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_composi"
L16-1194,N15-1099,0,0.155378,"13) and Glove4 (Pennington et al., 2014). Modeling semantic compositionality in DSMs is a hot topic in NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_compositionality, described in Figure 1. It outputs a compositionality score for each MWE in a list of input candidates, based on an input word-embeddings file. Figure 1: Overview of feat_composit"
L16-1194,D15-1201,0,0.247861,"n NLP. As word meanings can be represented as vectors, composition can be effectively modeled through simple operations like vector addition and multiplication (Mitchell and Lapata, 2010). Some authors have proposed models for estimating the degree of semantic idiomaticity of MWEs, focusing on noun compounds. Reddy et al. (2011) suggest a compositionality measure which is the cosine similarity between the MWE vector and the sum of the vectors of the component words. This model was also used by Salehi et al. (2015), in combination with word translation information coming from parallel corpora. Yazdani et al. (2015) propose and evaluate more sophisticated composition functions, based on linear, non-linear and neural network projections. The mwetoolkit+sem framework is based on vector addition and cosine similarity. 3. The mwetoolkit+sem Framework For the semantic processing, we developed a new tool, called feat_compositionality, described in Figure 1. It outputs a compositionality score for each MWE in a list of input candidates, based on an input word-embeddings file. Figure 1: Overview of feat_compositionality → representing each The first step combines the vectors − w i word wi in an MWE. The appropri"
L16-1363,P14-1070,0,0.0147402,"ation of DeQue takes place in the context of the development of a statistical dependency parser for French (Nasr et al., 2011). The need to quantify ambiguity has a practical consequence: unambiguous constructions can be included in the lexicon as frozen multiword tokens, while ambiguous ones need to be annotated and dealt with at parsing time. One way of disambiguating ambiguous multiword units is to keep the tokens as individual lexical units during tokenisation and POS tagging, and then use special syntactic dependencies to indicate the presence of a CPRE or a CCONJ (McDonald et al., 2013; Candito and Constant, 2014; Green et al., 2013). In previous experiments, we demonstrated that this approach is superior to treating all units systematically as words with spaces (Nasr et al., 2015). However, this was only demonstrated for a small set of 8 CCONJs and 4 determiners in French. The present work substantially extends the coverage of the list of potentially ambiguous constructions that can be modelled using that approach. In the remainder of this paper, we discuss the general properties and syntactic behaviour of prepositions and constructions in French (§ 2.). Then, we present the criteria (§ 3.) and metho"
L16-1363,J13-1009,0,0.0691578,"Missing"
L16-1363,P13-2017,0,0.0245369,"wrong analysis. The creation of DeQue takes place in the context of the development of a statistical dependency parser for French (Nasr et al., 2011). The need to quantify ambiguity has a practical consequence: unambiguous constructions can be included in the lexicon as frozen multiword tokens, while ambiguous ones need to be annotated and dealt with at parsing time. One way of disambiguating ambiguous multiword units is to keep the tokens as individual lexical units during tokenisation and POS tagging, and then use special syntactic dependencies to indicate the presence of a CPRE or a CCONJ (McDonald et al., 2013; Candito and Constant, 2014; Green et al., 2013). In previous experiments, we demonstrated that this approach is superior to treating all units systematically as words with spaces (Nasr et al., 2015). However, this was only demonstrated for a small set of 8 CCONJs and 4 determiners in French. The present work substantially extends the coverage of the list of potentially ambiguous constructions that can be modelled using that approach. In the remainder of this paper, we discuss the general properties and syntactic behaviour of prepositions and constructions in French (§ 2.). Then, we present t"
L16-1363,P11-4015,1,0.75139,"ches in French. A simplistic approach such as treating all occurrences of bien que as a single word with spaces inside would introduce an error for sentences like example 2. Conversely, ignoring it in example 1 would mean that both words are treated independently, not capturing the fact that the whole behaves like a conjunction. And what is more, these errors would be propagated to the following processing steps like POS tagging and parsing, certainly generating a wrong analysis. The creation of DeQue takes place in the context of the development of a statistical dependency parser for French (Nasr et al., 2011). The need to quantify ambiguity has a practical consequence: unambiguous constructions can be included in the lexicon as frozen multiword tokens, while ambiguous ones need to be annotated and dealt with at parsing time. One way of disambiguating ambiguous multiword units is to keep the tokens as individual lexical units during tokenisation and POS tagging, and then use special syntactic dependencies to indicate the presence of a CPRE or a CCONJ (McDonald et al., 2013; Candito and Constant, 2014; Green et al., 2013). In previous experiments, we demonstrated that this approach is superior to tr"
L16-1363,P15-1108,1,0.900374,"Missing"
L16-1363,sagot-2010-lefff,0,0.198248,"and complex conjunctions (CCONJ) are two types of function words that consist of more than one orthographic word (Piot, 1993). They can be considered as fixed multiword expressions that allow little or no variability. Examples in English include CCONJs even though, as well as and CPREs up to and in front of. Examples in French are shown in Table 1 along with their English (EN) meaningful and literal translations. CPRE and CCONJ constructions are quite frequent in French. Their linguistic description in the literature is generally limited to building comprehensive lists of such constructions (Sagot, 2010). Most authors assume that these constructions allow no or very little variability (inflection, insertion). Therefore, they would not require a very sophisticated description and representation in machine-readable lexicons and NLP systems, such as the ones required for verbs, for instance (Dubois and Dubois-Charlier, 2004). An aspect which is often neglected is the segmentation and structural ambiguity that arises when the words composing the complex function word co-occur by pure chance. Consider examples 1 and 2 containing the French CCONJ bien que. It is composed by the words bien (well) an"
laranjeira-etal-2014-comparing,P02-1040,0,\N,Missing
laranjeira-etal-2014-comparing,P07-2045,0,\N,Missing
laranjeira-etal-2014-comparing,J10-4006,0,\N,Missing
laranjeira-etal-2014-comparing,W12-3904,1,\N,Missing
laranjeira-etal-2014-comparing,D13-1147,0,\N,Missing
N18-2068,baldwin-etal-2004-road,0,0.0283401,"Jacquemin, 2001) and hinder straightforward search of MWE citation forms in a corpus (Nissim and Zaninello, 2013). They introduce discontinuities which challenge sequence labeling approaches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variabilit"
N18-2068,W17-1727,0,0.0369668,"Missing"
N18-2068,J17-4005,1,0.880083,"Missing"
N18-2068,W16-1809,0,0.0681629,"Missing"
N18-2068,J14-2007,0,0.0428445,"ches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper presents measures of VMWE variabili"
N18-2068,J09-1005,0,0.716401,"fer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper presents measures of VMWE variability based on variant-to-variant similarity, taking synOne of the outstanding properties of multiword expressions (MWEs), especially verbal ones (VMWEs), important both in theoretical models and applications, is their idiosyncratic variability. Some MWEs are always continuous, while some others admit certain types of insertions. Components of some MWEs are rarely or ne"
N18-2068,weller-heid-2010-extraction,0,0.0227707,"equence labeling approaches. Even when employing parsers to cope with discontinuities, MWE recognizers can still fail to capture some syntactic transformations such as complex determiners, which can break a direct link between a verb and a noun in a dependency tree (pay a series of visits). These facts have important implications for downstream tasks and applications, e.g. parsers can heavily suffer from incorrectly identified MWEs (Baldwin et al., 2004). The restricted variability of MWEs as compared to their regular counterparts can also be seen as an advantage in their automatic discovery (Weller and Heid, 2010; Tsvetkov and Wintner, 2014; Buljan ˇ and Snajder, 2017). Substitution-based MWE discovery techniques based on lexico-semantic variability have been largely explored (Pearce, 2001; Farahmand and Henderson, 2016). Morphological and syntactic variability, however, have rarely been studied for MWE discovery (Ramisch et al., 2008) and even less so for in-context identification (Fazly et al., 2009). Given the importance of MWE variability (Constant et al., 2017) as well as its gradual nature, especially for VMWEs, we suggest that this phenomenon should be subject to measurement. This paper present"
N19-1393,J19-2006,0,0.0420386,"Missing"
N19-1393,D14-1082,0,0.0401592,"that are difficult to assess and would have prevented 1 2 https://wals.info/ http://universaldependencies.org 3919 Proceedings of NAACL-HLT 2019, pages 3919–3930 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics to measure the precise influence of the typological features on the behaviour of the parser. The fourth issue concerns the parser, which must be language independent and produce syntactic trees based on combinations of parameter values and sentential configurations. We use a transition-based parser with a multi-layer perceptron classifier (Chen and Manning, 2014), responsible for proposing how parameter values match observable patterns in the data. Our research hypotheses are: (a) features derived from the WALS enable cross-lingual sharing in multilingual parsing, and (b) these features do more than acting as mere language identifiers. Our main contributions are to reassess the utility of the WALS as informant of typological features of parsed languages, to evaluate their benefit in a controlled multilingual setting with full supervision, and to perform a set of analyses to better understand how they interact with the parser model. In addition to mult"
N19-1393,C12-1059,0,0.0406222,"Missing"
N19-1393,2016.jeptalnrecital-invite.2,0,0.0666192,"for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification. 1 Introduction Human languages may share some syntactic features, but differ on others. For example, some languages tend to place the subject before the verb (e.g., English) whereas others favour the reverse order (e.g., Arabic), and some do not exhibit a clear preference (e.g., Polish). These features can be viewed as the parameters of a language’s syntax (Greenberg, 1963; Chomsky, 1995). When training a multilingual parser, it could be interesting to explicitly represent these parameters, and to integrate them into the parsing model. If a successful strategy to do so was found, then, a parser could be trained simultaneously on several languages whose syntactic parameters have been explicitly represented. Such parser could then use a single model to parse texts in any language with known syntactic parameters. In theory, if we had at our disposal a set of parameters that completely describes the syntax of languages as well as treebanks that explore the whole sp"
N19-1393,P15-1119,0,0.0526325,": (a) features derived from the WALS enable cross-lingual sharing in multilingual parsing, and (b) these features do more than acting as mere language identifiers. Our main contributions are to reassess the utility of the WALS as informant of typological features of parsed languages, to evaluate their benefit in a controlled multilingual setting with full supervision, and to perform a set of analyses to better understand how they interact with the parser model. In addition to multilingual parsing, our method is suitable for zero-shot learning for under-resourced languages (Ammar et al., 2016; Guo et al., 2015). After discussing related work (Sec. 2), we describe UD (Sec. 3), the WALS (Sec. 4) and our parser (Sec. 5). The experimental setup (Sec. 6) precedes our results (Sec. 7), analyses (Sec. 8) and conclusions (Sec. 9). 2 Related Work Our work is at the intersection of three trends in the multilingual dependency parsing literature. The first is transfer parsing, when a parser is trained on a language (or a collection of languages) and tested on another one. The second is delexicalised parsing, which aims at abstracting away from the lexicon in order to neutralise genre, domain and topic biases wh"
N19-1393,N16-1121,0,0.0901739,"’ vocabulary. The third trend is the use of a handcrafted typological resources, such as the WALS, in multilingual NLP methods. Transfer parsing is often a suitable solution when dealing with low-resource languages (McDonald et al., 2011). Projected transfer relies on parallel corpora in which one of the languages does not have labelled training data to learn a parser, but the other does. One commonly employed solution is to use word alignments to project parsed sentences from one side onto the low-resource side of the parallel text, using heuristics (Hwa et al., 2005) or partial annotations (Lacroix et al., 2016). Agi´c et al. (2016) parse the resource-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on lan"
N19-1393,W14-4606,0,0.0214781,"source-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on language similarities. For instance, Lynn et al. (2014) build parser for Irish by first training a delexicalised parser on another language, and then applying it on Irish. They surprisingly found out that Indonesian was the language providing the best parsing results for Irish, even if they do not belong to the same language family, because longdistance dependencies are better represented in Indonesian than in the other languages tested. Low-resource languages may have some (insufficient) amount of training material available. One can employ bilingual parsing, concatenating training corpora in two languages, to verify if there is an improvement in"
N19-1393,P14-1126,0,0.0930726,"low-resource side of the parallel text, using heuristics (Hwa et al., 2005) or partial annotations (Lacroix et al., 2016). Agi´c et al. (2016) parse the resource-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on language similarities. For instance, Lynn et al. (2014) build parser for Irish by first training a delexicalised parser on another language, and then applying it on Irish. They surprisingly found out that Indonesian was the language providing the best parsing results for Irish, even if they do not belong to the same language family, because longdistance dependencies are better represented in Indonesian than in the other languages tested. Low-resource languages may have some (insufficient) amoun"
N19-1393,D11-1006,0,0.166397,"Missing"
N19-1393,P12-1066,0,0.589557,"then use a single model to parse texts in any language with known syntactic parameters. In theory, if we had at our disposal a set of parameters that completely describes the syntax of languages as well as treebanks that explore the whole space of parameters and their values, then such a universal parser could be designed. To make such a program realistic, though, several issues have to be addressed. In this paper, we propose to study the feasibility of learning such multilingual parser by addressing some of these issues. The first one is the choice of syntactic parameters that will be used (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015). In our work, we approximate these parameters by extracting syntactic information from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). 1 A language is represented by a vector containing the values it selects in the WALS. This vector plays the role of the parameters mentioned above. The second issue is the design of a unified scheme for representing syntax. Our natural choice is the Universal Dependencies (UD) initiative. 2 UD specifically proposes a set of universal dependency relations, part-of-speech tags and m"
N19-1393,J08-4003,0,0.0903223,"Missing"
N19-1393,P05-1013,0,0.103528,"Missing"
N19-1393,E17-2102,0,0.0315184,"the WALS provide information about the structure of languages (Dryer and Haspelmath, 2013). These could be useful to guide multilingual parsers, informing them about the model parameters that can be shared among languages with similar characteristics. Naseem et al. (2012) and Zhang and Barzilay (2015) use word-order features available for all their languages, while Ponti et al. (2018) used features they judged relevant in many categories (not only word order). The parameters proposed in the WALS are not the only way to represent properties of languages. Meth¨ ods based on language embeddings (Ostling and Tiedemann, 2017; Bjerva et al., 2019) also constitute interesting language representation. T¨ackstr¨om et al. (2013) use a multilingual delexicalised transfer method, showing how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based dependency parser. They select the typological features based on those used by Naseem et al. (2012), removing two features not considered useful. The work closest to ours experimented with concatenating treebanks to train a multilingual parser (Ammar et al., 2016). The authors use an S-LSTM t"
N19-1393,P18-1142,0,0.133767,"Missing"
N19-1393,P17-1049,0,0.0583782,"Missing"
N19-1393,N13-1126,0,0.187912,"Missing"
N19-1393,W15-2137,0,0.094949,"te training corpora. However, in our case, we combine treebanks from many more sources (around 40 languages) and include typological features. The combination of corpora in multiple languages for parser training is facilitated by the recent advent of multilingual standards and resources, in particular in Universal Dependencies for dependency syntax (Nivre et al., 2016). This initiative enables the annotation of POS, morphology and syntactic dependencies for all languages with the same guidelines and label sets. The availability of such corpora favours the development of cross-lingual methods (Tiedemann, 2015). Multilingual parsing research is also encouraged by initiatives such as the CoNLL 2017 and 2018 shared tasks, on highly multilingual dependency parsing from raw text (Zeman et al., 2017, 2018). Delexicalised parsers ignore the word forms and lemmas when analysing a sentence, usually relying on more abstract features such as word classes 3920 and POS tags. The use of delexicalised parsers is especially relevant when learning multilingual parsers, since languages generally share only a limited amount of lexical units. The approach proposed by Zeman and Resnik (2008) consists in adapting a pars"
N19-1393,K17-3001,0,0.157189,"guages for parser training is facilitated by the recent advent of multilingual standards and resources, in particular in Universal Dependencies for dependency syntax (Nivre et al., 2016). This initiative enables the annotation of POS, morphology and syntactic dependencies for all languages with the same guidelines and label sets. The availability of such corpora favours the development of cross-lingual methods (Tiedemann, 2015). Multilingual parsing research is also encouraged by initiatives such as the CoNLL 2017 and 2018 shared tasks, on highly multilingual dependency parsing from raw text (Zeman et al., 2017, 2018). Delexicalised parsers ignore the word forms and lemmas when analysing a sentence, usually relying on more abstract features such as word classes 3920 and POS tags. The use of delexicalised parsers is especially relevant when learning multilingual parsers, since languages generally share only a limited amount of lexical units. The approach proposed by Zeman and Resnik (2008) consists in adapting a parser for a new related language using either parallel corpora or delexicalised parsing. This method can be used to quickly construct a parser if the source and target languages are sufficie"
P15-1108,C10-1011,0,0.0338988,"ost simple one is the arcfactored or first-order model, which simply decomposes a tree into single dependencies and assigns them a score, independently of their context. We used a second-order parser which decomposes a tree into factors of three types: 1. first-order factors, made of one dependency; 2. sibling factors, made of two dependencies sharing a common governor; 3. grandchildren factors, made of two dependencies where the dependent of one of them is the governor of the other one. 5 Integration with a Syntactic Lexicon Although this kind of parsers achieve state-of-theart performances (Bohnet, 2010), their predictions are limited to the phenomena that occur in the treebanks they are trained on. In particular, they often fail at correctly distinguishing elements that are subcategorized by a verb (henceforth complements) from others (modifiers). This is due to the fact that the nature and number of the complements is specific to each verb. If the verb did not occur, or did not occur often enough, in the treebank, the nature and number of its complements will not be correctly modeled by the parser. A precise description of verb complements plays an important role in the task of predicting t"
P15-1108,P14-1070,0,0.338931,"our model on ADV+que and de+DET constructions. 3 The MORPH Dependency In order to let the parser take the tokenization decisions, we propose not to group sequences of tokens of the form ADV+que and de+DET at tokenization time. Instead, we transform the task of segmentation decision into a parsing decision task. We associate a syntactic structure to ADV+que and de+DET constructions by introducing a new type of dependency that we call MORPH. It is not a standard syntactic dependency, but a reminiscent of the morphological dependencies of Mel’ˇcuk (1988), similar to the DEP CPD label proposed by Candito and Constant (2014) or the ID dependency of Nivre and Nilsson (2004), except that we focus on syntactically-motivated MWEs, proposing a regular structure for them. The syntactic structures of examples 1 and 2, introduced in Section 1, are represented below4 . Example 1. MOD SUJ CLS Je OBJ MORPH VRB ADV mange bien CSU que ... ... VRB ... ... VRB ... ... aie Example 2. OBJ SUJ CLS Je 4 OBJ MOD VRB pense ADV bien CSU que ... ... ai In the examples, parts of speech CLS, VRB, ADV and CSU respectively stand for subject clitic pronoun, verb, adverb and subordinating conjunction. Syntactic labels SUJ, MOD, OBJ, DE - OBJ"
P15-1108,2009.jeptalnrecital-long.4,0,0.132562,"Missing"
P15-1108,W11-0809,0,0.233442,"Missing"
P15-1108,W13-4905,0,0.184526,"Missing"
P15-1108,W13-4906,0,0.0305705,"oices are difficult to make without 1 This paper considers dependency syntactic structures. taking syntax into account. To avoid the pitfall of premature decisions, probabilistic tokenizers and taggers can produce several solutions in the form of lattices (Green and Manning, 2010; Goldberg and Elhadad, 2011). Such approaches usually lead to severe computational overhead due to the huge search space in which the parser looks for the optimal parse tree. Besides, the parser might be biased towards short solutions, as it compares scores of trees associated to sequences of different lengths (De La Clergerie, 2013). This problem is particularly hard when parsing multiword expressions (MWEs), that is, groups of tokens that must be treated as single units (Baldwin and Kim, 2010). The solution we present in this paper is different from the usual pipeline. We propose to jointly parse and tokenize MWEs, transforming segmentation decisions into linking decisions. Our experiments concentrate on two difficult tokenization cases. Hence, it is the parser that will choose, in such cases, whether to group or not several tokens. Our first target phenomenon is the family of ADV+que constructions, a type of complex co"
P15-1108,P11-2124,0,0.0242174,"entation into linguistically relevant units (words), tagging the words with POS tags and linking the (word, POS) pairs by means of syntactic dependencies. This setup is clearly not ideal, as some decisions are made too early in the pipeline (Branco and Silva, 2003). More specifically, some tokenization and tagging choices are difficult to make without 1 This paper considers dependency syntactic structures. taking syntax into account. To avoid the pitfall of premature decisions, probabilistic tokenizers and taggers can produce several solutions in the form of lattices (Green and Manning, 2010; Goldberg and Elhadad, 2011). Such approaches usually lead to severe computational overhead due to the huge search space in which the parser looks for the optimal parse tree. Besides, the parser might be biased towards short solutions, as it compares scores of trees associated to sequences of different lengths (De La Clergerie, 2013). This problem is particularly hard when parsing multiword expressions (MWEs), that is, groups of tokens that must be treated as single units (Baldwin and Kim, 2010). The solution we present in this paper is different from the usual pipeline. We propose to jointly parse and tokenize MWEs, tra"
P15-1108,C10-1045,0,0.0851174,"Missing"
P15-1108,J13-1009,0,0.204866,"Missing"
P15-1108,W11-0818,0,0.117479,"Missing"
P15-1108,Q14-1016,0,0.201312,"Missing"
P15-1108,D07-1110,1,0.726501,"Missing"
P15-1108,P13-2046,0,0.0386249,", while the latter can significantly improve parsing accuracy and robustness in general. However, the authors admit that “it remains to be seen how much of theoretically possible improvement can be realized when using automatic methods for MWU recognition”. Several methods of increasing complexity have been proposed for fully automatic MWE tokenization: simple lexicon projection onto a corpus (Kulkarni and Finlayson, 2011), synchronous lexicon lookup and parsing (Wehrli et al., 2010; Seretan, 2011), token-based classifiers trained using 1117 association measures and other contextual features (Vincze et al., 2013a), or contextual sequence models like conditional random fields (Constant and Sigogne, 2011; Constant et al., 2013b; Vincze et al., 2013b) and structured perceptron (Schneider et al., 2014). In theory, compound function words like ADV+que and de+DET allow no internal variability, thus they should be represented as words-with-spaces. However, to date no satisfactory solution has been proposed for automatically tokenizing ambiguous MWEs. Green et al. (2013) propose a constituency parsing model which, as a by-product, performs MWE identification. They propose a flat representation for contiguous"
P15-1108,I13-1024,0,0.528563,", while the latter can significantly improve parsing accuracy and robustness in general. However, the authors admit that “it remains to be seen how much of theoretically possible improvement can be realized when using automatic methods for MWU recognition”. Several methods of increasing complexity have been proposed for fully automatic MWE tokenization: simple lexicon projection onto a corpus (Kulkarni and Finlayson, 2011), synchronous lexicon lookup and parsing (Wehrli et al., 2010; Seretan, 2011), token-based classifiers trained using 1117 association measures and other contextual features (Vincze et al., 2013a), or contextual sequence models like conditional random fields (Constant and Sigogne, 2011; Constant et al., 2013b; Vincze et al., 2013b) and structured perceptron (Schneider et al., 2014). In theory, compound function words like ADV+que and de+DET allow no internal variability, thus they should be represented as words-with-spaces. However, to date no satisfactory solution has been proposed for automatically tokenizing ambiguous MWEs. Green et al. (2013) propose a constituency parsing model which, as a by-product, performs MWE identification. They propose a flat representation for contiguous"
P15-1108,W10-3705,0,0.543959,"Missing"
P15-1108,N10-1089,0,\N,Missing
P16-1187,W03-1812,0,0.0436116,"ment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relati"
P16-1187,J10-4006,0,0.0261791,"such as level of corpus preprocessing, context window size and number of dimensions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived from their parts according to the language’s gr"
P16-1187,P14-1023,0,0.0505953,"have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more"
P16-1187,P15-2001,0,0.0650836,"Missing"
P16-1187,S13-2025,0,0.164865,"Missing"
P16-1187,P02-1030,0,0.182521,"Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The"
P16-1187,P06-2064,0,0.0247328,"achine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality prediction of nominal compounds in French and English. We examine parameters like the level of corpus preprocessing, the size of the context window and the number of dimensions for context representation. Additionally, we compare standard DSMs based on positive pointwise mutual information (PPMI) 198"
P16-1187,P13-4006,0,0.0113018,"ant contexts (highest PPMI) for each target. No further dimensionality reduction is applied. In PPMI-TopK, we use a fixed global list of 1000 contexts, built by looking at the most frequent words in the corpus: the top 50 are skipped, and the next 1000 are taken (Salehi et al., 2015). No further dimensionality reduction is applied. 5 Syntactic context definition is planned as future work. 6 PPMI vectors are built using minimantics https:// github.com/ceramisch/minimantics. In PPMI-SVD, for each target, contexts that appear less than 1000 times are discarded.7 We then use the Dissect toolkit8 (Dinu et al., 2013) in order to build a PPMI matrix and reduce its dimensionality using singular value decomposition (SVD) to factorize the matrix. w2v Uses the word2vec toolkit based on neural networks to predict target/context cooccurrence (Mikolov et al., 2013a). We build models from two variants of word2vec: CBOW (w2v-cbow) and skipgram (w2v-sg). In both cases, the configurations are the default ones, except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations. We use the default minimum word count threshold of 5. glove W"
P16-1187,Q14-1041,0,0.0402451,"ality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures"
P16-1187,P10-2017,0,0.0168323,"Missing"
P16-1187,W15-0904,0,0.141369,"in cash cow versus tears in crocodile tears. Indeed, various annotation scales have been proposed as means to collect human judgments about compositionality. Particularly for nominal compounds, Reddy et al. (2011) used a 6-point scale to collect judgments on the literal or figurative use of nominal compounds and its components in English. Similar judgments have also been collected for 244 German compounds, for which an average of 30 judgments on a scale from 1 to 7 were gathered through crowdsourcing (Roller et al., 2013). An alternative to multi-point scales is the binary judgment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2"
P16-1187,P13-1055,0,0.0503805,"n (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been stud"
P16-1187,W05-0604,0,0.119621,"In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM. Yazdani et al. (2015) focus on the composition function, using a lightly supervised neural network to learn the best combination strategy for individual word vectors. In order to consolidate previous punctual results, we p"
P16-1187,Q15-1016,0,0.279976,"(Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and French. For the choice of particular DSM, contradictory results have been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM."
P16-1187,P99-1041,0,0.169934,"impact of different parameters, such as level of corpus preprocessing, context window size and number of dimensions. The results obtained have a high correlation with human judgments, being comparable to or outperforming the state of the art for some datasets (Spearman’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived f"
P16-1187,W03-1810,0,0.0717748,".g. silver bullet, eager beaver). Precision-oriented NLP applications need to be able to identify partly-compositional and idiomatic cases and ensure meaning preservation during processing. Compositionality identification is a first step towards complete semantic interpretation in tasks such as machine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality pre"
P16-1187,N13-1090,0,0.623429,"an’s ρ=.82 for the Reddy dataset). 1 Introduction Distributional semantic models (DSMs) use context information to represent the meaning of lexical units as vectors. They normally focus on the accurate semantic representation of single words. It is based on single words that many optimizations for these models have been proposed (Lin, 1999; Erk and Pad´o, 2010; Baroni and Lenci, 2010). This is particularly true for word embeddings, that is, a type of DSM where distributional vectors are obtained as a by-product of training a neural network to learn a function between words and their contexts (Mikolov et al., 2013a). Simultaneously, there has been intensive research on models to compose individual word vectors in order to create representations for larger units such as phrases, sentences and even whole documents (Mitchell and Lapata, 2010; Mikolov et al., 2013a). Larger units can often be assumed to have their meanings derived from their parts according to the language’s grammar, but this is not always the case (Sag et al., 2002). Many multiword units are associated with idiomatic interpretations, unrelated to the meaning of the component words (e.g. silver bullet, eager beaver). Precision-oriented NLP"
P16-1187,P08-1028,0,0.124367,"l., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context"
P16-1187,P03-1017,0,0.394971,"Missing"
P16-1187,J07-2002,0,0.0940225,"Missing"
P16-1187,D14-1162,0,0.096709,"iction of nominal compounds in French and English. We examine parameters like the level of corpus preprocessing, the size of the context window and the number of dimensions for context representation. Additionally, we compare standard DSMs based on positive pointwise mutual information (PPMI) 1986 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1986–1997, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics against widely used word embedding tools such as word2vec, henceforth w2v (Mikolov et al., 2013c), and GloVe (Pennington et al., 2014). We start with a discussion of related work (§2) and the materials and methods used (§3). We report on the evaluations performed (§4) and finish with conclusions and future work (§5). 2 Related Work We define nominal compounds as conventional noun phrases composed by two or more words, such as science fiction (Nakov, 2013). In English, they are often expressed as noun compounds but their syntactic realization may vary for different languages. For instance, one of the equivalent forms in French involves a denominal adjective used as modifier (e.g. cell death and the corresponding mort cellulai"
P16-1187,P16-2026,1,0.898254,"r 90 compounds and their individual word components, in a scale of literality from 0 (idiomatic) to 5 (literal), collected with Mechanical Turk (Reddy et al., 2011). For each compound, compositionality scores are averaged over its annotators. Compounds included in the dataset were selected to balance frequency range and degree of compositionality (low, middle and high). We use only the global compositionality score, ignoring individual word judgments. With a few exceptions (e.g. sacred cow), most compounds are formed exclusively by nouns. Reddy++ is a new resource created for this evaluation (Ramisch et al., 2016). It extends the Reddy set with an additional 90 English nominal compounds, in a total of 180 entries. Scores also range from 0 to 5 and were collected through Mechanical Turk and averaged over the annotators. The extra 90 entries include some adjective-noun compounds and are balanced with respect to frequency and compositionality. We focus our evaluation on this combined dataset, since it includes Reddy. However, to allow comparison with state of the art, we also report results individually for Reddy. Farahmand contains 1042 English compounds extracted from Wikipedia with binary noncompositio"
P16-1187,I11-1024,0,0.213183,"r beaver). Precision-oriented NLP applications need to be able to identify partly-compositional and idiomatic cases and ensure meaning preservation during processing. Compositionality identification is a first step towards complete semantic interpretation in tasks such as machine translation (to translate non-compositional compounds as a unit), word sense disambiguation (to avoid assigning a sense to parts of non-compositional compounds), and semantic parsing (to identify complex predicates and their arguments). Even when larger units are explicitly represented in DSMs (McCarthy et al., 2003; Reddy et al., 2011; Mikolov et al., 2013c; Ferret, 2014), it is not clear whether the quality of these representations is comparable to the representations of single words. In particular, when building vectors for larger units, their generally lower frequencies in corpora (Kim and Baldwin, 2006) may combine with morphosyntactic phenomena to increase sparsity even further, often requiring non-trivial preprocessing (lemmatization and word reordering) to conflate variants. This paper presents a large-scale multilingual evaluation of DSMs and their parameters for the task of compositionality prediction of nominal c"
P16-1187,W12-3307,0,0.00862446,"n constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation include the context window size and the number of context dimensions adopted for a model (Lapesa and Evert, 2014); the choice of contexts to be used with targets (syntactic dependencies vs. bag-of-words) (Agirre et al., 2009); the use of morphosyntactic information (Pad´o and Lapata, 2003; Pad´o and Lapata, 2007); context filtering (Riedl and Biemann, 2012; Padr´o et al., 2014a); and dimensionality reduction methods (van de Cruys et al., 2012). Important model parameters that have been studied include the choice of association and similarity measures (Curran and Moens, 2002) and the use of subsampling and negative sampling techniques (Mikolov et al., 2013c). However, the particular effects may be heterogeneous and depend on the task and model (Lapesa and Evert, 2014). In this paper, we examine the impact of both corpus and context parameters for a variety of models, for the task of nominal compound compositionality prediction in English and Fre"
P16-1187,W13-1005,0,0.149033,"ay vary considerably, independently from its status as a syntactic head or modifier, as cash in cash cow versus tears in crocodile tears. Indeed, various annotation scales have been proposed as means to collect human judgments about compositionality. Particularly for nominal compounds, Reddy et al. (2011) used a 6-point scale to collect judgments on the literal or figurative use of nominal compounds and its components in English. Similar judgments have also been collected for 244 German compounds, for which an average of 30 judgments on a scale from 1 to 7 were gathered through crowdsourcing (Roller et al., 2013). An alternative to multi-point scales is the binary judgment adopted by Farahmand et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or mul"
P16-1187,E14-1050,0,0.0217874,"effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 2005). The ability of DSMs for accurately capturing semantic information may be affected by a number of factors involved in constructing the models, such as the source corpus, context representation, and parameters of the model. Relevant corpus parameters include size (Ferret, 2013; Mikolov et al., 2013c) and quality (Lapesa and Evert, 2014). Factors related to context representation"
P16-1187,padro-etal-2014-comparing,1,0.919044,"Missing"
P16-1187,N15-1099,0,0.172554,"Missing"
P16-1187,D14-1047,1,0.90979,"Missing"
P16-1187,S10-1049,0,0.0561105,"and et al. (2015), for a dataset of English nominal compounds. There has been much interest in creating semantic representations of larger units, such as phrases (Mikolov et al., 2013b), sentences and 1 In French, one can also use a preposition and optional determiner, like cancer du poumon (lung cancer). 2 It refers to an initiative that provides money to many people without much effort. documents (Le and Mikolov, 2014), and in examining whether it is possible to accurately derive the semantics of a compound or multiword expression from its parts (McCarthy et al., 2003; Baldwin et al., 2003; Tratz and Hovy, 2010; Reddy et al., 2011). For the latter, proposals include using additive and multiplicative functions to combine vector representations of component words (Mitchell and Lapata, 2008; Reddy et al., 2011), calculating the overlap between the components and the expression (McCarthy et al., 2003) and looking at the literality of translations into multiple languages (Salehi et al., 2014). Other proposals to explicitly represent the semantics of nominal compounds include the use of paraphrases (Lauer, 1995; Nakov, 2008; Hendrickx et al., 2013), and inventories of semantic relations (Girju et al., 200"
P16-1187,C12-1165,0,0.205814,"Missing"
P16-1187,D15-1201,0,0.655121,"e been published showing the superiority of neural models (Baroni et al., 2014) and of more traditional but carefully designed models (Levy et al., 2015). The former were also reported as a better fit to behavioral data on semantic prim1987 ing tasks (Mandera et al., 2016). Moreover, these evaluations are often performed on single-word similarity tasks (Freitag et al., 2005; CamachoCollados et al., 2015) and little has been said about the use of word embeddings for the compositionality prediction of multiword expressions. Two notable exceptions are the recent works of Salehi et al. (2015) and Yazdani et al. (2015). Salehi et al. (2015) show that word embeddings are more accurate in predicting compositionality than a simplistic count-based DSM. Yazdani et al. (2015) focus on the composition function, using a lightly supervised neural network to learn the best combination strategy for individual word vectors. In order to consolidate previous punctual results, we present a large-scale and systematic evaluation, comparing DSMs and their parameters, on several compositionality datasets. 3 Materials and Methods We examine the impact of corpus parameters related to the target language and the degree of corpus"
P16-1187,N09-1003,0,\N,Missing
P16-2026,boos-etal-2014-identification,1,0.827008,"s the following steps: (1) compound selection; (2) sentence selection; and (3) questionnaire design. Compound selection The initial set of idiomatic and partially compositional candidates was constructed by introspection, independently for each language, since these may be harder to find in corpora because of lower frequency. This list of compounds was complemented by selecting entries from lists of frequent adjective+noun and noun+noun pairs. These were automatically extracted through POS-sequence queries using the mwetoolkit (Ramisch, 2015) from ukWaC (Baroni et al., 2009), frWaC and brWaC (Boos et al., 2014). We removed all compounds in which the complement is not an adjective in Portuguese/French (e.g. PT noun-noun abelha rainha), those in which the head is not necessarily a noun (e.g. FR aller simple, as aller is also a verb) and those in which the literal sense is very common in the corpus (e.g. EN low blow). For each language, we attempted to select a balanced set of 60 idiomatic, 60 partially compositional and 60 fully compositional compounds by rough manual preannotation.1 Sentence selection For each compound, we selected 3 sentences from a WaC corpus where the compound is used with the sam"
P16-2026,W15-0904,0,0.314258,"Missing"
P16-2026,W13-1017,0,0.258951,"Missing"
P16-2026,S13-2025,0,0.110204,"• Farahmand et al. (2015): individual binary judgments for non-compositionality and conventionality for 1,042 English noun compounds, annotated by 4 experts. One possible source of divergence among annotators is that some datasets do not take polysemy into account. Authors ask annotators to think about the most common sense of an MWE without providing context. Some of these datasets address this issue by providing example sentences to attenuate this problem. We also employ this strategy in our questionnaires. The most similar datasets to ours are the ones presented by Reddy et al. (2011) and Hendrickx et al. (2013). Our dataset combines the methodology from both of these, extending it to French and Portuguese. 3 preposition and optional determiner; e.g. lung cancer (EN) → cancer du poumon (FR), câncer de pulmão (PT). 2. N2 ADJ1 , using a denominal adjective which is derived from N1 ; e.g. cell death (EN) → mort cellulaire (FR), morte celular (PT). We describe the construction of datasets for English, French and Brazilian Portuguese. Given the two syntactic forms above, we focus on N2 ADJ1 for French and Portuguese, as its simpler structure resembles more closely the English noun-noun compound structure,"
P16-2026,W03-1810,0,0.515673,"Missing"
P16-2026,I11-1024,0,0.593144,"n a given language may correspond to a single word in the other languages. Even when it does translate as a compound, its POS pattern and level of compositionality may be widely different. 157 Figure 1: Evaluating compositionality regarding a compounds’ head. Questionnaire design We collect data for each compound through a separate HIT (Human Intelligence Task). Each HIT page contains a list of instructions followed by the questionnaire associated with that compound. In the instructions, we briefly describe the task and require that the users fill in an external identification form, following Reddy et al. (2011). This form provides us with demographics about the annotators, ensuring that they are native speakers of the target language. At the end of the form, they are also given extra example questions with annotated answers for training. After filling in the identification form, users can start working on the task. This section of the HIT is structured in 5 subtasks: 1. Read the compound itself. 2. Read 3 sentences containing the compound. 3. Provide 2 to 3 synonym expressions for the target compound seen in the sentences. 4. Using a Likert scale from 0 to 5, judge how much of the meaning of the com"
P16-2026,J08-4004,0,0.0460253,"ly accessory role in the meaning of the compound in relation to the head. EN Pearson r head-compound 0.75 Pearson r mod-compound 0.74 compound σ > 1.5 22 head σ > 1.5 23 modifier σ > 1.5 35 FR 0.81 0.89 41 44 55 PT 0.80 0.84 30 33 34 Table 1: Pearson correlation r and number of cases of high standard deviation σ. Out of all human judges, 3 of them annotated a large subset of 119 compounds in PT. For this subset, we report inter-annotator agreement. Pairwise weighted κ values range from .28 to .58 depending on the question (head, mod or comp) and on the annotator pair. Multi-rater α agreement (Artstein and Poesio, 2008) values are α = .52 for head, α = .36 for mod and α = .42 for comp scores. We have also calculated the α score of an expert annotator with himself, performing the same task a few weeks later. The score ranges from 2 We include the 90 compounds from Reddy et al. (2011), which are compatible with the new dataset. 158 5 4 4 Average compositionality score Average compositionality score 5 3 2 1 0 0 Compound Head Modifier 20 40 60 80 100 Instances 120 140 160 3 2 1 0 0 180 Compound Head Modifier 20 40 60 (a) English 80 100 Instances 120 140 160 180 (b) French Average compositionality score 5 4 3 2 1"
P16-2026,W13-1005,0,0.40842,"Missing"
P16-2026,W02-2001,1,0.764102,"Missing"
P16-2026,N15-1099,0,0.111551,"Missing"
P16-2026,D15-1201,0,0.469734,"Missing"
padro-etal-2014-comparing,C04-1146,0,\N,Missing
padro-etal-2014-comparing,W02-0908,0,\N,Missing
padro-etal-2014-comparing,W05-0604,0,\N,Missing
padro-etal-2014-comparing,W03-1810,0,\N,Missing
padro-etal-2014-comparing,J10-4006,0,\N,Missing
padro-etal-2014-comparing,P06-4020,0,\N,Missing
padro-etal-2014-comparing,P94-1019,0,\N,Missing
padro-etal-2014-comparing,P07-1061,0,\N,Missing
padro-etal-2014-comparing,P98-2127,0,\N,Missing
padro-etal-2014-comparing,C98-2122,0,\N,Missing
ramisch-etal-2010-mwetoolkit,pearce-2002-comparative,0,\N,Missing
ramisch-etal-2010-mwetoolkit,messiant-etal-2008-lexschem,0,\N,Missing
ramisch-etal-2010-mwetoolkit,copestake-etal-2002-multiword,1,\N,Missing
ramisch-etal-2010-mwetoolkit,J93-1007,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W03-1806,0,\N,Missing
ramisch-etal-2010-mwetoolkit,D07-1110,1,\N,Missing
ramisch-etal-2010-mwetoolkit,W06-1206,1,\N,Missing
ramisch-etal-2010-mwetoolkit,calzolari-etal-2002-towards,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W02-1030,0,\N,Missing
ramisch-etal-2010-mwetoolkit,W08-2107,1,\N,Missing
ramisch-etal-2010-mwetoolkit,P07-1115,0,\N,Missing
ramisch-etal-2010-mwetoolkit,C02-1013,0,\N,Missing
S16-1140,C08-1009,0,0.0312425,"llow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work"
S16-1140,S01-1014,0,0.0924011,"; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised p"
S16-1140,W11-0809,0,0.413578,"brary for direct lexicon projection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers on it (Constant and Sigogne, 2011; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers ("
S16-1140,W09-2903,0,0.0316836,"s can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds were manually chosen bas"
S16-1140,J09-1005,0,0.577136,"guation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds"
S16-1140,C14-1123,0,0.0196432,"MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep"
S16-1140,W11-0818,0,0.0309068,"andidates are extracted without losing track of their tokenlevel occurrences, to guarantee that all the MWE occurrences learned from the training data are projected onto the test corpus. For semantic tagging we adopted a predominant-sense heuristic. In the remainder of this paper, we present related work (§ 2), then we present and discuss the results of the MWE identification subsystem (§ 3) and of the supersense tagging subsystem (§ 4). We then conclude and share ideas for future improvements (§ 5). 2 Related Work Practical solutions for rule-based MWE identification include tools like jMWE (Kulkarni and Finlayson, 2011), a library for direct lexicon projection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers o"
S16-1140,J07-4005,0,0.0310987,"et to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Unsupervised approaches using clustering and distributional similarity (Brody and Lapata, 2008; Goyal and Hovy, 2014) can also be employed for WSD. Both supervised and unsupervised WSD techniques have also been used to distinguish literal from idiomatic uses of MWEs (Fazly et al., 2009; Diab and Bhutada, 2009). Nonetheless, systematically choosing the most frequent sense is a surprisingly good baseline, not always easy to beat (McCarthy et al., 2007; Navigli, 2009). This was also verified for MWE disambiguation (Uchiyama et al., 2005). Thus, in this work, we implemented a simple supervised predominant-sense heuristic and will investigate more sophisticated WSD techniques as future work. other words, we keep MWE candidates whose proportion of annotated instances with respect to all occurrences in the training corpus is above a threshold t, discarding the rest. The thresholds were manually chosen based on what seemed to yield better results on the development set. Finally, we project the resulting list of MWE candidates on the test data, t"
S16-1140,P15-1108,1,0.852029,"ic combinations. • Using fixedness features to identify and disambiguate very productive patterns like ADJ_N (Ramisch et al., 2008; Fazly et al., 2009). • Replacing the C ONTIG method by a sequence tagger for contiguous MWEs (e.g. using a CRF), in order to identify unknown MWEs based on generalizations made from known MWEs (Constant and Sigogne, 2011; Schneider et al., 2014). • Developing a more realistic WSD algorithm for supersense tagging, able to tag unseen words and MWEs and to take context into account. • Taking parse trees into account to distinguish MWEs from accidental cooccurrences (Nasr et al., 2015). • Using semantic-based association measures and semantic-based features based on word embeddings to target idiomatic MWEs (Salehi et al., 2015). 915 Acknowledgments This work has been funded by the French Agence Nationale pour la Recherche through projects PARSEME-FR (ANR-14-CERA-0001) and ORFEO (ANR-12-CORP-0005), and by French-Brazilian cooperation projects CAMELEON (CAPES-COFECUB #707/11) and AIM-WEST (FAPERGSINRIA 1706-2551/13-7). Part of the results presented in this paper were obtained through research on a project titled &quot;Simplificação Textual de Expressões Complexas&quot;, sponsored by Sa"
S16-1140,P07-1115,0,0.0870416,"Missing"
S16-1140,N15-1099,0,0.024269,"Missing"
S16-1140,Q14-1016,0,0.391857,"jection based on preexisting MWE lists. Finite-state transducers can also be used to take into account the internal morphology of component words and perform efficient tokenization based on MWE dictionaries (Savary, 2009). The problem of MWE identification 910 Proceedings of SemEval-2016, pages 910–917, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics has also been modeled using supervised machine learning. Probabilistic MWE taggers usually encode the data using a begin-inside-outside scheme and learn CRF-like taggers on it (Constant and Sigogne, 2011; Schneider et al., 2014). The mwetoolkit (Ramisch, 2015) provides command-line programs that allow one to discover new MWE candidate lists, filter them and project them back on text according to some parameters. Our system uses the latter as basis for MWE identification. Word sense disambiguation (WSD) methods can be roughly classified into knowledge-based, supervised and unsupervised. Knowledge-based methods use lexico-semantic taxonomies like WordNet to calculate the similarity between context and target words (Lesk, 1986). Supervised approaches generally use context-sensitive classifiers (Cabezas et al., 2001). Un"
S16-1140,S16-1084,0,0.0245054,"late it into an equivalent meaning in the target language. While determining the meaning of single words is a difficult task on its own, the problem is compounded by the pervasiveness of Multiword Expressions (MWEs). MWEs are semantic units that span over multiple lexemes in the text (e.g. dry run, look up, fall flat). Their meaning cannot be inferred by applying regular composition rules on the meanings of their component words. The task of semantic tagging is thus deeply intertwined with the identification of multiword expressions. This paper presents our solution to the DiMSUM shared task (Schneider et al., 2016), where the evaluated systems are expected to perform both semantic tagging and multiword identification. Our pipeline system first detects and groups MWEs and then assigns supersense tags, as two consecutive steps. For MWE identification, we use a task-specific instantiation of the mwetoolkit (Ramisch, 2015), handling both contiguous and non-contiguous MWEs with some degree of customization (Cordeiro et al., 2015). Additionally, MWE type-level candidates are extracted without losing track of their tokenlevel occurrences, to guarantee that all the MWE occurrences learned from the training data"
W08-2107,W03-1810,0,0.635709,"ere no other complement is required, or it may occur as a transitive VPC which requires a further NP complement (e.g. in She gave up alcohol while she was pregnant ). Since in English particles tend to be homographs with prepositions (up, out, in), a verb followed by a preposition/particle and an NP can be ambiguous between a transitive VPC and a prepositional verb (e.g. rely on, in He relies on his wife for everything). Some criteria that characterise VPCs are discussed by Bolinger (1971):2 proposed using a variety of approaches such as statistical, substitutional, distributional, etc. (e.g. McCarthy et al. (2003), Bannard (2005) and Fazly and Stevenson (2006)). In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In this work we investigate the automatic extraction of VPCs, looking into a variety of methods, combining linguistic with statistical information, ranging from frequencies to association measures: Mutual Information (MI), χ2 and Entropy. We also investigate the determination of compositionality of VPCs"
W08-2107,P04-1036,0,0.0291894,"t of the synonym synsets are related to secondary senses or very specific uses of a verb and are thus not correctly disambiguated. In what concerns the WNS sets, only the smallest and first synset were kept, suggesting again that it may not be a good idea to maximise the synonyms set and for future work, we intent to establish a threshold for a synset to be taken into account. In addition, we can also infer a positive contribution of the frequency of a sense with the choice of the first synset returned by Wordnet resulting in a reasonable WSD heuristic (which is compatible with the results by McCarthy et al. (2004)). On the other hand, the algorithm selected the The results confirm that the use of statistical and linguistic information to automatically identify verb-particle constructions presents a reasonable way of improving coverage of existing lexical resources in a very simple and straightforward manner. In terms of grammar engineering, the information about compositional candidates belonging to productive classes provides us with the basis for constructing a family of fine-grained redundancy rules for these classes. These rules are applied in a constrained way to verbs already in the lexicon, acco"
W08-2107,baldwin-etal-2004-road,0,0.0410734,"s (MWEs), like compound nouns (science fiction) and phrasal verbs (carry out) (e.g. Pearce (2002), Evert and Krenn (2005) and Zhang et al. (2006)). Some of them employ language and/or type dependent linguistic knowledge for the task, while others employ independent statistical methods, such as Mutual Information and Log-likelihood (e.g. Pearce (2002) and, Zhang et al. (2006)), or even a combination of them (e.g. c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 See Baldwin et al. (2004) for a discussion of the effects of multiword expressions like VPCs on a parser’s performance. 49 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 49–56 Manchester, August 2008 (2002)). Idiomatic VPCs, on the other hand, cannot have their meaning determined by interpreting their components literally (e.g. get on, meaning to be on friendly terms with someone). The third class is that of aspectual VPCs, which have the particle providing the verb with an endpoint, suggesting that the action described by the verb is performed completely, thoroughly o"
W08-2107,pearce-2002-comparative,0,0.605132,"TALP Laboratory, Joseph Fourier University - Grenoble INP (France) ♠ Department of Computer Sciences, Bath University (UK) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) {ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br Abstract Baldwin (2005) and Sharoff (2004)), as basis for helping to determine whether a given sequence of words is in fact an MWE. Although some research aims at developing methods for dealing with MWEs in general (e.g. Zhang et al. (2006), Ramisch et al. (2008)), there is also some work that deals with specific types of MWEs (e.g. Pearce (2002) on collocations and Villavicencio (2005) on verb-particle constructions (VPCs)) as each of these MWE types has distinct distributional and linguistic characteristics. VPCs are combinations of verbs and particles, such as take off in Our plane took off late, that due to their complex characteristics and flexible nature, provide a real challenge for NLP. In particular, there is a lack of adequate resources to identify and treat them, and those that are available provide only limited coverage, in face of the huge number of combinations in use. For tasks like parsing and generation, it is essenti"
W08-2107,W04-0403,0,0.0694786,"Missing"
W08-2107,copestake-flickinger-2000-open,0,0.0327811,"Missing"
W08-2107,W06-1206,1,0.818583,"eonardo Moura♣ and Marco Idiart♥ ♣ Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) ♦ GETALP Laboratory, Joseph Fourier University - Grenoble INP (France) ♠ Department of Computer Sciences, Bath University (UK) ♥ Institute of Physics, Federal University of Rio Grande do Sul (Brazil) {ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br Abstract Baldwin (2005) and Sharoff (2004)), as basis for helping to determine whether a given sequence of words is in fact an MWE. Although some research aims at developing methods for dealing with MWEs in general (e.g. Zhang et al. (2006), Ramisch et al. (2008)), there is also some work that deals with specific types of MWEs (e.g. Pearce (2002) on collocations and Villavicencio (2005) on verb-particle constructions (VPCs)) as each of these MWE types has distinct distributional and linguistic characteristics. VPCs are combinations of verbs and particles, such as take off in Our plane took off late, that due to their complex characteristics and flexible nature, provide a real challenge for NLP. In particular, there is a lack of adequate resources to identify and treat them, and those that are available provide only limited cover"
W08-2107,E06-1043,0,0.0463223,"may occur as a transitive VPC which requires a further NP complement (e.g. in She gave up alcohol while she was pregnant ). Since in English particles tend to be homographs with prepositions (up, out, in), a verb followed by a preposition/particle and an NP can be ambiguous between a transitive VPC and a prepositional verb (e.g. rely on, in He relies on his wife for everything). Some criteria that characterise VPCs are discussed by Bolinger (1971):2 proposed using a variety of approaches such as statistical, substitutional, distributional, etc. (e.g. McCarthy et al. (2003), Bannard (2005) and Fazly and Stevenson (2006)). In particular, Fazly and Stevenson (2006) look at the correlation between syntactic fixedness (in terms of e.g. passivisation, choice of determiner type and pluralisation) and non-compositionality of verb-noun compounds such as shoot the breeze. In this work we investigate the automatic extraction of VPCs, looking into a variety of methods, combining linguistic with statistical information, ranging from frequencies to association measures: Mutual Information (MI), χ2 and Entropy. We also investigate the determination of compositionality of VPCs verifying whether the degree of semantic flexi"
W11-0812,2009.mtsummit-btm.1,0,0.276847,"scuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995"
W11-0812,1992.tmi-1.3,0,0.0203849,"ian Portuguese, focusing on the development of a lexical resource for NLP tasks, such as SRL. The remainder of this paper is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs"
W11-0812,W10-1812,0,0.139354,"Missing"
W11-0812,W10-1810,0,0.110853,"e present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010)"
W11-0812,C10-3015,1,0.709696,"es from active sentences, both afﬁrmative and negative. Cases which present intervening material between the verb and the other element of the CP are not captured, but this is not a serious problem considering the size of our corpus, although it inﬂuences the frequencies used in candidate selection. In order to facilitate human analysis of candidate lists, we used the mwetoolkit4 : a tool that has been developed speciﬁcally to extract MWEs from corpora, which encompasses candidate extraction through pattern matching, candidate ﬁltering (e.g. through association measures) and evaluation tools (Ramisch et al., 2010). After generating separate lists of candidates for each pattern, we ﬁltered out all those occurring less than 10 times in the corpus. The entries resulting of automatic identiﬁcation were classiﬁed by their frequency and their annotation is discussed in the following section. 4 Discussion Each pattern of POS tags returned a large number of candidates. Our expectation was to identify CPs among the most frequent candidates. First we annotated “interesting” candidates and then, in a deep analysis, we judged their idiomaticity. In the Table 1, we show the total number of candidates extracted befo"
W11-0812,C90-3043,0,0.541419,"er is organized as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominal"
W11-0812,W04-0401,0,0.0631246,"as follows: in §2 we discuss related work, in §3 we present the corpus and the details about our methodology, in §4 we present and discuss the resulting lists of candidates, in §5 we envisage further work and draw our conclusions. 2 Related Work Part of the CPs focused on here are represented by LVCs and SVCs. These CPs have been studied in several languages from different points of view: diacronic (Ranchhod, 1999; Marchello-Nizia, 1996), language contrastive (Danlos and Samvelian, 1992; Athayde, 2001), descriptive (Butt, 2003; Langer, 2004; Langer, 2005) and for NLP purposes (Salkoff, 1990; Stevenson et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Gre"
W11-0812,E95-1014,0,0.786068,"n et al., 2004; Barreiro and Cabral, 2009; Hwang et al., 2010). Closer to our study, Hendrickx et al. (2010) annotated a Treebank of 1M tokens of European Portuguese with almost 2,000 CPs, which include LVCs and verbal chains. This lexicon is relevant for many NLP applications, notably for automatic translation, since in any task involving language generation they confer ﬂuency and naturalness to the output of the system. Work focusing on the automatic extraction of LVCs or SVCs often take as starting point a list of recurrent light verbs (Hendrickx et al., 2010) or a list of nominalizations (Teufel and Grefenstette, 1995; Dras, 1995; Hwang et al., 2010). These approaches are not adopted here because our goal is precisely to identify which are the verbs, the nouns and other lexical elements that take part in CPs. Similar motivation to study LVCs/SVCs (for SRL) is found within the scope of Framenet (Atkins et al., 2003) and Propbank (Hwang et al., 2010). These projects have taken different decisions on how to annotate such constructions. Framenet annotates the head of the construction (noun or adjective) as argument taker (or frame evoker) and the light verb separately; Propbank, on its turn, ﬁrst annotates sep"
W11-0822,C10-3015,1,0.595171,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W11-0822,ramisch-etal-2010-mwetoolkit,1,0.704795,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W11-0822,C10-2120,1,0.86255,"ee and Pedersen, 2003), and filtering like in UCS (Evert, 2004). The pattern matching and n-gram counting steps are the focus of the improvements described in this paper. We present an experimental environment for computer-assisted extraction of Multiword Expressions (MWEs) from corpora. Candidate extraction works in two steps: generation and filtering. We focus on recent improvements in the former, for which we increased speed and flexibility. We present examples that show the potential gains for users and applications. 2 1 Project Description The mwetoolkit was presented and demonstrated in Ramisch et al. (2010b) and in Ramisch et al. (2010a), and applied to several languages (Linardaki et al., 2010) and domains (Ramisch et al., 2010c). It is a downloadable open-source1 set of commandline tools mostly written in Python. Our target users are researchers with a background in computational linguistics. The system performs language- and type-independent candidate extraction in two steps2 : 1. Candidate generation • Pattern matching3 • n-gram counting 2. Candidate filtering • Thresholds, stopwords and patterns • Association measures, classifiers An Example Our toy corpus, consisting of the first 20K sent"
W12-0911,calzolari-etal-2002-towards,0,0.0602377,"fficulties may arise as the interpretation of these expressions often demands more knowledge than just about (1) unitary words and (2) word-to-word relations. This introduces a distinction between what a learner is able to computationally disambiguate or figure out automatically from language and what must be explicitly stored/memorized and retrieved whole from memory at the time of There has been considerable discussion about the challenges imposed by Multiword Expressions (MWEs) which in addition to crossing word boundaries act as a single lexical unit at some levels of linguistic analysis (Calzolari et al., 2002; Sag et al., 2002; Fillmore, 2003). They include a wide range of grammatical constructions such as verb-particle constructions (VPCs), idioms, compound nouns and listable word configurations, 43 Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43–50, c Avignon, France, April 24 2012. 2012 Association for Computational Linguistics tion 3 presents the resources and methods used in this paper. The analyses of VPCs in children and adults sentences are in section 4. We finish with conclusions and possibilities of future works. use, rather than b"
W12-0911,pearce-2002-comparative,0,0.129259,"nd an arm and a leg for a nominal egg (Fillmore, 2003). For second language (L2) learners in particular (Wray, 2002) MWEs are indeed a well-known cause of problems and less likely to be used by them than by native speakers in informal spoken contexts (Siyanova and Schmitt, 2007). Even if L2 learners may be capable of producing a large number of MWEs, their underlying intuitions and fluency do not match those of native speakers (Siyanova and Schmitt, 2008) and they may produce marked combinations that are not conventionally used together (e.g. plastic surgery/?operation, strong/?powerful tea) (Pearce, 2002; Siyanova and Schmitt, 2007). Given the potential additional sources of complexity of MWEs for learning, in this paper we investigate whether children shy away from using them when they communicate. We focus on a particular type of MWEs, VPCs, which present a wide range of syntactic and semantic idyosincrasies examining whether children produce proportionally less VPCs than adults. In addition, we analyze whether any potential added processing costs for VPCs are reflected in a reduced choice of VPCs or verbs to form these combinations in child-produced sentences compared to adult usage. Final"
W12-0911,ramisch-etal-2010-mwetoolkit,1,0.868102,"Missing"
W12-0911,villavicencio-etal-2012-large,1,0.291982,"ed to produce even more object dropping errors for VPCs than children with typ3 Materials and Methods For this work we use the English corpora from the CHILDES database (MacWhinney, 1995) containing transcriptions of child-produced and child-directed speech from interactions involving children of different age groups and in a variety of settings, from naturalistic longitudinal studies to task oriented latitudinal cases. These corpora are available in raw, part-of-speech-tagged, lemmatized and parsed formats (Sagae et al., 2010). Moreover the English CHILDES Verb Construction Database (ECVCD) (Villavicencio et al., 2012) also adds for each sentence the RASP parsing and grammatical relations (Briscoe and Carroll, 2006), verb semantic classes (Levin, 1993), age of acquisition, familiarity, frequency (Coltheart, 1981) and other psycholinguistic and distributional characteristics. These annotated sentences are divided into two groups according to the speaker annotation available in CHILDES, the Adults Set and the Children Set contain respectively all the sentences spoken by adults and by children1 , as shown in table 1 as Parsed. VPCs in these corpora are detected by looking in the RASP annotation for all occurre"
W12-0911,P06-2006,0,\N,Missing
W12-3301,N10-1029,0,0.0915846,"evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisition approaches was in the form of a shared task (Grégoire et al., 2008). However, the prese"
W12-3301,W11-0805,0,0.0793016,"(2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisition approaches was in the form of a shared task (Grégoire e"
W12-3301,W11-0818,0,0.114271,"Missing"
W12-3301,pearce-2002-comparative,0,0.168553,"l., 2002). Existing approaches are either generic but present relatively low pre1 The equivalent expressions in French would be raining ropes, in German raining young dogs, in Portuguese raining Swiss knives, etc. MWE Acquisition Approaches Efforts for the evaluation of MWE acquisition approaches usually focus on a single technique or compare the quality of association measures (AMs) used to rank a fixed annotated list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available,"
W12-3301,P05-2003,0,0.560253,"opes, in German raining young dogs, in Portuguese raining Swiss knives, etc. MWE Acquisition Approaches Efforts for the evaluation of MWE acquisition approaches usually focus on a single technique or compare the quality of association measures (AMs) used to rank a fixed annotated list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography"
W12-3301,W11-0821,0,0.0674415,"ge 5,000 133,859 145,888 50,000 1,355,482 1,483,428 500,000 13,164,654 14,584,617 Table 1: Number of sentences and of words of each fragment of the Europarl corpus in fr and in en. of statistical AMs. It is an integrated framework for MWE treatment, providing from corpus preprocessing facilities to the automatic evaluation of the resulting list with respect to a reference. Its input is a corpus annotated with POS, lemmas and dependency syntax, or if these are not available, raw text. 3. Ngram Statistics Package7 (NSP) is a traditional approach for the statistical analysis of n-grams in texts (Pedersen et al., 2011). It provides tools for counting n-grams and calculating AMs, where an ngram is a sequence of n words occurring either contiguously or within a window of w words in a sentence. While most of the measures are only applicable to bigrams, some of them are also extended to trigrams and 4-grams. The set of available AMs includes robust and theoretically sound measures such as log-likelihood and Fischer’s exact test. Although there is no direct support to linguistic information such as POS, it is possible to simulate them to some extent using the same workaround as for LocMax. 4. UCS toolkit8 provid"
W12-3301,C10-3015,1,0.654588,"information in order to target a specific type of construction.4 The evaluation includes both LocalMaxs Strict which prioritizes high precision (henceforth LocMax-S) and LocalMaxs Relaxed which focuses on high recall (henceforth LocMax-R). A variation of the original algorithm, SENTA, has been proposed to deal with noncontiguous expressions (da Silva et al., 1999). However, it is computationally costly5 and there is no freely available implementation. 2. MWE toolkit6 (mwetk) is an environment for type and language-independent MWE acquisition, integrating linguistic and frequency information (Ramisch et al., 2010). It generates a targeted list of MWE candidates extracted and filtered according to user-defined criteria like POS sequences and a set 3 http://hlt.di.fct.unl.pt/luis/multiwords/ index.html 4 Although this can be simulated by concatenating words and POS tags together in order to form a token. 5 It is based on the calculation of all possible n-grams in a sentence, which explode in number when going from contiguous to noncontiguous n-grams. 6 http://mwetoolkit.sourceforge.net 2 # sentences # en words # fr words Small Medium Large 5,000 133,859 145,888 50,000 1,355,482 1,483,428 500,000 13,164,6"
W12-3301,W10-3708,0,0.0682433,"list of MWEs. For instance, Evert and Krenn (2005) and Seretan (2008) specifically evaluate and analyse the lexical AMs used in MWE extraction on small samples of bigram candidates. Pearce (2002), systematically evaluates a set of techniques for MWE extraction on a small test set of English collocations. Analogously, Pecina (2005) and Ramisch et al. (2008) present extensive comparisons of individual AMs and of their combination for MWE extraction in Czech, German and English. There have also been efforts for the extrinsic evaluation of MWEs for NLP applications such as information retrieval (Xu et al., 2010), word sense disambiguation (Finlayson and Kulkarni, 2011) and MT (Carpuat and Diab, 2010). One recent initiative aiming at more comparable eval2 We consider only freely available, downloadable and openly documented tools. Therefore, outside the scope of this work are proprietary tools, terminology and lexicography tools, translation aid tools and published techniques for which no available implementation is provided. 1 Proceedings of the 2012 Student Research Workshop, pages 1–6, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics uations of MWE acquisiti"
W12-3301,W10-3700,0,\N,Missing
W12-3311,W11-0815,0,0.161846,"ir evaluation scenario, comparing manual and intuitive research with the automatic association ratio they proposed. • Word sense disambiguation: MWEs tend to be less polysemous than simple words. Finlayson and Kulkarni (2011) exemplify that the word world has 9 senses in Wordnet 1.6, record has 14, but world record has only 1. • POS tagging and parsing: recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities (Seretan, 2008). • Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011). 2 Examples of MWEs that breach standard grammatical rules include kingdom come and by and large. 3 For example, Smadja (1993) classifies them according to syntactic function while Sag et al. (2002) classify them according to flexibility. 4 Although they define MWEs as “sequences”, assuming contiguity, we assume “sets” of words for greater generality. 5 Automatic translations (MT) by Google (http://translate. google.com/) on 2012/02/18. Reference (REF) by native speaker. 62 2 Thesis contributions Despite the importance of MWEs in several applications, they are often neglected in the design an"
W12-3311,calzolari-etal-2002-towards,0,0.152075,"d syntactic variability: standard grammatical rules do not apply to MWEs. This can be expressed in terms of (i) lexicalisation, as one cannot list all MWEs in the lexicon (undergeneration) nor include them all in the grammar (overgeneration) and (ii) extragrammaticality, as MWEs are unpredictable and seem “weird” for a second language learner who only knows general rules.2 • Heterogeneity: MWEs are hard to define because they encompass a large amount of phenomena. Thus, NLP applications cannot use a unified approach and need to rely on some typology3 . In this paper, I adopt the definition by Calzolari et al. (2002), who define MWEs as: different but related phenomena [which] can be described as a sequence4 of words that acts as a single unit at some level of linguistic analysis. This generic and intentionally vague definition can be narrowed down according to the application needs. For example, for the statistical machine translation (MT) system5 used in the examples shown in Table 1, an MWE is any sequence of words which, when not translated as a unit, generates errors: ungrammatical or unnatural verbal constructions (sentence 1), awkward literal translations of idioms (sentence 2) and problems of lexi"
W12-3311,N10-1029,0,0.224761,"Missing"
W12-3311,J90-1003,0,0.197271,"le, for the statistical machine translation (MT) system5 used in the examples shown in Table 1, an MWE is any sequence of words which, when not translated as a unit, generates errors: ungrammatical or unnatural verbal constructions (sentence 1), awkward literal translations of idioms (sentence 2) and problems of lexical choice and word order in specialised texts (sentence 3). These examples illustrate the importance of correctly dealing with MWEs in MT applications and, more generally, MWEs can speed up and help remove ambiguities in many current NLP applications, for example: • Lexicography: Church and Hanks (1990) used a lexicographic environment as their evaluation scenario, comparing manual and intuitive research with the automatic association ratio they proposed. • Word sense disambiguation: MWEs tend to be less polysemous than simple words. Finlayson and Kulkarni (2011) exemplify that the word world has 9 senses in Wordnet 1.6, record has 14, but world record has only 1. • POS tagging and parsing: recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities (Seretan, 2008). • Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of th"
W12-3311,W03-1806,0,0.364206,"Missing"
W12-3311,W11-0812,1,0.874729,"Missing"
W12-3311,W11-0805,0,0.036245,"al translations of idioms (sentence 2) and problems of lexical choice and word order in specialised texts (sentence 3). These examples illustrate the importance of correctly dealing with MWEs in MT applications and, more generally, MWEs can speed up and help remove ambiguities in many current NLP applications, for example: • Lexicography: Church and Hanks (1990) used a lexicographic environment as their evaluation scenario, comparing manual and intuitive research with the automatic association ratio they proposed. • Word sense disambiguation: MWEs tend to be less polysemous than simple words. Finlayson and Kulkarni (2011) exemplify that the word world has 9 senses in Wordnet 1.6, record has 14, but world record has only 1. • POS tagging and parsing: recent work in parsing and POS tagging indicates that MWEs can help remove syntactic ambiguities (Seretan, 2008). • Information retrieval: when MWEs like pop star are indexed as a unit, the accuracy of the system improves on multiword queries (Acosta et al., 2011). 2 Examples of MWEs that breach standard grammatical rules include kingdom come and by and large. 3 For example, Smadja (1993) classifies them according to syntactic function while Sag et al. (2002) class"
W12-3311,pearce-2002-comparative,0,0.0150244,"on raw text. But since such tools are not available for all languages, the methodology was conceived to be applicable even in the absence of preprocessing. 6 However, it is designed to deal with languages that use spaces to separate words. Thus, when working with Chinese, Japanese, or even with German compounds, some additional preprocessing is required. Evaluation of MWE Acquisition Published results comparing MWE extraction techniques usually evaluate them on small controlled data sets using objective measures such as precision, recall and mean average precision (Schone and Jurafsky, 2001; Pearce, 2002; Evert and Krenn, 2005). On the one hand, the results of intrinsic evaluation are often vague or inconclusive: although they shed some light on the optimal parameters for the given scenario, they are hard to generalise and cannot be directly applied to other configurations. The quality of acquired MWEs as measured by objective criteria depends on the language, domain and type of the target construction, on corpus size and genre, on already available resources7 , on the applied filters, preprocessing steps, etc. On the other hand, extrinsic evaluation consists of inserting acquired MWEs into a"
W12-3311,C10-3015,1,0.801177,"methods is available, ranging from simple frequency thresholds to stopword lists and sophisticated association measures. Finally, the resulting filtered candidates are either directly injected into an NLP application or further manually validated before application. An alternative use for the validated candidates is to train a machine learning model which can be applied on new corpora in order to automatically identify and extract MWEs based on the characteristics of the previously acquired ones. For further details, please refer to the website of the framework8 and to previous publications (Ramisch et al., 2010a; Ramisch et al., 2010b). 4 Application-oriented evaluation In this section, I present summarised results of extrinsic quantitative and qualitative evaluation of the framework for MWE acquisition propose in § 3. The target applications are computer-aided lexicography (§ 4.1) and statistical machine translation (§ 4.2). 8 http://mwetoolkit.sf.net is useless to acquire MWEs already present in the dictionary. extraction patterns Language English French Greek Portuguese Type PV NC NC CP Corpus (words) Europarl (13M) Europarl (14.5M) Europarl (26M) PLN-BR-FULL (29M) Candidates 5.3K 104K 25K 407K F"
W12-3311,ramisch-etal-2010-mwetoolkit,1,0.493752,"methods is available, ranging from simple frequency thresholds to stopword lists and sophisticated association measures. Finally, the resulting filtered candidates are either directly injected into an NLP application or further manually validated before application. An alternative use for the validated candidates is to train a machine learning model which can be applied on new corpora in order to automatically identify and extract MWEs based on the characteristics of the previously acquired ones. For further details, please refer to the website of the framework8 and to previous publications (Ramisch et al., 2010a; Ramisch et al., 2010b). 4 Application-oriented evaluation In this section, I present summarised results of extrinsic quantitative and qualitative evaluation of the framework for MWE acquisition propose in § 3. The target applications are computer-aided lexicography (§ 4.1) and statistical machine translation (§ 4.2). 8 http://mwetoolkit.sf.net is useless to acquire MWEs already present in the dictionary. extraction patterns Language English French Greek Portuguese Type PV NC NC CP Corpus (words) Europarl (13M) Europarl (14.5M) Europarl (26M) PLN-BR-FULL (29M) Candidates 5.3K 104K 25K 407K F"
W12-3311,W12-3301,1,0.74788,"Missing"
W12-3311,W01-0513,0,0.550866,"er than running the methods on raw text. But since such tools are not available for all languages, the methodology was conceived to be applicable even in the absence of preprocessing. 6 However, it is designed to deal with languages that use spaces to separate words. Thus, when working with Chinese, Japanese, or even with German compounds, some additional preprocessing is required. Evaluation of MWE Acquisition Published results comparing MWE extraction techniques usually evaluate them on small controlled data sets using objective measures such as precision, recall and mean average precision (Schone and Jurafsky, 2001; Pearce, 2002; Evert and Krenn, 2005). On the one hand, the results of intrinsic evaluation are often vague or inconclusive: although they shed some light on the optimal parameters for the given scenario, they are hard to generalise and cannot be directly applied to other configurations. The quality of acquired MWEs as measured by objective criteria depends on the language, domain and type of the target construction, on corpus size and genre, on already available resources7 , on the applied filters, preprocessing steps, etc. On the other hand, extrinsic evaluation consists of inserting acquir"
W12-3311,J93-1007,0,0.744298,"and homogeneity properties with the central moment Il partage la traduction-invariance et propriétés d’homogénéité avec le moment central Il partage les propriétés d’invariance par translation et d’homogénéité avec le moment central Table 1: Examples of SMT errors due to MWEs. Introduction Multiword expressions (MWEs) range over linguistic constructions such as idioms (to pay an arm and a leg), fixed phrases (rock ’n’ roll) and noun compounds (dry ice). There is no unique and widely accepted definition for the term multiword expression. It can be an “arbitrary and recurrent word combination” (Smadja, 1993) or “a syntactic and semantic unit whose exact and unambiguous meaning or connotation cannot be derived directly from the meaning or connotation of its components” (Choueka, 1988) or simply an “idiosyncratic interpretation that crosses word boundaries (or spaces)” (Sag et al., 2002). MWEs lie in the fuzzy zone between lexicon and syntax, thus constituting a real challenge for NLP systems. In addition, they are very pervasive, occurring frequently in everyday language as well as in specialised communications. Some common properties of MWEs are:1 1 These are not binary yes/no flags, but values i"
W12-3311,P11-3003,0,0.0289883,"Missing"
W12-3311,weller-heid-2010-extraction,0,\N,Missing
W12-3311,D07-1039,0,\N,Missing
W12-3311,J93-1004,0,\N,Missing
W12-3311,C10-2120,1,\N,Missing
W12-3311,J87-3006,0,\N,Missing
W12-3311,N10-1089,0,\N,Missing
W12-3311,W97-0311,0,\N,Missing
W12-3311,J99-4005,0,\N,Missing
W12-3311,1999.tc-1.8,0,\N,Missing
W12-3311,bonin-etal-2010-contrastive,0,\N,Missing
W12-3311,messiant-etal-2008-lexschem,0,\N,Missing
W12-3311,C10-2073,0,\N,Missing
W12-3311,W10-1812,0,\N,Missing
W12-3311,W10-1810,0,\N,Missing
W12-3311,W06-2403,0,\N,Missing
W12-3311,heid-weller-2008-tools,0,\N,Missing
W12-3311,C10-1014,0,\N,Missing
W12-3311,spina-2010-dictionary,0,\N,Missing
W12-3311,papageorgiou-etal-2000-unified,0,\N,Missing
W12-3311,zhang-kordoni-2006-automated,0,\N,Missing
W12-3311,C02-1166,0,\N,Missing
W12-3311,E95-1014,0,\N,Missing
W12-3311,E09-2012,0,\N,Missing
W12-3311,W04-0412,0,\N,Missing
W12-3311,W09-2904,0,\N,Missing
W12-3311,W06-1205,0,\N,Missing
W12-3311,W07-1105,0,\N,Missing
W12-3311,W08-1901,0,\N,Missing
W12-3311,W08-1914,0,\N,Missing
W12-3311,C94-1074,0,\N,Missing
W12-3311,J93-2003,0,\N,Missing
W12-3311,W11-0804,0,\N,Missing
W12-3311,J01-1001,0,\N,Missing
W12-3311,W06-1207,0,\N,Missing
W12-3311,W09-2905,0,\N,Missing
W12-3311,C00-2090,0,\N,Missing
W12-3311,C90-3043,0,\N,Missing
W12-3311,W04-0410,0,\N,Missing
W12-3311,N03-5005,0,\N,Missing
W12-3311,W07-1106,0,\N,Missing
W12-3311,W06-1006,0,\N,Missing
W12-3311,H93-1052,0,\N,Missing
W12-3311,W09-0424,0,\N,Missing
W12-3311,W03-1802,0,\N,Missing
W12-3311,A94-1006,0,\N,Missing
W12-3311,W09-2901,0,\N,Missing
W12-3311,W04-0401,0,\N,Missing
W12-3311,E09-3008,0,\N,Missing
W12-3311,C08-1045,0,\N,Missing
W12-3311,W07-1103,0,\N,Missing
W12-3311,W03-1810,0,\N,Missing
W12-3311,W07-1101,0,\N,Missing
W12-3311,W07-0612,0,\N,Missing
W12-3311,W03-1803,0,\N,Missing
W12-3311,W04-0404,0,\N,Missing
W12-3311,P97-1061,0,\N,Missing
W12-3311,W09-2906,0,\N,Missing
W12-3311,W04-0407,0,\N,Missing
W12-3311,D07-1110,1,\N,Missing
W12-3311,W06-1204,0,\N,Missing
W12-3311,W06-1206,0,\N,Missing
W12-3311,C04-1200,0,\N,Missing
W12-3311,W10-3702,0,\N,Missing
W12-3311,W10-3704,0,\N,Missing
W12-3311,P06-2023,0,\N,Missing
W12-3311,J04-4002,0,\N,Missing
W12-3311,W03-1807,0,\N,Missing
W12-3311,2005.mtsummit-tutorials.1,0,\N,Missing
W12-3311,P05-2003,0,\N,Missing
W12-3311,W05-1008,0,\N,Missing
W12-3311,J09-1005,0,\N,Missing
W12-3311,J03-3005,0,\N,Missing
W12-3311,P07-2045,0,\N,Missing
W12-3311,W03-1812,0,\N,Missing
W12-3311,P06-1099,0,\N,Missing
W12-3311,W07-1104,0,\N,Missing
W12-3311,W09-2907,0,\N,Missing
W12-3311,P06-4020,0,\N,Missing
W12-3311,J02-3004,0,\N,Missing
W12-3311,P11-2099,0,\N,Missing
W12-3311,W11-0814,0,\N,Missing
W12-3311,D11-1067,0,\N,Missing
W12-3311,D09-1050,0,\N,Missing
W12-3311,P08-1052,0,\N,Missing
W12-3311,W11-0821,0,\N,Missing
W12-3311,N03-1017,0,\N,Missing
W12-3311,W03-1809,0,\N,Missing
W12-3311,C90-3010,0,\N,Missing
W12-3311,J03-1002,0,\N,Missing
W12-3311,W10-3707,0,\N,Missing
W12-3311,W11-0802,0,\N,Missing
W12-3311,J03-1005,0,\N,Missing
W12-3311,W12-3904,1,\N,Missing
W12-3311,W05-0603,0,\N,Missing
W12-3311,W07-1102,0,\N,Missing
W12-3311,W02-2001,0,\N,Missing
W12-3311,W11-0809,0,\N,Missing
W12-3311,S10-1007,0,\N,Missing
W12-3311,W08-2107,1,\N,Missing
W12-3311,P07-1115,0,\N,Missing
W12-3311,2005.mtsummit-papers.11,0,\N,Missing
W12-3311,W11-0808,0,\N,Missing
W12-3311,W10-3712,0,\N,Missing
W12-3311,W10-3709,0,\N,Missing
W12-3311,W11-0803,0,\N,Missing
W12-3311,W10-3705,0,\N,Missing
W12-3311,W10-3708,0,\N,Missing
W12-3311,W11-0817,0,\N,Missing
W12-3311,W10-3711,0,\N,Missing
W12-3311,W11-0801,0,\N,Missing
W12-3311,W11-0816,0,\N,Missing
W12-3311,W10-3710,0,\N,Missing
W12-3311,W10-3714,0,\N,Missing
W12-3311,W10-3706,0,\N,Missing
W12-3311,W10-3713,0,\N,Missing
W12-3311,W12-0911,1,\N,Missing
W12-3311,W11-0806,0,\N,Missing
W12-3311,P98-2226,0,\N,Missing
W12-3311,C98-2221,0,\N,Missing
W12-3311,C10-2132,0,\N,Missing
W12-3311,daille-etal-2004-french,0,\N,Missing
W12-3311,2006.iwslt-evaluation.3,0,\N,Missing
W12-3311,W12-4002,1,\N,Missing
W12-3311,mangeot-chalvin-2006-dictionary,0,\N,Missing
W12-3311,I05-3007,0,\N,Missing
W12-3311,W99-0610,0,\N,Missing
W12-3311,fritzinger-etal-2010-survey,0,\N,Missing
W12-3311,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
W12-3311,E06-1043,0,\N,Missing
W12-3311,C10-2093,0,\N,Missing
W12-3311,zaninello-nissim-2010-creation,0,\N,Missing
W12-3311,D11-1060,0,\N,Missing
W12-3311,W11-0819,0,\N,Missing
W12-3311,boulaknadel-etal-2008-multi,0,\N,Missing
W12-3311,2009.mtsummit-btm.1,0,\N,Missing
W12-3311,W11-0822,1,\N,Missing
W12-3311,P00-1056,0,\N,Missing
W12-3311,W06-1208,0,\N,Missing
W12-3311,J03-3001,0,\N,Missing
W12-3311,W03-1804,0,\N,Missing
W12-3311,W11-0818,0,\N,Missing
W12-3311,W11-0823,0,\N,Missing
W12-3904,baroni-bernardini-2004-bootcat,0,0.0329397,"set of seven ontologies that cover the different aspects of the domain of organizing scientific conferences. We have used this dataset as the basis for generating our corpora. 3 Methodology The main contribution of this paper is the proposal of the methodology to build corpora. This section describes the proposed methodology presenting our own corpus crawler, but also its application to construct three corpora, in English, Portuguese, and French. These corpora are constructed from the MultiFarm dataset. 3.1 Tools and Resources Instead of using an off-the-shelf web corpus tool such as BootCaT (Baroni and Bernardini, 2004), we implemented our own corpus crawler. This allowed us to have more control on query and corpus construction process. Even though our corpus construc4 www.cs.vu.nl/˜laurah/oaei/2009 oaei.ontologymatching.org/2008/ mldirectory 6 web.informatik.uni-mannheim.de/ multifarm 5 tion strategy is similar to the one implemented in BootCaT, there are some significant practical issues to take into account, such as: • The predominance of multiword keywords; • The use of the fixed keyword conference; • The expert tuning of the cleaning process; • The use of a long term support search AP[b]. Besides, BootC"
W12-3904,1999.tc-1.8,0,0.095455,"the resulting corpora are evaluated (§4) and discussed 25 Proceedings of the First Workshop on Multilingual Modeling, pages 25–31, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our"
W12-3904,W11-1217,0,0.028453,"Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our work is similar to their proposed approach (see §3). Moreover, when access to parallel corpora is limited, comparable corpora can minimize data sparseness, as discussed by Skadina et"
W12-3904,W02-1030,0,0.0693583,"Missing"
W12-3904,D11-1060,0,0.0852301,"ssociation for Computational Linguistics (§5). We conclude by outlining their future applications (§ 6). 2 Related Work Web as corpus (WAC) approaches have been successfully adopted in many cases where data sparseness plays a major limiting role, either in specific linguistic constructions and words in a language (e.g. compounds and multiword expressions), or for less resourced languages in general1 . For instance, Grefenstette (1999) uses WAC for machine translation of compounds from French into English, Keller et al. (2002) for adjective-noun, noun-noun and verb-object bigram discovery, and Kim and Nakov (2011) for compound interpretation. Although a corpus derived from the web may contain noise, the sheer size of data available should compensate for that. Baroni and Ueyama (2006) discuss in details the process of corpus construction from web pages for both generic and domainspecific corpora. In particular, they focus on the cleaning process applied to filter the crawled web pages. Much of the methodology applied in our work is similar to their proposed approach (see §3). Moreover, when access to parallel corpora is limited, comparable corpora can minimize data sparseness, as discussed by Skadina et"
W12-3904,P03-1054,0,0.00322028,"pplied to remove very short sentences (less than 3 words), email addresses, URLs and dates, since the main purpose of the corpus is related to concept, instance and relations extraction. Finally, heuristics to filter out page menus and footnotes are included, leaving only the text of the body of the page. The raw version of the text still contains those expressions in case they are needed for other purposes. In the second step, the text undergoes linguistic annotation, where sentences are automatically lemmatized, POS tagged and parsed. Three well-known parsers were employed: Stanford parser (Klein and Manning, 2003) for texts in English, PALAVRAS (Bick, 2000) for texts in Portuguese, and Berkeley parser (Petrov et al., 2006) for texts in French. 4 Evaluation The characteristics of the resulting corpora are summarized in tables 2 and 3. Column D of table 2 shows that the number of documents retrieved is much higher in en than in pt and fr, and this is not proportional to the number of queries (Q). Indeed, if we look in table 3 at the average ratio of documents retrieved per query (D/Q), the en queries return much more documents than queries in other languages. This indicates that the search engine returns"
W12-3904,W11-1205,0,0.0292069,". Comparable corpora is a very active research subject, being in the core of several European projects (e.g. TTC2 , Accurat3 ). Nonetheless, to date most of 1 Kilgarriff (2007) warns about the dangers of statistics heavily based on a search engine. However, since we use the downloaded texts of web pages instead of search engine count estimators, this does not affect the results obtained in this work. 2 www.ttc-project.eu 3 www.accurat-project.eu 26 the research on comparable corpora seems to focus on lexicographic tasks (Forsyth and Sharoff, 2011; Sharoff, 2006), bilingual lexicon extraction (Morin and Prochasson, 2011), and more generally on machine translation and related applications (Ion et al., 2011). Likewise, there is much to be gained from the potential mutual benefits of comparable corpora and ontology-related tasks. Regarding multilingually aligned ontologies, very few data sets have been made available for use in the research community. Examples include the vlcr4 and the mldirectory5 datasets. The former contains a reduced set of alignments between the thesaurus of the Netherlands Institute for Sound and Vision and two other resources, English WordNet and DBpedia. The latter consists of a set of a"
W12-3904,P06-1055,0,0.016967,"of the corpus is related to concept, instance and relations extraction. Finally, heuristics to filter out page menus and footnotes are included, leaving only the text of the body of the page. The raw version of the text still contains those expressions in case they are needed for other purposes. In the second step, the text undergoes linguistic annotation, where sentences are automatically lemmatized, POS tagged and parsed. Three well-known parsers were employed: Stanford parser (Klein and Manning, 2003) for texts in English, PALAVRAS (Bick, 2000) for texts in Portuguese, and Berkeley parser (Petrov et al., 2006) for texts in French. 4 Evaluation The characteristics of the resulting corpora are summarized in tables 2 and 3. Column D of table 2 shows that the number of documents retrieved is much higher in en than in pt and fr, and this is not proportional to the number of queries (Q). Indeed, if we look in table 3 at the average ratio of documents retrieved per query (D/Q), the en queries return much more documents than queries in other languages. This indicates that the search engine returns more distinct results in en and more duplicate URLs in fr and in pt. The high discrepancy in 7 research.google"
W12-4002,W11-2609,0,0.0951373,"Missing"
W13-1014,W06-1207,0,0.0310531,"the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUTIVE PARTICLE of pronominal verbs is the only one in which the verb and the clitic form a multiword lexical unit on its own. In the other uses, the clitic keeps a separate syntactic and/or semantic function, as presented in Table 1. The particle se is an integral part of pronominal verbs in the same way as the particles of English phrasal verbs. As future work, we would like to investigate possible semantic contributions of the se particle to the meaning of pronominal verbs, as done by Cook and Stevenson (2006), for example, who try to automatically classify the uses of the particle up in verb-particle constructions. Like in the present paper, they estimate a set of linguistic features which are in turn used to train a Support Vector Machine (SVM) classifier citecook:2006:mwe. 3 Methodology For the automatic identification of multiword verb+se occurrences, we performed corpus searches on the PLN-BR-FULL corpus (Muniz et al., 2007), which consists of news texts extracted from a major Brazilian newspaper, Folha de S˜ao Paulo, from 1994 to 2005, with 29,014,089 tokens. The corpus was first preprocessed"
W13-1014,C10-3015,1,0.771149,"Missing"
W13-1014,slavcheva-2006-semantic,0,0.0193784,"SIVE uses (Morais Nunes, 1990; Cyrino, 2007; Pereira-Santos, 2010); REFLEXIVE use (Godoy, 2012), and IN CHOATIVE use (Fonseca, 2010; Nunes-Ribeiro, 2010; Ros´ario Ribeiro, 2011). Despite none of these works concerning specifically pronominal verbs, they provided us an important theoretical basis for the analysis undertaken herein. The problem of the multifunctional use of clitic pronouns is not restricted to Portuguese. Romance languages, Hebrew, Russian, Bulgarian and others also have similar constructions. There are crosslinguistic studies regarding this matter reported in Siloni (2001) and Slavcheva (2006), showing that there are partial coincidence of verbs taking clitic pronouns to produce alternations and reflexive voice. From an NLP perspective, the problem of the ambiguity of the clitic pronoun se was studied by Martins et al. (1999) to solve a problem of categorization, that is, to decide which part-of-speech tag should be assigned to se. However, we have not found studies regarding pronominal verbs aiming at Portuguese automatic language processing. Even though in Portuguese all the uses of the clitic pronoun se share the same realization at the surface form level, the use as a CONSTITUT"
W13-1014,vincze-2012-light,0,0.0198436,"n the one hand, the annotation of uses can be semi-automatically projected on the sentences extracted from the corpus. On the other hand, the findings of this work in terms of syntactic and semantic characteristics can be used to propose features for the classifier, trying to reproduce those that can be automatically obtained (e.g., subcategorization frame) and to simulate those that cannot be easily automated (e.g., whether the subject is animate). For these future experiments, we intend to compare different learning models, based on SVM and on sequence models like conditional random fields (Vincze, 2012). As languages are different in what concerns allowed alternations, the use of clitic se in Portuguese becomes even more complex when approached from a bilingual point of view. Depending on how different the languages compared are, the classification of se adopted here may be of little use. For example, several verbs classified as reflexive in Portuguese, like vestir-se (to dress), barbear-se (to shave) and demitir-se (to resign) are not translated into a reflexive form in English (*to dress oneself, *to shave oneself and *to dismiss oneself ). Similarly, typical inchoative verb uses in Portug"
W15-0908,W11-0822,1,0.89726,"Missing"
W15-0908,W10-3704,0,0.164652,"Missing"
W15-0908,J90-1003,0,0.0766432,"es 45–53, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The remainder of this paper is structured as follows: we discuss related work on MWE extraction (Section 2) and never-ending learning methods (Section 3). Then, we present the architecture and detail the modules in NEMWEL (Section 4). Finaly, we present the results of automatic and manual evaluation in Brazilian Portuguese (Section 5) and ideas for future work (Section 6). 2 MWE Extraction Automatic unsupervised MWE learning from corpora has been proposed based on pairwise association measures (Church and Hanks, 1990; Smadja, 1993; Pedersen et al., 2011), string matching (Duan et al., 2006), extraction patterns based on expert linguistic knowledge and automatic analysis (Justeson and Katz, 1995; Seretan and Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they a"
W15-0908,W11-0809,0,0.0483134,"09) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annota"
W15-0908,P06-2023,0,0.0314085,"tational Linguistics The remainder of this paper is structured as follows: we discuss related work on MWE extraction (Section 2) and never-ending learning methods (Section 3). Then, we present the architecture and detail the modules in NEMWEL (Section 4). Finaly, we present the results of automatic and manual evaluation in Brazilian Portuguese (Section 5) and ideas for future work (Section 6). 2 MWE Extraction Automatic unsupervised MWE learning from corpora has been proposed based on pairwise association measures (Church and Hanks, 1990; Smadja, 1993; Pedersen et al., 2011), string matching (Duan et al., 2006), extraction patterns based on expert linguistic knowledge and automatic analysis (Justeson and Katz, 1995; Seretan and Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been"
W15-0908,W14-0812,0,0.0232773,"expert linguistic knowledge and automatic analysis (Justeson and Katz, 1995; Seretan and Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia"
W15-0908,W14-0802,0,0.276449,"eson and Katz, 1995; Seretan and Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candida"
W15-0908,D11-1060,0,0.0215529,"MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annotated as true or false MWEs. 2 Such models require corpora where sentences are annotated with the MWE sequences they contain. 46 2010), WordNet synonyms (Pearce, 2001) and distributional neighbors (Reddy et al., 2011). The web has also been considered as a source for MWE learning, often using page hit counts from search engines (Lapata and Keller, 2005; Kim and Nakov, 2011). However, in related work, candidates are not extracted from web texts, but from traditional corpora. Differently from previous corpus-based or web-based learning approaches, our goal is not to build one static MWE lexicon. Instead, we propose to build a system that continuously learns new expressions from the web. It populates and enriches the lexicon with new MWEs every day. Our proposal is to employ bootstrapping on a traditional supervised machine learning setting, enriched with new features and dynamically crawled corpora. At any given time, a snapshot of the database will include the cu"
W15-0908,W08-0615,0,0.0333139,"d LibSVM (Chang and Lin, 2011) as the core. The result is a support vector machine that distinguishes true MWEs from ordinary noun phrases. As training data, it uses previously annotated instances. The Promoter is generated based on examples that were already classified, either manually, for the Promoter-0, or manually+automatically, for the Promoters built in subsequent iterations. SVM was the chosen classifier because it has presented good performance on diverse NLP tasks such as text categorization (Sassano, 2003), sentiment analysis (Mullen and Collier, 2004) and named entity recognition (Li et al., 2008), as well as standard corpus-based MWE extraction (Farahmand and Martins, 2014). 5 Evaluation An initial training corpus was generated from texts of the G1 news portal. From this corpus, NEMWEL extracted 1,100 candidate MWEs which were manually annotated by two native speakers of Brazilian Portuguese: 600 candidates for each one with an intersection of 100 candidates. The annotation interface showed the candidate and the sentences from the G1 corpus from which the candidate was extracted (see Figure 6). The annotators had to perform a binary choice as to whether the candidate was a true MWE (“"
W15-0908,W04-3253,0,0.0514046,"l trained using Weka (Hall et al., 2009) as a wrapper and LibSVM (Chang and Lin, 2011) as the core. The result is a support vector machine that distinguishes true MWEs from ordinary noun phrases. As training data, it uses previously annotated instances. The Promoter is generated based on examples that were already classified, either manually, for the Promoter-0, or manually+automatically, for the Promoters built in subsequent iterations. SVM was the chosen classifier because it has presented good performance on diverse NLP tasks such as text categorization (Sassano, 2003), sentiment analysis (Mullen and Collier, 2004) and named entity recognition (Li et al., 2008), as well as standard corpus-based MWE extraction (Farahmand and Martins, 2014). 5 Evaluation An initial training corpus was generated from texts of the G1 news portal. From this corpus, NEMWEL extracted 1,100 candidate MWEs which were manually annotated by two native speakers of Brazilian Portuguese: 600 candidates for each one with an intersection of 100 candidates. The annotation interface showed the candidate and the sentences from the G1 corpus from which the candidate was extracted (see Figure 6). The annotators had to perform a binary choic"
W15-0908,W11-0821,0,0.0142592,"09) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annota"
W15-0908,I11-1024,0,0.035452,"onal random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annotated as true or false MWEs. 2 Such models require corpora where sentences are annotated with the MWE sequences they contain. 46 2010), WordNet synonyms (Pearce, 2001) and distributional neighbors (Reddy et al., 2011). The web has also been considered as a source for MWE learning, often using page hit counts from search engines (Lapata and Keller, 2005; Kim and Nakov, 2011). However, in related work, candidates are not extracted from web texts, but from traditional corpora. Differently from previous corpus-based or web-based learning approaches, our goal is not to build one static MWE lexicon. Instead, we propose to build a system that continuously learns new expressions from the web. It populates and enriches the lexicon with new MWEs every day. Our proposal is to employ bootstrapping on a traditional sup"
W15-0908,S13-1039,0,0.31692,"n tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annotated as true or false MWEs. 2 Such models require corpora where sentences are annotated with the MWE sequences they contain. 46 2010), WordNet synonyms (Pearce, 2001) and distributional neighbors (Reddy et al., 2011). The web has also been considered as a source for MWE learning, often using page hit counts from search engines (Lapata and Keller, 2005; Kim and Nakov, 2011). However, in related work, candidates are not extracted from web texts, but from traditional corpora. Differe"
W15-0908,W03-1027,0,0.0422104,"omoter applies a classification model trained using Weka (Hall et al., 2009) as a wrapper and LibSVM (Chang and Lin, 2011) as the core. The result is a support vector machine that distinguishes true MWEs from ordinary noun phrases. As training data, it uses previously annotated instances. The Promoter is generated based on examples that were already classified, either manually, for the Promoter-0, or manually+automatically, for the Promoters built in subsequent iterations. SVM was the chosen classifier because it has presented good performance on diverse NLP tasks such as text categorization (Sassano, 2003), sentiment analysis (Mullen and Collier, 2004) and named entity recognition (Li et al., 2008), as well as standard corpus-based MWE extraction (Farahmand and Martins, 2014). 5 Evaluation An initial training corpus was generated from texts of the G1 news portal. From this corpus, NEMWEL extracted 1,100 candidate MWEs which were manually annotated by two native speakers of Brazilian Portuguese: 600 candidates for each one with an intersection of 100 candidates. The annotation interface showed the candidate and the sentences from the G1 corpus from which the candidate was extracted (see Figure 6"
W15-0908,Q14-1016,0,0.0579752,"misch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annotated as true or false MWEs. 2 Such models require corpora where sentences are annotated with the MWE sequences they contain. 46 2010), WordNet synonyms (Pearce, 2001) and distributional neighbors (Reddy et al., 2011). The web has also been considered as a source for MWE learning, often using pa"
W15-0908,J93-1007,0,0.0234051,"rado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics The remainder of this paper is structured as follows: we discuss related work on MWE extraction (Section 2) and never-ending learning methods (Section 3). Then, we present the architecture and detail the modules in NEMWEL (Section 4). Finaly, we present the results of automatic and manual evaluation in Brazilian Portuguese (Section 5) and ideas for future work (Section 6). 2 MWE Extraction Automatic unsupervised MWE learning from corpora has been proposed based on pairwise association measures (Church and Hanks, 1990; Smadja, 1993; Pedersen et al., 2011), string matching (Duan et al., 2006), extraction patterns based on expert linguistic knowledge and automatic analysis (Justeson and Katz, 1995; Seretan and Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation e"
W15-0908,C10-2144,0,0.110792,"terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annotated as true or false MWEs. 2 Such models require corpora where sentences are annotated with the MWE sequences they contain. 46 2010), WordNet synonyms (Pearce, 2001) and distributional neighbors (Reddy et al., 2011). The web has also been considered as a source for MWE learning, often using page hit counts from search engines (Lapata and Keller, 2005; Kim and Nakov, 2011). However, in related work, candidates are not extracted from web"
W15-0908,W11-4529,0,0.0310949,"nd Wehrli, 2009) or a combination of these methods (Araujo et al., 2011). Supervised machine learning methods have also been used for MWE lexicon learning.1 Pecina (2008) proposes a logistic regression classifier which uses as features a set of 84 different lexical association measures. Ramisch et al. (2008) use decision trees for classifying MWEs based on standard association measures as well, but they add variation entropy. In terms of classifiers, many alternatives have been tested like bayesian networks (Dubremetz and Nivre, 2014) and support vector machines (Farahmand and Martins, 2014). Zilio et al. (2011) use a stable set of features, but compare several classification algorithms implemented in Weka. Furthermore, in-context MWE tagging has been performed using sequence learning models like conditional random fields (Constant and Sigogne, 2011) and structured perceptron (Schneider et al., 2014).2 Many alternative sources and methods have been tested for MWE extraction, like parallel texts (Caseli et al., 2010; Tsvetkov and Wintner, 2010), bilingual lexicons (Salehi and Cook, 2013), Wikipedia interlingual links (Attia et al., 1 Usually, such methods require a list of candidate expressions annota"
W16-1804,W15-0904,0,0.134987,"seful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset"
W16-1804,P16-2026,1,0.80584,"been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset quality in these three languages. This analysis aims at studying the distributions and characteristics of the human ratings, examining quality measures for the collected data, and measuring the impact of simple filtering techniques on these quality measures. In particular, we look at how the scores obtained are distributed across the compositionality scale, whether the scores of the individual components are correlated with This paper analyzes datasets with numerical scores that"
W16-1804,I11-1024,0,0.426514,"Judgments Silvio Cordeiro1,2 , Carlos Ramisch1 , Aline Villavicencio2 1 Aix Marseille Université, CNRS, LIF UMR 7279 (France) 2 Institute of Informatics, Federal University of Rio Grande do Sul (Brazil) silvioricardoc@gmail.com carlos.ramisch@lif.univ-mrs.fr avillavicencio@inf.ufrgs.br Abstract Low values imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction function"
W16-1804,W13-1005,0,0.665579,"imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters r"
W16-1804,W15-0909,0,0.0827082,"ramisch@lif.univ-mrs.fr avillavicencio@inf.ufrgs.br Abstract Low values imply idiomaticity, while high values imply compositionality. This information can be useful, e.g. to decide how an MWE should be translated (Cap et al., 2015). Many datasets with compositionality judgments have been collected (e.g. Gurrutxaga and Alegria (2013) and McCarthy et al. (2003)). Reddy et al. (2011) asked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English,"
W16-1804,D15-1201,0,0.241386,"ked Mechanical Turkers to annotate 90 English noun-noun compounds on a scale from 0 to 5 with respect to the literality of member words. This resource has been used to evaluate compositionality prediction systems (Salehi et al., 2015). A similar resource has been created for German by Roller et al. (2013), who propose two filtering techniques adopted in our experiments. Farahmand et al. (2015) created a dataset of 1042 compounds in English with binary annotations by 4 experts. The sum of the binary judgments has been used as a numerical score to evaluate compositionality prediction functions (Yazdani et al., 2015). In this paper we report a cross-lingual examination of quality measures and filtering strategies for compound compositionality annotations. Using the dataset by Reddy et al. (2011) and its extension to English, French and Portuguese by Ramisch et al. (2016), we examine the filters reported by Roller et al. (2013) for German and assess whether they improve overall dataset quality in these three languages. This analysis aims at studying the distributions and characteristics of the human ratings, examining quality measures for the collected data, and measuring the impact of simple filtering tec"
W16-1804,W03-1810,0,\N,Missing
W16-1804,W15-0903,0,\N,Missing
W16-1804,W13-1017,0,\N,Missing
W17-1704,J13-1009,0,0.0664231,"Missing"
W17-1704,W06-2408,0,0.14106,"Missing"
W17-1704,C14-1177,0,0.0618437,"Missing"
W17-1704,H05-1004,0,0.013573,"0. Note that these measures operate both on a micro scale (the optimal bijections are looked for within a given sentence) and a macro scale (the results are summed up for all sentences in the corpus). Alternatively, micro-only measures, i.e. the average values of precision and recall for individual sentences, could be considered. Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate. Note also that the measures in (2) are comparable to the CEAF-M measures (Luo, 2005) used in the coreference resolution task.20 There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for. The main difference with our approach resides in the fact that, while coreference • T P 1max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t2,t3} |= 2 R = T P 1max /||G ||= 2/3 P = T P 1max /||S1 ||= 2/3. • T P 2max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |= 2 R = T P 2max /||G ||= 2/3 P = T P 2max /||S2 ||= 2/3. • T P 3max = |{t1,t2} ∩ {t1} |+ |{t3} ∩ {t3} |+ |∅ ∩ {t2} |+ |∅ ∩ {t1,t3} |= 2 R = T P 3max /||G ||= 2/3 P = T P 3max /||S3"
W17-1704,I11-1024,0,0.0544171,"Missing"
W17-1704,J15-3003,0,0.143842,"ation of this large project included the definition of roles – project leaders, technical experts, language group leaders (LGLs), language leaders (LLs) and annotators – and their tasks. Annotation Methodology In order to bring about substantial progress in the state of the art presented in the preceding section, the European PARSEME network5 , dedicated to parsing and MWEs, proposed a shared task on automatic identification of VMWEs. This initiative required the construction of a large multilingual VMWE-annotated corpus. Within the challenging features of linguistic annotation, as defined by Mathet et al. (2015), the VMWE annotation task is concerned by: 3.1 The biggest challenge in the initial phase of the project was the development of the annotation guidelines7 which would be as universal as possible but which would still allow for languagespecific categories and tests. To this end, a twophase pilot annotation in most of the participating languages was carried out. Some corpora were annotated at this stage not only by native but also by near-native speakers, so as to promote cross-language convergences. Each pilot annotation phase provided feedback from annotators and was followed by enhancements"
W17-1704,Q14-1016,0,0.0486965,"and/or verbal idioms. They also underline the heterogeneity of these MWE annotations. Nivre and Vincze (2015) show that this is also the case in the treebanks of Universal Dependencies (UD), despite the homogenizing objective of the UD project (McDonald et al., 2013). More recent efforts (Adalı et al., 2016), while addressing VMWEs in a comprehensive way, still suffer from missing annotation standards. 3 2 4 http://multiword.sourceforge.net/sharedtask2017 32 http://multiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al"
W17-1704,P15-1108,1,0.941692,"ultiword.sf.net/ http://dimsum16.github.io (DE) auf|machen (lit. out|make) ’open’.6 discontinuous. They were annotated following Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portug"
W17-1704,C16-1042,1,0.824829,"r et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that the 4 last are non-Indo-E"
W17-1704,S16-1084,0,0.0544884,"Missing"
W17-1704,W10-3705,0,0.0808784,"Missing"
W17-1704,W11-0807,0,0.0214913,"Missing"
W17-1704,W14-0804,0,0.0310488,"owing Schneider et al. (2014b), and thus contain several VMWEs types on top of non-verbal MWEs. Links between MWE identification and syntactic parsing have also long been an issue. While the former has often been treated as a pre-processing step before the latter, both tasks are now more and more often integrated, in particular for continuous MWE categories (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Fewer works deal with verbal MWEs (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). 3 This complexity is largely increased by the multilingual nature of the task, and calls for efficient project management. The 21 participating languages were divided into four language groups (LGs): Balto-Slavic: Bulgarian (BG), Croatian (HR), Czech (CS), Lithuanian (LT), Polish (PL) and Slovene (SL); Germanic: English (EN), German (DE), Swedish (SV) and Yiddish (YI); Romance: French (FR), Italian (IT), Romanian (RO), Spanish (ES) and Brazilian Portuguese (PT); and others: Farsi (FA), Greek (EL), Hebrew (HE), Hungarian (HU), Maltese (MT) and Turkish (TR). Note that t"
W17-1704,S12-1010,0,0.026905,"Missing"
W17-1704,W16-1812,0,0.0461931,"Missing"
W17-1704,C10-1125,1,0.793436,"Missing"
W17-1704,R11-1040,1,0.890663,"Missing"
W17-1704,N09-1037,0,\N,Missing
W17-1704,P14-1070,1,\N,Missing
W17-1704,P16-1016,0,\N,Missing
W17-1711,barreiro-etal-2014-linguistic,0,0.0307394,"ctic structure, using aligned dependency-parsed corpora for discovery (Zarrieß and Kuhn, 2009). Instead of focusing on 1-tomany alignments, Tsvetkov and Wintner (2010) propose a method which incrementally removes from parallel sentences word pairs that are surely not MWEs. Therefore, they use bilingual dictionaries and alignment reliability scores. The remaining units are considered candidate MWEs. Bilingual lexicons containing MWEs are important resources for MT systems. It has been shown that the presence of MWEs can harm the quality of both statistical (Ramisch et al., 2013) and rulebased (Barreiro et al., 2014) MT systems. Simple techniques for taking MWEs into account such as binary features (Carpuat and Diab, 2010) and special token markers (Cap et al., 2015) can help improving translation quality. However, this may not suffice if the expressions are not correctly identified with the help of bilingual MWE lexicons. P (tj |si ) = c(si , tj ) c(si ) Here, c(si , tj ) is the number of times a source candidate si was found in a sentence whose transla1 92 http://mwetoolkit.sf.net/ tion contained tj and c(si ) is simply the number of occurrences of the candidate in the source corpus. Since candidates si"
W17-1711,L16-1662,0,0.0653751,"Missing"
W17-1711,W15-0903,0,0.0266296,"er (2010) propose a method which incrementally removes from parallel sentences word pairs that are surely not MWEs. Therefore, they use bilingual dictionaries and alignment reliability scores. The remaining units are considered candidate MWEs. Bilingual lexicons containing MWEs are important resources for MT systems. It has been shown that the presence of MWEs can harm the quality of both statistical (Ramisch et al., 2013) and rulebased (Barreiro et al., 2014) MT systems. Simple techniques for taking MWEs into account such as binary features (Carpuat and Diab, 2010) and special token markers (Cap et al., 2015) can help improving translation quality. However, this may not suffice if the expressions are not correctly identified with the help of bilingual MWE lexicons. P (tj |si ) = c(si , tj ) c(si ) Here, c(si , tj ) is the number of times a source candidate si was found in a sentence whose transla1 92 http://mwetoolkit.sf.net/ tion contained tj and c(si ) is simply the number of occurrences of the candidate in the source corpus. Since candidates si and tj can be discontinuous, their numbers of occurrences are not necessarily n-gram counts, but must be obtained during monolingual candidate discovery"
W17-1711,N10-1029,0,0.0252918,"focusing on 1-tomany alignments, Tsvetkov and Wintner (2010) propose a method which incrementally removes from parallel sentences word pairs that are surely not MWEs. Therefore, they use bilingual dictionaries and alignment reliability scores. The remaining units are considered candidate MWEs. Bilingual lexicons containing MWEs are important resources for MT systems. It has been shown that the presence of MWEs can harm the quality of both statistical (Ramisch et al., 2013) and rulebased (Barreiro et al., 2014) MT systems. Simple techniques for taking MWEs into account such as binary features (Carpuat and Diab, 2010) and special token markers (Cap et al., 2015) can help improving translation quality. However, this may not suffice if the expressions are not correctly identified with the help of bilingual MWE lexicons. P (tj |si ) = c(si , tj ) c(si ) Here, c(si , tj ) is the number of times a source candidate si was found in a sentence whose transla1 92 http://mwetoolkit.sf.net/ tion contained tj and c(si ) is simply the number of occurrences of the candidate in the source corpus. Since candidates si and tj can be discontinuous, their numbers of occurrences are not necessarily n-gram counts, but must be ob"
W17-1711,J90-1003,0,0.337546,"heir translations from parallel corpora. First, we apply independent monolingual MWE extraction in source and target languages simultaneously. Then, we calculate translation probability, association score and distributional similarity of co-occurring pairs. Finally, we rank all translations of a given MWE using a linear combination of these features. Preliminary experiments on light verb constructions show promising results. 1 Introduction The automatic discovery of multiword expressions (MWEs) has been a topic of interest in the computational linguistics community for a while (Choueka, 1988; Church and Hanks, 1990). In the last 20 years, multilingual discovery of MWEs has gained some popularity thanks to the widespread use of statistical machine translation (MT), automatic word alignment tools and freely available parallel corpora (Zarrieß and Kuhn, 2009; Attia et al., 2010; Caseli et al., 2010). MWEs tend to be non compositional or show some kind of lexicosyntactic inflexibility, which is often reflected in translation asymmetries (Manning and Sch¨utze, 1999). Therefore, parallel corpora are rich resources to mine for MWEs. Techniques adapted from machine translation can help to exploit translation inf"
W17-1711,W10-3704,0,0.050124,"Missing"
W17-1711,W97-0311,0,0.308145,"ionaries. Afterwards, the existence of these automatically generated potential translations can be assessed in large monolingual corpora (Morin and Daille, 2010). This can be used as a feature, among other sources of information, in supervised or semi-supervised monolingual MWE discovery (Tsvetkov and Wintner, 2011; Rondon et al., 2015). Bilingual dictionaries can also be used to predict the compositionality of MWEs by estimating the string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014b) between translations of an MWE and of the single words it contains. Melamed (1997) describes one of the earliest attempts to extract MWEs from parallel corpora. The method is based on lexical alignment and mutual information. Statistical lexical alignment can provide straightforward MWE candidates, which can be further filtered using POS patterns and association scores. If two or more words in a source language are aligned to the same word on the target side, the source is likely an MWE (Caseli et al., 2010). Conversely, one can assume that some types of MWEs such as verbnoun combinations tend to be translated as MWEs with the same syntactic structure, using aligned depende"
W17-1711,J14-2007,0,0.0772891,"Missing"
W17-1711,2013.mtsummit-wmwumttt.8,0,0.0136362,"ranslated as MWEs with the same syntactic structure, using aligned dependency-parsed corpora for discovery (Zarrieß and Kuhn, 2009). Instead of focusing on 1-tomany alignments, Tsvetkov and Wintner (2010) propose a method which incrementally removes from parallel sentences word pairs that are surely not MWEs. Therefore, they use bilingual dictionaries and alignment reliability scores. The remaining units are considered candidate MWEs. Bilingual lexicons containing MWEs are important resources for MT systems. It has been shown that the presence of MWEs can harm the quality of both statistical (Ramisch et al., 2013) and rulebased (Barreiro et al., 2014) MT systems. Simple techniques for taking MWEs into account such as binary features (Carpuat and Diab, 2010) and special token markers (Cap et al., 2015) can help improving translation quality. However, this may not suffice if the expressions are not correctly identified with the help of bilingual MWE lexicons. P (tj |si ) = c(si , tj ) c(si ) Here, c(si , tj ) is the number of times a source candidate si was found in a sentence whose transla1 92 http://mwetoolkit.sf.net/ tion contained tj and c(si ) is simply the number of occurrences of the candidate in"
W17-1711,W09-2904,0,0.0607343,"rring pairs. Finally, we rank all translations of a given MWE using a linear combination of these features. Preliminary experiments on light verb constructions show promising results. 1 Introduction The automatic discovery of multiword expressions (MWEs) has been a topic of interest in the computational linguistics community for a while (Choueka, 1988; Church and Hanks, 1990). In the last 20 years, multilingual discovery of MWEs has gained some popularity thanks to the widespread use of statistical machine translation (MT), automatic word alignment tools and freely available parallel corpora (Zarrieß and Kuhn, 2009; Attia et al., 2010; Caseli et al., 2010). MWEs tend to be non compositional or show some kind of lexicosyntactic inflexibility, which is often reflected in translation asymmetries (Manning and Sch¨utze, 1999). Therefore, parallel corpora are rich resources to mine for MWEs. Techniques adapted from machine translation can help to exploit translation information for the specific needs of MWE discovery. Parallel corpora can be useful for MWE discovery in many ways. First, a second (target) language can be used to model features, which in turn help in the discovery of new MWEs in a single (sourc"
W17-1711,W15-0908,1,0.847462,"ulate the conditional probability of each potential translation (tj ) in T given a source (si ): Another possibility to model non-translatability without recurring to parallel corpora consists in building up artificial word-for-word MWE translations using bilingual single-word dictionaries. Afterwards, the existence of these automatically generated potential translations can be assessed in large monolingual corpora (Morin and Daille, 2010). This can be used as a feature, among other sources of information, in supervised or semi-supervised monolingual MWE discovery (Tsvetkov and Wintner, 2011; Rondon et al., 2015). Bilingual dictionaries can also be used to predict the compositionality of MWEs by estimating the string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014b) between translations of an MWE and of the single words it contains. Melamed (1997) describes one of the earliest attempts to extract MWEs from parallel corpora. The method is based on lexical alignment and mutual information. Statistical lexical alignment can provide straightforward MWE candidates, which can be further filtered using POS patterns and association scores. If two or more words in a source"
W17-1711,S13-1039,0,0.0545964,"., 2010; Caseli et al., 2010). MWEs tend to be non compositional or show some kind of lexicosyntactic inflexibility, which is often reflected in translation asymmetries (Manning and Sch¨utze, 1999). Therefore, parallel corpora are rich resources to mine for MWEs. Techniques adapted from machine translation can help to exploit translation information for the specific needs of MWE discovery. Parallel corpora can be useful for MWE discovery in many ways. First, a second (target) language can be used to model features, which in turn help in the discovery of new MWEs in a single (source) language (Salehi and Cook, 2013; Caseli 2 Related Work Multilingual resources in general can be used for MWE discovery. Attia et al. (2010), for instance, do not rely on parallel texts but on short Wikipedia page titles, cross-linked across multiple languages. They consider that, if a page whose title contains a cross-lingual link to a page whose title is a single word (in any available language), then the original page title is probably a MWE. Similarly, translation links in Wiktionary can be exploited, among 91 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 91–96, c Valencia, Spain, April 4. 2"
W17-1711,D14-1189,0,0.0907182,"for instance, do not rely on parallel texts but on short Wikipedia page titles, cross-linked across multiple languages. They consider that, if a page whose title contains a cross-lingual link to a page whose title is a single word (in any available language), then the original page title is probably a MWE. Similarly, translation links in Wiktionary can be exploited, among 91 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 91–96, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics 3 other features, for predicting the compositionality of MWEs (Salehi et al., 2014a). Bilingual MWE Lexicon Creation Most existing methods exploit parallel corpora to discover MWEs in a single language. They use translation information, among other sources, to confirm the idiosyncratic behaviour of the MWE in the source language, but do not output possible translations as a result of the discovery algorithm. In this section, we propose a method to create probabilistic bilingual MWE dictionaries using minimal supervision. First, we extract MWE candidates from preprocessed (POS-tagged and lemmatized) source and target texts separately. In our experiments, the texts were pre-p"
W17-1711,E14-1050,0,0.122368,"for instance, do not rely on parallel texts but on short Wikipedia page titles, cross-linked across multiple languages. They consider that, if a page whose title contains a cross-lingual link to a page whose title is a single word (in any available language), then the original page title is probably a MWE. Similarly, translation links in Wiktionary can be exploited, among 91 Proceedings of the 13th Workshop on Multiword Expressions (MWE 2017), pages 91–96, c Valencia, Spain, April 4. 2017 Association for Computational Linguistics 3 other features, for predicting the compositionality of MWEs (Salehi et al., 2014a). Bilingual MWE Lexicon Creation Most existing methods exploit parallel corpora to discover MWEs in a single language. They use translation information, among other sources, to confirm the idiosyncratic behaviour of the MWE in the source language, but do not output possible translations as a result of the discovery algorithm. In this section, we propose a method to create probabilistic bilingual MWE dictionaries using minimal supervision. First, we extract MWE candidates from preprocessed (POS-tagged and lemmatized) source and target texts separately. In our experiments, the texts were pre-p"
W17-1711,C10-2144,0,0.0187499,"exical alignment and mutual information. Statistical lexical alignment can provide straightforward MWE candidates, which can be further filtered using POS patterns and association scores. If two or more words in a source language are aligned to the same word on the target side, the source is likely an MWE (Caseli et al., 2010). Conversely, one can assume that some types of MWEs such as verbnoun combinations tend to be translated as MWEs with the same syntactic structure, using aligned dependency-parsed corpora for discovery (Zarrieß and Kuhn, 2009). Instead of focusing on 1-tomany alignments, Tsvetkov and Wintner (2010) propose a method which incrementally removes from parallel sentences word pairs that are surely not MWEs. Therefore, they use bilingual dictionaries and alignment reliability scores. The remaining units are considered candidate MWEs. Bilingual lexicons containing MWEs are important resources for MT systems. It has been shown that the presence of MWEs can harm the quality of both statistical (Ramisch et al., 2013) and rulebased (Barreiro et al., 2014) MT systems. Simple techniques for taking MWEs into account such as binary features (Carpuat and Diab, 2010) and special token markers (Cap et al"
W17-1711,D11-1077,0,0.0211393,"ences tj . To do so, we calculate the conditional probability of each potential translation (tj ) in T given a source (si ): Another possibility to model non-translatability without recurring to parallel corpora consists in building up artificial word-for-word MWE translations using bilingual single-word dictionaries. Afterwards, the existence of these automatically generated potential translations can be assessed in large monolingual corpora (Morin and Daille, 2010). This can be used as a feature, among other sources of information, in supervised or semi-supervised monolingual MWE discovery (Tsvetkov and Wintner, 2011; Rondon et al., 2015). Bilingual dictionaries can also be used to predict the compositionality of MWEs by estimating the string similarity (Salehi and Cook, 2013) or distributional similarity (Salehi et al., 2014b) between translations of an MWE and of the single words it contains. Melamed (1997) describes one of the earliest attempts to extract MWEs from parallel corpora. The method is based on lexical alignment and mutual information. Statistical lexical alignment can provide straightforward MWE candidates, which can be further filtered using POS patterns and association scores. If two or m"
W17-1723,J09-1005,0,0.0177054,"Using Sequence Models and Lexical Resources Manon Scholivet and Carlos Ramisch Aix Marseille Univ, CNRS, LIF, Marseille, France manon.scholivet@etu.univ-amu.fr carlos.ramisch@lif.univ-mrs.fr Abstract and Kulkarni, 2011) or (b) using lexicons of inflected MWEs (Silberztein et al., 2012). Things get more complicated when the target MWEs are ambiguous, though. An MWE is ambiguous when its member words can cooccur without forming an expression. For instance, to make a face is an idiom meaning ‘to show a funny facial expression’, but it can also be used literally when someone is making a snowman (Fazly et al., 2009). Additionally, the words of the expression can cooccur by chance, not forming a phrase (Boukobza and Rappoport, 2009; Shigeto et al., 2013). For example, up to is an MWE in they accepted up to 100 candidates but not in you should look it up to avoid making typos. This paper focuses on a specific category of highly frequent and ambiguous MWEs in French. Indeed, in French some of the most recurrent function words are ambiguous MWEs. For instance, some conjunctions are formed by combining adverbs like ainsi (likewise) and maintenant (now) with subordinate conjunctions like que (that). However, t"
W17-1723,W11-0805,0,0.0749762,"Missing"
W17-1723,J13-1009,0,0.161414,"Missing"
W17-1723,P15-1108,1,0.269372,"et al., 2013). For example, up to is an MWE in they accepted up to 100 candidates but not in you should look it up to avoid making typos. This paper focuses on a specific category of highly frequent and ambiguous MWEs in French. Indeed, in French some of the most recurrent function words are ambiguous MWEs. For instance, some conjunctions are formed by combining adverbs like ainsi (likewise) and maintenant (now) with subordinate conjunctions like que (that). However, they may also cooccur by chance when the adverb modifies a verb followed by a subordinate clause, as in the example taken from Nasr et al. (2015) : We present a simple and efficient tagger capable of identifying highly ambiguous multiword expressions (MWEs) in French texts. It is based on conditional random fields (CRF), using local context information as features. We show that this approach can obtain results that, in some cases, approach more sophisticated parserbased MWE identification methods without requiring syntactic trees from a treebank. Moreover, we study how well the CRF can take into account external information coming from a lexicon. 1 Introduction Identifying multiword expressions (MWEs) in running text with the help of a"
W17-1723,D09-1049,0,0.0348216,"F, Marseille, France manon.scholivet@etu.univ-amu.fr carlos.ramisch@lif.univ-mrs.fr Abstract and Kulkarni, 2011) or (b) using lexicons of inflected MWEs (Silberztein et al., 2012). Things get more complicated when the target MWEs are ambiguous, though. An MWE is ambiguous when its member words can cooccur without forming an expression. For instance, to make a face is an idiom meaning ‘to show a funny facial expression’, but it can also be used literally when someone is making a snowman (Fazly et al., 2009). Additionally, the words of the expression can cooccur by chance, not forming a phrase (Boukobza and Rappoport, 2009; Shigeto et al., 2013). For example, up to is an MWE in they accepted up to 100 candidates but not in you should look it up to avoid making typos. This paper focuses on a specific category of highly frequent and ambiguous MWEs in French. Indeed, in French some of the most recurrent function words are ambiguous MWEs. For instance, some conjunctions are formed by combining adverbs like ainsi (likewise) and maintenant (now) with subordinate conjunctions like que (that). However, they may also cooccur by chance when the adverb modifies a verb followed by a subordinate clause, as in the example ta"
W17-1723,P14-1070,0,0.225337,"Missing"
W17-1723,Q14-1016,0,0.313341,"he other hand, can take longerdistance relations and features into account when building a parse tree, at the expense of using more complex models. Sequence taggers have been proven useful in identifying MWEs. MWE identification is also sometimes included into part-of-speech (POS) taggers in the form of special tags. Experiments have shown the feasibility of sequence tagging for general expressions and named entities in English and Hungarian (Vincze et al., 2011), verbnoun idioms in English (Diab and Bhutada, 2009) and general expressions in French (Constant and Sigogne, 2011) and in English (Schneider et al., 2014). Shigeto et al. (2013) tackle specifically English function words and build a CRF from the Penn Treebank, additionally correcting incoherent annotations. We develop a similar system for French, using the MWE annotation of the French Treebank as training data. Parsing-based MWE identification requires a treebank annotated with MWEs. Lexicalized constituency parsers model MWEs as special nonterminal nodes included in regular rules (Green et al., 2013). In constituency parsers, it is possible to employ a similar approach, using special dependency labels to identify relations between words that m"
W17-1723,N10-1029,0,0.0113815,"ication methods without requiring syntactic trees from a treebank. Moreover, we study how well the CRF can take into account external information coming from a lexicon. 1 Introduction Identifying multiword expressions (MWEs) in running text with the help of a lexicon is often considered as a trivial task. In theory, one could simply scan the text once and mark (e.g. join with an underscore) all sequences of tokens that appear in the MWE lexicon. Direct matching and projection of lexical entries onto the corpus can be employed as a preprocessing step in parsing and MT (Nivre and Nilsson, 2004; Carpuat and Diab, 2010). Afterward, MWEs can be retokenized and treated as words with spaces, improving parsing and MT quality. However, this simple pipeline does not work for many categories of MWEs, since variability and inflection may pose problems. For instance, if a lexicon contains the idiom to make a face, string matching will fail to identify it in children are always making faces. Since lexicons contain canonical (lemmatized) forms, matching must take inflection into account. This can be carried out by (a) pre-analysing the text and matching lemmas and POS tags instead of word forms (Finlayson 1. Je mange b"
W17-1723,W11-0809,0,0.496957,"ing data. Parsing-based MWE identification requires a treebank annotated with MWEs. Lexicalized constituency parsers model MWEs as special nonterminal nodes included in regular rules (Green et al., 2013). In constituency parsers, it is possible to employ a similar approach, using special dependency labels to identify relations between words that make up an expression (Candito and Constant, 2014). This technique has shown good performance in identifying ambiguous grammatical MWEs in French (Nasr et al., 2015). Our paper adapts a standard CRF model like the ones proposed by Constant and Sigogne (2011) and Shigeto et al. (2013) to deal with ambiguous contiguous MWEs. Our hypothesis is that sophisticated techniques like the ones described by Green et al. (2013) and Nasr et al. (2015) are not required to obtain good performances on these expressions. 2 3 Related Work CRF-Based MWE Tagger We trained a CRF tagger using CRFSuite1 (Okazaki, 2007). We used a modified version of the French Treebank (Abeill´e et al., 2003) as trainToken identification of ambiguous MWEs in running text can be modelled as a machine learning problem that learns from MWE-annotated corpora and treebanks. To date, it has"
W17-1723,W11-0817,0,0.071615,"Missing"
W17-6941,S10-1007,0,0.070865,"Missing"
W17-6941,cholakov-etal-2014-lexical,0,0.0430054,"Missing"
W17-6941,J90-1003,0,0.568905,"itutes, all compounds have been annotated with corpus frequency, association strength and compositionality information. Frequency is used as a predictor of human familiarity with a word, assuming that the higher the frequency the more familiar the compound. The association strength is used as an indication of the conventionality of a compound, with the assumption that the higher the strength, the more conventional the compound is. This follows the agreement between association measures and conventionality found by Farahmand et al. (2015). In this work we use pointwise mutual information (PMI, Church and Hanks (1990)), an association measure widely used for MWEs. 12Both frequency and PMI are calculated based on counts from a combined corpus of around 1.91 billion tokens. The corpus is formed by a concatenation of the brWaC (Wagner Filho et al., 2016; Boos et al., 2014), the Brazilian Corpus (Berber Sardinha et al., 2008), and the Portuguese Wikipedia. For compositionality we use the scores collected by Ramisch et al. (2016). This ensures a balance of compositionality, since the dataset was designed to contain 60 compositional (e.g., acampamento militar – lit. camp military [military camp]), 60 partly comp"
W17-6941,W14-3348,0,0.0130917,"r and type of responses collected for the construction of the dataset.1 LexSubNC is potentially useful for the evaluation and development of several NLP tasks and applications. For example, it could be used to tune the development of distributional semantic models that maximize the similarity between an MWE and its substitutes, similarly to what is currently done for single words (Hill et al., 2015; Levy et al., 2015). It could also be used for the evaluation of machine translation methods that focus on non-compositional expressions, similarly to what is currently done for instance in METEOR (Denkowski and Lavie, 2014). For automatic text simplification, paraphrases could be used to replace non-compositional expressions by more explicit paraphrases (Specia et al., 2012). This paper is structured as follows: we discuss similar resources and the techniques used to collect them (§2), and describe the protocol used for collecting human responses (§3). The responses are analyzed for possible correlations between characteristics of the compounds and the responses provided (§4). We finish with conclusions and a discussion of future work (§5). 2 Related Work A variety of protocols have been adopted for collecting s"
W17-6941,W15-0904,0,0.0619943,"at http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 compounds per language for E"
W17-6941,W13-1017,0,0.0499273,"Missing"
W17-6941,S13-2025,0,0.0655306,"Missing"
W17-6941,J15-4004,0,0.239499,"n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as ve"
W17-6941,Q15-1016,0,0.0188473,"re then manually validated and classified according to the particular semantic relations involved. We examine the impact of factors like frequency, conventionality and compositionality on the number and type of responses collected for the construction of the dataset.1 LexSubNC is potentially useful for the evaluation and development of several NLP tasks and applications. For example, it could be used to tune the development of distributional semantic models that maximize the similarity between an MWE and its substitutes, similarly to what is currently done for single words (Hill et al., 2015; Levy et al., 2015). It could also be used for the evaluation of machine translation methods that focus on non-compositional expressions, similarly to what is currently done for instance in METEOR (Denkowski and Lavie, 2014). For automatic text simplification, paraphrases could be used to replace non-compositional expressions by more explicit paraphrases (Specia et al., 2012). This paper is structured as follows: we discuss similar resources and the techniques used to collect them (§2), and describe the protocol used for collecting human responses (§3). The responses are analyzed for possible correlations betwee"
W17-6941,P98-2127,0,0.0351344,"lving lexical substitution, alternatives need to be identified for a given target word (McCarthy and Navigli, 2007, 2009), usually in a particular context. Candidates can be chosen to maximize word properties that are relevant for the particular task, such as unigram and n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms"
W17-6941,W13-3512,0,0.0410611,"y. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform these tasks. Various initiatives for collecting word norms resulted in datasets such as the South Florida Association Norms (Nelson et al., 2004), SimLex-999 (Hill et al., 2015), Hyperlex (Vuli´c et al., 2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds"
W17-6941,W03-1810,0,0.0628767,"2016) and Rare Words (Luong et al., 2013). These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds (Reddy et al., 2011), and tend to focus on numerical scores that model compositionality and conventionality. Resources with lexical substitutes or paraphrases for MWEs are rare and often only include compositional expressions (Hendrickx et al., 2013). However, MWEs may also involve some degree of semantic or statistical idiosyncrasy with respect to regular combinations (Baldwin and Kim, 2010) and these may have an impact on the quality of the collected data. For instance, there may be less agreement among annotators for an idiomatic nominal compound like Black Friday as it"
W17-6941,S07-1009,0,0.0293613,"of these characteristics on the suggestions of lexical substitution made by native speakers. No strong correlations are found for these factors on the number or type of responses provided. However, a significant effect of compositionality is found in the use of one of the component words (head or modifier) as a substitute. The resulting resource, LexSubNC, contains over 1,500 manually validated substitutes for 180 compounds, further classified according to the type of response. 1 Introduction In tasks involving lexical substitution, alternatives need to be identified for a given target word (McCarthy and Navigli, 2007, 2009), usually in a particular context. Candidates can be chosen to maximize word properties that are relevant for the particular task, such as unigram and n-gram frequencies, concreteness, imageability, and conventionality. Depending on the target word, more than one possible alternative substitution may fulfill the criteria and produce an acceptable result (e.g. acquire/buy/purchase a painting). Resources such as thesauri, containing semantically related words (Fellbaum, 1998; Lin, 1998), and word norms, with information about word properties (Nelson et al., 2004), may be used to inform th"
W17-6941,S10-1002,0,0.0573036,"Missing"
W17-6941,P16-2026,1,0.850379,"is publicly available at http://pageperso.lif.univ-mrs.fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 com"
W17-6941,I11-1024,0,0.14838,"These resources form valuable gold standards for evaluating the quality of a variety of tasks and applications, including text simplification and machine translation. However, they often concentrate on single words as targets. The collection of norms for longer units is particularly challenging due to the arbitrary interactions between their member words, particularly if they involve multiword expressions (MWEs), such as nominal compounds or verbal idioms. Available MWE datasets often target specific types of MWEs such as verb-particle constructions (McCarthy et al., 2003) and noun compounds (Reddy et al., 2011), and tend to focus on numerical scores that model compositionality and conventionality. Resources with lexical substitutes or paraphrases for MWEs are rare and often only include compositional expressions (Hendrickx et al., 2013). However, MWEs may also involve some degree of semantic or statistical idiosyncrasy with respect to regular combinations (Baldwin and Kim, 2010) and these may have an impact on the quality of the collected data. For instance, there may be less agreement among annotators for an idiomatic nominal compound like Black Friday as it may be perceived as being related to var"
W17-6941,W13-1005,0,0.0175762,".fr/~carlos.ramisch/ ?page=downloads/compounds&lang=en. any part of speech, as long as the resulting paraphrase was a well-formed noun phrase. In all of these cases, the compounds were compositional: they could be paraphrased using combinations of their parts, and lexical substitution could be performed for each component individually. Other MWE datasets contain compositionality assessment, and, as a consequence, they also include idiomatic cases. Datasets for nominal compound compositionality are available in English (Reddy et al., 2011; Ramisch et al., 2016; Farahmand et al., 2015), German (Roller et al., 2013), Portuguese and French (Ramisch et al., 2016), and for noun-verb expressions in Basque (Gurrutxaga and Alegria, 2013). For instance, Reddy et al. (2011) collected numerical scores for 90 English nominal compounds regarding their compositionality. Data for each compound and component word was gathered through crowdsourcing using a 6-point scale ranging from totally idiomatic (0) to fully compositional (5). Ramisch et al. (2016) extended this set with additional compounds and also applied it to other languages, generating a total of 180 compounds per language for English, Portuguese, and French"
W17-6941,C98-2122,0,\N,Missing
W17-6941,J17-4004,0,\N,Missing
W18-4925,W17-1717,1,0.829766,"on in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our effo"
W18-4925,W17-1716,1,0.892903,"Missing"
W18-4925,P14-1070,1,0.86208,"mon guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The fi"
W18-4925,F12-2024,1,0.868442,"om the Croatian version of the SETimes corpora: mostly running text but also selected fragments, such as introductory blurbs and image descriptions characteristic of newswire text. The English corpus consists of 7,437 sentences taken from three of the UD: the Gold Standard Universal Dependencies Corpus for English, the LinES parallel corpus and the Parallel Universal Dependencies treebank. The Farsi corpus is built on top of the MULTEXT-East corpora (QasemiZadeh and Rahimi, 2006) and VMWE annotations are added to a portion of Orwell’s 1984 novel. The French corpus contains the Sequoia corpus (Candito and Seddah, 2012) converted to UD, the GDS French UD treebank, the French part of the Partut corpus, and part of the Parallel UD (PUD) corpus. The German corpus contains shuffled sentences crawled from online news, reviews and wikis, derived from the WMT16 shared task data (Bojar et al., 2016), and Universal Dependencies v2.0. The Greek corpus comprises Wikipedia articles and newswire texts from various on-line newspaper editions and news portals. The Hebrew corpus contains news and articles from Arutz 7 and HaAretz news websites, collected by the MILA Knowledge Center for Processing Hebrew. The Hindi corpus r"
W18-4925,P16-1016,0,0.0692628,"actices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generat"
W18-4925,J17-4005,1,0.881403,"Missing"
W18-4925,N09-1037,0,0.0413995,"anks for 15 languages, collaboratively documented according to common guidelines. They highlight the heterogeneity of MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural netwo"
W18-4925,D11-1067,0,0.0237076,"Missing"
W18-4925,J13-1009,0,0.0221261,"Missing"
W18-4925,W17-1707,0,0.0299396,"en et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation guidelines were gradually enhanced, so as"
W18-4925,C14-1177,0,0.023678,"Missing"
W18-4925,W14-0406,0,0.112167,"Missing"
W18-4925,W17-1715,0,0.019845,"ore integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWErelated phenomena, and towards a better synergy of terminologies across languages and linguistic traditions. The annotation"
W18-4925,P15-1108,1,0.836029,"f MWE annotation practices. Similar conclusions have been drawn for Universal Dependencies (McDonald et al., 2013). With regard to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary e"
W18-4925,W17-1706,0,0.0136581,"ing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better"
W18-4925,L16-1262,0,0.0499042,"Missing"
W18-4925,S16-1084,1,0.93644,"participating systems, their methods and obtained results are also presented and analysed. 1 Introduction Across languages, multiword expressions (MWEs) are widely recognized as a significant challenge for natural language processing (NLP) (Sag et al., 2002; Baldwin and Kim, 2010). An international and highly multilingual research community, forged via regular workshops and initiatives such as the PARSEME network (Savary et al., 2015), has rallied around the goals of characterizing MWEs in lexicons, grammars and corpora and enabling systems to process them. Recent shared tasks, namely DiMSUM (Schneider et al., 2016) and the first edition of the PARSEME Shared Task on automatic identification of verbal multiword expressions in 2017 (Savary et al., 2017), have helped drive MWE research forward, yielding new corpora and testbeds for MWEs identification systems. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 222 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 222–240 Santa Fe, New Mexico, USA, August 25-26, 2018. This paper describ"
W18-4925,W17-1705,1,0.741206,"ome popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous version of the PARSEME corpora. About 80 issues were raised and discussed among dozens of contributors.1 This boosted our efforts towards a better understanding of VMWE"
W18-4925,I13-1024,1,0.84782,"ns, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on lan"
W18-4925,C16-1042,1,0.834941,"guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not covered by the previous ver"
W18-4925,W10-3705,0,0.0133537,"rd to these conclusions, we intended to provide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new"
W18-4925,W14-0804,0,0.0128345,"ovide unified guidelines for all the participating languages, in order to avoid heterogeneous, hence incomparable, datasets. MWE identification in syntactic parsing has also gained some popularity in recent years. While often treated as a pre-processing step for parsing, both tasks are now more and more integrated (Finkel and Manning, 2009; Green et al., 2011; Green et al., 2013; Candito and Constant, 2014; Le Roux et al., 2014; Nasr et al., 2015; Constant and Nivre, 2016). Although fewer works deal with verbal MWEs, there are some notable exceptions (Wehrli et al., 2010; Vincze et al., 2013; Wehrli, 2014; Waszczuk et al., 2016). Some systems that participated in edition 1.0 of the PARSEME Shared Task are also based on parsing (Al Saied et al., 2017; Nerima et al., 2017; Simkó et al., 2017). Other approaches to MWE identification include sequence labeling using CRFs (Boro¸s et al., 2017; Maldonado et al., 2017) and neural networks (Klyueva et al., 2017). 3 Enhanced Annotation Methodology The first PARSEME annotation campaign (Savary et al., forthcoming) generated a rich feedback from annotators and language team leaders. It also attracted the interest of new teams, working on languages not cov"
W18-4932,J17-4005,1,0.854135,"Missing"
W18-4932,C18-1219,1,0.558536,"Missing"
W18-4933,W11-0809,0,0.0257245,"he RNN is to predict the correct BIO+category tag for each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for"
W18-4933,J17-4005,1,0.905563,"Missing"
W18-4933,W17-1707,0,0.0758703,", however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMULS) and the number of recurrent layers (1 in MUMULS, 2 in Veyn). In the remainder of this paper, we describe the system This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 http://multiword.sourceforge.net/sharedtask2018 2 https://github.com/zamp13/Veyn. Th"
W18-4933,W17-1715,0,0.0268949,"each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialize"
W18-4933,W95-0107,0,0.559976,"Missing"
W18-4933,W16-1816,0,0.301187,"rect BIO+category tag for each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), t"
W18-4933,Q14-1016,0,0.122787,"ence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMULS) and the number of recurrent layers (1 in MUMULS"
W18-4933,W17-1723,1,0.844061,"xternal corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMU"
W19-5110,copestake-etal-2002-multiword,0,0.130229,"ki et al., 2017; McShane et al., 2015), i.e. they implicitly assume the existence of regular grammar rules, and explicitly describe only those MWE properties which do not conform to these rules. Although these lexicons suffer from insufficient formalization (Lichte et al., 2019), they could be successfully applied to parsing after ad hoc conversion to particular grammar formalisms. On the other hand, some approaches accommodate some types of MWEs directly in the lexicons of computational grammars within particular grammatical frameworks: head-driven phrase structure grammar (Sag et al., 2002; Copestake et al., 2002; Villavicencio et al., 2004; Bond et al., 2015; Herzig Sheinfux et al., 2015), lexical functional grammar (Attia, 2006; Dyvik et al., 2019), tree-adjoining grammar (Abeillé and Schabes, 1989, 1996; Vaidya et al., 2014; Lichte and Kallmeyer, 2016), and dependency grammar (Diaconescu, 2004). of the Lexicon Grammar (Gross, 1986) and of the Explanatory Combinatorial Dictionary (Mel’ˇcuk et al., 1988; Pausé, 2018). These approaches assume that units of meaning are located at the level of elementary sentences (predicates with their arguments) rather than of words, and MWEs, especially verbal, are s"
W19-5110,S16-1140,1,0.843654,"hich previously seen verb-noun pairs are tagged as MWEs as soon as they have the same lemmas as a seen MWE and maintain a direct dependency relation, whatever the label and direction of this dependency. This very simple method achieves F1=0.88 on French. A comparable result was observed in the 2016 DiMSUM shared task (Schneider et al., 2014), in which a rule-based baseline was ranked second. This system extracted MWEs from the training corpus and then annotated them in the test corpus based on lemma/part-of-speech matching and heuristics such as allowing a limited number of intervening words (Cordeiro et al., 2016). Secondly, there is a large gap to bridge for seen data whose surface form is not identical to the ones seen in train. Tab. 3 shows that, indeed, the difference between identical-to-train and TRAVERSAL SHOMA identical to train variants of train identical to train variants of train BG .85 .55 .89 .52 PL .92 .80 .95 .71 PT .87 .72 .93 .81 Table 3: PARSEME shared task 1.1 identification scores on identical-to-train and variant-of-train data for TRAVERSAL and SHOMA. Thirdly, significant progress can also be achieved if another important challenge is explicitly addressed: discontinuity of verbal M"
W19-5110,C96-2182,0,0.0402559,"pus of Polish. In DUELME (Grégoire, 2010), a Dutch MWE lexicon, all MWE were automatically acquired from a large raw corpus on the basis of a short list of morpho-syntactic patterns. Lexicon entries contain example sentences illustrating the use of MWEs. Finally, when MWEs were directly accommodated in implemented formal grammars, More elaborate are approaches based on finitestate-related formalisms. They usually indicate the morphological categories and features of individual MWE components, and offer rule-based combinatorial description of their variability patterns (Karttunen et al., 1992; Breidt et al., 1996; Oflazer et al., 2004; Silberztein, 2005; Krstev et al., 2010; Al-Haj et al., 2014; Lobzhanidze, 2017; Czerepowicka and Savary, 2018). They mostly cover continuous (e.g. nominal) MWEs in which morphosyntactic phenomena remain local (Savary, 2008). Therefore, additionally to the intentional format, i.e. rules describing the analysis and production of MWE instances, they often come with an extensional format, which stores the MWE instances (inflected forms) themselves. Plain-text extensional lists can be straightforwardly matched against a text. Such finite-state frameworks do not account for d"
W19-5110,J17-4005,1,0.905166,"Missing"
W19-5110,J09-1005,0,0.166615,"al is of course trivial with respect to most learning problems in NLP. But we believe that its applicability is particularly relevant in the domain of GL-MWE identification for at least four reasons. Firstly, there is a particularly acute discrepancy between the performance on seen vs. unseen data, as discussed in Sec. 3, so the potential of the gain in this respect is huge. Secondly, unsupervised discovery of (previously unseen) MWEs has a rich bibliography and proves particularly effective when type-specific idiosyncrasies are exploited (Pdiscr ), for instance, in verb-noun idiom discovery (Fazly et al., 2009). Thirdly, the low effective ambiguity of word combinations occurring in MWEs (Pambig ) implies scarcity of naturally occurring negative examples. Therefore, the Zipfian distribution (Pzipf ) can be partly balanced, with minor bias, by complementing a (small) annotated corpus with several minimal positive occurrence examples for lower-frequency MWEs discovered in very large corpora by unsupervised methods. Fourthly, the relatively low proliferation speed (Pprolif ) of GLMWEs makes them good candidates for largecoverage lexical encoding. Thus, it should be possible to produce relatively stable"
W19-5110,C86-1001,0,0.51759,"c conversion to particular grammar formalisms. On the other hand, some approaches accommodate some types of MWEs directly in the lexicons of computational grammars within particular grammatical frameworks: head-driven phrase structure grammar (Sag et al., 2002; Copestake et al., 2002; Villavicencio et al., 2004; Bond et al., 2015; Herzig Sheinfux et al., 2015), lexical functional grammar (Attia, 2006; Dyvik et al., 2019), tree-adjoining grammar (Abeillé and Schabes, 1989, 1996; Vaidya et al., 2014; Lichte and Kallmeyer, 2016), and dependency grammar (Diaconescu, 2004). of the Lexicon Grammar (Gross, 1986) and of the Explanatory Combinatorial Dictionary (Mel’ˇcuk et al., 1988; Pausé, 2018). These approaches assume that units of meaning are located at the level of elementary sentences (predicates with their arguments) rather than of words, and MWEs, especially verbal, are special instances of predicates in which some arguments are lexicalized. Those works paved the way towards systematic syntactic description of MWEs, but suffered from insufficient formalization and required substantial accommodation to be applicable to NLP (Constant and Tolone, 2010; Lareau et al., 2012). With the growing under"
W19-5110,W17-1413,0,0.0393064,"Missing"
W19-5110,C02-1117,0,0.00976095,"s to name develop rapidly, multiword terms and NEs strongly proliferate. On the other hand, general language MWEs (GL-MWEs)2 are coined by much larger commu3 A noun is predicative if it has at least one semantic argument, according to the PARSEME guidelines (http://parsemefr.lif.univ-mrs.fr/ parseme-st-guidelines/1.1). 2 The border between SL-MWEs and GL-MWEs is fuzzy, but this characterization is useful for our argumentation. 80 ested in is the Zipfian distribution of MWEs. As most language phenomena, few MWE types occur frequently in texts, and there is a long tail of MWEs occurring rarely (Ha et al., 2002; Ryland Williams et al., 2015). The success of machine learning generalization relies on dealing with rare or unseen events, based on their similarity with frequent ones. Such similarity is hard to define for the heterogeneous phenomena included under the MWE denomination. cause of the predicative nature of the nouns but also because they contain one of the few very frequent light verbs like make, take, etc. (Savary et al., 2018). Note, however, that these verbs, are also highly frequent in regular constructions, i.e. Psim is moderate but Pdiscr is still restricted to the level of types. Comp"
W19-5110,W17-1706,0,0.0688358,"Missing"
W19-5110,C92-1025,0,0.351978,"ly from the National Corpus of Polish. In DUELME (Grégoire, 2010), a Dutch MWE lexicon, all MWE were automatically acquired from a large raw corpus on the basis of a short list of morpho-syntactic patterns. Lexicon entries contain example sentences illustrating the use of MWEs. Finally, when MWEs were directly accommodated in implemented formal grammars, More elaborate are approaches based on finitestate-related formalisms. They usually indicate the morphological categories and features of individual MWE components, and offer rule-based combinatorial description of their variability patterns (Karttunen et al., 1992; Breidt et al., 1996; Oflazer et al., 2004; Silberztein, 2005; Krstev et al., 2010; Al-Haj et al., 2014; Lobzhanidze, 2017; Czerepowicka and Savary, 2018). They mostly cover continuous (e.g. nominal) MWEs in which morphosyntactic phenomena remain local (Savary, 2008). Therefore, additionally to the intentional format, i.e. rules describing the analysis and production of MWE instances, they often come with an extensional format, which stores the MWE instances (inflected forms) themselves. Plain-text extensional lists can be straightforwardly matched against a text. Such finite-state frameworks"
W19-5110,W04-0409,0,0.0411006,"Missing"
W19-5110,W18-4932,1,0.903969,".07 seen .92 .65 .58 SHOMA unseen .59 .21 .10 PL IRV LVC VID All .92 .76 .57 .85 .26 .20 .04 .17 .90 .69 .58 .82 .24 .19 .04 .18 All .76 .13 .78 .31 PT IRV LVC VID .89 .77 .69 .12 .25 .07 .86 .88 .84 .42 .35 .08 All .78 .20 .87 .31 Table 2: PARSEME shared task 1.1 identification scores on seen and unseen data for TRAVERSAL and SHOMA. Verbal MWE categories are inherently reflexive verbs (IRVs), light-verb constructions (LVCs) and verbal idioms (VIDs). variant-of-train scores ranges from 0.12 (in Polish for TRAVERSAL and Portuguese for SHOMA) to 0.37 (in Bulgarian for SHOMA). At the same time, Pasquer et al. (2018a) show that morphosyntactic variability, relatively high in verbal MWEs, can be neutralized with dedicated methods. Namely, cooccurrences of previously seen MWE components can be effectively recognized by a Naive Bayes classifier, with features leveraging type-specific idiosyncrasies (Pdiscr ). This method scored the best in the PARSEME shared task 1.1 for Bulgarian, even if it was restricted to the seen data only. specific idiosyncrasies are used. The few tokenspecific hints (if any) which may help such systems generalize over unseen data are mostly limited to the presence of particular ligh"
W19-5110,C18-1219,1,0.945923,".07 seen .92 .65 .58 SHOMA unseen .59 .21 .10 PL IRV LVC VID All .92 .76 .57 .85 .26 .20 .04 .17 .90 .69 .58 .82 .24 .19 .04 .18 All .76 .13 .78 .31 PT IRV LVC VID .89 .77 .69 .12 .25 .07 .86 .88 .84 .42 .35 .08 All .78 .20 .87 .31 Table 2: PARSEME shared task 1.1 identification scores on seen and unseen data for TRAVERSAL and SHOMA. Verbal MWE categories are inherently reflexive verbs (IRVs), light-verb constructions (LVCs) and verbal idioms (VIDs). variant-of-train scores ranges from 0.12 (in Polish for TRAVERSAL and Portuguese for SHOMA) to 0.37 (in Bulgarian for SHOMA). At the same time, Pasquer et al. (2018a) show that morphosyntactic variability, relatively high in verbal MWEs, can be neutralized with dedicated methods. Namely, cooccurrences of previously seen MWE components can be effectively recognized by a Naive Bayes classifier, with features leveraging type-specific idiosyncrasies (Pdiscr ). This method scored the best in the PARSEME shared task 1.1 for Bulgarian, even if it was restricted to the seen data only. specific idiosyncrasies are used. The few tokenspecific hints (if any) which may help such systems generalize over unseen data are mostly limited to the presence of particular ligh"
W19-5110,W17-1412,0,0.016812,"are relatively easy to predict due to the moderate strength of Psim . The Czech corpus was not annotated from scratch but converted from a previously annotated resource, and inherently reflexive verbs are probably over-represented there. The rate of inherently reflexive verbs in other Slavic languages in the PARSEME corpora range from 0.3 to 0.48. Identification of general-language MWEs Within GL-MWEs, multilingual benchmarking data are available mainly for verbal MWEs via editions 1.0 and 1.1 of the PARSEME shared tasks 6 In the first shared task on NE recognition in Balto-Slavic languages (Piskorski et al., 2017), only test data but no annotated training data were published. 82 focus on known challenges posed by MWEs. Thus, results were reported separately for continuous vs. discontinuous, multi-token vs. singletoken, seen vs. unseen, and identical-to-train vs. variant-of-train verbal MWEs.9 The most dramatic performance differences appear in the seen vs. unseen opposition. A verbal MWE from the corpus is considered seen if another verbal MWE with the same multiset of lemmas is annotated at least once in the training corpus. For instance, given the occurrence of has a new look in the training corpus,"
W19-5110,W14-5811,0,0.188193,"Missing"
W19-5110,Q14-1016,0,0.365013,"ctive ambiguity of MWEs (Pambig ) means that identifying morphosyntactically well-formed combinations of previously seen MWE components constitutes a strong baseline for MWEI. For instance, Pasquer et al. (2018b) propose a very simple baseline for verbnoun MWE identification in which previously seen verb-noun pairs are tagged as MWEs as soon as they have the same lemmas as a seen MWE and maintain a direct dependency relation, whatever the label and direction of this dependency. This very simple method achieves F1=0.88 on French. A comparable result was observed in the 2016 DiMSUM shared task (Schneider et al., 2014), in which a rule-based baseline was ranked second. This system extracted MWEs from the training corpus and then annotated them in the test corpus based on lemma/part-of-speech matching and heuristics such as allowing a limited number of intervening words (Cordeiro et al., 2016). Secondly, there is a large gap to bridge for seen data whose surface form is not identical to the ones seen in train. Tab. 3 shows that, indeed, the difference between identical-to-train and TRAVERSAL SHOMA identical to train variants of train identical to train variants of train BG .85 .55 .89 .52 PL .92 .80 .95 .71"
W19-5110,W16-1816,0,0.100552,"e following scenario for future development in MWEI. MWE lexicons in MWE identification Handcrafted MWE lexicons, as those addressed in the previous section, can significantly enhance MWEI. In sequence tagging MWEI methods, such resources can be used as sources of lexical features (Schneider et al., 2014). In parsing-based approaches they may serve as a basis for word-lattice representation of an input sentence, in which the compositional vs. MWE interpretation of a word sequence is represented jointly (Constant et al., 2013). The impact of lexical resources on MWEI is explicitly addressed by Riedl and Biemann (2016). Using a CRF-based MWEI system, they show that the addition of an automatically discovered lexicon of MWEs can benefit MWEI quality. The systems competing in PARSEME shared tasks used lexical resources to a much lesser degree. In both editions only one, rule-based, system applied a MWE lexicon, for French in edition 1.0 and for English, French, German and Greek in edition 1.1 (Nerima et al., 2017). Other systems, even those from the open track, employed only one type of external resources, namely word embeddings, but no MWE lexicons. This is probably due mainly to the fact that the competitio"
W19-5110,N19-1275,0,0.103999,"Missing"
W19-5110,R11-1015,0,0.032783,"Missing"
W19-5110,weller-heid-2010-extraction,0,0.0213712,"the lexical encoding adventure relatively feasible, with the help of fully and/or semi-automatic methods. 7 propose subtasks dedicated specifically to unseen data. New MWEI tools may leverage the typespecific idiosyncrasy of MWEs (Pdiscr ), so as to achieve better generalization over unseen data. The community should also put more effort into the development of large-coverage syntactic MWE lexicons. To this end, the MWE discovery task should be redefined so that not only bare lists of MWE candidates but also their syntactic structures for at least some morphosyntactic variants are extracted (Weller and Heid, 2010). Many existing discovery methods are dedicated to selected MWE categories, syntactic patterns and languages. New methods should, conversely, be more generic so as to cover the large variety of MWE categories and adapt to many languages. In order to incrementally achieve high quality for such resources (e.g. via manual validation), MWE discovery should not be performed from scratch, but should take as input and enrich existing MWE lexicons. MWE discovery evaluation measures should explicitly account for this enrichment aspect. Steps should also be taken towards defining MWE lexicon formats whi"
W19-5110,W02-2024,0,0.151101,"or most SL-MWE results, the F1measure additionally accounts for categorisation, i.e. a correctly identified span of words must also be assigned the correct NE category. 3.1 Identification of sublanguage MWEs For SL-MWEs, identification methods have been developed for decades, but most often fuse multiword objects with single-word ones. Two typical examples are NE recognition and term identification. In these two domains, state-of-the-art results have been encouraging or good already in early systems and evaluation campaigns. In the CoNLL 2002 and 2003 shared tasks on NE recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), dedicated mainly to person, organization and location names, the top-3 systems obtained F1-measures of 0.71, 0.74, 0.77, and 0.86, with datasets of 20,000, 13,000, 18,000 and 35,000 annotated NEs, for German, Dutch, Spanish and English, respectively. All of these systems used machine learning techniques such as hidden Markov models, decision trees, MaxEnt classifiers, conditional random fields, support-vector machines, recurrent neural networks, with features that often included external entity list lookup.5 Yadav and Bethard (2018) provide more recent s"
W19-5110,C18-1182,0,0.0132898,"tasks on NE recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), dedicated mainly to person, organization and location names, the top-3 systems obtained F1-measures of 0.71, 0.74, 0.77, and 0.86, with datasets of 20,000, 13,000, 18,000 and 35,000 annotated NEs, for German, Dutch, Spanish and English, respectively. All of these systems used machine learning techniques such as hidden Markov models, decision trees, MaxEnt classifiers, conditional random fields, support-vector machines, recurrent neural networks, with features that often included external entity list lookup.5 Yadav and Bethard (2018) provide more recent state-of-the-art results for NE 4 The same metric is called MWE-based, as opposed to token-based, in the PARSEME shared task campaigns. 5 Results of the same systems without external entity list lookup are not provided. Finally, the fifth property (Pzipf ) we are inter81 recognition based on neural networks on the same datasets. There, the best results mostly exceed 0.78 for German, 0.85 for Dutch and Spanish, and 0.9 for English, even without external dictionary lookup. In Slavic languages, where NE recognition is substantially hardened by the rich declension of nouns and"
W19-5110,H93-1052,0,0.085616,"mber of idiomatic and literal occurrences, ranges from 0.96 to 0.98. This means that, whenever the morphosyntactic conditions for an idiomatic reading are fulfilled, this reading occurs almost always. A similarly high idiomaticity rate (0.95) was also observed for Polish on other, non-verbal categories of MWEs: nominal, adjectival, and adverbial GL-MWEs, as well as multiword NEs (Waszczuk et al., 2016). This property might be related to the fact that ambiguity is reduced with the addition of words to the context, a hypothesis that has been employed in word-sense disambiguation for many years (Yarowsky, 1993). (1) We often took pains not to harm them. ‘We often tried hard not to harm them.’ (2) I could not :::: take the :::: pain any longer. 3 State of the art in MWE identification In this section we offer a comparative analysis of state-of-the-art results with respect to two axes: SL-MWEs vs. GL-MWEs and seen vs. unseen data. All results are indicated in terms of the F1measure, with the exact-match metric. In other words, a prediction for a text fragment is considered correct only when the identified unit corresponds to exactly the same words as in the gold standard.4 For most SL-MWE results, the"
W19-5110,W14-5816,0,0.0132085,"ffer from insufficient formalization (Lichte et al., 2019), they could be successfully applied to parsing after ad hoc conversion to particular grammar formalisms. On the other hand, some approaches accommodate some types of MWEs directly in the lexicons of computational grammars within particular grammatical frameworks: head-driven phrase structure grammar (Sag et al., 2002; Copestake et al., 2002; Villavicencio et al., 2004; Bond et al., 2015; Herzig Sheinfux et al., 2015), lexical functional grammar (Attia, 2006; Dyvik et al., 2019), tree-adjoining grammar (Abeillé and Schabes, 1989, 1996; Vaidya et al., 2014; Lichte and Kallmeyer, 2016), and dependency grammar (Diaconescu, 2004). of the Lexicon Grammar (Gross, 1986) and of the Explanatory Combinatorial Dictionary (Mel’ˇcuk et al., 1988; Pausé, 2018). These approaches assume that units of meaning are located at the level of elementary sentences (predicates with their arguments) rather than of words, and MWEs, especially verbal, are special instances of predicates in which some arguments are lexicalized. Those works paved the way towards systematic syntactic description of MWEs, but suffered from insufficient formalization and required substantial"
W19-5110,W04-0411,0,0.0636027,"et al., 2015), i.e. they implicitly assume the existence of regular grammar rules, and explicitly describe only those MWE properties which do not conform to these rules. Although these lexicons suffer from insufficient formalization (Lichte et al., 2019), they could be successfully applied to parsing after ad hoc conversion to particular grammar formalisms. On the other hand, some approaches accommodate some types of MWEs directly in the lexicons of computational grammars within particular grammatical frameworks: head-driven phrase structure grammar (Sag et al., 2002; Copestake et al., 2002; Villavicencio et al., 2004; Bond et al., 2015; Herzig Sheinfux et al., 2015), lexical functional grammar (Attia, 2006; Dyvik et al., 2019), tree-adjoining grammar (Abeillé and Schabes, 1989, 1996; Vaidya et al., 2014; Lichte and Kallmeyer, 2016), and dependency grammar (Diaconescu, 2004). of the Lexicon Grammar (Gross, 1986) and of the Explanatory Combinatorial Dictionary (Mel’ˇcuk et al., 1988; Pausé, 2018). These approaches assume that units of meaning are located at the level of elementary sentences (predicates with their arguments) rather than of words, and MWEs, especially verbal, are special instances of predicat"
W19-5110,R11-1040,0,0.07028,"Missing"
W19-5110,W18-4931,0,0.212322,"Missing"
W19-5110,C16-1042,1,0.828415,"sed literally, they are rarely used so in corpora. Namely, in 5 languages from different language genera, the idiomaticity rate of verbal MWEs, i.e. the proportion of idiomatic occurrences with respect to the total number of idiomatic and literal occurrences, ranges from 0.96 to 0.98. This means that, whenever the morphosyntactic conditions for an idiomatic reading are fulfilled, this reading occurs almost always. A similarly high idiomaticity rate (0.95) was also observed for Polish on other, non-verbal categories of MWEs: nominal, adjectival, and adverbial GL-MWEs, as well as multiword NEs (Waszczuk et al., 2016). This property might be related to the fact that ambiguity is reduced with the addition of words to the context, a hypothesis that has been employed in word-sense disambiguation for many years (Yarowsky, 1993). (1) We often took pains not to harm them. ‘We often tried hard not to harm them.’ (2) I could not :::: take the :::: pain any longer. 3 State of the art in MWE identification In this section we offer a comparative analysis of state-of-the-art results with respect to two axes: SL-MWEs vs. GL-MWEs and seen vs. unseen data. All results are indicated in terms of the F1measure, with the exa"
W19-5121,S17-1006,0,0.196115,"Missing"
W19-5121,J13-1009,0,0.062952,"Missing"
W19-5121,W18-4920,0,0.033138,"Missing"
W19-5121,C18-1139,0,0.0277841,"ation, we used the gensim library7 to train 256-dimensional vectors for both forms and lemmas on the training corpus of the shared task for 10 epochs. Furthermore, all embeddings use the CBOW algorithm with the same hyper-parameter values of 5 for the window size (left/right context of words) and 1 for min-count (minimum number of occurrences of words). For FastText, we set the size of character n-grams to 1 to combine the whole word’s embedding with the embeddings of its characters. We did not use contextual representations, like BERT, Elmo or Flair (Devlin et al., 2018; Peters et al., 2018; Akbik et al., 2018), because they have to be pre-trained on large corpora and we wanted to have an experimental setup compatible with the closed track of the PARSEME shared task. tal setup implies that it is difficult for a system to predict a VMWE without a reliable representation for a verb, learned from the training data. MWE Identification System We use our inhouse MWE identification system Veyn (Zampieri et al., 2018), based on sequence tagging using recurrent neural networks.5 The system takes as input the concatenation of the embeddings of the words’ features (e.g. lemmas and POS). It uses a CRF output la"
W19-5121,W17-1707,0,0.122077,"Missing"
W19-5121,Q17-1010,0,0.27521,"gical and syntactic variability (Savary et al., 2018). Our goal is to study the impact of word representations on verbal MWE (VMWE) identification, comparing lemmas, surface forms, traditional word embeddings and subword representations. We compare the performance of an off-the-shelf MWE identification system based on neural sequence tagging (Zampieri et al., 2018) using lemmas and surface forms as input features, encoded in the form of classical pre-initialised word2vec embeddings (Mikolov et al., 2013) or, alternatively, using new-generation FastText embeddings built from character n-grams (Bojanowski et al., 2017). Our main hypothesis is that the latter can model morphological variability, representing an alternative for lemmatisation. We carry out experiments in 3 languages with varying morphological complexity: French, Polish and Basque. Recent initiatives such as the PARSEME shared task have allowed the rapid development of MWE identification systems. Many of those are based on recent NLP advances, using neural sequence models that take continuous word representations as input. We study two related questions in neural verbal MWE identification: (a) the use of lemmas and/or surface forms as input fea"
W19-5121,P16-1101,0,0.0908242,"Missing"
W19-5121,J17-4005,1,0.906443,"Missing"
W19-5121,N18-1202,0,0.0419821,"). For each representation, we used the gensim library7 to train 256-dimensional vectors for both forms and lemmas on the training corpus of the shared task for 10 epochs. Furthermore, all embeddings use the CBOW algorithm with the same hyper-parameter values of 5 for the window size (left/right context of words) and 1 for min-count (minimum number of occurrences of words). For FastText, we set the size of character n-grams to 1 to combine the whole word’s embedding with the embeddings of its characters. We did not use contextual representations, like BERT, Elmo or Flair (Devlin et al., 2018; Peters et al., 2018; Akbik et al., 2018), because they have to be pre-trained on large corpora and we wanted to have an experimental setup compatible with the closed track of the PARSEME shared task. tal setup implies that it is difficult for a system to predict a VMWE without a reliable representation for a verb, learned from the training data. MWE Identification System We use our inhouse MWE identification system Veyn (Zampieri et al., 2018), based on sequence tagging using recurrent neural networks.5 The system takes as input the concatenation of the embeddings of the words’ features (e.g. lemmas and POS). It"
W19-5121,Q14-1016,0,0.121795,"ons and WordNet (MWE-WN 2019), pages 169–175 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 popular models for MWE identification (Constant et al., 2017). Parsing-based methods take the (recursive) structure of language into account, trying to identify MWEs as a by-product of parsing (Green et al., 2013; Constant et al., 2013), or jointly (Constant and Nivre, 2016). Sequence tagging models, on the other hand, consider only linear context, using models such as CRFs (Vincze et al., 2011; Shigeto et al., 2013; Riedl and Biemann, 2016) and averaged perceptron (Schneider et al., 2014) combined with some variant of begin-inside-outside (BIO) encoding (Ramshaw and Marcus, 1995). Experimental Setup Corpora The PARSEME shared task 1.1 released freely available VMWE-annotated corpora in 20 languages.1 Each language’s corpus is split into training, development and test parts. To choose our target languages, we analysed the PARSEME corpora, choosing 3 languages with varying morphological richness: Basque (EU), French (FR) and Polish (PL), shown in Table 1.2 The FR training corpus has more than 420K tokens, whereas the PL and EU training corpora have around 220K and 117K tokens. E"
W19-5121,S16-1084,0,0.0197434,"many years, MWE identification was considered unrealistic, with most MWE research focusing on out-of-context MWE discovery (Ramisch et al., 2013). Indeed, the availability of MWE-annotated corpora was limited to some treebanks with partial annotations, often a by-product of syntax trees (Green et al., 2013; Constant et al., 2013). This prevented the widespread development and evaluation of MWE identification systems, as compared to other tasks such as POS tagging and named entity recognition. This landscape has drastically changed in the last few years, thanks to shared tasks such as DiMSUM (Schneider et al., 2016) and PARSEME 1.0 and 1.1 (Savary et al., 2017; Ramisch et al., 2018) and to the release of open corpora annotated for MWEs in ∼20 languages. These initiatives provide a unified framework for MWE identifi2 Related Work Rule-based matching, supervised classification, sequence tagging, and parsing are among the most 169 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 169–175 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 popular models for MWE identification (Constant et al., 2017). Parsing-based methods take the (re"
W19-5121,W95-0107,0,0.136505,"ation for Computational Linguistics 3 popular models for MWE identification (Constant et al., 2017). Parsing-based methods take the (recursive) structure of language into account, trying to identify MWEs as a by-product of parsing (Green et al., 2013; Constant et al., 2013), or jointly (Constant and Nivre, 2016). Sequence tagging models, on the other hand, consider only linear context, using models such as CRFs (Vincze et al., 2011; Shigeto et al., 2013; Riedl and Biemann, 2016) and averaged perceptron (Schneider et al., 2014) combined with some variant of begin-inside-outside (BIO) encoding (Ramshaw and Marcus, 1995). Experimental Setup Corpora The PARSEME shared task 1.1 released freely available VMWE-annotated corpora in 20 languages.1 Each language’s corpus is split into training, development and test parts. To choose our target languages, we analysed the PARSEME corpora, choosing 3 languages with varying morphological richness: Basque (EU), French (FR) and Polish (PL), shown in Table 1.2 The FR training corpus has more than 420K tokens, whereas the PL and EU training corpora have around 220K and 117K tokens. EU contains less annotated VMWE occurrences than both FR and PL. The average length of annotat"
W19-5121,W13-1021,0,0.0269041,"ong the most 169 Proceedings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 169–175 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 popular models for MWE identification (Constant et al., 2017). Parsing-based methods take the (recursive) structure of language into account, trying to identify MWEs as a by-product of parsing (Green et al., 2013; Constant et al., 2013), or jointly (Constant and Nivre, 2016). Sequence tagging models, on the other hand, consider only linear context, using models such as CRFs (Vincze et al., 2011; Shigeto et al., 2013; Riedl and Biemann, 2016) and averaged perceptron (Schneider et al., 2014) combined with some variant of begin-inside-outside (BIO) encoding (Ramshaw and Marcus, 1995). Experimental Setup Corpora The PARSEME shared task 1.1 released freely available VMWE-annotated corpora in 20 languages.1 Each language’s corpus is split into training, development and test parts. To choose our target languages, we analysed the PARSEME corpora, choosing 3 languages with varying morphological richness: Basque (EU), French (FR) and Polish (PL), shown in Table 1.2 The FR training corpus has more than 420K tokens,"
W19-5121,W16-1816,0,0.10689,"edings of the Joint Workshop on Multiword Expressions and WordNet (MWE-WN 2019), pages 169–175 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics 3 popular models for MWE identification (Constant et al., 2017). Parsing-based methods take the (recursive) structure of language into account, trying to identify MWEs as a by-product of parsing (Green et al., 2013; Constant et al., 2013), or jointly (Constant and Nivre, 2016). Sequence tagging models, on the other hand, consider only linear context, using models such as CRFs (Vincze et al., 2011; Shigeto et al., 2013; Riedl and Biemann, 2016) and averaged perceptron (Schneider et al., 2014) combined with some variant of begin-inside-outside (BIO) encoding (Ramshaw and Marcus, 1995). Experimental Setup Corpora The PARSEME shared task 1.1 released freely available VMWE-annotated corpora in 20 languages.1 Each language’s corpus is split into training, development and test parts. To choose our target languages, we analysed the PARSEME corpora, choosing 3 languages with varying morphological richness: Basque (EU), French (FR) and Polish (PL), shown in Table 1.2 The FR training corpus has more than 420K tokens, whereas the PL and EU tra"
W19-5121,N19-1275,0,0.376069,"Missing"
W19-5121,W18-4933,1,0.891813,"words than a system based on lemmas, especially for morphologically-rich languages in which a single lemma may correspond to dozens of surface forms (Seddah et al., 2013). This problem is particularly relevant for verbal MWEs, which present high morphological and syntactic variability (Savary et al., 2018). Our goal is to study the impact of word representations on verbal MWE (VMWE) identification, comparing lemmas, surface forms, traditional word embeddings and subword representations. We compare the performance of an off-the-shelf MWE identification system based on neural sequence tagging (Zampieri et al., 2018) using lemmas and surface forms as input features, encoded in the form of classical pre-initialised word2vec embeddings (Mikolov et al., 2013) or, alternatively, using new-generation FastText embeddings built from character n-grams (Bojanowski et al., 2017). Our main hypothesis is that the latter can model morphological variability, representing an alternative for lemmatisation. We carry out experiments in 3 languages with varying morphological complexity: French, Polish and Basque. Recent initiatives such as the PARSEME shared task have allowed the rapid development of MWE identification syst"
