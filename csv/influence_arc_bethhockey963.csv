2004.tmi-1.3,C02-1095,0,0.0922259,"ss. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-based language model (GLM), compiled using the standar"
2004.tmi-1.3,P01-1022,1,0.849306,"ce&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-based language model ("
2004.tmi-1.3,2000.iwpt-1.15,0,0.00995447,"deling tools, like Nuance&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et al. 2001), in which one of the present authors participated. In this study, two speech understanding systems were constructed for the same domain, a medium-vocabulary command and control task. Both systems ran on the Nuance 7 platform. The first had a hand-coded grammar-b"
2004.tmi-1.3,P03-2024,1,0.63371,"slated by the system, and the patient responds nonverbally, for example by nodding or shaking their head, or pointing. The key requirement of the project is a high level of accuracy: doctors are only interested in using systems they can trust. Since speech recognition can never be wholly reliable, the user interface is structured so that the initial recognition hypothesis is echoed back to the user, who has the option to abort further processing if necessary. Reliability thus means reliability on the utterances which the user considers to be correctly recognized. The current prototype system (Rayner et al. 2003) translates questions in a headache domain from English into Japanese or French, using a vocabulary of about 200 words; a production version would need to cover at least another 25 to 50 similar subdomains. The recognition component for the SLM and GLM version was built with a training corpus of 450 utterances. The initial set of training utterances were supplied to us by a physician, who then interacted with us to expand it by adding enough synonyms and alternative phrasings to make the coverage reasonably habitable. Later versions may use larger development sets, but it is unreasonable to as"
2004.tmi-1.3,E03-2010,1,0.743526,"slated by the system, and the patient responds nonverbally, for example by nodding or shaking their head, or pointing. The key requirement of the project is a high level of accuracy: doctors are only interested in using systems they can trust. Since speech recognition can never be wholly reliable, the user interface is structured so that the initial recognition hypothesis is echoed back to the user, who has the option to abort further processing if necessary. Reliability thus means reliability on the utterances which the user considers to be correctly recognized. The current prototype system (Rayner et al. 2003) translates questions in a headache domain from English into Japanese or French, using a vocabulary of about 200 words; a production version would need to cover at least another 25 to 50 similar subdomains. The recognition component for the SLM and GLM version was built with a training corpus of 450 utterances. The initial set of training utterances were supplied to us by a physician, who then interacted with us to expand it by adding enough synonyms and alternative phrasings to make the coverage reasonably habitable. Later versions may use larger development sets, but it is unreasonable to as"
2004.tmi-1.3,W00-0311,1,0.879114,"Missing"
2004.tmi-1.3,P99-1024,0,0.0383449,"language interface for a new application in a timely fashion, rule-based methods are often the only practicable alternative. Over the last few years, however, the above picture has become more blurred. Commercial platform vendors have begun to introduce statistical modeling tools, like Nuance&apos;s SayAnythingTM module, with some success. In the other direction, academics have become more interested in grammar-based methods. The literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al. 1999, Rayner et. al. 2000, Lemon et al. 2001, Rayner et. al. 2001a). Much of this work has involved the idea of compiling grammar-based language models out of descriptions written in higher-level formalisms, in particular unification grammars (Moore 1998, Kiefer & Krieger 2000, Dowding et al. 2001, Rayner et al. 2001b, Bos 2002). Given that a great deal of practical and theoretical work is going on using both statistical and rule-based methods, it is remarkable that there is almost no reported work attempting a systematic comparison of the two approaches. The only example known to us is (Knight et"
2005.eamt-1.8,A97-1001,0,0.0737007,"Missing"
2005.eamt-1.8,W02-0710,1,0.794173,"Missing"
2005.eamt-1.8,P03-2024,1,0.844535,"Missing"
2005.eamt-1.8,2005.eamt-1.8,1,0.106122,"Missing"
2005.eamt-1.8,2004.tmi-1.3,1,\N,Missing
2005.mtsummit-papers.25,E03-2010,1,0.865288,"Missing"
2005.mtsummit-papers.25,2005.eamt-1.8,1,0.80735,"ality of pain, and the factors that increase or decrease the pain. The answers to these questions can be successfully communicated by a limited number of one or two word responses (e.g. yes/no, left/right, numbers) or even gestures (e.g. nodding or shaking the head, pointing to an area of the body). Translation can thus be unidirectional. In order to obtain an accurate translation, the system uses a grammar-based speech recogniser. For this type of application a grammar-based approach appears to give better results than a statistical-based recognition (Knight et al., 2001, Rayner et al. 2004, Bouillon et al. 2005). Diagnosis seems to be a very convergent sublanguage, where it is possible to guess the syntactic structures that a doctor will use, and thus to describe them in a grammar. The advantage of this approach is that the grammar enforces more global constraints on the recognized utterance than the simple bigrams or trigrams of a statistical language model: more complete sentences are thus well recognized, which improves the translation. The drawback is the lack of robustness: if the sentence structure is not in the grammar or if a word is not in the lexicon, the recognition completely fails. Helpi"
2005.mtsummit-papers.25,lavie-etal-2002-nespole,0,0.0623047,"Missing"
2005.mtsummit-papers.25,W02-0710,1,\N,Missing
2005.mtsummit-papers.25,2004.tmi-1.3,1,\N,Missing
A00-1016,P93-1008,0,0.0218054,"ature, pressure and carbon dioxide levels, and the status of t h e Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representations. The aspects of the system which are of primary interest here concern the dialogue manager"
A00-1016,A97-1001,0,0.115693,"The basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995). In most of this and other related work the treatment is some variant of the"
A00-1016,A97-1008,0,0.0184187,"is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this task in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-antonomous robots include SRrs Flakey robot (Konolige et al., 1993) and NCARArs InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and TRAINS-96 (Traum and Allen, 1994; Tranm and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pyre et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech int"
A00-1016,P99-1024,0,0.0144947,"nmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of t h e Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SPd Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out using the SRI Gemini system (Dowding et al., 1993), using a domain~independent unification grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grarnm~kr using the methods of (Moore et al., 1997); the net result is that only grammatically wellformed utterances can be recognized. O"
A00-1016,P94-1001,0,0.0282514,"Missing"
A00-1016,H93-1008,0,\N,Missing
bouillon-etal-2008-developing,H05-2014,1,\N,Missing
bouillon-etal-2008-developing,2005.mtsummit-papers.25,1,\N,Missing
bouillon-etal-2008-developing,2005.eamt-1.8,1,\N,Missing
bouillon-etal-2008-developing,2005.jeptalnrecital-long.17,1,\N,Missing
bouillon-etal-2008-developing,W07-0806,1,\N,Missing
bouillon-etal-2008-developing,W06-3702,1,\N,Missing
C00-2097,A97-1001,0,0.113553,"ation becomes increasingly burdensome. The grammar tends to become large and unwieldy, with many rules appearing in multiple versions that constantly need to be kept in step with each other. It represents a large development cost, is hard to maintain, and does not usually port well to new applications. It is tempting to consider the option of moving towards a more expressive grammar formalism, like uni cation grammar, writing the original grammar in uni cation grammar form and compiling it down to the context-free notation required by the underlying toolkit. At least one such system (Gemini; (Moore et al 1997)) has been implemented and used to build successful and non-trivial applications, most notably CommandTalk (Stent et al 1999). Gemini accepts a slightly constrained version of the uni cation grammar formalism originally used in the Core Language Engine (Alshawi 1992), and compiles it into context-free grammars in the GSL formalism supported by the Nuance Toolkit. The Nuance Toolkit compiles GSL grammars into sets of probabilistic nite state graphs (PFSGs), which form the nal language model. The relative success of the Gemini system suggests a new question. Uni cation grammars have been used ma"
C00-2097,P99-1024,1,\N,Missing
C08-1090,2005.eamt-1.8,1,0.935429,"Missing"
C08-1090,W06-3702,1,0.850481,"model, which produces a source-langage semantic representation. This is first translated by one set of rules into an interlingual form, and then by a second set into a target language representation. A target-language grammar, compiled into generation form, turns this into one or more possible surface strings, after which a set of generation preferences picks one out. Finally, the selected string is realised in spoken form. There is also some use of corpus-based statistical methods, both to tune the language model (Rayner et al., 2006, Section 11.5) and to drive a robust embedded help system (Chatzichrisafis et al., 2006). The treatment of syntactic structure is a carefully thought-out compromise between linguistic and engineering traditions. All grammars used are extracted from general linguistically motivated resource grammars, using corpus-based methods driven by small sets of examples (Rayner et al., 2006, Chapter 9). This results in a simpler and flatter grammar specific to the domain, whose structure is similar to the ad hoc phrasal grammars typical of engineering approaches. The treatment of semantics is however less sophisticated, and basically represents a minimal approach in the engineering tradition"
C08-1090,P02-1035,0,0.0176156,"l and Frege and views predicate calculus as the paradigm representation language. On this view of things, a c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Approaches based in the linguistic tradition were dominant about 10 to 15 years ago, when they were used in major systems like Germany’s Verbmobil (Wahlster, 2000) and SRI’s Spoken Language Translator (Rayner et al., 2000). They are still reasonably popular today, as exemplified by major systems like PARC’s XLE (Riezler et al., 2002). The competing heritage has its roots in engineering approaches to spoken language systems, which historically have been intimately connected with Machine Learning. On this view of things, a typical semantic representation is a flat list of feature-value pairs, with the features representing semantic concepts: here, “I want a pepperoni pizza” would be represented as something like [utterance_type=request, food=pizza, type=pepperoni] It is interesting to see how little contact there has been between these two traditions. Writers on formal semantics usually treat ad hoc featurevalue representat"
C08-1090,J90-1004,0,0.321346,"Missing"
C94-2149,A88-1019,0,0.183587,"Missing"
C94-2149,C94-1024,1,0.737617,"Missing"
C94-2149,C92-3145,1,0.876586,"Missing"
C94-2149,J93-2004,0,0.0613621,"Missing"
C94-2149,H93-1025,0,0.0520506,"Missing"
C94-2149,C88-2121,0,0.120276,"Missing"
C94-2149,C90-3029,0,\N,Missing
C94-2149,H94-1052,0,\N,Missing
E03-1075,P93-1008,1,0.823149,"a good testbed for Targeted Help. 2.2 The Targeted Help Module 2 System Description 2.1 The WITAS Dialogue System Targeted Help was deployed and tested as part of the WITAS dialogue system l , a command and control and mixed-initiative dialogue system for interacting with a simulated robotic helicopter or UAV (Unmanned Aerial Vehicle) (Lemon et al., 2001). The dialogue system is implemented as a suite of agents communicating though the SRI Open Agent Architecture (OAA) (Martin et al., 1998). The agents include: Nuance Communications Recognizer (Nuance, 2002); the Gemini parser and generator (Dowding et al., 1993) (both 'See http://www.ida.liu.se/ext/witas and http://www-csli.stanford.edu/semlab/ witas 148 The Targeted Help Module is a separate component that can be added to an existing dialogue system with minimal changes to accomodate the specifics of the domain. This modular design makes it quite portable, and a version of this agent is in fact being used in a second command and control dialogue system (Hockey et al., 2002a; Hockey et al., 2002b). It is argued in (Lemon and Cavedon, 2003) that &quot;low-level&quot; processing components such as the Targeted Help module are an important focus for future dialog"
E03-1075,W02-0216,1,0.780999,"antage of the strengths of both types of language models by using the grammar based model for in-coverage utterances and the SLM as part of the Targeted Help system for outof-coverage utterances. In this paper we report on controlled experiments, testing the effectiveness of an implementation of Targeted Help in a mixed initiative dialogue system to control a simulated robotic helicopter. using a grammar designed for the UAV application); Festival text-to-speech synthesizer (Systems, 2001); a GUI which displays a map of the area of operation and shows the UAV's location; the Dialogue Manager (Lemon et al., 2002); the Robot Control and Report component, which translates commands and queries bi-directionally between the dialogue interface and the UAV. The Dialogue Manager interleaves multiple planning and execution dialogue threads (Lemon et al., 2002). While the helicopter is airborne, an on-board active vision system will interpret the scene below to interpret ongoing events, which may be reported (via NL generation) to the operator. The robot can carry out various activities such as flying to a location, fighting fires, following a vehicle, and landing. Interaction in WITAS thus involves joint-activ"
E03-1078,H94-1010,0,0.0626114,"Missing"
E03-1078,W00-0311,1,0.861623,"change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smoothly from an initial"
E03-1078,J80-3005,0,0.452417,"Missing"
E03-1078,P99-1024,0,0.0215818,"ed it is not easy to change the coverage of the system. For these reasons, commercial speech recognition platform vendors like Nuance and SpeechWorks have focussed on rule-based approaches, which allow rapid prototyping of systems from only very modest quantities of corpus data. Although most commercial rule-based spoken language dialogue systems use directed dialogue strategies and moderately simple recognition grammars, the literature now contains descriptions of several research systems built using rule-based methods, which successfully use mixed-initiative strategies and complex grammars (Stent et al., 1999; Rayner et al., 2000; Lemon et al., 2001; Rayner et al., 2001b). If a project of this kind is developed over a substantial period of time, corpus material accumulates automatically as input to the system is logged. The more corpus material there is, the stronger the reasons for moving towards datadriven processing; this will however only be easy if the architecture is originally set up to use statistics as well as rules. Summarising the argument so far, we would like an architecture which combines rule-based and data-driven methods as transparently as possible. This will allow us to shift smo"
E03-2010,P93-1008,1,0.739557,"The core functionality provided by the REGULUS environment is compilation of typed unification grammars into annotated context-free grammar language models expressed in Nuance Grammar Specification Language (GSL) notation (Nuance, 2002). GSL language models can be converted into runnable speech recognisers by invoking the Nuance Toolkit compiler utility, so the net result is the ability to compile a unification grammar into a speech recogniser. The REGULUS unification grammar formalism is closely related to the one used in the SRI Core Language Engine (CLE) and Gemini systems (Alshawi, 1992; Dowding et al., 1993), and it is most reasonable to compare it with Gemini, which offers a broadly similar range of functionalities. One important difference relates to the treatment of semantics. CLE and Gemini support a general unification-based semantics; REGULUS, however, does not permit the use of unification when constructing semantic representations. Although this involves a slight loss of expressive power, it offers the very significant advantage of allowing the 223 semantics of the original grammar to be compiled into semantic annotations on the target GSL rules. The resulting recogniser can thus be used"
E03-2010,A97-1001,0,0.0394991,"which performs a suitable factoring of the grammar. The current version of REGULUS further refines the naive method by iteratively alternating the expansion and filtering stages, nondeterministically expanding each feature in turn and then filtering the result before proceeding to the next feature. On large grammars, this ""iterative expansion"" technique can reduce time and space requirements of the compilation algorithm by several orders of magnitude. Use of iterative expansion has allowed REGULUS successfully to compile several grammars which exceeded resource bounds for the Gemini compiler (Moore et al., 1997; Moore, 1998). ing corpus-based techniques which extract a specialised version of the original general grammar. REGULUS implements a version of the grammar specialisation scheme which extends the Explanation Based Learning method described in (Rayner et al., 2002). There is a general unification grammar, loosely based on the Core Language Engine grammar for English (Pulman, 1992), which has been developed over the course of about ten individual projects. The cun ent version of the grammar contains 145 unification grammar rules, 465 function word entries, and 72 features. The grammar for each"
E03-2010,2000.iwpt-1.18,0,0.024039,"tween recognition performance and either vocabulary size or size of the training corpus, with performance depending rather more heavily on average utterance length and the types of constructions covered by the specialised grammar. We are actively investigating these issues at the moment. 3 The Development Environment All the functionalities in the REGULUS environment are available via a command-line interface, and also from within a top-loop designed primarily for interactive grammar debugging. In this mode, the grammar is compiled into an efficient left-corner parser, using the algorithm of (Moore, 2000), and also into a Definite Clause Grammar (DCG) form; the advantage of the DCG representation is that it can often be used to help diagnose grammar bugs by attempting to parse non-top constituents. Each separate domain-specific grammar is defined though a config file, which also specifies settings for the various user-defined parameters relevant to the compilation process. 4 Structure of the demo We will demo two recognisers built using REGULUS, one for a command and control application and one for a medical speech translation application. The command and control recogniser is an extended vers"
E03-2010,H93-1008,1,\N,Missing
H05-2014,2005.eamt-1.8,1,0.774145,"me recognizer using Nuance tools. Previously, the R EGULUS grammar specialization programme has only been implemented for English. In this demo, we will show how we can apply the same methodology to Japanese. Japanese is structurally a very different language from English, so it is by no means obvious that methods which work for English will be applicable in this new context: in fact, they appear to work very well. We will demo the grammars and resulting recognizers in the context of Japanese → English and Japanese → French versions of the Open Source MedSLT medical speech translation system (Bouillon et al., 2005; MedSLT, 2005). The generic problem to be solved when building any sort of recognition grammar is that syntax alone is insufficiently constraining; many of the real constraints in a given domain and use situation tend to be semantic and pragmatic in nature. The challenge is thus to include enough non-syntactic constraints in the grammar to create a language model that can support reliable domain-specific speech recognition: we sketch our solution for Japanese. The basic structure of our current general Japanese grammar is as follows. There are four main groups of rules, covering NP, PP, VP an"
H05-2014,H01-1007,0,0.0292655,"ram language model requires substantial quantities of corpus data, which is generally not available at the start of a new project. Head-to-head comparisons of class N-gram/robust and grammar-based systems also suggest that users who are familiar with system coverage get better results from grammar-based architectures (Knight et al., 2001). As a consequence, deployed spoken dialogue systems for real-world applications frequently use grammar-based methods. This is particularly the case for speech translation systems. Although leading research systems like Verbmobil and NESPOLE! (Wahlster, 2000; Lavie et al., 2001) usually employ complex architectures combining statistical and rule-based methods, successful practical examples like Phraselator and S-MINDS (Phraselator, 2005; Sehda, 2005) are typically phrasal translators with grammar-based recognizers. Voice recognition platforms like the Nuance Toolkit provide CFG-based languages for writing grammar-based language models (GLMs), but it is challenging to develop and maintain grammars consisting of large sets of ad hoc phrase-structure rules. For this reason, there has been considerable interest in developing systems that permit language models be specifi"
H05-2014,E03-2010,1,0.69704,"lop and maintain grammars consisting of large sets of ad hoc phrase-structure rules. For this reason, there has been considerable interest in developing systems that permit language models be specified in higher-level formalisms, normally some kind of unification grammar (UG), and then compile these grammars down to the low-level platform formalisms. A prominent early example of this approach is the Gemini system (Moore, 1998). Gemini raises the level of abstraction significantly, but still assumes that the grammars will be domain-dependent. In the Open Source R EGULUS project (Regulus, 2005; Rayner et al., 2003), we have taken a further step in the direction of increased abstraction, and derive all recognizers from a single linguistically motivated UG. This derivation procedure starts with a large, application-independent UG for a language. An application-specific UG is then derived using an Explanation Based Learning (EBL) specialization technique. This corpus-based specialization process is parameterized by the training corpus and operationality criteria. The training corpus, which can be relatively small, consists of examples of utterances that should be recognized by the target application. The s"
N01-1030,H92-1021,0,\N,Missing
N01-1030,H91-1043,0,\N,Missing
N01-1030,P93-1008,1,\N,Missing
N01-1030,H93-1008,1,\N,Missing
N01-1030,C00-2097,1,\N,Missing
N01-1030,P99-1024,1,\N,Missing
N01-1030,A97-1001,0,\N,Missing
P01-1022,W89-0229,0,0.049144,"u 1999). While theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. This expressiveness allows us to"
P01-1022,H94-1010,0,0.0146366,"ble. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address this problem is to write the grammar in a more expressive formalism and generate an approximation of this grammar in the format needed by the recognizer. This approach has been used in several systems, CommandTalk (Moore et al. 1997), RIALI"
P01-1022,P98-1101,0,0.081262,"e theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. This expressiveness allows us to write grammars"
P01-1022,2000.iwpt-1.15,0,0.0970413,"L Conversion GSL Grammar `) nuance_compiler Recognition System Package Figure 1: Compilation Process The GSL representation is then compiled into a Nuance package with the nuance compiler. This package is the input to the speech recognizer. In our experience, each of the compilation stages, as well as speech recognition itself, has the potential to lead to a combinatorial explosion that exceeds practical memory or time bounds. We will now describe implementations of the first stage, generating a context-free grammar from a typed unification grammar, by two different algorithms, one defined by Kiefer and Krieger (2000) and one by Moore and Gawron, described in Moore (1998) The critical difficulty for both of these approaches is how to select the set of derived nonterminals that will appear in the final CFG. 3.1 Kiefer&Krieger’s Algorithm The algorithm of Kiefer&Krieger (K&K) divides this compilation step into two phases: first, the set of context-free nonterminals is determined by iterating a bottom-up search until a least fixedpoint is reached; second, this least fixed-point is used to instantiate the set of context-free produc,/.10@2A45.1076B8:9;8=<DC  GFIHKJ@ local FIHKJ ! F   6 for each r 4UW"
P01-1022,A97-1001,0,0.0146976,"2001, W3C 2001). The difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases."
P01-1022,A00-2033,0,0.188027,"cognition performance. In the earlier implementations, cycles were fortuitously avoided, probably due to the fact that there were more unique nonterminals overall. We expect that these transformations may be effective for some grammars, but not others. We plan to continue to explore refinements to these techiques to prevent them from applying in cases where cycles or leftrecursion may be introduced. 5 Left Recursion Elimination We have used two left-recursion elimination techniques, the traditional one based on Paull’s algorithm, as reported by Hopcroft and Ullman (1979), and one described by Moore (2000)5 , based on a technique described by Johnson (1998). Our experience concurs with Moore that the left-corner transform he describes produces a more compact left-recursion free grammar than that of Paull’s algorithm. For the K&K approximation, we were unable to get any grammar to compile through to a working language model using Paull’s algorithm (the models built with Paull’s algorithm caused the recognizer to exceed memory bounds), and only succeeded with Moore’s left-recursion elimination technique. 6 Conclusions We have presented descriptions of two algorithms for approximating typed unific"
P01-1022,P91-1032,0,0.034738,"e recognizer. This approach has been used in several systems, CommandTalk (Moore et al. 1997), RIALIST PSA simulator (Rayner et al. 2000), WITAS (Lemon et al. 2001), and SETHIVoice (Gauthron and Colineau 1999). While theoretically straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification gramm"
P01-1022,P96-1030,0,0.0216402,"y straight-forward, this approach is more demanding in practice, as each of the compilation stages contains the potential for a combinatorial explosion that will exceed memory and time bounds. There is also no guarantee that the resulting language model will lead to accurate and efficient speech recognition. We will be interested in this paper in sound approximations (Pereira and Wright 1991) in which the language accepted by the approximation is a superset of language accepted by the original grammar. While we conceed that alternative techniques that are not sound (Black 1989, (Johnson 1998, Rayner and Carter 1996) may still be useful for many purposes, we prefer sound approximations because there is no chance that the correct hypothesis will be eliminated. Thus, further processing techniques (for instance, N-best search) will still have an opportunity to find the optimal solution. We will describe and evaluate two compilation approaches to approximating a typed unification grammar with a context-free grammar. We will also describe and evaluate additional techniques to reduce the size and structural ambiguity of the language model. This expressiveness allows us to write grammars with a small number of r"
P01-1022,W00-0311,1,0.863494,"e difference revolves around the availability of data. Research systems can achieve impressive performance using statistical language models trained on large amounts of domain-targeted data, but for many domains sufficient data is not available. Data may be unavailable because the domain has not been explored before, the relevant data may be confidential, or the system may be designed to do new functions for which there is no human-human analog interaction. The statistical approach is unworkable in such cases for both the commercial developers and for some research systems (Moore et al. 1997, Rayner et al. 2000, Lemon et al. 2001, Gauthron and Colineau 1999). Even in cases for which there is no impediment to collecting data, the expense and time required to collect a corpus can be prohibitive. The existence of the ATIS database (Dahl et al. 1994) is no doubt a factor in the popularity of the travel domain among the research community for exactly this reason. A major problem with grammar-based finitestate or context-free language models is that they can be tedious to build and difficult to maintain, as they can become quite large very quickly as the scope of the grammar increases. One way to address"
P01-1022,P85-1018,0,0.161908,"uage model. This expressiveness allows us to write grammars with a small number of rules (from dozens to a few hundred) that correspond to grammars with large numbers of CF rules. Note that the approximation need not incorporate all of the features from the original grammar in order to provide a sound approximation. In particular, in order to derive a finite CF grammar, we will need to consider only those features that have a finite number of possible values, or at least consider only finitely many of the possible values for infinitely valued features. We can use the technique of restriction (Shieber 1985) to remove these features from our feature structures. Removing these features may give us a more permissive language model, but it will still be a sound approximation. The experimental results reported in this paper are based on a grammar under development at RIACS for a spoken dialogue interface to a semi-autonomous robot, the Personal Satellite Assistant (PSA). We consider this grammar to be medium-sized, with 61 grammar rules and 424 lexical entries. While this may sound small, if the grammar were expanded by instantiating variables in all legal permutations, it would contain over"
P01-1022,C98-1098,0,\N,Missing
P01-1022,P93-1008,1,\N,Missing
P01-1022,H93-1008,1,\N,Missing
P03-2024,C00-2097,1,0.838812,"ght, numbers) or even gestures (e.g. pointing to an area of the body). This is clearly a domain in which the constraints of the task are sufficient for a limited domain, one way spoken translation system to be a useful tool. 2 An architecture for limited-domain speech translation The basic philosophy behind the architecture of the system is to attempt an intelligent compromise between fixed-phrase translation on one hand (e.g. (IntegratedWaveTechnologies, 2002)) and linguistically motivated grammar-based processing on the other (e.g. V ERBMOBIL (Wahlster, 2000) and Spoken Language Translator (Rayner et al., 2000a)). At run-time, the system behaves essentially like a phrasal translator which allows some variation in the input language. This is close in spirit to the approach used in most normal phrase-books, which typically allow “slots” in at least some phrases (“How much does — cost?”; “How do I get to — ?”). However, in order to minimize the overhead associated with defining and maintaining large sets of phrasal patterns, these patterns are derived from a single large linguistically motivated unification grammar; thus the compile-time architecture is that of a linguistically motivated system. Phras"
P03-2024,E03-2010,1,0.673693,"le for source language speech recognition, including parsing and production of semantic representation; transfer and generation; and synthesis of target language speech. The speech processing modules (recognition and synthesis) are implemented on top of the standard Nuance Toolkit platform (Nuance, 2003). Recognition is constrained by a CFG language model written in Nuance Grammar Specification Language (GSL), which also specifies the semantic representations produced. This language model is compiled from a linguistically motivated unification grammar using the Open Source REGULUS 2 platform (Rayner et al., 2003; Regulus, 2003); the compilation process is driven by a small corpus of examples. The language processing modules (transfer and generation) are a suite of simple routines written in SICStus Prolog. The speech and language processing modules communicate with each other through a minimal file-based protocol. The semantic representations on both the source and target sides are expressed as attribute-value structures. In accordance with the generally minimalistic design philosophy of the project, semantic representations have been kept as simple as possible. The basic principle is that the repres"
P03-2038,E03-1078,1,0.807195,"anguage Engine’s Quasi Logical Form notaREGULUS tion (van Eijck and Moore, 1992). A grammar built on top of the general grammar is transformed into a specialised Nuance grammar in the following processing stages: 1. The training corpus is converted into a “treebank” of parsed representations. This is done using a left-corner parser representation of the grammar. 2. The treebank is used to produce a specialised grammar in REGULUS format, using the EBL algorithm (van Harmelen and Bundy, 1988; Rayner, 1988). 3. The final specialised grammar is compiled into a Nuance GSL grammar. 4 ALTERF ALTERF (Rayner and Hockey, 2003) is another Open Source toolkit, whose purpose is to allow a clean combination of rule-based and corpus-driven processing in the semantic interpretation phase. There is typically no corpus data available at the start of a project, but considerable amounts at the end: the intention behind ALTERF is to allow us to shift smoothly from an initial version of the system which is entirely rule-based, to a final version which is largely data-driven. ALTERF characterises semantic analysis as a task slightly extending the “decision-list” classification algorithm (Yarowsky, 1994; Carter, 2000). We start"
P03-2038,E03-2010,1,0.858532,"Missing"
P03-2038,P94-1013,0,\N,Missing
P05-3008,E03-1078,1,0.880446,"Missing"
P05-3008,E03-2010,1,0.241003,"involved (Knight et al., 2001) suggested that grammar-based systems outperformed statistical ones for this kind of user. Given that neither of the above arguments is very strong, we wanted to implement a framework which would allow us to compare grammar-based methods with statistical ones, and retain the option of switching from a grammar-based framework to a statistical one if that later appeared justified. The Regulus and Alterf platforms, which we have developed under Clarissa and other earlier projects, are designed to meet these requirements. The basic idea behind Regulus (Regulus, 2005; Rayner et al., 2003) is to extract grammar-based language models from a single large unification grammar, using example-based methods driven by small corpora. Since grammar construction is now a corpus-driven process, the same corpora can be used to build statistical language models, facilitating a direct comparison between the two methodologies. On its own, however, Regulus only permits comparison at the level of recognition strings. Alterf (Rayner and Hockey, 2003) extends the paradigm to ID 1 2 3 4 5 6 Rec SLM GLM SLM GLM SLM GLM Features Confidence Confidence Confidence + Lexical Confidence + Lexical Confiden"
rayner-etal-2006-regulus,N01-1030,1,\N,Missing
rayner-etal-2006-regulus,E03-2010,1,\N,Missing
rayner-etal-2006-regulus,C02-1095,0,\N,Missing
rayner-etal-2006-regulus,H05-2014,1,\N,Missing
rayner-etal-2006-regulus,P01-1022,1,\N,Missing
rayner-etal-2006-regulus,2005.eamt-1.8,1,\N,Missing
W00-0311,P93-1008,0,0.0432865,"erature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out Using the SRI Gemini system (Dowding et al., 1993), using a domain-independent unification. grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grammar using the methods of (Moore et al., 1997); the n ~ resnlt is that only grammatically wellformed utterances Gan be recognized. Output from the initial language-processing step is represented in a version of Quasi Logical Form (van Eijck and Moore, 1992), and passed in that form to the dialogue manager. We refer to these as linguistic level representations. The aspects of the system which are of primary interest here concern the dialogue manager"
W00-0311,A97-1001,0,0.462118,"he basic task we consider in this paper is that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the"
W00-0311,A97-1008,0,0.017903,"that of using spoken language to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech in"
W00-0311,P99-1024,0,0.39148,"ronmental functions. In particular, our simulation allows voice access to the current and past values of the fixed sensor readings. The initial PSA speech interface demo consists of a simple simulation of the Shuttle. State parameters include the PSA's current position, some environmental variables such as local temperature, pressure and carbon dioxide levels, and the status of the Shuttle's doors (open/closed). A visual display gives direct feedback on some of these parameters. The speech and language processing architecture is based on that of the SRI CommandTalk system (Moore et al., 1997; Stent et al., 1999). The system comprises a suite of about 20 agents, connected together using the SRI Open Agent Architecture (OAA; (Martin et al., 1998)). Speech recognition is performed using a version of the Nuance recognizer (Nuance, 2000). Initial language processing is carried out Using the SRI Gemini system (Dowding et al., 1993), using a domain-independent unification. grammar and a domain-specific lexicon. The language processing grammar is compiled into a recognition grammar using the methods of (Moore et al., 1997); the n ~ resnlt is that only grammatically wellformed utterances Gan be recognized. Ou"
W00-0311,P94-1001,0,0.0117831,"to give commands to a semiautonomous robot or other similar system. As evidence of the importance of this ta~k in the NLP community note that the early, influential system SHRDLU (Winograd, 1973) was intended to address just this type of problem. More recent work on spoken language interfaces to semi-autonomous robots include SRI's Flakey robot (Konolige et al., 1993) and NCARAI's InterBOT project (Perzanowski et al., 1998; Perzanowski et al., 1999). A number of other systems have addressed part of the task. CommandTalk (Moore et al., 1997), Circuit Fix-It Shop (Smith, 1997) and Tl:tAINS-96 (Traum and Allen, 1994; Traum and Andersen, 1999) are spoken language systems but they interface to simulation or help facilities rather than semi-autonomous agents. Jack's MOOse Lodge (Badler et al., 1999) takes text rather than speech as natural language input and the avatars being controlled are not semi-autonomous. Other researchers have considered particular aspects of the problem such as accounting for various aspects of actions (Webber, 1995; Pym et al., 1995). In most of this and other related work the treatment is some variant of the following. If there is a speech interface, the input speech signal is con"
W00-0311,H93-1008,0,\N,Missing
W00-2026,C86-1016,0,0.156261,"Missing"
W00-2026,J81-4003,0,0.346452,"Missing"
W00-2026,E87-1049,0,0.171809,"Missing"
W00-2026,E87-1003,0,\N,Missing
W06-3702,2005.eamt-1.8,1,0.658561,"Missing"
W06-3702,1999.mtsummit-1.8,0,0.0751905,"systems utilizing probabilistic context-free grammar tuning appear to deliver better results when training data is sparse (Rayner et al., 2005a). One drawback of grammar-based systems is that out-of-coverage utterances will be neither recognized nor translated, an objection that critics have sometimes painted as decisive. It is by no means obvious, however, that restricted coverage is such a serious problem. In text processing, work on several generations of controlled language systems has developed a range of techniques for keeping users within the bounds of system coverage (Kittredge, 2003; Mitamura, 1999). If these techniques work for text processing, it is surely not inconceivable that variants of them will be equally successful for spoken language applications. Users are usually able to adapt to a controlled language system given enough time. The critical questions are how to provide efficient support to guide them towards the system&apos;s coverage, and how much time they will then need before they have acclimatized. With regard to top-level translation functionality, the choice is between unidirectional and bidirectional systems. Bidirectional systems are certainly possible today1, but the argu"
W06-3702,E03-2010,1,0.889866,"Missing"
W06-3702,H05-2014,1,0.898237,"Missing"
W06-3702,2005.mtsummit-papers.25,1,0.809631,"s. Resource grammars are now available for several languages, including English, Japanese (Rayner et al., 2005b), French (Bouillon et al., 2006) and Spanish. MedSLT includes a help module, whose purpose is to add robustness to the system and guide the user towards the supported coverage. The help module uses a second backup recognizer, equipped with a statistical language model; it matches the results from this second recognizer against a corpus of utterances, which are within system coverage and have already been judged to give correct translations. In previous studies (Rayner et al., 2005a; Starlander et al., 2005), we showed that the grammar-based recognizer performs much better than the statistical one on in-coverage utterances, and rather worse on out-of-coverage ones. We also found that having the help module available approximately doubled the speed at which subjects learned to use the system, measured as the average difference in semantic error rate between the results for their first quarter-session and their last quarter-session. It is also possible to recover from recognition errors by selecting one of the displayed help sentences; in the cited studies, we found that this increased the number o"
W06-3707,2005.eamt-1.8,1,0.885548,"Missing"
W06-3707,2002.tmi-papers.17,0,0.0583709,"s domain for the three main input languages. Differences in the sizes of the recognition vocabularies are primarily due to differences in use of inflection. Japanese, with little inflectional morphology, has the smallest vocabulary; French, which inflects most parts of speech, has the largest. 3 The development environment Although the MedSLT system is rule-based, we would, for the usual reasons, prefer to acquire these rules from corpora using some well-defined method. There is, however, little or no material available for most medical speech translation domains, including ours. As noted in (Probst and Levin, 2002), scarcity of data generally implies use of some strategy to obtain a carefully structured training corpus. If the corpus is not organised in this way, conflicts between alternate learned rules occur, and it is hard to inWhere? “do you experience the pain in your jaw” “does the pain spread to the shoulder” When? “have you had the pain for more than a month” “do the headaches ever occur in the morning” How long? “does the pain typically last a few minutes” “does the pain ever last more than two hours” How often? “do you get headaches several times a week” “are the headaches occurring more often"
W06-3707,E03-2010,1,0.863259,"Missing"
W06-3707,2005.jeptalnrecital-long.17,1,0.922576,"k is for the moment statistical, but rule-based systems are still a very respectable alternative. In particular, nearly all systems which have actually been deployed are rulebased. Prominent examples are (Phraselator, 2006; S-MINDS, 2006; MedBridge, 2006). MedSLT (MedSLT, 2005; Bouillon et al., 2005) is a unidirectional medical speech translation system for use in doctor-patient diagnosis dialogues, which covers several different language pairs and subdomains. Recognition is performed using grammarThe MedSLT demonstrator has already been extensively described elsewhere (Bouillon et al., 2005; Rayner et al., 2005a), so this section will only present a brief summary. The main components are a set of speech recognisers for the source languages, a set of generators for the target languages, a translation engine, sets of rules for translating to and from interlingua, a simple discourse engine for dealing with context-dependent translation, and a top-level which manages the information flow between the other modules and the user. MedSLT also includes an intelligent help module, which adds robustness to the system and guides the user towards the supported coverage. The help module uses a backup recogniser,"
W06-3707,2005.mtsummit-papers.25,1,0.872166,"Missing"
W07-0806,2005.eamt-1.8,1,0.863353,"or: moderate? Trad: !؟23$45 ؟63$45 ؟3$45 (muhtamala, muhtamalan, muhtamal) ‘moderate_fem_attributive_adj, moderate_vocalized-predicative_adj, moderate_attributive_adj’. It is also essential for rules of translation to be applied consistently. For instance, in MedSLT, 42 Doctor: was the onset of headaches sudden? Trad: ة؟9 ,  ا;اع- 7 ( هMedSLT) (hal dhahara al sudaa fajatan?) (Q appear-past-3 the headache suddenly?) The Architecture MedSLT is a grammar-based medical speech translation system which uses the commercial Nuance speech recognition platform. It has two main features (Bouillon et al., 2005). First, all the language models (for recognition, analysis, generation) are produced from linguistically motivated, general unification grammars using the Regulus platform (Rayner, et al., 2006). First, domain specific unification grammars are created from the general grammar for the different domains of medical diagnosis through a trainable corpus-based automatic grammar specialization process. They are, next, compiled into Context Free Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven genera"
W07-0806,W06-3702,1,0.856343,"Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven generation (Shieber et al., 1990). Therefore, the different grammars needed by the system under this approach are easy to build and maintain. This leads us to the second feature. Because grammar-based speech recognition only produces competitive results for the sentences covered by the grammar, the user will need to learn the coverage of the system. In order to assist in this, a help system is included in the system (Starlander et al., 2005 and Chatzichrisafis et al., 2006). The help system suggests, after each user utterance, similar utterances covered by the grammar which can be taken as a model. In order to derive the help sentences, the system performs, in parallel, a statistical recognition of the input speech. It then compares the recognition result using an N-gram based metric, against a set of known correct in-coverage questions to extract the most similar ones. It is in that way that we introduce some of the robustness of the statistical systems in the controlled application. Once the sentence recognized, the translation is interlingua-based. Regulus al"
W07-0806,W06-3701,0,0.011483,"h, Japanese, Spanish and Catalan. This article focuses on the system development for Arabic. In general, translation in this context raises two specific questions: 1) how to achieve recognition quality that is good enough for translation, and 2) how to get translations to be as idiomatic as possible so they can be understood by the patient. For close languages and domains where accuracy is not very important (e.g. information requests), it may be possible to combine a statistical recognizer with a commercial translation system as it is often done in commercial tools such as SpokenTranslation (Seligman and Dillinger, 2006). However, for this specific application in a multilingual context, this solution is not applicable at all: even if perfect recognition were possible (which is far from being the case), current commercial tools for translating to Arabic do not guarantee good quality. The domain dealt with here contains, in fact, many structures specific to this type of oral dialogue that can not be handled by these systems. For example, all the doctor’s interactions with the MedSLT system consist of questions whose structures differ from one language to another, with each language having its own constraints. C"
W07-0806,2005.mtsummit-papers.25,1,0.844904,"compiled into Context Free Grammars (CFGs) in a format suitable for use with the Nuance speech recognition platform, and into a form needed for a variant of Semantic Head-driven generation (Shieber et al., 1990). Therefore, the different grammars needed by the system under this approach are easy to build and maintain. This leads us to the second feature. Because grammar-based speech recognition only produces competitive results for the sentences covered by the grammar, the user will need to learn the coverage of the system. In order to assist in this, a help system is included in the system (Starlander et al., 2005 and Chatzichrisafis et al., 2006). The help system suggests, after each user utterance, similar utterances covered by the grammar which can be taken as a model. In order to derive the help sentences, the system performs, in parallel, a statistical recognition of the input speech. It then compares the recognition result using an N-gram based metric, against a set of known correct in-coverage questions to extract the most similar ones. It is in that way that we introduce some of the robustness of the statistical systems in the controlled application. Once the sentence recognized, the translatio"
W07-1806,2006.jeptalnrecital-long.6,1,0.802795,"Missing"
W07-1806,1999.mtsummit-1.8,0,\N,Missing
W07-1806,2005.mtsummit-papers.25,1,\N,Missing
W07-1806,P05-3008,1,\N,Missing
W07-1806,2005.eamt-1.8,1,\N,Missing
W07-1806,W06-3705,0,\N,Missing
W07-1806,W06-3702,1,\N,Missing
W08-0210,2005.eamt-1.8,1,0.929071,"Missing"
W08-0210,J00-4006,0,0.00369613,"or system, usually with a video or a runnable version. Examples of system web materials that we use include: (Resurrected)SHRDLU (http://www.semaphorecorp.com/ misc/shrdlu.html), TRIPS and TRAINS (http://www.cs.rochester.edu/ research/cisd/projects/trips/ Galaxy movies/TRIPS Overview/), (http://groups.csail.mit.edu/sls/ /applications/jupiter.shtml), VocalJoyStick (http://ssli.ee.washington. edu/vj/), and ProjectListen (http:// www.cs.cmu.edu/∼listen/mm.html)and NASA’s Clarissa Procedure Browser (http:// ti.arc.nasa.gov/projects/clarissa/ gallery.php?ta=&gid=&pid=). Jurasfsky and Martin (Jurafsky and Martin, 2000) is used as an additional text and various research papers are given as reading in addition to the Regulus material. Jurafsky and Martin is also good source for exercises. The Jurafsky and Martin material and the Regulus material are fairly complementary and fit together well in the context of this type of course. Various other exercises are used, including two student favorites: a classic ’construct your own ELIZA’ task, and a exercise in reverse engineering a telephone call center, which is an original created for this course. 5 Programming languages Prolog is used as the primary language in"
W08-1506,W07-1806,1,0.886739,"Missing"
W08-1506,bouillon-etal-2008-developing,1,0.895278,"fr Abstract 1 Introduction MedSLT is a medium-vocabulary grammar-based medical speech translation system built on top of the Regulus platform (Rayner et al., 2006). It is intended for use in doctor-patient diagnosis dialogues, and provides coverage of several subdomains and a large number of different languagepairs. Coverage is based on standard examination questions obtained from physicians, and focusses primarily on yes/no questions, though there is also support for WH-questions and elliptical utterances. Detailed descriptions of MedSLT can be found in earlier papers (Bouillon et al., 2005; Bouillon et al., 2008)1 . In the rest of this note, we will briefly sketch several versions of the system that we intend to demo at the workshop, each of which displays new features developed over the last year. Section 2 describes an any-language-toany-language multilingual version of the system; Section 3, a bidirectional English ↔ Spanish version; Section 4, a version running on a mobile PDA MedSLT is a grammar-based medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different subdomains and multiple language pairs. Vocabulary ranges from"
W08-1506,W06-3702,1,0.905634,"Missing"
W08-1506,W07-1807,1,0.893685,"Missing"
W08-1506,tsourakis-etal-2008-building,1,0.874379,"Missing"
W08-1506,2005.eamt-1.8,1,0.858642,"ukie.nakao@univ-nantes.fr Abstract 1 Introduction MedSLT is a medium-vocabulary grammar-based medical speech translation system built on top of the Regulus platform (Rayner et al., 2006). It is intended for use in doctor-patient diagnosis dialogues, and provides coverage of several subdomains and a large number of different languagepairs. Coverage is based on standard examination questions obtained from physicians, and focusses primarily on yes/no questions, though there is also support for WH-questions and elliptical utterances. Detailed descriptions of MedSLT can be found in earlier papers (Bouillon et al., 2005; Bouillon et al., 2008)1 . In the rest of this note, we will briefly sketch several versions of the system that we intend to demo at the workshop, each of which displays new features developed over the last year. Section 2 describes an any-language-toany-language multilingual version of the system; Section 3, a bidirectional English ↔ Spanish version; Section 4, a version running on a mobile PDA MedSLT is a grammar-based medical speech translation system intended for use in doctor-patient diagnosis dialogues, which provides coverage of several different subdomains and multiple language pairs."
W08-1511,1983.tc-1.13,0,0.250159,"Missing"
W09-1503,2008.amta-govandcom.4,1,0.832517,"d a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In particular, it is possible to compile grammars into generators, and use them to support paraphrasing from the internal semantic representations created during dialogue processing. This capability is key to the newest part of our regression testing approach, and is discussed in detail in Section 3. First, though, Section 2 gives an overview of Regulus and the architecture of Regulus-based systems; we discuss features that complicate regression testing, and how to address these problems"
W09-1503,W08-0119,0,0.0238802,"Missing"
W09-1503,P05-3008,1,0.816873,"ses the methodology we have developed to address regression testing issues within the Regulus framework. Regulus (Rayner et al., 2006) is an Open Source toolkit for builting medium 14 Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14–21, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics vocabulary spoken dialogue and translation applications, and has been used to build a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In par"
W09-1503,P00-1013,0,0.0223923,"lopment cycle. We will in particular be interested in regression testing. 3 Context, regression testing and paraphrasing The three main components of the spoken dialogue system — the IM, DM and OM — all transform one or more inputs into one or more outputs. With the current focus on machine learning techniques, a natural thought is to learn the relevant tranformations from examples. Implemented mainly through Partially Observable Markov Decision Processes (POMDPs), this idea is attractive theoretically, but has been challenging to scale up. Systems have been restricted to very simple domains (Roy et al., 2000; Zhang et al., 2001) and only recently have techniques been developed that show promise for use in real-world systems (Williams and Young, 2007; Gasi´c et al., 2008). The representations required in many systems are more complex than those employed even in the more recent POMDP based work, and there is also the usual problem that it is not easy to obtain training data. In practice, most people are forced to construct the transformation rules by hand; the Regulus framework assumes this will be the case. Hand-coding of dialogue processing components involves the usual software engineering probl"
W09-1503,tsourakis-etal-2008-building,1,0.851551,"s framework. Regulus (Rayner et al., 2006) is an Open Source toolkit for builting medium 14 Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14–21, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics vocabulary spoken dialogue and translation applications, and has been used to build a number of non-trivial spoken dialogue systems. Prominent examples include NASA’s Clarissa Procedure Navigator (Rayner et al., 2005), Geneva University’s multi-modal mobile-platform Calendar application (Tsourakis et al., 2008), SDS, a prototype in-car system developed by UC Santa Cruz in collaboration with Ford Motors Research which was voted first in Ford’s 2007 internal technology fair, and Taxi, a speech-enabled game in which the user interacts with a simulated cab driver to navigate around a map of Manhattan. It has also been used to build the MedSLT medical speech translation system (Bouillon et al., 2008). The Regulus platform includes tools for developing feature grammars, and compiling them in various ways. In particular, it is possible to compile grammars into generators, and use them to support paraphrasi"
W09-2607,2008.amta-govandcom.4,1,\N,Missing
W09-2607,E06-1008,0,\N,Missing
W09-2607,P02-1040,0,\N,Missing
W09-2607,W08-0327,0,\N,Missing
W09-2607,2006.amta-papers.24,0,\N,Missing
W09-2607,2005.eamt-1.8,1,\N,Missing
W09-2607,W08-1511,1,\N,Missing
W09-2607,P00-1056,0,\N,Missing
W09-2607,W06-3702,1,\N,Missing
W97-1505,A88-1019,0,0.155472,"Missing"
W97-1505,W94-0104,0,0.0419199,"n performed along either of the hierarchy dimensions. The expansion could be done by general principles (add all trees of a certain subcat frame if any are present), or could be done based on performance of the sub-grammar on held-out training data. Most domains have a rich terminological vocabulary, which if not taken into account can cause prohibitive ambiguity in parsing and interpretation. Identifying and demarcating domain specific terminology is helpful for all of these approaches, since the terms can then be treated as single tokens. This can been done either manually or automatically (Daille, 1994; Jacquemin and Royaut, 1994). Once the sub-grammar has been finalized, strategies for recovering from failure to parse should be developed. One simple strategy is to fall back to the large/whole grammar. A more sophisticated strategy would be to back off using a lexical hierarchy in the same way it was used for generalizing from the training set. hierarchy. Without the description hierarchy, there would be no need to reconcile these differences, since they would be entirely independent pieces of a flat grammar. 3 Tailoring X T A G to t h e W e a t h e r Domain While it is certainly interestin"
W97-1505,C94-1024,1,0.834435,"there is training data (usually unannotated) available; that default mechanisms will be adequate for handling over-specialization (since we know training data will not perfectly reflect the genre) and that the smaller grammar combined with defaults will still be more efficient than the large grammar. Based on these assumptions, the first choice is whether to do full parsing at all in the final application. If the domain contains a large number of fragments, it might be preferable to use a partial parsing approach, in which case development of a sub-grammar will be less crucial. Supertagging (Joshi and Srinivas, 1994) is one such approach; once the supertagger is trained for the domain, it could be used in place of the full parser. If, however, it is determined that full parsing is practicable for the domain, there are still a number of considerations in deriving the sub-grammar. In the ideal situation, there would already be a corrected parsed corpus (treebank), which can be used for crafting a sub-grammar for the domain. This is exceptionally unlikely, and in the more common case, training data will have to be constructed, either manually or automatically. In a lexicalized grammar like LTAG, this turns o"
W97-1505,C92-3145,0,0.0309251,"and more portable implementation of the X-interface to the grammar and all of the supporting tools in CLISP, which is freely available. We also present a methodology for specializing our grammar to a particular domain, and give some results on this effort. 1 1.1 Development of XTAG and Current Status 1.2 Current status of XTAG Working with and developing a large grammar is a challenging process, and the importance of having good visualization tools cannot be over-emphasized. Currently the XTAG system has X-windows based tools for viewing and updating the morphological and syntactic databases (Karp et al., 1992; Egedi and Martin, 1994), and a sophisticated parsing and grammar development interface. This interface includes a tree editor, the ability to vary parameters H i s t o r y of X T A G The XTAG project has been ongoing at Penn in some form or another since 1988. It began with a toy grammar run on LISP machines, and currently has a large English grammar, small grammars in several other languages, a sophisticated X-windows based grammar development environment and numerous satellite tools. Approximately 35 people have worked extensively on the system, and at least that many have worked more peri"
W97-1505,C96-2120,0,0.0494734,"Missing"
W97-1505,C96-1034,0,\N,Missing
