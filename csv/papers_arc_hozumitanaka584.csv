I05-4002,Evaluation of a {J}apanese {CFG} Derived from a Syntactically Annotated Corpus with Respect to Dependency Measures,2005,11,4,5,0,48618,tomoya noro,Proceedings of the Fifth Workshop on {A}sian Language Resources ({ALR}-05) and First Symposium on {A}sian Language Resources Network ({ALRN}),0,"Parsing is one of the important processes for natural language processing and, in general, a large-scale CFG is used to parse a wide variety of sentences. For many languages, a CFG is derived from a large-scale syntactically annotated corpus, and many parsing algorithms using CFGs have been proposed. However, we could not apply them to Japanese since a Japanese syntactically annotated corpus has not been available as of yet. In order to solve the problem, we have been building a large-scale Japanese syntactically annotated corpus. In this paper, we show the evaluation results of a CFG derived from our corpus and compare it with results of some Japanese dependency analyzers."
I05-2019,e{B}onsai: An Integrated Environment for Annotating Treebanks,2005,6,6,5,0,39152,hiroshi ichikawa,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,None
bilac-etal-2004-evaluating,Evaluating the {FOKS} Error Model,2004,8,2,3,1,47895,slaven bilac,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Learners of Japanese face great difficulty when trying to lookup words containing kanji in a dictionary, due to the requirement of knowing the correct reading of the target word. We propose a system that imitates the cognitive process learners go through in generating readings for novel kanji strings, and provide direct access to the dictionary entries based on the generated readings. In doing so we remove the correct reading requirement. The system described here is implemented in a web-based environment and freely available for general use. In this paper we provide an analysis of query and error data collected by our server."
yoshida-etal-2004-retrieving,Retrieving Annotated Corpora for Corpus Annotation,2004,2,2,4,0,41969,kyosuke yoshida,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper introduces a tool {\textbackslash}Bonsai which supports human in annotating corpora with morphosyntactic information, and in retrieving syntactic structures stored in the database. Integrating annotation and retrieval enables users to annotate a new instance while looking back at the already annotated sentences which share the similar morphosyntactic structure. We focus on the retrieval part of the system, and describe a method to decompose a large input query into smaller ones in order to gain retrieval efficiency. The proposed method is evaluated with the Penn Treebank corpus, showing significant improvements."
C04-1086,A hybrid back-transliteration system for {J}apanese,2004,16,24,2,1,47895,slaven bilac,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Transliterating words and names from one language to another is a frequent and highly productive phenomenon. Transliteration is information losing since important distinctions are not preserved in the process. Hence, automatically converting transliterated words back into their original form is a real challenge. In addition, due to its wide applicability in MT and CLIR, it is an interesting problem from a practical point of view. In this paper, we propose a new method, combining the transliterated string segmentation module with phoneme-based and grapheme-based transliteration modules in order to enhance the back-transliterations of Japanese words. Our experiments show significant improvements achieved by the hybrid approach."
W03-2711,The Interactive Navigation to the Stored {Q}{\\&}{A} data using Simple Questions,2003,0,0,2,0,33479,kunio matsui,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,None
W03-1611,Paraphrasing {J}apanese Noun Phrases using Character-based Indexing,2003,17,4,2,1,301,takenobu tokunaga,Proceedings of the Second International Workshop on Paraphrasing,0,"This paper proposes a novel method to extract paraphrases of Japanese noun phrases from a set of documents. The proposed method consists of three steps: (1) retrieving passages using character-based index terms given a noun phrase as an input query, (2) filtering the retrieved passages with syntactic and semantic constraints, and (3) ranking the passages and reformatting them into grammatical forms. Experiments were conducted to evaluate the method by using 53 noun phrases and three years worth of newspaper articles. The accuracy of the method needs to be further improved for fully automatic paraphrasing but the proposed method can extract novel paraphrases which past approaches could not."
W03-1107,Feature Selection in Categorizing Procedural Expressions,2003,15,25,4,0,52705,mineki takechi,Proceedings of the Sixth International Workshop on Information Retrieval with {A}sian Languages,0,"Text categorization, as an essential component of applications for user navigation on the World Wide Web using Question-Answering in Japanese, requires more effective features for the categorization of documents and the efficient acquisition of knowledge. In the questions addressed by such navigation, we focus on those questions for procedures and intend to clarify specification of the answers."
tokunaga-etal-2002-constructing,Constructing a lexicon of action,2002,3,2,4,1,301,takenobu tokunaga,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes a Japanese speech dialogue system that enables a user to interact with agents in a virtual world and proposes a design framework for building a lexicon of action. This lexicon is used to realize the behavior of the agents in response to the userxe2x80x99s commands. The lexicon has two levels xe2x80x93 a macro and micro level. The system uses the macro-level lexicon, which is similar to a conventional plan library, to translate the userxe2x80x99s goal to a sequence of basic movements. This process is the same as conventional planning with symbol manipulation. The micro-level lexicon is used to translate the basic movements into animation, which is represented by a sequence of avatar postures. We discuss how to define a set of basic movements and how to make these basic movements reusable."
shirai-etal-2002-towards,Towards a Thesaurus of Predicates,2002,4,2,4,0,51459,satoshi shirai,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We propose a thesaurus of predicates that can help to resolve pre-editing and/or post-editing problems in machine translation environments. It differs from earlier approaches such as conventional dictionaries in that we are aiming to link a wide range of near-synonyms and paraphrases. We are compiling such similar examples through both introspection and the use of translation data, giving us a large collection of monolingual and bilingual equivalences. This thesaurus enables the following machine translation techniques. (a) Unification of synonymous expressions in the source language (source language paraphrasing). (b) Conversion of homonymous expressions to more easily translated ones (source language rewriting). (c) Development of expressions appearing in the target language into various expressions (target language paraphrasing)."
baldwin-etal-2002-enhanced,Enhanced {J}apanese Electronic Dictionary Look-up,2002,7,2,5,0.942029,1468,timothy baldwin,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes the process of data preparation and reading generation for an ongoing project aimed at improving the accessibility of unknown words for learners of foreign languages, focusing initially on Japanese. Rather then requiring absolute knowledge of the readings of words in the foreign language, we allow look-up of dictionary entries by readings which learners can predictably be expected to associate with them. We automatically extract an exhaustive set of phonemic readings for each grapheme segment and learn basic morpho-phonological rules governing compound word formation, associating a probability with each. Then we apply the naive Bayes model to generate a set of readings and give each a likeliness score based on previously extracted evidence and corpus frequencies."
C02-1059,Processing {J}apanese Self-correction in Speech Dialog Systems,2002,6,5,3,0,10700,kotaro funakoshi,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Speech dialog systems need to deal with various kinds of ill-formed speech inputs that appear in natural human-human dialog. Self-correction (or speech-repair) is a particularly problematic phenomenon. Although many ways of dealing with self-correction have been proposed, these have limitations in both detecting and correcting for this phenomenon. In this paper, we propose a method to overcome these problems in Japanese speech dialog. We evaluate the proposed method using our speech dialog corpus and discuss its limitations and the work that remains to be done."
C02-1140,Bringing the Dictionary to the User: The {FOKS} System,2002,9,9,3,1,47895,slaven bilac,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"The dictionary look-up of unknown words is particularly difficult in Japanese due to the complicated writing system. We propose a system which allows learners of Japanese to look up words according to their expected, but not necessarily correct, reading. This is an improvement over previous systems which provide no handling of incorrect readings. In preprocessing, we calculate the possible readings each kanji character can take and different types of phonological and conjugational changes that can occur, and associate a probability with each. Using these probabilities and corpus-based frequencies we calculate a plausibility measure for each generated reading given a dictionary entry, based on the naive Bayes model. In response to a reading input, we calculate the plausibility of each dictionary entry corresponding to the reading and display a list of candidates for the user to choose from. We have implemented our system in a web-based environment and are currently evaluating its usefulness to learners of Japanese."
S01-1013,The {J}apanese Translation Task: Lexical and Structural Perspectives,2001,4,1,4,1,1468,timothy baldwin,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"This paper describes two distinct attempts at the SENSEVAL-2 Japanese translation task. The first implementation is based on lexical similarity and builds on the results of Baldwin (2001b; 2001a), whereas the second is based on structural similarity via the medium of parse trees and includes a basic model of conceptual similarity. Despite its simplistic nature, the lexical method was found to perform the better of the two, at 49.1% accuracy, as compared to 41.2% for the structural method and 36.8% for the baseline."
2001.mtsummit-papers.28,Decision lists for determining adjective dependency in {J}apanese,2001,5,1,5,1,49931,taiichi hashimoto,Proceedings of Machine Translation Summit VIII,0,"In Japanese constructions of the form [N1 no Adj N2], the adjective Adj modifies either N1 or N2. Determing the semantic dependencies of adjective in such phrase is an important task for machine translation. This paper describes a method for determining the adjective dependency in such constructions using decision lists, and inducing decision lists from training contexts with correct semantic dependencies and without. Based on evaluation, our method is able to determine adjective dependency with an precision of about 94{\%}. We further analyze rules in the induced decision lists and examine effective features to determine the semantic dependencies of adjectives."
Y00-1002,"Verb Alternations and {J}apanese : How, What and Where",2000,9,3,2,1,1468,timothy baldwin,"Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation",0,"We set out to empirically identify the range and frequency of basic verb alternation types in Japanese, through analysis of the Goi-Taikei Japanese pattern-based valency dictionary. This is achieved through comparison of the selectional preference annotation on corresponding case slots, based on the assumption that selectional preferences are preserved under alternation. Three separate extraction methods are considered, founded around: (1) simple match of selectional restrictions; (2) selectional restriction matching, with recourse to penalised backing-off; and (3) semantic density, again with recourse to backing-off."
shirai-etal-2000-semi,Semi-automatic Construction of a Tree-annotated Corpus Using an Iterative Learning Statistical Language Model,2000,9,0,2,1,29633,kiyoaki shirai,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"In this paper, we propose a method to construct a tree-annotated corpus, when a certain statistical parsing system exists and no tree-annotated corpus is available as training data. The basic idea of our method is to sequentially annotate plain text inputs with syntactic trees using a parser with a statistical language model, and iteratively retrain the statistical language model over the obtained annotated trees. The major characteristics of our method are as follows: (1)in the first step of the iterative learning process, we manually construct a tree-annotated corpus to initialize the statistical language model over, and (2) at each step of the parse tree annotation process, we use both syntactic statistics obtained from the iterative learning process and lexical statistics pre-derived from existing language resources, to choose the most probable"
C00-1006,The Effects of Word Order and Segmentation on Translation Retrieval Performance,2000,12,20,2,1,1468,timothy baldwin,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"This research looks at the effects of word order and segmentation on translation retrieval performance for an experimental Japanese-English translation memory system. We implement a number of both bag-of-words and word order-sensitive similarity metrics, and test each over character-based and word-based indexing. The translation retrieval performance of each system configuration is evaluated empirically through the notion of word edit distance between translation candidate outputs and the model translation. Our results indicate that character-based indexing is consistently superior to word-based indexing, suggesting that segmentation is an unnecessary luxury in the given domain. Word order-sensitive approaches are demonstrated to generally outperform bag-of-words methods, with source language segment-level edit distance proving the most effective similarity metric."
W99-0902,The applications of unsupervised learning to {J}apanese grapheme-phoneme alignment,1999,10,8,2,1,1468,timothy baldwin,Unsupervised Learning in Natural Language Processing,0,"In this paper, we adapt the TF-IDF model to the Japanese grapheme-phoneme alignment task, by way of a simple statistical model and an incremental learning method. In the incremental learning method, grapheme-phoneme alignment paradigms are disambiguated one at a t ime according to the relative plausibility of the highest scoring alignment schema, and the statistical model is re-trained accordingly. On limited evaluation, the learning method achieved an accuracy of 93.28%, representing a slight improvement over a baseline rule-based method."
E99-1013,Complementing {W}ord{N}et with {R}oget{'}s and Corpus-based Thesauri for Information Retrieval,1999,23,25,3,0,54967,rila mandala,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,This paper proposes a method to overcome the drawbacks of WordNet when applied to information retrieval by complementing it with Roget's thesaurus and corpus-derived thesauri. Words and relations which are not included in WordNet can be found in the corpus-derived thesauri. Effects of polysemy can be minimized with weighting method considering all query terms and all of the thesauri. Experimental results show that our method enhances information retrieval performance significantly.
1999.tmi-1.20,Argument status in {J}apanese verb sense disambiguation,1999,-1,-1,2,1,1468,timothy baldwin,Proceedings of the 8th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
1999.mtsummit-1.1,What should we do next for {MT} system development?,1999,5,2,1,1,50993,hozumi tanaka,Proceedings of Machine Translation Summit VII,0,"Machine translation (MT) research and development began at the end of 1950xe2x80x99s when not only natural language processing (NLP) technology but also linguistic theory was at a primitive level. Given the restricted memory sizes and computing power at that time, MT presented one of the most difficult and challenging research themes of the day. Thus. MT researchers and developers were forgiven when they complained that their poor translation results were due to shortages in memory and computing power. But now, we cannot say such things. About 40 years have passed since then and computing power and memory capacities have increased dramatically in that time. Many new NLP technologies and linguistic theories have been proposed, based on which MT systems have been developed. Consequently, the scale of MT has grown and many MT s y s t e m s a r e available at affordable prices. However, even though the domain of almost all curren t MT systems is highly restricted, more improvements are necessary, since their translation results are still unsatisfactory. As before, high quality MT remains a difficult and challenging research theme. In the 1990's, information networks have diffused throughout the world, enhancing the importance of MT systems. Through the Internet, we find ourselves surrounded by an enormous amount of documents written in many languages, such that we feel the urgent need for multilingual translation systems. MT technology is now considered a key technology in the field of Internet-based information retrieval. In this paper, after reviewing MT research history focusing on MT technology (not individual MT projects). I discuss what we should do in the next century."
1999.mtsummit-1.80,Sharing syntactic structures,1999,-1,-1,3,0,55042,masahiro ueki,Proceedings of Machine Translation Summit VII,0,"Bracketed corpora are a very useful resource for natural language processing, but hard to build efficiently, leading to quantitative insufficiency for practical use. Disparities in morphological information, such as word segmentation and part-of-speech tag sets, are also troublesome. An application specific to a particular corpus often cannot be applied to another corpus. In this paper, we sketch out a method to build a corpus that has a fixed syntactic structure but varying morphological annotation based on the different tag set schemes utilized. Our system uses a two layered grammar, one layer of which is made up of replaceable tag-set-dependent rules while the other has no such tag set dependency. The input sentences of our system are bracketed corresponding to structural information of corpus. The parser can work using any tag set and grammar, and using the same input bracketing, we obtain corpus that shares partial syntactic structure."
W98-1510,An Empirical Evaluation on Statistical Parsing of {J}apanese Sentences Using Lexical Association Statistics,1998,11,7,4,1,29633,kiyoaki shirai,Proceedings of the Third Conference on Empirical Methods for Natural Language Processing,0,"We are proposing a new framework of statistical language modeling which integrates lexical association statistics with syntactic preference, while maintaining the modularity of those different statistics types, facilitating both training of the model and analysis of its behavior. In this paper, we report the result of an empirical evaluation of our model, where the model is applied to disambiguation of dependency structures of Japanese sentences. We also discussed the room remained for further improvement based on our error analysis."
W98-1227,A Method of Incorporating Bigram Constraints into an {LR} Table and Its Effectiveness in Natural Language Processing,1998,14,0,2,0,55150,hiroki imai,New Methods in Language Processing and Computational Natural Language Learning,0,"In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table. Using a bigram LR table, it is possible for a GLR parser to make use of both bigram and CFG constraints in natural language processing.n n Applying bigram LR tables to our GLR method has the following advantages:n n (1) Language models utilizing bigram LR tables have lower perplexity than simple bigram language models, since local constraints (bigram) and global constraints (CFG) are combined in a single bigram LR table.n n (2) Bigram constraints are easily acquired from a given corpus. Therefore data sparseness is not likely to arise.n n (3) Separation of local and global constraints keeps down the number of CFG rules.n n The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing.n n Our experiments demonstrate the effectiveness of our method."
W98-0704,The Use of {W}ord{N}et in Information Retrieval,1998,10,104,3,0,55196,mandala rila,Usage of {W}ord{N}et in Natural Language Processing Systems,0,None
J98-4002,Selective Sampling for Example-based Word Sense Disambiguation,1998,49,95,4,1,37781,atsushi fujii,Computational Linguistics,0,"This paper proposes an efficient example sampling method for example-based word sense disambiguation systems. To construct a database of practical size, a considerable overhead for manual sense disambiguation (overhead for supervision) is required. In addition, the time complexity of searching a large-sized database poses a considerable problem (overhead for search). To counter these problems, our method selectively samples a smaller-sized effective subset from a given example set for use in word sense disambiguation. Our method is characterized by the reliance on the notion of training utility: the degree to which each example is informative for future example sampling when used for the training of the system. The system progressively collects examples by selecting those with greatest utility. The paper reports the effectiveness of our method through experiments on about one thousand sentences. Compared to experiments with other example sampling methods, our method reduced both the overhead for supervision and the overhead for search, without the degeneration of the performance of the system."
W97-0803,Extending a thesaurus by classifying words,1997,15,16,4,1,301,takenobu tokunaga,Automatic Information Extraction and Building of Lexical Semantic Resources for {NLP} Applications,0,None
O97-1006,Incorporating Bigram Constraints into an {LR} Table,1997,0,0,3,0,55150,hiroki imai,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
1997.mtsummit-papers.13,{MT} {R}{\\&}{D} in {A}sia,1997,-1,-1,1,1,50993,hozumi tanaka,Proceedings of Machine Translation Summit VI: Papers,0,"There is a big shift in MT R{\&}D in this region after many large-scale projects conducted in the past ten years. Multi-lingual Machine Translation (MMT) project is one of the significant R{\&}D projects that increased a great number of NLP related researchers and research activities which can be seen in the increasing number of the research institutes in the recent years. We learned a lot from the collaboration research across languages and we still hope that it will be a rigorous step for the future MT R{\&}D in this region. Though the MT systems are still far from the extreme goal of the perfect translation, it can be observed that the MT systems are actually used to support information retrieval from the Internet."
1997.iwpt-1.16,A New Formalization of Probabilistic {GLR} Parsing,1997,-1,-1,3,0,55795,kentaro unui,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"This paper presents a new formalization of probabilistic GLR language modeling for statistical parsing. Our model inherits its essential features from Briscoe and Carroll{'}s generalized probabilistic LR model, which obtains context-sensitivity by assigning a probability to each LR parsing action according to its left and right context. Briscoe and Carroll{'}s model, however, has a drawback in that it is not formalized in any probabilistically well-founded way, which may degrade its parsing performance. Our formulation overcomes this drawback with a few significant refinements, while maintaining all the advantages of Briscoe and Carroll{'}s modeling."
W96-0105,Selective Sampling of Effective Example Sentence Sets for Word Sense Disambiguation,1996,18,4,4,1,37781,atsushi fujii,Fourth Workshop on Very Large Corpora,0,None
C96-2208,The Automatic Extraction of Open Compounds from Text Corpora,1996,4,11,2,1,33512,virach sornlertlamvanich,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper describes a new method for extracting open compounds (uninterrupted sequences of words) from text corpora of languages, such as Thai, Japanese and Korea that exhibit unexplicit word segmentation. Without applying word segmentation techniques to the inputted plain text, we generate n-gram data from it. We then count the occurence of each string and sort them in alphabetical order. It is significant that the frequency of occurrence of strings decreases when the window size of observation is extended. From the statistical point of view, a word is a string with a fixed pattern that is used repeatedly, meaning that it should occur with a higher frequency than a string that is not a word. We observe the variation of frequency of the sorted n-gram data and extract the strings that experience a significant change in frequency of occurrence when their length is extended. We apply this occurrence test to both the right and left hand sides of all strings to ensure the accurate detection of both boundaries of the string. The method returned satisfying results regardless of the size of the input file."
C96-1012,To what extent does case contribute to verb sense disambiguation?,1996,13,7,4,1,37781,atsushi fujii,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"Word sense disambugation has recently been utillized in corpus-based approaches, reflecting the growth in the number of machine readable texts. One category of approaches disambiguates an input verb sense based on the similarity between its governing case fillers and those in given examples. In this paper, we introduce the degree of contribution of case to verb sense disambiguation into this existing method. In this, greater diversity of semantic range of case filler examples will lead to that case contributing to verb sense disambiguation more. We also report the result of a comparative experiment, in which the performance of disambiguation is improved by considering this notion of semantic contribution."
C94-2139,Analysis of {J}apanese Compound Nouns using Collocational Information,1994,9,22,3,0,56453,yosiyuki kobayasi,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Analyzing compound nouns is one of the crucial issues for natural language processing systems, in particular for those systems that aim at a wide coverage of domains. In this paper, we propose a method to analyze structures of Japanese compound nouns by using both word collocations statistics and a thesaurus. An experiment is conducted with 160,000 word collocations to analyze compound nouns of with an average length of 4.9 characters. The accuracy of this method is about 80%."
C94-2197,A {B}ayesian Approach for User Modeling in Dialogue Systems,1994,9,25,2,0,43140,tomoyosi akiba,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"User modeling is an important components of dialog systems. Most previous approaches are rule-based methods. In this paper, we propose to represent user models through Bayesian networks. Some advantages of the Bayesian approach over the rule-based approach are as follows. First, rules for updating user models are not necessary because updating is directly performed by the evaluation of the network based on probability theory; this provides us a more formal way of dealing with uncertainties. Second, the Bayesian network provides more detailed information of users' knowledge, because the degree of belief on each concept is provided in terms of probability. We prove these advantages through a preliminary experiment."
C92-1062,A Chart-based Method of {ID}/{LP} Parsing with Generalized Discrimination Networks,1992,12,0,3,0,55268,surapant meknavin,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
C90-2053,A New Parallel Algorithm for Generalized {LR} Parsing,1990,8,4,2,0,57651,hiroaki numazaki,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"Tomita's parsing algorithm [Tomita 86], which adapted the LR parsing algorithm to context free grammars, makes use of a breadth-first strategy to handl LR table conflicts. As the breadth-first strategy is compatible with parallel processing, we can easily develop a parallel generalized LR parser based on Tomita's algorithm [Tanaka 89]. However, there is a problem in that this algorithm synchronizes parsing processes on each shift action for the same input word to merge many stacks into Graph Structured Stacks (GSS). In other words, a process that has completed a shift action must wait until all other processes have ended theirs --- a strategy that reduces parallel performance. We have developed a new parallel parsing algorithm that does not need to wait for shift actions before merging many stacks, using stream communication of a concurrent logic programming language called GHC [Ueda 85]. Thus we obtain a parallel generalized LR parser implemented in GHC."
W89-0234,Parallel Generalized {LR} Parsing based on Logic Programming,1989,0,6,1,1,50993,hozumi tanaka,Proceedings of the First International Workshop on Parsing Technologies,0,"A generalized LR parsing algorithm, which has been developed by Tomita [Tomita 86], can treat a context free grammar. His algorithm makes use of breadth first strategy when a conflict occcurs in a LR parsing table. It is well known that the breadth first strategy is suitable for parallel processing. This paper presents an algorithm of a parallel parsing system (PLR) based on a generalized LR parsing. PLR is implemented in GHC [Ueda 85] that is a concurrent logic programming language developed by Japanese 5th generation computer project. The feature of PLR is as follows: Each entry of a LR parsing table is regarded as a process which handles shift and reduce operations. If a process discovers a conflict in a LR parsing table, it activates subprocesses which conduct shift and reduce operations. These subprocesses run in parallel and simulate breadth first strategy. There is no need to make some subprocesses synchronize during parsing. Stack information is sent to each subprocesses from their parent process. A simple experiment for parsing a sentence revealed the fact that PLR runs faster than PAX [Matsumoto 87][Matsumoto 89] that has been known as the best parallel parser."
1989.mtsummit-1.26,Research and development of cooperation project on a machine translation system for {J}apanese and its neighboring countries,1989,-1,-1,1,1,50993,hozumi tanaka,Proceedings of Machine Translation Summit II,0,None
C88-2136,{L}ang{LAB}: A Natural Language Analysis System,1988,11,3,3,1,301,takenobu tokunaga,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper presents a natural language analysis system LangLAB based on BUP-XG which parses with a bottom-up and depth-first strategy and has ability to handle left extraposition. We have already developed a grammar formalism XGS, which is a superset of DCG. With XGS, left extraposition phenomena is naturally expressed in grammar rules. We have also optimized BUP-XG clauses. Experiments showed that in comparison to the original BUP-XG system, the analysis sped up 10 times in the interpreter mode and 4 times in the compiled mode. The TRIE structured dictionary in LangLAB requires less memory, provides faster dictionary reference and also handles complicated idioms with versatility. Consequently, the utilization of LangLAB for practical purposes has become feasible."
C86-1052,{DCKR} {--} Knowledge Representation in {P}rolog and Its Application to Natural Language Processing,1986,6,1,1,1,50993,hozumi tanaka,Coling 1986 Volume 1: The 11th International Conference on Computational Linguistics,0,"Semantic processing is one of the important tasks for natural language processing. Basic to semantic processing is descriptions of lexical items. The most frequently used form of description of lexical items is probably Frames or Objects. Therefore in what form Frames or Objects are expressed is a key issue for natural language processing. A method of the object representation in Prolog called DCKR will be introduced. It will be seen that if part of general knowledge and a dictionary are described in DCKR, part of context processing and the greater part of semantic processing can be left to the functions built in Prolog."
C80-1058,Unit-to-Unit Interaction as a Basis for Semantic Interpretation of {J}apanese Sentences,1980,13,0,1,1,50993,hozumi tanaka,{COLING} 1980 Volume 1: The 8th International Conference on Computational Linguistics,0,"The notion of UNIT-to-UNIT interaction is introduced to analyse dependency relations between words in a sentence. A UNIT is a basic framework for concept representation and is composed of many slots. After generating a parsed tree from an input sentence, our semantic interpretation begins traversing the tree from right to left to discern the case frame in a stage as early as possible, since Japanese is a language in which verb is in the sentence-final and has a case frame. UNIT-to-UNIT interaction, which is performed at each node of the parsed tree, follows a bottom-up progression. There are UNIT descriptions at terminal (bottom) nodes and the UNIT descriptions are modified or merged into other UNITs in the course of the interaction. The results of the interaction will be transferred to upper nodes. The interaction process continues on upward until the top node; at this point, the semantic structure of the input sentence is finally obtained. The notion of UNIT-to-UNIT interaction is feasibly applicable to semantic interpretation of English."
