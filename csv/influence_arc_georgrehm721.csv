2016.tc-1.14,W16-2329,1,0.877361,"Missing"
2016.tc-1.14,L16-1002,0,0.0220002,"Missing"
2016.tc-1.14,P07-2045,0,0.0133392,", if the data such as a multilingual lexicon is stored as a linked data (NIF standard), then SPARQL is a tool to retrieve information from the linked data such as translations in the required target language. 2.4 DBpedia DBpedia7 is a linked open dataset (extracted from Wikipedia) consisting of 4.58 million entities in up to 125 languages and 29.8 million links to external web pages. DBpedia Spotlight8 is an open-source tool for automatically annotating mentions of DBpedia resources in text. Note that the translations may be prone to error on account of being user generated. 2.5 Moses Moses9 (Koehn et al., 2007) is an open-source SMT system used in our experiments as a test bed for Semantic Web-enabled MT. We have employed Phrase-based Statistical Machine Translation system with standard configurations, as specified in Section 4. The translations from the LOD such as DBpedia are inserted in a forced decoding framework, wherein the translation of selected named entities are chosen from DBpedia instead of the Moses decoder. 3 Methodology Having touched upon the basic building blocks for configuring a SMT system with LOD resources in Section 2, we now describe the framework to interface a Moses-based SM"
2016.tc-1.14,W13-5203,0,0.0393125,"Missing"
2016.tc-1.14,P02-1040,0,0.0977351,"Missing"
2016.tc-1.14,2006.amta-papers.25,0,0.076637,"Missing"
2020.coling-main.545,S12-1059,0,0.0829389,"Missing"
2020.coling-main.545,D19-1371,0,0.174252,"BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and Cohan et al. (2020) study domain-specific Transformers for NLP tasks on scientific documents. Moreover, Cohan et al. (2020) are the first to use Transformers to encode titles and abstracts of papers to generate recommendations. Mohamed Hassan et al. (2019) also use BERT for recommendations, but only to encode paper titles. Other recent recommender systems rely on other techniques such as 6195 Fully Connected Classification Layer Seed Transformer Target for Sequence Pair Classification cites in sections Labels Introduction ? ? … Predicted section titles [CLS] Title+Abstract [SEP] Title+Abs"
2020.coling-main.545,Q17-1010,0,0.0271349,"get paper (Figure 2b). This procedure is based on our prior work (Ostendorff et al., 2020). We do not use full-texts in our experiments as many papers are not freely available and the selected Transformer models impose a hard limit of 512 tokens. Baseline LSTM As a baseline, we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997). To derive representations for document pairs, we feed the title and abstract of two papers through the LSTM, where the papers are separated with a special separator token. We use the SpaCy tokenizer (Honnibal and Montani, 2020) and word vectors from fastText (Bojanowski et al., 2017). The word vectors are pretrained on the abstracts of ACL Anthology or CORD-19 datasets. 6197 BERT, Covid-BERT & SciBERT BERT is a neural language model based on the Transformer architecture (Devlin et al., 2019). Commonly, BERT models are pretrained on large text corpora in unsupervised fashion. The two pretraining objectives are the recovery of masked tokens (i. e., mask language modeling) and next sentence prediction (NSP). After pretraining, BERT models are fine-tuned for specific tasks like sentence similarity (Reimers and Gurevych, 2019) or document classification (Ostendorff et al., 201"
2020.coling-main.545,D15-1075,0,0.0154271,"related approach for citation recommendations. They classify sections into discourse facets and build document vectors for each facet. Nevertheless, segmentation is a suboptimal alternative as it breaks the coherence of documents. With pairwise document classification, the similarity is aspect-based without sacrificing the document coherence. Our experiments investigate Transformer language models (Vaswani et al., 2017). BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and Cohan et al. (2020) study domain-specific Transformers for NLP tasks on scientific documents. Moreover, Cohan et al. (2020) are the first to use Transformers to encode t"
2020.coling-main.545,S17-2001,0,0.0209216,"scourse facets and build document vectors for each facet. Nevertheless, segmentation is a suboptimal alternative as it breaks the coherence of documents. With pairwise document classification, the similarity is aspect-based without sacrificing the document coherence. Our experiments investigate Transformer language models (Vaswani et al., 2017). BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and Cohan et al. (2020) study domain-specific Transformers for NLP tasks on scientific documents. Moreover, Cohan et al. (2020) are the first to use Transformers to encode titles and abstracts of papers to generate recommendations. Mohamed Hassan e"
2020.coling-main.545,2020.acl-main.207,0,0.189492,"spect similarity in research papers as aspect-based document similarity. Figure 1 contrasts aspect-based with aspect-free similarity (traditional). Following the research paper example, aspect a1 concerns findings and aspect a2 methods (red and green in Figure 1b). In prior work (Ostendorff et al., 2020), we propose to infer an aspect for document similarity formulating the problem as a multi-class classification of document pairs. We extend our prior work to a multi-label scenario and focus on scientific literature instead of Wikipedia articles. Similar to the work of Jiang et al. (2019) and Cohan et al. (2020), we use citations as training signals. Instead of using citations for binary classification (i. e., similar and dissimilar), we include the title of the section in which a citation occurs, as a label for a document pair. The section titles of citations describe the aspect-based This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 6194 Proceedings of the 28th International Conference on Computational Linguistics, pages 6194–6206 Barcelona, Spain (Online), December 8-13, 2020 a2 a1 a1 a1 a2 Seed Seed"
2020.coling-main.545,N19-1423,0,0.442613,"milar papers for a specific aspect. Huang et al. (2020) apply the same segmentation approach on the CORD-19 corpus (Wang et al., 2020). Kobayashi et al. (2018) follow a related approach for citation recommendations. They classify sections into discourse facets and build document vectors for each facet. Nevertheless, segmentation is a suboptimal alternative as it breaks the coherence of documents. With pairwise document classification, the similarity is aspect-based without sacrificing the document coherence. Our experiments investigate Transformer language models (Vaswani et al., 2017). BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and C"
2020.coling-main.545,J98-1002,0,0.272347,"92) is published before Winterboer and Moore (2007) and, therefore, a citation cannot exist. Nonetheless, the two papers cover a related topic. Thus, one could expect a citation of Polifroni et al. (1992) in Winterboer and Moore (2007) in the introduction section as SciBERT predicted. The model finds this semantic similarity given their latent information on the topic. Example 5-6 present two pairs for which None was correctly predicted according to the ground truth. Agarwal et al. (2011) and Gandhe et al. (2006) from Example 6 are topically unrelated as their titles already suggest. However, Karov and Edelman (1998) and Wang et al. (2012) on Example 5 share the topic of disambiguation. Thus, we would agree with the prediction of a positive label. In summary, the qualitative evaluation does not contradicts the quantitative findings. SciBERT distinguishes documents at a higher level and classifies which aspects makes them similar. In addition to traditional document similarity, the aspect-based predictions allow to asses how two papers relate to each other at a semantic level. For instance, whether two papers are similar in the aspects of Introduction or Experiment is valuable information, especially in li"
2020.coling-main.545,D14-1162,0,0.0874224,"e samples for ACL Anthology and 33,083 for CORD-19. These samples let the models distinguish between similar and dissimilar documents. 3.4 Systems We focus on sequence pair classification with models based on the Transformer architecture (Vaswani et al., 2017). Transformer-based models are often used in text similarity tasks (Jiang et al., 2019; Reimers and Gurevych, 2019). Moreover, Ostendorff et al. (2020) found vanilla Transformers, e. g., BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), outperform Siamese networks (Bromley et al., 1993) and traditional word embeddings (e. g., GloVe (Pennington et al., 2014), Paragraph Vectors (Le and Mikolov, 2014)) in the pairwise document classification task. Hence, we exclude Siamese networks and pretrained word embedding models in our experiments. Instead, we investigate six Transformer variations and an additional baseline for comparison. The titles and abstracts of research paper pairs are used as input for the model, so that the the [SEP] token separates seed and target paper (Figure 2b). This procedure is based on our prior work (Ostendorff et al., 2020). We do not use full-texts in our experiments as many papers are not freely available and the selected"
2020.coling-main.545,H92-1005,0,0.143633,"abel). Both abstracts of example 2 also refer to “mutual information and EM optimization” as their methods. In example 3, Zhang and Clark (2009) and Xi et al. (2012) do not share any citation. Hence, the paper pair is assigned with the None label according to the ground truth data even though they are topically related. Zhang and Clark (2009) and Xi et al. (2012) are both about Chinese machine translation. Still, we disagree with the model’s prediction of Experiment since the two papers conduct different experiments making Experiment an invalid prediction. Example 4’s predictions are correct. Polifroni et al. (1992) is published before Winterboer and Moore (2007) and, therefore, a citation cannot exist. Nonetheless, the two papers cover a related topic. Thus, one could expect a citation of Polifroni et al. (1992) in Winterboer and Moore (2007) in the introduction section as SciBERT predicted. The model finds this semantic similarity given their latent information on the topic. Example 5-6 present two pairs for which None was correctly predicted according to the ground truth. Agarwal et al. (2011) and Gandhe et al. (2006) from Example 6 are topically unrelated as their titles already suggest. However, Kar"
2020.coling-main.545,D17-1035,0,0.016576,"s in addition to mask language modelling the pretraining objective of detecting replaced tokens in the input sequence. For this objective, Clark et al. (2020) use a generator that replaces tokens and a discriminator network that detects the replacements. The generator and discriminator are both Transformer models. ELECTRA does not use the NSP objective. For our experiments, we use the discriminator model of ELECTRA. The pretrained ELECTRA discriminator model is pretrained on the same data as BERT. Hyperparameters & Implementation We choose the LSTM hyperparameters according to the findings of Reimers and Gurevych (2017) as follows: 10 epochs for training, batch size b = 8, learning rate η = 1−5 , two LSTM layers with 100 hidden size, attention, and dropout with probability d = 0.1. While the LSTM baseline uses vanilla PyTorch, all Transformer-based techniques are implemented using the Huggingface API (Wolf et al., 2019). Each Transformer model is used in its BASE version. The hyperparameters for Tranformer fine-tuning are aligned with Devlin et al. (2019): four training epochs, learning rate η = 2−5 , batch size b = 8, and Adam optimizer with ε = 1−8 . We conduct the evaluation in a stratified k-fold cross-v"
2020.coling-main.545,D19-1410,0,0.226856,"build document vectors for each facet. Nevertheless, segmentation is a suboptimal alternative as it breaks the coherence of documents. With pairwise document classification, the similarity is aspect-based without sacrificing the document coherence. Our experiments investigate Transformer language models (Vaswani et al., 2017). BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and Cohan et al. (2020) study domain-specific Transformers for NLP tasks on scientific documents. Moreover, Cohan et al. (2020) are the first to use Transformers to encode titles and abstracts of papers to generate recommendations. Mohamed Hassan et al. (2019) also use BERT fo"
2020.coling-main.545,N18-1101,0,0.0205601,"citation recommendations. They classify sections into discourse facets and build document vectors for each facet. Nevertheless, segmentation is a suboptimal alternative as it breaks the coherence of documents. With pairwise document classification, the similarity is aspect-based without sacrificing the document coherence. Our experiments investigate Transformer language models (Vaswani et al., 2017). BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ELECTRA (Clark et al., 2020) improve many NLP tasks, e. g., natural language inference (Bowman et al., 2015; Williams et al., 2018) and semantic textual similarity (Cer et al., 2017). Reimers and Gurevych (2019) demonstrate how BERT models can be combined in a Siamese network (Bromley et al., 1993) to produce embeddings that can be compared using cosine similarity. Adhikari et al. (2019) and Ostendorff et al. (2019) explore BERT for the classification of single documents with respect to sentiment or topic. Beltagy et al. (2019) and Cohan et al. (2020) study domain-specific Transformers for NLP tasks on scientific documents. Moreover, Cohan et al. (2020) are the first to use Transformers to encode titles and abstracts of p"
2020.coling-main.545,P12-2056,0,0.0126738,"5 - 7 - - - 5 2 - 1 - 1 Table 4: Confusion matrix of selected multi-labels for SciBERT on CORD-19 (N=None, C=Conclusion, O=Other, D=Discussion, I=Introduction, R=Results). For example (in bold), 459 test samples are assigned to Discussion and Introduction (D,I), of which 103 are correctly classified. The remaining samples are mostly classified as single-label, i. e., either Discussion (163) or Introduction (146). abstract (hint for Introduction label). Both abstracts of example 2 also refer to “mutual information and EM optimization” as their methods. In example 3, Zhang and Clark (2009) and Xi et al. (2012) do not share any citation. Hence, the paper pair is assigned with the None label according to the ground truth data even though they are topically related. Zhang and Clark (2009) and Xi et al. (2012) are both about Chinese machine translation. Still, we disagree with the model’s prediction of Experiment since the two papers conduct different experiments making Experiment an invalid prediction. Example 4’s predictions are correct. Polifroni et al. (1992) is published before Winterboer and Moore (2007) and, therefore, a citation cannot exist. Nonetheless, the two papers cover a related topic. T"
2020.coling-main.545,W09-3825,0,0.016318,"- 12 - 14 - D,O,R 23 - - - 5 - 7 - - - 5 2 - 1 - 1 Table 4: Confusion matrix of selected multi-labels for SciBERT on CORD-19 (N=None, C=Conclusion, O=Other, D=Discussion, I=Introduction, R=Results). For example (in bold), 459 test samples are assigned to Discussion and Introduction (D,I), of which 103 are correctly classified. The remaining samples are mostly classified as single-label, i. e., either Discussion (163) or Introduction (146). abstract (hint for Introduction label). Both abstracts of example 2 also refer to “mutual information and EM optimization” as their methods. In example 3, Zhang and Clark (2009) and Xi et al. (2012) do not share any citation. Hence, the paper pair is assigned with the None label according to the ground truth data even though they are topically related. Zhang and Clark (2009) and Xi et al. (2012) are both about Chinese machine translation. Still, we disagree with the model’s prediction of Experiment since the two papers conduct different experiments making Experiment an invalid prediction. Example 4’s predictions are correct. Polifroni et al. (1992) is published before Winterboer and Moore (2007) and, therefore, a citation cannot exist. Nonetheless, the two papers cov"
2020.coling-main.545,P09-2047,0,0.101771,"Missing"
2020.iwltp-1.12,hinrichs-krauwer-2014-clarin,0,0.330529,"se the NLP Interchange Format (NIF) (Hellmann et al., 2013). Using NIF ensures interoperability for different NLP tasks while at the same time addressing storage and scalability needs. Since NIF is based on RDF triples, the resulting annotations can be included in a triple store to allow for efficient storage and querying. In addition, the above-mentioned systems are designed to run on single systems. Our workflow manager is designed to combine output from different micro-services that address different NLP services, potentially running on different machines. In addition to the above, CLARIN (Hinrichs and Krauwer, 2014) provides an infrastructure for natural language research data and tools. The focus, however, is on sharing resources and not on building NLP pipelines or workflows. A more exhaustive and complete overview of related work can be found in (Rehm et al., 2020a). Related Work The orchestration and operationalisation of the processing of large amounts of content through a series of tools has been studied and tested in the field of NLP (and others) from many different angles for decades. There is a sizable amount of tools, systems, frameworks and initiatives that address the issue but their off-the-"
2020.iwltp-1.12,2020.lrec-1.420,1,0.710854,"Missing"
2020.iwltp-1.12,2020.lrec-1.413,1,0.847224,"Missing"
2020.iwltp-1.12,2020.iwltp-1.15,1,\N,Missing
2020.iwltp-1.15,W16-3503,1,0.91547,"PI Manager Workflow Manager Preprocessing Provisioning of Datasets and Content Language Identification Storage Knowledge Graph File Storage Duplicate Detection Document Structure Analysis Semantic Analysis Content Generation Summarization Named Entity Recognition and Linking Paraphrasing Temporal Expression Analysis Machine Translation Relation Extraction Semantic Storytelling Event Detection Security Figure 5: Technical architecture of the QURATOR platform velops a curation technology platform, which is also being populated with services, simplifying and accelerating the curation of content (Bourgonje et al., 2016a; Rehm et al., 2019a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). The project develops, evaluates and integrates services for preprocessing, analyzing and generating content, spanning use cases from the sectors of culture, media, health and industry. To process and transform incoming data, text or multimedia streams into device-adapted, publishable content, various groups of components, services and technologies are applied. These include adapters to data, content and knowledge sources, as well as infrastructural tools and AI methods for the acquisition, analysis and generation of"
2020.iwltp-1.15,2020.lrec-1.696,1,0.735511,"nt annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009). Its field of application included the formalization of annotation schemes and concept-based querying over heterogeneously annotated corpora (Rehm et al., 2008a). As several institutions and resources from various disciplines were involved, no holistic annotation standard could be enforced onto the contributors. 101 3.4. Figure 8: Modular OLiA ontologies 3.2. Level 1: Simple Cross-Platform"
2020.iwltp-1.15,W12-5201,0,0.0160466,"tral hub for linguistic annotation terminology in the web of data. OLiA was designed for mediating between various terminology repositories on the one hand and annotated resources (i. e., their annotation schemes), on the other. Four different types of ontologies are distinguished (Fig. 8): (1) The OLiA Reference Model is an OWL ontology that specifies the common terminology that different annotation schemes can refer to. (2) Multiple OLiA Annotation Models formalize annotation schemes and tagsets. Fig. 8 illustrates this with an annotation model developed as part of the Korean NLP2RDF stack (Hahm et al., 2012). (3) For every annotation model, a linking model defines subclass-relationships between concepts in the annotation model and the reference model. Linking models are interpretations of annotation model concepts and properties in terms of the reference model. (4) Similarly, other community-maintained vocabularies are linked with OLiA, e. g., the CLARIN Concept Registry (Chiarcos et al., 2020). OLiA was developed as part of an infrastructure for the sustainable maintenance of linguistic resources (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Witt et al., 2009; Rehm et al., 2009"
2020.iwltp-1.15,2020.lrec-1.420,1,0.820145,"Missing"
2020.iwltp-1.15,W17-4212,1,0.852914,". g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6: The Lynx technology platform The platform’s microservice architecture is a variant of the service-oriented architecture (SOA), in which an application is structured as a collection of loosely coupled services. It uses Docker containers ho"
2020.iwltp-1.15,2020.iwltp-1.12,1,0.914049,"ments, yet others provide a user interface. The Document Manager provides the storage and annotation of documents with an emphasis on keeping them synchronized, providing read and write access, as well as updates of documents and annotations. It can be queried in terms of annotations and documents, through REST APIs. The interface includes a set of create, read, update, and delete APIs to manage collections, documents and annotations. The orchestration and execution of services involved in more complex tasks is addressed by a Workflow Manager. It defines combinations of services as workflows (Moreno-Schneider et al., 2020b; Bourgonje et al., 2016a; Schneider and Rehm, 2018a; Schneider and Rehm, 2018b). Workflows are described using BPMN and executed using Camunda.8 Interoperability is addressed at the following levels: Since the QURATOR platform is a closed ecosystem, the platform can be thought of as an experimental toolbox with services customised by the partners for their own use cases. As the platform is used only by the QURATOR partners, it does not contain a catalogue or any kind or structured metadata. However, two of the ten QURATOR projects have a focus on service composition and workflows with protot"
2020.iwltp-1.15,2020.lrec-1.284,1,0.886537,"Missing"
2020.iwltp-1.15,piperidis-2012-meta,1,0.875093,"t least upon a certain (obligatory) subset (Labropoulou et al., 2020; McCrae et al., 2015). Such a more detailed, semantics-driven approach enables more efficient and more user-friendly search results from multiple platforms that can be visually aggregated and also easily ranked. The actual search can be performed through publicly available APIs but returned objects would be semantically richer. Alternatively, the metadata records of external repositories can be harvested using standard protocols such as OAI-PMH, which allow the construction of a master index out of decentralised inventories (Piperidis, 2012). A known issue that needs to be addressed using such an approach involves the detection of duplicate resources. Figure 9: A cross-platform workflow example A similar approach was implemented in the project OpenMinTeD (OMTD) (Labropoulou et al., 2018) using the Galaxy workflow management system.10 Three types of LT components are supported: (1) components packaged in Docker images that follow the OMTD specifications; (2) components wrapped with UIMA or GATE, available in a Maven repository; (3) Text and Data Mining web services that run outside the OMTD platform and that follow the OMTD specif"
2020.iwltp-1.15,L18-1519,1,0.396376,"ed in AI4EU Experiments. 2.2. European Language Grid (ELG) Multilingualism and cross-lingual communication in Europe can only be enabled through Language Technologies (LTs) (Rehm et al., 2016). The European LT landscape is fragmented (Vasiljevs et al., 2019), holding back its impact. Another crucial issue is that many languages are underresourced and, thus, in danger of digital extinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014). There is an enormous need for an European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018; Rehm et al., 2020c). The project European Language Grid (2019-2021) attempts to establish the primary platform and marketplace for the European LT community, both industry and research (Rehm et al., 2020a). This scalable cloud platform will provide access to hundreds of LTs for all European languages, including running services as well as data sets. ELG will enable the European LT community to upload their technologies and data sets, to deploy them, and to connect with other resources. ELG caters for commercial and non-commercial LTs (i. e., LTs with a high Technol"
2020.iwltp-1.15,rehm-etal-2008-ontology,1,0.781144,"Missing"
2020.iwltp-1.15,W17-2707,1,0.837412,"road groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data Documents Figure 6:"
2020.iwltp-1.15,2020.lrec-1.413,1,0.802365,"Missing"
2020.iwltp-1.15,2016.tc-1.14,1,0.737947,"n be divided into three broad groups: (1) Preprocessing encompasses services for obtaining and processing information from different content sources so that they can be used in the platform and integrated into other services (Schneider et al., 2018), e. g., provisioning content, language and duplicate detection as well as document structure recognition. (2) Semantic analysis services process a document and add information in the form of annotations, e. g., NER, temporal expression analysis, relation extraction, event detection, fake news as well as discourse analysis (Bourgonje et al., 2016b; Srivastava et al., 2016; Rehm et al., 2017b; Ostendorff et al., 2019). (3) Content generation services enable the creation of a new piece of content, e. g., summarization, paraphrasing, and semantic storytelling (Rehm et al., 2019c; Rehm et al., 2018; Moreno-Schneider et al., 2017; Rehm et al., 2017a; Schneider et al., 2017; Schneider et al., 2016). ENVISIONED SOLUTION Pilot Contracts Pilot GeoThermal Pilot Labor Law Legal Knowledge Graph Workflows Ontologies ... Smart Services Vocabulary ETL linking annotation classification Legal resources Language resources Standards Linked Data Private documents Other open data"
2020.lrec-1.284,boella-etal-2012-nlp,0,0.0397813,"Missing"
2020.lrec-1.284,W16-3503,1,0.889093,"Missing"
2020.lrec-1.284,N16-1030,0,0.215186,"Missing"
2020.lrec-1.284,2020.lrec-1.551,1,0.875513,"Missing"
2020.lrec-1.284,W19-2207,1,0.853778,"Missing"
2020.lrec-1.284,2020.lrec-1.413,1,0.363513,"Missing"
2020.lrec-1.284,P18-2020,0,0.0246612,"Missing"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
2020.lrec-1.413,cassidy-etal-2014-alveo,0,0.0760054,"Missing"
2020.lrec-1.413,W03-0810,0,0.137563,"Missing"
2020.lrec-1.413,gavrilidou-etal-2012-meta,1,0.915791,"Missing"
2020.lrec-1.413,hinrichs-krauwer-2014-clarin,0,0.182909,"Missing"
2020.lrec-1.413,P10-4005,0,0.0528445,"Missing"
2020.lrec-1.413,2020.lrec-1.420,1,0.821941,"Missing"
2020.lrec-1.413,L18-1213,1,0.812955,"0b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 addition"
2020.lrec-1.413,2020.iwltp-1.12,1,0.789706,"ercial services, written in disparate programming languages (Java/Spring, .NET, Python) with just a few days work in the first iteration, falling to a few hours once developers became more familiar with the infrastructure and required formats. 14 These requests are received and handled by the LT Service Execution Server (Section 3.1). 3370 The composition of individual services offered by ELG directly or other cloud platforms is not addressed by ELG itself. However, we experiment with workflow composition and platform interoperability in other contexts (Rehm et al., 2020a; Rehm et al., 2020b; Moreno-Schneider et al., 2020a; Moreno-Schneider et al., 2020b) and plan to integrate experimental workflow functionality into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent"
2020.lrec-1.413,2020.lrec-1.284,1,0.363908,"Missing"
2020.lrec-1.413,piperidis-etal-2014-meta,1,0.89681,"metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will increase the numbers in Table"
2020.lrec-1.413,L18-1205,1,0.945027,"tities of interest to users (Section 3.6), appropriately indexed and described so that they can easily search, find and select the resources that meet their requirements and deploy them, as well as visualise the LT domain activities, stakeholders and resources with specific criteria (e. g., service type, language, etc.). All entities are described in compliance with the ELG-SHARE metadata schema (Labropoulou et al., 2019; Labropoulou et al., 2020).7 The schema builds upon, consolidates and updates previous activities, especially the META-SHARE schema and its profiles (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account the ELG user requirements (Melnika et al., 2019a), recent developments in the (meta)data domain (e. g., FAIR8 , data and software citation recommendations9 , Open Science movement, etc.), and the need for establishing a common pool of resources through exchange mechanisms with collaborating projects and initiatives (Rehm et al., 2020c), cf. Section 3.6. The schema caters for the description of the ELG core entities (Figure 2), i. e., Language Technologies (tools/services), including functional services and nonfunctional ones (e. g., downloadable"
2020.lrec-1.413,piperidis-2012-meta,1,0.90149,"y into ELG. SHARE metadata schemas and a set of resources has already been ingested from each of them into ELG (Table 3). ELRA ELRC-SHARE META-SHARE Corpora Lexicons Models 20 180 52 2 7 12 – – 7 3.5. Data Sets and Language Resources The ELG consortium has defined an LR identification and sharing strategy. It starts by liaising with and capitalizing on existing activities to ingest LRs into the ELG. We have started by focusing on providers who are part of the consortium (ELDA/ELRA and ELG) and on recent activities such as ELRC-SHARE (Lösch et al., 2018; Piperidis et al., 2018) and META-SHARE (Piperidis, 2012; Piperidis et al., 2014). Table 2 provides an overview of what has been identified in these repositories and what is planned to be ingested into ELG, if their access and licensing conditions allow it. Corpora Lexicons Models Total ELRA ELRC-SHARE META-SHARE ELG 848 396 1580 78 1084 132 1261 109 0 0 18 76 1932 528 2859 263 Total 2902 2586 94 5582 Table 2: Identified LRs in the ELG consortium LR modalities covered are text (corpora, lexicons, etc.), speech/audio, video/audiovisual, images/OCR, sign language, and others. About 220 additional repositories have been located so far, which will incr"
2020.lrec-1.413,L18-1519,1,0.634981,"any are world-class, with technologies that outperform the global players. However, European LT business is also fragmented – by nation states, languages, domains and sectors (Vasiljevs et al., 2019) –, significantly holding back its impact. In addition, many European languages are severely under-resourced and, thus, in danger of digital language exinction (Rehm and Uszkoreit, 2012; Kornai, 2013; Rehm et al., 2014; Rehm et al., 2016a), which is why there is an enormous need for a European LT platform as a unifying umbrella (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). The project European Language Grid (ELG; 2019-2021) addresses this fragmentation by establishing the ELG as the primary platform and marketplace for the European LT community, both industry and research.1 The ELG is developed to be a scalable cloud platform, providing, in an easy-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data sets and 1 https://www.european-language-grid.eu resources. Once fully operational, it will enable the commercial and non-commercial E"
2020.lrec-1.413,L16-1251,1,0.856739,"Missing"
2020.lrec-1.413,P14-5010,0,\N,Missing
2020.lrec-1.420,broeder-etal-2012-standardizing,0,0.0591638,"Missing"
2020.lrec-1.420,gavrilidou-etal-2012-meta,1,0.80935,"Missing"
2020.lrec-1.420,jorg-etal-2010-lt,0,0.0940319,"Missing"
2020.lrec-1.420,P15-4017,1,0.844943,"functionalities of the ELG platform as well as for future extensions and collaborations with other platforms(Rehm et al., 2020b). Thus, format and language can be used to match together tools/services with candidate input resources and initiate their processing; for instance, a tool that takes as input PDF files can be matched with datasets in PDF format. Similar information can also be used to semi-automatically compose workflows of tools and/or match together tools with compatible ancillary resources (annotation resources, ontologies, ML models) to create services and end-user applications (Piperidis et al., 2015; Labropoulou et al., 2018). Figure 3 shows a simplified subset of the metadata schema with its structuring layers and optionality status. Figure 3: Simplified subset of the ELG metadata schema 4.3. Describing LT-related entities ELG intends to offer the who is who of actors and projects in LT. Thus, the module for actors and projects, in comparison to META-SHARE profiles, has been enriched. Besides identification and descriptive metadata elements, such as name/title, identifier(s), contact information, etc., of particular importance are features related to and/or promoting LT activities, prod"
2020.lrec-1.420,L18-1205,1,0.686988,"). Its main source is META-SHARE, a wellestablished and widely used schema catering for the description of LRTs in the LT domain, together with its application profiles6 , which adapt the core properties and re6 It should be noted that META-SHARE is also registered in the CLARIN Component Registry (https://catalog.clarin.eu/ ds/ComponentRegistry) and used in the Greek CLARIN (https: //www.clarin.gr/) and various META-SHARE nodes harvested by the Virtual Language Observatory (https://vlo.clarin.eu/). 3429 lations to the needs of specific platforms (Gavrilidou et al., 2012; McCrae et al., 2015; Piperidis et al., 2018; Labropoulou et al., 2018). META-SHARE was based on an extensive study of related metadata schemas and catalogues, focusing mainly on LRTs but also taking into account general trends in the metadata domain (Desipri et al., 2012). In the course of time, its principles and implementation policies have been updated to reflect advancements in the metadata area. In ELG, modifications, updates and extensions in the contents (metadata elements and values) are made in response to user requirements (Melnika et al., 2019a) and new descriptive needs, such as: • integration and deployment of functional s"
2020.lrec-1.420,piperidis-2012-meta,1,0.867004,") are described with mainly bibliographic metadata and, optionally, a category of the LT area to which they belong. Licences and terms of use are described by a set of mainly administrative metadata (e.g., licence name, access URL) and elements facilitating human users to understand the main access conditions (Rodriguez-Doncel and Labropoulou, 2015). The module will also include a set of information for billing requirements of commercial services (currently work in progress). 3432 5. Language Technology Taxonomy For standardization purposes, the ELG schema, in line with META-SHARE principles (Piperidis, 2012), favours controlled vocabularies over free-text fields, especially when these are associated with internationally acknowledged standards, best practices or widespread vocabularies (e.g., ISO 3166 for region codes, RFC 5646 for languages, etc.). Specially devised vocabularies are used for various metadata elements, mainly for features specific to the LT sector. One such prominent case is the LT application area. The ’LT application area’ element is the main linking bridge between all entities in the ELG catalogue. It is used, for instance, to classify LTs by the function/task they perform (’se"
2020.lrec-1.420,L18-1519,1,0.401448,"-to-integrate way, access to hundreds of commercial and non-commercial LTs for all European languages, including running tools and services as well as data resources. Discovery of and access to these resources can only be achieved through an appropriate metadata schema. We present here the ELG-SHARE schema, which is used for the description of LT-related resources shared through the ELG platform and its contribution to the project goals. 2. Objectives The ELG project (Rehm et al., 2020a) aims to foster European LT by addressing the fragmentation that hinders its development; see indicatively (Rehm and Hegele, 2018; Rehm et al., 2016). To this end, it builds a platform dedicated to the distribution and deployment of Language Resources and Technologies (LRT), aspiring to establish it as the primary platform and marketplace for industry-relevant LT in Europe. The promotion of LT stakeholders and activities and growth of their visibility and outreach is also one of its goals. Together with complementary material in the portal (e.g., training material, information on events, job offerings, etc.), ELG offers a comprehensive picture of the European LT sector. The ELG platform5 will offer access to hundreds of"
2020.lrec-1.420,L16-1251,1,0.79915,"Missing"
2020.lrec-1.420,2020.lrec-1.413,1,0.821544,"Missing"
2020.lrec-1.420,2020.iwltp-1.15,1,0.820346,"Missing"
2020.lrec-1.551,benikova-etal-2014-nosta,0,0.0248959,"Missing"
2020.lrec-1.551,W16-4011,0,0.0632103,"Missing"
2020.lrec-1.551,N16-1030,0,0.084437,"Missing"
2020.lrec-1.551,P16-1101,0,0.0772728,"Missing"
2020.lrec-1.551,2020.lrec-1.284,1,0.875513,"Missing"
2020.lrec-1.551,W16-2607,0,0.0140449,"s5 , 107 documents from each court were selected (see Table 1). The data was collected from the XML documents, i. e., it was extracted from the XML elements Mitwirkung, Titelzeile, Leitsatz, Tenor, Tatbestand, Entscheidungsgr¨ unde, Gr¨ unden, abweichende Meinung, and sonstiger Titel. The metadata at the beginning of the documents (name of court, date of decision, file number, European Case Law Identifier, document type, laws) and those that belonged to previous legal proceedings was deleted. Paragraph numbers were removed. The extracted data was split into sentences, tokenised using SoMaJo6 (Proisl and Uhrig, 2016) and manually annotated in WebAnno7 (Eckart de Castilho et al., 2016). The annotated documents are available in CoNNL-2002. The information originally represented by and through the XML markup was lost in the conversion process. We decided to use CoNNL-2002 because our primary focus was on the NER task and experiments. CoNNL is one of the best practice formats for NER datasets. All relevant tools support CoNNL, including WebAnno for manual annotation. Nevertheless, it is possible, of course, to re-insert the annotated information back into the XML documents. 4.2. https://www.rechtsprechung-im-"
2020.lrec-1.551,W17-2707,1,0.890016,"Missing"
2020.lrec-1.551,W19-2207,1,0.764898,"Missing"
2020.lrec-1.551,2020.lrec-1.413,1,0.826993,"Missing"
2020.lrec-1.551,W03-0419,0,0.539033,"me expressions, and text structure. One of the fundamental processing tasks is the identification and categorisation of named entities (Named Entity Recognition, NER). Typically, NER is focused upon the identification of semantic categories such as person, location and organization but, especially in domain-specific applications, other typologies have been developed that correspond to task-, language- or domainspecific needs. With regard to the legal domain, the lack of freely available datasets has been a stumbling block for text analytics research. German newspaper datasets from CoNNL 2003 (Sang and Meulder, 2003) or GermEval 2014 (Benikova et al., 2014) are simply not suitable in terms of domain, text type or semantic categories covered. The work described in this paper was carried out under the umbrella of the project Lynx: Building the Legal Knowledge Graph for Smart Compliance Services in Multilingual Europe, a three-year EU-funded project that started in December 2017 (Montiel-Ponsoda et al., 2017).1 Its objective is the creation of a legal knowledge graph that contains different types of legal and regulatory data (Schneider and Rehm, 2018a; Schneider and Rehm, 2018b; MorenoSchneider et al., 2020)"
2020.lrec-1.551,E99-1023,0,0.629221,"Missing"
2020.lrec-1.551,W02-2024,0,0.393646,"Missing"
2020.lrec-1.551,S10-1010,0,\N,Missing
2020.lrec-1.553,D18-1307,0,0.130494,"icant increase of the optimization metric is achieved in three consecutive epochs. 3.3. Multi-Task Learning Multi-Task Learning (MTL) (Ruder, 2017) has become popular with the progress in deep learning. This model family is characterized by simultaneous optimization of multiple loss functions and transfer of knowledge achieved this way. The knowledge is transferred through the use of one or multiple shared layers. Through finding supporting patterns in related tasks, MTL provides better generalization on unseen cases and the main tasks we are trying to solve. We rely on the model presented by Bekoulis et al. (2018b) and reuse the implementation provided by the authors.11 The model jointly trains two objectives supported by the dataset: the main task of NER and a supporting task of Relation Extraction (RE). Two separate models are developed for each of the tasks. The NER task is solved with the help of a BiLSTM-CRF model, similar to the one presented in Section 3.2. The RE task is solved by using a multi-head selection approach, where each token can have none or more relationships to in-sentence tokens. Additionally, this model also leverages the output of the NER branch model (the CRF prediction) to le"
2020.lrec-1.553,N19-1423,0,0.0169117,"multihead_joint_entity_relation_ extraction functions across layers. Although it is possible to use adversarial training (Bekoulis et al., 2018a), we omit from using it. We also omit the publication of results for the task of RE as we consider it to be a supporting task and no other competing approaches have been developed. 3.4. BioBERT Deep neural language models have recently evolved to a successful method for representing text. In particular, Bidirectional Encoder Representations from Transformers (BERT) outperformed previous state-of-the-art methods by a large margin on various NLP tasks (Devlin et al., 2019). For our experiments, we use BioBERT, an adaptation of BERT for the biomedical domain, pre-trained on PubMed abstracts and PMC full-text articles (Lee et al., 2019). The BERT architecture12 for deriving text representations uses 12 hidden layers, consisting of 768 units each. For NER, token level BIO-tag probabilities are computed with a single output layer based on the representations from the last layer of BERT. We fine-tune the model on the entity recognition task during four training epochs with batch size b = 32, dropout probability d = 0.1 and learning rate η = 2−5 . These hyper-paramet"
2020.lrec-1.553,W16-4011,0,0.070364,"Missing"
2020.lrec-1.553,W19-5029,0,0.0301167,"ce are medical entities. Only very few annotated corpora in the medical domain exist. Many of them focus on the relation between chemicals and diseases or proteins and diseases, such as the BC5CDR corpus (Li et al., 2016), the Comparative Toxicogenomics Database1 (Davis et al., 2019), the FSU PRotein GEne corpus2 (Hahn et al., 2010) or the ADE (adverse drug effect) corpus (Gurulingappa et al., 2012). The NCBI Disease Corpus (Doğan et al., 2014) contains condition mention annotations along with annotations of symptoms. Several new corpora of annotated case reports were made available recently. Grouin et al. (2019) presented a corpus with medical entity annotations of clinical cases written in French, Ju et al. (2019) presented a corpus focusing on phenotypic information for chronic obstructive pulmonary disease while Smalheiser et al. (2019) presented a corpus focusing on identifying main finding sentences in case reports. The corpus most comparable to ours is the French corpus of clinical case reports by Grouin et al. (2019). Their annotations are based on UMLS semantic types. Even though there is an overlap in annotated entities, semantic classes are not the same. Lab results are subsumed under findi"
2020.lrec-1.553,W10-1838,0,0.0144154,"ortant role in the recent development of the digital health area. To enable dedicated Natural Language Processing (NLP) that is highly accurate with respect to medically relevant categories, manually annotated data from this domain is needed. One category of high interest and relevance are medical entities. Only very few annotated corpora in the medical domain exist. Many of them focus on the relation between chemicals and diseases or proteins and diseases, such as the BC5CDR corpus (Li et al., 2016), the Comparative Toxicogenomics Database1 (Davis et al., 2019), the FSU PRotein GEne corpus2 (Hahn et al., 2010) or the ADE (adverse drug effect) corpus (Gurulingappa et al., 2012). The NCBI Disease Corpus (Doğan et al., 2014) contains condition mention annotations along with annotations of symptoms. Several new corpora of annotated case reports were made available recently. Grouin et al. (2019) presented a corpus with medical entity annotations of clinical cases written in French, Ju et al. (2019) presented a corpus focusing on phenotypic information for chronic obstructive pulmonary disease while Smalheiser et al. (2019) presented a corpus focusing on identifying main finding sentences in case reports"
2020.lrec-1.553,W19-5034,0,0.0680948,"Missing"
2020.lrec-1.825,P18-1063,0,0.057266,"Missing"
2020.lrec-1.825,P19-1264,0,0.0348763,"Missing"
2020.lrec-1.825,L16-1130,0,0.108516,"bstractive summarization remains challenging. The paraphrasing capabilities of all state-of-the-art systems are low and the models are not guaranteed to produce summaries which follow the initial order of the sequence of events. 7. Discussion: Summarization Evaluation ROUGE (Lin, 2004) is the most widely adopted metric used for evaluating automatic text summarization approaches. The evaluation is made though comparison of a set of system-generated candidate summaries with a gold standard summary. The availability of the corresponding software and its performance contributed to its popularity (Cohan and Goharian, 2016). Despite its adoption in many studies, the metric faced some key criticisms. The main criticism of ROUGE is that it does not take into account the meaning expressed in the sequences. The metric was developed based on the assumption that a high quality generated candidate summary should share many words with a single human-made gold standard summary. This assumption may be very relevant to extractive, but not to abstractive summarization, where different terminology and paraphrasing can be used to express the same meaning (Cohan and Goharian, 2016). This results in the metric assigning low sco"
2020.lrec-1.825,N18-2097,0,0.019341,"he size of the input sequence. Zhang et al. (2019) incorporate BERT into the Transformer-based model. They use a two-stage procedure exploiting the mask learning strategy. Others attempt to improve their abstractive summarization models by incorporating an extractive model. For example, Li et al. (2018) use the Key information guide network to guide the summary generation process. In Bottom-up summarization (Gehrmann et al., 2018) the extractive model is used to increase the precision of the Pointer Generator mechanism. Another strand of research adapts existing models to cope with long text. Cohan et al. (2018) present the DiscourseAware Attention model which introduces hierarchy in the attention mechanism via calculating an additional attention vector over the sections of the input text. Subramanian et al. (2019) showed that the language model trained on the combination of the original text, extractive summaries generated by the model and the golden summary can achieve results comparable to standard encoder-decoder based summarization models. 3. Approach Our text summarization model is based on the Transformer architecture. This architecture adopts the original model of Vaswani et al. (2017). On to"
2020.lrec-1.825,C08-1019,0,0.0219788,"are highlighted in green and a typical abstractive feature, illustrating re-arranging of the sentence is highlighted in blue. maries having very high ROUGE scores. Sj¨obergh (2007) show how this can be achieved by choosing the most frequent bigrams from the input document. ROUGE adoption relies on its correlation with human assessment. In the first research on the DUC and TDT-3 datasets containing news articles, ROUGE indeed showed a high correlation with the human judgments (Lin, 2004; Dorr et al., 2005). However, more recent research questions the suitability of ROUGE for various settings. Conroy and Dang (2008) show that on DUC data the linguistic and responsiveness scores of some systems do not correspond to the high ROUGE scores. Cohan and Goharian (2016) demonstrate that for summarization of scientific texts, ROUGE-1 and ROUGE-L have very low correlations with the gold summaries. ROUGE-N correlates better but is still far from the ideal case. This follows the result of Murray et al. (2005), showing that the unigram match between the candidate summary and gold summary is not an accurate metric to assess quality. Another problem is that the credibility of ROUGE was demonstrated for the systems whic"
2020.lrec-1.825,N19-1423,0,0.582395,"ome other technique without a significant drop in performance (Domhan, 2018; Wu et al., 2019). Following this strategy, we develop a model that introduces convolution into the vanilla Self-Attention, allowing to better encode the local dependencies between tokens. To overcome the data sparsity problem, we use a pre-trained language model for the encoding part of the encoder-decoder setup, which creates a contextualized representation of the input sequence. Specifically, we use BERT due to its bi-directional context conditioning, multilingualism and state-of-the-art scores on many other tasks (Devlin et al., 2019). Furthermore, we propose a new method which allows applying BERT on longer texts. The main contributions of this paper are: (1) Designing two new abstractive text summarization models based on the ideas of conditioning on the pre-trained language model and application of convolutional self-attention at the bottom layers of the encoder. (2) Proposing a method of encoding the input sequence in windows which alleviates BERT’s input limitations1 and allows the processing of longer input texts. (3) Evaluating the performance of our models on the English and German language by conducting an ablatio"
2020.lrec-1.825,P18-1167,0,0.0312724,"ng previous iterations). The last layer of a decoder, the generator, maps hidden states to token probabilities. We use a state-of-the-art Transformer for sequence-to-sequence tasks which is built primarily on the attention mechanism (Vaswani et al., 2017). We attempt to improve performance of abstractive text summarization by improving the language encoding capabilities of the model. Recent results have shown that the main contribution of the Transformer is its multi-layer architecture, allowing Self-Attention to be replaced with some other technique without a significant drop in performance (Domhan, 2018; Wu et al., 2019). Following this strategy, we develop a model that introduces convolution into the vanilla Self-Attention, allowing to better encode the local dependencies between tokens. To overcome the data sparsity problem, we use a pre-trained language model for the encoding part of the encoder-decoder setup, which creates a contextualized representation of the input sequence. Specifically, we use BERT due to its bi-directional context conditioning, multilingualism and state-of-the-art scores on many other tasks (Devlin et al., 2019). Furthermore, we propose a new method which allows app"
2020.lrec-1.825,W05-0901,0,0.065307,"Comparison of the output of models on an example form CNN/Daily Mail testset. Surface realisation mistakes are highlighted in green and a typical abstractive feature, illustrating re-arranging of the sentence is highlighted in blue. maries having very high ROUGE scores. Sj¨obergh (2007) show how this can be achieved by choosing the most frequent bigrams from the input document. ROUGE adoption relies on its correlation with human assessment. In the first research on the DUC and TDT-3 datasets containing news articles, ROUGE indeed showed a high correlation with the human judgments (Lin, 2004; Dorr et al., 2005). However, more recent research questions the suitability of ROUGE for various settings. Conroy and Dang (2008) show that on DUC data the linguistic and responsiveness scores of some systems do not correspond to the high ROUGE scores. Cohan and Goharian (2016) demonstrate that for summarization of scientific texts, ROUGE-1 and ROUGE-L have very low correlations with the gold summaries. ROUGE-N correlates better but is still far from the ideal case. This follows the result of Murray et al. (2005), showing that the unigram match between the candidate summary and gold summary is not an accurate m"
2020.lrec-1.825,D18-1443,0,0.163835,"e, were the first to use the Transformer model for summarization. It was only used in the decoder on top of the extraction model with various attention compression techniques to increase the size of the input sequence. Zhang et al. (2019) incorporate BERT into the Transformer-based model. They use a two-stage procedure exploiting the mask learning strategy. Others attempt to improve their abstractive summarization models by incorporating an extractive model. For example, Li et al. (2018) use the Key information guide network to guide the summary generation process. In Bottom-up summarization (Gehrmann et al., 2018) the extractive model is used to increase the precision of the Pointer Generator mechanism. Another strand of research adapts existing models to cope with long text. Cohan et al. (2018) present the DiscourseAware Attention model which introduces hierarchy in the attention mechanism via calculating an additional attention vector over the sections of the input text. Subramanian et al. (2019) showed that the language model trained on the combination of the original text, extractive summaries generated by the model and the golden summary can achieve results comparable to standard encoder-decoder b"
2020.lrec-1.825,N18-2009,0,0.0203659,"ropy optimization achieved high scores eliminating the unreliability problem. Liu et al. (2018), to the best of our knowledge, were the first to use the Transformer model for summarization. It was only used in the decoder on top of the extraction model with various attention compression techniques to increase the size of the input sequence. Zhang et al. (2019) incorporate BERT into the Transformer-based model. They use a two-stage procedure exploiting the mask learning strategy. Others attempt to improve their abstractive summarization models by incorporating an extractive model. For example, Li et al. (2018) use the Key information guide network to guide the summary generation process. In Bottom-up summarization (Gehrmann et al., 2018) the extractive model is used to increase the precision of the Pointer Generator mechanism. Another strand of research adapts existing models to cope with long text. Cohan et al. (2018) present the DiscourseAware Attention model which introduces hierarchy in the attention mechanism via calculating an additional attention vector over the sections of the input text. Subramanian et al. (2019) showed that the language model trained on the combination of the original tex"
2020.lrec-1.825,W04-1013,0,0.0422183,"l shows some level of abstractiveness merging parts of the two sentences into the single one (in the second summary’s sentence). This is far from the gold summary where every sentence in some way paraphrases the original text. Hence, given this particular example, our models demonstrate some explicit improvements. Still, abstractive summarization remains challenging. The paraphrasing capabilities of all state-of-the-art systems are low and the models are not guaranteed to produce summaries which follow the initial order of the sequence of events. 7. Discussion: Summarization Evaluation ROUGE (Lin, 2004) is the most widely adopted metric used for evaluating automatic text summarization approaches. The evaluation is made though comparison of a set of system-generated candidate summaries with a gold standard summary. The availability of the corresponding software and its performance contributed to its popularity (Cohan and Goharian, 2016). Despite its adoption in many studies, the metric faced some key criticisms. The main criticism of ROUGE is that it does not take into account the meaning expressed in the sequences. The metric was developed based on the assumption that a high quality generate"
2020.lrec-1.825,D19-1387,0,0.206685,"lly, non-contextualized embedding vectors were used for pre-training neural-based NLP models (Mikolov et al., 2013; Pennington et al., 2014). Recently, pretrained language models exploiting contextualized embeddings, such as ELMo, GPT-2, BERT and XLNet raised the bar in many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Recent attempts to use these models for text summarization demon1 6680 BERT can process sequences with a maximum of 512 tokens. strated their suitability by achieving new state-of-the-art results (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019). 2.2. Neural Abstractive Text Summarization The neural approach toward abstractive summarization was largely adopted by state-of-the-art models (Shi et al., 2018). A significant contribution was the pointer Generator Network (See et al., 2017). It uses a special layer on top of the decoder network to be able to both generate tokens from the dictionary and extract them from the input text. It uses the coverage vector mechanism to pay less attention to tokens already covered by previous iterations. An example of earlier work adapting Reinforcement Learning (RL) is described by Paulus et al. (20"
2020.lrec-1.825,2020.lrec-1.284,1,0.860836,"Missing"
2020.lrec-1.825,K16-1028,0,0.0584491,"Missing"
2020.lrec-1.825,D14-1162,0,0.0950542,"Missing"
2020.lrec-1.825,N18-1202,0,0.0179713,"nd allows the processing of longer input texts. (3) Evaluating the performance of our models on the English and German language by conducting an ablation study on CNN/Dail Mail and SwissText datasets and comparing it with other state-of-the-art methods. 2. 2.1. Related Work Pre-trained Language Models Traditionally, non-contextualized embedding vectors were used for pre-training neural-based NLP models (Mikolov et al., 2013; Pennington et al., 2014). Recently, pretrained language models exploiting contextualized embeddings, such as ELMo, GPT-2, BERT and XLNet raised the bar in many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Recent attempts to use these models for text summarization demon1 6680 BERT can process sequences with a maximum of 512 tokens. strated their suitability by achieving new state-of-the-art results (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019). 2.2. Neural Abstractive Text Summarization The neural approach toward abstractive summarization was largely adopted by state-of-the-art models (Shi et al., 2018). A significant contribution was the pointer Generator Network (See et al., 2017). It uses a special layer on top of the d"
2020.lrec-1.825,P19-1101,0,0.0257936,"c and responsiveness scores of some systems do not correspond to the high ROUGE scores. Cohan and Goharian (2016) demonstrate that for summarization of scientific texts, ROUGE-1 and ROUGE-L have very low correlations with the gold summaries. ROUGE-N correlates better but is still far from the ideal case. This follows the result of Murray et al. (2005), showing that the unigram match between the candidate summary and gold summary is not an accurate metric to assess quality. Another problem is that the credibility of ROUGE was demonstrated for the systems which operated in the lowscoring range. Peyrard (2019b) show that different summarization evaluation metrics correlate differently with human judgements for the higher-scoring range in which state-ofthe-art systems now operate. Furthermore, improvements measured with one metric do not necessarily lead to improvements when using others. This concern led to the development of new evaluation metrics. Peyrard (2019a) define metrics for important concepts with regard to summariazion: Redundancy, Relevance, and Informativeness in line with Shannon’s entropy. From these definitions they formulate a metric of Importance which better correlates to human"
2020.lrec-1.825,P19-1502,0,0.0162513,"c and responsiveness scores of some systems do not correspond to the high ROUGE scores. Cohan and Goharian (2016) demonstrate that for summarization of scientific texts, ROUGE-1 and ROUGE-L have very low correlations with the gold summaries. ROUGE-N correlates better but is still far from the ideal case. This follows the result of Murray et al. (2005), showing that the unigram match between the candidate summary and gold summary is not an accurate metric to assess quality. Another problem is that the credibility of ROUGE was demonstrated for the systems which operated in the lowscoring range. Peyrard (2019b) show that different summarization evaluation metrics correlate differently with human judgements for the higher-scoring range in which state-ofthe-art systems now operate. Furthermore, improvements measured with one metric do not necessarily lead to improvements when using others. This concern led to the development of new evaluation metrics. Peyrard (2019a) define metrics for important concepts with regard to summariazion: Redundancy, Relevance, and Informativeness in line with Shannon’s entropy. From these definitions they formulate a metric of Importance which better correlates to human"
2020.lrec-1.825,W18-5431,0,0.142479,"Pcopy (w) is the probability of copying a specific word w from the source document, Psof tmax (w) is the probability of generation a word calculated by the abstractive summarization model and pgen is the probability of copying instead of generation. 3.1. Convolutional Self-Attention The Transformer, like any other self-attention network, has a hierarchical multi-layer architecture. In many experiFigure 1: Model overview ments it was shown that this architecture tends to learn lexical information in the first layers, sentence-level patterns in the middle and the semantics in the upper layers (Raganato and Tiedemann, 2018; Tenney et al., 2019). The disadvantage of this approach is that during the attention operation it considers all tokens as equally important, whereas syntactic information is mostly concentrated in certain local areas. This problem is usually specified as the problem of locality modeling. As syntactic information can help in identifying more important words or phrases, it could be beneficial to focus attention on these regions. A successful approach to the locality modeling task are the so-called convolutions (local) self-attention networks (Yang et al., 2019a). Essentially, the problem is de"
2020.lrec-1.825,2020.lrec-1.413,1,0.708211,"Missing"
2020.lrec-1.825,P17-1099,0,0.6336,"LNet raised the bar in many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Recent attempts to use these models for text summarization demon1 6680 BERT can process sequences with a maximum of 512 tokens. strated their suitability by achieving new state-of-the-art results (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019). 2.2. Neural Abstractive Text Summarization The neural approach toward abstractive summarization was largely adopted by state-of-the-art models (Shi et al., 2018). A significant contribution was the pointer Generator Network (See et al., 2017). It uses a special layer on top of the decoder network to be able to both generate tokens from the dictionary and extract them from the input text. It uses the coverage vector mechanism to pay less attention to tokens already covered by previous iterations. An example of earlier work adapting Reinforcement Learning (RL) is described by Paulus et al. (2018). The pure RL model achieved high ROUGE-1 and ROUGE-L scores but produced unreadable summaries. Its combination with typical cross-entropy optimization achieved high scores eliminating the unreliability problem. Liu et al. (2018), to the bes"
2020.lrec-1.825,P19-1452,0,0.0225774,"of copying a specific word w from the source document, Psof tmax (w) is the probability of generation a word calculated by the abstractive summarization model and pgen is the probability of copying instead of generation. 3.1. Convolutional Self-Attention The Transformer, like any other self-attention network, has a hierarchical multi-layer architecture. In many experiFigure 1: Model overview ments it was shown that this architecture tends to learn lexical information in the first layers, sentence-level patterns in the middle and the semantics in the upper layers (Raganato and Tiedemann, 2018; Tenney et al., 2019). The disadvantage of this approach is that during the attention operation it considers all tokens as equally important, whereas syntactic information is mostly concentrated in certain local areas. This problem is usually specified as the problem of locality modeling. As syntactic information can help in identifying more important words or phrases, it could be beneficial to focus attention on these regions. A successful approach to the locality modeling task are the so-called convolutions (local) self-attention networks (Yang et al., 2019a). Essentially, the problem is dealt with by the applic"
2020.lrec-1.825,N19-1407,0,0.360148,"he performance of our models on the English and German language by conducting an ablation study on CNN/Dail Mail and SwissText datasets and comparing it with other state-of-the-art methods. 2. 2.1. Related Work Pre-trained Language Models Traditionally, non-contextualized embedding vectors were used for pre-training neural-based NLP models (Mikolov et al., 2013; Pennington et al., 2014). Recently, pretrained language models exploiting contextualized embeddings, such as ELMo, GPT-2, BERT and XLNet raised the bar in many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Recent attempts to use these models for text summarization demon1 6680 BERT can process sequences with a maximum of 512 tokens. strated their suitability by achieving new state-of-the-art results (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019). 2.2. Neural Abstractive Text Summarization The neural approach toward abstractive summarization was largely adopted by state-of-the-art models (Shi et al., 2018). A significant contribution was the pointer Generator Network (See et al., 2017). It uses a special layer on top of the decoder network to be able to both generate tokens from the dic"
2020.lrec-1.825,K19-1074,0,0.203327,"ined Language Models Traditionally, non-contextualized embedding vectors were used for pre-training neural-based NLP models (Mikolov et al., 2013; Pennington et al., 2014). Recently, pretrained language models exploiting contextualized embeddings, such as ELMo, GPT-2, BERT and XLNet raised the bar in many NLP tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). Recent attempts to use these models for text summarization demon1 6680 BERT can process sequences with a maximum of 512 tokens. strated their suitability by achieving new state-of-the-art results (Zhang et al., 2019; Liu, 2019; Liu and Lapata, 2019). 2.2. Neural Abstractive Text Summarization The neural approach toward abstractive summarization was largely adopted by state-of-the-art models (Shi et al., 2018). A significant contribution was the pointer Generator Network (See et al., 2017). It uses a special layer on top of the decoder network to be able to both generate tokens from the dictionary and extract them from the input text. It uses the coverage vector mechanism to pay less attention to tokens already covered by previous iterations. An example of earlier work adapting Reinforcement Learning (RL)"
2021.eacl-demos.26,hinrichs-krauwer-2014-clarin,0,0.0190659,"arts in February 2021 with a duration of 912 months. In the first call, 110 proposals were accepted for evaluation with applicants from 29 countries. We received more proposals from SMEs (62) than re search organisations (48). While 79 proposals fo 2.10 Legal Entity 3 Related Work ELG builds upon previous work of the ELG con sortium and the wider European LT community, es pecially METANET/META and ELRC. In addition, we have collected more than 30 plat forms, projects or initiatives that can be considered relevant for ELG including, among others, UIMA (Ferrucci and Lally, 2003), CLARIN (Hinrichs and Krauwer, 2014), DKPro (Gurevych et al., 2007); Rehm et al. (2020a) provide an exhaustive com parison. They share at least one of the following goals with ELG, i. e., they provide: 1) a collec tion of LT/NLP tools or data sets; 2) a platform, which harvests metadata records from distributed sources, 3) a platform for the sharing of tools or data sets. While related projects do exist, the ap proach of ELG is unique. The platform that most closely resembles ELG is the National Platform for LT, operated by the Ministry of Electronics and In formation Technology in India.16 Several global technology enterpri"
2021.eacl-demos.26,2020.iwltp-1.12,1,0.888601,"Missing"
2021.eacl-demos.26,piperidis-2012-meta,1,0.736597,"four TTS and two text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) Gen"
2021.eacl-demos.26,L18-1205,1,0.930475,"ps://www.postgresql.org 8 https://www.keycloak.org 9 https://prometheus.io 10 https://helm.sh 5 2.3 Catalogue The metadata records stored in the catalogue en able access to services and data resources. They are described using the ELG metadata schema (Labropoulou et al., 2020) and can be browsed and explored. The catalogue also includes a registry of stakeholders who develop LT services or products, and relevant projects, thus providing an overview of the whole European LT landscape. The ELG metadata schema builds upon, consolidates and updates the METASHARE schema (Gavrilidou et al., 2012; Piperidis et al., 2018; Labropoulou et al., 2018), taking into account ELG’s require ments, recent developments in the metadata do main (e. g., FAIR11 ), and the need for creating a common pool of resources through exchange mechanisms with collaborating initiatives. The metadata schema caters for the descrip tion of the ELG core entities, i. e., Language Technologies (tools/services), including functional services and nonfunctional ones, and Data Lan guage Resources, comprising data sets (corpora), language descriptions (i. e., models) and lexical/ conceptual resources (e. g., gazetteers, ontologies, etc.). I"
2021.eacl-demos.26,piperidis-etal-2014-meta,1,0.869104,"text categori sation services. Further services are being added on a regular basis with 200+ additional IE and text analysis services, 21 MT, eight ASR and nine TTS scheduled to be included by the time of ELG Re lease 2 in February 2021. We aim to make it as simple as possible for LT providers to integrate their services, but in a Already now ELG provides access to more than 2700 language resources. We ingested substan tial resources from existing repositories, especially ELDA/ELRA, ELRCSHARE (Lösch et al., 2018; Piperidis et al., 2018; Smal et al., 2020) and META SHARE (Piperidis, 2012; Piperidis et al., 2014). We have also been working on ‘external’ reposito ries, about 220 of which have been identified so far. Some (e. g., Zenodo, Quantum Stat) are al ready being ingested together with two reposito ries related to ELG, LINDAT/CLARIAHCZ and ELRASHARELRs (LRs published at LREC). 2.6 Access Methods and User Interfaces Our main groups of users are: (1) LT/LR providers – companies or research organisations with tools, services or data that can be provided through the ELG; (2) Developers and integrators – companies and research institutions interested in using LT; (3) General LT information seeke"
2021.eacl-demos.26,L16-1251,1,0.869271,"Missing"
2021.eacl-demos.26,L18-1519,1,0.849444,"et al., 2019; Rehm et al., 2020d). We describe Release 2 of the European Lan guage Grid (ELG) cloud platform.1 This scal able system is targeted to evolve into the primary 1 https://www.europeanlanguagegrid.eu. We provide a screencast demo video at https://youtu.be/LD6QadkkZiM. platform for LT in Europe. It will provide one umbrella platform for all LTs developed by the European LT landscape, including research and industry, addressing a gap that has been repeat edly raised by the European LT community for many years (Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). ELG is meant to be a virtual home and marketplace for all products, services and organisations active in the LT space in Europe (Rehm et al., 2020a). The platform can be used by all stakeholders to show case, share and distribute their products, services, tools and resources. At the end of the EU project ELG (20192022), which will establish a legal en tity in early 2022, the platform will provide access to approx. 1300 commercial and noncommercial tools and services for all European languages, as well as thousands of language resources (LRs). ELG will enable t"
2021.eacl-demos.26,2020.lrec-1.422,0,0.0498058,"Missing"
2021.germeval-1.4,W19-3506,0,0.0491751,"Missing"
2021.germeval-1.4,2020.coling-main.598,0,0.0627842,"Missing"
2021.germeval-1.4,P19-1271,0,0.054239,"Missing"
2021.germeval-1.4,2020.acl-main.747,0,0.0944976,"Missing"
2021.germeval-1.4,2020.tacl-1.47,0,0.0542861,"Missing"
2021.germeval-1.4,N19-1423,0,0.0278763,"when the input label corresponds to a non-toxic comment and “toxic” when the input label is toxic. We also add the task prefix “speech review” at the beginning of each input sequence. As T5, mT5 exists in five sizes: Small, Base, Large, XL, XXL. The XXL version of mT5 performs better than other multilingual models such as XLM-RoBERTa on many multilingual benchmarks, however, due to computational limits, we use the mT5 Base version that produces results comparable to XLM-RoBERTa (Xue et al., 2021). GBERT GBERT (Chan et al., 2020) is a German language model using the same architecture as BERT (Devlin et al., 2019). GBERT is an encoderonly Transformer model. It was trained using masked language modeling with whole word masking which corresponds to masking all of the tokens corresponding to a word. The pre-training corpus consists of German texts from Wikipedia, Common Crawl (Ortiz Su´arez et al., 2019), OPUS (Tiedemann, 2012), and Open Legal Data (Ostendorff et al., 2020). GBERT outperforms the state-of-theart for the GermEval 2018 hatespeech detection task and the GermEval 2014 NER task (Chan et al., 2020). We use the GBERT Base version. 3.3 Training scenarios To evaluate the benefit of the task-specif"
2021.germeval-1.4,W19-3512,0,0.0389871,"Missing"
2021.germeval-1.4,gao-huang-2017-detecting,0,0.0277878,"Missing"
2021.germeval-1.4,W18-5102,0,0.0636587,"Missing"
2021.germeval-1.4,2020.emnlp-demos.6,0,0.081646,"Missing"
2021.germeval-1.4,D19-1474,0,0.061608,"Missing"
2021.germeval-1.4,2021.naacl-main.41,0,0.0256987,"nted dataset 4,640 648 0.40 31 13 34 Table 2: Comparison of the original shared task dataset, the dataset created using data augmentation, and the augmented dataset, i. e., the combination of the other two datasets. 3.2 Models (Conneau et al., 2020). We use the Base version of XLM-RoBERTa. The task-specific pre-training is based on a multilingual dataset (Section 2.1). We picked two multilingual Transformer models, XLM-RoBERTa and mT5. In addition, we compare multilingual models with the German Transformer based language model GBERT that we evaluate with our data augmentation method. mT5 mT5 (Xue et al., 2021) is a multilingual variant of T5 (Raffel et al., 2020) covering 101 languages. It uses the same architecture as T5, an encoder-decoder Transformer model. Being a text-to-text model, we transform the binary classification task into a text generation task where we train mT5 to generate “neutral” when the input label corresponds to a non-toxic comment and “toxic” when the input label is toxic. We also add the task prefix “speech review” at the beginning of each input sequence. As T5, mT5 exists in five sizes: Small, Base, Large, XL, XXL. The XXL version of mT5 performs better than other multiling"
2021.germeval-1.4,tiedemann-2012-parallel,0,0.0129098,"as XLM-RoBERTa on many multilingual benchmarks, however, due to computational limits, we use the mT5 Base version that produces results comparable to XLM-RoBERTa (Xue et al., 2021). GBERT GBERT (Chan et al., 2020) is a German language model using the same architecture as BERT (Devlin et al., 2019). GBERT is an encoderonly Transformer model. It was trained using masked language modeling with whole word masking which corresponds to masking all of the tokens corresponding to a word. The pre-training corpus consists of German texts from Wikipedia, Common Crawl (Ortiz Su´arez et al., 2019), OPUS (Tiedemann, 2012), and Open Legal Data (Ostendorff et al., 2020). GBERT outperforms the state-of-theart for the GermEval 2018 hatespeech detection task and the GermEval 2014 NER task (Chan et al., 2020). We use the GBERT Base version. 3.3 Training scenarios To evaluate the benefit of the task-specific pretraining and data augmentation, we train the models in four different scenarios. XLM-RoBERTa XLM-RoBERTa (Conneau et al., 2020) is the multilingual version of RoBERTa (Liu et al., 2019). It was trained on the Common Crawl corpus in 100 languages using masked language modeling. We choose XLM-RoBERTa instead of"
2021.konvens-1.10,pustejovsky-etal-2010-iso,0,0.0423581,"an. Our methods can be adapted to other languages. • We present an annotated sample dataset and preliminary classification experiments. 2 Background and Related Work Our approach is primarily based on the categorization of precise time expressions according to TimeML and LTIMEX (Section 2.1) as well as on the categorization of VTEs provided by Tissot et al. (2019) (Section 2.2). Channell (1983) and Dinu et al. (2017) describe approaches on vague expressions in domains other than time. 2.1 Categories of Precise Time Expressions The ISO standard for the annotation of time expressions is TimeML (Pustejovsky et al., 2010). Temporal expressions are marked up using TimeML’s TIMEX3 tag to capture their meaning. Important attributes of this tag are type and value: Type records whether the expression is a duration, a point in time (either a specific date, or a time of the day) or a set of points in time (Saurí et al., 2006). The type of an expression determines how the expression is normalised in the value attribute. Temporal expressions with a modifier that cannot be expressed using the value attribute, e. g., “in about 3 days” are handled by the optional attribute mod, which was adapted from TIDES (Ferro et al.,"
2021.konvens-1.10,W01-1309,0,0.495748,"Missing"
2021.woah-1.13,2020.emnlp-main.404,0,0.166446,"d Work 2.1 Data sets For benchmarking purposes, we run our system on the data from Kiesel et al. (2019). They introduce a small number of articles (1,273) manually labeled by content, and a large number of articles (754,000) labeled by publisher via distant supervision, using labels from BuzzFeed news2 and Media Bias Fact Check3 . Due to the lack of article-level labels for German media, we adopt the strategy of labeling articles by publisher. Several studies use the data from allsides.com4 , which provides annotations on political ideology for individual articles in English. Using this data, Baly et al. (2020) introduce adversarial domain adaptation and triplet loss pre-training that prevents over-fitting to the style of a specific news medium, Kulkarni et al. (2018) demonstrate the importance of the article’s title and link structure for bias prediction and Li and Goldwasser (2019) explore how social content can be used to improve bias prediction by leveraging Graph Convolutional Networks to encode a social network graph. Zhou et al. (2021) analysed several unreliable news data sets and showed that heterogeneity of the 2 https://github.com/BuzzFeedNews/2017-08-partisan-sit es-and-facebook-pages 3"
2021.woah-1.13,S19-2007,0,0.0285819,"had a significant effect on the way people consume information in general, and news in particular (Newman et al., 2016). This development is accompanied by a number of challenges, which resulted in various NLP tasks that deal with information quality (Derczynski and Bontcheva, 2014; Dale, 2017; Saquete et al., 2020). Due to the data-driven nature of these tasks, they are often evaluated under the umbrella of (un)shared tasks, on topics such as rumour detection or verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kiesel et al., 2019), whi"
2021.woah-1.13,W19-3519,0,0.0207652,"s test/evaluation data comprised English news articles and used labels obtained by Vincent and Mestre (2018), but their five-point scale was binarised so the challenge was to label articles as being either hyperpartisan or not hyperpartisan. We follow Wich et al. (2020) in claiming that, in order to better understand online abuse and hate speech, biases in data sets and trained classifiers should be made transparent, as what can be considered hateful or abusive depends on many factors (relating to both sender and recipient), including race (Vidgen et al., 2020; Davidson et al., 2019), gender (Brooke, 2019; Clarke and Grieve, 2017), and political orientation (Vidgen and Derczynski, 2021; Jiang et al., 2020). This paper contributes to the detection of online abuse by attempting to uncover political bias in content. We describe the creation of a new data set of German news articles labeled for political bias. For annotation, we adopt the semi-supervised strategy of Kiesel et al. (2019) who label (English) articles 121 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 121–131 August 6, 2021. ©2021 Association for Computational Linguistics according to their publisher. In addition"
2021.woah-1.13,W17-3001,0,0.018536,"ion data comprised English news articles and used labels obtained by Vincent and Mestre (2018), but their five-point scale was binarised so the challenge was to label articles as being either hyperpartisan or not hyperpartisan. We follow Wich et al. (2020) in claiming that, in order to better understand online abuse and hate speech, biases in data sets and trained classifiers should be made transparent, as what can be considered hateful or abusive depends on many factors (relating to both sender and recipient), including race (Vidgen et al., 2020; Davidson et al., 2019), gender (Brooke, 2019; Clarke and Grieve, 2017), and political orientation (Vidgen and Derczynski, 2021; Jiang et al., 2020). This paper contributes to the detection of online abuse by attempting to uncover political bias in content. We describe the creation of a new data set of German news articles labeled for political bias. For annotation, we adopt the semi-supervised strategy of Kiesel et al. (2019) who label (English) articles 121 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 121–131 August 6, 2021. ©2021 Association for Computational Linguistics according to their publisher. In addition to opening up this line of"
2021.woah-1.13,W19-3504,0,0.0218515,"over 30 publications. This task’s test/evaluation data comprised English news articles and used labels obtained by Vincent and Mestre (2018), but their five-point scale was binarised so the challenge was to label articles as being either hyperpartisan or not hyperpartisan. We follow Wich et al. (2020) in claiming that, in order to better understand online abuse and hate speech, biases in data sets and trained classifiers should be made transparent, as what can be considered hateful or abusive depends on many factors (relating to both sender and recipient), including race (Vidgen et al., 2020; Davidson et al., 2019), gender (Brooke, 2019; Clarke and Grieve, 2017), and political orientation (Vidgen and Derczynski, 2021; Jiang et al., 2020). This paper contributes to the detection of online abuse by attempting to uncover political bias in content. We describe the creation of a new data set of German news articles labeled for political bias. For annotation, we adopt the semi-supervised strategy of Kiesel et al. (2019) who label (English) articles 121 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 121–131 August 6, 2021. ©2021 Association for Computational Linguistics according to their p"
2021.woah-1.13,N19-1423,0,0.00957198,"ce as a vector of |V |features with V being the vocabulary size. Each feature value contains the frequency of the word associated with the position in the vector in the input text. The vocabulary is based on the training data. TF-IDF Term-Frequency times InverseDocument-Frequency (TF-IDF) differs from BOW in that it takes into account the frequency of terms in the entire corpus (the training data, in our case). In addition to its popularity in all kinds of IR and NLP tasks, TF-IDF has recently been used in hate speech detection tasks (Salminen et al., 2019). BERT Since its introduction, BERT (Devlin et al., 2019), has been used in many NLP tasks. We use the German BERT base model from the Hugging Face Transformers library13 . We adopt the fine-tuning strategy from (Salminen et al., 2020): first, we fine-tune the BertForSequenceClassification model, consisting of BERT’s model and a linear softmax activation layer. After training, we 13 https://medienkompass.org/deutsche-medienlandschaft/ 125 https://huggingface.co/bert-base-german-cased drop the softmax activation layer and use BERT’s hidden state as the feature vector, which we then use as input for different classification algorithms. 4.2 Models Logi"
2021.woah-1.13,W19-4809,0,0.027794,"dwasser (2019) explore how social content can be used to improve bias prediction by leveraging Graph Convolutional Networks to encode a social network graph. Zhou et al. (2021) analysed several unreliable news data sets and showed that heterogeneity of the 2 https://github.com/BuzzFeedNews/2017-08-partisan-sit es-and-facebook-pages 3 https://mediabiasfactcheck.com 4 https://www.allsides.com/media-bias news sources is crucial for the prevention of sourcerelated bias. We adopt their strategy of splitting the sources into two disjoint sets used for building train and test data sets respectively. Gangula et al. (2019) work on detecting bias in news articles in the Indian language Telugu. They annotate 1,329 articles concentrating on headlines, which they find to be indicative of political bias. In contrast to Kiesel et al. (2019), but similar to our approach, Gangula et al. (2019) treat bias detection as a multi-class classification problem. They use the five main political parties present in the Teluguspeaking region as their classification labels, but do not position these parties on the political spectrum. Taking into account the political orientation of the author, SemEval 2016 Task 6 (Mohammad et al.,"
2021.woah-1.13,S19-2147,0,0.0281238,"ttention since their emergence 15-20 years ago. Their popularity among billions of users has had a significant effect on the way people consume information in general, and news in particular (Newman et al., 2016). This development is accompanied by a number of challenges, which resulted in various NLP tasks that deal with information quality (Derczynski and Bontcheva, 2014; Dale, 2017; Saquete et al., 2020). Due to the data-driven nature of these tasks, they are often evaluated under the umbrella of (un)shared tasks, on topics such as rumour detection or verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on s"
2021.woah-1.13,C18-1158,0,0.0242422,"red tasks, on topics such as rumour detection or verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kiesel et al., 2019), which predict either the stance of the author towards the topic of a news piece, or whether or not they exhibit allegiance to a particular party or cause. We argue that transparency and de-centralisation (i. e., moving away from a single, objective “truth” and a single institution, organisation or algorithm that decides on this) are essential in the analysis and dissemination of online information (Rehm, 2018). The predictio"
2021.woah-1.13,P14-1105,0,0.0246225,"believe that a sub-discipline dealing with bias detection benefits especially from a wide range of different data sets, ideally from as many different languages and cultural backgrounds as possible. We contribute to this cause by publishing and working with a German data set. 2.2 Models With regard to the system architecture, Bießmann (2016) use similar techniques as we do (bag-ofwords and a Logistic Regression classifier, though we do not use these two in combination), but work on the domain of German parliament speeches, attempting to predict the speaker’s affiliation based on their speech. Iyyer et al. (2014) use a bag-ofwords and Logistic Regression system as well, but improve over this with a Recursive Neural Network setup, working on the Convote data set (Thomas et al., 2006) and the Ideological Book Corpus6 . Hamborg et al. (2020) use BERT for sentiment analysis after finding Named Entities first, in order to find descriptions of entities that suggest either a left-wing or a right-wing bias (e. g., using either “freedom fighters” or “terrorists” to denote the same target entity or group). Salminen et al. (2020) work on hate speech classification. We adopt their idea of evaluating several metho"
2021.woah-1.13,N19-1357,0,0.0208371,"Missing"
2021.woah-1.13,S19-2145,0,0.171692,"2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kiesel et al., 2019), which predict either the stance of the author towards the topic of a news piece, or whether or not they exhibit allegiance to a particular party or cause. We argue that transparency and de-centralisation (i. e., moving away from a single, objective “truth” and a single institution, organisation or algorithm that decides on this) are essential in the analysis and dissemination of online information (Rehm, 2018). The prediction of political bias was recently examined by the 2019 Hyperpartisan News Detection task (Kiesel et al., 2019) with 42 teams submitting valid runs, resulting in over 30 pu"
2021.woah-1.13,D18-1388,0,0.0972029,"anually labeled by content, and a large number of articles (754,000) labeled by publisher via distant supervision, using labels from BuzzFeed news2 and Media Bias Fact Check3 . Due to the lack of article-level labels for German media, we adopt the strategy of labeling articles by publisher. Several studies use the data from allsides.com4 , which provides annotations on political ideology for individual articles in English. Using this data, Baly et al. (2020) introduce adversarial domain adaptation and triplet loss pre-training that prevents over-fitting to the style of a specific news medium, Kulkarni et al. (2018) demonstrate the importance of the article’s title and link structure for bias prediction and Li and Goldwasser (2019) explore how social content can be used to improve bias prediction by leveraging Graph Convolutional Networks to encode a social network graph. Zhou et al. (2021) analysed several unreliable news data sets and showed that heterogeneity of the 2 https://github.com/BuzzFeedNews/2017-08-partisan-sit es-and-facebook-pages 3 https://mediabiasfactcheck.com 4 https://www.allsides.com/media-bias news sources is crucial for the prevention of sourcerelated bias. We adopt their strategy o"
2021.woah-1.13,P19-1247,0,0.063381,"sing labels from BuzzFeed news2 and Media Bias Fact Check3 . Due to the lack of article-level labels for German media, we adopt the strategy of labeling articles by publisher. Several studies use the data from allsides.com4 , which provides annotations on political ideology for individual articles in English. Using this data, Baly et al. (2020) introduce adversarial domain adaptation and triplet loss pre-training that prevents over-fitting to the style of a specific news medium, Kulkarni et al. (2018) demonstrate the importance of the article’s title and link structure for bias prediction and Li and Goldwasser (2019) explore how social content can be used to improve bias prediction by leveraging Graph Convolutional Networks to encode a social network graph. Zhou et al. (2021) analysed several unreliable news data sets and showed that heterogeneity of the 2 https://github.com/BuzzFeedNews/2017-08-partisan-sit es-and-facebook-pages 3 https://mediabiasfactcheck.com 4 https://www.allsides.com/media-bias news sources is crucial for the prevention of sourcerelated bias. We adopt their strategy of splitting the sources into two disjoint sets used for building train and test data sets respectively. Gangula et al."
2021.woah-1.13,S19-2149,0,0.0214513,"r verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kiesel et al., 2019), which predict either the stance of the author towards the topic of a news piece, or whether or not they exhibit allegiance to a particular party or cause. We argue that transparency and de-centralisation (i. e., moving away from a single, objective “truth” and a single institution, organisation or algorithm that decides on this) are essential in the analysis and dissemination of online information (Rehm, 2018). The prediction of political bias was recently examined by t"
2021.woah-1.13,S16-1003,0,0.0873677,"Missing"
2021.woah-1.13,E17-2088,0,0.022176,") and are written by professional authors and edited before posted. And 2) unlike the shared task setup, we have no target entity or issue and aim to predict the political stance, bias or orientation (in the context of this paper, we consider these three words synonymous and use the phrase political bias throughout the rest of this paper) from the text, irrespective of a particular topic, entity or issue. One of the key challenges acknowledged in the literature is cross-target or cross-topic performance of stance detection systems (Küçük and Can, 2020). Trained for a specific target or topic (Sobhani et al., 2017), performance is considerably lower when these systems are applied to new targets. Vamvas and Sennrich (2020) address this issue by annotating and publishing a multilingual (standard Swiss 5 The shared task took place before Twitter increased the character limit of one tweet from 140 to 280 in 2017. 122 German, French, Italian) stance detection corpus that covers a considerably higher number of targets (over 150, compared to six in Mohammad et al., 2016). Vamvas and Sennrich (2020) work with comments, which are longer than tweets (on average 26 words), but still shorter than our news articles."
2021.woah-1.13,W06-1639,0,0.391992,"(standard Swiss 5 The shared task took place before Twitter increased the character limit of one tweet from 140 to 280 in 2017. 122 German, French, Italian) stance detection corpus that covers a considerably higher number of targets (over 150, compared to six in Mohammad et al., 2016). Vamvas and Sennrich (2020) work with comments, which are longer than tweets (on average 26 words), but still shorter than our news articles. Similar to Mohammad et al. (2016) but unlike our approach, the data is annotated for stance toward a particular target. Earlier work on political stance is represented by Thomas et al. (2006), who work on a corpus of US congressional debates, which is labeled for stance with regard to a particular issue (i. e., a proposed legislation) and which uses binary labels for supporting or opposing the proposed legislation. From this, political bias could potentially be deduced, if information on the party of the person that proposed the legislation is available. However, first of all this correlation is not necessarily present, and second, it results in a binary (republican vs. democratic) labeling scheme, whereas we use a larger set of labels covering the political spectrum from left-win"
2021.woah-1.13,D19-6601,0,0.0204127,"as rumour detection or verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kiesel et al., 2019), which predict either the stance of the author towards the topic of a news piece, or whether or not they exhibit allegiance to a particular party or cause. We argue that transparency and de-centralisation (i. e., moving away from a single, objective “truth” and a single institution, organisation or algorithm that decides on this) are essential in the analysis and dissemination of online information (Rehm, 2018). The prediction of political bias w"
2021.woah-1.13,2020.alw-1.19,0,0.016627,"d runs, resulting in over 30 publications. This task’s test/evaluation data comprised English news articles and used labels obtained by Vincent and Mestre (2018), but their five-point scale was binarised so the challenge was to label articles as being either hyperpartisan or not hyperpartisan. We follow Wich et al. (2020) in claiming that, in order to better understand online abuse and hate speech, biases in data sets and trained classifiers should be made transparent, as what can be considered hateful or abusive depends on many factors (relating to both sender and recipient), including race (Vidgen et al., 2020; Davidson et al., 2019), gender (Brooke, 2019; Clarke and Grieve, 2017), and political orientation (Vidgen and Derczynski, 2021; Jiang et al., 2020). This paper contributes to the detection of online abuse by attempting to uncover political bias in content. We describe the creation of a new data set of German news articles labeled for political bias. For annotation, we adopt the semi-supervised strategy of Kiesel et al. (2019) who label (English) articles 121 Proceedings of the Fifth Workshop on Online Abuse and Harms, pages 121–131 August 6, 2021. ©2021 Association for Computational Linguist"
2021.woah-1.13,2020.alw-1.7,0,0.0278283,"stitution, organisation or algorithm that decides on this) are essential in the analysis and dissemination of online information (Rehm, 2018). The prediction of political bias was recently examined by the 2019 Hyperpartisan News Detection task (Kiesel et al., 2019) with 42 teams submitting valid runs, resulting in over 30 publications. This task’s test/evaluation data comprised English news articles and used labels obtained by Vincent and Mestre (2018), but their five-point scale was binarised so the challenge was to label articles as being either hyperpartisan or not hyperpartisan. We follow Wich et al. (2020) in claiming that, in order to better understand online abuse and hate speech, biases in data sets and trained classifiers should be made transparent, as what can be considered hateful or abusive depends on many factors (relating to both sender and recipient), including race (Vidgen et al., 2020; Davidson et al., 2019), gender (Brooke, 2019; Clarke and Grieve, 2017), and political orientation (Vidgen and Derczynski, 2021; Jiang et al., 2020). This paper contributes to the detection of online abuse by attempting to uncover political bias in content. We describe the creation of a new data set of"
2021.woah-1.13,D19-1002,0,0.0372098,"Missing"
2021.woah-1.13,S19-2010,0,0.0157439,"g billions of users has had a significant effect on the way people consume information in general, and news in particular (Newman et al., 2016). This development is accompanied by a number of challenges, which resulted in various NLP tasks that deal with information quality (Derczynski and Bontcheva, 2014; Dale, 2017; Saquete et al., 2020). Due to the data-driven nature of these tasks, they are often evaluated under the umbrella of (un)shared tasks, on topics such as rumour detection or verification (Derczynski et al., 2017; Gorrell et al., 2019), offensive language and hate speech detection (Zampieri et al., 2019; Basile et al., 2019; 1 This work was done while all co-authors were at DFKI. The new affiliations of the first two authors are ambeRoad Tech GmbH, Aachen, Germany (dmitrii@amberoad.de) and Morningsun Technology GmbH, Saarbrücken, Germany (peter.bourgonje@morningsun-technology.com). Struß et al., 2019; Waseem et al., 2017; Fišer et al., 2018; Roberts et al., 2019; Akiwowo et al., 2020) or fake news and fact-checking (Hanselowski et al., 2018; Thorne et al., 2019; Mihaylova et al., 2019). Several shared tasks concentrate on stance (Mohammad et al., 2016) and hyper-partisan news detection (Kies"
2021.woah-1.13,2021.eacl-main.211,0,0.0346266,"lisher. Several studies use the data from allsides.com4 , which provides annotations on political ideology for individual articles in English. Using this data, Baly et al. (2020) introduce adversarial domain adaptation and triplet loss pre-training that prevents over-fitting to the style of a specific news medium, Kulkarni et al. (2018) demonstrate the importance of the article’s title and link structure for bias prediction and Li and Goldwasser (2019) explore how social content can be used to improve bias prediction by leveraging Graph Convolutional Networks to encode a social network graph. Zhou et al. (2021) analysed several unreliable news data sets and showed that heterogeneity of the 2 https://github.com/BuzzFeedNews/2017-08-partisan-sit es-and-facebook-pages 3 https://mediabiasfactcheck.com 4 https://www.allsides.com/media-bias news sources is crucial for the prevention of sourcerelated bias. We adopt their strategy of splitting the sources into two disjoint sets used for building train and test data sets respectively. Gangula et al. (2019) work on detecting bias in news articles in the Indian language Telugu. They annotate 1,329 articles concentrating on headlines, which they find to be indi"
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
L16-1251,piperidis-etal-2014-meta,1,0.621298,"itectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next steps. 2. Recent Developm"
L16-1251,piperidis-2012-meta,0,0.0809183,"ructures and Architectures 1. Introduction A truly multilingual Europe, which is supported through sophisticated Language Technologies (LT) is still far from being a reality. Since its inception in 2010, it has been one of the key goals of META-NET to foster and stimulate research and technology development towards this scenario. Important milestones along the way were the publication of the META-NET White Papers (Rehm and Uszkoreit, 2012; Rehm et al., 2014) and the Strategic Research Agenda for Multilingual Europe 2020 (SRA) (Rehm and Uszkoreit, 2013) as well as the deployment of META-SHARE (Piperidis, 2012; Piperidis et al., 2014). While all these activities did have a certain amount of impact in various European countries (Rehm et al., 2016b), new challenges and new opportunities have been emerging in the last two years. In this paper we1 provide an overview of the most recent developments around META-NET and the topic of multilingual Europe (Section 2.). We describe two new emerging initiatives that are becoming increasingly important for the community (Section 3.). The main current challenges and opportunities are sketched in Section 4. while Section 5. concludes with several suggested next"
L16-1251,W15-4941,1,0.744968,"lishment of a Memorandum of Understanding among the 12 organisations is foreseen with the goal of forming a “Coalition for a Multilingual Europe”. 5 http://rigasummit2015.eu The Riga Summit 2015 was jointly organised by METANET (through the project CRACKER), LT-Innovate (through the project LT_Observatory), Tilde and the European Commission. 7 BDVA, CITIA, CLARIN, EFNIL, ELEN, ELRA, GALA, LTInnovate, META-NET, NPLD, TAUS, W3C. 6 2.5. The Strategic Agenda for the Multilingual Digital Single Market Building upon past activities, in particular the METANET SRA (2013), the two EU-projects CRACKER (Rehm, 2015) and LT_Observatory prepared the Strategic Agenda for the Multilingual Digital Single Market (SRIA, 2015). Here, we can only provide a brief summary: the setup of the large and ambitious strategic programme towards the MDSM consists of three layers. On the top layer we have a set of focused Technology Solutions for Businesses and Public Services. These innovative application scenarios and solutions are supported, enabled, and driven by the middle layer which consists of a small group of Services, Infrastructures and Platforms that provide, through standardised interfaces, data exchange formats"
L16-1388,piperidis-etal-2014-meta,1,0.930923,"cludes the evaluation and validation of a resource through external parties to gauge a resource’s quality and sustainability; here, data is validated against XML schemas, documentation is checked and software is tested. The other phases refer to storing and backing up resources on servers (management), preparing distribution versions and regularly performing checks if a resource needs maintenance, making resources available through a web shop as a downloadable file or through other media based on open or commercial license agreements (distribution), as well as support services. In META-SHARE (Piperidis et al., 2014) a similar yet different scenario with regard to the description of resources with metadata is used: META-SHARE functions as an open resource exchange infrastructure, i. e., when making existing language resources available through META-SHARE, the resources are being described with the built-in metadata editor, which implements the metadata schema that was prepared for META-SHARE specifically. Several automatic procedures exist to transform existing metadata descriptions based on commonly used formats into the META-SHARE format. META-SHARE does not support a full language resource life cycle y"
L16-1388,rehm-etal-2008-ontology,1,0.86239,"Missing"
L16-1388,rehm-etal-2008-towards,1,0.841702,"Missing"
L16-1388,rehm-etal-2008-metadata,1,0.622524,"is beyond language resources proper: “Tools are often released according to different status, development versions and productive versions are common [...], for other language resources the release is the end of the development process. Hence, the status corresponds to the life cycle model of the resource type. The examples selected should be general enough to be usable in various contexts.” 3. The Language Resource Life Cycle Based on the analysis of these relevant publications and previous research and development experience by the author, e. g., (Wörner et al., 2006; Schmidt et al., 2006; Rehm et al., 2008b; Rehm et al., 2008c; Lehmberg et al., 2007; Rehm et al., 2008a; Rehm et al., 2009; Piperidis et al., 2014; Rehm et al., 2014), a comprehensive Language Resource Life Cycle is proposed. The author’s own contributions highlight best practice insights and specific aspects of selected phases of the proposed life cycle, the initial version of which is shown in Figure 1.2 The life cycle consists of a set of external factors and forces, the internal project context (i. e., the goal and objective of the Language Resource development project) and seven individual phases: • External Context and LR/LT"
L18-1384,W17-4215,1,0.917404,"s fragmented, unreliable and not efficient (Babakar, Mevan and Moy, Will, 2016). Identifying bias in online articles is another fundamental challenge. Watanabe (2017) analyses the influence of the Russian government on ITAR-TASS during the Ukraine crisis using the state-owned news agency, while Yeo et al. (2017) studied the effect of uncivil comments in online news articles in order to avoid bias interpretations. apply neural networks to identify polarity in news. Valdeón (2017) examines the impact of bias introduced in translations. Clickbait is often subsumed under the label of “fake news” (Bourgonje et al., 2017). Wei and Wan (2017) identifies misleading headlines – supposed to generate clicks – using class sequential rules to exploit structure information in ambiguous headlines. The BuzzFeed Marketing Challenge (Cowley, 2017) encourages the creation, publication and promotion of an article for generating 1,000 article views in one week. An overview of clickbait analysis approaches was published by Chen and Rubin (2017). Important related characteristics are also the dissemination (Maheshwari, 2016) and spreading (Giglietto et al., 2016) false news exhibit. Satirical articles are in stark contrast to"
L18-1384,W16-0802,0,0.0190358,"class sequential rules to exploit structure information in ambiguous headlines. The BuzzFeed Marketing Challenge (Cowley, 2017) encourages the creation, publication and promotion of an article for generating 1,000 article views in one week. An overview of clickbait analysis approaches was published by Chen and Rubin (2017). Important related characteristics are also the dissemination (Maheshwari, 2016) and spreading (Giglietto et al., 2016) false news exhibit. Satirical articles are in stark contrast to false news whose objective often is, in the severe cases, to misinform and to manipulate. Rubin et al. (2016) show that online satire often mimics the format and style of journalistic reporting. In addition to false news, other online phenomena need to be taken into account such as abusive language (Nobata et al., 2016; Bourgonje et al., 2018) and hatespeech (Warner and Hirschberg, 2012; Djuric et al., 2015; Schmidt and Wiegand, 2017). A large number of industrial approaches are focusing on detecting fake news. In early 2017, Facebook announced that they collaborate with the factchecking group Correctiv1 in Germany (Reuters, 2017). Fakeblok2 is a Chrome plugin that aims to sanitize the Facebook newsf"
L18-1384,W17-1101,0,0.0125534,"17). Important related characteristics are also the dissemination (Maheshwari, 2016) and spreading (Giglietto et al., 2016) false news exhibit. Satirical articles are in stark contrast to false news whose objective often is, in the severe cases, to misinform and to manipulate. Rubin et al. (2016) show that online satire often mimics the format and style of journalistic reporting. In addition to false news, other online phenomena need to be taken into account such as abusive language (Nobata et al., 2016; Bourgonje et al., 2018) and hatespeech (Warner and Hirschberg, 2012; Djuric et al., 2015; Schmidt and Wiegand, 2017). A large number of industrial approaches are focusing on detecting fake news. In early 2017, Facebook announced that they collaborate with the factchecking group Correctiv1 in Germany (Reuters, 2017). Fakeblok2 is a Chrome plugin that aims to sanitize the Facebook newsfeed from fake news sites using a curated, factchecked and monitored list of links curated by a group of independent media professionals. Another relevant tool is Fakenews Dataset Anal1 2 ysis, a machine learning system that analyses online news and provides a user-friendly visualisation.3 Hoaxy4 visualises how reported claims –"
L18-1384,S17-2085,1,0.892903,"Missing"
L18-1384,W12-2103,0,0.0520903,"sis approaches was published by Chen and Rubin (2017). Important related characteristics are also the dissemination (Maheshwari, 2016) and spreading (Giglietto et al., 2016) false news exhibit. Satirical articles are in stark contrast to false news whose objective often is, in the severe cases, to misinform and to manipulate. Rubin et al. (2016) show that online satire often mimics the format and style of journalistic reporting. In addition to false news, other online phenomena need to be taken into account such as abusive language (Nobata et al., 2016; Bourgonje et al., 2018) and hatespeech (Warner and Hirschberg, 2012; Djuric et al., 2015; Schmidt and Wiegand, 2017). A large number of industrial approaches are focusing on detecting fake news. In early 2017, Facebook announced that they collaborate with the factchecking group Correctiv1 in Germany (Reuters, 2017). Fakeblok2 is a Chrome plugin that aims to sanitize the Facebook newsfeed from fake news sites using a curated, factchecked and monitored list of links curated by a group of independent media professionals. Another relevant tool is Fakenews Dataset Anal1 2 ysis, a machine learning system that analyses online news and provides a user-friendly visual"
L18-1519,P10-1010,0,0.0647029,"Missing"
L18-1519,P07-2045,0,0.00490826,"an umbrella initiative for European projects and organisations working on technologies for multilingual Europe. Europe has a long-standing research, development and innovation tradition with several hundred universities and research centers performing excellent, highly visible and internationally recognised research on all European and many non-European languages. Especially in the field of Machine Translation most of the basic research has happened in Eu3282 2 http://www.meta-net.eu http://www.cracker-project.eu 4 http://www.cracking-the-language-barrier.eu 3 ropean research projects. Moses (Koehn et al., 2007), until 2016 the state of the art phrase-based statistical MT system, and recent European NMT results, especially those of the European research project QT21, are just two examples for excellence and world class research (Bojar et al., 2017). Nonetheless, challenges are omnipresent and must be addressed by the EU, the Member States as well as stakeholders from academia and industry. 3. Method The survey contains a total of 29 questions (see Appendix A) of which 16 are open questions with free text answers. The remaining ones are a mixture of multiple choice and yes/no questions. The findings o"
L18-1519,L16-1251,1,0.354494,"Missing"
piperidis-etal-2014-meta,wittenburg-etal-2010-resource,1,\N,Missing
piperidis-etal-2014-meta,choukri-etal-2012-using,1,\N,Missing
piperidis-etal-2014-meta,piperidis-2012-meta,1,\N,Missing
piperidis-etal-2014-meta,gavrilidou-etal-2012-meta,1,\N,Missing
piperidis-etal-2014-meta,broeder-etal-2010-data,0,\N,Missing
piperidis-etal-2014-meta,soria-etal-2012-flarenet,1,\N,Missing
piperidis-etal-2014-meta,federmann-etal-2012-meta,1,\N,Missing
rehm-etal-2008-metadata,rehm-etal-2008-ontology,1,\N,Missing
rehm-etal-2008-metadata,W03-0804,0,\N,Missing
rehm-etal-2008-metadata,rehm-etal-2008-towards,1,\N,Missing
rehm-etal-2008-ontology,ide-etal-2000-xces,0,\N,Missing
rehm-etal-2008-ontology,telljohann-etal-2004-tuba,0,\N,Missing
rehm-etal-2008-ontology,J93-2004,0,\N,Missing
rehm-etal-2008-ontology,rehm-etal-2008-metadata,1,\N,Missing
rehm-etal-2008-towards,C94-2174,0,\N,Missing
rehm-etal-2008-towards,P97-1005,0,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
S17-2085,W02-2018,0,0.034782,"Missing"
S17-2085,2016.eamt-2.23,1,0.833478,"Missing"
W16-3503,P14-5010,0,0.00564672,"Missing"
W16-3503,N07-4013,0,0.0113224,"poulos, 2007), (Bontcheva and Wilks, 2004) in that it does not focus on one specific domain and it includes collecting the information that goes into the ontology. Our approach is based on extracting relations between entities. Because of our focus on domain adaptability, we plan to avoid the labour-intensive selection of seed patterns that comes with (semi-)supervised approaches as described in (Xu et al., 2013), (Brin, 1998), (Agichtein and Gravano, 2000) and (Etzioni et al., 2005). We use an unsupervised approach instead with an open set of relation types. A similar system is described in (Yates et al., 2007). For ease of integration reasons we implement an in-house approach to relation extract, as described in 4. 3 SPARQL queries are used to retrieve type-specific related information (like birth and death dates and places for persons, geo-coordinates for locations, etc.). This information is added to the semantic layer and is used to fill particular slots in our story templates. 4 Semantic Storytelling For the task of storytelling we use a templatefilling approach. The user selects one or more concepts and one or more templates that we attempt to fill using the semantic layer. We have defined a b"
W16-3503,W13-2102,0,0.262983,"Missing"
W17-2707,W16-3503,1,0.687765,"Missing"
W17-2707,W16-5708,0,0.0114397,"e human expert specific relationships between entities • Automatic transformation of RDF database contents into play-out formats for different channels and media RDF DB iOS App ePub HTML5 Android App … Figure 1: The Semantic Storytelling architecture and workflow written on March 14, 1944, and March 10, 1949, respectively: tuples we attempt to construct a travelogue as a list of six-tuples (see Figure 1). Many researchers working on, among others, text linguistics have emphasised the relationship between generalised text structure patterns and their respective text types or genres. Recently, (Caselli and Vossen, 2016) proposed the Storyline Annotation and Representation Scheme, which is primarily aimed at news articles to “identify salient events (climax events) as the central elements around which a specific topic develops”. With regard to the travelogue example, the notion of one “climax event”, “rising actions” and “falling actions” is not applicable, also see (Pang et al., 2011; Ye et al., 2011). Storyline applications tailored to specific text types (news articles vs. letters and travelogue) have different requirements regarding their storyline abstraction models. Accordingly, we focus on the identifi"
W17-2707,W13-2102,0,0.246079,"Missing"
W17-2707,P89-1009,0,0.645151,"candidates (Obama) While, in our manual evaluation, many MAE candidates turned out to be genuine MAEs, we also found instances of false positives, which contained information extracted from non-article contents such as, for example, the imprint and copy7 Based on a list of links to news articles in https://en.wikipedia.org/wiki/List_of_ international_presidential_trips_made_ by_Barack_Obama 6 This approach performed better on the Obama news corpus (Section 4). 48 Figure 2: The storytelling dashboard showing Movement Action Events annotations (Gerv´as, 2013), for recipes (Cimiano et al., 2013; Dale, 1989) or for weather reports (Belz, 2008; Goldberg et al., 1994; Reiter et al., 2005; Turner et al., 2006), requiring knowledge about characters, actions, locations, events, or objects that exist in this particular domain (Gerv´as et al., 2005; Riedl and Young, 2010; Turner, 2014). The most closely related approach is the one developed by (Poulakos et al., 2015), which presents “an accessible graphical platform for content creators and even end users to create their own story worlds, populate it with smart characters and objects, and define narrative events that can be used by existing tools for au"
W17-2707,doddington-etal-2004-automatic,0,0.279743,"Missing"
W17-2707,S10-1071,0,0.0981313,"Missing"
W17-2707,E06-2020,0,0.0241528,"uine MAEs, we also found instances of false positives, which contained information extracted from non-article contents such as, for example, the imprint and copy7 Based on a list of links to news articles in https://en.wikipedia.org/wiki/List_of_ international_presidential_trips_made_ by_Barack_Obama 6 This approach performed better on the Obama news corpus (Section 4). 48 Figure 2: The storytelling dashboard showing Movement Action Events annotations (Gerv´as, 2013), for recipes (Cimiano et al., 2013; Dale, 1989) or for weather reports (Belz, 2008; Goldberg et al., 1994; Reiter et al., 2005; Turner et al., 2006), requiring knowledge about characters, actions, locations, events, or objects that exist in this particular domain (Gerv´as et al., 2005; Riedl and Young, 2010; Turner, 2014). The most closely related approach is the one developed by (Poulakos et al., 2015), which presents “an accessible graphical platform for content creators and even end users to create their own story worlds, populate it with smart characters and objects, and define narrative events that can be used by existing tools for automated narrative synthesis”. right information; we tried to remove all HTML boilerplates and templat"
W17-2707,P07-2045,0,0.00711848,"Missing"
W17-2707,P13-1008,0,0.0335092,"Janeiro after a day of talks in the capital, Brasilia, with Ms Rousseff and business leaders.] 5 Related Work Most approaches in the event detection literature are machine learning-based and adhere to a modular approach (Ahn, 2006), i. e., they use the output from constituency and dependency parsers, named entity recognisers, coreference resolution systems, and part-of-speech taggers to build classifiers for subtasks of trigger labelling and argument labelling. However, recently, state-ofthe-art results have been achieved by joint entity and event extraction systems (Yang and Mitchell, 2016; Li et al., 2013), i. e., approaches which compute joint inference in one combined model to minimise the errors introduced by sub-modules. Several approaches are related to our Semantic Storytelling concept, all of them concentrating on their own objectives and providing solutions for their respective challenges. A few systems focus on providing content for entertainment purposes (Wood, 2008). Other researchers focus on specific domains, for example, storytelling in gaming 6 Summary and Future Work We present an approach at identifying a specific class of events, movement action events, in the Mendelsohn data"
W17-2707,N16-1033,0,0.0332924,"r Obama arrived in Rio de Janeiro after a day of talks in the capital, Brasilia, with Ms Rousseff and business leaders.] 5 Related Work Most approaches in the event detection literature are machine learning-based and adhere to a modular approach (Ahn, 2006), i. e., they use the output from constituency and dependency parsers, named entity recognisers, coreference resolution systems, and part-of-speech taggers to build classifiers for subtasks of trigger labelling and argument labelling. However, recently, state-ofthe-art results have been achieved by joint entity and event extraction systems (Yang and Mitchell, 2016; Li et al., 2013), i. e., approaches which compute joint inference in one combined model to minimise the errors introduced by sub-modules. Several approaches are related to our Semantic Storytelling concept, all of them concentrating on their own objectives and providing solutions for their respective challenges. A few systems focus on providing content for entertainment purposes (Wood, 2008). Other researchers focus on specific domains, for example, storytelling in gaming 6 Summary and Future Work We present an approach at identifying a specific class of events, movement action events, in th"
W17-4212,W17-2707,1,0.819223,"Missing"
W17-4212,W16-3503,1,0.650844,"news consumption experience. 3 (temporal, geographical, semantic, etc.) form a story. In several use cases, the atomic building blocks are documents; we use a more fine-grained approach in which events are the building blocks of storylines, i. e., a set of entities governed by a trigger element (normally a verb) and which together represent an occurring action in the text. The linguistic processing is done by a semantic annotation pipeline that creates a layer of named entities, temporal expressions and other information on top of the document collection, also augmented with event detection (Bourgonje et al., 2016a,b). For example, in the sentence “Barack Obama visited Ukraine last summer and had a meeting with Petr´o Poroshenko” there are two persons (Barack Obama, Petr´o Poroshenko), one location (Ukraine) and one temporal expression (last summer). There are also two trigger verbs: “visited” and “meet”. Therefore, there are two events in this sentence: [visited, Barack Obama, Ukraine, last summer] and [meet, Barack Obama, Petr´o Poroshenko]. The sentence “Vladimir Putin will meet next summer with president Petr´o Poroshenko in Moscow” contains one event: [meet, Vladimir Putin, Petr´o Poroshenko, Mosc"
W17-4212,W13-2102,0,0.0407262,"Missing"
W17-4212,doddington-etal-2004-automatic,0,0.0568548,"e map can be used to filter events by time. Additional details and case studies can be found in (Rehm et al., 2017a; Schneider et al., 2017). We will explore if we can integrate part of this prototype tool into the Superdesk extension. 3.1 Cross-lingual Event Detection We implemented a cross-lingual event detection system, i. e., we automatically translate non-English documents to English using Moses (Koehn et al., 2007) and detect events in the translated documents using a state-of-the-art event extraction system based on (Yang and Mitchell, 2016), trained on the English section of ACE 2005 (Doddington et al., 2004). The cross-lingual detection of events encompasses a pipeline that ends up with a list of annotated events in every document (Rehm et al., 2017b). 3.2 Semantic Storytelling Extracted events themselves are not useful to a journalist who works on a set of documents. They have to be analysed further, summarised, rearranged and then presented in a way that speeds up (human) access and understanding. In a previous approach (Schneider et al., 2016) we focused upon template-filling, using the results of relation extraction to fill (biography) templates to present these as content pieces to the knowl"
W17-4212,P07-2045,0,0.00466128,"antic analysis information and can be used interactively to explore and evaluate the system. The map shows locations involved in extracted events with highlighted annotations. The slider below the map can be used to filter events by time. Additional details and case studies can be found in (Rehm et al., 2017a; Schneider et al., 2017). We will explore if we can integrate part of this prototype tool into the Superdesk extension. 3.1 Cross-lingual Event Detection We implemented a cross-lingual event detection system, i. e., we automatically translate non-English documents to English using Moses (Koehn et al., 2007) and detect events in the translated documents using a state-of-the-art event extraction system based on (Yang and Mitchell, 2016), trained on the English section of ACE 2005 (Doddington et al., 2004). The cross-lingual detection of events encompasses a pipeline that ends up with a list of annotated events in every document (Rehm et al., 2017b). 3.2 Semantic Storytelling Extracted events themselves are not useful to a journalist who works on a set of documents. They have to be analysed further, summarised, rearranged and then presented in a way that speeds up (human) access and understanding."
W17-4212,W15-4507,0,0.0555251,"Missing"
W17-4212,N16-1033,0,0.0211748,"in extracted events with highlighted annotations. The slider below the map can be used to filter events by time. Additional details and case studies can be found in (Rehm et al., 2017a; Schneider et al., 2017). We will explore if we can integrate part of this prototype tool into the Superdesk extension. 3.1 Cross-lingual Event Detection We implemented a cross-lingual event detection system, i. e., we automatically translate non-English documents to English using Moses (Koehn et al., 2007) and detect events in the translated documents using a state-of-the-art event extraction system based on (Yang and Mitchell, 2016), trained on the English section of ACE 2005 (Doddington et al., 2004). The cross-lingual detection of events encompasses a pipeline that ends up with a list of annotated events in every document (Rehm et al., 2017b). 3.2 Semantic Storytelling Extracted events themselves are not useful to a journalist who works on a set of documents. They have to be analysed further, summarised, rearranged and then presented in a way that speeds up (human) access and understanding. In a previous approach (Schneider et al., 2016) we focused upon template-filling, using the results of relation extraction to fill"
W17-4215,D16-1084,0,0.0445744,"Missing"
W17-4215,P15-2070,0,0.00751725,"Missing"
W17-4215,W16-3503,1,0.882425,"Missing"
W17-4215,S17-2085,1,0.841095,"Missing"
W17-4215,R15-1086,0,0.0126865,"Missing"
W17-4215,N16-1138,0,0.0109497,"f fact checking approaches. 4 Approach and Methods In line with the scoring system of the challenge, we first apply a procedure to decide whether a particular headline/article combination is related or unrelated. This is done based on n-gram matching of the lemmatised input (headline or article), using the CoreNLP Lemmatiser (Manning et al., 2014). The number of matching n-grams (where n = 1..6) in the headline and article is multiplied by length and IDF value of the matching n-gram 2 https://github.com/FakeNewsChallenge/fnc-1 86 cally motivated features, some of them inspired by the work of (Ferreira and Vlachos, 2016). From rather basic ones (like a question mark at the end of a headline to detect “disagree” instances) to more sophisticated ones (like extracting a dependency graph, looking for negation-type typed dependencies and calculate their normalised distance to the root node of the graph, and compare this value for headline and article), but these did not improve upon the final weighted score reported in Table 2. (n-grams containing only stop words or punctuation are not considered), then divided by the total number of n-grams. If the resulting score is above some threshold (we established 0.0096 as"
W17-4215,W16-5618,0,0.0599357,"Missing"
W19-2207,S12-1059,0,0.0703153,"Missing"
W19-2207,P05-1045,0,0.0134553,"Missing"
W19-2207,W17-4511,0,0.0172487,"ons, subsections, paragraphs, etc. cor59 of-words sentence representations, our approach tries to improve on this, by analysing the texts first. Searching the embedding space of all words used in the text, we cluster similar words so that morphological variants of a word like “tree” and “trees” or “eat” and “eating”, but also synonyms like “fast” and “rapid” are considered as belonging to the same cluster. Based on these groupings we encode all documents and then calculate the weights for the sentences using TF-IDF. The second tool is based on the concept of centroids (Rossiello et al., 2017; Ghalandari, 2017) and benefits from the composability of word embeddings. Initially, keywords and concepts are extracted from the document. By composing their embeddings, the centroid is created, which represents the document’s condensed meaningful information. It is then projected into the embedding space together with all sentence embeddings. Sentences receive relevance scores depending on their distance to the centroid in the embedding space. To avoid redundancy in the summary, sentences that are too similar to the ones already added to the summary are not used. Both tools can be used for multiple languages"
W19-2207,boella-etal-2012-nlp,0,0.319513,"Missing"
W19-2207,N16-1030,0,0.277206,"Missing"
W19-2207,W16-3503,1,0.729762,"specific relations such as activity requires permit or permit was issued on date. So far, such relations can be recognized in English language texts, but training for German, Spanish and Dutch, using the 3.13 Question Answering The Question Answering (QA) service accepts a natural language question and responds with an 61 of the containerised microservices we use OpenShift; alternative technologies such as, among others, Kubernetes, could also be used. In our project we conceptualise the specific requirements of the different use cases as content curation workflows (Schneider and Rehm, 2018b; Bourgonje et al., 2016a,b; Rehm et al., 2018). Workflows are defined as the execution of specific services to perform the processing of one or more documents under the umbrella of a certain task or use case. The specification of a workflow includes its input and output as well as the functionality it is supposed to perform: annotate or enrich a document, add a document to the knowledge base, search for information, etc. The project offers compliance-related features and functionalities through common services and data sets included in the LKG. Workflows make use of these services to implement the required functiona"
W19-2207,P18-2020,0,0.36431,"Missing"
W19-2207,P02-1040,0,0.103502,"l) which are available for English, German, Spanish and Dutch, among others. In order to compare documents in two different languages, machine translation between them, or to a third language must be available. The semantic similarity service is a prototype, requiring further testing and refining. ing background data curation processes. The synchronous translation service endpoint serves translation functionality for texts and documents annotated with the Natural Language Processing Interchange Format ontology (NIF) (Hellmann et al., 2013). The systems were automatically evaluated using BLEU (Papineni et al., 2002) on held-out evaluation sets. The sets were created from the indomain parts of the parallel corpora used for training of the NMT systems. Table 5 contains statistics of the training data and the automatic evaluation results of the NMT systems. 3.11 Semantic Similarity Legal Knowledge Graph Population For the definition of our Knowledge Graph, we benefit from predefined vocabularies such as EUROVOC. However, their knowledge is limited to that intended by their creators, and their level of specificity and focus will, in general, not match the ones required for an application. One possible option"
W19-2207,W17-1003,0,0.0146932,"cument to identify sections, subsections, paragraphs, etc. cor59 of-words sentence representations, our approach tries to improve on this, by analysing the texts first. Searching the embedding space of all words used in the text, we cluster similar words so that morphological variants of a word like “tree” and “trees” or “eat” and “eating”, but also synonyms like “fast” and “rapid” are considered as belonging to the same cluster. Based on these groupings we encode all documents and then calculate the weights for the sentences using TF-IDF. The second tool is based on the concept of centroids (Rossiello et al., 2017; Ghalandari, 2017) and benefits from the composability of word embeddings. Initially, keywords and concepts are extracted from the document. By composing their embeddings, the centroid is created, which represents the document’s condensed meaningful information. It is then projected into the embedding space together with all sentence embeddings. Sentences receive relevance scores depending on their distance to the centroid in the embedding space. To avoid redundancy in the summary, sentences that are too similar to the ones already added to the summary are not used. Both tools can be used for"
W19-2207,D14-1162,0,0.0819742,"on services (Allahyari et al., 2017). While extractive summarisation has been popular in the past, the progress in neural technologies has renewed the interest in abstractive summarisation, i. e., generating new sentences that capture a document’s meaning. This approach requires highly complex models and a lot of training data. In the absence of labeled training data, extractive methods are often used as the basis for abstractive methods, by assigning relevance scores to sentences in an unsupervised way. Abstractive summarisation is often augmented using word embeddings (Mikolov et al., 2013; Pennington et al., 2014) that provide a shared semantic space for those strongly related sentences that do no share the same but similar or related words. We develop two methods. The first tool is based on TF-IDF (Neto et al., 2000). This is a popular baseline as it is easy to implement, unsupervised, and language independent. Instead of using bag3.10 Machine Translation To enable multilingualism and cross-lingual extraction, linking and search, we use the Machine Translation (MT) service Tilde MT11 . In order to populate and process the Legal Knowledge Graph in a multilingual way, custom Neural Machine Translation ("
W19-2207,S10-1071,0,0.0609821,"Missing"
W19-2207,D17-1035,0,0.0234888,"Missing"
