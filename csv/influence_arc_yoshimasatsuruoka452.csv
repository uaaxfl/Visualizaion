2020.acl-main.94,P17-1042,0,0.0236357,"larger captures topical similarities. in the form of word dictionaries (Duong et al., 2016), parallel corpora (Gouws et al., 2015; Luong et al., 2015), document-aligned corpora (Vulic and Moens, 2016). Another line of research is off-line mappingbased approaches (Ruder et al., 2019), where monolingual embeddings are independently trained in multiple languages, and a post-hoc alignment matrix is learned to align the embedding spaces with a seed word dictionary (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), with only a little supervision such as identical strings or numerals (Artetxe et al., 2017; Smith et al., 2017), or even in a completely unsupervised manner (Lample et al., 2018; Artetxe et al., 2018). Mapping-based approaches have recently been popularized by their cheaper computational cost compared to joint approaches, as they can make use of pre-trained monolingual word embeddings. The assumption behind the mapping-based methods is the isomorphism of monolingual embedding spaces, i.e., the embedding spaces are structurally similar, or the nearest neighbor graphs from the different languages are approximately isomorphic (Søgaard et al., 2018). Considering that the structures of"
2020.acl-main.94,P19-1070,0,0.121966,"Missing"
2020.acl-main.94,P18-1073,0,0.0329859,"(Gouws et al., 2015; Luong et al., 2015), document-aligned corpora (Vulic and Moens, 2016). Another line of research is off-line mappingbased approaches (Ruder et al., 2019), where monolingual embeddings are independently trained in multiple languages, and a post-hoc alignment matrix is learned to align the embedding spaces with a seed word dictionary (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), with only a little supervision such as identical strings or numerals (Artetxe et al., 2017; Smith et al., 2017), or even in a completely unsupervised manner (Lample et al., 2018; Artetxe et al., 2018). Mapping-based approaches have recently been popularized by their cheaper computational cost compared to joint approaches, as they can make use of pre-trained monolingual word embeddings. The assumption behind the mapping-based methods is the isomorphism of monolingual embedding spaces, i.e., the embedding spaces are structurally similar, or the nearest neighbor graphs from the different languages are approximately isomorphic (Søgaard et al., 2018). Considering that the structures of the monolingual embedding spaces are closely related to the choice of the context window, it is natural to exp"
2020.acl-main.94,P14-1023,0,0.0469733,"can capture word semantics invariant among multiple languages, and facilitate cross-lingual transfer for lowresource languages (Ruder et al., 2019). Recent research has focused on mapping-based methods, which find a linear transformation from the source to target embedding spaces (Mikolov et al., 2013b; Artetxe et al., 2016; Lample et al., 2018). Learning a linear transformation is based on a strong assumption that the two embedding spaces are structurally similar or isometric. The structure of word embeddings heavily depends on the co-occurrence information of words (Turney and Pantel, 2010; Baroni et al., 2014), i.e., word embeddings are computed by counting other words that appear in a specific context window of each word. The choice of context window changes the co-occurrence statistics of words and thus is crucial to determine the structure of an embedding space. For example, it has been known that an embedding space trained with a smaller linear window captures functional similarities, while a larger window captures topical similarities (Levy and Goldberg, 2014a). Despite this important relationship between the choice of context window and the structure of embedding space, how the choice of cont"
2020.acl-main.94,Q17-1010,0,0.0465572,"nfigurable. This setting assumes common situations where the embedding of the target language is available in the form of pre-trained embeddings. Figure 1 shows the result of the four languages. Firstly, we observe that too small windows (1 to 3) for source embeddings do not yield good performance, probably because the model failed to train accurate word embedding models with insufficient training word-context pairs that the small windows capture. At first, this result may seem to contradict with the result from Søgaard et al. (2018). They trained English and Spanish embeddings with fasttext (Bojanowski et al., 2017) and the window size of 2, and then aligned them with an unsupervised mapping algorithm (Lample et al., 2018). When they changed the window size of the Spanish embedding to 10, they only observed a very slight drop on top-1 precision (from 81.89 to 81.28). We suspect that the discrepancy with our result is due to the different settings. First of 998 Figure 2: BLI performance for each PoS in the comparable setting. Figure 3: BLI performance in the comparable setting. Figure 4: BLI performance in the different domain setting. all, fasttext adopts a dynamic window mechanism, which may obfuscate t"
2020.acl-main.94,N18-2030,0,0.0203298,"ectively. The results of the comparable setting in each language are shown in Figure 6. 9 The frequencies were calculated from our subset of the English Wikipedia corpus. 1000 Figure 8: Downstream evaluations in the comparable settings. SA: sentiment analysis; DC: document classification; DP: dependency parsing. The window sizes of both the source and target embeddings are varied. The scores for the frequent words (top500) are notably higher than the rare words (bottom500). This confirms previous empirical results that existing mapping-based methods perform significantly worse for rare words (Braune et al., 2018; Czarnowska et al., 2019). With respect to the relation with the context size, both frequent and rare words benefit from larger window sizes, although the gain in the rare words is less obvious in some languages (Ja and Ru). In the different domain settings, as shown in Figure 7, the rare words, in turn, suffer from larger window sizes, especially for Fr and Ru, but the performance on frequent words still improves as the context window increases. We conjecture that when training a skip-gram model, frequent words observe many context words, and that would mitigate the effect of irrelevant word"
2020.acl-main.94,D19-1090,0,0.0143779,"of the comparable setting in each language are shown in Figure 6. 9 The frequencies were calculated from our subset of the English Wikipedia corpus. 1000 Figure 8: Downstream evaluations in the comparable settings. SA: sentiment analysis; DC: document classification; DP: dependency parsing. The window sizes of both the source and target embeddings are varied. The scores for the frequent words (top500) are notably higher than the rare words (bottom500). This confirms previous empirical results that existing mapping-based methods perform significantly worse for rare words (Braune et al., 2018; Czarnowska et al., 2019). With respect to the relation with the context size, both frequent and rare words benefit from larger window sizes, although the gain in the rare words is less obvious in some languages (Ja and Ru). In the different domain settings, as shown in Figure 7, the rare words, in turn, suffer from larger window sizes, especially for Fr and Ru, but the performance on frequent words still improves as the context window increases. We conjecture that when training a skip-gram model, frequent words observe many context words, and that would mitigate the effect of irrelevant words (noise) caused by a larg"
2020.acl-main.94,P14-2050,0,0.335117,"similar or isometric. The structure of word embeddings heavily depends on the co-occurrence information of words (Turney and Pantel, 2010; Baroni et al., 2014), i.e., word embeddings are computed by counting other words that appear in a specific context window of each word. The choice of context window changes the co-occurrence statistics of words and thus is crucial to determine the structure of an embedding space. For example, it has been known that an embedding space trained with a smaller linear window captures functional similarities, while a larger window captures topical similarities (Levy and Goldberg, 2014a). Despite this important relationship between the choice of context window and the structure of embedding space, how the choice of context window affects the structural similarity of two embedding spaces has not been fully explored yet. In this paper, we attempt to deepen the understanding of cross-lingual word embeddings from the perspective of the choice of the context window through carefully designed experiments. We experiment with a variety of settings, with different domains and languages. We train monolingual word embeddings varying the context window sizes, align them with a mapping-"
2020.acl-main.94,W14-1618,0,0.558071,"similar or isometric. The structure of word embeddings heavily depends on the co-occurrence information of words (Turney and Pantel, 2010; Baroni et al., 2014), i.e., word embeddings are computed by counting other words that appear in a specific context window of each word. The choice of context window changes the co-occurrence statistics of words and thus is crucial to determine the structure of an embedding space. For example, it has been known that an embedding space trained with a smaller linear window captures functional similarities, while a larger window captures topical similarities (Levy and Goldberg, 2014a). Despite this important relationship between the choice of context window and the structure of embedding space, how the choice of context window affects the structural similarity of two embedding spaces has not been fully explored yet. In this paper, we attempt to deepen the understanding of cross-lingual word embeddings from the perspective of the choice of the context window through carefully designed experiments. We experiment with a variety of settings, with different domains and languages. We train monolingual word embeddings varying the context window sizes, align them with a mapping-"
2020.acl-main.94,D17-1257,0,0.0449011,"Missing"
2020.acl-main.94,N15-1142,0,0.0275752,", wt+1 , ..., wt+k ]. The choice of context is crucial to the resulting embeddings as it will change the co-occurrence statistics associated with each target word. Table 1 demonstrates the effect of the context window size on the nearest neighbor structure of embedding space; with a small window size, the resulting embeddings capture functional similarity, while with a larger window size, the embeddings capture topical similarities. Among the other types of context windows that have been explored by researchers are linear windows enriched with positional information (Levy and Goldberg, 2014b; Ling et al., 2015a; Li et al., 2017), syntactically informed context windows based on dependency trees (Levy and Goldberg, 2014a; Li et al., 2017), and one that dynamically weights the surrounding words with the attention mechanism (Ling et al., 2015b). In this paper, we mainly discuss the most common linear window and investigate how the choice of the window size affects the isomorphism of two embedding spaces and the performance of cross-lingual transfer. 2.2 Cross-lingual Word Embeddings Cross-lingual word embeddings aim to learn a shared semantic space in multiple languages. One promising solution is to jo"
2020.acl-main.94,D15-1161,0,0.0263836,", wt+1 , ..., wt+k ]. The choice of context is crucial to the resulting embeddings as it will change the co-occurrence statistics associated with each target word. Table 1 demonstrates the effect of the context window size on the nearest neighbor structure of embedding space; with a small window size, the resulting embeddings capture functional similarity, while with a larger window size, the embeddings capture topical similarities. Among the other types of context windows that have been explored by researchers are linear windows enriched with positional information (Levy and Goldberg, 2014b; Ling et al., 2015a; Li et al., 2017), syntactically informed context windows based on dependency trees (Levy and Goldberg, 2014a; Li et al., 2017), and one that dynamically weights the surrounding words with the attention mechanism (Ling et al., 2015b). In this paper, we mainly discuss the most common linear window and investigate how the choice of the window size affects the isomorphism of two embedding spaces and the performance of cross-lingual transfer. 2.2 Cross-lingual Word Embeddings Cross-lingual word embeddings aim to learn a shared semantic space in multiple languages. One promising solution is to jo"
2020.acl-main.94,W15-1521,0,0.0367471,"ulary. words typological window size 1 phrases loanwords morphemes verses phonemes synchronic mechanistic numerological architectonic dialectical window size 10 word phrases phrase ungrammatical homographs totemism typology categorizations dialectology fusional Table 1: The top-5 nearest neighbors in English embedding spaces trained with different context windows in our experiment. The smaller window size captures functional similarities (-s, -cal, -ic), while the larger captures topical similarities. in the form of word dictionaries (Duong et al., 2016), parallel corpora (Gouws et al., 2015; Luong et al., 2015), document-aligned corpora (Vulic and Moens, 2016). Another line of research is off-line mappingbased approaches (Ruder et al., 2019), where monolingual embeddings are independently trained in multiple languages, and a post-hoc alignment matrix is learned to align the embedding spaces with a seed word dictionary (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), with only a little supervision such as identical strings or numerals (Artetxe et al., 2017; Smith et al., 2017), or even in a completely unsupervised manner (Lample et al., 2018; Artetxe et al., 2018). Mapping-based appr"
2020.acl-main.94,P10-1114,0,0.0167176,"se and domain difference to result in an inaccurate alignment of them. 5 Downstream Tasks Although BLI is a common evaluation method for bilingual embeddings, good performance on BLI does not necessarily generalize to downstream tasks (Glavaˇs et al., 2019). To further gain insight into the effect of the context size on bilingual embeddings, we evaluate the embeddings with three downstream tasks: 1) sentiment analysis; 2) document classification; 3) dependency parsing. Here, we briefly describe the dataset and model used for each task. Sentiment Analysis (SA). We use the Webis-CLS10 corpus10 (Prettenhofer and Stein, 2010), which is comprised of Amazon product reviews in the four languages: English, German, French, and Japanese (no Russian data available). We cast sentiment analysis as a binary classification task, where we label reviews with the scores of 1 or 2 as negative and reviews with 4 or 5 as positive. For the model, we employ a simple CNN encoder followed by a multi-layer perceptrons classifier. 10 https://webis.de/data/webis-cls-10. html 1001 Document Classification (DC). MLDoc11 (Schwenk and Li, 2018) is compiled from the Reuters corpus for eight languages including all the languages used in this pa"
2020.acl-main.94,L18-1560,0,0.0944639,"set and model used for each task. Sentiment Analysis (SA). We use the Webis-CLS10 corpus10 (Prettenhofer and Stein, 2010), which is comprised of Amazon product reviews in the four languages: English, German, French, and Japanese (no Russian data available). We cast sentiment analysis as a binary classification task, where we label reviews with the scores of 1 or 2 as negative and reviews with 4 or 5 as positive. For the model, we employ a simple CNN encoder followed by a multi-layer perceptrons classifier. 10 https://webis.de/data/webis-cls-10. html 1001 Document Classification (DC). MLDoc11 (Schwenk and Li, 2018) is compiled from the Reuters corpus for eight languages including all the languages used in this paper. The task is a four-way classification of the news article topics: Corporate/Industrial, Economics, Government/Social, and Markets. We use the same model architecture as sentiment analysis. Dependency Parsing (DP). We train deep biaffine parsers (Dozat and Manning, 2017) with the UD English EWT dataset12 (Silveira et al., 2014). We use the PUD treebanks13 as test data. The hyperparameters used in this experiment are shown in Appendix B. Evaluation Setup. We evaluate in a cross-lingual transf"
2020.acl-main.94,silveira-etal-2014-gold,0,0.0951158,"Missing"
2020.acl-main.94,P18-1072,0,0.0720707,"Missing"
2020.acl-main.94,P19-1300,0,0.0190163,"languageagnostic semantics of words and thus are easier to be aligned among different languages. This hypothesis can be further supported by looking at the metrics of each part-of-speech (PoS). Intuitively, nouns tend to be more representative of topics than other PoS, and thus are expected to show a high correlation with the window size. Figure 2 shows the scores for each PoS. 8 In all languages, nouns and adjectives show stronger (almost perfect) correlation than verbs and adverbs. 8 We assigned to each word its most frequent PoS tag in the Brown Corpus (Kucera and Francis, 1967), following Wada et al. (2019). 999 Figure 5: BLI performance for each PoS in the different domain setting. Figure 6: BLI performance with the top 500 frequent and rare words in the comparable setting. Figure 7: BLI performance on the top 500 frequent and rare words in the different domain setting. Different-domain Settings. The results so far are obtained in the settings where the source and target corpora are comparable. When the corpora are comparable, it is natural that topical embeddings are easier to be aligned as comparable corpora share their topics. In order to see if the observations from the comparable settings"
2020.acl-main.94,N18-1190,0,0.0162492,"stem is currently a common practice in this field, it should be acknowledged that this may sometimes pose problems: the generated dictionaries are noisy, and the definition of word translation is unclear (e.g., how do we handle polysemy?). It can hinder valid comparisons between systems or detailed analysis of them, and should be addressed in future research. For each setting, we train three pairs of aligned embeddings with different random seeds in the monolingual embedding training, as training word embeddings is known to be unstable and different runs result in different nearest neighbors (Wendlandt et al., 2018). The following results are presented with their averages and standard deviations. 7 https://translate.google.com/ (October 2019) 4 Bilingual Lexicon Induction We first evaluate the learned bilingual embeddings with bilingual lexicon induction (BLI). The task is to retrieve the target translations with source words by searching for nearest neighbors with cosine similarity in the bilingual embedding space. The evaluation metric used in prior work is usually top-k precision, but here we use a more informative measure, mean reciprocal rank (MRR) as recommended by Glavaˇs et al. (2019). Fixed Targ"
2020.acl-main.94,N15-1104,0,0.0335529,"windows in our experiment. The smaller window size captures functional similarities (-s, -cal, -ic), while the larger captures topical similarities. in the form of word dictionaries (Duong et al., 2016), parallel corpora (Gouws et al., 2015; Luong et al., 2015), document-aligned corpora (Vulic and Moens, 2016). Another line of research is off-line mappingbased approaches (Ruder et al., 2019), where monolingual embeddings are independently trained in multiple languages, and a post-hoc alignment matrix is learned to align the embedding spaces with a seed word dictionary (Mikolov et al., 2013b; Xing et al., 2015; Artetxe et al., 2016), with only a little supervision such as identical strings or numerals (Artetxe et al., 2017; Smith et al., 2017), or even in a completely unsupervised manner (Lample et al., 2018; Artetxe et al., 2018). Mapping-based approaches have recently been popularized by their cheaper computational cost compared to joint approaches, as they can make use of pre-trained monolingual word embeddings. The assumption behind the mapping-based methods is the isomorphism of monolingual embedding spaces, i.e., the embedding spaces are structurally similar, or the nearest neighbor graphs fr"
2020.acl-main.94,Q15-1016,0,\N,Missing
2020.acl-main.94,D16-1250,0,\N,Missing
2021.acl-srw.17,P18-1073,0,0.355721,"h the pseudo data from unsupervised machine translation is especially effective for mappingbased CLWEs because (1) the pseudo data makes the source and target corpora (partially) parallel; (2) the pseudo data contains information on the original language that helps to learn similar embedding spaces between the source and target languages. 1 Introduction Cross-lingual word embedding (CLWE) methods aim to learn a shared meaning space between two languages (the source and target languages), which is potentially useful for cross-lingual transfer learning or machine translation (Yuan et al., 2020; Artetxe et al., 2018b; Lample et al., 2018a). Although early methods for learning CLWEs often utilize multilingual resources such as parallel corpora (Gouws et al., 2015; Luong et al., 2015) and word dictionaries (Mikolov et al., 2013), recent studies have focused on fully unsupervised methods that do not require any cross-lingual supervision (Lample et al., 2018b; Artetxe et al., 2018a; Patra et al., 2019). Most unsupervised methods fall into the category of mapping-based methods, which generally consist of the following procedures: train monolingual word embeddings independently in two languages; then, find a l"
2021.acl-srw.17,P19-1494,0,0.211086,"s paper, we argue that we can facilitate structural correspondence of two embedding spaces by augmenting the source or/and target corpora with the output from an unsupervised machine translation system (Lample et al., 2018c). proved by pseudo parallel corpora. Unsupervised Machine Translation Exploiting UMT for Cross-lingual Applications Our proposed method utilizes the output of a USMT system to augment the training corpus for CLWEs. Unsupervised machine translation (UMT) is the task of building a translation system without any parallel corpora (Artetxe et al., 2018b; Lample et al., 2018a,c; Artetxe et al., 2019b). UMT is accomplished by three components: (1) a wordby-word translation model learned using unsupervised CLWEs; (2) a language model trained on the source and target monolingual corpora; (3) a backtranslation model where the model uses input and its own translated output as parallel sentences and learn how to translate them in both directions. More specifically, the initial source-to-target 0 translation model Ps→t is created by the word-byword translation model and the language model of 1 the target language. Then, Pt→s is learned in a supervised setting using the source original monolingu"
2021.acl-srw.17,P19-1019,0,0.238348,"s paper, we argue that we can facilitate structural correspondence of two embedding spaces by augmenting the source or/and target corpora with the output from an unsupervised machine translation system (Lample et al., 2018c). proved by pseudo parallel corpora. Unsupervised Machine Translation Exploiting UMT for Cross-lingual Applications Our proposed method utilizes the output of a USMT system to augment the training corpus for CLWEs. Unsupervised machine translation (UMT) is the task of building a translation system without any parallel corpora (Artetxe et al., 2018b; Lample et al., 2018a,c; Artetxe et al., 2019b). UMT is accomplished by three components: (1) a wordby-word translation model learned using unsupervised CLWEs; (2) a language model trained on the source and target monolingual corpora; (3) a backtranslation model where the model uses input and its own translated output as parallel sentences and learn how to translate them in both directions. More specifically, the initial source-to-target 0 translation model Ps→t is created by the word-byword translation model and the language model of 1 the target language. Then, Pt→s is learned in a supervised setting using the source original monolingu"
2021.acl-srw.17,J82-2005,0,0.703279,"Missing"
2021.acl-srw.17,D15-1075,0,0.0197604,"ssification and define rating values 1-2 as “negative” and 4-5 as “positive”, and exclude the rating 3. Again, we use the CNN-based classifier for this task. Dependency Parsing We train the deep biaffine parser (Dozat and Manning, 2017) with the UD English EWT dataset14 (Silveira et al., 2014). We use the PUD treebanks15 as test data. Natural Language Inference We use the English MultiNLI corpus (Williams et al., 2018) for training and the multilingual XNLI corpus for evaluation (Conneau et al., 2018). XNLI only covers French and German from our experiment. We train the LSTM-based classifier (Bowman et al., 2015), which encodes two sentences, concatenated the representations, and then feed them to a multi-layer perceptron. 12 https://github.com/facebookresearch/ MLDoc https://webis.de/data/webis-cls-10. html 14 https://universaldependencies.org/ treebanks/en_ewt/index.html 15 https://universaldependencies.org/ conll17/ 13 In each task, we train the model using English training data with the embedding parameters fixed . We then evaluate the model on the test data in other target languages. Result and Discussion Table 6 shows the test set accuracy of downstream tasks. For topic classification, our metho"
2021.acl-srw.17,D18-1269,0,0.0183395,"data consists of review texts for amazon products and their ratings from 1 to 5. We cast the problem as binary classification and define rating values 1-2 as “negative” and 4-5 as “positive”, and exclude the rating 3. Again, we use the CNN-based classifier for this task. Dependency Parsing We train the deep biaffine parser (Dozat and Manning, 2017) with the UD English EWT dataset14 (Silveira et al., 2014). We use the PUD treebanks15 as test data. Natural Language Inference We use the English MultiNLI corpus (Williams et al., 2018) for training and the multilingual XNLI corpus for evaluation (Conneau et al., 2018). XNLI only covers French and German from our experiment. We train the LSTM-based classifier (Bowman et al., 2015), which encodes two sentences, concatenated the representations, and then feed them to a multi-layer perceptron. 12 https://github.com/facebookresearch/ MLDoc https://webis.de/data/webis-cls-10. html 14 https://universaldependencies.org/ treebanks/en_ewt/index.html 15 https://universaldependencies.org/ conll17/ 13 In each task, we train the model using English training data with the embedding parameters fixed . We then evaluate the model on the test data in other target languages."
2021.acl-srw.17,N13-1073,0,0.0308463,"l pseudo corpora. We concatenate the pseudo corpora with the original corpora, and learn monolingual word embeddings for each language. Finally, we map these word embeddings to a shared CLWE space with the unsupervised mapping algorithm. Models We compare our method with a baseline with no data augmentation as well as the existing related methods: dictionary induction from a phrase table (Artetxe et al., 2019a) and the unsupervised jointtraining method (Marie and Fujita, 2019). These two methods both exploit word alignments in the pseudo parallel corpus, and to obtain them we use Fast Align8 (Dyer et al., 2013) with the default hyperparameters. For the joint-training method, we adopt bivec9 to train CLWEs with the parameters used in Upadhyay et al. (2016) using the pseudo parallel corpus and the word alignments. To ensure fair comparison, we implement all of these methods with the same UMT system. 4 Evaluation of Cross-lingual Mapping In this section, we conduct a series of experiments to evaluate our method. We first evaluate the performance of cross-lingual mapping in our method (§ 4.1) and investigate the effect of UMT quality (§ 4.2). Then, we analyze why our method improves the bilingual lexico"
2021.acl-srw.17,D16-1235,0,0.0289586,"Missing"
2021.acl-srw.17,P19-1070,0,0.0221621,"Missing"
2021.acl-srw.17,P13-2121,0,0.0595476,"Missing"
2021.acl-srw.17,P07-2045,0,0.00754989,"Having done that, we simply concatenate the machine-translated corpus with the original training corpus, and learn monolingual word embeddings independently for each language. Finally, we map these embeddings to a shared CLWE space. en - fr → ← 19.2 19.1 Corpora We implement our method with two similar language pairs: English-French (en-fr), EnglishGerman (en-de), and one distant language pair: English-Japanese (en-ja). We use plain texts from Wikipedia dumps1 , and randomly extract 10M sentences for each language. The English, French, and German texts are tokenized with the Moses tokenizer (Koehn et al., 2007) and lowercased. For Japanese texts, we use kytea2 to tokenize and normalize them3 . Training mapping-based CLWEs Given tokenized texts, we train monolingual word embeddings using fastText4 with 512 dimensions, a context window of size 5, and 5 negative examles. We then map these word embeddings on a shared embedding space using the open-source implementation VecMap5 with the unsupervised mapping algorithm (Artetxe et al., 2018a). Training UMT models To implement UMT, we first build a phrase table by selecting the most frequent 300,000 source phrases and taking their 200 nearest-neighbors in t"
2021.acl-srw.17,W15-1521,0,0.499497,"tially) parallel; (2) the pseudo data contains information on the original language that helps to learn similar embedding spaces between the source and target languages. 1 Introduction Cross-lingual word embedding (CLWE) methods aim to learn a shared meaning space between two languages (the source and target languages), which is potentially useful for cross-lingual transfer learning or machine translation (Yuan et al., 2020; Artetxe et al., 2018b; Lample et al., 2018a). Although early methods for learning CLWEs often utilize multilingual resources such as parallel corpora (Gouws et al., 2015; Luong et al., 2015) and word dictionaries (Mikolov et al., 2013), recent studies have focused on fully unsupervised methods that do not require any cross-lingual supervision (Lample et al., 2018b; Artetxe et al., 2018a; Patra et al., 2019). Most unsupervised methods fall into the category of mapping-based methods, which generally consist of the following procedures: train monolingual word embeddings independently in two languages; then, find a linear mapping that aligns the two embedding spaces. The mappingbased method is based on a strong assumption that the two independently trained embedding spaces have simil"
2021.acl-srw.17,P19-1312,0,0.307997,"o corpus because it produces better translations than unsupervised neural machine translation on low-resource languages (Lample et al., 2018c). The difference of the unsupervised SMT (USMT) model from its supervised counterpart is that the initial phrase table is derived based on the cosine similarity of unsupervised CLWEs, and the translation model is iteratively imThere is some previous work on how to use UMT to induce bilingual word dictionaries or improve CLWEs. Artetxe et al. (2019a) explored an effective way of utilizing a phrase table from a UMT system to induce bilingual dictionaries. Marie and Fujita (2019) generate a synthetic parallel corpus from a UMT system, and jointly train CLWEs along with the word alignment information (Luong et al., 2015). In our work, we use the synthetic parallel corpus generated from a UMT system not for joint-training but for data augmentation to train monolingual word embeddings for each language, which are subsequently aligned through unsupervised mapping. In the following sections, we empirically show that our approach leads to the creation of improved CLWEs and analyze why these results are achieved. 165 3 Experimental Design In this section, we describe how we"
2021.acl-srw.17,2020.repl4nlp-1.7,0,0.0720816,"Missing"
2021.acl-srw.17,P02-1040,0,0.112399,"their 200 nearest-neighbors in the CLWE space following the setting of Lample et al. (2018c). We then train a 5-gram language model for each language with KenLM (Heafield et al., 2013) and combine it with the phrase table, which results in an unsupervised phrase-based SMT model. Then, we refine the UMT model through three iterative back-translation steps. At each step, we translate 100k sentences randomly sampled from the monolingual data set. We use a phrase table containing phrases up to a length of 4 except for initialization. The quality of our UMT models is indicated by the BLEU scores (Papineni et al., 2002) in Table 1. We use newstest2014 from WMT146 to evaluate En-Fr and En-De translation accuracy and the Tanaka corpus7 for En-Ja evaluation. 1 https://dumps.wikimedia.org/ http://www.phontron.com/kytea/ index-ja.html 3 We convert all alphabets and numbers to half-width, and all katakana to full-width with the mojimoji library https: //github.com/studio-ousia/mojimoji 4 https://fasttext.cc 5 https://github.com/artetxem/vecmap 6 http://www.statmt.org/wmt14/ translation-task.html 7 http://www.edrdg.org/wiki/index.php/ TanakaCorpus 2 en - de → ← 10.3 13.7 en - ja → ← 3.6 1.4 Table 1: BLEU scores of"
2021.acl-srw.17,P19-1018,0,0.0192993,"methods aim to learn a shared meaning space between two languages (the source and target languages), which is potentially useful for cross-lingual transfer learning or machine translation (Yuan et al., 2020; Artetxe et al., 2018b; Lample et al., 2018a). Although early methods for learning CLWEs often utilize multilingual resources such as parallel corpora (Gouws et al., 2015; Luong et al., 2015) and word dictionaries (Mikolov et al., 2013), recent studies have focused on fully unsupervised methods that do not require any cross-lingual supervision (Lample et al., 2018b; Artetxe et al., 2018a; Patra et al., 2019). Most unsupervised methods fall into the category of mapping-based methods, which generally consist of the following procedures: train monolingual word embeddings independently in two languages; then, find a linear mapping that aligns the two embedding spaces. The mappingbased method is based on a strong assumption that the two independently trained embedding spaces have similar structures that can be aligned by a linear transformation, which is unlikely to hold true when the two corpora are from different domains or the two languages are typologically very different (Søgaard et al., 2018). T"
2021.acl-srw.17,2020.acl-main.94,1,0.843565,"eudo corpus. In each cell, the left cell shows the result of MRR, and the right cell shows the result of p@1. tifying word translation pairs, and is a common benchmark for evaluating CLWE methods. In these experiments, we use Cross-Domain Similarity Local Scaling (Lample et al., 2018b) as the method for identifying translation pairs in the two embedding spaces. For BLI scores, we adopt the mean reciprocal rank (MRR) (Glavaˇs et al., 2019) and P@1. We use XLing-Eval10 as test sets for En-Fr and En-Ge. For En-Ja. We create the word dictionaries automatically using Google Translate11 , following Ri and Tsuruoka (2020). Other than BLI from a phrase table, we train three sets of embeddings with different random seeds and report the average of the results. We compare the proposed method with other alternative approaches in BLI as shown in Table 2. In all the language pairs, the mapping method with pseudo data augmentation achieves better performance than the other methods. Here, one may think that the greater amount of data can lead to better performance, and thus augmenting both the source and target corpora shows the best performance. However, the result shows that it is not necessarily the case: for our ma"
2021.acl-srw.17,L18-1560,0,0.0256832,"joint training 70.4 (91.4) 62.5 (70.3) 55.9 (74.7) 44.7 (69.7) mapping 70.4 (92.2) 63.5 (70.7) 17.8 (72.9) - en-ja mapping (+ pseudo) 71.6† (93.3) 62.8 (70.6) 18.1 (73.3) - joint training 66.7 (91.9) 57.3 (66.8) 17.3 (74.8) - Table 6: Results of Downstream tasks. Numbers in parentheses indicate the score of English validation data. The scores indicate averages of 20 experiments with different seeds. Statistically significant correlations are marked with a dagger (p &lt;0.01). (NLI). Topic Classification This task is classifying the topics of news articles. We use the MLDoc 12 corpus compiled by Schwenk and Li (2018). It includes four topics: CCAT (Corporate / Industrial), ECAT (Economics), GCAT (Government / Social), MCAT (Markets). As the classifier, we implemented a simple light-weight convolutional neural network (CNN)-based classifier. Sentiment Analysis In this task, a model is used to classify sentences as either having a positive or negative opinion. We use the Webis-CLS-10 corpus 13 . This data consists of review texts for amazon products and their ratings from 1 to 5. We cast the problem as binary classification and define rating values 1-2 as “negative” and 4-5 as “positive”, and exclude the ra"
2021.acl-srw.17,silveira-etal-2014-gold,0,0.0171968,"Missing"
2021.acl-srw.17,P18-1072,0,0.0375096,"Missing"
2021.acl-srw.17,P16-1157,0,0.0250889,", we map these word embeddings to a shared CLWE space with the unsupervised mapping algorithm. Models We compare our method with a baseline with no data augmentation as well as the existing related methods: dictionary induction from a phrase table (Artetxe et al., 2019a) and the unsupervised jointtraining method (Marie and Fujita, 2019). These two methods both exploit word alignments in the pseudo parallel corpus, and to obtain them we use Fast Align8 (Dyer et al., 2013) with the default hyperparameters. For the joint-training method, we adopt bivec9 to train CLWEs with the parameters used in Upadhyay et al. (2016) using the pseudo parallel corpus and the word alignments. To ensure fair comparison, we implement all of these methods with the same UMT system. 4 Evaluation of Cross-lingual Mapping In this section, we conduct a series of experiments to evaluate our method. We first evaluate the performance of cross-lingual mapping in our method (§ 4.1) and investigate the effect of UMT quality (§ 4.2). Then, we analyze why our method improves the bilingual lexicon induction (BLI) performance. Through carefully controlled experiments, we argue that it is not simply because of data augmentation but because: ("
2021.acl-srw.17,W19-6622,0,0.0182079,"step but does not improve the score at further steps compared to the CLWE without the pseudo data. Marie and Fujita (2019) also demonstrate the same tendency in the CLWE with joint-training. To investigate this point, we compare the lexical densities of the training corpus and the pseudocorpus used in the above experiments (§ 4, 5) using type-token ratio (Table 7). The results demonstrate that the pseudo corpus has a smaller vocabulary per word than the training corpus, and thus it is 17 en→fr fr→en CLWE (no pseudo) 14.7 16.7 18.8 18.8 19.2 19.2 19.1 standardized to some extent as reported in Vanmassenhove et al. (2019). As a result, specific words might be easily mapped in CLWEs using a pseudo corpus18 , and then the translation model makes it easier to translate phrases in more specific patterns. Hence, the model cannot generate diverse data during back-translation, and the accuracy is not improved due to easy learning. 7 Conclusion and Future Work In this paper, we show that training cross-lingual word embeddings with pseudo data augmentation improves performance in BLI and downstream tasks. We analyze the reason for this improvement and found that the pseudo corpus reflects the co-occurrence statistics a"
2021.acl-srw.17,D19-1449,0,0.0379885,"Missing"
2021.acl-srw.17,N18-1101,0,0.0128686,"ither having a positive or negative opinion. We use the Webis-CLS-10 corpus 13 . This data consists of review texts for amazon products and their ratings from 1 to 5. We cast the problem as binary classification and define rating values 1-2 as “negative” and 4-5 as “positive”, and exclude the rating 3. Again, we use the CNN-based classifier for this task. Dependency Parsing We train the deep biaffine parser (Dozat and Manning, 2017) with the UD English EWT dataset14 (Silveira et al., 2014). We use the PUD treebanks15 as test data. Natural Language Inference We use the English MultiNLI corpus (Williams et al., 2018) for training and the multilingual XNLI corpus for evaluation (Conneau et al., 2018). XNLI only covers French and German from our experiment. We train the LSTM-based classifier (Bowman et al., 2015), which encodes two sentences, concatenated the representations, and then feed them to a multi-layer perceptron. 12 https://github.com/facebookresearch/ MLDoc https://webis.de/data/webis-cls-10. html 14 https://universaldependencies.org/ treebanks/en_ewt/index.html 15 https://universaldependencies.org/ conll17/ 13 In each task, we train the model using English training data with the embedding parame"
2021.acl-srw.17,2020.emnlp-main.482,0,0.0322443,"Missing"
2021.acl-srw.17,P19-1307,0,0.10973,"olingual word embeddings independently in two languages; then, find a linear mapping that aligns the two embedding spaces. The mappingbased method is based on a strong assumption that the two independently trained embedding spaces have similar structures that can be aligned by a linear transformation, which is unlikely to hold true when the two corpora are from different domains or the two languages are typologically very different (Søgaard et al., 2018). To address this problem, several studies have focused on improving the structural similarity of monolingual spaces before learning mapping (Zhang et al., 2019; Vuli´c et al., 2020), but few studies have focused on how to leverage the text data itself. In this paper, we show that the pseudo sentences generated from an unsupervised machine translation (UMT) system (Lample et al., 2018c) facilitates the structural similarity without any additional cross-lingual resources. In the proposed method, the training data of the source and/or target language are augmented with the pseudo sentences (Figure 1). We argue that this method facilitates the structural similarity between the source and target embeddings for the following two reasons. Firstly, the sour"
2021.mtsummit-research.19,D17-1098,0,0.0217668,"e evaluation of how well the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018). Constrained decoding can be applied to any existing NMT models without modifying its architecture and training regime, but imposes a significant cost on the decoding speed. It is also unclear how to incorporate lexical inflection into constrained decoding. Therefore, we focus on the placeholder and code-switching methods in this study. 2.4 Modeling Morphological Inflection in Neural Machine Translation Explicitly modeling morphological inflection into NMT models has been studied mainly to enable effective generalization over morphological variatio"
2021.mtsummit-research.19,P16-5005,0,0.0169338,"okens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to t"
2021.mtsummit-research.19,P19-1294,0,0.11253,"he placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Code-switching Method Another way to introduce terminology constraints is the code-switching method (Song et al., 2019; Dinu et al., 2019; Exel et al., 2020). The model is trained with source sentences where some words are replaced or followed by specific target words and expected to copy the words to the translation. One advantage of the code-switching method is that, unlike the placeholder methods, it preserves the meaning of the original words, which likely leads to better translation quality. Also, the model can incorporate the specified terminology in a flexible way: a model trained with the code-switching method not only copies the pre-specified target words but can inflect the words according to the target-side context ("
2021.mtsummit-research.19,2021.dravidianlangtech-1.36,0,0.0828928,"Missing"
2021.mtsummit-research.19,W18-2501,0,0.0155329,"nces. Then, we split the dictionary into noun and verb entries to facilitate the analysis of the results and remove noise. If both the Japanese and English phrases are noun phrases, the entry is registered in the noun dictionary. If the Japanese phrase is a nominal verb5 and English is a verb, the entry is registered in the verb dictionary. In this study, we evaluate the model’s ability to inflect a provided lemma. Lemmas for the target language (English) are obtained with spaCy. 4.3 Models As the baseline, we implement a Transformer (Vaswani et al., 2017) translation model based on AllenNLP (Gardner et al., 2018). We configure the model in the Transformer-base setting and sentences are tokenized using sentencepiece (Kudo, 2018), which has a shared sourcetarget vocabulary of about 16k sub-words. The overviews of lexically constrained models are summarized in Fig. 2. Placeholder (PH). In the placeholder method, the model is trained to translate sentences with a placeholder token and pass that through to the translation. In our experiments, we use different placeholder tokens [NOUN] and [VERB] for nouns and verbs. Predicted placeholder tokens are replaced by the pre-specified term in the post-processing"
2021.mtsummit-research.19,P17-1141,0,0.028964,"l the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is constrained decoding (Anderson et al., 2017; Hokamp and Liu, 2017; Post and Vilar, 2018). Constrained decoding can be applied to any existing NMT models without modifying its architecture and training regime, but imposes a significant cost on the decoding speed. It is also unclear how to incorporate lexical inflection into constrained decoding. Therefore, we focus on the placeholder and code-switching methods in this study. 2.4 Modeling Morphological Inflection in Neural Machine Translation Explicitly modeling morphological inflection into NMT models has been studied mainly to enable effective generalization over morphological variation of words. Tamchyna e"
2021.mtsummit-research.19,N03-1017,0,0.203332,"anese-to-English translation task in the scientific writing domain, and show that our model can incorporate specified terms in the correct form more successfully than other comparable models.1 1 Introduction Over the last several years, neural machine translation (NMT) has pushed the quality of machine translation to near-human performance (Sutskever et al., 2014; Vaswani et al., 2017). However, due to its end-to-end nature, this comes with the cost of losing a certain degree of control over the produced translation, which once was explicitly modeled, for example, in the form of phrase table (Koehn et al., 2003) in statistical machine translation (SMT). In practice, users often want to specify how certain words are translated in order to ensure the consistency of document-level translation or to guarantee the model to produce the correct translation for words that may be underrepresented in the training corpus such as proper nouns, technical terms, or novel words. Given this motivation, a line of previous research has investigated placeholder translation (Post et al., 2019). With a source sentence where certain words are replaced with a special placeholder token, the model produces a translation with"
2021.mtsummit-research.19,P18-1007,0,0.0237552,"oth the Japanese and English phrases are noun phrases, the entry is registered in the noun dictionary. If the Japanese phrase is a nominal verb5 and English is a verb, the entry is registered in the verb dictionary. In this study, we evaluate the model’s ability to inflect a provided lemma. Lemmas for the target language (English) are obtained with spaCy. 4.3 Models As the baseline, we implement a Transformer (Vaswani et al., 2017) translation model based on AllenNLP (Gardner et al., 2018). We configure the model in the Transformer-base setting and sentences are tokenized using sentencepiece (Kudo, 2018), which has a shared sourcetarget vocabulary of about 16k sub-words. The overviews of lexically constrained models are summarized in Fig. 2. Placeholder (PH). In the placeholder method, the model is trained to translate sentences with a placeholder token and pass that through to the translation. In our experiments, we use different placeholder tokens [NOUN] and [VERB] for nouns and verbs. Predicted placeholder tokens are replaced by the pre-specified term in the post-processing step. We evaluate three types of placeholder baselines, each of which differs in what inflected form the target place"
2021.mtsummit-research.19,W16-4602,0,0.0182396,"ecified term in the appropriately inflected form in the translation with higher accuracy than a comparable code-switching method. We also perform a careful error analysis to understand the weaknesses of each system and suggest directions for future work. 2 Related Work 2.1 Placeholder Translation To ensure that certain words appear in the translated sentence, previous studies have explored the method of replacing certain classes of words with special placeholder tokens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, s"
2021.mtsummit-research.19,P15-1002,0,0.0318881,"ethod can include the specified term in the appropriately inflected form in the translation with higher accuracy than a comparable code-switching method. We also perform a careful error analysis to understand the weaknesses of each system and suggest directions for future work. 2 Related Work 2.1 Placeholder Translation To ensure that certain words appear in the translated sentence, previous studies have explored the method of replacing certain classes of words with special placeholder tokens and restore the words in a post-processing step, which we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token wit"
2021.mtsummit-research.19,2020.coling-main.348,0,0.0428395,"h we call placeholder translation in this paper. Luong et al. (2015) and Long et al. (2016) employed placeholder tokens to improve the translation of rare words or technical terms. However, simply replacing words with a unique placeholder token loses the information on the original words. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Co"
2021.mtsummit-research.19,L16-1350,1,0.866457,"MT Research Track Page 231 Specified Translation: 管理 → controlling Source: フローセンサーの原理は浮遊式流量計のテーパー管内フロートの位置を差 動トランスで検出し,これの電圧制御により流量を[VERB]する。 Reference: The sensor controls the flow rate by detecting the position of the float in the tepered tube with a a differential transformer and [VERB] it with the obtained voltage. System Output: The principle of the flow sensor is that the position of the float in the taper tube of the floating flowmeter is detected by the differential transformer, and the flow rate is [VERB] by this voltage control. Table 1: A translation example from the ASPEC corpus (Nakazawa et al., 2016) with a placeholder translation model. The specified target term grammatically fits the placeholder in the reference, but not in the system output as it is. the produced translation. To illustrate the problem, we show an actual output from a normal placeholder translation model in Japanese to English translation in Table 1. The system is supposed to translate the word 管理 into controlling as in the reference, but the output has a different grammatical construction and thus the progressive form controlling is invalid in this context; instead, controlled should be injected in the placeholder. The"
2021.mtsummit-research.19,2021.eacl-main.70,0,0.0209484,"th source sentences where some words are replaced or followed by specific target words and expected to copy the words to the translation. One advantage of the code-switching method is that, unlike the placeholder methods, it preserves the meaning of the original words, which likely leads to better translation quality. Also, the model can incorporate the specified terminology in a flexible way: a model trained with the code-switching method not only copies the pre-specified target words but can inflect the words according to the target-side context (Dinu et al., 2019). In parallel to our work, Niehues (2021) offers a quantitative evaluation of how well the code-switching method handles inflection of a pre-specified terminology when the terminology is given in the lemma form. Although the code-switching method is flexible, one disadvantage is that it tends to ignore the pre-specified terminology more often than the placeholder method (§5). We propose a placeholder method that handles inflection of pre-specified terms, aiming for both flexibility and faithfulness to terminology constraints. 2.3 Constrained Decoding Another approach to ensure that a pre-specified term appears in the translation is c"
2021.mtsummit-research.19,P02-1040,0,0.109172,"are code-switched, but we observe no significant difference by adding source factors. Therefore, we simply report the results from the model with minimal components. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 236 and then only update the parameters of the additional modules. In this second training stage, we use the loss value as validation metric and stop the training when the lowest value is updated for 5 epochs. 5 Results 5.1 Evaluation For each model, we evaluate the overall translation quality with BLEU (Papineni et al., 2002).8 We also evaluate the specified term use rate, a metric to check if the model correctly includes the specified target term. Note that this is only an approximate measure of what we want to measure: whether the specified term is used in the correct form in the output translation. Since a single source sentence can be translated into different grammatical constructions, it is possible that the inflected form in the system output is different from the one in the reference but still correct in the context. Still, we find a substantial overlap in the inflectional form of the specified term betwee"
2021.mtsummit-research.19,W18-6319,0,0.033639,"Missing"
2021.mtsummit-research.19,W19-6618,0,0.0504017,"n degree of control over the produced translation, which once was explicitly modeled, for example, in the form of phrase table (Koehn et al., 2003) in statistical machine translation (SMT). In practice, users often want to specify how certain words are translated in order to ensure the consistency of document-level translation or to guarantee the model to produce the correct translation for words that may be underrepresented in the training corpus such as proper nouns, technical terms, or novel words. Given this motivation, a line of previous research has investigated placeholder translation (Post et al., 2019). With a source sentence where certain words are replaced with a special placeholder token, the model produces a translation with the special placeholder token in an appropriate position, and then that placeholder token is replaced with a pre-specified term in a post-processing step. Although this approach ensures that certain words appear in the translation, one limitation is that the user must specify the term that fits in the context surrounding the placeholder token, or specifically, the term should be properly inflected according to the syntactic structure of 1 Code is available at https:"
2021.mtsummit-research.19,N19-1044,0,0.0742236,"lly in translation between grammatically distant languages, such as Japanese and English. As manually correcting the inflection in post-editing significantly hurts the convenience of placeholder translation, we need a way to automatically handle inflection. One possible approach to this problem is the code-switching methods, in which certain words in the source sentence are replaced with the specific target words, and the model is encouraged to include those specific words in the translation. This approach is flexible in that the model can inflect the specified words according to the context (Song et al., 2019), but less faithful to the lexical constraints, often ignoring the specified terms (§5). To address this problem, we propose a model that automatically inflects a pre-specified term according to the context of the produced translation. We extend the sequence-to-sequence encoder and decoder with an additional character-level decoder that predicts the inflected form of the pre-specified term. Our approach combines the advantages of both the placeholder and the code-switching methods: the faithfulness to lexical constraints and the flexibility of dynamically deciding the word form in the output."
2021.mtsummit-research.19,W17-4704,0,0.0422079,"Missing"
2021.mtsummit-research.19,W17-4742,0,0.0141885,"ords. To alleviate this problem, subProceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 232 sequent studies distinguish different types of placeholders, such as named entity types (Crego et al., 2016; Post et al., 2019) or parts-of-speech (Michon et al., 2020). Instead of replacing the placeholder token with a dictionary entry, some studies propose generating the content of the placeholder with a character-level sequence-to-sequence model to translate words not covered in the bilingual dictionary. Li et al. (2016) and Wang et al. (2017) incorporated a named entity translator, which is supposed to learn transliteration of named entities. As in their work, our proposed model also uses a character-level decoder to generate the content of placeholders, but our focus is to inflect a lemma to the appropriately inflected form given the context. 2.2 The Code-switching Method Another way to introduce terminology constraints is the code-switching method (Song et al., 2019; Dinu et al., 2019; Exel et al., 2020). The model is trained with source sentences where some words are replaced or followed by specific target words and expected to"
2021.mtsummit-research.19,2020.acl-main.389,0,0.0871391,"Missing"
2021.wat-1.11,2020.coling-main.304,0,0.0835648,"Missing"
2021.wat-1.11,N18-1118,0,0.0194207,"l computation at inference time, but significantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation o"
2021.wat-1.11,D13-1095,0,0.611688,"most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model learn heuristic relations between ZPs and local context within the sentence (Hangyo et al., 2013; Kudo et al., 2015) and produce appropriate English pronouns. 2.3 sentence (Kudo et al., 2015), and incorporated into the resulting translation. On the other hand, in neural machine translation, the missing pronouns can be automatically inferred by the translation model because of the nature of end-to-end learning, although the correctness cannot be guaranteed. To improve the quality of ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals"
2021.wat-1.11,P09-2022,0,0.116993,"Missing"
2021.wat-1.11,W03-1024,0,0.856685,"e also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let t"
2021.wat-1.11,2020.acl-main.321,0,0.0160392,"cantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresp"
2021.wat-1.11,A92-1028,0,0.643315,"lel corpus in the conversational domain. Besides the published data, we also use the in-house version of the corpus, which amounts to a total of 104,961 sentence pairs. 3.1 Identifying sentence pairs that contain ZPs. As the corpus does not contain annotations of ZPs, we first identify sentence pairs that contain zero pronouns. We exploit the word alignment information from parallel sentences to detect ZPs. The specific procedure is as follows. ZPs in Translation In the context of statistical machine translation, Japanese ZPs are explicitly predicted by considering verbal semantic attributes (Nakaiwa and Ikehara, 1992), local context in the source and target 118 1. We obtain the word alignments of the parallel data with GIZA++1 . We use Mecab2 for Japanese word segmentation, spaCy3 for English. 2. When a pronoun in an English sentence is associated with NULL, the pronoun in the English sentence is considered to correspond to a ZP in the Japanese sentence. The resulting number of pronouns is shown in Figure 2. It can be seen that in the conversational domain, the first person pronoun I and the second 1 https://github.com/moses-smt/giza-pp https://taku910.github.io/mecab/ 3 https://spacy.io/ 2 baseline logist"
2021.wat-1.11,P02-1040,0,0.109125,"ore difficult for the model to find correlations between ZPs and local context. 5 Conclusion Table 3: The number of sentences in the corpus. Model Transformer (Vaswani et al., 2017) was used as the translation model. We adopt the hyperparameters recommended for the corpus of our size in Araabi and Monz (2020) (Appendix B). In addition to the single-sentence translation, we also experimented with the 2to1 setting (Tiedemann and Scherrer, 2017), in which the previous sentence in the document is added to the input. Evaluation We evaluate the overall translation quality on the test set with BLEU (Papineni et al., 2002). We also conduct a targeted evaluation with the ZP evaluation dataset for Japanese-to-English translation (Shimazu et al., 2020). The ZP evaluation dataset contains 724 triples of a source sentence, a target sentence with a correct pronoun, and one with an incorrect pronoun. To evaluate a translation model, we see if the model assigns a lower perplexity to the correct target sentence, and calculate the accuracy. 4.2 To address the problem of zero pronoun translation, we proposed zero pronoun data augmentation. Through the analysis with the Japanese-English conversational parallel corpus, we s"
2021.wat-1.11,D19-5204,1,0.850314,"ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals useful to correctly translate ZPs. 3 Is Local Context Useful for Predicting Zero Pronouns? Our proposed method is based on the assumption that local context in Japanese sentences is useful for predicting ZPs. We begin by analyzing to what extent ZPs can be inferred from local context, and what kind of local context is useful. For the analysis, we use the Business Scene Dialogue Corpus (Rikters et al., 2019), which is a Japanese and English parallel corpus in the conversational domain. Besides the published data, we also use the in-house version of the corpus, which amounts to a total of 104,961 sentence pairs. 3.1 Identifying sentence pairs that contain ZPs. As the corpus does not contain annotations of ZPs, we first identify sentence pairs that contain zero pronouns. We exploit the word alignment information from parallel sentences to detect ZPs. The specific procedure is as follows. ZPs in Translation In the context of statistical machine translation, Japanese ZPs are explicitly predicted by c"
2021.wat-1.11,2020.wmt-1.74,1,0.843243,"Missing"
2021.wat-1.11,C08-1097,0,0.67291,"of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model"
2021.wat-1.11,2020.acl-main.693,0,0.0228025,"he accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntac"
2021.wat-1.11,P18-1054,0,0.012562,"machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challenge when the corresponding pronoun is syntactically required on the target language side: the model has to infer the omitted pronoun. The task of identifying the omitted pronouns is called ZP resolution and for Japanese, this has been a long-standing problem (Isozaki and Hirao, 2003; Sasano et al., 2008; Imamura et al., 2009; Shibata and Kurohashi, 2018). Japanese is one of the most difficult languages because Japanese words usually do not have any inflectional forms that depend on the omitted pronoun, unlike other pro-drop languages such as Portuguese and Spanish in which ZPs can be inferred from the grammatical case of other words. Still, Japanese sentences sometimes contain expressions indicative of the missing pronoun. For example, Japanese honorifics naturally indicate the subject is the second person. In this work, we do not explicitly solve ZP resolution but let the translation model learn heuristic relations between ZPs and local cont"
2021.wat-1.11,2020.lrec-1.447,1,0.859112,"Missing"
2021.wat-1.11,W17-4811,0,0.0610415,"Missing"
2021.wat-1.11,D19-1081,0,0.0378569,"Missing"
2021.wat-1.11,P19-1116,0,0.0173133,"rence time, but significantly improves the accuracy of the ZP translation. 117 Proceedings of the 8th Workshop on Asian Translation, pages 117–123 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics 2 2.1 Related Work Contextual Neural Machine Translation As the quality of single-sentence machine translation has improved dramatically with the advent of neural machine translation (Sutskever et al., 2014; Vaswani et al., 2017), translation models that take wider contexts into account have seen a surge of interest (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2019b,a; Ma et al., 2020; Saunders et al., 2020). In contrast to the studies trying to incorporate information outside the sentence, in this work, we propose a method to improve zeropronoun translation by only considering the information within the sentence, but we also explore the effect of combining our method with a contextual machine translation model. 2.2 ZP Resolution in Japanese In some languages, pronouns are sometimes omitted when they are inferable from the context. Such languages are called pro-drop languages and the omitted pronouns are called ZPs. The translation of ZPs poses a challe"
2021.wat-1.11,D19-1085,0,0.0351255,"Missing"
2021.wat-1.11,N16-1113,0,0.022761,"t the translation model learn heuristic relations between ZPs and local context within the sentence (Hangyo et al., 2013; Kudo et al., 2015) and produce appropriate English pronouns. 2.3 sentence (Kudo et al., 2015), and incorporated into the resulting translation. On the other hand, in neural machine translation, the missing pronouns can be automatically inferred by the translation model because of the nature of end-to-end learning, although the correctness cannot be guaranteed. To improve the quality of ZP translation, previous studies have explored a multi-task approach with ZP prediction (Wang et al., 2016, 2019). In this study, we propose a ZP data augmentation method to provide additional training signals useful to correctly translate ZPs. 3 Is Local Context Useful for Predicting Zero Pronouns? Our proposed method is based on the assumption that local context in Japanese sentences is useful for predicting ZPs. We begin by analyzing to what extent ZPs can be inferred from local context, and what kind of local context is useful. For the analysis, we use the Business Scene Dialogue Corpus (Rikters et al., 2019), which is a Japanese and English parallel corpus in the conversational domain. Beside"
C08-1106,W99-0606,0,0.0124845,"der generative probabilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synt"
C08-1106,W02-1001,0,0.218668,"Missing"
C08-1106,N01-1025,0,0.218911,"cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conventionally labeled by ‘BO"
C08-1106,W02-2018,0,0.0112059,"g previous studies on shallow parsing, our experiments are performed on the CoNLL 2000 LDCRF for Shallow Parsing We implemented LDCRFs in C++, and optimized the system to cope with large scale problems, in which the feature dimension is beyond millions. We employ similar predicate sets defined in Sha & Pereira (2003). We follow them in using predicates that depend on words as well as POS tags in the neighborhood of a given position, taking into account only those 417,835 features which occur at least once in the training data. The features are listed in Table 3. As for numerical optimization (Malouf 2002; Wallach 2002), we performed gradient decent with the Limited-Memory BFGS (L-BFGS) optimization technique (Nocedal & Wright 1999). L-BFGS is a second-order Quasi-Newton method that numerically estimates the curvature from previous gradients and updates. With no requirement on specialized Hessian approximation, L-BFGS can handle large-scale problems in an efficient manner. We implemented an L-BFGS optimizer in C++ by modifying the OWLQN package (Andrew & Gao 2007) developed by Galen Andrew. In our experiments, storing 10 pairs of previous gradients for the approximation of the function’s inver"
C08-1106,P05-1010,1,0.787072,"as shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the concept of latentdynamics for shallow parsing, showing how hidden states automatically learned by the model present similar characteristics. We"
C08-1106,H05-1124,0,0.502342,"ssifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on POS tagging tasks. As for the task of shallow parsing, CRFs also outperform many other state-of-the-art models (Sha & Pereira 2003; McDonald et al. 2005). When the data has distinct sub-structures, models that exploit hidden state variables are advantageous in learning (Matsuzaki et al. 2005; Petrov et al. 2007). Sutton et al. (2004) presented an extension to CRF called dynamic conditional random field (DCRF) model. As stated by the authors, training a DCRF model with unobserved nodes (hidden variables) makes their approach difficult to optimize. In the vision community, the LDCRF model was recently proposed by Morency et al. (2007), and shown to outperform CRFs, SVMs, and HMMs for visual sequence labeling. In this paper, we introduce the conc"
C08-1106,W95-0107,0,0.054769,"Missing"
C08-1106,W96-0213,0,0.150328,"abilistic models of paired input sequences and label sequences, such as HMMs (Freitag & McCallum 2000; Kupiec 1992) or multilevel Markov models (Bikel et al. 1999). The generative model provides well-understood training and inference but requires stringent conditional independence assumptions. To accommodate multiple overlapping features on observations, some other approaches view the sequence labeling problem as a sequence of classification problems, including support vector machines (SVMs) (Kudo & Matsumoto 2001) and a variety of other classifiers (Punyakanok & Roth 2001; Abney et al. 1999; Ratnaparkhi 1996). Since these classifiers cannot trade off decisions at different positions against each other (Lafferty et al. 2001), the best classifier based shallow parsers are forced to resort to heuristic combinations of multiple classifiers. A significant amount of recent work has shown the power of CRFs for sequence labeling tasks. CRFs use an exponential distribution to model the entire sequence, allowing for non-local dependencies between states and observations (Lafferty et al. 2001). Lafferty et al. (2001) showed that CRFs outperform classification models as well as HMMs on synthetic data and on P"
C08-1106,W00-0726,0,0.0690631,"33 1.00 1.00 0.98 1.00 1.00 1.00 1.00 0.99 0.99 0.88 0.73 0.67 1.00 1.00 0.62 0.94 0.93 0.92 0.97 0.94 0.92 Word Features: {wi−2 , wi−1 , wi , wi+1 , wi+2 , wi−1 wi , wi wi+1 } ×{hi , hi−1 hi , hi−2 hi−1 hi } POS Features: {ti−1 , ti , ti+1 , ti−2 ti−1 , ti−1 ti , ti ti+1 , ti+1 ti+2 , ti−2 ti−1 ti , ti−1 ti ti+1 , ti ti+1 ti+2 } ×{hi , hi−1 hi , hi−2 hi−1 hi } Table 3: Feature templates used in the experiments. wi is the current word; ti is current POS tag; and hi is the current hidden state (for the case of latent models) or the current label (for the case of conventional models). data set (Sang & Buchholz 2000; Ramshow & Marcus 1995). The training set consists of 8,936 sentences, and the test set consists of 2,012 sentences. The standard evaluation metrics for this task are precision p (the fraction of output chunks matching the reference chunks), recall r (the fraction of reference chunks returned), and the Fmeasure given by F = 2pr/(p + r). 6.1 Table 2: Latent-dynamics learned automatically by the LDCRF model. This table shows the top three words and their gold-standard POS tags for each hidden states. lar roles in modeling the dynamics in shallow parsing. Further, the singular proper nouns and t"
C08-1106,N03-1028,0,0.245876,"s the non-recursive cores of various phrase types in text. The paradigmatic shallow parsing problem is noun phrase chunking, in which the non-recursive cores of noun phrases, called base NPs, are identified. As the representative problem in shallow parsing, noun phrase chunking has received much attention, with the development of standard evaluation datasets and with c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. extensive comparisons among methods (McDonald 2005; Sha & Pereira 2003; Kudo & Matsumoto 2001). Syntactic contexts often have a complex underlying structure. Chunk labels are usually far too general to fully encapsulate the syntactic behavior of word sequences. In practice, and given the limited data, the relationship between specific words and their syntactic contexts may be best modeled at a level finer than chunk tags but coarser than lexical identities. For example, in the noun phrase (NP) chunking task, suppose that there are two lexical sequences, “He is her –” and “He gave her – ”. The observed sequences, “He is her” and “He gave her”, would both be conve"
C08-1106,A00-2007,0,\N,Missing
C10-2015,W06-2920,0,0.0858243,"accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a sho"
C10-2015,D07-1101,0,0.167428,"d parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph"
C10-2015,I08-1012,1,0.900686,"Missing"
C10-2015,D09-1060,1,0.694572,"by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 show"
C10-2015,N06-1021,0,0.0247458,"Missing"
C10-2015,P04-1054,0,0.037329,"e long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Ni"
C10-2015,C96-1058,0,0.451123,"s in w10 A w3 . This simple example shows how to use the decision history to help parse the long distance dependencies. 3 Background: graph-based parsing models Before we describe our method, we briefly introduce the graph-based parsing models. We denote input sentence w by w = (w0 , w1 , ..., wn ), where w0 = ROOT is an artificial root token inserted at the beginning of the sentence and does not depend on any other token in w and wi refers to a word. We employ the second-order projective graphbased parsing model of Carreras (2007), which is an extension of the projective parsing algorithm of Eisner (1996). The parsing algorithms used in Carreras (2007) independently find the left and right dependents of a word and then combine them later in a bottomup style based on Eisner (1996). A subtree that spans the words in [s, t] (and roots at s or t) is represented by chart item [s, t, right/lef t, C/I], where right (left) indicates that the root of the subtree is s (t) and C means that the item is complete while I means that the item is incomplete (McDonald, 2006). Here, complete item in the right (left) direction means that the words other than s (t) cannot have dependents outside [s, t] and incompl"
C10-2015,P07-1050,0,0.0219216,"93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking frameworks to achieve higher accuracy. 7 Conclusions This paper proposes an approach for improving graph-based dependency parsing by using the decision history. For the graph-based model, we design a set of features over"
C10-2015,P08-1068,0,0.158647,"ub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing,"
C10-2015,J93-2004,0,0.0336939,"history-based features for all the word pairs without any restriction. • Sub-sentences: use the history-based features only for the relation of two words from sub-sentences. Here, we use punctuation marks to split sentences into sub-sentences. • Distance: use the history-based features for the relation of two words within a predefined distance. We set the thresholds to 3, 5, and 10. 5 Experimental results and our new systems OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For"
C10-2015,D07-1013,0,0.0985631,"pendency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as repor"
C10-2015,E06-1011,0,0.179838,"ted by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach"
C10-2015,D07-1100,0,0.0140656,"systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models . Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graphbased model by introducing the Gibbs sampling algorithm. Compared with their approach, our approach is much simpler yet effective. Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser. Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. One parser uses dependency predictions made by another parser. Our results show that our approach can be used in the stacking fr"
C10-2015,2006.iwslt-evaluation.9,0,0.0170896,"models and may be used as features to help parse long dependencies. The mechanism can easily be implemented by modifying a graphbased parsing model and introducing a set of new features. The experimental results show that our system achieves state-ofthe-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese. 1 Introduction Dependency parsing is an approach to syntactic analysis inspired by dependency grammar. In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for tra"
C10-2015,P08-1108,0,0.0909482,"n2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Cor"
C10-2015,W04-2407,0,0.0205074,"4). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in later scans. In the botto"
C10-2015,W96-0213,0,0.215296,"ms OURS. In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et"
C10-2015,D09-1058,0,0.0509405,"tem achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Complete 44.28 45.24 38.4 37.6 45.4 47.06 – 47.15 – based and transition-based models. OURS performed better than Z&C 2008, but worse than STACK. The last three systems that used largescale unlabeled data performed better than OURS. 6 Related work There are seve"
C10-2015,W03-3023,0,0.326783,"on (Culotta and Sorensen, 2004). Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). In this paper, we propose an approach to improve graph-based dependency parsing by using decision history. Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance. This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in lat"
C10-2015,C08-1132,0,0.0733273,"ned on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set. For Chinese, we used the Chinese Treebank (CTB) version 4.04 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens with the correct HEAD 5 . And we also evaluated on complete dependency analysis. In our experiments, we implemented our systems on the MSTParser6 and extended with the parent-child-grandchild structures (McDonald and Pereira, 2006; Carreras, 2007). For the baseline systems, we used the first- and second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007). In the following se"
C10-2015,D08-1059,0,0.136222,"hat OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improvement. In future work, we can combine our approach with the parser of Chen et al. (2009). Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al. (2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of Koo et al. (2008), Chen2009 refers to the parser of Chen et al. (2009), and Suzuki2009 refers to the parser of Suzuki et al. (2009) that is the best reported result for this data. The results shows that OURS outperformed the first two systems that were based on single models. Z&C 2008 and STACK were the combination systems of graph7 Baseline OURS Y&M2003 CO2006 Z&C2008 STACK KOO2008 Chen2009 Suzuki2009 UAS 91.92 92.21 (+0.29) 90.3 90.8 92.1 92.53 93.16 93.16 93.79 Comple"
C10-2015,P09-1007,0,0.0129002,"s are shown in parentheses. The results show that OURS provided better performance over the Baselines by 1.02 points for Chi132 Table 5: Results for English Table 4: Results for Chinese Baseline OURS OURS+STACK Zhao2009 Yu2008 STACK Chen2009 UAS 88.41 89.43(+1.02) 89.53 87.0 87.26 88.95 89.91 Complete 48.85 50.86 49.42 – – 49.42 48.56 nese and 0.29 points for English. The improvements of (OURS) were significant in McNemar’s Test with p &lt; 10−4 for Chinese and p &lt; 10−3 for English. 5.3 Comparative results Table 4 shows the comparative results for Chinese, where Zhao2009 refers to the result of (Zhao et al., 2009), Yu2008 refers to the result of Yu et al. (2008), Chen2009 refers to the result of Chen et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser7 . The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used largescale unlabeled data (Chen et al., 2009). We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4. The new system achieved further improv"
C10-2015,D07-1096,0,\N,Missing
D08-1047,H05-1120,0,0.606315,"p yoshimasa.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,I08-1007,0,0.0973065,"native framework of string similarity. MaCallum et al. (2005) proposed a method to train the costs of edit operations using Conditional Random Fields (CRFs). Bergsma and Kondrak (2007) correct comparative and superlative adjectives, e.g., unpopular → unpopularer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary loo"
D08-1047,J96-1002,0,0.0553682,"be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applicati"
D08-1047,P07-1083,0,0.0802925,"ormance of the L1 regularized logistic regression as a discriminative model, we also built two classifiers based on the Support Vector Machine (SVM). These SVM classifiers were implemented by the SVMperf 7 on a linear kernel8 . An SVM classifier employs the same feature set (substitution rules) as the proposed method so that we can directly compare the L1 regularized logistic regression and the linear-kernel SVM. Another SVM classifier incorporates the five string metrics; this system can be considered as our reproduction of the discriminative string similarity proposed by Bergsma and Kondrak (2007). Table 3 reports the precision (P), recall (R), and F1 score (F1) based on the number of correct decisions for positive instances. The proposed method outperformed the baseline systems, achieving 0.919, 0.888, and 0.984 of F1 scores, respectively. Porter’s stemmer worked on the Inflection set, but not on the Orthography set, which is beyond the scope of the stemming algorithms. CST’s lemmatizer suffered from low recall on the Inflection set because it removed suffixes of base forms, e.g., (cloning, clone) → (clone, clo). Morpha and CST’s lemma6 We used CST’s lemmatiser version 2.13: http://ww"
D08-1047,P00-1037,0,0.103699,"ary of the model. The advantage of this approach is that candidate strings can be enumerated by an efficient algorithm because the processes of string transformation are tractable in the model. We demonstrate the remarkable performance of the proposed method in normalizing inflected words and spelling variations. 1 t∗ = argmax P (t|s). (1) t∈gen(s) Here, the candidate generator gen(s) enumerates candidates of destination (correct) strings, and the scorer P (t|s) denotes the conditional probability of the string t for the given s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in whi"
D08-1047,J95-4004,0,0.0169888,"iven s. The scorer was modeled by a noisy-channel model (Shannon, 1948; Brill and Moore, 2000; Ahmad and Kondrak, 2005) and maximum entropy framework (Berger et al., 1996; Li et al., 2006; Chen et al., 2007). The candidate generator gen(s) also affects the accuracy of the string transformation. Previous studies of spelling correction mostly defined gen(s), gen(s) = {t |dist(s, t) &lt; δ}. Introduction String transformation maps a source string s into its destination string t∗ . In the broad sense, string transformation can include labeling tasks such as partof-speech tagging and shallow parsing (Brill, 1995). However, this study addresses string transformation in its narrow sense, in which a part of a source string is rewritten with a substring. Typical applications of this task include stemming, lemmatization, spelling correction (Brill and Moore, 2000; Wilbur et al., 2006; Carlson and Fette, 2007), OCR error correction (Kolak and Resnik, 2002), approximate string (2) Here, the function dist(s, t) denotes the weighted Levenshtein distance (Levenshtein, 1966) between strings s and t. Furthermore, the threshold δ requires the distance between the source string s and a candidate string t to be less"
D08-1047,D07-1019,0,0.710798,"ia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative instances that affect the decision b"
D08-1047,dalianis-jongejan-2006-hand,0,0.022719,"larer → unpopularest and refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,2005.mtsummit-papers.40,0,0.165856,"a.tsuruoka@manchester.ac.uk Sophia Ananiadou‡ Jun’ichi Tsujii†‡ sophia.ananiadou@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp † Graduate School of Information Science and Technology University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo 113-8656, Japan ‡ School of Computer Science, University of Manchester National Centre for Text Mining (NaCTeM) Manchester Interdisciplinary Biocentre 131 Princess Street, Manchester M1 7DN, UK Abstract matching (Navarro, 2001), and duplicate record detection (Bilenko and Mooney, 2003). Recent studies have formalized the task in the discriminative framework (Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007), String transformation, which maps a source string s into its desirable form t∗ , is related to various applications including stemming, lemmatization, and spelling correction. The essential and important step for string transformation is to generate candidates to which the given string s is likely to be transformed. This paper presents a discriminative approach for generating candidate strings. We use substring substitution rules as features and score them using an L1 -regularized logistic regression model. We also propose a procedure to generate negative"
D08-1047,P06-1129,0,0.339768,"nd refundable → refundabler → refundablest. Therefore, we removed inflection entries for comparative and superlative adjectives from the dataset. presented an alignment-based discriminative string similarity. They extracted features from substring pairs that are consistent to a character-based alignment of two strings. Aramaki et al. (2008) also used features that express the different segments of the two strings. However, these studies are not suited for a candidate generator because the processes of string transformations are intractable in their discriminative models. Dalianis and Jongejan (2006) presented a lemmatiser based on suffix rules. Although they proposed a method to obtain suffix rules from a training data, the method did not use counter-examples (negatives) for reducing incorrect string transformations. Tsuruoka et al. (2008) proposed a scoring method for discovering a list of normalization rules for dictionary look-ups. However, their objective was to transform given strings, so that strings (e.g., studies and study) referring to the same concept in the dictionary are mapped into the same string (e.g., stud); in contrast, this study maps strings into their destination stri"
D08-1047,J99-1003,0,0.038867,"f the tasks: classification (Section 3.2) and normalization (Section 3.3). 3.2 Experiment 1: Candidate classification In this experiment, we measured the performance of the classification task in which pairs of strings were assigned with positive or negative labels. We trained and evaluated the proposed method by performing ten-fold cross validation on each dataset5 . Eight baseline systems were prepared for comparison: Levenshtein distance (LD), normalized Levenshtein distance (NLD), Dice coefficient on letter bigrams (DICE) (Adamson and Boreham, 1974), Longest Common Substring Ratio (LCSR) (Melamed, 1999), Longest Common Prefix Ratio (PREFIX) (Kondrak, 2005), Porter’s stemmer (Porter, 1980), Morpha (Minnen et al., 2001), and CST’s lemmatiser (Dalianis and Jonge3 LRSPL table includes trivial spelling variants that can be handled using simple character/string operations. For example, the table contains spelling variants related to case sensitivity (e.g., deg and Deg) and symbols (e.g., Feb and Feb.). 4 LRAGR table also provides agreement information even when word forms do not change. For example, the table contains an entry indicating that the first-singular present form of the verb study is st"
D08-1047,P99-1004,0,\N,Missing
D11-1007,D08-1092,0,0.318764,"design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual tre"
D11-1007,D07-1101,0,0.144195,"sponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x an"
D11-1007,D09-1060,1,0.927841,"list of the target monolingual subtrees 1 For the second order features, Dir is the combination of the directions of two dependencies. or bilingual subtrees, this constraint will probably be reliable. We first parse the large-scale unannotated monolingual and bilingual data. Subsequently, we extract the monolingual and bilingual subtrees from the parsed data. We then verify the bilingual constraints using the extracted subtrees. Finally, we generate the bilingual features based on the verified results for the parsing models. 5.1 Verified constraint functions 5.1.1 Monolingual target subtrees Chen et al. (2009) proposed a simple method to extract subtrees from large-scale monolingual data and used them as features to improve monolingual parsing. Following their method, we parse large unannotated data with the Parsert and obtain the subtree list (STt ) on the target side. We extract two types of subtrees: bigram (two words) subtree and trigram (three words) subtree. 5.1.2 Verified target constraint function: Fvt (rtk ) We use the extracted target subtrees to verify the rtk of the bilingual constraints. In fact, rtk is a candidate subtree. If the rtk is included in STt , function Fvt (rtk ) = T ype(rt"
D11-1007,P10-1003,1,0.243719,"based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both si"
D11-1007,P07-1003,0,0.126522,"ale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that"
D11-1007,D09-1127,0,0.0657088,"Missing"
D11-1007,N03-1017,0,0.0160094,"Missing"
D11-1007,P10-1001,0,0.0121108,"ley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a future study. Suppose that we have a (candidate) dependency relation rs that can be a bigram or trigram dependency. We examine whether the corresponding words of the source words of rs have a dependency relation rt in the target trees. We also consider the direction of the dependency relation. The corresponding word of the head should also be the head in rt . We define a binary function for this bilingual constraint: Fbn (rsn : rtk ), where n and k refers to the types of the dependencies (2 for bigram and 3 for trigram). For example, in rs2 : rt3 , rs2 is a bigram dependency on the sour"
D11-1007,P09-1058,1,0.820133,"//www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same procedure as we did for the BLLIP corpus. Word alignment was performed using the Berkeley Aligner. We reported the parser quality by the UAS, i.e., the percentage of tokens (excluding all punctuation tokens) with correct HEADs. 6.1 Experimental settings For baseline systems, we used the monolingual features mentioned in Section 3. We called these features basic features. To compare the results of (Burk"
D11-1007,N06-1014,0,0.079112,"ed by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order models (Koo and Collins, 2010) for a fu"
D11-1007,P10-5002,0,0.0237031,"able is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree structures on both sides. Huang et al. (2009) presented a method to train a source-language parser by using the reordering information on words between the sentences on two sides. It uses another type of bilingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen"
D11-1007,J93-2004,0,0.0430468,"ingual treebanks that have tree structures on the source sentences and their human-translated sentences. Chen et al. (2010) also used bilingual treebanks and made use of tree structures on the target side. However, the bilingual treebanks are hard to obtain, partly because of the high cost of human translation. Thus, in their experiments, they applied their methods to a small data set, the manually translated portion of the Chinese Treebank (CTB) which contains only about 3,000 sentences. On the other hand, many large-scale monolingual treebanks exist, such as the Penn English Treebank (PTB) (Marcus et al., 1993) (about 40,000 sentences in Version 3) and the latest version of CTB (over 50,000 sentences in Version 7). In this paper, we propose a bitext parsing approach in which we produce the bilingual constraints on existing monolingual treebanks with the help of SMT systems. In other words, we aim to improve source-language parsing with the help of automatic translations. In our approach, we first use an SMT system to translate the sentences of a source monolingual treebank into the target language. Then, the target sentences are parsed by a parser trained on a target monolingual treebank. We then ob"
D11-1007,E06-1011,0,0.17547,"ure, the word “skills” corresponding to “技巧(jiqiao)/skill” is a grandchild of the word “play” corresponding to “发挥(fahui)/demonstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the in"
D11-1007,W03-3017,0,0.0373708,"monstrate”. This is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Si"
D11-1007,J03-1002,0,0.00306085,"ased on the bilingual constraints verified by using large-scale unannotated data. 4.1 Auto-generated bilingual treebank Assuming that we have monolingual treebanks on the source side, an SMT system that can translate the source sentences into the target language, and a Parsert trained on the target monolingual treebank. We first translate the sentences of the source monolingual treebank into the target language using the SMT system. Usually, SMT systems can output the word alignment links directly. If they can not, we perform word alignment using some publicly available tools, such as Giza++ (Och and Ney, 2003) or Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007). The translated sentences are parsed by the Parsert . Then, we have a newly auto-generated bilingual treebank. 76 In this paper, we focus on the first- and secondorder graph models (McDonald and Pereira, 2006; Carreras, 2007). Thus we produce the constraints for bigram (a single edge) and trigram (adjacent edges) dependencies in the graph model. For the trigram dependencies, we consider the parent-sibling and parent-child-grandchild structures described in McDonald and Pereira (2006) and Carreras (2007). We leave the third-order"
D11-1007,W96-0213,0,0.0608525,"e trained first-order and second-order Parsert on the training data. The unlabeled attachment score (UAS) of the second-order Parsert was 91.92, indicating state-of-the-art accuracy on the test data. We used the second-order Parsert to parse the autotranslated/human-made target sentences in the CTB 3 http://www.statmt.org/moses/ http://www.speech.sri.com/projects/srilm/download.html 5 http://www.itl.nist.gov/iad/mig//tests/mt/2008/ 4 data. To extract English subtrees, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ texts. We used the MXPOST tagger (Ratnaparkhi, 1996) trained on training data to assign POS tags and used the first-order Parsert to process the sentences of the BLLIP corpus. To extract bilingual subtrees, we used the FBIS corpus and an additional bilingual corpus containing 800,000 sentence pairs from the training data of NIST MT08 evaluation campaign. On the Chinese side, we used the morphological analyzer described in (Kruengkrai et al., 2009) trained on the training data of CTBtp to perform word segmentation and POS tagging and used the first-order Parsers to parse all the sentences in the data. On the English side, we used the same proced"
D11-1007,W04-3207,0,0.0134497,"ify the constraints and design a set of effective bilingual features for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their"
D11-1007,W03-3023,0,0.0472776,"his is a positive evidence for supporting “发 挥(fahui)/demonstrate” as being the head of “技 巧(jiqiao)/skill”. From this example, although the sentences and parse trees on the target side are not perfect, we still can explore useful information to improve bitext parsing. In this paper, we focus on how to design a method to verify such unreliable bilingual constraints. 3 Parsing model In this paper, we implement our approach based on graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007). Note that our approach can also be applied to transition-based parsing models (Nivre, 2003; Yamada and Matsumoto, 2003). The graph-based parsing model is to search for the maximum spanning tree (MST) in a graph (McDonald and Pereira, 2006). The formulation defines the score of a dependency tree to be the sum of edge scores, s(x, y) = X g∈y score(w, x, g) = X g∈y 4.2 Bilingual constraint functions w ·f (x, g) (1) where x is an input sentence, y is a dependency tree for x, and g is a spanning subgraph of y. f (x, g) can be based on arbitrary features of the subgraph and the input sequence x and the feature weight vector w are the parameters to be learned by using MIRA (Crammer and Singer, 2003) during training."
D11-1007,P09-1007,0,0.0415887,"for parsing models based on the verified results. The experimental results show that our new parsers significantly outperform state-of-theart baselines. Moreover, our approach is still able to provide improvement when we use a larger monolingual treebank that results in a much stronger baseline. Especially notable is that our approach can be used in a purely monolingual setting with the help of SMT. 1 Introduction Recently there have been several studies aiming to improve the performance of parsing bilingual texts (bitexts) (Smith and Smith, 2004; Burkett and Klein, 2008; Huang et al., 2009; Zhao et al., 2009; Chen et al., 2010). In bitext parsing, we can use the information based on “bilingual constraints” (Burkett and Klein, 2008), which do not exist in monolingual sentences. More accurate bitext parsing results can be effectively used in the training of syntax-based machine translation systems (Liu and Huang, 2010). Most previous studies rely on bilingual treebanks to provide bilingual constraints for bitext parsing. Burkett and Klein (2008) proposed joint models on bitexts to improve the performance on either or both sides. Their method uses bilingual treebanks that have human-annotated tree s"
D13-1137,P13-1088,0,0.220172,"that both phrase categories and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as"
D13-1137,J08-1002,0,0.0106005,"o add W add to θ: θ = (We , Wlr , b, W label , W add , blabel ). The gradient of J(θ) ∂J(θ) ∑ ∂E(x) = + λθ ∂θ ∂θ x is efficiently computed via backpropagation through structure (Goller and K¨uchler, 1996). To minimize J(θ), we use batch L-BFGS1 (Hermann and Blunsom, 2013; Socher et al., 2012). 2.4 Averaging We use averaged model parameters 1 ∑ θt T +1 T θ= t=0 at test time, where θt is the vector of model parameters after t iterations of the L-BFGS optimization. Our preliminary experimental results suggest that averaging θ except We works well. 3 Experimental Settings We used the Enju parser (Miyao and Tsujii, 2008) for syntactic parsing. We used 13 phrase categories given by Enju. 3.1 Task: Semantic Relation Classification We evaluated our model on a semantic relation classification task: SemEval 2010 Task 8 (Hendrickx et al., 2010). Following Socher et al. (2012), we regarded the task as a 19-class classification problem. There are 8,000 samples for training, and 2,717 for 1 We used libLBFGS provided at http://www. chokkan.org/software/liblbfgs/. 1374 Figure 2: Classifying the relation between two entities. test. For the validation set, we randomly sampled 2,182 samples from the training data. To predi"
D13-1137,S10-1057,0,0.265915,"Missing"
D13-1137,D12-1110,0,0.746211,"s and task-specific weighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as well, and whether a s"
D13-1137,P13-1045,0,0.0558948,"eighting significantly improve the prediction accuracy of the model. We also show that averaging the model parameters is effective in stabilizing the learning and improves generalization capacity. The proposed model marks scores competitive with state-of-the-art RNN-based models. 1 Introduction Recursive Neural Network (RNN) models are promising deep learning models which have been applied to a variety of natural language processing (NLP) tasks, such as sentiment classification, compound similarity, relation classification and syntactic parsing (Hermann and Blunsom, 2013; Socher et al., 2012; Socher et al., 2013). RNN models can represent phrases of arbitrary length in a vector space of a fixed dimension. Most of them use minimal syntactic information (Socher et al., 2012). Recently, Hermann and Blunsom (2013) proposed a method for leveraging syntactic information, namely CCG combinatory operators, to guide composition of phrases in RNN models. While their models were successfully applied to binary sentiment classification and compound similarity tasks, there are questions yet to be answered, e.g., whether such enhancement is beneficial in other NLP tasks as well, and whether a similar improvement can"
D13-1137,P06-1104,0,0.0468267,"irectly connect features on any other nodes to the softmax classifier. In this work, we used three such internal features: two vector representations of target entities and one averaged vector representation of words between the entities2 . 3.2 Weights on Phrases We tuned the weight αl (or αr ) introduced in Section 2.2 for this particular task. There are two factors: syntactic heads and syntactic path between target entities. Our model puts a weight β ∈ [0.5, 1] on head phrases, and 1 − β on the others. For relation classification tasks, syntactic paths between target entities are important (Zhang et al., 2006), so our model also puts another weight γ ∈ [0.5, 1] on phrases on the path, and 1 − γ on the others. When both child nodes are on the path or neither of them on the path, we set γ = 0.5. The two weight factors are summed up and divided by 2 to be the final weights αl and αr to combine the phrases. For exand αr = β+(1−γ) ample, we set αl = (1−β)+γ 2 2 when the right child node is the head and the left child node is on the path. 3.3 Initialization of Model Parameters and Tuning of Hyperparameters We initialized We with 50-dimensional word vectors3 trained with the model of Collobert et 2 Socher"
D13-1137,S10-1006,0,\N,Missing
D14-1163,D10-1115,0,0.605812,"phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS"
D14-1163,D12-1050,0,0.219803,"nsional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011"
D14-1163,D08-1094,0,0.231181,"Missing"
D14-1163,W13-3203,0,0.0157278,"s jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) investigated a variety of compositional operators to combine word vectors into phrasal representations. Among these operato"
D14-1163,D11-1129,0,0.0465968,"Missing"
D14-1163,D13-1137,1,0.512175,"The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using"
D14-1163,P13-1088,0,0.278818,"ith these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structur"
D14-1163,P12-1092,0,0.347535,"ationship to previous work. If we omit the the category-specific weight vectors hci in Eq. (1), our model is similar to the CBOW model in Mikolov et al. (2013a). CBOW predicts a target word given its surrounding bag-of-words context, while our model uses its PAS-based context. To incorporate the PAS information in our model more efficiently, we use category-specific weight vectors. Similarly, the vLBL model of Mnih and Kavukcuoglu (2013) uses different weight vectors depending on the position relative to the target word. As with previous neural network language models (Collobert et al., 2011; Huang et al., 2012), our model and vLBL can use weight matrices rather than weight vectors. However, as discussed by Mnih and Teh (2012), using weight vectors makes the training significantly faster than using weight matrices. Despite the simple formulation of the element-wise operations, the categoryspecific weight vectors efficiently propagate PASbased context information as explained next. 3.2.2 Training Word Vectors To train the PAS-LBLM, we use a scoring function to evaluate how well the target word wt fits the given context: s(wt , p(wt )) = v˜(wt )T p(wt ), (4) where v˜(wt ) ∈ Rd×1 is the scoring weight v"
D14-1163,D13-1166,0,0.282877,"Missing"
D14-1163,W13-3513,0,0.0612916,"nexpectedly high scores for these three tasks. Previously these kinds of models (Mikolov et al., 2013b; Mnih and Kavukcuoglu, 2013) have mainly been evaluated for word analogy tasks and, to date, there has been no work using these word vectors for the task of measuring the semantic similarity between phrases. However, this experimental result suggests that word2vec can serve as a strong baseline for these kinds of tasks, in addition to word analogy tasks. In Table 3, BL, HB, KS, and K denote the work of Blacoe and Lapata (2012), Hermann and Blunsom (2013), Kartsaklis and Sadrzadeh (2013), and Kartsaklis et al. (2013) respectively. Among these, 5 1550 https://code.google.com/p/word2vec/ Model PAS-CLBLM (Addl ) PAS-CLBLM (Addnl ) PAS-CLBLM (Waddl ) PAS-CLBLM (Waddnl ) PAS-LBLM word2vec Grefenstette and Sadrzadeh (2011) Tsubaki et al. (2013) Van de Cruys et al. (2013) Human agreement Corpus BNC BNC BNC ukWaC ukWaC Averaged SVO-SVO SVO-V 0.29 0.34 0.27 0.32 0.25 0.26 0.42 0.50 0.21 0.06 0.12 0.32 n/a n/a n/a 0.47 n/a n/a 0.75 Non-averaged SVO-SVO SVO-V 0.24 0.28 0.24 0.28 0.21 0.23 0.34 0.41 0.18 0.08 0.12 0.28 0.21 n/a n/a n/a 0.32 0.37 0.62 Table 4: Spearman’s rank correlation scores ρ for the SVO task. Ave"
D14-1163,P14-2050,0,0.418227,"o, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs). Our model jointly learns word representations and their composition functions using bagof-words and dependency-based contexts. Unlike previous word-sequencebased models, our PAS-base"
D14-1163,P08-1028,0,0.788883,"ver, the proposed model does not require any pre-trained word vectors produced by external models, but rather induces word vectors jointly while training. 2 Related Work There is a large body of work on how to represent the meaning of a word in a vector space. Distributional approaches assume that the meaning of a word is determined by the contexts in which it appears (Firth, 1957). The context of a word is often defined as the words appearing in a window of fixed-length (bag-of-words) and a simple approach is to treat the co-occurrence statistics of a word w as a vector representation for w (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010); alternatively, dependencies between words can be used to define contexts (Goyal et al., 2013; Erk and Pad´o, 2008; Thater et al., 2010). In contrast to distributional representations, NNLMs represent words in a low-dimensional vector space (Bengio et al., 2003; Collobert et al., 2011). Recently, Mikolov et al. (2013b) and Mnih and Kavukcuoglu (2013) proposed highly scalable models to learn high-dimensional word vectors. Levy and Goldberg (2014) extended the model of Mikolov et al. (2013b) by treating syntactic dependencies as contexts. Mitchell and Lapata (2008) i"
D14-1163,J08-1002,0,0.0501574,"sis of our model. We then introduce a Log-Bilinear Language Model using Predicate-Argument Structures (PAS-LBLM) to learn word representations using both bag-ofwords and dependency-based contexts. Finally, we propose integrating compositions of words into the model. Figure 1 (b) shows the overview of the proposed model. 3.1 Predicate-Argument Structures Due to advances in deep parsing technologies, syntactic parsers that can produce predicateargument structures are becoming accurate and fast enough to be used for practical applications. In this work, we use the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008) to obtain the predicate-argument structures of individual sentences. In its grammar, each word in a sentence is treated as a predicate of a certain category with zero or more arguments. Table 1 shows some exCategory adj arg1 noun arg1 verb arg12 prep arg12 predicate heavy car cause at arg1 rain accident rain eat arg2 accident restaurant Table 1: Examples of predicates of different categories from the grammar of the Enju parser. arg1 and arg2 denote the first and second arguments. amples of predicates of different categories.1 For example, a predicate of the category verb arg12 expresses a ver"
D14-1163,P14-1009,0,0.0244756,"o composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) introduced a novel compositional NNLM mainly focusing on verb-object dependencies and achieved state-of-the-art performance for the task of measuring the semantic similarity between subjectverb-object phrases. 3 PAS-CLBLM: A Compositional Log-Bilinear Language Model Using Predicate-Argument Structures In some recent studies on representing words as vectors, word vectors are learned by solving word prediction tasks (Mikolov et al."
D14-1163,D12-1110,0,0.862473,"presentations and Composition Functions Using Predicate-Argument Structures Kazuma Hashimoto† , Pontus Stenetorp† , Makoto Miwa‡ , and Yoshimasa Tsuruoka† †The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs)."
D14-1163,P13-1045,0,0.255766,"ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in"
D14-1163,D13-1170,0,0.21026,"ated a variety of compositional operators to combine word vectors into phrasal representations. Among these operators, simple element-wise addition and multiplication are now widely used to represent short phrases (Mitchell and Lapata, 2010; Blacoe and Lapata, 2012). The obvious limitation with these simple approaches is that information about word order and syntactic relations is lost. To incorporate syntactic information into composition functions, a variety of compositional models have been proposed. These include recursive neural networks using phrase-structure trees (Socher et al., 2012; Socher et al., 2013b) and models in which words have a specific form of parameters according to their syntactic roles and composition functions are syntactically dependent on the relations of input words (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hermann and Blunsom, 2013; Socher et al., 2013a). More recently, syntactic dependency-based 1545 compositional models have been proposed (Paperno et al., 2014; Socher et al., 2014; Tsubaki et al., 2013). One of the advantages of these models is that they are less restricted by word order. Among these, Tsubaki et al. (2013) in"
D14-1163,P10-1097,0,0.0689392,"Missing"
D14-1163,D13-1014,0,0.122155,"azuma Hashimoto† , Pontus Stenetorp† , Makoto Miwa‡ , and Yoshimasa Tsuruoka† †The University of Tokyo, 3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,pontus,tsuruoka}@logos.t.u-tokyo.ac.jp ‡Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp Abstract Recently, the main focus of research on vector space representation is shifting from word representations to phrase representations (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Combining the ideas of NNLMs and semantic composition, Tsubaki et al. (2013) introduced a novel NNLM incorporating verb-object dependencies. More recently, Levy and Goldberg (2014) presented a NNLM that integrated syntactic dependencies. However, to the best of our knowledge, there is no previous work on integrating a variety of syntactic and semantic dependencies into NNLMs in order to learn composition functions as well as word representations. The following question thus arises naturally: We introduce a novel compositional language model that works on PredicateArgument Structures (PASs). Our model jointly learns word representations and their composition functions"
D14-1163,P10-1040,0,0.0613101,"Missing"
D14-1163,N13-1134,0,0.281979,"Missing"
D14-1163,Q14-1017,0,\N,Missing
D15-1277,P13-1075,0,0.0916695,"tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segment"
D15-1277,D14-1093,0,0.0538287,"there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symb"
D15-1277,P15-1167,0,0.0358304,"Missing"
D15-1277,I08-7018,0,0.112746,"didate as the score of the candidate. After that, we get the summation of, the average of, or the maximum in the scores of the same candidate over the whole dataset. Finally we select the top R percent of word candidates in descending order of the value of sum, ave, or max and add them to the WS dictionary and retrain the model. 5 Evaluation We conducted word segmentation experiments in the following settings. 5.1 Corpora The annotated corpus we used to build the baseline word segmenter is the manually annotated part (core data) of the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa, 2008), plus newspaper articles and daily conversation sentences. We also used a 234,652-word dictionary (UniDic) provided with the BCCWJ. A small portion of the BCCWJ core data is reserved for testing. In addition, we manually segmented sentences randomly obtained from Shogi commentaries. We divided these sentences into two parts: a development set and a test set. Table 1 shows the details of these corpora. To make a pSSC, we prepared 33,151 pairs of a Shogi position and a commentary sentence. The 2300 Table 2: WS accuracy on BCCWJ. Recall Prec. F-meas. Baseline 98.99 99.06 99.03 + Sym.Gro. 99.03 9"
D15-1277,C96-2202,1,0.413116,"is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symbol grounding results. To extract word candidates from raw sentences, we first segment sentences stochastically, and then match the word candidate sequences with game states that are described by the sentences. Finally,"
D15-1277,D08-1045,0,0.0363732,"Missing"
D15-1277,C94-1032,0,0.126072,"s valuable as the annotation additions. From a close look at the comparison of the recall and the precision, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding successfully acquired new words with a few erroneous words. As the final remark, the result on the general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word"
D15-1277,P11-2093,1,0.65013,"rpus Cr (hereafter referred to as the character sequence xn1 r ) and word boundary probabilities of the form Pi , which is the probability that a word boundary exists between two characters xi and xi+1 . These probabilities are estimated by a model based on logistic regression (LR) (Fan et al., 2008) trained on a manually segmented corpus by referring to the surrounding characters1 . Since there are word boundaries before the first character and after the last character of the corpus, P0 = Pnr = 1. The expected frequency of a word 1 In the experiment we used the same features as those used in Neubig et al., (2011). . 2298 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2298–2303, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Figure 1: Overview of our method. w in an SSC {∏ is calculated as } follows: fr (w) = ∑ k−1 j=1 (1 − Pi+j ) Pi+k , where O = i∈O Pi {i |xi+k i+1 = w} is the set of all the occurrences of the string matching with w2 . 2.2 Pseudo-Stochastically Segmented Corpora The computational cost (in terms of both time and space) for calculating the expected frequencies in an SSC is very high3 , so it is no"
D15-1277,Q13-1003,0,0.054047,"oximation errors decrease to 0 when m → ∞. 3 Symbol Grounding As the target of symbol grounding, we use states (piece positions) of a Shogi game and commen2 For a detailed explanation and a mathematical proof of this method, please refer to Mori and Takuma (2004) . 3 This is because an SSC has many words and word fragments. Additionally, word 1-gram frequencies must be calculated using floating point numbers instead of integers. taries associated with them. We should note, however, that our framework is general and applicable to different types of combinations such as image/description pairs (Regneri et al., 2013). 3.1 Game Commentary The Japanese language is one of the languages without clear word boundaries and we need an automatic WS as the first step of NLP. In Shogi, there are many professional players and many commentaries about game states are available. 3.2 Grounding Words We build a symbol grounding model using a Shogi commentary dataset. We use a set of pairs of a Shogi state Si and a commentary sentence Ci as the training set. A Shogi state Si is converted into a feature vector f(Si ). We generate m (in our experiment, m = 4) pSSC Ci′ from Ci . Ci′ contains m corpora of the same text body bu"
D15-1277,J96-3004,0,0.442213,"sion, we see that the improvement in the recall is higher than that of the precision. This result shows that the symbol grounding successfully acquired new words with a few erroneous words. As the final remark, the result on the general domain (Table 2) shows that our framework does not cause a severe performance degradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no co"
D15-1277,C08-1113,1,0.785905,"ystem with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for improving word segmentation based on symbol grounding results. To extract word candidates from raw sentences, we first segment sentences stochastically, and then match the word candidate sequences with game states that are described by"
D15-1277,O03-4002,0,0.0575001,"egradation in the general domain. 6 Related Work The NLP task we focus on in this paper is word segmentation. One of the first empirical methods was based on a hidden Markov model (Nagata, 1994). In parallel, there were attempts at solving Chinese word segmentation in a similar way (Sproat and Chang, 1996). These methods take words as the modeling unit. Recently, Neubig et al. (2011) have presented a method for directly deciding whether there is a word boundary or not at each point between characters. For Chinese word segmentation, there are some attempts at tagging characters with BIES tags (Xue, 2003) by a sequence labeller such as CRFs (Lafferty et al., 2001), where B, I, E, and S means the beginning of a word, intermediate of a word, the end of a word, and a single character word, respectively. The pointwise WS can be seen as character tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however,"
D15-1277,D14-1010,0,0.0945302,"er tagging with the BI tag system, in which there is no constraint between neighboring tags. For Japanese WS, our preliminary experiments showed that the combination of the BI tag system with SVMs is slightly better than the BIES tag system with CRFs. This is another reason why we used the former in this paper. Our extension of word segmentation is, however, applicable to the BIES/CRFs combination as well. The method we describe in this paper is unsupervised and requires a small amount of annotated data to tune the hyperparameter. From this viewpoint, the approach based on natural annotation (Yang and Vozila, 2014; Jiang et al., 2013; Liu et al., 2014) may come to readers’ mind. In these studies, tags in hyper-texts were regarded as partial annotations and used to improve WS performance using CRFs trainable from such data (Tsuboi et al., 2008). Mori and Nagao (1996) proposed a method for extracting new words from a large amount of raw text. Murawaki and Kuro2301 hashi (2008) proposed an online method in a similar setting. In contrast to these studies, this paper proposes to use other modalities, game states as the first trial, than languages. 7 Conclusion We have described an unsupervised method for im"
D15-1277,D11-1041,0,\N,Missing
D17-1012,J82-2005,0,0.684998,"Missing"
D17-1012,Q17-1010,0,0.0038778,"sampling (Ji et al., 2016). By this joint learning using Equation (3) and (7), the latent graph representations are automatically learned according to the target task. Decoder with Attention Mechanism s(i, t) = (dec) ht+1 = LSTM(ht Note on character n-gram embeddings In NMT models, sub-word units are widely used to address rare or unknown word problems (Sennrich et al., 2016). In our model, the character n-gram embeddings are fed through the latent graph parsing component. To the best of our knowledge, the character n-gram embeddings have never been used in NMT models. Wieting et al. (2016), Bojanowski et al. (2017), and Hashimoto et al. (2017) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words. 3.2 (8) (6) i=1 (7) 127 (10) where p(Ly |Lx ) is the probability that sentences of length Ly are generated given source-side sentences of length Lx . The statistics are taken by using the training data in advance. In our experiments, we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short. We used an English-to-Japanese translation task of the Asian Scienti"
D17-1012,D14-1082,0,0.0279175,"the latent graph parser. The word and character n-gram embeddings of the latent graph parser 1 Pre-Training of Latent Graph Parser The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing. In this paper we used 2 The pre-trained embeddings can be found at https: //github.com/hassyGo/charNgram2vec. http://www.phontron.com/kytea/. 128 the widely-used Wall Street Journal (WSJ) training data to jointly train the POS tagging and dependency parsing components. We used the standard training split (Section 0-18) for POS tagging. We followed Chen and Manning (2014) to generate the training data (Section 2-21) for dependency parsing. From each training dataset, we selected the first K sentences to pre-train our model. The training dataset for POS tagging includes 38,219 sentences, and that for dependency parsing includes 39,832 sentences. The parser including the POS tagger was first trained for 10 epochs in advance according to the multi-task learning procedure of Hashimoto et al. (2017), and then the overall NMT model was trained. When pre-training the POS tagging and dependency parsing components, we did not apply dropout to the model and did not fine"
D17-1012,P17-1177,0,0.0554217,"ential 7 These English sentences were created by manual simplification of sentences in the development data. 8 The translations were obtained at https: //translate.google.com in Feb. and Mar. 2017. 131 (a) 0.86 0.97 7 ROOT 1.0 0.99 1.0 All the calculated electronic band structures are metallic . 0.85 0.86 (b) 0.23 0.26 0.60 0.95 0.82 While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser"
D17-1012,W14-4012,0,0.0298081,"Missing"
D17-1012,W16-4616,0,0.318821,"8 15.13±1.67 13.34±0.67 Small Training Dataset 3 We did not observe such significant difference when using the larger datasets, and we used all the training samples in the remaining part of this paper. Table 1 shows the results of using the small training dataset. LGP-NMT performs worse than SEQ 129 LGP-NMT LGP-NMT+ SEQ UNI DEP BLEU 28.70±0.27 29.06±0.25 28.60±0.24 28.25±0.35 26.83±0.38 RIBES 77.51±0.13 77.57±0.24 77.39±0.15 77.13±0.20 76.05±0.22 Perplexity 12.10±0.16 12.09±0.27 12.15±0.12 12.37±0.08 13.33±0.23 B./R. LGP-NMT LGP-NMT+ SEQ LGP-NMT LGP-NMT+ SEQ Ensemble of the above three models Cromieres et al. (2016) Neubig et al. (2015) Eriguchi et al. (2016a) Neubig and Duh (2014) Zhu (2015) Lee et al. (2015) biomedicine and computer science. These results suggest that our model can be improved by a small amount of parsing and tagging datasets in different domains. Considering the recent universal dependency project4 which covers more than 50 languages, our model has the potential of being applied to a variety of language pairs. BLEU 39.19 39.42 38.96 41.18 38.20 38.17 36.95 36.58 36.21 35.75 RIBES 82.66 82.83 82.18 83.40 82.39 81.38 82.45 79.65 80.91 81.15 Medium Training Dataset Again, we see that the"
D17-1012,P96-1011,0,0.174606,"Missing"
D17-1012,W16-4617,1,0.45686,"d by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset. Introduction Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences 2 Latent Graph Parser We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and 125 Proceedings of"
D17-1012,P16-1078,1,0.688988,"d by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset. Introduction Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences 2 Latent Graph Parser We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and 125 Proceedings of"
D17-1012,P17-2012,1,0.792643,"learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation. They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems. By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional preference and PP attachments, in a task-oriented fashion. Our model learns latent graph structures in a source-side language. Eriguchi et al. (2017) have proposed a model which learns to parse and translate by using automatically-parsed data. Thus, it is also an interesting direction to learn latent structures in a target-side language. As for the learning of latent syntactic structure, there are several studies on learning task-oriented syntactic structures. Yogatama et al. (2017) used a reinforcement learning method on shift-reduce action sequences to learn task-oriented binary constituency trees. They have shown that the learned trees do not necessarily highly correlate with the human-annotated treebanks, which is consistent with our e"
D17-1012,D15-1278,0,0.00902245,".66 37.14±1.96 38.33±1.18 39.24±1.88 Table 2: Effects of the size K of the training datasets for POS tagging and dependency parsing. and UNI, which shows that the small training dataset is not enough to learn useful latent graph structures from scratch. However, LGP-NMT+ (K = 10,000) outperforms SEQ and UNI, and the standard deviations are the smallest. Therefore, the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model. We can also see that DEP performs the worst. This is not surprising because previous studies, e.g., Li et al. (2015), have reported that using syntactic structures do not always outperform competitive sequential models in several NLP tasks. Now that we have observed the effectiveness of pre-training our model, one question arises naturally: Model Configurations LGP-NMT+ is constructed by pre-training the latent parser in LGP-NMT as described in Section 4.3. SEQ is constructed by removing the dependency composition in Equation (3), forming a sequential NMT model with the multi-layer encoder. DEP is constructed by using pre-trained dependency relations rather than learning them. That is, p(Hwi = wj |wi ) is f"
D17-1012,D13-1137,1,0.794454,"(wi )) , PN (dec) ·dep(wj )) j=1 exp (ht PN 0 i=1 s (i, t)dep(wi ), ]), (9) Implementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences. This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model. We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016; Press and Wolf, 2017). Also, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy. The decoder of our model is a single-layer LSTM (enc) network, and the initial state is set with hN +1 and its corresponding memory cell. Given the t-th hid(dec) den state ht ∈ Rd3 , the decoder predicts the t-th word in the target language using an attention mechanism. The attention mechanism in Luong et al. (2015) computes the weighted average of the (enc) hidden states hi of the encoder: exp (ht ·hi ) PN +1 (dec) (enc) , exp (h ·h ) t j=1 j (dec) ˜ , [vdec (wt ); h t ˜ (dec) is called input feeding prowhere the use of h t posed by Luong et al."
D17-1012,P17-1064,0,0.0881299,"nual simplification of sentences in the development data. 8 The translations were obtained at https: //translate.google.com in Feb. and Mar. 2017. 131 (a) 0.86 0.97 7 ROOT 1.0 0.99 1.0 All the calculated electronic band structures are metallic . 0.85 0.86 (b) 0.23 0.26 0.60 0.95 0.82 While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a"
D17-1012,D17-1206,1,0.190937,"parent nodes, and p(`wi |wi ) is the probability distribution of the dependency labels. For example, p(Hwi = wj |wi ) is the probability that wj is the parent node of wi . Here, we assume that a special token hEOSi is appended to the end of the sentence, and we treat the hEOSi token as ROOT. This approach is similar to that of graph-based dependency parsing (McDonald et al., 2005) in that a sentence is represented with a set of weighted arcs between the words. To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al. (2017). 2.1 Dependency parsing is performed in the second (2) layer. A hidden state hi ∈ R2d1 is computed → − (2) → − (2) → − (1) by h i = LSTM( h i−1 , [x(wi ); y(wi ); h i ]) ← −(2) ← −(2) ← −(1) and h i = LSTM( h i+1 , [x(wi ); y(wi ); h i ]), (1) (1) where y(wi ) = W` pi ∈ Rd2 is the POS in(1) formation output from the first layer, and W` ∈ (1) Rd2 ×C is a weight matrix. Then, (soft) edges of our latent graph representation are obtained by computing the probabilities exp (m(i, j)) , k6=i exp (m(i, k)) p(Hwi = wj |wi ) = P (2)T (1) (2) where m(i, k) = hk Wdp hi (1 ≤ k ≤ N + 1, k 6= i) is a scorin"
D17-1012,D15-1166,0,0.58863,"d NMT model. The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task. In experiments, we demonstrate that our model can be effectively pre-trained by the treebank annotations, outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset. Introduction Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences 2 Latent Gr"
D17-1012,P05-1012,0,0.0228034,", where `wi is a dependency label. In this paper, we remove the constraint of using the tree structure and represent a sentence as a set of tuples (wi , p(Hwi |wi ), p(`wi |wi )), where p(Hwi |wi ) is the probability distribution of wi ’s parent nodes, and p(`wi |wi ) is the probability distribution of the dependency labels. For example, p(Hwi = wj |wi ) is the probability that wj is the parent node of wi . Here, we assume that a special token hEOSi is appended to the end of the sentence, and we treat the hEOSi token as ROOT. This approach is similar to that of graph-based dependency parsing (McDonald et al., 2005) in that a sentence is represented with a set of weighted arcs between the words. To obtain the latent graph representation of the sentence, we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al. (2017). 2.1 Dependency parsing is performed in the second (2) layer. A hidden state hi ∈ R2d1 is computed → − (2) → − (2) → − (1) by h i = LSTM( h i−1 , [x(wi ); y(wi ); h i ]) ← −(2) ← −(2) ← −(1) and h i = LSTM( h i+1 , [x(wi ); y(wi ); h i ]), (1) (1) where y(wi ) = W` pi ∈ Rd2 is the POS in(1) formation output from the first layer, and W` ∈ (1) Rd2 ×C is a weig"
D17-1012,J08-1002,0,0.0406908,"search algorithm helps the NMT models to avoid generating translation sentences that are too short. We used an English-to-Japanese translation task of the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b) used in the Workshop on Asian Translation (WAT), since it has been shown that syntactic information is useful in English-to-Japanese translation (Eriguchi et al., 2016b; Neubig et al., 2015). We followed the data preprocessing instruction for the English-toJapanese task in Eriguchi et al. (2016b). The English sentences were tokenized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea tool1 . Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs where the maximum sentence length is 50. In what follows, we call this dataset the large training dataset. We further selected the first 20,000 and 100,000 pairs to construct the small and medium training datasets, respectively. The development data include 1,790 pairs, and the test data 1,812 pairs. For the small and medium datasets, we built the vocabulary with words whose minimum frequency is two, and for the large dataset, we used wo"
D17-1012,D10-1092,0,0.022496,"he BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training. We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique. For regularization, we used L2-norm regularization with a coefficient of 10−6 and applied dropout (Hinton et al., 2012) to Equation (8) with a dropout rate of 0.2. The beam size for the beam search algorithm was 12 for the small and medium training datasets, and 50 for the large training dataset. We used BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and perplexity scores as our evaluation metrics. Note that lower perplexity scores indicate better accuracy. 4.2 4.3 4 Experimental Settings 4.1 Data Parameter Optimization and Translation We turned hyper-parameters of the model using development data. We set (d1 , d2 ) = (100, 50) for the latent graph parser. The word and character n-gram embeddings of the latent graph parser 1 Pre-Training of Latent Graph Parser The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing. In this paper we used 2 The pre-trained embeddings can be foun"
D17-1012,W15-3014,0,0.0244175,"i et al. (2016b) rely on ensemble techniques while our results mentioned above are obtained using single models. Moreover, our model is more compact6 than the previous best NMT model in Cromieres et al. (2016). By applying the ensemble technique to LGP-NMT, LGP-NMT+, Results on Large Dataset Table 4 shows the BLEU and RIBES scores on the development data achieved with the large training dataset. Here we focus on our models and SEQ because UNI and DEP consistently perform worse than the other models as shown in Table 1 and 3. The averaging technique and attentionbased unknown word replacement (Jean et al., 2015; Hashimoto et al., 2016) improve the scores. 4 +UnkRep 38.77/82.29 39.37/82.48 38.61/82.18 Table 5: BLEU and RIBES scores on the test data. Table 3 shows the results of using the medium training dataset. In contrast with using the small training dataset, LGP-NMT is slightly better than SEQ. LGP-NMT significantly outperforms UNI, which shows that our adaptive learning is more effective than using the uniform graph weights. By pre-training our model, LGP-NMT+ significantly outperforms SEQ in terms of the BLEU score. Again, DEP performs the worst among all the models. By using our beam search st"
D17-1012,P14-2024,0,0.0215759,"rted investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation. They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems. By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional prefe"
D17-1012,W15-5003,0,0.0413701,"entences of length Ly are generated given source-side sentences of length Lx . The statistics are taken by using the training data in advance. In our experiments, we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short. We used an English-to-Japanese translation task of the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b) used in the Workshop on Asian Translation (WAT), since it has been shown that syntactic information is useful in English-to-Japanese translation (Eriguchi et al., 2016b; Neubig et al., 2015). We followed the data preprocessing instruction for the English-toJapanese task in Eriguchi et al. (2016b). The English sentences were tokenized by the tokenizer in the Enju parser (Miyao and Tsujii, 2008), and the Japanese sentences were segmented by the KyTea tool1 . Among the first 1,500,000 translation pairs in the training data, we selected 1,346,946 pairs where the maximum sentence length is 50. In what follows, we call this dataset the large training dataset. We further selected the first 20,000 and 100,000 pairs to construct the small and medium training datasets, respectively. The de"
D17-1012,P02-1040,0,0.125248,"n accuracy decreased. We used the BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training. We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique. For regularization, we used L2-norm regularization with a coefficient of 10−6 and applied dropout (Hinton et al., 2012) to Equation (8) with a dropout rate of 0.2. The beam size for the beam search algorithm was 12 for the small and medium training datasets, and 50 for the large training dataset. We used BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010), and perplexity scores as our evaluation metrics. Note that lower perplexity scores indicate better accuracy. 4.2 4.3 4 Experimental Settings 4.1 Data Parameter Optimization and Translation We turned hyper-parameters of the model using development data. We set (d1 , d2 ) = (100, 50) for the latent graph parser. The word and character n-gram embeddings of the latent graph parser 1 Pre-Training of Latent Graph Parser The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing. In this paper we used 2 The pre-"
D17-1012,E17-2025,0,0.00681929,"ntion to the dependency composition vectors: s0 (i, t) = a0t = (dec) exp (ht ·dep(wi )) , PN (dec) ·dep(wj )) j=1 exp (ht PN 0 i=1 s (i, t)dep(wi ), ]), (9) Implementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences. This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model. We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016; Press and Wolf, 2017). Also, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy. The decoder of our model is a single-layer LSTM (enc) network, and the initial state is set with hN +1 and its corresponding memory cell. Given the t-th hid(dec) den state ht ∈ Rd3 , the decoder predicts the t-th word in the target language using an attention mechanism. The attention mechanism in Luong et al. (2015) computes the weighted average of the (enc) hidden states hi of the encoder: exp (ht ·hi ) PN +1 (dec) (enc) , exp (h ·h ) t j=1 j (dec) ˜ , [vdec (wt );"
D17-1012,W15-5007,0,0.121694,"Missing"
D17-1012,W16-2209,0,0.302286,"outperforming a state-of-the-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset. Introduction Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences 2 Latent Graph Parser We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and 125 Proceedings of the 2017 Conference on Empirical"
D17-1012,N16-1145,0,0.0296316,"ted word sequence y = (y1 , y2 , . . . , yLy ) given a source word sequence x = (x1 , x2 , . . . , xLx ):   Ly X 1  log p(yi |x, y1:i−1 ) + log p(Ly |Lx ) , Ly (5) where s(i, t) is a scoring function which specifies how much each source-side hidden state contributes to the word prediction. In addition, like the attention mechanism over constituency tree nodes (Eriguchi et al., 2016b), our model uses attention to the dependency composition vectors: s0 (i, t) = a0t = (dec) exp (ht ·dep(wi )) , PN (dec) ·dep(wj )) j=1 exp (ht PN 0 i=1 s (i, t)dep(wi ), ]), (9) Implementation Tips Inspired by Zoph et al. (2016), we further speed up BlackOut sampling by sharing noise samples across words in the same sentences. This technique has proven to be effective in RNN language modeling, and we have found that it is also effective in the NMT model. We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction (Inan et al., 2016; Press and Wolf, 2017). Also, we have found that a parameter averaging technique (Hashimoto et al., 2013) is helpful in improving translation accuracy. The decoder of our model is a single-layer LSTM (enc) ne"
D17-1012,P16-1162,0,0.0104782,"all model parameters, including those of the latent graph parser, are jointly learned by minimizing the negative log-likelihood of the prediction probabilities of the target words in the training data. To speed up the training, we use BlackOut sampling (Ji et al., 2016). By this joint learning using Equation (3) and (7), the latent graph representations are automatically learned according to the target task. Decoder with Attention Mechanism s(i, t) = (dec) ht+1 = LSTM(ht Note on character n-gram embeddings In NMT models, sub-word units are widely used to address rare or unknown word problems (Sennrich et al., 2016). In our model, the character n-gram embeddings are fed through the latent graph parsing component. To the best of our knowledge, the character n-gram embeddings have never been used in NMT models. Wieting et al. (2016), Bojanowski et al. (2017), and Hashimoto et al. (2017) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words. 3.2 (8) (6) i=1 (7) 127 (10) where p(Ly |Lx ) is the probability that sentences of length Ly are generated given source-side sentences of length Lx . The statistics are taken by using the training d"
D17-1012,D11-1014,0,0.0818968,"Missing"
D17-1012,P16-2049,0,0.156773,"he-art sequential counterpart and a pipelined syntax-based model. Our final ensemble model outperforms the previous best results by a large margin on the WAT English-to-Japanese dataset. Introduction Neural Machine Translation (NMT) is an active area of research due to its outstanding empirical results (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014). Most of the existing NMT models treat each sentence as a sequence of tokens, but recent studies suggest that syntactic information can help improve translation accuracy (Eriguchi et al., 2016b, 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016). The existing syntax-based NMT models employ a syntactic parser trained by supervised learning in advance, and hence the parser is not adapted to the translation tasks. An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences 2 Latent Graph Parser We model the latent graph parser based on dependency parsing. In dependency parsing, a sentence is represented as a tree structure where each node corresponds to a word in the sentence and 125 Proceedings of the 2017 Conference on Empirical Methods in Natural Langua"
D17-1012,D16-1157,0,0.00504724,"aining, we use BlackOut sampling (Ji et al., 2016). By this joint learning using Equation (3) and (7), the latent graph representations are automatically learned according to the target task. Decoder with Attention Mechanism s(i, t) = (dec) ht+1 = LSTM(ht Note on character n-gram embeddings In NMT models, sub-word units are widely used to address rare or unknown word problems (Sennrich et al., 2016). In our model, the character n-gram embeddings are fed through the latent graph parsing component. To the best of our knowledge, the character n-gram embeddings have never been used in NMT models. Wieting et al. (2016), Bojanowski et al. (2017), and Hashimoto et al. (2017) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words. 3.2 (8) (6) i=1 (7) 127 (10) where p(Ly |Lx ) is the probability that sentences of length Ly are generated given source-side sentences of length Lx . The statistics are taken by using the training data in advance. In our experiments, we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short. We used an English-to-Japanese translation"
D17-1012,J97-3002,0,0.0647329,"Missing"
D17-1012,P01-1067,0,0.0839935,"o the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automatically extract syntactic information for machine translation. They rely on the output from the parser, and therefore parsing errors are propagated through the whole systems. By contrast, our model allows the parser to be adapted to the translation task, thereby providing a first step towards addressing ambiguous syntactic and semantic problems, such as domain-specific selectional preference and PP attachments,"
D17-1012,D17-1150,0,0.0950913,"slations were obtained at https: //translate.google.com in Feb. and Mar. 2017. 131 (a) 0.86 0.97 7 ROOT 1.0 0.99 1.0 All the calculated electronic band structures are metallic . 0.85 0.86 (b) 0.23 0.26 0.60 0.95 0.82 While initial studies on NMT treat each sentence as a sequence of words (Bahdanau et al., 2015; Luong et al., 2015; Sutskever et al., 2014), researchers have recently started investigating into the use of syntactic structures in NMT models (Bastings et al., 2017; Chen et al., 2017; Eriguchi et al., 2016a,b, 2017; Li et al., 2017; Sennrich and Haddow, 2016; Stahlberg et al., 2016; Yang et al., 2017). In particular, Eriguchi et al. (2016b) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder, which motivated the use of the dependency composition vectors in our proposed model. Prior to the advent of NMT, the syntactic structures had been successfully used in statistical machine translation systems (Neubig and Duh, 2014; Yamada and Knight, 2001). These syntax-based approaches are pipelined; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset, and then the parser is used to automa"
D17-1206,D15-1159,0,0.00674333,"= [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the labe"
D17-1206,P16-1231,0,0.284093,"(2) (1) (pos) (1) gt = [ht−1 ; ht ; xt ; yt ], where ht is the hidden state of the first (POS) layer. We define (pos) the weighted label embedding yt as follows: (pos) yt = C X j=1 (1) p(yt (1) = j|ht )`(j), (2) (1) where C is the number of the POS tags, p(yt = (1) j|ht ) is the probability value that the j-th POS tag is assigned to wt , and `(j) is the corresponding label embedding. The probability values are predicted by the POS layer, and thus no gold POS tags are needed. This output embedding is similar to the K-best POS tag feature which has been shown to be effective in syntactic tasks (Andor et al., 2016; Alberti et al., 2015). For predicting the chunking tags, we employ the same strategy as POS tagging by using the concatenated bi→ − (2) ← −(2) (2) directional hidden states ht = [ h t ; h t ] in the chunking layer. We also use a single ReLU hidden layer before the softmax classifier. 1924 2.4 Syntactic Task: Dependency Parsing Dependency parsing identifies syntactic relations (such as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word repres"
D17-1206,C10-1011,0,0.0644419,"Missing"
D17-1206,Q17-1010,0,0.0304414,"encies improve the relatedness task. The relatedness and entailment tasks are closely related to each other. If the semantic relatedness between two sentences is very low, they are unlikely to entail each other. Based on this observation, we make use of the information from the relatedness task for improving the entailment task. 2.1 it = σ (Wi gt + bi ) , ft = σ (Wf gt + bf ) , ut = tanh (Wu gt + bu ) , ct = it ut + ft ct−1 , Word-Level Task: POS Tagging The first layer of the model is a bi-directional LSTM (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997) whose hidden states 1 Bojanowski et al. (2017) previously proposed to train the character n-gram embeddings by the Skip-gram objective. (1) ot = σ (Wo gt + bo ) , ht = ot tanh (ct ) , → − where we define the input gt as gt = [ h t−1 ; xt ], i.e. the concatenation of the previous hidden state and the word representation of wt . The backward pass is expanded in the same way, but a different set of weights are used. For predicting the POS tag of wt , we use the concatenation of the forward and backward states in a one-layer bi-LSTM layer corresponding to the → − ← − t-th word: ht = [ h t ; h t ]. Then each ht (1 ≤ t ≤ L) is fed into a standa"
D17-1206,D16-1257,0,0.0160407,"histicated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91"
D17-1206,P15-1033,0,0.0112343,"Missing"
D17-1206,P96-1011,0,0.0606322,"didates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We greedily select the parent node and the dependency label for each word. When the parsing result is not a well-formed tree, we apply the first-order Eisner’s algorithm (Eisner, 1996) to obtain a well-formed tree from it. (3) (3) [ht ; hj ] 2.5 Semantic Task: Semantic relatedness The next two tasks model the semantic relationships between two input sentences. The first task measures the semantic relatedness between two sentences. The output is a real-valued relatedness score for the input sentence pair. The second task is textual entailment, which requires one to determine whether a premise sentence entails a hypothesis sentence. There are typically three classes: entailment, contradiction, and neutral. We use the fourth and fifth bi-LSTM layer for the relatedness and enta"
D17-1206,P16-1078,1,0.760695,"on Sentence1 Sentence2 Figure 1: Overview of the joint many-task model predicting different linguistic outputs at successively deeper layers. different tasks either entirely separately or at the same depth (Collobert et al., 2011). Introduction The potential for leveraging multiple levels of representation has been demonstrated in various ways in the field of Natural Language Processing (NLP). For example, Part-Of-Speech (POS) tags are used for syntactic parsers. The parsers are used to improve higher-level tasks, such as natural language inference (Chen et al., 2016) and machine translation (Eriguchi et al., 2016). These systems are often pipelines and not trained end-to-end. Deep NLP models have yet shown benefits from predicting many increasingly complex tasks each at a successively deeper layer. Existing models often ignore linguistic hierarchies by predicting ∗ Entailment encoder word level semantic level Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model togethe"
D17-1206,D17-1012,1,0.826872,"Missing"
D17-1206,P82-1020,0,0.819591,"Missing"
D17-1206,N01-1025,0,0.0296584,"Missing"
D17-1206,J81-4005,0,0.687478,"Missing"
D17-1206,E17-1117,0,0.0281293,"Missing"
D17-1206,S14-2055,0,0.0131081,"Missing"
D17-1206,D16-1076,0,0.0204543,"Missing"
D17-1206,D15-1176,0,0.0300327,"heir constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking Dependency UAS Dependency LAS Relatedness Entailment C↑ D↓ E↑ JMTall 97.55 n/a 94.67 92.90 0.233 86.2 JMTAB 97.52 95.77 n/a n/a n/a n/a JMTABC 97.54 n/a 94.71 92.92 n/a n/a JMTDE n/a n/a n/a n/a 0.238 86.8 JMTCD n/a n/a 93.53 91.62 0.251 n/a JMTCE n/a n/a 93.57 91.69 n/a 82.4 Table 1: Test set results for the five tasks. In the relatedness task, the lower scores are better. Method JMTall Ling et al. (2015) Kumar et al. (2016) Ma and Hovy (2016) Søgaard (2011) Collobert et al. (2011) Tsuruoka et al. (2011) Toutanova et al. (2003) Acc. ↑ 97.55 97.78 97.56 97.55 97.50 97.29 97.28 97.27 Method JMTAB Single Søgaard and Goldberg (2016) Suzuki and Isozaki (2008) Collobert et al. (2011) Kudo and Matsumoto (2001) Tsuruoka et al. (2011) Table 3: Chunking results. Table 2: POS tagging results. Method JMTall JMTDE Zhou et al. (2016) Tai et al. (2015) MSE ↓ 0.233 0.238 0.243 0.253 JMTall 97.88 97.59 94.51 92.60 0.236 84.6 w/o SC 97.79 97.08 94.52 92.62 0.698 75.0 w/o LE 97.85 97.40 94.09 92.14 0.261 81.6 LA"
D17-1206,P16-1101,0,0.181132,"Missing"
D17-1206,P16-1105,0,0.122728,"closely-related tasks, such as POS tagging and chunking. However, the number of tasks was limited or they have very similar task settings like word-level tagging, and it was not clear how lower-level tasks could be also improved by combining higher-level tasks. More related to our work, Godwin et al. (2016) also followed Søgaard and Goldberg (2016) to jointly learn POS tagging, chunking, and language modeling, and Zhang and Weiss (2016) have shown that it is effective to jointly learn POS tagging and dependency parsing by sharing internal representations. In the field of relation extraction, Miwa and Bansal (2016) proposed a joint learning model for entity detection and relation extraction. All of them suggest the importance of multi-task learning, and we investigate the potential of handling different types of NLP tasks rather than closely-related ones in a single hierarchical deep model. In the field of computer vision, some transfer and multi-task learning approaches have also been proposed (Li and Hoiem, 2016; Misra et al., 2016). For example, Misra et al. (2016) proposed a multi-task learning model to handle different tasks. However, they assume that each data sample has annotations for the differ"
D17-1206,P11-2009,0,0.0126651,"Missing"
D17-1206,P16-2038,0,0.691838,"ce Research. † Corresponding author. We introduce a Joint Many-Task (JMT) model, outlined in Figure 1, which predicts increasingly complex NLP tasks at successively deeper layers. Unlike traditional pipeline systems, our single JMT model can be trained end-to-end for POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment, by considering linguistic hierarchies. We propose an adaptive training and regularization strategy to grow this model in its depth. With the help of this strategy we avoid catastrophic interference between the tasks. Our model is motivated by Søgaard and Goldberg (2016) who showed that predicting two different tasks is more accurate when performed in different layers than in the same layer (Collobert et al., 2011). Experimental results show that our single model achieves competitive results for all of the five different tasks, demonstrating that us1923 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1923–1933 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing linguistic hierarchies is more important than handling different tasks in the same layer. 2 The Joint Many-Task"
D17-1206,N03-1033,0,0.0461524,"Missing"
D17-1206,W11-0328,1,0.301353,"Missing"
D17-1206,P15-1032,0,0.0320417,"Missing"
D17-1206,Q16-1019,0,0.0649491,"04 92.03 0.765 71.2 POS Chunking Dependency UAS Dependency LAS Table 7: Effectiveness of the Shortcut Connections (SC) and the Label Embeddings (LE). investigating such simple methods before developing complex methods for simple tasks. Currently, our JMT model does not explicitly use the learned dependency structures, and thus the explicit use of the output from the dependency layer should be an interesting direction of future work. Textual entailment Table 6 shows the results of textual entailment, and our JMT model achieves the state-of-the-art result. The previous state-ofthe-art result in Yin et al. (2016) relied on attention mechanisms and dataset-specific data preprocessing and features. Again, our simple maxpooling strategy achieves the state-of-the-art result boosted by the joint training. These results show the importance of jointly handling related tasks. 6.2 UAS ↑ 94.67 93.35 95.74 94.61 94.23 94.10 93.99 93.10 92.88 Table 4: Dependency results. Method JMTall JMTDE Yin et al. (2016) Lai and Hockenmaier (2014) Table 5: Semantic relatedness results. POS Chunking Dependency UAS Dependency LAS Relatedness Entailment Method JMTall Single Dozat and Manning (2017) Andor et al. (2016) Alberti et"
D17-1206,E17-1063,0,0.0843233,"ch as an adjective modifying a noun) between word pairs in a sentence. We use the third biLSTM layer to classify relations between all pairs of words. The input vector for the LSTM includes hidden states, word representations, and the label embeddings for the two previous tasks: (3) (3) (2) (pos) (chk) gt = [ht−1 ; ht ; xt ; (yt + yt )], where we computed the chunking vector in a similar fashion as the POS vector in Eq. (2). We predict the parent node (head) for each word. Then a dependency label is predicted for each child-parent pair. This approach is related to Dozat and Manning (2017) and Zhang et al. (2017), where the main difference is that our model works on a multi-task framework. To predict the parent node of wt , we define a matching function between wt and the candidates of the par(3) (3) ent node as m (t, j) = ht · (Wd hj ), where Wd is a parameter matrix. For the root, we define (3) hL+1 = r as a parameterized vector. To compute the probability that wj (or the root node) is the parent of wt , the scores are normalized: exp (m (t, j)) (3) p(j|ht ) = PL+1 . k=1,k6=t exp (m (t, k)) (3) The dependency labels are predicted using as input to a softmax classifier with a single ReLU layer. We gr"
D17-1206,P16-1147,0,0.107798,"Missing"
D17-1206,C16-1274,0,0.0956201,"uracy without the POS and chunking information. The best result to date has been achieved by the model propsoed in Dozat and Manning (2017), which uses higher dimensional representations than ours and proposes a more sophisticated attention mechanism called biaffine attention. It should be promising to incorporate their attention mechanism into our parsing component. Semantic relatedness Table 5 shows the results of the semantic relatedness task, and our JMT model achieves the state-of-the-art result. The result of “JMTDE ” is already better than the previous state-of-the-art results. Both of Zhou et al. (2016) and Tai et al. (2015) explicitly used syntactic trees, and Zhou et al. (2016) relied on attention mechanisms. However, our method uses the simple maxpooling strategy, which suggests that it is worth 4 Choe and Charniak (2016) employed a tri-training method to expand the training data with 400,000 trees in addition to the WSJ data, and they reported 95.9 UAS and 94.1 LAS by converting their constituency trees into dependency trees. Kuncoro et al. (2017) also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter. 1928 A↑ B↑ Single 97.45 95.02 93.35 91.42 0.247 81.8 POS Chunking De"
D17-1206,P08-1076,0,0.0292929,"Missing"
D17-1206,P15-1150,1,0.333124,"Missing"
E09-1090,W06-2920,0,0.0543335,"Missing"
E09-1090,P05-1022,0,0.749547,"obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their superior accuracy (Charniak and Johnson, 2005; Huang, 2008) Proceedings of the 12th Conference of the European Chapter of the ACL, pages 790–798, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 790 NP VBN VP QP NN VBD DT JJ CD CD NNS . NP VBD NP . ounces . Estimated volume was a light 2.4 million ounces . volume was Figure 1: Chunking, the first (base) level. Figure 3: Chunking, the 3rd level. NP S NP VBD DT JJ volume was a light QP million NNS . ounces . Figure 2: Chunking, the 2nd level. NP VP . volume was . Figure 4: Chunking, the 4th level. chain CRF model to perform chunking. Although our pa"
E09-1090,A00-2018,0,0.0635557,"Missing"
E09-1090,C04-1041,0,0.0107461,"of history-based approaches, it is one step closer to the whole-sentence approaches because the parser uses a whole-sequence model (i.e. CRFs) for individual chunking tasks. In other words, our parser could be located somewhere between traditional history-based approaches and whole-sentence approaches. One of our motivations for this work was that our parsing model may achieve a better balance between accuracy and speed than existing parsers. It is also worth mentioning that our approach is similar in spirit to supertagging for parsing with lexicalized grammar formalisms such as CCG and HPSG (Clark and Curran, 2004; Ninomiya et al., 2006), in which significant speed-ups for parsing time are achieved. In this paper, we show that our approach is indeed appealing in that the parser runs very fast and gives competitive accuracy. We evaluate our parser on the standard data set for parsing experiments (i.e. the Penn Treebank) and compare it with existing approaches to full parsing. This paper is organized as follows. Section 2 presents the overall chunk parsing strategy. Section 3 describes the CRF model used to perform individual chunking steps. Section 4 describes the depth-first algorithm for finding the b"
E09-1090,P07-1104,0,0.0328599,"a sentence. We examined the distribution of the heights of the trees in sections 2-21 of the Wall Street Journal (WSJ) corpus. The result is shown in Figure 5. Most of the sentences have less than 20 levels. The average was 10.0, which means we need to perform, on average, 10 chunking tasks to obtain a full parse tree for a sentence if the parsing is performed in a deterministic manner. 3 T X K X where R(λ) is introduced for the purpose of regularization which prevents the model from overfitting the training data. The L1 or L2 norm is commonly used in statistical natural language processing (Gao et al., 2007). We used L1-regularization, which is defined as R(λ) = Chunking with CRFs K 1 X |λk |, C k=1 where C is the meta-parameter that controls the degree of regularization. We used the OWL-QN algorithm (Andrew and Gao, 2007) to obtain the parameters that maximize the L1-regularized conditional log-likelihood. The accuracy of chunk parsing is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for ins"
E09-1090,P08-1067,0,0.269083,"ms. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Petrov and Klein (2008) introduced latent variables in tree CRFs and proposed a caching mechanism to speed up the computation. In general, the latter whole-sentence approaches give better accuracy than history-based approaches because they can better trade off decisions made in different parts in a parse tree. However, the whole-sentence approaches tend to require a large computational cost both in training and parsi"
E09-1090,P95-1037,0,0.0328373,"recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution of tree height in WSJ sections 2-21. Z(x) = X exp y a maximum entropy classifier and achieved an fscore of 85.9. However"
E09-1090,N06-1020,0,0.0327579,"porate some useful restrictions in producing chunking hypotheses. For example, we could naturally incorporate the restriction that every chunk has to contain at least one symbol that has just been created in the previous level3 . It is hard for the normal CRF model to incorporate such restrictions. Introducing latent variables into the CRF model may be another promising approach. This is the main idea of Petrov and Klein (2008), which significantly improved parsing accuracy. A totally different approach to improving the accuracy of our parser is to use the idea of “selftraining” described in (McClosky et al., 2006). The basic idea is to create a larger set of training data by applying an accurate parser (e.g. reranking parser) to a large amount of raw text. We can then use the automatically created treebank as the additional training data for our parser. This approach suggests that accurate (but slow) parsers and fast (but not-so-accurate) parsers can actually help each other. Also, since it is not difficult to extend our parser to produce N-best parsing hypotheses, one could build a fast reranking parser by using the parser as the base (hypotheses generating) parser. from the history-based model’s inab"
E09-1090,P08-1006,1,0.693944,"computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discriminative models attract more attention due to their s"
E09-1090,W06-1619,1,0.941698,"University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,W97-0301,0,0.205987,"Comparison with Previous Work Table 6 shows the performance of our parser on the test data and summarizes the results of previous work. Our parser achieved an f-score of 88.4 on the test data, which is comparable to the accuracy achieved by recent discriminative approaches such as Finkel et al. (2008) and Petrov & Klein (2008), but is not as high as the state-of-the-art accuracy achieved by the parsers that can incorporate global features such as Huang (2008) and Charniak & Johnson (2005). Our parser was more accurate than traditional history-based approaches such as Sagae & Lavie (2006) and Ratnaparkhi (1997), and was significantly better than previous cascaded chunking approaches such as Tsuruoka & Tsujii (2005) and Tjong Kim Sang (2001). Although the comparison presented in the table is not perfectly fair because of the differences in hardware platforms, the results show that our parsing model is a promising addition to the parsing frameworks for building a fast and accurate parser. 8 Conclusion 7 Discussion Although the idea of treating full parsing as a series of chunking problems has a long history, there has not been a competitive parser based on this parsing framework. In this paper, we hav"
E09-1090,P06-2089,0,0.308636,"mputer Science, University of Manchester, UK ‡ National Centre for Text Mining (NaCTeM), UK ∗ Department of Computer Science, University of Tokyo, Japan {yoshimasa.tsuruoka,j.tsujii,sophia.ananiadou}@manchester.ac.uk Abstract and adaptability to new grammars and languages (Buchholz and Marsi, 2006). A traditional approach to discriminative full parsing is to convert a full parsing task into a series of classification problems. Ratnaparkhi (1997) performs full parsing in a bottom-up and left-toright manner and uses a maximum entropy classifier to make decisions to construct individual phrases. Sagae and Lavie (2006) use the shiftreduce parsing framework and a maximum entropy model for local classification to decide parsing actions. These approaches are often called history-based approaches. A more recent approach to discriminative full parsing is to treat the task as a single structured prediction problem. Finkel et al. (2008) incorporated rich local features into a tree CRF model and built a competitive parser. Huang (2008) proposed to use a parse forest to incorporate non-local features. They used a perceptron algorithm to optimize the weights of the features and achieved state-of-the-art accuracy. Pet"
E09-1090,N03-1028,0,0.504054,"is highly dependent on the accuracy of each level of chunking. This section describes our approach to the chunking task. A common approach to the chunking problem is to convert the problem into a sequence tagging task by using the “BIO” (B for beginning, I for inside, and O for outside) representation. For example, the chunking process given in Figure 1 is expressed as the following BIO sequences. 3.2 Features Table 1 shows the features used in chunking for the base level. Since the task is basically identical to shallow parsing by CRFs, we follow the feature sets used in the previous work by Sha and Pereira (2003). We use unigrams, bigrams, and trigrams of part-of-speech (POS) tags and words. The difference between our CRF chunker and that in (Sha and Pereira, 2003) is that we could not use second-order CRF models, hence we could not use trigram features on the BIO states. We B-NP I-NP O O O B-QP I-QP O O This representation enables us to use the linearchain CRF model to perform chunking, since the task is simply assigning appropriate labels to a sequence. 792 Symbol Unigrams Symbol Bigrams Symbol Trigrams Word Unigrams Word Bigrams Word Trigrams s−2 , s−1 , s0 , s+1 , s+2 s−2 s−1 , s−1 s0 , s0 s+1 , s"
E09-1090,W05-1514,1,0.908906,", and converts it into NP. This process is repeated until the whole sentence is chunked at the fourth level. The full parse tree is recovered from the chunking history in a straightforward way. This idea of converting full parsing into a series of chunking tasks is not new by any means— the history of this kind of approach dates back to 1950s (Joshi and Hopely, 1996). More recently, Brants (1999) used a cascaded Markov model to parse German text. Tjong Kim Sang (2001) used the IOB tagging method to represent chunks and memory-based learning, and achieved an f-score of 80.49 on the WSJ corpus. Tsuruoka and Tsujii (2005) improved upon their approach by using 1 The head word is identified by using the headpercolation table (Magerman, 1995). 791 # sentences 5000 3.1 Linear Chain CRFs 4000 A linear chain CRF defines a single log-linear probabilistic distribution over all possible tag sequences y for the input sequence x: 3000 K T X X 1 λk fk (t, yt , yt−1 , x), exp p(y|x) = Z(x) t=1 k=1 2000 1000 where fk (t, yt , yt−1 , x) is typically a binary function indicating the presence of feature k, λk is the weight of the feature, and Z(X) is a normalization function: 0 0 5 10 15 Height 20 25 30 Figure 5: Distribution"
E09-1090,P06-1006,0,0.0111322,"king. The probability of an entire parse tree is computed as the product of the probabilities of individual chunking results. The parsing is performed in a bottom-up manner and the best derivation is efficiently obtained by using a depthfirst search algorithm. Experimental results demonstrate that this simple parsing framework produces a fast and reasonably accurate parser. 1 Introduction Full parsing analyzes the phrase structure of a sentence and provides useful input for many kinds of high-level natural language processing such as summarization (Knight and Marcu, 2000), pronoun resolution (Yang et al., 2006), and information extraction (Miyao et al., 2008). One of the major obstacles that discourage the use of full parsing in large-scale natural language processing applications is its computational cost. For example, the MEDLINE corpus, a collection of abstracts of biomedical papers, consists of 70 million sentences and would require more than two years of processing time if the parser needs one second to process a sentence. Generative models based on lexicalized PCFGs enjoyed great success as the machine learning framework for full parsing (Collins, 1999; Charniak, 2000), but recently discrimina"
E09-1090,E99-1016,0,\N,Missing
E09-1090,J03-4003,0,\N,Missing
E09-1090,P08-1109,0,\N,Missing
H05-1059,J96-1002,0,0.11937,"As an example, consider the situation where we are going to annotate a three-word sentence with part-of-speech tags. Figure 1 shows the four possible ways of decomposition. They correspond to the following equations: (a) P (t1 ...t3 |o) = P (t1 |o)P (t2 |t1 o)P (t3 |t2 o) (5) (b) P (t1 ...t3 |o) = P (t3 |o)P (t2 |t3 o)P (t1 |t2 o) (6) p(ti |ti−1 o). (3) i=1 Then we can employ a probabilistic classifier trained with the preceding tag and observations in order to obtain p(ti |ti−1 o) for local classification. A common choice for the local probabilistic classifier is maximum entropy classifiers (Berger et al., 1996). The best tag sequence can be efficiently computed by using a Viterbi decoding algorithm in polynomial time. 468 (c) P (t1 ...t3 |o) = P (t1 |o)P (t3 |o)P (t2 |t3 t1 o) (7) (d) P (t1 ...t3 |o) = P (t2 |o)P (t1 |t2 o)P (t3 |t2 o) (8) (a) and (b) are the standard left-to-right and rightto-left decompositions. Notice that in decomposition (c), the local classifier can use the information about the tags on both sides when deciding t2 . If, for example, the second word is difficult to tag (e.g. an unknown word), we might as well take the decomposition structure (c) because the local classifier can"
H05-1059,A00-1031,0,0.174993,"Missing"
H05-1059,W02-1001,0,0.126992,"S Previous POS POS two back Next POS POS two ahead Bigram POS features Trigram POS features Previous tag Tag two back Next tag Tag two ahead Bigram tag features wi wi−1 wi−2 wi+1 wi+2 wi−2 , wi−1 wi−1 , wi wi , wi+1 wi+1 , wi+2 pi pi−1 pi−2 pi+1 pi+2 pi−2 , pi−1 pi−1 , pi pi , pi+1 pi+1 , pi+2 pi−2 , pi−1 , pi pi−1 , pi , pi+1 pi , pi+1 , pi+2 ti−1 ti−2 ti+1 ti+2 ti−2 , ti−1 ti−1 , ti+1 ti+1 , ti+2 & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti & ti Table 4: Feature templates used in chunking experiments. (Collins, 2002) and used POS-trigrams as well. Table 4 lists the features used in chunking experiments. Table 5 shows the results on the development set. Again, bidirectional methods exhibit better performance than unidirectional methods. The difference is bigger with the Start/End representation. Dependency networks did not work well for this chunking task, especially with the Start/End representation. We applied the best model on the development set in each chunk representation type to the test data. Table 6 summarizes the performance on the test set. Our bidirectional methods achieved Fscores of 93.63 and"
H05-1059,W03-1018,1,0.624902,"Markov assumption obviously allows us to skip most of the probability updates, resulting in O(kn) invocations of local classifiers. This enables us to build a very efficient tagger. 3 Maximum Entropy Classifier For local classifiers, we used a maximum entropy model which is a common choice for incorporating various types of features for classification problems in natural language processing (Berger et al., 1996). Regularization is important in maximum entropy modeling to avoid overfitting to the training data. For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). The model gives equally good performance as the maximum entropy modeling with Gaussian priors (Chen and Rosenfeld, 1999), and the size of the resulting model is much smaller than that of Gaussian priors because most of the parameters become zero. This characteristic enables us to easily handle the model data and carry out quick decoding, which is convenient when we repetitively perform experiments. This modeling has one parameter to tune, which is called the width factor. We tuned this parameter using the development data in each type of experiments. Current word Previous word Next word Bigr"
H05-1059,N01-1025,0,0.134836,"methods for structured data share problems of computational cost (Altun et al., 2003). Another advantage is that one can employ a variety of machine learning algorithms as the local classifier. There is huge amount of work about developing classification algorithms that have high generalization performance in the machine learning community. Being able to incorporate such state-of-theart machine learning algorithms is important. Indeed, sequential classification approaches with kernel support vector machines offer competitive performance in POS tagging and chunking (Gimenez and Marquez, 2003; Kudo and Matsumoto, 2001). One obvious way to improve the performance of sequential classification approaches is to enrich the information that the local classifiers can use. In standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the target word. To make use of the information about future tags, Toutanova et al. proposed a tagging algorithm based on bidirectional dependency networks 467 Proceedings of Human Language Technology Conference and Conference on Empirical Met"
H05-1059,W00-0730,0,0.0351022,"Missing"
H05-1059,W00-0726,0,0.148821,"Missing"
H05-1059,E99-1023,0,0.0050195,"py modeling can achieve comparable performance to other state-of-the-art POS tagging methods. 4.2 Chunking Experiments The task of chunking is to find non-recursive phrases in a sentence. For example, a chunker segments the sentence “He reckons the current account deficit will narrow to only 1.8 billion in September” into the following, [NP He] [VP reckons] [NP the current account deficit] [VP will narrow] [PP to] [NP only 1.8 billion] [PP in] [NP September] . We can regard chunking as a tagging task by converting chunks into tags on tokens. There are several ways of representing text chunks (Sang and Veenstra, 1999). We tested the Start/End representation in addition to the popular IOB2 representation since local classifiers can have fine-grained information on the neighboring tags in the Start/End representation. For training and testing, we used the data set provided for the CoNLL-2000 shared task. The training set consists of section 15-18 of the WSJ corpus, and the test set is section 20. In addition, we made the development set from section 21 3 . We basically adopted the feature set provided in 3 We used the Perl script http://ilk.kub.nl/˜ sabine/chunklink/ provided on 472 Current POS Previous POS"
H05-1059,P03-1064,0,0.0349904,"Missing"
H05-1059,N03-1033,0,0.399624,"s can use. In standard decomposition techniques, the local classifiers cannot use the information about future tags (e.g. the right-side tags in left-to-right decoding), which would be helpful in predicting the tag of the target word. To make use of the information about future tags, Toutanova et al. proposed a tagging algorithm based on bidirectional dependency networks 467 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 467–474, Vancouver, October 2005. 2005 Association for Computational Linguistics (Toutanova et al., 2003) and achieved the best accuracy on POS tagging on the Wall Street Journal corpus. As they pointed out in their paper, however, their method potentially suffers from “collusion” effects which make the model lock onto conditionally consistent but jointly unlikely sequences. In their modeling, the local classifiers can always use the information about future tags, but that could cause a double-counting effect of tag information. In this paper we propose an alternative way of making use of future tags. Our inference method considers all possible ways of decomposition and chooses the “best” decompo"
H05-1059,J93-2004,0,\N,Missing
I08-2122,I05-1018,1,0.788734,"se. In order to observe such differences, we need to integrate available combinations of tools into a workflow and to compare the combinatorial results. Although generic frameworks like UIMA (Unstructured Information Management Architecture) provide interoperability to solve this problem, the solution they provide is only partial. In order for truly interoperable toolkits to become a reality, we also need 1 Introduction Recently, an increasing number of TM/NLP tools such as part-of-speech (POS) taggers (Tsuruoka et al., 2005), named entity recognizers (NERs) (Settles, 2005) syntactic parsers (Hara et al., 2005) and relation or event extractors (ERs) have been developed. Nevertheless, it is still very difficult to integrate independently developed tools into an aggregated application that achieves a specific task. The difficulties are caused not only by differences in programming platforms and different input/output data formats, but also by the lack of higher level interoperability among modules developed by different groups. 859 uima.jcas.cas.TOP tcas.uima.Annotation -begin: int -end: int SyntacticAnnotation POS SemanticAnnotation UnknownPOS PennPOS -posType: String Token Sentence Phrase NamedEntit"
I08-2122,W04-1213,0,0.0372378,"Missing"
I08-2122,J93-2004,0,0.0293416,"type systems have to be related through a sharable type system, which our platform defines. Such a shared type system can bridge modules with different type systems, though the bridging module may lose some information during the translation process. Whether such a sharable type system can be defined or not is dependent on the nature of each problem. For example, a sharable type system for POS tags in English can be defined rather easily, since most of POS-related modules (such as POS taggers, shallow parsers, etc.) more or less follow the well established types defined by the Penn Treebank (Marcus et al., 1993) tag set. Figure 1 shows a part of our sharable type system. We deliberately define a highly organized type hierarchy as described above. Secondly we should consider that the type system may be used to compare a similar sort of tools. Types should be defined in a distinct and 861 hierarchical manner. For example, both tokenizers and POS taggers output an object of type Token, but their roles are different when we assume a cascaded pipeline. We defined Token as a supertvpe, POSToken as subtypes of Token. Each tool should have an individual type to make clear which tool generated which instance,"
I08-2122,E06-1015,0,0.0352696,"Missing"
I08-2122,J96-1002,0,0.0129233,"Missing"
I11-1035,D08-1017,0,0.0345737,"t. 3.2 New Features for POS Tagging We generate n-gram and lexicon features for POS tagging as well. In addition, the features that incorporate word clusters derived from a large autoanalyzed corpus (referred to as cluster features) are introduced. • For the development and test sets, we collect a lexicon using the entire training corpus and use it for feature generation. Because the lexicon is extracted from other sets, the weights for this feature will not be overestimated by the learning algorithm. This kind of cross-validation-like techniques are used in studies such as Collins (2002) and Martins et al. (2008) to avoid over-fitting to the training data. Our method can be considered as its application to lexicon extraction. Using the extracted lexicon, we generate lexicon features as follows. If a character sequence starting with character c0 matches some words in the lexicon, we greedily choose the longest such matching word w. Letting LEN (w) be the length (the number of characters) of w, we add the following feature for each character ck in c0 , c1 , ..., cLEN (w) : (b) P (ck )/LEN (w)-P OSs(w) Here, P (ck ) is the position number (i.e., k) of the character ck in w and P OSs(w) represents the POS"
I11-1035,P09-1058,1,0.736673,"Missing"
I11-1035,W03-1719,0,0.0122318,"81 0.9112 CTB7 0.8996 0.9017 0.9020 0.9019 0.9046 Table 7: Results of word segmentation POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) CTB5 0.9318 0.9333 0.9350 0.9346 0.9359 CTB6 0.8999 0.9014 0.9026 0.9015 0.9048 CTB7 0.8937 0.8958 0.8959 0.8959 0.8985 POS tag method Baseline +(c) n-gram +(d) cluster +(e) lexicon +(c)+(d)+(e) Table 8: F1 results of segmentation and POS tagging (baseline model for word segmentation) Table 9: F1 results of segmentation and POS tagging (our best model for word segmentation) the words in the test set that are not in the training set (Sproat and Emerson, 2003). The development sets were used to obtain the optimal values of tunable parameters and feature configurations. The unlabeled data for our experiments were taken from the XIN_CMN portion of Chinese Gigaword Version 2.0 (LDC2009T14), which has approximately 311 million words. Some of CTB data and Chinese Gigaword data are from the same source: Xinhua newswire between 1994 and 1998. In order to avoid overlap between the CTB data and the unlabeled data, we used only the articles published in 1991- 1993 and 1999-2004 as unlabeled data, with 204 million words.8 Note that we only used one million wo"
I11-1035,Y06-1012,0,0.122089,"(and labeled) data into the above baseline models through features. We preprocess unlabeled data with our baseline models and obtain wordsegmented sentences with POS tags, and generate new features from the auto-analyzed data. Although the focus of the paper is semi-supervised learning, we also extract a lexicon from the training corpus and use it to generate features. Figure 1 shows an overview of our approach. The rest of this section describes our features in detail. Segmentation and POS tagging Models We implement our approach using sequential tagging models. Following the previous work (Zhao et al., 2006; Zhao et al., 2010), we employ the linear chain CRFs (Lafferty et al., 2001) as our learning model. Specifically, we use its CRF++ (version 0.54) implementation by Taku Kudo. 1 2.1 Baseline Segmentation Model 3.1 New features for Word Segmentation We employ character-based sequence labeling for word segmentation. In addition to its simplicity, the advantage of a character-based model is its robustness to the unknown word problem (Xue, 2003). In a character-based Chinese word segmentation task, a character in a given sequence is labeled by a tag that stands for its position in the word that th"
I11-1035,P08-1068,0,0.0436031,"Missing"
I11-1035,P07-2055,0,0.142272,"ry is correctly identified. For Seg &Tag, a word is considered correct only when both the word boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was"
I11-1035,P08-1102,0,0.21839,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,C08-1049,0,0.103149,"rence from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the version with Yates’ correction, using correction factor 0.5 315 Sentences added 0(Baseline) 5k 10k 30k 150k 300k 600k Segmentation F1 0.9498"
I11-1035,I08-1012,1,0.81753,"gging by incorporating large unlabeled data. We first preprocess unlabeled data with our baseline models. We then extract various items of dictionary information from the auto-analyzed data. Finally, we generate new features that incorporate the extracted information for both word segmentation and POS tagging. We also perform word clustering on the auto-segmented data and use word clusters as features in POS tagging. In addition, we introduce lexicon features by using a crossvalidation technique. The use of sub-structures from the autoannotated data has been presented previously (Noord, 2007; Chen et al., 2008; Chen et al., 2009). Chen et al. (2009) extracted different types of subtrees from the auto-parsed data and used them as new features in standard learning methods. They showed this simple method greatly improves the accuracy of dependency parsing. The idea of combining word clusters with discriminative learning has been previously reported in the context of named entity recognition (Miller et al., 2004; Kazama and Torisawa, 2008) and dependency parsing (Koo et al., 2008). We adapt and extend these techniques to Chinese word segmentation and POS tagging, and demonstrate their effectiveness in"
I11-1035,D09-1060,1,0.381889,"Missing"
I11-1035,I08-4029,0,0.134356,"be a character n-gram (e.g., uni-gram ci , bi-gram ci ci+1 , trigram ci−1 ci ci+1 and so on)2 , and seg be a segmentation profile for n-gram g observed at each position. The segmentation profile can be tag ti or the combination of tags. Take a bi-gram for example, seg may be ti or ti ti+1 . Then, 2.2 Baseline POS Tagging Model Since we employ a pipelined method, the POS tagging can be performed as a word labeling task, where the input is a sequence of segmented words. We use a CRF here as well. The feature set of our baseline POS tagger, is listed in Table 3. These are adopted from Wu et al. (2008). 1 2 Note that there are several alternative ways for extracting n-grams at position i, for example ci−1 ci for a bi-gram. In this paper, we used the way as explained here. Available from http://crfpp.sourceforge.net/ 310 Feature Type Word Unigram Nearing Word Bigram Jump Word Bigram First Character Last Character Length Context Position w−2 ,w−1 ,w0 ,w1 ,w2 (w−2 w−1 ),(w−1 w0 ),(w1 w0 ),(w1 w2 ) (w−1 ,w1 ) F c(w0 ) Lc(w0 ) Len(w0 ) Description Word unigram Word bigram Previous word and next word First character of current word Last character of current word Length of current word Table 3: Fe"
I11-1035,D09-1058,0,0.0118399,"r-level NLP tasks such as parsing and information extraction. Although the performance of Chinese word segmentation and POS tagging has been greatly improved over the past years, the task is still challenging. To improve the accuracy of NLP systems, one of the current trends is semi-supervised learning, which utilizes large unlabeled data in supervised learning. Several studies have demonstrated that the use of unlabeled data can improve the performance of NLP tasks, such as text chunking (Ando and Zhang, 2005), POS tagging and named entity recognition (Suzuki and Isozaki, 2008), and parsing (Suzuki et al., 2009; Chen et al., 2009; Koo et al., 2008). Therefore, it is attractive to consider adopting semi-supervised learning in Chinese word segmentation and POS tagging tasks. 309 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 309–317, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP Word Length Tags 1 S 2 BE 3 BB2 E 4 BB2 B3 E 5 BB2 B3 M E 6 BB2 B3 M M E 7 or more BB2 B3 M...M E Table 1: Word representation with a 6-tag tagset : S, B, B2 , B3 , M, E Type Character Unigram Nearing Character Bigram Jump Character Bigram Punctuation Character Type Feat"
I11-1035,D10-1082,0,0.4394,"boundary and its POS tag are correctly identified. Table 13 summarizes the results on test sets. These tests suggest that although the difference from K 09b for CTB5 data is not statistically significant, all other differences are clearly statistically significant (p < 10−5 ). 4.4 Comparative Results In this section, we compare our approach with the best previous approaches reported in the literature. The performance scores of previous studies are directly taken from their papers, except for N&U 07 (Nakagawa and Uchimoto, 2007), which is taken from Kruengkrai et al. (2009b). Z&C 10 refers to Zhang and Clark (2010). Two methods in Kruengkrai et al. (2009a; 2009b) are referred to as K 09a and K 09b. Jiang 08a and Jiang 08b refer to Jiang et al. (2008a; 2008b). Table 10 compares F1 results on CTB5.0. The best score in each column is in boldface. The results of our approach are superior to those of previous studies for both 4.6 Comparison with Self-Training An alternative method of incorporating unlabeled data is self-training, so we also compared our results to the self-training method. Because no existing research was found concerning the selftraining method on word segmentation and POS 9 We used the ver"
I11-1035,N04-1043,0,\N,Missing
I11-1035,W07-2201,0,\N,Missing
I11-1035,P08-1047,1,\N,Missing
I11-1035,P08-1076,0,\N,Missing
I11-1035,O03-4002,0,\N,Missing
I11-1035,I05-3025,0,\N,Missing
I11-1035,W03-1726,0,\N,Missing
I11-1035,P02-1062,0,\N,Missing
I11-1035,N06-1020,0,\N,Missing
J19-2003,P17-2021,0,0.0175083,"urce sentence depending on a task of interest. 288 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Although it is relatively easy to encode a source syntactic tree into a vector space, decoding a target sentence with a parse tree is known as a challenging task and has been recently explored in sentence generation tasks (Dong and Lapata 2016) and language modeling tasks (Dyer et al. 2016). Wu et al. (2017) and Eriguchi, Tsuruoka, and Cho (2017) have proposed hybrid models that jointly learn to parse and translate by using target-side dependency trees, and Aharoni and Goldberg (2017) serialize a target parse tree to a sequence of units in order to apply it to the sequence-to-sequence NMT model. As we can see in the recent active studies of syntax-based approaches in the MT area, we believe that incorporating structural biases into NLP models is a promising research direction. 8. Conclusion We propose a syntactic approach that extends the existing sequence-to-sequence NMT models. We focus on source-side phrase structures and build a tree-based encoder following the parse trees. Our proposed tree-based encoder is a natural extension of the sequential encoder model, where th"
J19-2003,D17-1209,0,0.0139592,"sed a more effective attention path in the calculation of the NMT models. To incorporate structural information into the NMT models, Cho et al. (2014a) proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance. These studies motivated us to investigate the role of syntactic structures explicitly given by existing syntactic parsers in the NMT models. Incorporating the syntactic structures into the NMT models has been actively studied recently. Chen et al. (2017) have extended our work by using bi-directional RNNs and RvNNs. Bastings et al. (2017) apply graph CNNs to encode the dependency trees. The previous work relies on syntactic trees provided by parsers and has a demerit of introducing parse errors into the models, whereas another trend is to learn and enhance latent structures or relations between any input units in source sentences in the NMT models (Bradbury and Socher 2017; Vaswani et al. 2017; Yoon Kim and Rush 2017). Hashimoto and Tsuruoka (2017) and Tran and Bisk (2018) reported an interesting observation that the latent parsed trees learned through the training are not always consistent with those obtained by the existing"
J19-2003,W17-4303,0,0.0155778,"tactic structures explicitly given by existing syntactic parsers in the NMT models. Incorporating the syntactic structures into the NMT models has been actively studied recently. Chen et al. (2017) have extended our work by using bi-directional RNNs and RvNNs. Bastings et al. (2017) apply graph CNNs to encode the dependency trees. The previous work relies on syntactic trees provided by parsers and has a demerit of introducing parse errors into the models, whereas another trend is to learn and enhance latent structures or relations between any input units in source sentences in the NMT models (Bradbury and Socher 2017; Vaswani et al. 2017; Yoon Kim and Rush 2017). Hashimoto and Tsuruoka (2017) and Tran and Bisk (2018) reported an interesting observation that the latent parsed trees learned through the training are not always consistent with those obtained by the existing parsers, which suggests that there might exist a favorable structure of a source sentence depending on a task of interest. 288 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Although it is relatively easy to encode a source syntactic tree into a vector space, decoding a target sentence with a parse t"
J19-2003,P17-1177,0,0.023463,"ynamically focus on local windows rather than the entire sentence. They also proposed a more effective attention path in the calculation of the NMT models. To incorporate structural information into the NMT models, Cho et al. (2014a) proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance. These studies motivated us to investigate the role of syntactic structures explicitly given by existing syntactic parsers in the NMT models. Incorporating the syntactic structures into the NMT models has been actively studied recently. Chen et al. (2017) have extended our work by using bi-directional RNNs and RvNNs. Bastings et al. (2017) apply graph CNNs to encode the dependency trees. The previous work relies on syntactic trees provided by parsers and has a demerit of introducing parse errors into the models, whereas another trend is to learn and enhance latent structures or relations between any input units in source sentences in the NMT models (Bradbury and Socher 2017; Vaswani et al. 2017; Yoon Kim and Rush 2017). Hashimoto and Tsuruoka (2017) and Tran and Bisk (2018) reported an interesting observation that the latent parsed trees learn"
J19-2003,W14-4012,0,0.169688,"Missing"
J19-2003,D14-1179,0,0.0609652,"Missing"
J19-2003,P16-1004,0,0.0278554,"(2018) reported an interesting observation that the latent parsed trees learned through the training are not always consistent with those obtained by the existing parsers, which suggests that there might exist a favorable structure of a source sentence depending on a task of interest. 288 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Although it is relatively easy to encode a source syntactic tree into a vector space, decoding a target sentence with a parse tree is known as a challenging task and has been recently explored in sentence generation tasks (Dong and Lapata 2016) and language modeling tasks (Dyer et al. 2016). Wu et al. (2017) and Eriguchi, Tsuruoka, and Cho (2017) have proposed hybrid models that jointly learn to parse and translate by using target-side dependency trees, and Aharoni and Goldberg (2017) serialize a target parse tree to a sequence of units in order to apply it to the sequence-to-sequence NMT model. As we can see in the recent active studies of syntax-based approaches in the MT area, we believe that incorporating structural biases into NLP models is a promising research direction. 8. Conclusion We propose a syntactic approach that exten"
J19-2003,N16-1024,0,0.072011,"Missing"
J19-2003,P16-1078,1,0.789606,"Missing"
J19-2003,P17-2012,1,0.884602,"Missing"
J19-2003,D17-1012,1,0.883033,"utputs shorter than the references. 4.2 Training Details We conduct experiments with the sequence-to-sequence NMT model as a baseline and our proposed model described in Sections 2 and 3, respectively. Each model has 256-dimensional hidden units and word embeddings. The biases, softmax weights, and BlackOut weights are initialized with zeros. The hyperparameter β of BlackOut is set to 0.4 as recommended by Ji et al. (2016). The number of negative samples K in BlackOut was set to K ∈ {500, 2000}. Here, we shared the negative samples of each target word in a sentence in training time, following Hashimoto and Tsuruoka (2017). ´ Following Jozefowicz, Zaremba, and Sutskever (2015), we initialize the forget gate biases of LSTM and Tree-LSTM with 1.0. The remaining model parameters in the NMT models in our experiments are uniformly initialized in [−0.1, 0.1]. The model parameters are optimized by plain SGD with the mini-batch size of 128. The initial learning rate of SGD is 1.0. When the development loss becomes worse per epoch, we halve the learning rate from the next epoch until it converges. When INF/NAN values appear in a mini-batch during training, we skip the mini-batch training. Gradient norms are clipped to 3"
J19-2003,D10-1092,0,0.0243083,"he optimal beam width found on the development data set. 5 Contrary to Stanford Parser, Enju returns a binarized tree. 6 When the Enju parser fails to parse a sentence because of “sentence length limit exceeded,” we let the sentence be parsed again with an additional option of “-W 200” to increase the limit size of sentences up to 200. We found one sentence in both the development data and test data that is parsed again with the option. 276 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al. 2010) and BLEU (Papineni et al. 2002) following WAT 2015. We used the KyTeabased evaluation script for the translation results.7 The RIBES score is a metric based on rank correlation coefficients with word precision, and this score is known to have stronger correlation with human judgments than BLEU in translation between English and Japanese, as discussed in Isozaki et al. (2010). The BLEU score is based on n-gram word precision and a brevity penalty for outputs shorter than the references. 4.2 Training Details We conduct experiments with the sequence-to-sequence NMT model as a baseline and our pr"
J19-2003,D13-1176,0,0.062634,"Missing"
J19-2003,W04-3250,0,0.201142,"orts the values of perplexity, RIBES, BLEU, and the training time on the development data with the NMT models. We conducted the experiments with our proposed methods and the baseline of the sequence-to-sequence model using BlackOut and the original softmax. We generated each translation by our proposed beam search with a beam size of 20. In both tables, we report the experimental results obtained by feeding the reversed inputs into the sequence-to-sequence NMT models (shown as “w/ reverse inputs” in tables), and by utilizing the bi-directional encoders. We ran the bootstrap resampling method (Koehn 2004) and observed a statistical significant difference on both RIBES and BLEU scores between the proposed method and the sequence-to-sequence NMT model with reversed inputs in all the settings. The symbol † indicates that our proposed model significantly outperforms the sequence-to-sequence NMT model both without and with reversed inputs except the model “w/ bi-directional encoder” in the corresponding settings on K (p &lt; 0.05). As the negative sample size K increases, we can see that both the proposed model (Tree-to-Sequence NMT) and the sequence-to-sequence NMT models (Sequenceto-Sequence NMT, Se"
J19-2003,W17-3204,0,0.025562,"m Search with Penalized Length We use beam search to decode a target sentence for an input sentence x and calculate the sum of the log-likelihood values of the target sentence y = (y1 , · · · , ym ) as the beam score: score(xx, y ) = m X log p(yj |y &lt;j , x ) (26) j=1 Decoding in the NMT models is a generative process and depends on the target language model given a source sentence. The score becomes smaller as the target sentence becomes longer, and thus the simple beam search does not work well when decoding a long sentence, as reported in Cho et al. (2014a), Pouget-Abadie et al. (2014), and Koehn and Knowles (2017). We apply a method to utilize statistics on sentence lengths in beam search. Assuming that the length of a target sentence correlates with the length of the source sentence, 7 http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/automatic_evaluation_systems/ automaticEvaluationJA.html. 277 Computational Linguistics Volume 45, Number 2 we redefine the score of each candidate as follows: score(xx, y ) = Lengthx ,yy + m X log p(yj |y &lt;j , x ) (27) j=1 Lengthx ,yy = log p(length(yy )|length(xx )) (28) where Lengthx ,yy denotes the penalty for the conditional probability of the target sentence length le"
J19-2003,W15-5008,0,0.0183971,"g, our implementation needs about 8 hours to perform one epoch on the large training data set with d = 512. It would take about 8 days without using the BlackOut sampling.10 Comparison with the NMT Models. Compared with our reimplementation of the sequence-to-sequence NMT model (Luong, Pham, and Manning 2015), we did not observe a significant difference from the tree-to-sequence model (d = 512). The model of Zhu (2015) is an NMT model (Bahdanau, Cho, and Bengio 2015) with a bi-directional LSTM encoder, and uses 1,024-dimensional hidden units and 1,000-dimensional word embeddings. The model of Lee et al. (2015) is also an NMT model with a bi-directional GRU encoder, and uses 1,000-dimensional hidden units and 200-dimensional word embeddings. Both models are sequence-based NMT models. Our single proposed model with d = 512 outperforms Zhu (2015)’s sequence-to-sequence NMT model with a bi-directional LSTM encoder by +1.96 RIBES and by +2.86 BLEU scores. 9 We found two sentences that end without EOS with d = 512, and then we decoded them again with the beam size of 1,000 following (Zhu 2015). 10 We run the experiments on multi-core CPUs; 16 threads on Intel Xeon CPU E5-2667 v3 @ 3.20 GHz. 283 Computati"
J19-2003,P03-1056,0,0.0261313,"no additional model parameters for BlackOut. 4. Experimental Design 4.1 Experimental Settings Chinese-to-Japanese. We used the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al. 2016) for the Chinese-to-Japanese translation provided by the Workshop of Asian Translation 2015 (WAT2015). Following the official preprocessing steps,1 we tokenized the data by using KyTea (Neubig, Nakata, and Mori 2011) as a Japanese segmenter and Stanford Segmenter as a Chinese word segmenter.2 We discarded a sentence pair when either of the sentences had more than 50 words. We used the Stanford Parser (Levy and Manning 2003)3 as an external parser and parsed the Chinese sentences. We did not use any more specific information output by the parsers such as phrase labels. If a sentence fails to be parsed, it is re-parsed by a simpler probabilistic context-free grammar parser. We used a script4 to convert the parse trees to their corresponding binary trees because our tree-to-sequence model assumes binary trees as inputs. We used the first 100,000 parallel sentences from the training data to investigate the effectiveness of the proposed NMT model. The vocabularies were composed of the words appearing in the training"
J19-2003,P06-1077,0,0.19929,"Missing"
J19-2003,D15-1166,0,0.11634,"Missing"
J19-2003,J08-1002,0,0.0403644,"0,000 2,090 Parsed (Chinese-Factored) 95,647 1,779 Parsed (PCFG) 4,353 311 Table 2 Statistics on the English-to-Japanese data set in ASPEC corpus. The number of “Sentences” is equal to the total number of the “Parsed successfully” sentences and the “Parsed unsuccessfully” sentences. Large Train Small Train Development Test Sentences 1,353,635 100,000 1,790 1,812 Parsed successfully 1,346,946 99,541 1,779 1,801 Parsed unsuccessfully 6,689 459 11 11 English-to-Japanese. We used the ASPEC corpus (Nakazawa et al. 2016) for the Englishto-Japanese translation task provided by WAT2015. We used Enju (Miyao and Tsujii 2008), a head-driven phrase structure grammar (Sag, Wasow, and Bender 2003) parser for English, and the tokenization in English follows the Enju parser.5 The English corpus was lowercased. We followed the official preprocessing for the Japanese corpus as in the Chinese-to-Japanese experimental settings. We used Enju only to obtain a binary phrase structure for each source-side sentence. We removed the sentence pairs in which either of the sentences is longer than 50 words. Enju returns either success or failure after parsing an English sentence. When the Enju parser fails to parse a sentence, it is"
J19-2003,W15-5001,0,0.465628,"aseline: Tree-to-String SMT model Tree-to-String SMT model (Neubig and Duh 2014) + Seq-to-Seq NMT Rerank (Neubig, Morishita, and Nakamura 2015) RIBES 81.66 81.83 81.94 83.27 BLEU 35.05 35.73 35.57 38.00 81.60 34.64 79.70 80.27 80.91 32.19 34.19 36.21 81.15 35.75 69.19 74.70 75.80 79.65 81.38 29.80 32.56 33.44 36.58 38.17 6. Experimental Results and Discussion on Large Data Set 6.1 Experimental Results on Large Data Set Table 9 shows the experimental results of RIBES and BLEU scores achieved by the trained models on the large data set.9 The results of the other systems are the ones reported in Nakazawa et al. (2015). All of our proposed models show similar performance regardless of the value of d. Our ensemble model is composed of the three models with d = 512,768, and 1,024, and it shows the best RIBES score among all systems. As for the time required for training, our implementation needs about 8 hours to perform one epoch on the large training data set with d = 512. It would take about 8 days without using the BlackOut sampling.10 Comparison with the NMT Models. Compared with our reimplementation of the sequence-to-sequence NMT model (Luong, Pham, and Manning 2015), we did not observe a significant di"
J19-2003,P14-2024,0,0.0812522,"the English sentence and align the phrase “a movie” with the Japanese word “ .” The verb phrase of “went to see a movie last night” is also related to the eight-word sequence “ .” Since Yamada and Knight (2001) proposed the first syntax-based alignment model, various approaches to leveraging the syntactic structures have been adopted in statistical machine translation (SMT) models (Liu, Liu, and Lin 2006). In SMT, it is known that incorporating source-side syntactic constituents into the models improves word alignment (Yamada and Knight 2001) and translation accuracy (Liu, Liu, and Lin 2006; Neubig and Duh 2014). However, the aforementioned NMT models do not allow one to perform this kind of alignment. To take advantage of syntactic information on the source side, we propose a syntactic NMT model. Following the phrase structure of a source sentence, we encode the Figure 1 The phrase structure of the English sentence “Mary and John went to see a movie last night” and its Japanese translation. 268 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT sentence recursively in a bottom–up fashion to produce a sentence vector by using a tree-structured recursive neural netw"
J19-2003,W15-5003,0,0.0373918,"Missing"
J19-2003,P11-2093,0,0.0616194,"Missing"
J19-2003,P17-1079,0,0.0200078,"tional probability, following (tree) Equations (6) and (7). Here, the context vector of d j is replaced with d j computed by Equation (20). 3.3 Sampling-Based Approximation to the NMT Models The computational cost in the softmax layer in Equation (6) occupies most of the training time because the cost increases linearly with the size of the vocabulary. A variety of approaches addressing this problem have been proposed, including negative sampling methods such as BlackOut sampling (Ji et al. 2016) and noise-contrastive estimation (NCE) (Gutmann and Hyv¨arinen 2012), and binary code prediction (Oda et al. 2017). BlackOut has been shown to be effective in training RNN language models even with one-million-word vocabulary on CPUs. The NMT models are trained as RNN-based conditional language models in Equation (10), and we apply the BlackOut sampling technique to the NMT models. We redefine the objective function as follows: J(θ ) = 1 |D | |y | X X    X log p( ˜ yk |y &lt;j , x )  (22) ˜ yj |y &lt;j , x ) + 1 − log p( (xx,yy )∈D j=1 k ∈S K j qj exp(sj )   ˜ yj |y &lt; j , x ) = p( P j qj exp(sj ) + k∈SK qk exp(skj ) (23) where |y |denotes the length of each target sentence y , and y k and SK denote a ne"
J19-2003,P02-1040,0,0.112315,"the development data set. 5 Contrary to Stanford Parser, Enju returns a binarized tree. 6 When the Enju parser fails to parse a sentence because of “sentence length limit exceeded,” we let the sentence be parsed again with an additional option of “-W 200” to increase the limit size of sentences up to 200. We found one sentence in both the development data and test data that is parsed again with the option. 276 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al. 2010) and BLEU (Papineni et al. 2002) following WAT 2015. We used the KyTeabased evaluation script for the translation results.7 The RIBES score is a metric based on rank correlation coefficients with word precision, and this score is known to have stronger correlation with human judgments than BLEU in translation between English and Japanese, as discussed in Isozaki et al. (2010). The BLEU score is based on n-gram word precision and a brevity penalty for outputs shorter than the references. 4.2 Training Details We conduct experiments with the sequence-to-sequence NMT model as a baseline and our proposed model described in Sectio"
J19-2003,W14-4009,0,0.0382141,"Missing"
J19-2003,P15-1150,0,0.133518,"Missing"
J19-2003,W18-2704,0,0.0157003,"actic structures into the NMT models has been actively studied recently. Chen et al. (2017) have extended our work by using bi-directional RNNs and RvNNs. Bastings et al. (2017) apply graph CNNs to encode the dependency trees. The previous work relies on syntactic trees provided by parsers and has a demerit of introducing parse errors into the models, whereas another trend is to learn and enhance latent structures or relations between any input units in source sentences in the NMT models (Bradbury and Socher 2017; Vaswani et al. 2017; Yoon Kim and Rush 2017). Hashimoto and Tsuruoka (2017) and Tran and Bisk (2018) reported an interesting observation that the latent parsed trees learned through the training are not always consistent with those obtained by the existing parsers, which suggests that there might exist a favorable structure of a source sentence depending on a task of interest. 288 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Although it is relatively easy to encode a source syntactic tree into a vector space, decoding a target sentence with a parse tree is known as a challenging task and has been recently explored in sentence generation tasks (Dong a"
J19-2003,P17-1065,0,0.0198323,"s learned through the training are not always consistent with those obtained by the existing parsers, which suggests that there might exist a favorable structure of a source sentence depending on a task of interest. 288 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Although it is relatively easy to encode a source syntactic tree into a vector space, decoding a target sentence with a parse tree is known as a challenging task and has been recently explored in sentence generation tasks (Dong and Lapata 2016) and language modeling tasks (Dyer et al. 2016). Wu et al. (2017) and Eriguchi, Tsuruoka, and Cho (2017) have proposed hybrid models that jointly learn to parse and translate by using target-side dependency trees, and Aharoni and Goldberg (2017) serialize a target parse tree to a sequence of units in order to apply it to the sequence-to-sequence NMT model. As we can see in the recent active studies of syntax-based approaches in the MT area, we believe that incorporating structural biases into NLP models is a promising research direction. 8. Conclusion We propose a syntactic approach that extends the existing sequence-to-sequence NMT models. We focus on sour"
J19-2003,1983.tc-1.13,0,0.492548,"Missing"
J19-2003,P01-1067,0,0.340531,"lly distant from each other in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this ” is aligned with the English word “movie.” The example, the Japanese word “ indefinite article “a” in English, however, is not explicitly translated into any Japanese words. One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase “a movie” with the Japanese word “ .” The verb phrase of “went to see a movie last night” is also related to the eight-word sequence “ .” Since Yamada and Knight (2001) proposed the first syntax-based alignment model, various approaches to leveraging the syntactic structures have been adopted in statistical machine translation (SMT) models (Liu, Liu, and Lin 2006). In SMT, it is known that incorporating source-side syntactic constituents into the models improves word alignment (Yamada and Knight 2001) and translation accuracy (Liu, Liu, and Lin 2006; Neubig and Duh 2014). However, the aforementioned NMT models do not allow one to perform this kind of alignment. To take advantage of syntactic information on the source side, we propose a syntactic NMT model. F"
J19-2003,Q16-1027,0,0.0232237,"Sutskever, Vinyals, and Le 2014), a recurrent neural network (RNN) called an encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called a decoder generates a sequence of target words from the vector. The Encoder-Decoder model has been extended with an attention mechanism (Bahdanau, Cho, and Bengio 2015; Luong, Pham, and Manning 2015), which allows the model to jointly learn soft alignments between the source words and the target words. Recently, NMT models have achieved state-of-the-art results in a variety of language pairs (Wu et al. 2016; Zhou et al. 2016; Gehring et al. 2017; Vaswani et al. 2017). In this work, we consider how to incorporate syntactic information into NMT. Figure 1 illustrates the phrase structure of an English sentence, which is represented as a binary tree. Each node of the tree corresponds to a grammatical phrase of the English sentence. Figure 1 also shows its translation in Japanese. The two languages are linguistically distant from each other in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this ” is aligned with the English word “movie.” The"
J19-2003,W15-5007,0,0.116684,"to-Japanese Proposed: Tree-to-Sequence NMT Dependency-to-Sequence NMT reimplementation of Hashimoto and Tsuruoka (2017) 282 Eriguchi, Hashimoto, and Tsuruoka Incorporating Source-Side Phrase Structures into NMT Table 9 Evaluation results on the test data in the English-to-Japanese translation task. NMT Model Tree-to-Sequence NMT model (d = 512) Tree-to-Sequence NMT model (d = 768) Tree-to-Sequence NMT model (d = 1,024) Ensemble of the above three models Sequence-to-Sequence NMT model (d = 512) reimplementation of Luong, Pham, and Manning (2015) Seq-to-Seq NMT with bi-directional LSTM encoder (Zhu 2015) + Ensemble, unk replacement + System combination, 3 pre-reordered ensembles Seq-to-Seq NMT with bi-directional GRU encoder (Lee et al. 2015) + character-based decoding, Begin/Inside representation SMT Model Baseline: Phrase-based SMT model Baseline: Hierarchical Phrase-based SMT model Baseline: Tree-to-String SMT model Tree-to-String SMT model (Neubig and Duh 2014) + Seq-to-Seq NMT Rerank (Neubig, Morishita, and Nakamura 2015) RIBES 81.66 81.83 81.94 83.27 BLEU 35.05 35.73 35.57 38.00 81.60 34.64 79.70 80.27 80.91 32.19 34.19 36.21 81.15 35.75 69.19 74.70 75.80 79.65 81.38 29.80 32.56 33.44 3"
K15-1027,P14-2131,0,0.316157,"paku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp words. For example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for"
K15-1027,D10-1115,0,0.0394017,"ings. To discriminate between words in n from those in win , wbef , and waf t , we have two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training samp"
K15-1027,D14-1199,0,0.136415,"Missing"
K15-1027,H05-1091,0,0.706306,"el features and no external annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused"
K15-1027,C14-1078,0,0.0277905,"Missing"
K15-1027,D14-1163,1,0.870336,"two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training sample, whose target word is w, √ is discarded with the probability t Pd (w) = 1 − p(w"
K15-1027,W06-1670,0,0.155868,"Missing"
K15-1027,P15-1061,0,0.28635,"RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBest . (a) Financial [stress]E1 is one of the main causes of [divorce]E2 (b) The [burst]E1 has been caused by water hammer [pressure]E2 Training example (a) is classified as CauseEffect(E1 , E2 ) which denotes that E2 is an effect caused by E1 , while training example (b) is cla"
K15-1027,N15-1133,0,0.0136109,"bag-of-words features included the noun pairs and words between, before, and after the pairs, and we used LIBLINEAR6 as our classifier. noun pairs in their contexts. The dataset, containing 8,000 training and 2,717 test samples, defines nine classes (Cause-Effect, Entity-Origin, etc.) for ordered relations and one class (Other) for other relations. Thus, the task can be treated as a 19class classification task. Two examples from the training set are shown below. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not re"
K15-1027,D13-1166,0,0.0424757,"Missing"
K15-1027,S07-1003,0,0.0433618,"Missing"
K15-1027,D11-1129,0,0.0721086,"Missing"
K15-1027,D14-1012,0,0.0246398,"r example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are"
K15-1027,D13-1137,1,0.921942,"ef , and waf t , we have two sets of word embeddings: N ∈ Rd×|N |and W ∈ Rd×|W |. W is a set of words and N is also a set of words but contains only nouns. Hence, the word cause has two embeddings: one in N and another in W. In general cause is used as a noun and a verb, and thus we expect the noun embeddings to capture the meanings focusing on their noun usage. This is inspired by some recent work on word representations that explicitly assigns an independent representation for each word usage according to its part-of-speech tag (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Hashimoto et al., 2013; Hashimoto et al., 2014; Kartsaklis and Sadrzadeh, 2013). n i=1 j=1 (3) where is a word randomly drawn from the unigram noise distribution weighted by an exponent of 0.75. Maximizing Junlabeled means that our method can discriminate between each target word and k noise words given the target word’s context. This approach is much less computationally expensive than the one-versus-rest approach and has proven effective in learning word embeddings. wj′ 270 noun pair: To reduce redundancy during training we use subsampling. A training sample, whose target word is w, √ is discarded with the probab"
K15-1027,P06-1104,0,0.0440265,"l annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-words features and features based on syntactic parse trees. For syntactic parse trees, the paths between the target entities on constituency and dependency trees have been demonstrated to be useful (Bunescu and Mooney, 2005; Zhang et al., 2006). On the shared task introduced by Hendrickx et al. (2010), Rink and Harabagiu (2010) achieved the best score using a variety of handcrafted features which were then used to train a Support Vector Machine (SVM). Recently, word embeddings have become popular as an alternative to hand-crafted features (Collobert et al., 2011). However, one of the limitations is that word embeddings are usually learned by predicting a target word in its context, leading to only local co-occurrence information being captured (Levy and Goldberg, 2014). Thus, several recent studies have focused on overcoming this li"
K15-1027,P14-2012,0,0.142311,"c1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-specific features on a large unlabeled corpus. We then use the word embeddings to construct lexical feature vectors for relation classification. Lastly, the feature vectors are used to train a relation cl"
K15-1027,S10-1057,0,0.278814,"use syntactic information or manually constructed external resources. 1 Introduction Automatic classification of semantic relations has a variety of applications, such as information extraction and the construction of semantic networks (Girju et al., 2007; Hendrickx et al., 2010). A traditional approach to relation classification is to train classifiers using various kinds of features with class labels annotated by humans. Carefully crafted features derived from lexical, syntactic, and semantic resources play a significant role in achieving high accuracy for semantic relation classification (Rink and Harabagiu, 2010). In recent years there has been an increasing interest in using word embeddings as an alternative to traditional hand-crafted features. Word embeddings are represented as real-valued vectors and capture syntactic and semantic similarity between 1 https://code.google.com/p/word2vec/. 268 Proceedings of the 19th Conference on Computational Language Learning, pages 268–278, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics Figure 1: The overview of our system (a) and the embedding learning method (b). In the example sentence, each of are, caused, and by is treate"
K15-1027,D12-1110,0,0.706942,"ty of useful features have been proposed for relation classification. Among them, we use dependency path features (Bunescu and Mooney, 2005) based on the untyped binary dependencies of the Stanford parser to find the shortest path between target nouns. The dependency path features are computed by averaging word embeddings from W on the shortest path, and are then concatenated to the feature vector e. Furthermore, we directly incorporate semantic information using word-level semantic features from Named Entity (NE) tags and WordNet hypernyms, as used in previous work (Rink and Harabagiu, 2010; Socher et al., 2012; Yu et al., 2014). We refer to this extended method as RelEmbFULL . Concretely, RelEmbFULL uses the same binary features as in Socher et al. (2012). The features come from NE tags and WordNet hypernym tags of target nouns provided by a sense tagger (Ciaramita and Altun, 2006). 4 4.2 Initialization and Optimization We initialized the embedding matrices N and W with zero-mean gaussian noise with a variance of 1 ˜ d . W and b were zero-initialized. The model parameters were optimized by maximizing the objective function in Eq. (3) using stochastic gradient ascent. The learning rate was set to α"
K15-1027,P14-1146,0,0.0362761,"target word to be predicted during training. Bansal et al. (2014) trained embeddings by defining parent and child nodes in dependency trees as contexts. Chen et al. (2014) introduced the concept of feature embeddings induced by parsing a large unannotated corpus and then learning embeddings for the manually crafted features. For information extraction, Boros et al. (2014) trained word embeddings relevant for event role extraction, and Nguyen and Grishman (2014) employed word embeddings for domain adaptation of relation extraction. Another kind of task-specific word embeddings was proposed by Tang et al. (2014), which used sentiment labels on tweets to adapt word embeddings for a sentiment analysis tasks. However, such an approach is only feasible when a large amount of labeled data is available. cal level features and no external annotated resources. Furthermore, our qualitative analysis of the learned embeddings shows that n-grams of our embeddings capture salient syntactic patterns similar to semantic relation types. 2 Related Work A traditional approach to relation classification is to train classifiers in a supervised fashion using a variety of features. These features include lexical bag-of-wo"
K15-1027,P10-1040,0,0.066139,"rsity College London, London, United Kingdom pontus@stenetorp.se §Toyota Technological Institute, 2-12-1 Hisakata, Tempaku-ku, Nagoya, Japan makoto-miwa@toyota-ti.ac.jp words. For example, word2vec1 (Mikolov et al., 2013b) is a well-established tool for learning word embeddings. Although word2vec has successfully been used to learn word embeddings, these kinds of word embeddings capture only co-occurrence relationships between words (Levy and Goldberg, 2014). While simply adding word embeddings trained using window-based contexts as additional features to existing systems has proven valuable (Turian et al., 2010), more recent studies have focused on how to tune and enhance word embeddings for specific tasks (Bansal et al., 2014; Boros et al., 2014; Chen et al., 2014; Guo et al., 2014; Nguyen and Grishman, 2014) and we continue this line of research for the task of relation classification. In this work we present a learning method for word embeddings specifically designed to be useful for relation classification. The overview of our system and the embedding learning process are shown in Figure 1. First we train word embeddings by predicting each of the words between noun pairs using lexical relation-sp"
K15-1027,P12-2018,0,0.0413567,"in our method, and 1.6 × 107 in CRCNNOther assuming N is 10. dos Santos et al. (2015) also boosted the score of CR-CNNOther by omitting the noisy class “Other” by a rankingbased classifier, and achieved the best score (CRCNNBest ). Our results may also be improved by using the same technique, but the technique is dataset-dependent, so we did not incorporate the technique. These results show that our task-specific word embeddings are more useful than those trained using window-based contexts. A point that we would like to emphasize is that the baselines are unexpectedly strong. As was noted by Wang and Manning (2012), we should carefully implement strong baselines and see whether complex models can outperform these baselines. 5.3.2 Comparison with SVM-Based Systems RelEmb performs much better than the bag-ofwords-based SVM. This is not surprising given that we use a large unannotated corpus and embeddings with a large number of parameters. RelEmb also outperforms the SVM system of Rink and Harabagiu (2010), which demonstrates the effectiveness of our task-specific word embeddings, despite our only requirement being a large unannotated corpus and a POS tagger. 5.3.3 Comparison with Neural Network Models Re"
K15-1027,C14-1220,0,0.0554248,"w. 5.2.3 Neural Network Models Socher et al. (2012) used Recursive Neural Network (RNN) models to classify the relations. Subsequently, Ebrahimi and Dou (2015) and Hashimoto et al. (2013) proposed RNN models to better handle the relations. These methods rely on syntactic parse trees. Yu et al. (2014) introduced their novel Factorbased Compositional Model (FCM) and presented results from several model variants, the best performing being FCMEMB and FCMFULL . The former only uses word embedding information and the latter relies on dependency paths and NE features, in addition to word embeddings. Zeng et al. (2014) used a Convolutional Neural Network (CNN) with WordNet hypernyms. Noteworthy in relation to the RNN-based methods, the CNN model does not rely on parse trees. More recently, dos Santos et al. (2015) have introduced CR-CNN by extending the CNN model and achieved the best result to date. The key point of CR-CNN is that it improves the classification score by omitting the noisy class “Other” in the dataset described in Section 5.1. We call CR-CNN using the “Other” class CR-CNNOther and CRCNN omitting the class CR-CNNBest . (a) Financial [stress]E1 is one of the main causes of [divorce]E2 (b) The"
K15-1027,J08-1002,0,\N,Missing
L16-1225,den-etal-2008-proper,0,0.0178113,"s, we decided to incorporate verbal expressions including copula verbs followed by an adjective. These include passive forms and causative forms. In the BIO system, there are 2J + 1 tags, where J is the number of the NE types. For shogi NE we defined J = 21 types and the annotation work is to choose one among 43 (= 2 × 21 + 1) tags for each word. We prepared an annotation tool shown in Figure 3. We first segmented sentences automatically with a tool KyTea1 (Neubig and Mori, 2010; Neubig et al., 2011), trained on the general domain corpus, BCCWJ (Maekawa et al., 2010) and a dictionary, UniDic (Den et al., 2008) containing 212,900 words. We then supplied the results to the tool. Finally an annotator corrected word boundaries and added BIO tags for words using the tool shown in Figure 3. Pushing a “＋” button connects the words to form a single word and pushing a “▲” button separates a word into two words. A BIO tag is annotated to each word by selecting one among those in the pull-down menu. 1 http://www.phontron.com/kytea/ (accessed on 2016 Feb. 19). 1417 Figure 3: Annotation tool for word segmentation and BIO tags (the depenency part is not used). Type manu. auto. #Matches #States #Sent. #NEs #Words"
L16-1225,D15-1021,0,0.13057,"rounding 1. Introduction In recent years there has been a surge of interest in the generation of natural language annotations to describe digital recordings of the real world. A notable example is sentence generation from images (Ushiku et al., 2011; Yang et al., 2011). In such studies the natural language annotations are often provided by human workers on Amazon Mechanical Turk, and thus are often somewhat artificial. Since Hashimoto et al. (2014) recorded cooking videos of recipes spontaneously posted to an Internet site (Mori et al., 2014b), there have been many other image/video datasets (Ferraro et al., 2015) published. These attempts at connecting language expressions to real world objects such as images are often called symbol grounding (Harnad, 1990), an exciting new area in natural language processing. However, images, videos, and many other forms of media have ambiguities that make symbol grounding difficult. In this task we propose to use a well-defined “real world,” that is game states, to concentrate on language ambiguities. The game we focus on is shogi (Japanese chess) (Leggett, 2009). We collected 742,286 commentary sentences in Japanese and then defined domain-specific named entities ("
L16-1225,D15-1277,1,0.761706,"ng characteristics: • The commentaries are spontaneously given by professional players or writers. • Each commentary has a corresponding game state in a real match. Figure 1: Starting setup of shogi (left: normal depiction, right: chess-like depiction). • The game states do not have any ambiguity. Typical usages of our corpus include automatic commentary generation, detection of domain specific expressions, and their classification referring to game states (in the real world). There has in the past been an attempt at predicting characteristic words given a game state to generate a commentary (Kameko et al., 2015b). In this study, however, they only predict words (e.g. “king’s”) and do not identify concepts (e.g. “king’s gambit”), nor concept types (e.g. strategy). With our corpus we can try generation using automatically generated templates (Reiter, 1995; Mori et al., 2014a) or deep learning with our NEs in place of dialog acts (Wen et al., 2015). 2. Game and Commentary In this section we briefly explain shogi and its commentaries. For detailed explaination of shogi, please refer to (Leggett, 2009). 1415 3. Shogi Commentary Expressions Commentaries contain many domain specific expressions (words or m"
L16-1225,D15-1015,0,0.117727,"Missing"
L16-1225,maekawa-etal-2010-design,0,0.0289174,"). 3.7. Actions Unlike the general NE definitions, we decided to incorporate verbal expressions including copula verbs followed by an adjective. These include passive forms and causative forms. In the BIO system, there are 2J + 1 tags, where J is the number of the NE types. For shogi NE we defined J = 21 types and the annotation work is to choose one among 43 (= 2 × 21 + 1) tags for each word. We prepared an annotation tool shown in Figure 3. We first segmented sentences automatically with a tool KyTea1 (Neubig and Mori, 2010; Neubig et al., 2011), trained on the general domain corpus, BCCWJ (Maekawa et al., 2010) and a dictionary, UniDic (Den et al., 2008) containing 212,900 words. We then supplied the results to the tool. Finally an annotator corrected word boundaries and added BIO tags for words using the tool shown in Figure 3. Pushing a “＋” button connects the words to form a single word and pushing a “▲” button separates a word into two words. A BIO tag is annotated to each word by selecting one among those in the pull-down menu. 1 http://www.phontron.com/kytea/ (accessed on 2016 Feb. 19). 1417 Figure 3: Annotation tool for word segmentation and BIO tags (the depenency part is not used). Type man"
L16-1225,W03-0430,0,0.0408632,"ther improvement in word segmentation on game commentaries by referring to the game states (Kameko et al., 2015a). So we can say that an accurate word segmenter for shogi commentaries is now available. 5.2. Named Entity Recognition We also conducted shogi NE recognition. We trained a BIO2-based NE recognizer (Sasada et al., 2015) and tested it. The corpus specifications are shown in Table 2. The precision and recall are 0.913 and 0.789, respectively. The F-measure is 0.847, which is comparable to the general domain case (around 0.9) trained from about 10,000 sentences (Sang and Meulder, 2003; McCallum and Li, 2003). These results suggest that an NE recognizer is ready to be used to detect shogi NEs in raw commentaries for various applications. 5.3. Symbol Grounding One of the most interesting research directions is symbol grounding. Contrary to images or videos (Regneri et al., 2013), game states do not cause recognition problems and we can concentrate on the ambiguities on the language side. Another interesting aspect of symbol grounding to game states is that we can connect natural language expressions to computer analysis and predictions. 5.4. Others The NE recognition and/or symbol grounding results"
L16-1225,mori-neubig-2014-language,1,0.852837,"65 7,161 9,470 7 NA 386 NA 1,777 57,281 7,922 — 27,025 1,339,500 36,589 1,931,751 Table 2: Corpus specifications for word segmentation and NE recognition experiments. Training BCCWJ BCCWJ + Shogi Precision 0.872 0.983 Recall 0.907 0.983 F-measure 0.889 0.983 Table 3: Word segmentation accuracies. In this section we show their results and describe other potential applications. 5.1. Word Segmentation Our corpus has word boundary information. We therefore first tested word segmentation performance. It is well known that an annotated corpus in the target domain improves the performance very well (Mori and Neubig, 2014). Thus in addition to the baseline (see Subsection 4.1.), we trained another model using our corpus additionally. Table 2 shows the experimental settings. The results are shown in Table 3. The performance of the baseline word segmenter is very bad. Our corpus, however, improves the performance drastically. We can realize a further improvement in word segmentation on game commentaries by referring to the game states (Kameko et al., 2015a). So we can say that an accurate word segmenter for shogi commentaries is now available. 5.2. Named Entity Recognition We also conducted shogi NE recognition."
L16-1225,W14-4418,1,0.890694,"d to the real world. Keywords: game commentary, named entity, symbol grounding 1. Introduction In recent years there has been a surge of interest in the generation of natural language annotations to describe digital recordings of the real world. A notable example is sentence generation from images (Ushiku et al., 2011; Yang et al., 2011). In such studies the natural language annotations are often provided by human workers on Amazon Mechanical Turk, and thus are often somewhat artificial. Since Hashimoto et al. (2014) recorded cooking videos of recipes spontaneously posted to an Internet site (Mori et al., 2014b), there have been many other image/video datasets (Ferraro et al., 2015) published. These attempts at connecting language expressions to real world objects such as images are often called symbol grounding (Harnad, 1990), an exciting new area in natural language processing. However, images, videos, and many other forms of media have ambiguities that make symbol grounding difficult. In this task we propose to use a well-defined “real world,” that is game states, to concentrate on language ambiguities. The game we focus on is shogi (Japanese chess) (Leggett, 2009). We collected 742,286 commenta"
L16-1225,mori-etal-2014-flow,1,0.893921,"Missing"
L16-1225,neubig-mori-2010-word,1,0.834625,"crete expressions, like “10 minutes,” this includes abstract ones such as “長時間” (long time). 3.7. Actions Unlike the general NE definitions, we decided to incorporate verbal expressions including copula verbs followed by an adjective. These include passive forms and causative forms. In the BIO system, there are 2J + 1 tags, where J is the number of the NE types. For shogi NE we defined J = 21 types and the annotation work is to choose one among 43 (= 2 × 21 + 1) tags for each word. We prepared an annotation tool shown in Figure 3. We first segmented sentences automatically with a tool KyTea1 (Neubig and Mori, 2010; Neubig et al., 2011), trained on the general domain corpus, BCCWJ (Maekawa et al., 2010) and a dictionary, UniDic (Den et al., 2008) containing 212,900 words. We then supplied the results to the tool. Finally an annotator corrected word boundaries and added BIO tags for words using the tool shown in Figure 3. Pushing a “＋” button connects the words to form a single word and pushing a “▲” button separates a word into two words. A BIO tag is annotated to each word by selecting one among those in the pull-down menu. 1 http://www.phontron.com/kytea/ (accessed on 2016 Feb. 19). 1417 Figure 3: Ann"
L16-1225,P11-2093,1,0.797173,"“10 minutes,” this includes abstract ones such as “長時間” (long time). 3.7. Actions Unlike the general NE definitions, we decided to incorporate verbal expressions including copula verbs followed by an adjective. These include passive forms and causative forms. In the BIO system, there are 2J + 1 tags, where J is the number of the NE types. For shogi NE we defined J = 21 types and the annotation work is to choose one among 43 (= 2 × 21 + 1) tags for each word. We prepared an annotation tool shown in Figure 3. We first segmented sentences automatically with a tool KyTea1 (Neubig and Mori, 2010; Neubig et al., 2011), trained on the general domain corpus, BCCWJ (Maekawa et al., 2010) and a dictionary, UniDic (Den et al., 2008) containing 212,900 words. We then supplied the results to the tool. Finally an annotator corrected word boundaries and added BIO tags for words using the tool shown in Figure 3. Pushing a “＋” button connects the words to form a single word and pushing a “▲” button separates a word into two words. A BIO tag is annotated to each word by selecting one among those in the pull-down menu. 1 http://www.phontron.com/kytea/ (accessed on 2016 Feb. 19). 1417 Figure 3: Annotation tool for word"
L16-1225,Q13-1003,0,0.0341371,"trained a BIO2-based NE recognizer (Sasada et al., 2015) and tested it. The corpus specifications are shown in Table 2. The precision and recall are 0.913 and 0.789, respectively. The F-measure is 0.847, which is comparable to the general domain case (around 0.9) trained from about 10,000 sentences (Sang and Meulder, 2003; McCallum and Li, 2003). These results suggest that an NE recognizer is ready to be used to detect shogi NEs in raw commentaries for various applications. 5.3. Symbol Grounding One of the most interesting research directions is symbol grounding. Contrary to images or videos (Regneri et al., 2013), game states do not cause recognition problems and we can concentrate on the ambiguities on the language side. Another interesting aspect of symbol grounding to game states is that we can connect natural language expressions to computer analysis and predictions. 5.4. Others The NE recognition and/or symbol grounding results allow for various applications. Firstly we can improve automatic commentary generation (Kaneko, 2012). Kameko et al. (2015b) proposed a method for finding characteristic words for game states and used them to generate commentaries automatically. With our corpus, one can tr"
L16-1225,W03-0419,0,0.807406,"(path) is used to denote bishop’s diagonal lines and rook’s orthogonal lines. There are special expressions to denote relative positions of a piece like “腹” (belly) meaning the side squares of a piece. Pq: Piece quantity. Usually it is a pair of a number and a counter word. This also includes expressions such as “切れ ” (lack of) and “豊富” (abundant). 4. Game Commentary Corpus In this section we briefly explain our annotation framework, show some statistics for our corpus, and describe the corpus availability. 4.1. Annotation Framework As the notation for shogi NEs, we adopt the BIO tag system (Sang and Meulder, 2003). B, I, and O stand for begin, intermediate, and other, respectively. Each word is annotated with O, indicating that the word is not any NE, or a combination of BI tag and an NE type tag such as Hu-B, which indicates that the word is the beginning (B) of a player name (Hu). The following is the correct annotation for the commentary in Figure 2. 広瀬/Hu-B は /O 対/O ゴ /St-B キゲン /St-I 中/St-I 飛車/St-I の/O 超速/St-B ▲/St-I ３七/St-I 銀/St-I 戦法/St-I を /O 採用/Ac し /O た/O 。/O 3.6. Describing Events Outside the Board Commentators sometimes refer to issues outside of the board but related to the match. They can b"
L16-1225,W04-1221,0,0.244304,"xpressions to real world objects such as images are often called symbol grounding (Harnad, 1990), an exciting new area in natural language processing. However, images, videos, and many other forms of media have ambiguities that make symbol grounding difficult. In this task we propose to use a well-defined “real world,” that is game states, to concentrate on language ambiguities. The game we focus on is shogi (Japanese chess) (Leggett, 2009). We collected 742,286 commentary sentences in Japanese and then defined domain-specific named entities (NEs), similar to previous work on bio-medical NEs (Settles, 2004; Tateisi et al., 2002) or recipe NEs (Mori et al., 2014b). For example, “central rook” is a shogi strategy expression similar to “king’s gambit” in chess. We finally annotated NEs for 2,508 sentences to form our game commentary corpus, which has the following distinguishing characteristics: • The commentaries are spontaneously given by professional players or writers. • Each commentary has a corresponding game state in a real match. Figure 1: Starting setup of shogi (left: normal depiction, right: chess-like depiction). • The game states do not have any ambiguity. Typical usages of our corpus"
L16-1225,D15-1199,0,0.0215308,"ic commentary generation, detection of domain specific expressions, and their classification referring to game states (in the real world). There has in the past been an attempt at predicting characteristic words given a game state to generate a commentary (Kameko et al., 2015b). In this study, however, they only predict words (e.g. “king’s”) and do not identify concepts (e.g. “king’s gambit”), nor concept types (e.g. strategy). With our corpus we can try generation using automatically generated templates (Reiter, 1995; Mori et al., 2014a) or deep learning with our NEs in place of dialog acts (Wen et al., 2015). 2. Game and Commentary In this section we briefly explain shogi and its commentaries. For detailed explaination of shogi, please refer to (Leggett, 2009). 1415 3. Shogi Commentary Expressions Commentaries contain many domain specific expressions (words or multi-word expressions), which can be categorized into groups in a similar way to the general domain or bio-medical NEs. All players are familiar with these expressions (e.g. King’s gambit, Ruy Lopez, etc. in chess) and category names (e.g. opening). To facilitate various studies, we created detailed definitions of shogi NEs and annotated N"
L16-1225,D11-1041,0,0.399437,"Missing"
N09-1007,W06-1655,0,0.354127,"entation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretical advantage of semi-CRFs over CRFs, however, some previous studies (Andrew, 2006; Liang, 2005) exploring the use of a semi-CRF for Chinese word segmentation did not ﬁnd signiﬁcant gains over the CRF ones. As discussed in Andrew (2006), the reason may be that despite the greater representational power of the semi-CRF, there are some valuable features that could be more naturally expressed in a character-based"
N09-1007,I05-3018,0,0.0345224,"s beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004). Despite the theoretica"
N09-1007,I05-3019,0,0.342649,"Missing"
N09-1007,I05-3017,0,0.737646,"the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax t"
N09-1007,P07-1104,0,0.0508784,"nt Variable Segmenter 2.1 Discriminative Probabilistic Latent Variable Model Given data with latent structures, the task is to learn a mapping between a sequence of observations x = x1 , x2 , . . . , xm and a sequence of labels y = y1 , y2 , . . . , ym . Each yj is a class label for the j’th character of an input sequence, and is a member of a set Y of possible class labels. For each sequence, the model also assumes a sequence of latent variables h = h1 , h2 , . . . , hm , which is unobservable in training examples. The DPLVM is deﬁned as follows (Morency et al., 2 The system was also used in Gao et al. (2007), with an improved performance in CWS. 3 In practice, one may add a few extra labels based on linguistic intuitions (Xue, 2003). 2007): P (y|x, Θ) =  P (y|h, x, Θ)P (h|x, Θ), (1) h where Θ are the parameters of the model. DPLVMs can be seen as a natural extension of CRF models, and CRF models can be seen as a special case of DPLVMs that have only one latent variable for each label. To make the training and inference efﬁcient, the model is restricted to have disjoint sets of latent variables associated with each class label. Each hj is a member in a set Hyj of possible latent variables for the"
N09-1007,C04-1081,0,0.541571,"Missing"
N09-1007,E09-1088,1,0.796261,"onal loglikelihood of the training data. The second term is a regularizer that is used for reducing overﬁtting in parameter estimation. For decoding in the test stage, given a test sequence x, we want to ﬁnd the most probable label sequence, y∗ : y∗ = argmaxy P (y|x, Θ∗ ). (5) For latent conditional models like DPLVMs, the best label path y∗ cannot directly be produced by the 4 It means that Eq. 2 is from Eq. 1 with additional deﬁnition. 58 Viterbi algorithm because of the incorporation of hidden states. In this paper, we use a technique based on A∗ search and dynamic programming described in Sun and Tsujii (2009), for producing the most probable label sequence y∗ on DPLVM. In detail, an A∗ search algorithm5 (Hart et al., 1968) with a Viterbi heuristic function is adopted to produce top-n latent paths, h1 , h2 , . . . hn . In addition, a forward-backward-style algorithm is used to compute the exact probabilities of their corresponding label paths, y1 , y2 , . . . yn . The model then tries to determine the optimal label path based on the top-n statistics, without enumerating the remaining low-probability paths, which could be exponentially enormous. The optimal label path y∗ is ready when the following"
N09-1007,I05-3027,0,0.634414,"utational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). While the CRF model is quite effective compared with other models designed for CWS, it may be limited by its restrictive independence assumptions on non-adjacent labels. Although the window can in principle be widened by increasing the Markov order, this may not be a practical solution, because the complexity of training and decoding a linearchain CRF grows exponentially with the Markov order (Andrew, 2006). To address this difﬁculty, a choice is to relax the Markov assumption by using the semi-Markov conditional random ﬁeld model (semi-CRF) (Sarawagi and Cohen, 2004)."
N09-1007,W06-0121,0,0.114226,"Missing"
N09-1007,O03-4002,0,0.816245,"purposes, e.g., full-text indexing. However, as is illustrated, recognizing long words (without sacriﬁcing the performance on short words) is challenging. Conventional approaches to Chinese word segmentation treat the problem as a character-based la1 Following previous work, in this paper, words can also refer to multi-word expressions, including proper names, long named entities, idioms, etc. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 56–64, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics beling task (Xue, 2003). Labels are assigned to each character in the sentence, indicating whether the character xi is the start (Labeli = B), middle or end of a multi-character word (Labeli = C). A popular discriminative model that have been used for this task is the conditional random ﬁelds (CRFs) (Lafferty et al., 2001), starting with the model of Peng et al. (2004). In the Second International Chinese Word Segmentation Bakeoff (the second SIGHAN CWS bakeoff) (Emerson, 2005), two of the highest scoring systems in the closed track competition were based on a CRF model (Tseng et al., 2005; Asahara et al., 2005). Wh"
N09-1007,P07-1106,0,0.496064,"h row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on those two corpora. Both of the MSR and PKU Corpus use simpliﬁed Chinese, w"
N09-1007,N06-2049,0,0.152838,"ts are grouped into three sub-tables according to different corpora. Each row represents a CWS model. For each group, the rows marked by ∗ represent our models with hybrid word/character information. Best05 represents the best system of the Second International Chinese Word Segmentation Bakeoff on the corresponding data; A06 represents the semi-CRF model in Andrew (2006)10 , which was also used in Gao et al. (2007) (denoted as G07) with an improved performance; Z06-a and Z06-b represents the pure subword CRF model and the conﬁdence-based combination of CRF and rule-based models, respectively (Zhang et al., 2006); ZC07 represents the word-based perceptron model in Zhang and Clark (2007); T05 represents the CRF model in Tseng et al. (2005); C05 represents the system in Chen et al. 10 It is a hybrid Markov/semi-Markov CRF model which outperforms conventional semi-CRF models (Andrew, 2006). However, in general, as discussed in Andrew (2006), it is essentially still a semi-CRF model. 61 (2005). The best F-score and recall of OOV words of each group is shown in bold. As is shown in the table, we achieved the best F-score in two out of the three corpora. We also achieved the best recall rate of OOV words on"
N19-1315,P16-1078,1,0.933092,"of the parameters in the softmax layer: Wp0 = MX Wp , b0p = MX bp , (8) and Wp0 ∈ RK×d and b0p ∈ RK are used instead of Wp and bp in Equation (4) and (5). Therefore, in mini-batched processes with a mini-batch size B, our method constructs B different sets of (Wp0 , b0p ). Relationship to previous work Samplingbased approximation methods have previously been studied to reduce the computational cost at the large softmax layer in probabilistic language modeling (Ji et al., 2016; Zoph et al., 2016), and such methods are also used to enable one to train neural machine translation models on CPUs (Eriguchi et al., 2016). The construction of (Wp0 , b0p ) in our method is similar to these softmax approximation methods in that they also sample small vocabularies either at the word level (Ji et al., 2016), sentence level (Hashimoto and Tsuruoka, 2017), or mini-batch level (Zoph et al., 2016). However, one significant difference is that the approximation methods work only at training time using the cross entropy loss, and full softmax computations are still required at test time. The difference is crucial because a sentence generation model needs to simulate its test-time behavior in reinforcement learning. 3 Tar"
N19-1315,P17-2012,1,0.844894,"nglishto-Vietnamese (En-Vi), and Chinese-to-Japanese (Ch-Ja). For image captioning, we used two datasets: MS COCO (Lin et al., 2014) and Flickr8K. Table 1 summarizes the statistics of the training datasets, where the number of training examples (“Size”), the target vocabulary size (|V |), and the maximum length of the target sentences (max(N )) are shown. For the machine translation datasets, we manually set max(N ) and omitted training examples which violate the constraints. En-De: We used 100,000 training sentence pairs from news commentary and newstest2015 as our development set, following Eriguchi et al. (2017). En-Ja: We used parallel sentences in ASPEC (Nakazawa et al., 2016) and constructed three types of datasets: En-Ja (100K), En-Ja (2M), and En-Ja (2M, SW). The 100K and 2M datasets were constructed with the first 100,000 and 2,000,000 sentence pairs, respectively. To test our method using subword units, we further preprocessed the 2M dataset by using the SentencePiece toolkit (Kudo and Richardson, 2018) to construct the En-Ja (2M, SW) dataset. En-Vi: We used the pre-processed datasets provided by Luong and Manning (2015). Our development dataset is the tst2012 dataset. Ch-Ja: We constructed th"
N19-1315,D17-1012,1,0.884703,"Missing"
N19-1315,P82-1020,0,0.678784,"Missing"
N19-1315,P15-1001,0,0.208849,"eng et al. (2017) reported that the joint learning improves the accuracy of their machine translation models, but our preliminary experiments did not indicate such accuracy gain. Such a joint training approach requires the model to continuously update the vocabulary predictor during REINFORCE, because the encoder is shared. That is, the action space for each input changes during reinforcement learning, and we observed unstable training. Therefore, this work separately models the vocabulary predictor and focuses on the effects of using the small vocabularies for REINFORCE. Another note is that Jean et al. (2015) and L’Hostis et al. (2016) also proposed to construct small vocabularies in advance to the cross entropybased training. They suggest that the use of word alignment works well, but using the word alignment is not general enough, considering that there exist different types of source input. By contrast, our method can be straightforwardly applied to the two sentence generation tasks with the different input modalities (i.e. image and text). 3.2 Multi-Label Classification Once the input representation v(X) is computed, we further transform it by a single residual block (He et al., 2016): r(X) ="
N19-1315,D18-2012,0,0.0123288,"ually set max(N ) and omitted training examples which violate the constraints. En-De: We used 100,000 training sentence pairs from news commentary and newstest2015 as our development set, following Eriguchi et al. (2017). En-Ja: We used parallel sentences in ASPEC (Nakazawa et al., 2016) and constructed three types of datasets: En-Ja (100K), En-Ja (2M), and En-Ja (2M, SW). The 100K and 2M datasets were constructed with the first 100,000 and 2,000,000 sentence pairs, respectively. To test our method using subword units, we further preprocessed the 2M dataset by using the SentencePiece toolkit (Kudo and Richardson, 2018) to construct the En-Ja (2M, SW) dataset. En-Vi: We used the pre-processed datasets provided by Luong and Manning (2015). Our development dataset is the tst2012 dataset. Ch-Ja: We constructed the Ch-Ja dataset by using the first 100,000 sentences from ASPEC. MS COCO and Flickr8K: We used the preprocessed datasets provided by Kiros et al. (2014). We can also download the 4096-dimensional feature vectors f (i.e., df = 4096). 4.2 Settings of Sentence Generation We set d = 256 with single-layer LSTMs for all the experiments, except for the En-Ja (2M) and (2M, SW) datasets. For the larger En-Ja dat"
N19-1315,2015.iwslt-evaluation.11,0,0.0148263,"irs from news commentary and newstest2015 as our development set, following Eriguchi et al. (2017). En-Ja: We used parallel sentences in ASPEC (Nakazawa et al., 2016) and constructed three types of datasets: En-Ja (100K), En-Ja (2M), and En-Ja (2M, SW). The 100K and 2M datasets were constructed with the first 100,000 and 2,000,000 sentence pairs, respectively. To test our method using subword units, we further preprocessed the 2M dataset by using the SentencePiece toolkit (Kudo and Richardson, 2018) to construct the En-Ja (2M, SW) dataset. En-Vi: We used the pre-processed datasets provided by Luong and Manning (2015). Our development dataset is the tst2012 dataset. Ch-Ja: We constructed the Ch-Ja dataset by using the first 100,000 sentences from ASPEC. MS COCO and Flickr8K: We used the preprocessed datasets provided by Kiros et al. (2014). We can also download the 4096-dimensional feature vectors f (i.e., df = 4096). 4.2 Settings of Sentence Generation We set d = 256 with single-layer LSTMs for all the experiments, except for the En-Ja (2M) and (2M, SW) datasets. For the larger En-Ja datasets, we set d = 512 with two-layer LSTMs. We used stochastic gradient decent with momentum, with a learning rate of 1."
N19-1315,D15-1166,0,0.0213359,"machine translation, the source input X corresponds to a source sentence (x1 , x2 , . . . , xM ) of length M . Each word xi is also associated with a word embedding e˜(xi ) ∈ Rd . We assume that a hidden ˜ i ∈ R2d is computed for each xi by using a state h bi-directional RNN with LSTM units (Graves and ˜ i is the concatenaSchmidhuber, 2005). That is, h → − ← − tion of xi ’s d-dimensional hidden states [ h i ; h i ] computed by a pair of forward and backward RNNs. We set the initial hidden state of the sen→ − ← − tence generator as h0 = h M + h 1 . Following an attention mechanism proposed in Luong et al. (2015), st for predicting yt is computed as follows: &quot; # ! M X ˜ i + bs , (3) st = tanh Ws ht ; ai h i=1 ˜ is the global-attention funcwhere ai = f (ht , i, h) tion in Luong et al. (2015), Ws ∈ Rd×3d is a (4) (5) where the weight and bias parameters are analogous to the ones in Equation (4). For both of the tasks, we use the weight-tying technique (Inan et al., 2017; Press and Wolf, 2017) by using Wp as the word embedding matrix. That is, e(yt ) is the yt -th row vector in Wp , and the technique has shown to be effective in machine translation (Hashimoto and Tsuruoka, 2017) and text summarization (P"
N19-1315,P17-1079,0,0.0180608,"If we increase the mini-batch size to 4096, our small softmax method works with S = 12. 7 We thank anonymous reviewers for their fruitful comments. This work was supported by JST CREST Grant Number JPMJCR1513, Japan. Related Work Reducing the computational cost at the large softmax layer in language modeling/generation is actively studied (Jean et al., 2015; Ji et al., 2016; Eriguchi et al., 2016; L’Hostis et al., 2016; Zoph et al., 2016; Wu et al., 2017). Most of the existing methods try to reduce the vocabulary size by either negative sampling or vocabulary prediction. One exception is that Oda et al. (2017a) propose to predict a binary code of its corresponding target word. Although such a sophisticated method is promising, we focused on the vocabulary reduction method to apply policy-based reinforcement learning in a straightforward way. As reported in this paper, one simple way to define a reward function for reinforcement learning is to use task-specific automatic evaluation metrics (Ranzato et al., 2016; Wu et al., 2016; Rennie et al., 2017; Zhang and Lapata, 2017; Paulus et al., 2018), but this is limited in that we can only use training data with gold target sentences. An alternative appr"
N19-1315,P02-1040,0,0.10401,"l networks plays a key role in many language processing tasks, including machine translation (Sutskever et al., 2014), image captioning (Lin et al., 2014), and abstractive summarization (Rush et al., 2015). The most common approach for learning the sentence generation models is maximizing the likelihood of the model on the gold-standard target sentences. Recently, approaches based on reinforcement learning have attracted increasing attention to reduce the gap between training and test situations and to directly incorporate taskspecific and more flexible evaluation metrics such as BLEU scores (Papineni et al., 2002) into optimization (Ranzato et al., 2016). While reinforcement learning-based sentence generation is appealing, it is often too computa∗ Work was done while the first author was working at the University of Tokyo. tionally demanding to be used with large training data. In reinforcement learning for sentence generation, selecting an action corresponds to selecting a word in the vocabulary V . The number of possible actions at each time step is thus equal to the vocabulary size, which often exceeds tens of thousands. Among such a large set of possible actions, at most N actions are selected if t"
N19-1315,E17-2025,0,0.0291518,"ensional hidden states [ h i ; h i ] computed by a pair of forward and backward RNNs. We set the initial hidden state of the sen→ − ← − tence generator as h0 = h M + h 1 . Following an attention mechanism proposed in Luong et al. (2015), st for predicting yt is computed as follows: &quot; # ! M X ˜ i + bs , (3) st = tanh Ws ht ; ai h i=1 ˜ is the global-attention funcwhere ai = f (ht , i, h) tion in Luong et al. (2015), Ws ∈ Rd×3d is a (4) (5) where the weight and bias parameters are analogous to the ones in Equation (4). For both of the tasks, we use the weight-tying technique (Inan et al., 2017; Press and Wolf, 2017) by using Wp as the word embedding matrix. That is, e(yt ) is the yt -th row vector in Wp , and the technique has shown to be effective in machine translation (Hashimoto and Tsuruoka, 2017) and text summarization (Paulus et al., 2018). 2.2 Applying Reinforcement Learning One well-known limitation of using the cross entropy loss in Equation (2) is that the sentence generation models work differently at the training and test time. More concretely, the models only observe gold sequences at the training time, whereas the models have to handle unseen sequences to generate sentences at the test time"
N19-1315,D15-1044,0,0.0389176,"arning steps, and also at test time. In our experiments on six machine translation and two image captioning datasets, our method achieves faster reinforcement learning (∼2.7x faster) with less GPU memory (∼2.3x less) than the full-vocabulary counterpart. We also show that our method more effectively receives rewards with fewer iterations of supervised pre-training. 1 Introduction Sentence generation with neural networks plays a key role in many language processing tasks, including machine translation (Sutskever et al., 2014), image captioning (Lin et al., 2014), and abstractive summarization (Rush et al., 2015). The most common approach for learning the sentence generation models is maximizing the likelihood of the model on the gold-standard target sentences. Recently, approaches based on reinforcement learning have attracted increasing attention to reduce the gap between training and test situations and to directly incorporate taskspecific and more flexible evaluation metrics such as BLEU scores (Papineni et al., 2002) into optimization (Ranzato et al., 2016). While reinforcement learning-based sentence generation is appealing, it is often too computa∗ Work was done while the first author was worki"
N19-1315,P16-1009,0,0.0179952,"observe that our method is robust to the changes, whereas Wu et al. (2017) reported that their dynamic vocabulary selection method is sensitive to such changes. For reference, we report the test set results in Table 4. We cite BLEU scores from previously published papers which reported results of single models (i.e., without ensemble). Our method with greedy translation achieves a competitive score. It should be noted that Morishita et al. (2017) achieve a better score presumably because they used additional in-domain one million parallel sentences obtained by the back-translation technique (Sennrich et al., 2016). 6 Efficiency of the Proposed Method This section discusses our main contribution: how efficient our method is in accelerating reinforcement learning for sentence generation. 6.1 Speedup at Training Time We have examined the training-time efficiency of our method. Table 5 shows the training time [minutes/epoch] for five different datasets. We selected the five datasets to show results with different vocabulary sizes and different maximum sentence lengths, and we observed the same trend on the other datasets. The vocabulary size |V |and the maximum sentence length max(N ) are shown for each tr"
N19-1315,D17-1013,0,0.01857,"∈ Rdv ×df is a weight matrix, and bv ∈ Rdv is a bias vector. For machine translation, we employ a P bag-of-embeddings repM 1 resentation: v(X) = M ˜v (xi ), where the i=1 e dv -dimensional word embedding e˜v (xi ) ∈ Rdv is different from e˜(xi ) used in the machine translation model. By using the different set of the model parameters, we avoid the situation that our vocabulary prediction model is affected during training the sentence generation models. Relationship to previous work Vocabulary prediction has gained attention for training sequenceto-sequence models with the cross entropy loss (Weng et al., 2017; Wu et al., 2017), but not for reinforcement learning. Compared to our method, previous methods jointly train a vocabulary predictor by directly using source encoders as input to the predictor. One may expect joint learning to improve both of the vocabulary predictor and the sentence generator, but in practice such positive effects are not clearly observed. Weng et al. (2017) reported that the joint learning improves the accuracy of their machine translation models, but our preliminary experiments did not indicate such accuracy gain. Such a joint training approach requires the model to contin"
N19-1315,N18-1122,0,0.0650274,"Missing"
N19-1315,D17-1062,0,0.232632,"achine translation (Hashimoto and Tsuruoka, 2017) and text summarization (Paulus et al., 2018). 2.2 Applying Reinforcement Learning One well-known limitation of using the cross entropy loss in Equation (2) is that the sentence generation models work differently at the training and test time. More concretely, the models only observe gold sequences at the training time, whereas the models have to handle unseen sequences to generate sentences at the test time. To bridge the gap, reinforcement learning has started gaining much attention (Ranzato et al., 2016; Wu et al., 2016; Rennie et al., 2017; Zhang and Lapata, 2017; Paulus et al., 2018; Yang et al., 2018). In this work, we focus on the most popular 3116 method called REINFORCE (Williams, 1992).1 In REINFORCE, the sentence generation model sets an initial state given a source input, and then iterates an action selection and its corresponding state transition. The action selection corresponds to randomly sampling a target word from Equation (4) and (5), and the state transition corresponds to the RNN transition in Equation (1). Once a sentence is generated, an approximated loss function is defined as follows: Lr (Y, X) = − N X Rt log p(y = yt |y&lt;t , X), ("
N19-1315,N16-1145,0,0.133231,"elements at position (i, wi ) for 1 ≤ i ≤ K. wi is a unique word index in V . MX is used to construct a small subset of the parameters in the softmax layer: Wp0 = MX Wp , b0p = MX bp , (8) and Wp0 ∈ RK×d and b0p ∈ RK are used instead of Wp and bp in Equation (4) and (5). Therefore, in mini-batched processes with a mini-batch size B, our method constructs B different sets of (Wp0 , b0p ). Relationship to previous work Samplingbased approximation methods have previously been studied to reduce the computational cost at the large softmax layer in probabilistic language modeling (Ji et al., 2016; Zoph et al., 2016), and such methods are also used to enable one to train neural machine translation models on CPUs (Eriguchi et al., 2016). The construction of (Wp0 , b0p ) in our method is similar to these softmax approximation methods in that they also sample small vocabularies either at the word level (Ji et al., 2016), sentence level (Hashimoto and Tsuruoka, 2017), or mini-batch level (Zoph et al., 2016). However, one significant difference is that the approximation methods work only at training time using the cross entropy loss, and full softmax computations are still required at test time. The difference"
P06-1059,W04-1221,0,0.154531,"Missing"
P06-1059,M95-1002,0,0.046288,"Missing"
P06-1059,W05-1514,1,0.831367,"Missing"
P06-1059,A97-1029,0,0.0419464,"Missing"
P06-1059,W04-1219,0,0.184969,"Missing"
P06-1059,W04-1217,0,0.0945932,"Missing"
P06-1059,P05-1045,0,0.0374246,"Missing"
P06-1059,W04-1213,0,0.505862,"Missing"
P06-1059,I05-1057,0,0.131398,"Missing"
P06-1128,P05-1022,0,0.00458241,"atabases. For biomedical terms other than genes/gene products, the Unified Medical Language System (UMLS) meta-thesaurus (Lindberg et al., 1993) is a large database that contains various names of biomedical and health-related concepts. Ontology databases provide mappings between textual expressions and entities in the real world. For example, Table 1 indicates that CRP, MGC88244, and PTX1 denote the same gene conceptually. Hence, these resources enable us to canonicalize variations of textual expressions of ontological entities. 2.2 Parsing technologies Recently, state-of-the-art CFG parsers (Charniak and Johnson, 2005) can compute phrase structures of natural sentences at fairly high accuracy. These parsers have been used in various NLP tasks including IE and text mining. In addition, parsers that compute deeper analyses, such as predicate argument structures, have become available for 1018 the processing of real-world sentences (Miyao and Tsujii, 2005). Predicate argument structures are canonicalized representations of sentence meanings, and express the semantic relations of words explicitly. Figure 1 shows an output of an HPSG parser (Miyao and Tsujii, 2005) for the sentence “A normal serum CRP measuremen"
P06-1128,I05-1018,1,0.746453,"aphies of articles, about half of which have abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our p"
P06-1128,W04-3102,0,0.0566302,"nd: Resources and Tools for Semantic Annotations The proposed system for the retrieval of relational concepts is a product of recent developments in NLP resources and tools. In this section, ontology databases, deep parsers, and search algorithms for structured data are introduced. 2.1 Ontology databases Ontology databases are collections of words and phrases in specific domains. Such databases have been constructed extensively for the systematic management of domain knowledge by organizing textual expressions of ontological entities that are detached from actual sentences. For example, GENA (Koike and Takagi, 2004) is a database of genes and gene products that is semi-automatically collected from well-known databases, including HUGO, OMIM, Genatlas, Locuslink, GDB, MGI, FlyBase, WormBase, Figure 1: An output of HPSG parsing Figure 2: A predicate argument structure CYGD, and SGD. Table 1 shows an example of a GENA entry. “Symbol” and “Name” denote short forms and nomenclatures of genes, respectively. “Species” represents the organism species in which this gene is observed. “Synonym” is a list of synonyms and name variations. “Product” gives a list of products of this gene, such as proteins coded by this"
P06-1128,P05-1011,1,0.829945,"advance with semantic structures and are stored in a structured database. User requests are converted on the fly into patterns of these semantic annotations, and texts are retrieved by matching these patterns with the pre-computed semantic annotations. The accurate retrieval of relational concepts is attained because we can precisely describe relational concepts using semantic annotations. In addition, real-time retrieval is possible because semantic annotations are computed in advance. This framework has been implemented for a text retrieval system for MEDLINE. We first apply a deep parser (Miyao and Tsujii, 2005) and a dictionary-based term recognizer (Tsuruoka and Tsujii, 2004) to MEDLINE and obtain annotations of predicate argument structures and ontological identifiers of genes, gene products, diseases, and events. We then provide a search engine for these annotated sentences. User requests are converted into queries of region algebra (Clarke et al., 1995) extended with variables (Masuda et al., 2006) on these annotations. A search engine for the extended region algebra efficiently finds sentences having semantic annotations that match the input queries. In this paper, we evaluate this system with"
P06-1128,I05-2038,1,0.581796,"abstracts. Research on IE and text mining in biomedical science has focused mainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that"
P06-1128,H05-1059,1,0.143897,"ainly on MEDLINE. In the present paper, we target all articles indexed in MEDLINE at the end of 2004 (14,785,094 articles). The following sections explain in detail off-/on-line processing for the text retrieval system for MEDLINE. 3.1 Off-line processing: HPSG parsing and term recognition We first parsed all sentences using an HPSG parser (Miyao and Tsujii, 2005) to obtain their predicate argument structures. Because our target is biomedical texts, we re-trained a parser (Hara et al., 2005) with the GENIA treebank (Tateisi et al., 2005), and also applied a bidirectional part-ofspeech tagger (Tsuruoka and Tsujii, 2005) trained with the GENIA treebank as a preprocessor. Because parsing speed is still unrealistic for parsing the entire MEDLINE on a single machine, we used two geographically separated computer clusters having 170 nodes (340 Xeon CPUs). These clusters are separately administered and not dedicated for use in the present study. In order to effectively use such an environment, GXP (Taura, 2004) was used to connect these clusters and distribute the load among them. Our processes were given the lowest priority so that our task would not disturb other users. We finished parsing the entire MEDLINE in"
P06-4005,P05-1011,1,0.777319,"Missing"
P06-4005,I05-1018,1,0.813084,"eather conditions forced them to scrub Monday’s scheduled return.” 3 MEDIE: a search engine for MEDLINE Figure 2 shows the top page of the MEDIE. MEDIE is an intelligent search engine for the accurate retrieval of relational concepts from MEDLINE 2 (Miyao et al., 2006). Prior to retrieval, all sentences are annotated with predicate argument structures and ontological identifiers by applying Enju and a term recognizer. 3.1 Automatically Annotated Corpus First, we applied a POS analyzer and then Enju. The POS analyzer and HPSG parser are trained by using the GENIA corpus (Tsuruoka et al., 2005; Hara et al., 2005), which comprises around 2,000 MEDLINE abstracts annotated with POS and Penn Treebank style syntactic parse trees (Tateisi et al., 2005). The HPSG parser generates parse trees in a stand-off format that can be converted to XML by combining it with the original text. We also annotated technical terms of genes and diseases in our developed corpus. Technical terms are annotated simply by exact matching of dictio2 Functions of MEDIE 4 Info-PubMed: a GUI-based MEDLINE search tool Info-PubMed is a MEDLINE search tool with GUI, helping users to find information about biomedical entities such as genes"
P06-4005,W05-1511,1,0.867094,"Missing"
P06-4005,I05-2038,1,0.900999,"e predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MOD, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. The lexicon of the grammar was extracted from Sections 02-21 of Penn Treebank (39,832 sentences). In the table, ‘HPSG-PTB’ means that the statistical model was trained on Penn Treebank. ‘HPSG-GENIA’ means that the statistical model was trained on both Penn Treebank and GENIA treebank as described in (Hara et al., 2005). The GENIA treebank (Tateisi et al., 2005) consists of 500 abstracts (4,446 sentences) extracted from MEDLINE. Figure 1 shows a part of the parse tree and feaRecently, biomedical researchers have been facing the vast repository of research papers, e.g. MEDLINE. These researchers are eager to search biomedical correlations such as protein-protein or gene-disease associations. The use of natural language processing technology is expected to reduce their burden, and various attempts of information extraction using NLP has been being made (Blaschke and Valencia, 2002; Hao et al., 2005; Chun et al., 2006). However, the framework of traditi"
P06-4005,W04-3102,0,0.0153543,"of Informatics, Kogakuin University ¶ Information Technology Center, University of Tokyo † 1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 17 Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 17–20, c Sydney, July 2006. 2006 Association for Computational Linguistics nary entries and the terms separated by space, tab, period, comma, hat, colon, semi-colon, brackets, square brackets and slash in MEDLINE. The entire dictionary was generated by applying the automatic generation method of name variations (Tsuruoka and Tsujii, 2004) to the GENA dictionary for the gene names (Koike and Takagi, 2004) and the UMLS (Unified Medical Language System) meta-thesaurus for the disease names (Lindberg et al., 1993). It was generated by applying the name-variation generation method, and we obtained 4,467,855 entries of a gene and disease dictionary. 3.2 MEDIE provides three types of search, semantic search, keyword search, GCL search. GCL search provides us the most fundamental and powerful functions in which users can specify the boolean relations, linear order relation and structural relations with variables. Trained users can enjoy all functions in MEDIE by the GCL search, but it is not easy for"
P06-4005,P06-1128,1,\N,Missing
P09-1054,P04-1014,0,0.00603949,"tal results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning f"
P09-1054,W05-0622,0,0.00845476,"i If the structure is a sequence, the model is called a linear-chain CRF model, and the marginal probabilities of the features and the partition function can be efficiently computed by using the forwardbackward algorithm. The model is used for a variety of sequence labeling tasks such as POS tagging, chunking, and named entity recognition. If the structure is a tree, the model is called a tree CRF model, and the marginal probabilities can be computed by using the inside-outside algorithm. The model can be used for tasks like syntactic parsing (Finkel et al., 2008) and semantic role labeling (Cohn and Blunsom, 2005). In this paper, we present a simple method for solving these two problems in SGD learning. The main idea is to keep track of the total penalty and the penalty that has been applied to each weight, so that the L1 penalty is applied based on the difference between those cumulative values. That way, the application of L1 penalty is needed only for the features that are used in the current sample, and also the effect of noisy gradient is smoothed away. 2.1 Training The weights of the features in a log-linear model are optimized in such a way that they maximize the regularized conditional log-like"
P09-1054,W02-1001,0,0.468961,"Missing"
P09-1054,P06-1059,1,0.746285,",10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the text chunking experiments. The results are shown in Figure 5 and Table 2. The trend in the results is the same as that of the text chunking task: our SGD algorithms show much faster convergence than the OWL-QN algorithm and produce compact models. Okanohara et al. (2006) report an f-score of 71.48 on the same data, using semi-Markov CRFs. -2.6 -2.8 -3 -3.2 -3.4 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -3.6 -3.8 0 10 20 30 40 50 Passes Figure 5: NLPBA 2004 named entity recognition task: Objective. -1.8 -1.9 Objective function -2 -2.1 -2.2 -2.3 -2.4 -2.5 OWL-QN SGD-L1 (Clipping) SGD-L1 (Cumulative) SGD-L1 (Cumulative + ED) -2.6 -2.7 -2.8 0 10 20 30 40 50 Passes Figure 6: POS tagging task: Objective. 4.3 Part-Of-Speech Tagging ing because it was “prohibitive” (7-8 days for sections 0-18 of the WSJ corpus). For the features, we used u"
P09-1054,W96-0213,0,0.530357,"evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and u"
P09-1054,P07-1096,0,0.0378243,"Missing"
P09-1054,D08-1016,0,0.0112981,"log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work is discussed in Section 5. Section 6 gives som"
P09-1054,P08-1109,0,0.0549519,"Missing"
P09-1054,P07-1104,0,0.0274516,"ion, which aims to obtain the weights of the features that maximize the conditional likelihood of the training data. In maximum likelihood training, regularization is normally needed to prevent the model from overfitting the training data, The two most common regularization methods are called L1 and L2 regularization. L1 regularization penalizes the weight vector for its L1-norm (i.e. the sum of the absolute values of the weights), whereas L2 regularization uses its L2-norm. There is usually not a considerable difference between the two methods in terms of the accuracy of the resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a m"
P09-1054,P06-1091,0,0.00589602,"L and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often requires much less training time in practice than batch training algorithms, especially when the training data is large and redundant. SGD was recently used for NLP tasks including machine translation (Tillmann and Zhang, 2006) and syntactic parsing (Smith and Eisner, 2008; Finkel et al., 2008). Also, SGD is very easy to implement because it does not need to use the Hessian information on the objective function. The implementation could be as simple as the perceptron algorithm. pact and accurate models much more quickly than the OWL-QN algorithm. This paper is organized as follows. Section 2 provides a general description of log-linear models used in NLP. Section 3 describes our stochastic gradient descent method for L1-regularized loglinear models. Experimental results are presented in Section 4. Some related work"
P09-1054,P05-1073,0,0.00918396,"compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models. 1 Introduction Log-linear models (a.k.a maximum entropy models) are one of the most widely-used probabilistic models in the field of natural language processing (NLP). The applications range from simple classification tasks such as text classification and history-based tagging (Ratnaparkhi, 1996) to more complex structured prediction tasks such as partof-speech (POS) tagging (Lafferty et al., 2001), syntactic parsing (Clark and Curran, 2004) and semantic role labeling (Toutanova et al., 2005). Loglinear models have a major advantage over other 477 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 477–485, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP An alternative approach to training a log-linear model is to use stochastic gradient descent (SGD) methods. SGD uses approximate gradients estimated from subsets of the training data and updates the weights of the features in an online fashion—the weights are updated much more frequently than batch training algorithms. This learning framework is attracting attention because it often re"
P09-1054,W03-1018,1,0.434131,"he resulting model (Gao et al., 2007), but L1 regularization has a significant advantage in practice. Because many of the weights of the features become zero as a result of L1-regularized training, the size of the model can be much smaller than that produced by L2-regularization. Compact models require less space on memory and storage, and enable the application to start up quickly. These merits can be of vital importance when the application is deployed in resource-tight environments such as cell-phones. A common way to train a large-scale L1regularized model is to use a quasi-Newton method. Kazama and Tsujii (2003) describe a method for training a L1-regularized log-linear model with a bound constrained version of the BFGS algorithm (Nocedal, 1980). Andrew and Gao (2007) present an algorithm called OrthantWise Limited-memory Quasi-Newton (OWLQN), which can work on the BFGS algorithm without bound constraints and achieve faster convergence. Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch tra"
P09-1054,wellner-vilain-2006-leveraging,0,0.0282722,"Missing"
P09-1054,W04-1213,0,0.0388038,"eature set as ours.8 Their library uses the OWL-QN algorithm for optimization. Although direct comparison of training times is not impor6 http://crfpp.sourceforge.net/ http://www.chokkan.org/software/crfsuite/benchmark.html 8 ditto 7 482 tant due to the differences in implementation and hardware platforms, these results demonstrate that our algorithm can actually result in a very fast implementation of a CRF trainer. -2.2 Objective function -2.4 4.2 Named Entity Recognition The second set of experiments used the named entity recognition data set provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 2004).9 The training data consist of 18,546 sentences in which each token is annotated with the “IOB” tags representing biomedical named entities such as the names of proteins and RNAs. The training and test data were preprocessed by the GENIA tagger,10 which provided POS tags and chunk tags. We did not use any information on the named entity tags output by the GENIA tagger. For the features, we used unigrams of neighboring chunk tags, substrings (shorter than 10 characters) of the current word, and the shape of the word (e.g. “IL-2” is converted into “AA-#”), on top of the features used in the tex"
P09-1054,J93-2004,0,\N,Missing
P16-1020,P14-2131,0,0.0314416,"is a composition function. The function can be simple ones such as element-wise addition or multiplication (Mitchell and Lapata, 2008). 206 Updating the model parameters Given the par∂J tial derivative δp = ∂v(p) ∈ Rd×1 for the target task, we can compute the partial derivative for updating W as follows: cusing on the task of learning the embeddings of transitive verb phrases. δα = α(p)(1 − α(p)){δp · (c(p) − n(p))} (4) ∂J = δα φ(p). (5) ∂W Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction using (syntactic) contexts. In this work, we focus on verb-object relationships and employ a phrase embedding learning method presented in Hashimoto and Tsuruoka (2015). The task is a plausibility judgment task for predicate-argument tuples. They extracted Subject-Verb-Object (SVO) and SVO-PrepositionNoun (SVOPN) tuples using a probabilistic HPSG parser, Enju (Miyao and Tsujii, 2008), from the training corpora. Transitive verbs and prepositi"
P16-1020,W09-2903,0,0.081355,"Missing"
P16-1020,P14-2050,0,0.0591679,"ple ones such as element-wise addition or multiplication (Mitchell and Lapata, 2008). 206 Updating the model parameters Given the par∂J tial derivative δp = ∂v(p) ∈ Rd×1 for the target task, we can compute the partial derivative for updating W as follows: cusing on the task of learning the embeddings of transitive verb phrases. δα = α(p)(1 − α(p)){δp · (c(p) − n(p))} (4) ∂J = δα φ(p). (5) ∂W Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction using (syntactic) contexts. In this work, we focus on verb-object relationships and employ a phrase embedding learning method presented in Hashimoto and Tsuruoka (2015). The task is a plausibility judgment task for predicate-argument tuples. They extracted Subject-Verb-Object (SVO) and SVO-PrepositionNoun (SVOPN) tuples using a probabilistic HPSG parser, Enju (Miyao and Tsujii, 2008), from the training corpora. Transitive verbs and prepositions are extracted as predicates with two arguments. For"
P16-1020,P99-1041,0,0.361157,"Missing"
P16-1020,W15-0904,0,0.0458574,"Missing"
P16-1020,W03-1810,0,0.131479,"Missing"
P16-1020,D11-1129,0,0.121197,"Missing"
P16-1020,D07-1039,0,0.379536,"eddings are learned by minimizing the cost function using AdaGrad (Duchi et al., 2011). The scoring function is parameterized as Learning Verb Phrase Embeddings This section describes a particular instantiation of our approach presented in the previous section, fos(p, a1 , a2 ) = v(a1 ) · (M (p)v(a2 )), 207 (10) More concretely, the first set of the features (indices of V, O, and VO) is the concatenation of traditional one-hot vectors. The second set of features, frequency and PMI (Church and Hanks, 1990) features, have proven effective in detecting the compositionality of transitive verbs in McCarthy et al. (2007) and Venkatapathy and Joshi (2005). Given the training corpus, the frequency feature for a VO pair is computed as and the VO and SVO embeddings are computed as v(V O) = M (V )v(O) v(SV O) = v(S) v(V O), (11) (12) as proposed by Kartsaklis et al. (2012). The operator denotes element-wise multiplication. In summary, the scores are computed as s(V, S, O) = v(S) · v(V O) (13) f req(V O) = log(count(V O)), s(P, SV O, N ) = v(SV O) · (M (P )v(N )). (14) where count(V O) counts how many times the VO pair appears in the training corpus, and the PMI feature is computed as With this method, the word and"
P16-1020,W15-4001,1,0.869669,"ction. The function can be simple ones such as element-wise addition or multiplication (Mitchell and Lapata, 2008). 206 Updating the model parameters Given the par∂J tial derivative δp = ∂v(p) ∈ Rd×1 for the target task, we can compute the partial derivative for updating W as follows: cusing on the task of learning the embeddings of transitive verb phrases. δα = α(p)(1 − α(p)){δp · (c(p) − n(p))} (4) ∂J = δα φ(p). (5) ∂W Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction using (syntactic) contexts. In this work, we focus on verb-object relationships and employ a phrase embedding learning method presented in Hashimoto and Tsuruoka (2015). The task is a plausibility judgment task for predicate-argument tuples. They extracted Subject-Verb-Object (SVO) and SVO-PrepositionNoun (SVOPN) tuples using a probabilistic HPSG parser, Enju (Miyao and Tsujii, 2008), from the training corpora. Transitive verbs and prepositions are extracted as predicate"
P16-1020,D14-1163,1,0.891339,"Missing"
P16-1020,D14-1079,0,0.0410506,"Missing"
P16-1020,P08-1028,0,0.112017,"rmulated as c(p) = f (v(w1 ), · · · , v(wL )), Non-Compositional Phrase Embeddings α(p) = σ(W · φ(p)), (1) (3) where φ(p) ∈ RN ×1 is a feature vector of the phrase p, W ∈ RN ×1 is a weight vector, N is the number of features, and σ(·) is the logistic function. The weight vector W is jointly optimized in conjunction with the objective J for the target task of learning phrase embeddings v(p). where d is the dimensionality, L is the phrase length, v(·) ∈ Rd×1 is a word embedding, and f (·) is a composition function. The function can be simple ones such as element-wise addition or multiplication (Mitchell and Lapata, 2008). 206 Updating the model parameters Given the par∂J tial derivative δp = ∂v(p) ∈ Rd×1 for the target task, we can compute the partial derivative for updating W as follows: cusing on the task of learning the embeddings of transitive verb phrases. δα = α(p)(1 − α(p)){δp · (c(p) − n(p))} (4) ∂J = δα φ(p). (5) ∂W Acquisition of selectional preference using embeddings has been widely studied, where word and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these metho"
P16-1020,C12-2054,0,0.130505,"us section, fos(p, a1 , a2 ) = v(a1 ) · (M (p)v(a2 )), 207 (10) More concretely, the first set of the features (indices of V, O, and VO) is the concatenation of traditional one-hot vectors. The second set of features, frequency and PMI (Church and Hanks, 1990) features, have proven effective in detecting the compositionality of transitive verbs in McCarthy et al. (2007) and Venkatapathy and Joshi (2005). Given the training corpus, the frequency feature for a VO pair is computed as and the VO and SVO embeddings are computed as v(V O) = M (V )v(O) v(SV O) = v(S) v(V O), (11) (12) as proposed by Kartsaklis et al. (2012). The operator denotes element-wise multiplication. In summary, the scores are computed as s(V, S, O) = v(S) · v(V O) (13) f req(V O) = log(count(V O)), s(P, SV O, N ) = v(SV O) · (M (P )v(N )). (14) where count(V O) counts how many times the VO pair appears in the training corpus, and the PMI feature is computed as With this method, the word and composed phrase embeddings are jointly learned based on cooccurrence statistics of predicate-argument structures. Using the learned embeddings, they achieved state-of-the-art accuracy on a transitive verb disambiguation task (Grefenstette and Sadrzade"
P16-1020,J08-1002,0,0.012359,"and/or phrase embeddings are learned based on syntactic links (Bansal et al., 2014; Hashimoto and Tsuruoka, 2015; Levy and Goldberg, 2014; Van de Cruys, 2014). As with language modeling, these methods perform word (or phrase) prediction using (syntactic) contexts. In this work, we focus on verb-object relationships and employ a phrase embedding learning method presented in Hashimoto and Tsuruoka (2015). The task is a plausibility judgment task for predicate-argument tuples. They extracted Subject-Verb-Object (SVO) and SVO-PrepositionNoun (SVOPN) tuples using a probabilistic HPSG parser, Enju (Miyao and Tsujii, 2008), from the training corpora. Transitive verbs and prepositions are extracted as predicates with two arguments. For example, the extracted tuples include (S, V, O) = (“importer”, “make”, “payment”) and (SVO, P, N) = (“importer make payment”, “in”, “currency”). The task is to discriminate between observed and unobserved tuples, such as the (S, V, O) tuple mentioned above and (S, V’, O) = (“importer”, “eat”, “payment”), which is generated by replacing “make” with “eat”. The (S, V’, O) tuple is unlikely to be observed. For each tuple (p, a1 , a2 ) observed in the training data, a cost function is"
P16-1020,Y14-1010,0,0.505621,"egative effect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training. These problems have motivated us to adaptively combine both types of embeddings. Most of the existing methods for learning phrase embeddings can be divided into two approaches. One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012). The other approach is to learn both types of embeddings separately and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al. (2014) report the opposite results on other tasks. These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the compositionality levels of the individual phrases in a more flexible fashion. In this paper, we propose a method that jointly learns compositional and non-compositional embeddings by adaptively weighting both types of phrase embeddings using a comp"
P16-1020,P14-2035,0,0.329214,"ompositional also has a negative effect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training. These problems have motivated us to adaptively combine both types of embeddings. Most of the existing methods for learning phrase embeddings can be divided into two approaches. One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012). The other approach is to learn both types of embeddings separately and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al. (2014) report the opposite results on other tasks. These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the compositionality levels of the individual phrases in a more flexible fashion. In this paper, we propose a method that jointly learns compositional and non-compositional embeddings by adaptively weighting both types of phrase"
P16-1020,D13-1147,0,0.476868,"bserved more than K times in each corpus. K was set to 10 for the BNC data and 100 for the Wikipedia and BNC-Wikipedia data. Consequently, the non-compositional embeddings were assigned to 17,817, 28,933, and 30,682 verb-object phrase types in the BNC, Wikipedia, and BNC-Wikipedia data, respectively. 2 http://www.logos.t.u-tokyo.ac.jp/ ˜hassy/publications/cvsc2015/ 3 https://github.com/hassyGo/ SVOembedding • frequency and Pointwise Mutual Information (PMI) values of VO. 208 4.2 Method Proposed method (Wikipedia) Proposed method (BNC) Proposed method (BNC-Wikipedia) Proposed method (Ensemble) Kiela and Clark (2013) w/ WordNet Kiela and Clark (2013) DSPROTO (McCarthy et al., 2007) PMI (McCarthy et al., 2007) Frequency (McCarthy et al., 2007) DSPROTO+ (McCarthy et al., 2007) Human agreement Training Details The model parameters consist of d-dimensional word embeddings for nouns, non-compositional phrase embeddings, d×d-dimensional matrices for verbs and prepositions, and a weight vector W for α(V O). All the model parameters are jointly optimized. We initialized the embeddings and matrices with zero-mean gaussian random values with a variance of d1 and d12 , respectively, and W with zeros. Initializing W"
P16-1020,D14-1162,0,0.101057,"technique is helpful in improving the results as shown in the examples. Another interesting observation in our results is that the result of the ensemble technique outperforms that of the BNC-Wikipedia data as shown in Table 1. This shows that separately using the training corpora of different nature and then performing the ensemble technique can yield better results. By contrast, many of the previous studies on embedding-based methods combine different corpora into a single dataset, or use multiple corpora just separately and compare them (Hashimoto and Tsuruoka, 2015; Muraoka et al., 2014; Pennington et al., 2014). It would be worth investigating whether the results in the previous work can be improved by ensemble techniques. 6 6.1 Ensemble technique We used the same ensemble technique described in Section 5.1. In this task we produced two ensemble results: Ensemble A and Ensemble B. The former used the averaged cosine similarity from the results of the BNC and Wikipedia data, and the latter further incorporated the result of the BNC-Wikipedia data. Baselines We compared our adaptive joint learning method with two baseline methods. One is the method in Hashimoto and Tsuruoka (2015) and it is equivalent"
P16-1020,D15-1201,0,0.155168,"ple ensemble technique further improves the results for both tasks. 1 Introduction Representing words and phrases in a vector space has proven effective in a variety of language processing tasks (Pham et al., 2015; Sutskever et al., 2014). In most of the previous work, phrase embeddings are computed from word embeddings by using various kinds of composition functions. Such composed embeddings are called compositional embeddings. An alternative way of computing phrase embeddings is to treat phrases as single units and assigning a unique embedding to each candidate phrase (Mikolov et al., 2013; Yazdani et al., 2015). Such embeddings are called noncompositional embeddings. 1 The definition is found at http://idioms. thefreedictionary.com/bear+fruit. 205 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 205–215, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics More complex ones such as recurrent neural networks (Sutskever et al., 2014) are also commonly used. The word embeddings and the composition function are jointly learned on a certain target task. Since compositional embeddings are built on word-level (i.e. unigram) infor"
P16-1020,P15-1094,0,0.102557,"ns “to yield results”1 but it is hard to infer its meaning by composing the meanings of “bear” and “fruit”. Treating all phrases as compositional also has a negative effect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training. These problems have motivated us to adaptively combine both types of embeddings. Most of the existing methods for learning phrase embeddings can be divided into two approaches. One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012). The other approach is to learn both types of embeddings separately and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al. (2014) report the opposite results on other tasks. These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the compositionality levels of the individual phrases in a more flexible fashion. In this"
P16-1020,W15-2701,0,0.0226753,"Missing"
P16-1020,I11-1024,0,0.170301,"Missing"
P16-1020,D12-1110,0,0.0739808,"s”1 but it is hard to infer its meaning by composing the meanings of “bear” and “fruit”. Treating all phrases as compositional also has a negative effect in learning the composition function because the words in those idiomatic phrases are not just uninformative but can serve as noisy samples in the training. These problems have motivated us to adaptively combine both types of embeddings. Most of the existing methods for learning phrase embeddings can be divided into two approaches. One approach is to learn compositional embeddings by regarding all phrases as compositional (Pham et al., 2015; Socher et al., 2012). The other approach is to learn both types of embeddings separately and use the better ones (Kartsaklis et al., 2014; Muraoka et al., 2014). Kartsaklis et al. (2014) show that non-compositional embeddings are better suited for a phrase similarity task, whereas Muraoka et al. (2014) report the opposite results on other tasks. These results suggest that we should not stick to either of the two types of embeddings unconditionally and could learn better phrase embeddings by considering the compositionality levels of the individual phrases in a more flexible fashion. In this paper, we propose a me"
P16-1020,P15-1150,0,0.128856,"Missing"
P16-1020,D14-1004,0,0.0960566,"Missing"
P16-1020,J90-1003,0,\N,Missing
P16-1020,H05-1113,0,\N,Missing
P16-1078,D10-1092,0,0.227419,"Missing"
P16-1078,W14-4012,0,0.327334,"Missing"
P16-1078,D13-1176,0,0.666936,"translation accuracy. 2 p(yj |y<j , x) = g(sj ), where g is a non-linear function. The j-th hidden unit of the decoder is calculated by using another non-linear function fdec as follows: sj = fdec (yj−1 , sj−1 ). (3) We employ Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) in place of vanilla RNN units. The tth LSTM unit consists of several gates and two different types of states: a hidden unit ht ∈ Rd×1 and a memory cell ct ∈ Rd×1 , Neural Machine Translation 2.1 Encoder-Decoder Model NMT is an end-to-end approach to data-driven machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). In other words, the NMT models directly estimate the conditional probability p(y|x) given a large collection of source and target sentence pairs (x, y). An NMT model consists of an encoder process and a decoder process, and hence they are often called Encoder-Decoder models. In the Encoder-Decoder models, a sentence is treated as a sequence of words. In the encoder process, the encoder embeds each of the source words x = (x1 , x2 , · · · , xn ) into a d-dimensional vector space. The decoder then outputs a word sequence y = (y1 , y2 , · · · , ym"
P16-1078,D14-1179,0,0.14417,"Missing"
P16-1078,P06-1077,0,0.114812,"defined in different lexical units. In this example, the Japanese word “緑茶” is aligned with the English words “green” and “tea”, and the English word sequence “a cup of” is aligned with a special symbol “null”, which is not explicitly translated into any Japanese words. One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase “a cup of green tea” with “緑茶”. In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment (Yamada and Knight, 2001) and translation accuracy (Liu et al., 2006; Neubig and Duh, 2014). However, the existing NMT models do not allow us to perform this kind of alignment. Introduction Machine Translation (MT) has traditionally been one of the most complex language processing problems, but recent advances of Neural Machine Translation (NMT) make it possible to perform translation using a simple end-to-end architecture. In the Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vector, and then another RNN called the decoder"
P16-1078,N16-1102,0,0.0209148,"ral attentional relations when decoding a sentence. In Figures 4 and 5, an English sentence represented as a binary tree is translated into Japanese, and several attentional relations between English words or phrases and 830 Figure 5: Translation example of a long sentence and the attentional relations by our proposed model. refined the attention model so that it can dynamically focus on local windows rather than the entire sentence. They also proposed a more effective attentional path in the calculation of ANMT models. Subsequently, several ANMT models have been proposed (Cheng et al., 2016; Cohn et al., 2016); however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages. synonyms of the reference words, e.g. “女” and “ 女性” (“female” in English) and “NASA” and “航 空宇宙局” (“National Aeronautics and Space Administration” in English). These translations are penalized in terms of BLEU scores, but they do not necessarily mean that the translations were wrong. This point may be supported by the fact that the NMT models were highly evaluated in WAT’15 by crowd sourcing (Nakazawa et al., 2015). 6 Related Work 7 Conclusion Kalchbrenner and"
P16-1078,D15-1166,0,0.576273,"-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT’15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. 1 Figure 1: Alignment between an English phrase and a Japanese word. lation tasks (Luong et al., 2015b; Luong et al., 2015a). However, it is yet to be seen whether NMT is competitive with traditional Statistical Machine Translation (SMT) approaches in translation tasks for structurally distant language pairs such as English-to-Japanese. Figure 1 shows a pair of parallel sentences in English and Japanese. English and Japanese are linguistically distant in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this example, the Japanese word “緑茶” is aligned with the English words “green” and “tea”, and the English word sequen"
P16-1078,W14-3348,0,0.0248125,"sed encoder. Table 5 shows the result on the development data of our proposed encoder and that of an attentional tree-based encoder without sequential LSTMs with BlackOut (K = 2000).7 The results show that our proposed encoder considerably outComparison with the NMT models The model of Zhu (2015) is an ANMT model (Bahdanau et al., 2015) with a bi-directional LSTM encoder, and uses 1024-dimensional hidden units and 10008 We found two sentences which ends without eos with d = 512, and then we decoded it again with the beam size of 1000 following Zhu (2015). 9 Our ensemble model yields a METEOR (Denkowski and Lavie, 2014) score of 53.6 with language option “-l other”. 7 For this evaluation, we used the 1,789 sentences that were successfully parsed by Enju because the encoder without sequential LSTMs always requires a parse tree. 829 Model Proposed model (d = 512) Proposed model (d = 768) Proposed model (d = 1024) Ensemble of the above three models ANMT with LSTMs (Zhu, 2015) + Ensemble, unk replacement + System combination, 3 pre-reordered ensembles ANMT with GRUs (Lee et al., 2015) + character-based decoding, Begin/Inside representation PB baseline HPB baseline T2S baseline T2S model (Neubig and Duh, 2014) +"
P16-1078,P15-1002,0,0.354269,"-end syntactic NMT model, extending a sequenceto-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT’15 Englishto-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system. 1 Figure 1: Alignment between an English phrase and a Japanese word. lation tasks (Luong et al., 2015b; Luong et al., 2015a). However, it is yet to be seen whether NMT is competitive with traditional Statistical Machine Translation (SMT) approaches in translation tasks for structurally distant language pairs such as English-to-Japanese. Figure 1 shows a pair of parallel sentences in English and Japanese. English and Japanese are linguistically distant in many respects; they have different syntactic constructions, and words and phrases are defined in different lexical units. In this example, the Japanese word “緑茶” is aligned with the English words “green” and “tea”, and the English word sequen"
P16-1078,J08-1002,0,0.05711,"rds are mapped to the special token “unk”. We added another special symbol “eos” for both languages and inserted it at the end of all the sentences. Table 2 shows the details of each training dataset and its corresponding vocabulary size. 4 Experiments 4.1 Training Data We applied the proposed model to the English-toJapanese translation dataset of the ASPEC corpus given in WAT’15.1 Following Zhu (2015), we extracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e., English, we used the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008). We used Enju only to obtain a binary phrase structure for each sentence and did not use any HPSG specific information. For the target language, i.e., Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT’15.2 We then filtered out the translation pairs whose sentence lengths are longer than 50 and whose source sentences are not parsed successfully. Table 1 shows the details of the datasets used in our experiments. We carried out two experiments on a small training dataset to investigate 4.2 Training Details The b"
P16-1078,W15-5001,0,0.503369,"the beam size and becomes lower as the beam size increases. We found that the BP had a relatively large impact on the BLEU score in the simple beam search as the beam size increased. Our search method works better than the simple beam search by keeping long sentences in the candidates with a large beam size. 5.2 Large Training Dataset Table 6 shows the experimental results of RIBES and BLEU scores achieved by the trained models on the large dataset. We decoded the target sentences by our proposed beam search with the beam size of 20.8 The results of the other systems are the ones reported in Nakazawa et al. (2015). All of our proposed models show similar performance regardless of the value of d. Our ensemble model is composed of the three models with d = 512, 768, and 1024, and it shows the best RIBES score among all systems.9 As for the time required for training, our implementation needs about one day to perform one epoch on the large training dataset with d = 512. It would take about 11 days without using the BlackOut sampling. Effects of the sequential LSTM units We also investigated the effects of the sequential LSTMs at the leaf nodes in our proposed tree-based encoder. Table 5 shows the result o"
P16-1078,N16-1004,0,0.0239831,"t ), n ∑ 3.4 Sampling-Based Approximation to the NMT Models (12) The biggest computational bottleneck of training the NMT models is in the calculation of the softmax layer described in Equation (8), because its computational cost increases linearly with the size of the vocabulary. The speedup technique with GPUs has proven useful for sequence-based NMT models (Sutskever et al., 2014; Luong et al., where gtree is the same function as ftree with another set of Tree-LSTM parameters. This initialization allows the decoder to capture information from both the sequential data and phrase structures. Zoph and Knight (2016) proposed a similar method using a Tree-LSTM for initializing the 826 2015a) but it is not easily applicable when dealing with tree-structured data. In order to reduce the training cost of the NMT models at the softmax layer, we employ BlackOut (Ji et al., 2016), a sampling-based approximation method. BlackOut has been shown to be effective in RNN Language Models (RNNLMs) and allows a model to run reasonably fast even with a million word vocabulary with CPUs. At each word prediction step in the training, BlackOut estimates the conditional probability in Equation (2) for the target word and K n"
P16-1078,P11-2093,0,0.0818757,"and its corresponding vocabulary size. 4 Experiments 4.1 Training Data We applied the proposed model to the English-toJapanese translation dataset of the ASPEC corpus given in WAT’15.1 Following Zhu (2015), we extracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e., English, we used the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008). We used Enju only to obtain a binary phrase structure for each sentence and did not use any HPSG specific information. For the target language, i.e., Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT’15.2 We then filtered out the translation pairs whose sentence lengths are longer than 50 and whose source sentences are not parsed successfully. Table 1 shows the details of the datasets used in our experiments. We carried out two experiments on a small training dataset to investigate 4.2 Training Details The biases, softmax weights, and BlackOut weights are initialized with zeros. The hyperparameter β of BlackOut is set to 0.4 as recommended by Ji et al. (2016). Following J´ozefowicz et al. (2015), we i"
P16-1078,W15-5003,0,0.0419898,"placement by +1.19 RIBES and by +0.17 BLEU scores. Our ensemble model shows better performance, in both RIBES and BLEU scores, than that of Zhu (2015)’s best system which is a hybrid of the ANMT and SMT models by +1.54 RIBES and by +0.74 BLEU scores and Lee et al. (2015)’s ANMT system with special character-based decoding by +1.30 RIBES and +1.20 BLEU scores. Comparison with the SMT models PB, HPB and T2S are the baseline SMT systems in WAT’15: a phrase-based model, a hierarchical phrase-based model, and a tree-to-string model, respectively (Nakazawa et al., 2015). The best model in WAT’15 is Neubig et al. (2015)’s treeto-string SMT model enhanced with reranking by ANMT using a bi-directional LSTM encoder. Our proposed end-to-end NMT model compares favorably with Neubig et al. (2015). 5.3 Qualitative Analysis We illustrate the translations of test data by our model with d = 512 and several attentional relations when decoding a sentence. In Figures 4 and 5, an English sentence represented as a binary tree is translated into Japanese, and several attentional relations between English words or phrases and 830 Figure 5: Translation example of a long sentence and the attentional relations by our proposed m"
P16-1078,P02-1040,0,0.0977546,"e first one million pairs of the training dataset. We allow the decoder to generate up to 100 words. Large Training Dataset Our proposed model has 512-dimensional word embeddings and ddimensional hidden units (d ∈ {512, 768, 1024}). K is set to 2500. Our code3 is implemented in C++ using the Eigen library,4 a template library for linear algebra, and we run all of the experiments on multicore CPUs.5 It takes about a week to train a model on the large training dataset with d = 512. 4.4 Evaluation We evaluated the models by two automatic evaluation metrics, RIBES (Isozaki et al., 2010) and BLEU (Papineni et al., 2002) following WAT’15. We used the KyTea-based evaluation script for the translation results.6 The RIBES score is a metric based on rank correlation coefficients with word precision, and the BLEU score is based on n-gram word precision and a Brevity Penalty (BP) for outputs shorter than the references. RIBES is known to have stronger correlation with human judgements than BLEU in translation between English and Japanese as discussed in Isozaki et al. (2010). 4.3 Decoding process We use beam search to decode a target sentence for an input sentence x and calculate the sum of the log-likelihoods of t"
P16-1078,W14-4009,0,0.0501196,"Missing"
P16-1078,P15-1150,0,0.401486,"Missing"
P16-1078,P01-1067,0,0.17774,"syntactic constructions, and words and phrases are defined in different lexical units. In this example, the Japanese word “緑茶” is aligned with the English words “green” and “tea”, and the English word sequence “a cup of” is aligned with a special symbol “null”, which is not explicitly translated into any Japanese words. One way to solve this mismatch problem is to consider the phrase structure of the English sentence and align the phrase “a cup of green tea” with “緑茶”. In SMT, it is known that incorporating syntactic constituents of the source language into the models improves word alignment (Yamada and Knight, 2001) and translation accuracy (Liu et al., 2006; Neubig and Duh, 2014). However, the existing NMT models do not allow us to perform this kind of alignment. Introduction Machine Translation (MT) has traditionally been one of the most complex language processing problems, but recent advances of Neural Machine Translation (NMT) make it possible to perform translation using a simple end-to-end architecture. In the Encoder-Decoder model (Cho et al., 2014b; Sutskever et al., 2014), a Recurrent Neural Network (RNN) called the encoder reads the whole sequence of source words to produce a fixedlength vecto"
P16-1078,W15-5007,0,0.494507,". The vocabulary consists of words observed in the training data more than or equal to N times. We set N = 2 for the small training dataset and N = 5 for the large training dataset. The out-ofvocabulary words are mapped to the special token “unk”. We added another special symbol “eos” for both languages and inserted it at the end of all the sentences. Table 2 shows the details of each training dataset and its corresponding vocabulary size. 4 Experiments 4.1 Training Data We applied the proposed model to the English-toJapanese translation dataset of the ASPEC corpus given in WAT’15.1 Following Zhu (2015), we extracted the first 1.5 million translation pairs from the training data. To obtain the phrase structures of the source sentences, i.e., English, we used the probabilistic HPSG parser Enju (Miyao and Tsujii, 2008). We used Enju only to obtain a binary phrase structure for each sentence and did not use any HPSG specific information. For the target language, i.e., Japanese, we used KyTea (Neubig et al., 2011), a Japanese segmentation tool, and performed the pre-processing steps recommended in WAT’15.2 We then filtered out the translation pairs whose sentence lengths are longer than 50 and w"
P16-1078,P15-1001,0,\N,Missing
P16-1078,W14-7001,0,\N,Missing
P17-2012,N16-1024,0,0.06925,"oposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown. Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences. We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG. We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time. There has been relatively little attention to incorporating linguistic prior to neural machine tra"
P17-2012,P17-2021,0,0.109183,"ing process of neural machine translation, which results in two separate models rather than a single end-to-end one. Despite the promising improvements, these explicit approaches are limited in that the trained translation model strictly requires the availability of external tools during inference time. More recently, researchers have proposed methods to incorporate target-side syntax into NMT models. Alvarez-Melis and Jaakkola (2017) have proposed a doubly-recurrent neural network that can generate a tree-structured sentence, but its effectiveness in a full scale NMT task is yet to be shown. Aharoni and Goldberg (2017) introduced a method to serialize a parsed tree and to train the serialized parsed sentences. We propose to implicitly incorporate linguistic prior based on the idea of multi-task learning (Caruana, 1998; Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vector"
P17-2012,P16-1078,1,0.77136,"Missing"
P17-2012,W16-3905,0,0.0143141,"We call this hybrid model NMT+RNNG. 4.2 Knowledge Distillation for Parsing A major challenge in training the proposed hybrid model is that there is not a parallel corpus augmented with gold-standard target-side parse, and vice versa. In other words, we must either parse the target-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses. As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016). We instead resort to noisy, but automated annotation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009). In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network’s R"
P17-2012,P84-1044,0,0.104662,"Missing"
P17-2012,P16-1231,0,0.0361626,"Collobert et al., 2011). More specifically, we design a hybrid decoder for NMT, called NMT+RNNG1 , that combines a usual conditional language model and a recently proposed recurrent neural network grammars (RNNGs, Dyer et al., 2016). This is done by plugging in the conventional language model decoder in the place of the buffer in RNNG, while sharing a subset of parameters, such as word vectors, between the language model and RNNG. We train this hybrid model to maximize both the log-probability of a target sentence and the log-probability of a parse action sequence. We use an external parser (Andor et al., 2016) to generate target parse actions, but unlike the previous explicit approaches, we do not need it during test time. There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate l"
P17-2012,D10-1092,0,0.0116664,"tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search. The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between “The” and “ transition” should not be “pobj”. BLEU 18.60 18.02 17.94 17.58 17.75 Table 3: Effect of each component in RNNG. 5.3 Results and Analysis In Table 2, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model. It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time. 6 Conclusion We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and training it encourages both the encoder and decoder to better incorporate li"
P17-2012,P16-1160,1,0.918573,"encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG. 1 Introduction Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1"
P17-2012,W04-3250,0,0.0135413,"ed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore. 4 75 We run all the experiments on multi-core CPUs (10 De-En NMT NMT+RNNG NMT NMT+RNNG Ru-En BLEU 16.61 12.03 16.41 12.46† RIBES 73.75 69.56 75.03† 71.04† Cs-En Jp-En 11.22 12.06† 17.88 18.84† 69.59 70.39† 71.27 72.25† Figure 1: An example of translation and its dependency relations obtained by our proposed model. Table 2: BLEU and RIBES scores by the baseline and proposed models on the test set. We use the bootstrap resampling method from Koehn (2004) to compute the statistical significance. We use † to mark those significant cases with p < 0.005. Jp-En (Dev) NMT+RNNG w/o Buffer w/o Action w/o Stack NMT ate a translated sentence and the RNNG decoder to predict its parsing actions. The proposed model can therefore output a dependency structure along with a translated sentence. Figure 1 shows an example of JP-EN translation in the development dataset and its dependency parse tree obtained by the proposed model. The special symbol (“EOS”) is treated as the root node (“ROOT”) of the parsed tree. The translated sentence was generated by using b"
P17-2012,P07-2045,0,0.00557262,"the ASPEC corpus (“train1.txt”) from the WAT’16 Jp-En translation task. We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT’16 (WAT, 2016). We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use “dev.txt” and “test.txt” provided by WAT’16 respectively as development and test sets. Cs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, Ru and De. The target-side (English) vocabulary was constructed with all the unique tokens 3 When the target sentence is parsed as data preprocessing, we use all the vocabularies in a corpus and do not cut off any words. We use the plain SyntaxNet and do not train it furthermore. 4 75 We run all the experiments on multi-core CPUs (10 De-En NMT NMT+RNNG NMT NMT+RNNG Ru-En BLEU 16.61 12.03 16.41 12.46† RIBES 73.75 69.56 75.03† 71.04† Cs-En Jp-En 11.22 12.06† 17.88 18.84† 69.59 7"
P17-2012,P15-1033,0,0.0181116,", xN ). The encoder returns a sequence of hidden states h = (h1 , h2 , . . . , hN ). Each hidden state hi is a concatenation of those from the forward and backward recurrent network: hi = h→ − ← −i h i ; h i , where 3 A recurrent neural network grammar (RNNG, Dyer et al., 2016) is a probabilistic syntax-based language model. Unlike a usual recurrent language model (see, e.g., Mikolov et al., 2010), an RNNG simultaneously models both tokens and their tree-based composition. This is done by having a (output) buffer, stack and action history, each of which is implemented as a stack LSTM (sLSTM, Dyer et al., 2015). At each time step, the action sLSTM predicts the next action based on the (current) hidden states of the buffer, stack and action sLSTM. That is, >f p(at = a|a<t ) ∝ eWa Vx (xi ) refers to the word vector of the i-th source word. The decoder is implemented as a conditional recurrent language model which models the target sentence, or translation, as X log p(y|x) = log p(yj |y<j , x), hbuffer = StackLSTM(hbuffer t top , Vy (yt−1 )), where y = (y1 , . . . , yM ). Each of the conditional probabilities in the r.h.s is computed by (2) sj = fdec (sj−1 , [Vy (yj−1 ); s˜j−1 ]), (3) (5) where Vy and"
P17-2012,E17-1117,0,0.0517758,"Missing"
P17-2012,P16-2049,0,0.118476,"Missing"
P17-2012,D15-1166,0,0.0475373,"Missing"
P17-2012,P09-1113,0,0.00428942,"arget-side sentences of an existing parallel corpus or translate sentences with existing goldstandard parses. As the target task of the proposed model is translation, we start with a parallel corpus and annotate the target-side sentences. It is however costly to manually annotate any corpus of reasonable size (Table 6 in Alonso et al., 2016). We instead resort to noisy, but automated annotation using an existing parser. This approach of automated annotation can be considered along the line of recently proposed techniques of knowledge distillation (Hinton et al., 2015) and distant supervision (Mintz et al., 2009). In knowledge distillation, a teacher network is trained purely on a training set with ground-truth annotations, and the annotations predicted by this teacher are used to train a student network, which is similar to our approach where the external parser could be thought of as a teacher and the proposed hybrid network’s RNNG as a student. On the other hand, what we 2 The j-th hidden state in Eq. (3) is calculated only when the action (shift) is predicted by the RNNG. This is why our proposed model can handle the sequences of words and actions which have different lengths. 74 Cs-En De-En Ru-En"
P17-2012,N16-1145,0,0.0234197,"ining. We use “newstest2015” and “newstest2016” as development and test sets respectively. 5.2 Models, Learning and Inference In all our experiments, each recurrent network has a single layer of LSTM units of 256 dimensions, and the word vectors and the action vectors are of 256 and 128 dimensions, respectively. To reduce computational overhead, we use BlackOut (Ji et al., 2015) with 2000 negative samples and α = 0.4. When employing BlackOut, we shared the negative samples of each target word in a sentence in training time (Hashimoto and Tsuruoka, 2017), which is similar to the previous work (Zoph et al., 2016). For the proposed NMT+RNNG, we share the target word vectors between the decoder (buffer) and the stack sLSTM. Each weight is initialized from the uniform distribution [−0.1, 0.1]. The bias vectors and the weights of the softmax and BlackOut are initialized to be zero. The forget gate biases of LSTMs and Stack-LSTMs are initialized to 1 as recommended in J´ozefowicz et al. (2015). We use stochastic gradient descent with minibatches of 128 examples. The learning rate starts from 1.0, and is halved each time the perplexity on the development set increases. We clip the norm of the gradient (Pasc"
P17-2012,P11-2093,0,0.0512957,"eline and the proposed model to train a full JP-EN parallel corpus in our implementation.4 Experiments Language Pairs and Corpora We compare the proposed NMT+RNNG against the baseline model on four different language pairs–Jp-En, Cs-En, De-En and Ru-En. The basic statistics of the training data are presented in Table 1. We mapped all the low-frequency words to the unique symbol “UNK” and inserted a special symbol “EOS” at the end of both source and target sentences. Ja We use the ASPEC corpus (“train1.txt”) from the WAT’16 Jp-En translation task. We tokenize each Japanese sentence with KyTea (Neubig et al., 2011) and preprocess according to the recommendations from WAT’16 (WAT, 2016). We use the first 100K sentence pairs of length shorter than 50 for training. The vocabulary is constructed with all the unique tokens that appear at least twice in the training corpus. We use “dev.txt” and “test.txt” provided by WAT’16 respectively as development and test sets. Cs, De and Ru We use News Commentary v8. We removed noisy metacharacters and used the tokenizer from Moses (Koehn et al., 2007) to build a vocabulary of each language using unique tokens that appear at least 6, 6 and 5 times respectively for Cs, R"
P17-2012,P02-1040,0,0.128416,"root node (“ROOT”) of the parsed tree. The translated sentence was generated by using beam search, which is the same setting of NMT+RNNG shown in Table 3. The parsing actions were obtained by greedy search. The resulting dependency structure is mostly correct but contains a few errors; for example, dependency relation between “The” and “ transition” should not be “pobj”. BLEU 18.60 18.02 17.94 17.58 17.75 Table 3: Effect of each component in RNNG. 5.3 Results and Analysis In Table 2, we report the translation qualities of the tested models on all the four language pairs. We report both BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010). Except for DeEn, measured in BLEU, we observe the statistically significant improvement by the proposed NMT+RNNG over the baseline model. It is worthwhile to note that these significant improvements have been achieved without any additional parameters nor computational overhead in the inference time. 6 Conclusion We propose a hybrid model, to which we refer as NMT+RNNG, that combines the decoder of an attention-based neural translation model with the RNNG. This model learns to parse and translate simultaneously, and training it encourages both the encoder and"
P17-2012,W16-2209,0,0.0501205,"ms even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1 Our code is available at https://github.com/ tempra28/nmtrnng. 72 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 72–78 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2012 state sj against each of the hidden states and assigns a scalar score: βi,j = exp(h> i Wd sj ) (Luong et al., 2015). These scores are then normalized across the hidden states to su"
P17-2012,P16-1162,0,0.0282931,"ing training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG. 1 Introduction Neural Machine Translation (NMT) has enjoyed impressive success without relying on much, if any, prior linguistic knowledge. Some of the most recent studies have for instance demonstrated that NMT systems work comparably to other systems even when the source and target sentences are given simply as flat sequences of characters (Lee et al., 2016; Chung et al., 2016) or statistically, not linguistically, motivated subword units (Sennrich et al., 2016; Wu et al., 2016). Shi et al. (2016) recently made an observation that the encoder of NMT captures syntactic properties of a source sentence automatically, indirectly suggesting that explicit linguistic prior may not be necessary. On the other hand, there have only been a couple of recent studies showing the potential benefit of explicitly encoding the linguistic prior into NMT. Sennrich and Haddow (2016) for instance proposed to augment each source word with its corresponding part-of-speech tag, lemmatized 1 Our code is available at https://github.com/ tempra28/nmtrnng. 72 Proceedings of the"
P17-2012,D16-1159,0,\N,Missing
P19-2056,C04-1051,0,0.113486,"nda bear is eating some bamboo.” and “A panda is eating bamboo.”. Such a sentence pair would receive an unfavourable score in similarity evaluation using token-wise comparison, because every word after “panda” would be considered as a mismatched token. In contrast, the STS score given to the pair is 4.2. Omission of words “bear” and “some” in the latter sentence does not alter the meaning from the first sentence, and thus the pair is considered semantically similar. STS is similar to other semantic comparison tasks such as textual entailment (Dagan et al., 2010) and paraphrase identification (Dolan et al., 2004). One key distinction that STS has from these two tasks is that STS expects the model to output continuous scores with interpretable intermediate values rather than discrete binary values describing whether or not given sentence pairs have certain semantic relationships. 3.2 STS Estimator The STS estimator model rψ consists of two modules. As described in Eq. (6), one is the BERT encoder with pooling layer B and the other is a linear output layer (with weight vector Wψ and bias bψ ) with ReLU activation rψ . B (Y1 , Y2 ) = Pool (BERT (Y1 , Y2 )) , (6) rψ (Y1 , Y2 ) = ReLU (Wψ · B (Y1 , Y2 ) +"
P19-2056,D15-1166,0,0.0450471,"4.2 monsense inference task SWAG (Zellers et al., 2018). STS is one of the tasks included in GLUE. Sentence Pair A man is playing a guitar. A girl is playing a guitar. A panda bear is eating some bamboo. A panda is eating bamboo. 3 Models 3.1 Sentence Generation Model The sentence generation model πθ used for this research is a neural machine translation (NMT) model consisting of a single-layer LSTM encoderdecoder model with attention mechanism and the softmax output layer. The model also incorporates input feeding to make itself aware of the alignment decision in the previous decoding step (Luong et al., 2015). The encoder LSTM is bidirectional while the decoder LSTM is unidirectional. On the other hand, STS scores are tolerant of modifications that do not change the meaning of sentence. This leniency is illustrated by the second sentence pair in Table 1, “A panda bear is eating some bamboo.” and “A panda is eating bamboo.”. Such a sentence pair would receive an unfavourable score in similarity evaluation using token-wise comparison, because every word after “panda” would be considered as a mismatched token. In contrast, the STS score given to the pair is 4.2. Omission of words “bear” and “some” in"
P19-2056,S12-1051,0,0.269421,"o token mismatches. As another example, the sentence pair “He often walked to school.” and “He walked to school often.” would be severely punished by the token misalignment, despite having identical meanings. To tackle the inflexible nature of model evaluation during training, we propose an approach of using semantic similarity between the output sequence and the ground-truth sequence to train the generation model. In the proposed framework, semantic similarity of sentence pairs is estimated by a BERT-based (Devlin et al., 2018) regression model fine-tuned against Semantic Textual Similarity (Agirre et al., 2012) dataset, and the resulting score is passed back to the model using reinforcement learning strategies. Our experiment on translation datasets suggests that the proposed method is better at improving the BLEU score than the traditional cross-entropy learning. However, since the model outputs had limited paraphrastic variations, the results are also inconclusive in supporting the effectiveness of applying the proposed method to sentence generation. Traditional model training for sentence generation employs cross-entropy loss as the loss function. While cross-entropy loss has convenient propertie"
P19-2056,N18-2102,0,0.0246464,"ure known as an Encoder-Decoder model. The decoder model, the portion of Encoder400 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 400–406 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Decoder responsible for generating tokens, is usually an RNN. For an intermediate representation X, output token distribution at time t yˆt for the RNN decoder πθ can be written as st+1 = Φθ (ˆ yt , st , X) (1) yˆt+1 ∼ πθ (yt |yˆt , st , X) (2) generation model against sentence-level metrics (Pasunuru and Bansal, 2018; Ranzato et al., 2015). Sentence-level metrics commonly used in RL settings, such as BLEU, ROUGE and METEOR, are typically not differentiable, and thus are not usable under the regular supervised training. One of the common RL algorithms used in sentence generation is REINFORCE (Williams, 1992). REINFORCE is a relatively simple policy gradient algorithm. In the context of sentence generation, the goal of the agent is to maximize the expectation of the reward provided as the function r as in the following: where st is the hidden state of the decoder at time t, Φθ is the state update function,"
P19-2056,P17-1161,0,0.0289446,"oined by a separation (SEP) token and outputs intermediate representations that are then fed into the linear layer through a pooling layer. The output layer projects the input into scalar values representing the estimated STS scores for input sentence pairs. The model rψ is trained using the mean squared error (MSE) to fit the corresponding real-valued label v as written in Eq. (8). 2.4 BERT Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2018) is a pre-training model based on the transformer Previous premodel (Vaswani et al., 2017). training models such as ELMo (Peters et al., 2017) and OpenAI-GPT (Radford et al., 2018) used unidirectional language models to learn general language representations and this limited their ability to capture token relationships in both directions. Instead, BERT employs a bidirectional self-attention architecture to capture the language representations more thoroughly. Upon its release, BERT broke numerous state-of-the-art records such as those on a general language understanding task GLUE (Wang et al., 2018), question answering task SQuAD v1.1 (Rajpurkar et al., 2016), and grounded comLBERT = |rψ (Y1 , Y2 ) − v|2 . (8) While the use of the B"
P19-2056,2012.eamt-1.60,0,0.00977491,"ion model Ωω is trained using the MSE loss as follows: 2 ) 1 ( ˆ LBSE = rψ Y , Y − Ωω (st ) . (13) 5 3.3 Baseline Estimator The reward predictor does not share its parameter with the NMT model. Following the previous work (Ranzato et al., 2015), the baseline estimator Ωω is defined as follows: Ωω (st ) = σ (Wω · st + bω ) , 4 Experiment 4.1 Dataset (9) The dataset used for fine-tuning the STS estimator is STS-B (Cer et al., 2017). The tokenizer used is a wordpiece tokenizer for BERT. For machine translation, we used De-En parallel corpora from multi30k-dataset (Elliott et al., 2016) and WIT3 (Cettolo et al., 2012). The multi30k-dataset is comprised of textual descriptions of images while the WIT3 consists of transcribed TED talks. Each corpus provides a single validation set and multiple test sets. We chose the best models based on their scores for the validation sets and used the two newest test sets from each corpus for testing. Both corpora are tokenized using the sentencepiece BPE tokenizer with a vocabulary size of 8,000 for each language. All letters are turned to lowercase and any consecutive spaces are turned into a single space before tokenization. The source and target vocabularies are kept s"
P19-2056,D15-1044,0,0.0321925,"ed by the training with crossentropy loss. We use the BERT-based scorer fine-tuned to the Semantic Textual Similarity (STS) task for semantic similarity estimation, and train the model with the estimated scores through reinforcement learning (RL). Our experiments show that reinforcement learning with semantic similarity reward improves the BLEU scores from the baseline LSTM NMT model. 1 Introduction Sentence generation using neural networks has become a vital part of various natural language processing tasks including machine translation (Sutskever et al., 2014) and abstractive summarization (Rush et al., 2015). Most previous work on sentence generation employ crossentropy loss between the model outputs and the ground-truth sentence to guide the maximumlikelihood training on the token-level. Differentiability of cross-entropy loss is useful for computing gradients in supervised learning; however, it lacks flexibility and may penalize the generation model for a slight shift or change in token sequence even if the sequence retains the meaning. For instance, consider the sentence pair, “I watched a movie last night.” and “I saw a film last 2 Related Work 2.1 Sentence Generation Recurrent neural network"
P19-2056,W18-5446,0,0.0354785,"t al., 2018) is a pre-training model based on the transformer Previous premodel (Vaswani et al., 2017). training models such as ELMo (Peters et al., 2017) and OpenAI-GPT (Radford et al., 2018) used unidirectional language models to learn general language representations and this limited their ability to capture token relationships in both directions. Instead, BERT employs a bidirectional self-attention architecture to capture the language representations more thoroughly. Upon its release, BERT broke numerous state-of-the-art records such as those on a general language understanding task GLUE (Wang et al., 2018), question answering task SQuAD v1.1 (Rajpurkar et al., 2016), and grounded comLBERT = |rψ (Y1 , Y2 ) − v|2 . (8) While the use of the BERT-based STS estimator as an evaluation mechanism allows the sentence generation model to train its outputs against sentence-wise evaluation criteria, there is a downside to this framework. The BERT encoder expects the input sentences to be sequences of tokens. As with most sentence generation models, the outputs of the encoderdecoder model described in the previous subsection are sequences of output probability distributions of tokens. 402 LRL in the RL stag"
P19-2056,D18-1009,0,0.0140093,"model training. Reinforcement learning, a framework in which the agent must choose a series of discrete actions to maximize the reward returned from its surrounding environment, is one of such approaches. The advantages of using RL are that the reward for an action does not have to be returned spontaneously and that the reward function does not have to be differentiable by the parameter of the agent model. Because of these advantages, RL has often been used as a means to train sentence 401 Table 1: Examples of STS similarity scores in STS-B dataset. Score 2.8 4.2 monsense inference task SWAG (Zellers et al., 2018). STS is one of the tasks included in GLUE. Sentence Pair A man is playing a guitar. A girl is playing a guitar. A panda bear is eating some bamboo. A panda is eating bamboo. 3 Models 3.1 Sentence Generation Model The sentence generation model πθ used for this research is a neural machine translation (NMT) model consisting of a single-layer LSTM encoderdecoder model with attention mechanism and the softmax output layer. The model also incorporates input feeding to make itself aware of the alignment decision in the previous decoding step (Luong et al., 2015). The encoder LSTM is bidirectional w"
S13-2015,P07-2044,0,0.113125,"Missing"
S13-2015,J08-1002,0,0.0104023,"are extracted using Stanford coreNLP (Stanford NLP Group, 2012). • Lexical semantic information Synonyms of event word tokens from WordNet lexical database (Fellbaum, 1998) are used as features. • Event-Event information For event-event TLINKs, we use same sentence feature to differentiate pairs of events in the same sentence from pairs of events from different sentences (Chambers et al., 2007). In the case that temporal entities of a particular TLINK are in the same sentence, we extract two new types of sentence-level semantic information from a deep syntactic parser. We use the Enju parser (Miyao and Tsujii, 2008). It analyzes syntactic/semantic structures of sentences and provides phrase structures and predicate-argument structures. The features we extract from the deep parser are • Paths between event words in the phrase structure tree, and up(↑)/down(↓) lengths of paths. We use 3-grams of paths as features instead of full paths since these are too sparse. An example is shown in Figure 1. In this case, the path between the event words, estimates and worth, is VBZ↑, VX↑, VP↑, VP↑, VP, PP↓, PX↓, IN↓. The 3-grams of the path are, therefore, {VBZ↑VX↑-VP↑, VX↑-VP↑-VP↑, VP↑-VP↑-VP, VP↑-VP-PP↓, VP-PP↓-PX↓,"
S13-2015,P11-2061,0,0.0418153,"produces many unreasonable and excessive links. We thus use a machine learning approach to filter out those unreasonable links by training the model in Section 2.2 with an additional relation type, UNKNOWN, for links that satisfy the rules in Section 2.1 but do not appear in the training data. In this way, for Task C, we first extract all the links that satisfy the rules and classify the relation types of those links. After classifying temporal relations, we remove the links that are classified as UNKNOWN. 3 Evaluation The scores are calculated by the graph-based evaluation metric proposed by UzZaman and Allen (2011). We trained the models with TimeBank and AQUAINT corpora. We also trained our models on the training set with inverse relations. The performance analysis is based on 10-fold cross validation on the development data. 3.1 Task C In Task C, a system has to identify appropriate temporal links and to classify each link into one temporal relation type. For Task C evaluation, we compare the results of the models trained with and without the features from the deep parser. The results are shown in Table 1. The rule-based approach gives a very low precision. 3.2 Task C-relation-only Task C-relation-onl"
saetre-etal-2008-connecting,N03-2020,1,\N,Missing
saetre-etal-2008-connecting,P06-1128,1,\N,Missing
saetre-etal-2008-connecting,P05-1011,1,\N,Missing
saetre-etal-2008-connecting,nenadic-etal-2006-towards,1,\N,Missing
W03-0417,P01-1005,0,0.0560337,"Missing"
W03-0417,J96-1002,0,0.0230026,"Missing"
W03-0417,W99-0613,0,0.0464709,"ne of the major obstacles to applying machine learning techniques to real-world NLP applications. Recently, learning algorithms called minimally supervised learning or unsupervised learning that can make use of unlabeled data have received much attention. Since collecting unlabeled data is generally much easier than annotating data, such techniques have potential for solving the problem of annotation cost. Those approaches include a naive Bayes classifier combined with the EM algorithm (Dempster et al., 1977; Nigam et al., 2000; Pedersen and Bruce, 1998), Co-training (Blum and Mitchell, 1998; Collins and Singer, 1999; Nigam and Ghani, 2000), and Transductive Support Vector Machines (Joachims, 1999). These algorithms have been applied to some tasks including text classification and word sense disambiguation and their effectiveness has been demonstrated to some extent. Combining a naive Bayes classifier with the EM algorithm is one of the promising minimally supervised approaches because its computational cost is low (linear to the size of unlabeled data), and it does not require the features to be split into two independent sets unlike cotraining. However, the use of unlabeled data via the basic EM algorit"
W03-0417,A00-2009,0,0.0173806,"and how to impose this constraint on the learning process. Section 4 describes the problem of confusion set disambiguation and the features used in the experiments. Experimental results are presented in Section 5. Related work is discussed in Section 6. Section 7 offers some concluding remarks. 2 Naive Bayes Classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing, information retrieval, etc. (Escudero et al., 2000; Lewis, 1998; Nigam and Ghani, 2000; Pedersen, 2000). In this section, we briefly review the naive Bayes classifier and the EM algorithm that is used for making use of unlabeled data. 2.1 Naive Bayes Model Let x be a vector we want to classify, and c k be a possible class. What we want to know is the probability that the vector x belongs to the class c k . We first transform the probability P (c k |x) using Bayes’ rule, P (x|ck ) . P (x) word occurrences. Despite this apparent violation of the assumption, the naive Bayes classifier exhibits good performance for various natural language processing tasks. There are some implementation varian"
W03-0417,P94-1013,0,0.0404462,"n is defined as the problem of choosing the correct word from a set of words that are commonly confused. For example, quite may easily be mistyped as quiet. An automatic proofreading system would need to judge which is the correct use given the context surrounding the target. Example confusion sets include: {principle, principal}, {then, than}, and {weather, whether}. Until now, many methods have been proposed for this problem including winnow-based algorithms (Golding and Roth, 1999), differential grammars (Powers, 1998), transformation based learning (Mangu and Brill, 1997), decision lists (Yarowsky, 1994). Confusion set disambiguation has very similar characteristics to a word sense disambiguation problem in which the system has to identify the meaning of a polysemous word given the surrounding context. The merit of using confusion set disambiguation as a test-bed for a learning algorithm is that since one does not need to annotate the examples to make labeled data, one can conduct experiments using an arbitrary amount of labeled data. 4.1 Features As the input of the classifier, the context of the target must be represented in the form of a vector. We use a binary feature vector which contain"
W03-0417,P95-1026,0,0.172867,"Missing"
W03-0417,W97-1011,0,\N,Missing
W03-1306,W02-0301,1,0.356425,"2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one in the dictionary to the target term. This ad"
W03-1306,A00-2009,0,0.0124049,"duct binary classification (“accept” or “reject”) on each candidate. The candidates that are classified into “rejected” are filtered out. In other words, only the candidates that are classified into “accepted” are recognized as protein names. In this paper, we use a naive Bayes classifier for this classification task. 4.1 Naive Bayes classifier The naive Bayes classifier is a simple but effective classifier which has been used in numerous applications of information processing such as image recognition, natural language processing and information retrieval (Lewis, 1998; Escudero et al., 2000; Pedersen, 2000; Nigam and Ghani, 2000). Here we briefly review the naive Bayes model. Let ~x be a vector we want to classify, and c k be a possible class. What we want to know is the probability that the vector ~x belongs to the class c k . We first transform the probability P (c k |~x) using Bayes’ rule, P (~x|ck ) (5) P (ck |~x) = P (ck ) × P (~x) Class probability P (ck ) can be estimated from training data. However, direct estimation of P (c k |~x) is impossible in most cases because of the sparseness of training data. By assuming the conditional independence among the elements of a vector, P (~x|c k )"
W03-1306,W02-2029,0,0.0332044,"the most important tasks today (Marcotte et al., 2001; Thomas et al., 2000; Ono et al., 2001). To extract information of proteins, one has to first recognize protein names in a text. This kind of problem has been studied in the field of natural language processing as named entity recognition tasks. Ohta et al. (2002) provided the GENIA corpus, an annotated corpus of MEDLINE abstracts, which can be used as a gold-standard for evaluating and training named entity recognition algorithms. There are some research efforts using machine learning techniques to recognize biological entities in texts (Takeuchi and Collier, 2002; Kim and Tsujii, 2002; Kazama et al., 2002). One drawback of these machine learning based approaches is that they do not provide identification information of recognized terms. For the purpose of information extraction of protein-protein interaction, the ID information of recognized proteins, such as GenBank 1 ID or SwissProt 2 ID, is indispensable to integrate the extracted information with the data in other information sources. Dictionary-based approaches, on the other hand, intrinsically provide ID information because they recognize a term by searching the most similar (or identical) one i"
W05-1304,W03-1018,1,\N,Missing
W05-1304,J96-1002,0,\N,Missing
W05-1304,nenadic-etal-2002-automatic,1,\N,Missing
W05-1511,E03-1052,0,0.140946,"-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The cont"
W05-1511,P03-1014,0,0.137203,"asa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the lar"
W05-1511,C04-1185,0,0.103326,"JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser. The contributions of the large constituent"
W05-1511,P02-1036,0,0.582034,"ity is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-scale corpora, the Penn treebank. We show how techniques developed for efficient deep parsing can improve the efficiency of probabilistic parsing. These techniques were evaluated in exp"
W05-1511,W97-0302,0,0.13169,"plications, a number of studies have focused on improving the parsing efficiency of unificationbased grammars (Oepen et al., 2002). Although significant improvements in efficiency have been made, parsing speed is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth Int"
W05-1511,J97-4005,0,0.0385754,"ion provides the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized prod"
W05-1511,P03-1046,0,0.0915903,"Tsujii (2005a). Table 1 shows the abbreviations used in presenting the results. We measured the accuracy of the predicateargument relations output by the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Precision/recall is the ratio of tuples correctly identified by the parser. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and performance was evaluated using sentences of less than 40 words in Section 23 (2,164 sentences, 20.3 words/sentence). The performance of each parsing technique was analyzed using the sentences in Section 24 of less than 15 words (305 sentences) and less than 40 words (1145 sentences). Table 2 shows the parsing performance using all"
W05-1511,J96-1002,0,0.0108747,"set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model"
W05-1511,P99-1069,0,0.0875204,"the phrasal sign of the mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp"
W05-1511,J93-1002,0,0.0695129,"hout using probabilities and then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG"
W05-1511,J98-2004,0,0.0287342,"on-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm"
W05-1511,A00-2018,0,0.0825924,"edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (N"
W05-1511,N04-1013,0,0.39086,"artment of Computer Science The University of Tokyo ninomi@is.s.u-tokyo.ac.jp Yoshimasa Tsuruoka CREST, JST and Department of Computer Science The University of Tokyo tsuruoka@is.s.u-tokyo.ac.jp Yusuke Miyao Department of Computer Science The University of Tokyo yusuke@is.s.u-tokyo.ac.jp Jun’ichi Tsujii Department of Computer Science The University of Tokyo and School of Informatics University of Manchester and CREST, JST tsujii@is.s.u-tokyo.ac.jp Abstract Next, we applied parsing techniques developed for deep parsing, including quick check (Malouf et al., 2000), large constituent inhibition (Kaplan et al., 2004) and hybrid parsing with a CFG chunk parser (Daum et al., 2003; Frank et al., 2003; Frank, 2004). The experiments showed how each technique contributes to the final output of parsing in terms of precision, recall, and speed for the Penn treebank. We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank. We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG. Next, we tested three techniques originally developed for deep parsing: quick check, large constituent in"
W05-1511,W89-0206,0,0.237937,"from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in"
W05-1511,P99-1061,0,0.0644723,"rse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their un"
W05-1511,P04-1014,0,0.182952,"hat is, only a nonterminal symbol of a mother is considered in further processing by ignoring the structure of its daughters. With this assumption, we can compute the figures of merit (FOMs) of partial parse results. This assumption restricts the possibility of feature functions that represent non-local dependencies expressed in a parse result. Since unification-based grammars can express semantic relations, such as predicate-argument relations, in their structure, the assumption unjustifiably restricts the flexibility of probabilistic modeling. However, previous research (Miyao et al., 2003; Clark and Curran, 2004; Kaplan et al., 2004) showed that predicate-argument relations can be represented under the assumption of feature locality. We thus assumed the locality of feature functions and exploited it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinat"
W05-1511,C02-1075,0,0.0132895,"d then select the highest probability parse. The behavior of their algorithms is like that of the Viterbi algorithm for PCFG parsing, so the correct parse with the highest probability is guaranteed. The interesting point of this approach is that, once the exhaustive parsing is completed, the probabilities of non-local dependencies, which cannot be computed during parsing, are computed after making a packed parse forest. Probabilistic models where probabilities are assigned to the CFG backbone of the unification-based grammar have been developed (Kasper et al., 1996; Briscoe and Carroll, 1993; Kiefer et al., 2002), and the most probable parse is found by PCFG parsing. This model is based on PCFG and not probabilistic unification-based grammar parsing. Geman and Johnson (Geman and Johnson, 2002) proposed a dynamic programming algorithm for finding the most probable parse in a packed parse forest generated by unification-based grammars without expanding the forest. However, the efficiency of this algorithm is inherently limited by the inefficiency of exhaustive parsing. In this paper we describe the performance of beam thresholding, including iterative parsing, in probabilistic HPSG parsing for a large-s"
W05-1511,N03-1016,0,0.0639364,"Missing"
W05-1511,W02-2018,0,0.0412473,"f unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a characteristic corresponding to fi appears in parse result T . Model parameters λi are estimated using numerical optimization methods (Malouf, 2002) so as to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). They assumed that features are functions"
W05-1511,J93-4001,0,0.0395362,"it for the efficient search of probable parse results. 3 Techniques for efficient deep parsing Many of the techniques for improving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification proces"
W05-1511,P03-1026,0,0.0211687,"ving the parsing efficiency of deep linguistic analysis have been developed in the framework of lexicalized grammars such as lexical functional grammar (LFG) (Bresnan, 105 1982), lexicalized tree adjoining grammar (LTAG) (Shabes et al., 1988), HPSG (Pollard and Sag, 1994) or combinatory categorial grammar (CCG) (Steedman, 2000). Most of them were developed for exhaustive parsing, i.e., producing all parse results that are given by the grammar (Matsumoto et al., 1983; Maxwell and Kaplan, 1993; van Noord, 1997; Kiefer et al., 1999; Malouf et al., 2000; Torisawa et al., 2000; Oepen et al., 2002; Penn and Munteanu, 2003). The strategy of exhaustive parsing has been widely used in grammar development and in parameter training for probabilistic models. We tested three of these techniques. Quick check Quick check filters out non-unifiable feature structures (Malouf et al., 2000). Suppose we have two non-unifiable feature structures. They are destructively unified by traversing and modifying them, and then finally they are found to be not unifiable in the middle of the unification process. Quick check quickly judges their unifiability by peeping the values of the given paths. If one of the path values is not unif"
W05-1511,P00-1061,0,0.132155,"e mother. The sign of the larger constituent is obtained by repeatedly applying schemata to lexical/phrasal signs. Finally, the parse result is output as a phrasal sign that dominates the sentence. Given set W of words and set F of feature structures, an HPSG is formulated as a tuple, G = hL, Ri, where L = {l = hw, F i|w ∈ W, F ∈ F} is a set of lexical entries, and R is a set of schemata, i.e., r ∈ R is a partial function: F × F → F. Given a sentence, an HPSG computes a set of phrasal signs, i.e., feature structures, as a result of parsing. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Miyao et al., 2003; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars as a log-linear model or maximum entropy model (Berger et al., 1996). The probability of parse result T assigned to given sentence w = hw1 , . . . , wn i is ! X 1 exp λi fi (T ) p(T |w) = Zw i Zw = X T0 exp X i ! λi fi (T 0 ) , where λi is a model parameter, and fi is a feature function that represents a characteristic of parse tree T . Intuitively, the probability is defined as the normalized product of the weights exp(λi ) when a character"
W05-1511,J01-2004,0,0.0203056,"s brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorithm, which is essentially a bottomup parser, is a natural choice for non-probabilistic HPSG parsers. Many of the constraints are expressed as lexical entries in HPSG, and bottom-up parsers can use those constraints to reduce the search space in the early stages of parsing. For PCFG, extending the CYK algorithm to output the Viterbi parse is straightforward (Ney, 1991; Jur"
W05-1511,P80-1024,0,0.762838,"Missing"
W05-1511,C88-2121,0,0.072195,"Missing"
W05-1511,P05-1011,1,0.829882,"is still not high enough for practical applications. Introduction We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic head-driven phrase structure grammar (HPSG) parsing for the Penn treebank. We first applied beam thresholding techniques developed for CFG parsing to HPSG parsing, including local thresholding, global thresholding (Goodman, 1997), and iterative parsing (Tsuruoka and Tsujii, 2005b). The recent introduction of probabilistic models of wide-coverage unification-based grammars (Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) has opened up the novel possibility of increasing parsing speed by guiding the search path using probabilities. That is, since we often require only the most probable parse result, we can compute partial parse results that are likely to contribute to the final parse result. This approach has been extensively studied in the field of probabilistic 103 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 103–114, c Vancouver, October 2005. 2005 Association for Computational Linguistics CFG (PCFG) parsing, such as Viterbi parsing and beam thresholding. While many"
W05-1511,J00-1003,0,0.0194491,"on. The grammar for the chunk parser is automatically extracted from the CFG treebank translated from the HPSG treebank, which is generated during grammar extraction from the Penn treebank. The principal idea of using the chunk parser is to use the bracket information, i.e., parse trees without non-terminal symbols, and prevent the HPSG parser from generating edges that cross brackets. 4 4.1 Beam thresholding for HPSG parsing Simple beam thresholding Many algorithms for improving the efficiency of PCFG parsing have been extensively investigated. They include grammar compilation (Tomita, 1986; Nederhof, 2000), the Viterbi algorithm, controlling search strategies without FOM such as left-corner parsing (Rosenkrantz and Lewis II, 1970) or headcorner parsing (Kay, 1989; van Noord, 1997), and with FOM such as the beam search, the best-first search or A* search (Chitrao and Grishman, 1990; Caraballo and Charniak, 1998; Collins, 1999; Ratnaparkhi, 1999; Charniak, 2000; Roark, 2001; Klein 106 and Manning, 2003). The beam search and bestfirst search algorithms significantly reduce the time required for finding the best parse at the cost of losing the guarantee of finding the correct parse. The CYK algorit"
W05-1511,W05-1514,1,0.885649,"Missing"
W05-1511,J97-3004,0,0.449191,"Missing"
W05-1511,J93-2004,0,\N,Missing
W05-1511,J03-4003,0,\N,Missing
W05-1514,N01-1025,0,0.0416873,"overed from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume V"
W05-1514,P05-1024,0,0.0119296,"rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on chunk parsing. However"
W05-1514,P95-1037,0,0.0610444,", the 3rd iteration. NP S NP VBD DT JJ volume was a light QP million NNS . ounces . Figure 2: Chunk parsing, the 2nd iteration. NP VP . volume was . Figure 4: Chunk parsing, the 4th iteration. 2 Chunk Parsing For the overall strategy of chunk parsing, we follow the method proposed by Sang (Tjong Kim Sang, 2001). Figures 1 to 4 show an example of chunk parsing. In the first iteration, the chunker identifies two base phrases, (NP Estimated volume) and (QP 2.4 million), and replaces each phrase with its nonterminal symbol and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved"
W05-1514,W05-1511,1,0.828986,"ider only the chunks whose probabilities are within the predefined margin from , .. In other words, the chunks whose probabilities 435 , .0/21 are larger than are considered as assured chunks, and thus are fixed when we generate alternative hypotheses of chunking..-7The chunks 435 , 6 whose probabilities are smaller than /21 are simply ignored.      We generate alternative hypotheses in each level of chunking, and search the best parse in a depthfirst manner. 7.3 Iterative parsing We also tried an iterative parsing strategy, which was successfully used in probabilistic HPSG parsing (Ninomiya et al., 2005). The parsing strategy is simple. The parser starts with a very low margin and tries to find a successful parse. If the parser cannot find a successful parse, then it increases the margin by a certain step and tries to parse with the wider margin. Threshold 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 8 Experiments We ran parsing experiments using the Penn Treebank corpus, which is widely used for evaluating parsing algorithms. The training set consists of sections 02-21. We used section 22 as the development data, with which we tuned the feature set and parameters for parsing. The test set consists of"
W05-1514,W95-0107,0,0.0275409,"and head. The head word is identified by using the head-percolation table (Magerman, 1995). In the second iteration, the chunker identifies (NP a light million ounces) and converts this phrase into NP. This chunking procedure is repeated until the whole sentence is chunked at the fourth iteration, and the full parse tree is easily recovered from the chunking history. This parsing strategy converts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as"
W05-1514,W96-0213,0,0.107079,") .-. 3. NP-volume VBD-was (.-.) 4. (NP-volume VBD-was) .-. 5. NP-volume (VBD-was .-.) The performance of chunk parsing heavily depends on the performance of each level of chunking. The popular approach to this shallow parsing is to convert the problem into a tagging task and use a variety 134 6. (NP-volume VBD-was .-.) The merit of taking the sliding window approach is that we can make use of a richer set of features on recognizing a phrase than in the sequential labeling    4 Filtering with the CFG Rule Dictionary We use an idea that is similar to the method proposed by Ratnaparkhi (Ratnaparkhi, 1996) for partof-speech tagging. They used a Tag Dictionary, with which the tagger considers only the tag-word pairs that appear in the training sentences as the candidate tags. A similar method can be used for reducing the number of phrase candidates. We first construct a rule dictionary consisting of all the CFG rules used in the training data. In both training and parsing, we filter out all the sub-sequences that do not match any of the entry in the dictionary. 4.1 Normalization The rules used in the training data do not cover all the rules in unseen sentences. Therefore, if we take a naive filt"
W05-1514,W97-0301,0,0.512586,"Missing"
W05-1514,J96-1002,0,0.0103728,"owing information as the features. The Right-Hand-Side (RHS) of the CFG rule The left-adjacent nonterminal symbol. The right-adjacent nonterminal symbol. By assuming the conditional independence among the features, we can compute the probability for filtering as follows:                                                        6 Phrase Recognition with a Maximum Entropy Classifier For the candidates which are not filtered out in the above two phases, we perform classification with maximum entropy classifiers (Berger et al., 1996). We construct a binary classifier for each type of phrases using the entire training set. The training samples for maximum entropy consist of the phrase candidates that have not been filtered out by the CFG rule dictionary and the naive Bayes classifier. One of the merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can co"
W05-1514,N03-1028,0,0.0464251,"nverts the problem of full parsing into smaller and simpler problems, namely, chunking, where we only need to recognize flat structures (base phrases). Sang used the IOB tagging method proposed by Ramshow(Ramshaw and Marcus, 1995) and memory-based learning for each level of chunking and achieved an f-score of 80.49 on the Penn Treebank corpus. 3 Chunking with a sliding-window approach of machine learning techniques that have been developed for sequence labeling problems such as Hidden Markov Models, sequential classification with SVMs (Kudo and Matsumoto, 2001), and Conditional Random Fields (Sha and Pereira, 2003). One of our claims in this paper is that we should not convert the chunking problem into a tagging task. Instead, we use a classical sliding-window method for chunking, where we consider all subsequences as phrase candidates and classify them with a machine learning algorithm. Suppose, for example, we are about to perform chunking on the sequence in Figure 4. NP-volume VBD-was .-. We consider the following sub sequences as the phrase candidates in this level of chunking. 1. (NP-volume) VBD-was .-. 2. NP-volume (VBD-was) .-. 3. NP-volume VBD-was (.-.) 4. (NP-volume VBD-was) .-. 5. NP-volume (V"
W05-1514,E99-1016,0,0.0609981,"the probabilities output by the maximum entropy classifiers, and show that the search method can further improve the parsing accuracy. The performance of chunk parsing is heavily dependent on the performance of phrase recognition in each level of chunking. We show in this paper that the chunk parsing strategy is indeed appealing in that it can give considerably better performance than previously reported by using a different approach for phrase recognition and that it enables us to build a very fast parser that gives high-precision outputs. 1 Introduction Chunk parsing (Tjong Kim Sang, 2001; Brants, 1999) is a simple parsing strategy both in implementation and concept. The parser first performs chunking by identifying base phrases, and convert the identified phrases to non-terminal symbols. The parser again performs chunking on the updated sequence and convert the newly recognized phrases into non-terminal symbols. The parser repeats this procedure until there are no phrases to be chunked. After finishing these chunking processes, we can reconstruct the complete parse tree of the sentence from the chunking results. This advantage could open up the possibility of using full parsers for large-sc"
W05-1514,A00-2018,0,0.0519962,"Japan Science and Technology Corporation)  Department of Computer Science, University of Tokyo  School of Informatics, University of Manchester  tsuruoka,tsujii  @is.s.u-tokyo.ac.jp Abstract Although the conceptual simplicity of chunk parsing is appealing, satisfactory performance for practical use has not yet been achieved with this parsing strategy. Sang achieved an f-score of 80.49 on the Penn Treebank by using the IOB tagging method for each level of chunking (Tjong Kim Sang, 2001). However, there is a very large gap between their performance and that of widely-used practical parsers (Charniak, 2000; Collins, 1999). Chunk parsing is conceptually appealing but its performance has not been satisfactory for practical use. In this paper we show that chunk parsing can perform significantly better than previously reported by using a simple slidingwindow method and maximum entropy classifiers for phrase recognition in each level of chunking. Experimental results with the Penn Treebank corpus show that our chunk parser can give high-precision parsing outputs with very high speed (14 msec/sentence). We also present a parsing method for searching the best parse by considering the probabilities out"
W05-1514,P02-1034,0,0.0208674,"till we will face unknown rules even when we have constructed the rule dictionary using the whole training data (note that the dotted line does not saturate). Additional feature sets for the maximum entropy classifiers could improve the performance. The bottom-up parsing strategy allows us to use information about sub-trees that have already been constructed. We thus do not need to restrict ourselves to use only head-information of the partial parses. Since many researchers have reported that information on partial parse trees plays an important role for achieving high performance (Bod, 1992; Collins and Duffy, 2002; Kudo et al., 2005), we expect that additional features will improve the performance of chunk parsing. Also, the methods for searching the best parse presented in sections 7.2 and 7.3 have much room for improvement. the search method does not have the device to avoid repetitive computations on the same nonterminal sequence in parsing. A chart-like structure which effectively stores the partial parse results could enable the parser to explore a broader search space and produce better parses. Our chunk parser exhibited a considerable improvement in parsing accuracy over the previous study on ch"
W05-1514,W03-1018,1,0.829715,"he merits of using a maximum entropy classifier is that we can obtain a probability from the classifier in each decision. The probability of each decision represents how likely the candidate is a correct chunk. We accept a chunk only when the probability is larger than the predefined threshold. With this thresholding scheme, we can control the trade-off between precision and recall by changing the threshold value. Regularization is important in maximum entropy modeling to avoid overfitting to the training data. For this purpose, we use the maximum entropy modeling with inequality constraints (Kazama and Tsujii, 2003). This modeling has one parameter to tune as in Gaussian prior modeling. The parameter is called the width factor. We set this parameter to be 1.0 throughout the experiments. For numerical optimization, we used the Limited-Memory Variable-Metric (LMVM) algorithm (Benson and Mor´e, 2001).  where is a binary output indicating whether  the candidate is a phrase of the  target type or not, is the RHS of the CFG rule, is the symbol on the left, and is the symbol on the right. We used the Laplace smoothing method for computing each probability. Note that the information about the result of the ru"
W05-1514,H05-1059,1,0.812454,"changing the maximum number of nodes in the search. The uncertainty margin for chunk recognition was 0.3. Figure 6 shows that Collins parser clearly outperforms our chunk parser when the beam size is large. However, the performance significantly drops with a smaller beam size. The break-even point is at around 200 sec (83 msec/sentence). 8.2 Comparison with previous work Table 6 summarizes our parsing performance on section 23 together with the results of previous studies. In order to make the results directly comparable, we produced POS tags as the input of our parsers by using a POS tagger (Tsuruoka and Tsujii, 2005) which was trained on sections 0-18 in the WSJ corpus. The table also shows the performance achieved 9 Discussion 90 F-Score 85 80 75 70 Chunk parser Collins parser 65 0 50 100 150 200 250 300 350 400 450 500 Time (sec) Figure 6: Time vs F-score on section 23. The xaxis represents the time required to parse the entire section. The time required for making a hash table in Collins parser is excluded. Ratnaparkhi (1997) Collins (1999) Charniak (2000) Kudo (2005) Sang (2001) Deterministic (tagger-POSs) Deterministic (gold-POSs) Search (tagger-POSs) Search (gold-POSs) Iterative Search (tagger-POSs)"
W05-1514,J03-4003,0,\N,Missing
W06-1619,J97-4005,0,0.142664,"verb SUBJ &lt; 1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree"
W06-1619,J99-2004,0,0.501786,"studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Joshi (1999) claimed that if words can be assigned correct supertags, syntactic parsing is almost trivial. Wha"
W06-1619,J96-1002,0,0.016666,"arsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters,"
W06-1619,P05-1022,0,0.134381,"Missing"
W06-1619,C04-1041,0,0.527929,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,P05-1011,1,0.474032,"a Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining"
W06-1619,P04-1014,0,0.410288,"ment of Computer Science University of Tokyo Yoshimasa Tsuruoka School of Informatics University of Manchester Yusuke Miyao Department of Computer Science University of Tokyo Jun’ichi Tsujii Department of Computer Science, University of Tokyo School of Informatics, University of Manchester SORST, Japan Science and Technology Agency Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {ninomi, matuzaki, tsuruoka, yusuke, tsujii}@is.s.u-tokyo.ac.jp Abstract niak and Johnson, 2005) or over complex phrase structures of head-driven phrase structure grammar (HPSG) or combinatory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which wa"
W06-1619,E03-1071,0,0.0114331,"k and Curran, 2004b). The CCG supertagger uses a maximum entropy classifier and is similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2"
W06-1619,W04-3308,0,0.07953,"ar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of parsing. Bangalore and Jos"
W06-1619,P02-1036,0,0.0599348,"he weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0 λu fu (T ) u T0 p0 (T |w) = X n Y p(li |wi ), i=1 where li is a lexical entry assigned to word wi in T and p(li |wi ) is the probability of selecting lexical entry li for wi . In the experiments, we compared our model with the probabilistic HPSG model of Miyao and Tsujii (2005). The features used in their model are combinations of the feature templates listed in Table 1. The feature templates fbinary and funary are defined for constituent"
W06-1619,W05-1511,1,0.805598,"← α + ∆α; β ← β + ∆β; κ ← κ + ∆κ; δ ← δ + ∆δ; θ ← θ + ∆θ; 4.2 We evaluated the speed and accuracy of parsing with extremely lexicalized models by using Enju 2.1, the HPSG grammar for English (Miyao et al., 2005; Miyao and Tsujii, 2005). The lexicon of the grammar was extracted from Sections 02-21 of the Penn Treebank (Marcus et al., 1994) (39,832 sentences). The grammar consisted of 3,797 lexical entries for 10,536 words1 . The probabilistic models were trained using the same portion of the treebank. We used beam thresholding, global thresholding (Goodman, 1997), preserved iterative parsing (Ninomiya et al., 2005) and other techFigure 3: Pseudo-code of iterative parsing for HPSG. Zw = X exp Ã X l0 ! 0 λu fu (l , w, i) , u where Zw is the sum over all possible lexical entries for the word wi . The feature templates used in our model are listed in Table 2 and are word trigrams and POS 5-grams. 4 Evaluation Experiments 1 An HPSG treebank is automatically generated from the Penn Treebank. Those lexical entries were generated by applying lexical rules to observed lexical entries in the HPSG treebank (Nakanishi et al., 2004). The lexicon, however, included many lexical entries that do not appear in the HPSG"
W06-1619,W97-0302,0,0.1716,", i). The FOM of a newly created partial parse, F , is computed by summing the values of ρ of the daughters and an additional FOM of F if the model is the previous model or model 3. The FOM for models 1 and 2 is computed by only summing the values of ρ of the daughters; i.e., weights exp(λu ) in the figure are assigned zero. The terms κ and δ are the thresholds of the number of phrasal signs in the chart cell and the beam width for signs in the chart cell. The terms α and β are the thresholds of the number and the beam width of lexical entries, and θ is the beam width for global thresholding (Goodman, 1997). Table 2: Features for the probabilities of lexical entry selection. procedure Parsing(hw1 , . . . , wn i, hL, Ri, α, β, κ, δ, θ) for i = 1 to n foreachP F 0 ∈ {F |hwi , F i ∈ L} p= λu fu (F 0 ) u π[i − 1, i] ← π[i − 1, i] ∪ {F 0 } if (p &gt; ρ[i − 1, i, F 0 ]) then ρ[i − 1, i, F 0 ] ← p LocalThresholding(i − 1, i,α, β) for d = 1 to n for i = 0 to n − d j =i+d for k = i + 1 to j − 1 foreach Fs ∈ φ[i, k], Ft ∈ φ[k, j], r ∈ R if F = r(Fs , Ft ) has succeeded P λu fu (F ) p = ρ[i, k, Fs ] + ρ[k, j, Ft ] + u π[i, j] ← π[i, j] ∪ {F } if (p &gt; ρ[i, j, F ]) then ρ[i, j, F ] ← p LocalThresholding(i, j,κ,"
W06-1619,P03-1046,0,0.045596,"f the parser. A predicate-argument relation is defined as a tuple hσ, wh , a, wa i, where σ is the predicate type (e.g., adjective, intransitive verb), wh is the head word of the predicate, a is the argument label (MODARG, ARG1, ..., ARG4), and wa is the head word of the argument. Labeled precision (LP)/labeled recall (LR) is the ratio of tuples correctly identified by the parser3 . Unlabeled precision (UP)/unlabeled recall (UR) is the ratio of tuples without the predicate type and the argument label. This evaluation scheme was the same as used in previous evaluations of lexicalized grammars (Hockenmaier, 2003; Clark and Curran, 2004b; Miyao and Tsujii, 2005). The experiments were conducted on an AMD Opteron server with a 2.4-GHz CPU. Section 22 of the Treebank was used as the development set, and the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is sho"
W06-1619,P99-1069,0,0.0141744,"1 &gt; COMPS &lt; &gt; come Spring/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum"
W06-1619,P00-1061,0,0.0728146,"ing/NN HEAD verb 2 SUBJ &lt; 1 &gt; COMPS &lt;&gt; has/VBZ come/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all po"
W06-1619,N04-1013,0,0.0264089,"e/VBN flex= &lt;spring, NN, Figure 1: HPSG parsing. Ã X T0 exp &gt; (2005) also introduced a preliminary probabilistic model p0 (T |w) whose estimation does not require the parsing of a treebank. This model is introduced as a reference distribution of the probabilistic HPSG model; i.e., the computation of parse trees given low probabilities by the model is omitted in the estimation stage. We have ! X 1 exp λu fu (T ) phpsg (T |w) = Zw u Ã X HEAD noun SUBJ &lt;&gt; COMPS &lt;&gt; Figure 2: Example of features. Previous studies (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) defined a probabilistic model of unification-based grammars including HPSG as a log-linear model or maximum entropy model (Berger et al., 1996). The probability that a parse result T is assigned to a given sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively,"
W06-1619,P03-1064,0,0.184888,"similar to our model. We evaluated the performance of our probabilistic model as a supertagger. The accuracy of the resulting supertagger on our development set (Section 22) is given in Table 5 and Table 6. The test sentences were automatically POS-tagged. Results of other supertaggers for automatically exWhen compared with other supertag sets of automatically extracted lexicalized grammars, the (effective) size of our supertag set, 1,361 lexical entries, is between the CCG supertag set (398 categories) used by Curran and Clark (2003) and the LTAG supertag set (2920 elementary trees) used by Shen and Joshi (2003). The relative order based on the sizes of the tag sets exactly matches the order based on the accuracies of corresponding supertaggers. 161 ambiguation of phrase structures. We have not yet investigated whether our results can be reproduced with other lexicalized grammars. Our results might hold only for HPSG because HPSG has strict feature constraints and has lexical entries with rich syntactic information such as wh-movement. 5.2 Efficacy of extremely lexicalized models The implemented parsers of models 1 and 2 were around four times faster than the previous model without a loss of accuracy"
W06-1619,P03-1054,0,0.0219404,"hrasestructure-based model. The hybrid model is not only significantly faster but also significantly more accurate by two points of precision and recall compared to the previous model. 1 Introduction For the last decade, accurate and wide-coverage parsing for real-world text has been intensively and extensively pursued. In most of state-of-theart parsers, probabilistic events are defined over phrase structures because phrase structures are supposed to dominate syntactic configurations of sentences. For example, probabilities were defined over grammar rules in probabilistic CFG (Collins, 1999; Klein and Manning, 2003; Char155 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 155–163, c Sydney, July 2006. 2006 Association for Computational Linguistics probabilistic model is defined as the probability of unigram supertagging. So, the hybrid model can be regarded as an extension of supertagging from unigram to n-gram. The hybrid model can also be regarded as a variant of the statistical CDG parser (Wang, 2003; Wang and Harper, 2004), in which the parse tree probabilities are defined as the product of the supertagging probabilities and the dependency pr"
W06-1619,H05-1059,1,0.811846,"nd the performance was evaluated using sentences of ≤ 40 and 100 words in Section 23. The performance of each parsing technique was analyzed using the sentences in Section 24 of ≤ 100 words. Table 3 details the numbers and average lengths of the tested sentences of ≤ 40 and 100 words in Sections 23 and 24, and the total numbers of sentences in Sections 23 and 24. The parsing performance for Section 23 is shown in Table 4. The upper half of the table shows the performance using the correct POSs in the Penn Treebank, and the lower half shows the performance using the POSs given by a POS tagger (Tsuruoka and Tsujii, 2005). The left and right sides of the table show the performances for the sentences of ≤ 40 and ≤ 100 words. Our models significantly increased not only the parsing speed but also the parsing accuracy. Model 3 was around three to four times faster and had around two points higher precision and recall than the previous model. Surprisingly, model 1, which used only lexical information, was very fast and as accurate as the previous model. Model 2 also improved the accuracy slightly without information of phrase structures. When the automatic POS tagger was introduced, both precision and recall droppe"
W06-1619,W04-0307,0,0.294366,"natory categorial grammar (CCG) (Clark and Curran, 2004b; Malouf and van Noord, 2004; Miyao and Tsujii, 2005). Although these studies vary in the design of the probabilistic models, the fundamental conception of probabilistic modeling is intended to capture characteristics of phrase structures or grammar rules. Although lexical information, such as head words, is known to significantly improve the parsing accuracy, it was also used to augment information on phrase structures. Another interesting approach to this problem was using supertagging (Clark and Curran, 2004b; Clark and Curran, 2004a; Wang and Harper, 2004; Nasr and Rambow, 2004), which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999). Supertagging is a process where words in an input sentence are tagged with ‘supertags,’ which are lexical entries in lexicalized grammars, e.g., elementary trees in LTAG, lexical categories in CCG, and lexical entries in HPSG. Supertagging was, in the first place, a technique to reduce the cost of parsing with lexicalized grammars; ambiguity in assigning lexical entries to words is reduced by the light-weight process of supertagging before the heavy process of pa"
W06-1619,W02-2018,0,0.00906575,"iven sentence w = hw1 , . . . , wn i is Zw = HEAD verb SUBJ &lt;NP&gt; COMPS &lt;&gt; (Previous probabilistic HPSG) Ã ! X 1 exp λu fu (T ) phpsg0 (T |w) = p0 (T |w) Zw u ! λu fu (T 0 ) , u where λu is a model parameter, fu is a feature function that represents a characteristic of parse tree T , and Zw is the sum over the set of all possible parse trees for the sentence. Intuitively, the probability is defined as the normalized product of the weights exp(λu ) when a characteristic corresponding to fu appears in parse result T . The model parameters, λu , are estimated using numerical optimization methods (Malouf, 2002) to maximize the log-likelihood of the training data. However, the above model cannot be easily estimated because the estimation requires the computation of p(T |w) for all parse candidates assigned to sentence w. Because the number of parse candidates is exponentially related to the length of the sentence, the estimation is intractable for long sentences. To make the model estimation tractable, Geman and Johnson (Geman and Johnson, 2002) and Miyao and Tsujii (Miyao and Tsujii, 2002) proposed a dynamic programming algorithm for estimating p(T |w). Miyao and Tsujii Zw = X Ã 0 p0 (T |w) exp ! 0"
W06-1619,J93-2004,0,\N,Missing
W06-1619,J03-4003,0,\N,Missing
W06-3327,I05-1018,1,0.898746,"Missing"
W06-3327,A00-2021,0,0.0413661,"Missing"
W06-3327,W03-1018,1,0.866569,"Missing"
W06-3327,W04-3111,0,0.0269627,"Missing"
W06-3327,I05-1006,0,0.0437125,"Missing"
W06-3327,J93-2004,0,0.0311788,"Missing"
W06-3327,W96-0213,0,0.284311,"Missing"
W06-3327,tateisi-tsujii-2004-part,1,0.904918,"Missing"
W06-3327,N03-1033,0,0.019641,"Missing"
W07-1505,brants-hansen-2002-developments,0,0.0214672,"nal) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation"
W07-1505,declerck-2006-synaf,0,0.0662362,"4), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time"
W07-1505,C96-1079,0,0.0606984,"(cf. Figure 16) links annotated (named) entities to the ontologies and databases through appropriate attributes, viz. ontologyEntry and sdbEntry. The attribute specificType specifies the analyzed entity in a more detailed way (e.g., Organism can be specified through the species values ‘human’, ‘mouse’, ‘rat’, etc.) The subtypes are currently being developed in the bio-medical domain and cover, e.g., genes, pro39 teins, organisms, diseases, variations. This hierarchy can easily be extended or supplemented with entities from other domains. For illustration purposes, we extended it here by MUC (Grishman and Sundheim, 1996) entity types such as Person, Organization, etc. This scheme is still under construction and will soon also incorporate the representation of relationships between entities and domain-specific events. The general type Relation will then be extended with specific conceptual relations such as location, part-of, etc. The representation of events will be covered by a type which aggregates pre-defined relations between entities and the event mention. An event type such as InhibitionEvent would link the text spans in the sentence ‘protein A inhibits protein B’ in attributes agent (‘protein A’), pati"
W07-1505,W02-1706,0,0.0720681,"Missing"
W07-1505,ide-etal-2000-xces,0,0.0297293,"nres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in ter"
W07-1505,W03-0804,0,0.0538625,"those single modules which serve, by and large, the same functionality? Second, how can we build NLP systems by composing them, at the abstract level of functional specification, from these already existing component building blocks disregarding concrete implementation matters? Yet another burning issue relates to the increasing availability of multiple metadata annotations both in corpora and language processors. If alternative annotation tag sets are chosen for the same functional task a ‘data conversion’ problem is created which should be solved at the abstract specification level as well (Ide et al., 2003). Software engineering methodology points out that these requirements are best met by properly identifying input/output capabilities of constituent components and by specifying a general data model (e.g., based on UML (Rumbaugh et al., 1999)) in order to get rid of the low-level implementation (i.e., coding) layer. A particularly promising proposal along this line of thought is the Unstructured Information Management Architecture (UIMA) (Ferrucci and Lally, 2004) originating from IBM research activities.1 UIMA is but the latest attempt in a series of proposals concerned with more generic NLP e"
W07-1505,J93-2004,0,0.0430432,"gn of annotation schemata for language resources and their standardization have a long-standing tradition in the NLP community. In the very beginning, this work often focused exclusively on subdomains of text analysis such as document structure meta-information, syntactic or semantic analysis. The Text Encoding Initiative (TEI)2 provided schemata for the exchange of documents of various genres. The Dublin Core Metadata Initiative3 established a de facto standard for the Semantic Web.4 For (computational) linguistics proper, syntactic annotation schemes, such as the one from the Penn Treebank (Marcus et al., 1993), or semantic annotations, such as the one underlying ACE (Doddington et al., 2004), are increasingly being used in a quasi standard way. In recent years, however, the NLP community is trying to combine and merge different kinds of annotations for single linguistic layers. XML formats play a central role here. An XML-based encoding standard for linguistic corpora XCES (Ide et al., 2000) is based on CES (Corpus Encoding Standard) as part of the E AGLES Guidelines.5 Work on T IGER (Brants and Hansen, 2002) is an example for the liaison of dependency- and constituent-based syntactic annotations."
W07-1505,W06-0606,0,0.0289483,"n of dependency- and constituent-based syntactic annotations. New standardization efforts such as the Syntactic Annotation Framework (S YNAF) (Declerck, 2006) aim to combine different proposals and create standards for syntactic annotation. We also encounter a tendency towards multiple annotations for a single corpus. Major bio-medical corpora, such as GENIA (Ohta et al., 2002) or PennBioIE,6 combine several layers of linguistic information in terms of morpho-syntactic, syntactic and semantic annotations (named entities and events). In the meantime, the Annotation Compatibility Working Group (Meyers, 2006) began to concentrate its activities on the mutual compatibility of annotation schemata for, e.g., POS tagging, treebanking, role labeling, time annotation, etc. The goal of these initiatives, however, has never been to design an annotation scheme for a complete 2 http://www.tei-c.org http://dublincore.org 4 http://www.w3.org/2001/sw 5 http://www.ilc.cnr.it/EAGLES96/ 6 http://bioie.ldc.upenn.edu 3 34 NLP pipeline as needed, e.g., for information extraction or text mining tasks (Hahn and Wermter, 2006). This lack is mainly due to missing standards for specifying comprehensive NLP software archi"
W07-1505,P05-1011,0,0.0136284,"ides the attribute parent, Constituent holds the attributes cat which stores the complex syntactic category of the current constituent (e.g., NP, VP), and head which links to the head word of the constituent. In order to account for multiple annotations in the constituent-based approach, we introduced corresponding constituent types which specialize Constituent. This parallels our approach which we advocate for alternatives in POS tagging and the management of alternative chunking results. Currently, the scheme supports three different constituent types, viz. PTBConstituent, GENIAConstituent (Miyao and Tsujii, 2005) and PennBIoIEConstituent. The attributes of the type PTBConstituent cover the complete repertoire of annotation items contained in the Penn Treebank, such as functional tags for form/function dicrepancies (formFuncDisc), grammatical role (gramRole), adverbials (adv) and miscellaneous tags (misc). The representation of null elements, topicalized elements and gaps with corresponding references to the lexicalized elements in a tree is reflected in attributes nullElement, tpc, map and ref, respectively. GENIAConstituent and PennBIoIEConstituent inherit from PTBConstituent all listed attributes an"
W07-1505,W06-2713,0,0.0542362,"Missing"
W07-1505,W06-2714,0,0.130035,"Missing"
W07-1505,doddington-etal-2004-automatic,0,\N,Missing
W07-1505,laprun-etal-2002-pratical,0,\N,Missing
W08-0605,P96-1042,0,0.0596406,"he scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies h"
W08-0605,W04-1213,0,0.0305898,"Missing"
W08-0605,W04-3111,0,0.0196017,"naniadou1,3 1 School of Computer Science, The University of Manchester, UK 2 Department of Computer Science, The University of Tokyo, Japan 3 National Centre for Text Mining (NaCTeM), Manchester, UK yoshimasa.tsuruoka@manchester.ac.uk tsujii@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk Abstract 1 Introduction However, the lack of annotated corpora, which are indispensable for training machine learning models, makes it difficult to broaden the scope of text mining applications. In the biomedical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a mac"
W08-0605,P06-1059,1,0.831588,"been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper presents an active learning"
W08-0605,W04-1221,0,0.0116076,"e learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). The problem with active learning is, however, that the resulting annotated data is highly dependent on the machine learning algorithm and the sampling strategy employed, because active learning annotates only a subset of the given corpus. This sampling bias is not a serious problem if one is to use the annotated corpus only for their own machine learning purpose and with the same machine learning algorithm. However, the existence of bias is not desirable if one also wants the corpus to be used by other applications or researchers. For the same reason, acThis paper pr"
W08-0605,P04-1075,0,0.0346889,"edical domain, for example, several annotated corpora such as GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004), and GENETAG (Tanabe et al., 2005) have been created and made publicly available, but the named entity categories annotated in these corpora are tailored to their specific needs and not always sufficient or suitable for text mining tasks that other researchers need to address. Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). In active learning, samples that need to be annotated by the human annotator are picked up by a machine learning model in an iterative and interactive manner, considering the informativeness of the samples. Active learning has been shown to be effective in several natural language processing tasks including named entity recognition. Named entities play a central role in conveying important domain specific information in text, and good named entity recognizers are often required in building practical information extraction systems. Previous studies have shown that automatic named entity recog"
W08-0605,W03-0419,0,0.0384343,"Missing"
W08-0605,D07-1051,0,0.0159407,"l dictionaries or using more sophisticated probabilistic models such as semi-Markov CRFs (Sarawagi and Cohen, 2004). These enhancements should further improve the coverage, keeping 36 the same degree of cost reduction. The idea of improving the efficiency of annotation work by using automatic taggers is certainly not new. Tanabe et al. (2005) applied a gene/protein name tagger to the target sentences and modified the results manually. Culotta and McCallum (2005) proposed to have the human annotator select the correct annotation from multiple choices produced by a CRF tagger for each sentence. Tomanek et al. (2007) discuss the reusability of named entityannotated corpora created by an active learning approach and show that it is possible to build a corpus that is useful to different machine learning algorithms to a certain degree. The limitation of our framework is that it is useful only when the target named entities are sparse because the upper bound of cost saving is limited by the proportion of the relevant sentences in the corpus. Our framework may therefore not be suitable for a situation where one wants to make annotations for named entities of many categories simultaneously (e.g. creating a corp"
W08-0605,W06-3328,0,0.0242091,"Missing"
W08-0609,C00-1030,0,0.0752154,"Missing"
W08-0609,W04-1217,0,0.0158855,"Missing"
W08-0609,W02-0301,0,0.0688202,"Missing"
W08-0609,W04-1213,0,0.271624,"Missing"
W08-0609,W04-3230,0,0.0122048,"ph (i.e., trellis) according to the word order. Estimate the score of every path using the weights of node and edges estimated by training using Conditional Random Fields. Select the best path. Figure 3 shows an example of our dictionarybased approach. Suppose that the input is “IL2-mediated activation”. A trellis is created based on the lexical entries in a dictionary. The selection criteria for the best path are determined by the CRF tagging model trained on the Genia corpus. In this example, IL-2/NN-PROTEIN -/- mediated/VVN activation/NN is selected as the best path. Following Kudo et al. (Kudo et al., 2004), we adapted the core engine of the CRF-based morphological analyzer, MeCab1 , to our POS/PROTEIN tagging task. MeCab’s dictionary databases employ double arrays (Aoe, 1989) which enable efficient lexical look-ups. The features used were: 2.2.2 Dictionary construction A dictionary-based approach requires the dictionary to cover not only a wide variety of biomedical terms but also entries with: • all possible capitalization • all possible linguistic inflections We constructed a freely available, wide-coverage English word dictionary that satisfies these conditions. We did consider the MedPost p"
W08-0609,P06-1059,1,0.86642,"Missing"
W08-0609,W04-1218,0,0.0519101,"Missing"
W08-0609,W04-1221,0,0.0201854,"Missing"
W08-0609,W04-1220,0,0.0327216,"Missing"
W08-0609,W03-1309,0,0.0372735,"Missing"
W08-0609,W04-1219,0,0.013421,"Missing"
W08-0609,W04-1214,0,\N,Missing
W08-0609,E99-1023,0,\N,Missing
W08-0609,W03-1305,0,\N,Missing
W11-0328,P04-1015,0,0.0634212,"scores on test data). clearly demonstrate that the lookahead search boosts parsing accuracy. As expected, training and test speed decreases, almost by a factor of three, which is the branching factor of the dependency parser. The table also lists accuracy figures reported in the literature on shift-reduce dependency parsing. Most of the latest studies on shift-reduce dependency parsing employ dynamic programing or beam search, which implies that deterministic methods were not as competitive as those methods. It should also be noted that all of the listed studies learn structured perceptrons (Collins and Roark, 2004), while our parser learns locally optimized perceptrons. In this table, our parser without lookahead search (i.e. depth = 0) resulted in significantly lower accuracy than the previous studies. In fact, it is worse than the deterministic parser of Huang et al. (2009), which uses (almost) the same set of features. This is presumably due to the difference between locally optimized perceptrons and globally optimized structured perceptrons. However, our parser with lookahead search is significantly better than their deterministic parser, and its accuracy is close to the levels of the parsers with b"
W11-0328,W02-1001,0,0.356736,"3 in Figure 3). The only difference between our algorithm and the standard algorithm for margin perceptrons is that we use the states and their scores obtained from lookahead searches (Line 11 in Figure 3), which are backed up from the leaves of the search trees. In Appendix A, we provide a proof of the convergence of our training algorithm and show that the margin will approach at least half the true margin (assuming that the training data are linearly separable). As in many studies using perceptrons, we average the weight vector over the whole training iterations at the end of the training (Collins, 2002). 4 Experiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presente"
W11-0328,N10-1115,0,0.0164406,"search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive as globally optim"
W11-0328,P10-1110,0,0.0167726,"subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & SGD training) No lookahead (depth = 0) Lookahead (depth = 1) Lookahead (depth = 2) No lookahead (depth = 0) + tag trigram features Lookahead (depth = 1) + tag trigram features Lookahead (depth = 2) + tag trigram features Structured perceptron (Collins, 2002) Guided learning (Shen et al.,"
W11-0328,D09-1127,0,0.0172615,"Missing"
W11-0328,W04-1213,0,0.0354731,"Missing"
W11-0328,N01-1025,0,0.0492284,"l., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007)."
W11-0328,P10-1052,0,0.0421723,"Missing"
W11-0328,W04-2407,0,0.00845323,"ed on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word “lookahead” differen"
W11-0328,W04-0308,0,0.0142111,"ead rules of Yamada and Matsumoto (2003).3 The data is split into training (section 02-21), development (section 22), and test (section 23) sets. The parsing accuracy was evaluated with auto-POS data, i.e., we used our lookahead POS tagger (depth = 2) presented in the previous subsection to assign the POS tags for the development and test data. Unlabeled attachment scores for all words excluding punctuations are reported. The development set is used for tuning the meta parameters, while the test set is used for evaluating the final accuracy. The parsing algorithm is the “arc-standard” method (Nivre, 2004), which is briefly described in Section 2. With this algorithm, state S corresponds to a parser configuration, i.e., the stack and the queue, and action a corresponds to shift, reduceL , and reduceR . In this experiment, we use the same set of feature templates as Huang and Sagae (2010). Table 4 shows training time, test time, and parsing accuracy. In this table, “No lookahead (depth = 0)” corresponds to a conventional shift-reduce parsing method without any lookahead search. The results 3 Penn2Malt is applied for this conversion, while dependency labels are removed. CRF (L1 regularization & S"
W11-0328,P06-1059,1,0.636231,"Missing"
W11-0328,W96-0213,0,0.382157,"and show its convergence properties. The proposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking pr"
W11-0328,P07-1096,0,0.0216958,"e experimental results are shown in Table 1. Note that the models in the top four rows use exactly the same feature set. It is clearly seen that the lookahead improves tagging accuracy, and our historybased models with lookahead is as accurate as the CRF model. We also created another set of models by simply adding tag trigram features, which cannot be employed by first-order CRF models. These features have slightly improved the tagging accuracy, and the final accuracy achieved by a search depth of 3 was comparable to some of the best results achieved by pure supervised learning in this task (Shen et al., 2007; Lavergne et al., 2010). The second set of experiments is about chunking. We used the data set for the CoNLL 2000 shared task, which contains 8,936 sentences where each token is annotated with the “IOB” tags representing text chunks. The experimental results are shown in Table 2. Again, our history-based models with lookahead were slightly more accurate than the CRF model using exactly the same set of features. The accuracy achieved by the lookahead model with a search depth of 2 was comparable to the accuracy achieved by a computationally heavy combination of max-margin classifiers (Kudo and"
W11-0328,H05-1059,1,0.67598,"ch strategies such as beam search. In that case, the search queue is not necessarily truncated. 244 (Tesauro, 2001; Hoki, 2006) in that the parameters are optimized based on the differences of the feature vectors realized by the correct and incorrect actions. In history-based models, the order of actions is often very important. For example, backward tagging is considerably more accurate than forward tagging in biomedical named entity recognition. Our lookahead method is orthogonal to more elaborate techniques for determining the order of actions such as easy-first tagging/parsing strategies (Tsuruoka and Tsujii, 2005; Elhadad, 2010). We expect that incorporating such elaborate techniques in our framework will lead to improved accuracy, but we leave it for future work. 6 Conclusion We have presented a simple and general framework for incorporating a lookahead process in historybased models and a perceptron-based training algorithm for the framework. We have conducted experiments using standard data sets for POS tagging, chunking, named entity recognition and dependency parsing, and obtained very promising results—the accuracy achieved by the history-based models enhanced with lookahead was as competitive a"
W11-0328,P09-1054,1,0.841931,"xperiments This section presents four sets of experimental results to show how the lookahead process improves the accuracy of history-based models in common NLP tasks. 4.1 Sequence prediction tasks First, we evaluate our framework with three sequence prediction tasks: POS tagging, chunking, and named entity recognition. We compare our method with the CRF model, which is one of the de facto standard machine learning models for such sequence prediction tasks. We trained L1-regularized first-order CRF models using the efficient stochastic gradient descent (SGD)-based training method presented in Tsuruoka et al. (2009). Since our main interest is not in achieving the state-of-the-art results for those tasks, we did not conduct feature engineering to come up with elaborate features—we simply adopted the feature sets described in their paper (with an exception being tag trigram features tested in the POS tagging experiments). The experiments for these sequence prediction tasks were carried out using one core of a 3.33GHz Intel Xeon W5590 processor. The first set of experiments is about POS tagging. The training and test data were created from the Wall Street Journal corpus of the Penn Treebank (Marcus et al.,"
W11-0328,W03-3023,0,0.114016,"roposed framework is evaluated on partof-speech tagging, chunking, named entity recognition and dependency parsing, using standard data sets and features. Experimental results demonstrate that history-based models with lookahead are as competitive as globally optimized models including conditional random fields (CRFs) and structured perceptrons. 1 Introduction History-based models have been a popular approach in a variety of natural language processing (NLP) tasks including part-of-speech (POS) tagging, named entity recognition, and syntactic parsing (Ratnaparkhi, 1996; McCallum et al., 2000; Yamada and Matsumoto, 2003; Nivre et al., 2004). The idea is to decompose the complex structured prediction problem into a series of simple classification problems and use a machine learning-based classifier to make each decision using the information about the past decisions and partially completed structures as features. In this paper, we argue that history-based models are not something that should be left behind in research history, by demonstrating that their accuracy can be significantly improved by incorporating a lookahead mechanism into their decisionmaking process. It should be emphasized that we use the word"
W11-0328,W07-1033,0,0.0609087,"(Kudo and Matsumoto, 2001). We also tested the effectiveness of additional features of tag trigrams using the development data, but there was no improvement in the accuracy. The third set of experiments is about named entity recognition. We used the data provided for the BioNLP/NLPBA 2004 shared task (Kim et al., 242 2004), which contains 18,546 sentences where each token is annotated with the “IOB” tags representing biomedical named entities. We performed the tagging in the right-to-left fashion because it is known that backward tagging is more accurate than forward tagging on this data set (Yoshida and Tsujii, 2007). Table 3 shows the experimental results, together with some previous performance reports achieved by pure machine leaning methods (i.e. without rulebased post processing or external resources such as gazetteers). Our history-based model with no lookahead was considerably worse than the CRF model using the same set of features, but it was significantly improved by the introduction of lookahead and resulted in accuracy figures better than that of the CRF model. 4.2 Dependency parsing We also evaluate our method in dependency parsing. We follow the most standard experimental setting for English"
W11-0328,D08-1059,0,0.029985,"Missing"
W11-0328,J93-2004,0,\N,Missing
W11-1814,H05-1013,0,0.0543187,"Missing"
W11-1814,N01-1008,0,0.0877422,"Missing"
W11-1814,P08-1068,0,0.0176791,"from other external resources. On top of the lexical and syntactic features, we use two additional types of information, which are expected to alleviate the data sparseness problem. In summary, we use four types of features including lexical and syntactic features, word cluster and word sense features as the input for the CRF model. 2.1 Word cluster features The idea of enhancing a supervised learning model with word cluster information is not new. Kamaza et. al. (2001) use a hidden Markov model (HMM) to produce word cluster features for their maximum entropy model for part-of-speech tagging. Koo et al. (2008) implement the Brown clustering algorithm to produce additional features for their dependency parser. For our NER task, we use an HMM to produce word cluster features for our CRF model. We employed an open source library2 for learning HMMs with the online Expectation Maximization (EM) algorithm proposed by Liang and Klein (2009). The online EM algorithm is much more efficient than the standard batch EM algorithm and allows us to use a large amount of data. For each hidden state, words that are produced by this state with the highest probability are written. We use this result of word clusterin"
W11-1814,N09-1069,0,0.0190518,"RF model. 2.1 Word cluster features The idea of enhancing a supervised learning model with word cluster information is not new. Kamaza et. al. (2001) use a hidden Markov model (HMM) to produce word cluster features for their maximum entropy model for part-of-speech tagging. Koo et al. (2008) implement the Brown clustering algorithm to produce additional features for their dependency parser. For our NER task, we use an HMM to produce word cluster features for our CRF model. We employed an open source library2 for learning HMMs with the online Expectation Maximization (EM) algorithm proposed by Liang and Klein (2009). The online EM algorithm is much more efficient than the standard batch EM algorithm and allows us to use a large amount of data. For each hidden state, words that are produced by this state with the highest probability are written. We use this result of word clustering as a feature for NER. The optimal number of hidden states is selected by evaluating its effectiveness on NER using the development set. To prepare the raw text for HMM clustering, we downloaded 686 documents (consisting of both full documents and abstracts) about bacteria biotopes 2 http://www-tsujii.is.s.u-tokyo.ac.jp/ hillbi"
W11-1814,P04-1018,0,0.0621172,"Missing"
W11-1814,J05-3004,0,0.0133549,"nd thus it may span over several sentences (or even paragraphs). This observation motivated us to perform coreference resolution as a pre-processing step, so that each event can be recognized within a narrower textual scope. There are two common approaches to coreference resolution: one mainly relies on heuristics, and the other employs machine learning. Some 94 Proceedings of BioNLP Shared Task 2011 Workshop, pages 94–101, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics instances of the heuristics-based approach are described in (Harabagiu et al., 2001; Markert and Nissim, 2005; Yang and Su, 2007), where they use lexical and encyclopedic knowledge. Machine learning-based methods (Soon and Ng, 2001; Ng and Cardie, 2002; Yang et al. , 2003; Luo et al. , 2004; Daume and Marcu, 2005) train a classifier or search model using a corpus annotated with anaphoric pairs. In our system, we employ the simple supervised method presented in Soon and Ng (2001). To create the training data, we have manually annotated the corpus with coreference information about bacteria. Our approach, consequently, has three processes: NER, coreference resolution of bacterium entities, and event ex"
W11-1814,P08-2026,0,0.0336016,"Missing"
W11-1814,P02-1014,0,0.119926,"step, so that each event can be recognized within a narrower textual scope. There are two common approaches to coreference resolution: one mainly relies on heuristics, and the other employs machine learning. Some 94 Proceedings of BioNLP Shared Task 2011 Workshop, pages 94–101, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics instances of the heuristics-based approach are described in (Harabagiu et al., 2001; Markert and Nissim, 2005; Yang and Su, 2007), where they use lexical and encyclopedic knowledge. Machine learning-based methods (Soon and Ng, 2001; Ng and Cardie, 2002; Yang et al. , 2003; Luo et al. , 2004; Daume and Marcu, 2005) train a classifier or search model using a corpus annotated with anaphoric pairs. In our system, we employ the simple supervised method presented in Soon and Ng (2001). To create the training data, we have manually annotated the corpus with coreference information about bacteria. Our approach, consequently, has three processes: NER, coreference resolution of bacterium entities, and event extraction. The latter two processes can be formulated as classification problems. Coreference resolution is to determine the relation between ca"
W11-1814,J01-4004,0,0.78177,"as a pre-processing step, so that each event can be recognized within a narrower textual scope. There are two common approaches to coreference resolution: one mainly relies on heuristics, and the other employs machine learning. Some 94 Proceedings of BioNLP Shared Task 2011 Workshop, pages 94–101, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics instances of the heuristics-based approach are described in (Harabagiu et al., 2001; Markert and Nissim, 2005; Yang and Su, 2007), where they use lexical and encyclopedic knowledge. Machine learning-based methods (Soon and Ng, 2001; Ng and Cardie, 2002; Yang et al. , 2003; Luo et al. , 2004; Daume and Marcu, 2005) train a classifier or search model using a corpus annotated with anaphoric pairs. In our system, we employ the simple supervised method presented in Soon and Ng (2001). To create the training data, we have manually annotated the corpus with coreference information about bacteria. Our approach, consequently, has three processes: NER, coreference resolution of bacterium entities, and event extraction. The latter two processes can be formulated as classification problems. Coreference resolution is to determine th"
W11-1814,W11-1816,0,0.0791198,"Missing"
W11-1814,P09-1054,1,0.779951,"ask consists of detecting the phrases that denote bacterial taxon names and localizations which are broken into eight types: Host, HostPart, Geographical, Food, Water, Soil, Medical and Environment. In this work, we use a CRF model to perform NER. CFRs (Lafferty et. al., 2001) are a sequence model95 ing framework that not only has all the advantages of MEMMs but also solves the label bias problem in a principled way. This model is suitable for labeling sequence data, especially for NER. Based on this model, our CRF tagger is trained with a stochastic gradient descent-based method described in Tsuruoka et al. (2009), which can produce a compact and accurate model. Due to the small size of the training corpus and the complexity of their category, the entities cannot be easily recognized by standard supervised learning. Therefore, we enhance our learning model by incorporating related information from other external resources. On top of the lexical and syntactic features, we use two additional types of information, which are expected to alleviate the data sparseness problem. In summary, we use four types of features including lexical and syntactic features, word cluster and word sense features as the input"
W11-1814,P03-1023,0,0.0632721,"Missing"
W11-1814,P07-1067,0,0.0130846,"everal sentences (or even paragraphs). This observation motivated us to perform coreference resolution as a pre-processing step, so that each event can be recognized within a narrower textual scope. There are two common approaches to coreference resolution: one mainly relies on heuristics, and the other employs machine learning. Some 94 Proceedings of BioNLP Shared Task 2011 Workshop, pages 94–101, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics instances of the heuristics-based approach are described in (Harabagiu et al., 2001; Markert and Nissim, 2005; Yang and Su, 2007), where they use lexical and encyclopedic knowledge. Machine learning-based methods (Soon and Ng, 2001; Ng and Cardie, 2002; Yang et al. , 2003; Luo et al. , 2004; Daume and Marcu, 2005) train a classifier or search model using a corpus annotated with anaphoric pairs. In our system, we employ the simple supervised method presented in Soon and Ng (2001). To create the training data, we have manually annotated the corpus with coreference information about bacteria. Our approach, consequently, has three processes: NER, coreference resolution of bacterium entities, and event extraction. The latter"
W11-1814,I05-2038,0,\N,Missing
W11-1814,W11-1809,0,\N,Missing
W14-3702,P02-1006,0,0.0270783,"he local approach. Our system outperformed the state-of-the-art system that utilizes global information and achieved about 1.4 percentage points higher accuracy. 1 Introduction Temporal relationships between entities, namely temporal expressions and events, are regarded as important information for deep understanding of documents. Being able to predict temporal relations between events and temporal expressions within a piece of text can support various NLP applications such as textual entailment (Bos et al., 2005), multi-document summarization (Bollegala et al., 2010), and question answering (Ravichandran and Hovy, 2002). Temporal relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 re6 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6–14, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: An example from the Timebank corpus temporal relation types. Following TempEval-3, all possible TLINKs are between: in our work, the full set of temporal relations specified in TimeML are used, rather t"
W14-3702,C08-1108,0,0.0179066,"connections between entities in a timegraph by following a set of inference rules. For example, if e1 happens AFTER e2 and e2 happens IMMEDIATELY AFTER e3, then we infer a new temporal relation “e1 happens AFTER e3”. In this paper, we add a new connection only when the inference gives only one type of temporal relation as a result from the relation inference. Figure 7b shows the timegraph after adding new inference relations to the original timegraph in Figure 7a. 4.2 (b) After relation inference. Two relations (e1-e2, e1-e3) are added. Time-time connection As with Chambers et al. (2007) and Tatu and Srikanth (2008), we also create new connections between time entities in a timegraph by applying some rules to normalized values of time entities provided in the corpus. after Figure 7c shows the timegraph after adding a time-time link and new inference relations to the original timegraph in Figure 7a. When the normalized value of t2 is more than the value of t1, a TLINK with the relation type AFTER is added between them. After that, as introduced in Subsection 4.2, new inference relations (e1-e2, e1-e3, e2-e3) are added. after (c) After time-time connection (t1-t2) and relation inference. Three relations (e"
W14-3702,P11-2061,0,0.013869,"y significant** (p < 10−5 , McNemar’s test, two-tailed) when applying deep syntactic information to the system. The overall result has about 1.4 pp higher accuracy than the result from their global model. Note that Yoshikawa et al. (2009) did not apply deep syntactic features in their system. The performance analysis is performed based on 10-fold cross validation over the training data. The classification F1 score improves by 0.18 pp and 0.16 pp compared to the local pairwise models with/without deep syntactic features. We evaluated the system using a graph-based evaluation metric proposed by UzZaman and Allen (2011). Table 5 shows the classification accuracy over the training set using graph-based evaluation. The stacked model affected the relation classification output of the local model, changing the relation types of 390 (out of 2520) E-E TLINKs and 169 (out of 2463) E-T TLINKs. 5.2 Comparison with the state of the art We compared our system to that of Yoshikawa et al. (2009) which uses global information to 12 Approach Yoshikawa et al. (2009) (local) Yoshikawa et al. (2009) (global) Our system (local) - baseline features Our system (local) - baseline + deep features Our system (stacked) - baseline fe"
W14-3702,S13-2001,0,0.0129706,"nd achieved about 1.4 percentage points higher accuracy. 1 Introduction Temporal relationships between entities, namely temporal expressions and events, are regarded as important information for deep understanding of documents. Being able to predict temporal relations between events and temporal expressions within a piece of text can support various NLP applications such as textual entailment (Bos et al., 2005), multi-document summarization (Bollegala et al., 2010), and question answering (Ravichandran and Hovy, 2002). Temporal relation classification, which is one of the subtasks TempEval-3 (UzZaman et al., 2013), aims to classify temporal relationships between pairs of temporal entities into one of the 14 re6 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 6–14, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: An example from the Timebank corpus temporal relation types. Following TempEval-3, all possible TLINKs are between: in our work, the full set of temporal relations specified in TimeML are used, rather than the reduced set used in the previous work. We evaluate our method on the TempEval-3’s Task C-r"
W14-3702,H05-1079,0,0.113493,"Missing"
W14-3702,S07-1014,0,0.0425913,"Missing"
W14-3702,P07-2044,0,0.184215,"be found in (Pustejovsky et al., 2005). X X X X All attributes associated with temporal expressions. The explanation of each attribute can be found in (Pustejovsky et al., 2005). X X X X X X Words, POS, lemmas within a window before/after event words extracted using Stanford coreNLP (Stanford NLP Group, 2012) X X X WordNet lexical database (Fellbaum, 1998) X X X X X X X X True if both temporal entities are in the same sentence X X X X Deep syntactic information extracted from Enju Parser (Miyao and Tsujii, 2008). The details are described in (Laokulrat et al., 2013) Details are described in (Chambers et al., 2007) Table 1: Local features Feature Adjacent nodes and links Other paths Generalized paths (E,V,E) tuples (V,E,V) tuples E-E X X X X X E-T X X X X X Description The details are described in Subsection 3.2 Table 2: Timegraph features 8 Figure 5: Local pairwise classification. Each TLINK is classified separately. Figure 2: path length ≤ 2 Figure 3: path length ≤ 3 3 Figure 6: Timegraph constructed from a document’s TLINKs Proposed method Rather than using only local information on two entities in a TLINK, our goal is to exploit more global information which can be extracted from a document’s timegr"
W14-3702,D08-1073,0,0.0449211,"Missing"
W14-3702,P06-1095,0,0.0708157,"Missing"
W14-3702,J08-1002,0,0.0147993,"cription X X X X X X X X X X All attributes associated with events. The explanation of each attribute can be found in (Pustejovsky et al., 2005). X X X X All attributes associated with temporal expressions. The explanation of each attribute can be found in (Pustejovsky et al., 2005). X X X X X X Words, POS, lemmas within a window before/after event words extracted using Stanford coreNLP (Stanford NLP Group, 2012) X X X WordNet lexical database (Fellbaum, 1998) X X X X X X X X True if both temporal entities are in the same sentence X X X X Deep syntactic information extracted from Enju Parser (Miyao and Tsujii, 2008). The details are described in (Laokulrat et al., 2013) Details are described in (Chambers et al., 2007) Table 1: Local features Feature Adjacent nodes and links Other paths Generalized paths (E,V,E) tuples (V,E,V) tuples E-E X X X X X E-T X X X X X Description The details are described in Subsection 3.2 Table 2: Timegraph features 8 Figure 5: Local pairwise classification. Each TLINK is classified separately. Figure 2: path length ≤ 2 Figure 3: path length ≤ 3 3 Figure 6: Timegraph constructed from a document’s TLINKs Proposed method Rather than using only local information on two entities in"
W14-3702,P09-1046,0,\N,Missing
W14-3702,S13-2015,1,\N,Missing
W15-2710,J15-2003,0,0.0229363,"ge processing technology, yet there are still many unsolved problems when the machine has to deal with the meaning of a document. Let us consider the following simple question-answering problem. • Document: 2 Related Work David left Paris on the 20th of July, driving his favorite Peugeot. He arrived in Athens on the 22nd. There is an increasing body of research on using world knowledge and inference in high-level text processing tasks such as textual entailment, coreference resolution and question answering (Tatu and Moldovan, 2005; Fowler et al., 2005; Rahman and Ng, 2011; Peng et al., 2015; Berant et al., 2015). However, most of the existing approaches use “static” knowledge that is typically expressed as a collection of n-ary relations between entities, and there is little work that attempts to model the dynamics of a world. • Question: Where was David on the 21st? A. London B. Budapest C. Berlin D. New York A possible answer to this question would be “He was probably in Budapest, although there is a small chance that he was in Berlin”. Putting aside the problem of natural language generation, the machine would have to have geographical knowledge and perform some kind of inference about his 83 Proc"
W15-2710,P08-1090,0,0.0326614,"ld model for toy blocks. More recent research efforts for connecting language with physical world include Logical Semantics with Perception (Krishnamurthy and Kollar, 2013), referential grounding (Liu et al., 2014), 3D scene generation from text (Chang et al., 2014) and generation of QA tasks by simulation (Weston et al., 2015). Our work can be seen as an attempt of grounding textual descriptions in history text to a simulation model for world history. Our work is also related to previous work on representing structured sequences of actions and events using scripts (Schank and Abelson, 1977). Chambers and Jurafsky (2008) proposed a narrative chain model based on scripts. They focused on a particular character, extracted chains of events on his behavior using verbs and their arguments, and sorted them by learning. 3.2 Alexander’s Expeditions In this work, we create a world model for interpreting documents on Alexander the Great, who was a famous king of ancient Macedonia. Figure 1 shows the graph that we have manually created from a map using frequent location names in Wikipedia. It shows the 35 locations names used in our experiments. Note that this graph is a very crude approximation to the real geographical"
W15-2710,Q13-1016,0,0.0344226,"ve as the constraints in finding a possible episode. Note that, in general, there are many episodes that satisfy the constraints, because documents rarely provide the full detail of the movement history of an agent. Once we obtain those episodes, we can use them to resolve questions about the location of the agent at any particular time. Our work is much closer in spirit to SHRDLU (Winograd, 1971), where natural language queries were processed using a world model for toy blocks. More recent research efforts for connecting language with physical world include Logical Semantics with Perception (Krishnamurthy and Kollar, 2013), referential grounding (Liu et al., 2014), 3D scene generation from text (Chang et al., 2014) and generation of QA tasks by simulation (Weston et al., 2015). Our work can be seen as an attempt of grounding textual descriptions in history text to a simulation model for world history. Our work is also related to previous work on representing structured sequences of actions and events using scripts (Schank and Abelson, 1977). Chambers and Jurafsky (2008) proposed a narrative chain model based on scripts. They focused on a particular character, extracted chains of events on his behavior using ver"
W15-2710,P14-2003,0,0.0142513,"e that, in general, there are many episodes that satisfy the constraints, because documents rarely provide the full detail of the movement history of an agent. Once we obtain those episodes, we can use them to resolve questions about the location of the agent at any particular time. Our work is much closer in spirit to SHRDLU (Winograd, 1971), where natural language queries were processed using a world model for toy blocks. More recent research efforts for connecting language with physical world include Logical Semantics with Perception (Krishnamurthy and Kollar, 2013), referential grounding (Liu et al., 2014), 3D scene generation from text (Chang et al., 2014) and generation of QA tasks by simulation (Weston et al., 2015). Our work can be seen as an attempt of grounding textual descriptions in history text to a simulation model for world history. Our work is also related to previous work on representing structured sequences of actions and events using scripts (Schank and Abelson, 1977). Chambers and Jurafsky (2008) proposed a narrative chain model based on scripts. They focused on a particular character, extracted chains of events on his behavior using verbs and their arguments, and sorted them by"
W15-2710,N15-1082,0,0.0236453,"Missing"
W15-2710,P11-1082,0,0.0326438,"ssed great strides in data-driven language processing technology, yet there are still many unsolved problems when the machine has to deal with the meaning of a document. Let us consider the following simple question-answering problem. • Document: 2 Related Work David left Paris on the 20th of July, driving his favorite Peugeot. He arrived in Athens on the 22nd. There is an increasing body of research on using world knowledge and inference in high-level text processing tasks such as textual entailment, coreference resolution and question answering (Tatu and Moldovan, 2005; Fowler et al., 2005; Rahman and Ng, 2011; Peng et al., 2015; Berant et al., 2015). However, most of the existing approaches use “static” knowledge that is typically expressed as a collection of n-ary relations between entities, and there is little work that attempts to model the dynamics of a world. • Question: Where was David on the 21st? A. London B. Budapest C. Berlin D. New York A possible answer to this question would be “He was probably in Budapest, although there is a small chance that he was in Berlin”. Putting aside the problem of natural language generation, the machine would have to have geographical knowledge and perform"
W15-2710,H05-1047,0,0.0233737,"blem. 1 Introduction Recent decades have witnessed great strides in data-driven language processing technology, yet there are still many unsolved problems when the machine has to deal with the meaning of a document. Let us consider the following simple question-answering problem. • Document: 2 Related Work David left Paris on the 20th of July, driving his favorite Peugeot. He arrived in Athens on the 22nd. There is an increasing body of research on using world knowledge and inference in high-level text processing tasks such as textual entailment, coreference resolution and question answering (Tatu and Moldovan, 2005; Fowler et al., 2005; Rahman and Ng, 2011; Peng et al., 2015; Berant et al., 2015). However, most of the existing approaches use “static” knowledge that is typically expressed as a collection of n-ary relations between entities, and there is little work that attempts to model the dynamics of a world. • Question: Where was David on the 21st? A. London B. Budapest C. Berlin D. New York A possible answer to this question would be “He was probably in Budapest, although there is a small chance that he was in Berlin”. Putting aside the problem of natural language generation, the machine would have"
W15-2710,W14-3102,0,\N,Missing
W15-2710,D14-1217,0,\N,Missing
W15-4001,P10-1046,0,0.0173299,"date values for d since PAS-CLBLM is computationally less expensive than our method. We thus evaluated PAS-CLBLM also on the plausibility judgment task. Concretely, for each type of predicateargument tuples (i, j, k) in the development data, 4.1.1 Evaluation Settings We evaluated the learned embeddings of transitive verbs using a transitive verb disambiguation task and three tasks for measuring the semantic similarity between transitive verb phrases. Each phrase 5 Van de Cruys (2014) reported much higher accuracy in a similar evaluation setting with a neural network model, but as discussed in Chambers and Jurafsky (2010), this is because using the uniform distribution over words for producing implausible tuples leads to optimistic results. 6 We replicated the results reported in their paper using the model parameters publicly provided at http://www.logos.t.u-tokyo.ac.jp/˜hassy/ publications/emnlp2014/. 6 Data Our method SVO SVOPN SVO PASCLBLM SVOPN d 25 50 100 25 50 100 25 50 100 200 25 50 100 200 Milajevs et al. (2014) Hashimoto et al. (2014)6 Polajnar et al. (2014) Dis. GS’11 0.410 0.374 0.373 0.574 0.535 0.508 0.270 0.412 0.390 0.369 0.241 0.281 0.337 0.342 0.456 0.422 0.35 Phrase similarity ML’10 KS’13 KS"
W15-4001,D14-1079,0,0.189704,"es. Thus, we factorize two tensors Tv and Tp for transitive verbs and prepositions, respectively. Tv is factorized into a verb tensor V (corresponding to P), a subject matrix S (corresponding to A1 ), and an object matrix O (corresponding to A2 ). To compute argument embeddings composed by subject-verb-object tuples, we use the copy-subject function in Kartsaklis et al. (2012): 2.2 Relationship to Previous Work Representing transitive verbs with matrices and computing transitive verb phrase embeddings have been proposed by Grefenstette and Sadrzadeh (2011) and others (Kartsaklis et al., 2012; Milajevs et al., 2014; Polajnar et al., 2014). All of them first construct word embeddings by using existing methods and then compute or learn transitive verb matrices. This kind of approach requires one to figure out which word embeddings are suitable for each method or task (Milajevs et al., 2014). By contrast, our method does not require any other word embedding methods and instead jointly learns word embeddings and matrices from scratch, which saves us from the time-consuming process to test which word representations learned by existing methods are suitable for which composition models. Moreover, our method l"
W15-4001,P08-1028,0,0.213525,"dings, the tensor-based methods represent words with tensors which are not limited to vectors. That is, higher order tensors such as matrices and threemode tensors are also used. In the case of representing transitive verb phrases, for example, each transitive verb is represented as a matrix and each noun is represented as a vector in Grefenstette and Sadrzadeh (2011). Based on Coecke et al. (2010), Grefenstette and Sadrzadeh (2011) presented a method for calculating a verb matrix using word embeddings of its observed subjects and objects. The word embeddings were constructed by the method in Mitchell and Lapata (2008). Grefenstette and Sadrzadeh (2011) then introduced composition functions using the verb matrices and the noun embeddings. Their approach has been followed by some recent studies (Kartsaklis et al., 2012; Milajevs et al., 2014; Polajnar et al., 2014; Van de Cruys et al., 2013). In the neural network-based methods each word is usually represented with a vector. Tsubaki et al. (2013) presented a neural network language model focusing on the binary relationship between verbs and their objects. Their co-compositionality method enables verb embeddings to be multiplicatively influenced by the object"
W15-4001,D11-1129,0,0.432025,"Missing"
W15-4001,J08-1002,0,0.207476,"djunct in another sentence, those two sentences tell us that the two phrases “make payment” and “pay money” are semantically similar to each other. We therefore expect such prepositional adjuncts to be useful in learning the meaning of verb phrases. In the disambiguation processes, strong interactions between transitive verbs and their arguments are desirable as with the method in Tsubaki et al. (2013). More specifically, the meaning of “make” changes according to its object “payment” and the meaning of “pay” changes according to its object “money”. We use the probabilistic HPSG parser Enju1 (Miyao and Tsujii, 2008) to identify transitive verbs with their subjects and objects, and as adjuncts, we also extract prepositional phrases with transitive verbs. In the grammar of the Enju parser, each word in a sentence is a predicate with zero or more arguments, i.e., prepositions, too, are treated as predicates. In the example sentence shown above, the transitive verb “make” and the preposition “in” are predicates which take two arguments. Table 1 shows the output from the Enju parser. In this example, the transitive verb “make” takes two arguments: the first argument (the subject) is the noun phrase “an import"
W15-4001,W13-0112,0,0.0199846,"re is a growing interest in learning vectorspace representations of words and phrases using large training corpora in the field of Natural Language Processing (NLP) (Mikolov et al., 2013; Mitchell and Lapata, 2010). The phrase representations are usually computed by composition models that combine the meanings of words into the meanings of phrases. While some studies focus on representing entire phrases or sentences using syntactic structures (Hermann and Blunsom, 2013; Socher et al., 2011), others focus on representing the meaning of transitive verb phrases (Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis et al., 2012). 1 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 1–11, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Predicate make in for learning the embeddings of transitive verb phrases. We assume a three-mode tensor in which the value of each element represents the level of plausibility of a tuple of a predicate and its two arguments (Van de Cruys et al., 2013). We then implicitly factorize the tensor into three latent factors, namely one predicate tensor and two argument matrices."
W15-4001,P06-1128,1,0.581128,"3-7-1 Hongo, Bunkyo-ku, Tokyo, Japan {hassy,tsuruoka}@logos.t.u-tokyo.ac.jp Abstract In this paper, we investigate vector-space representations of transitive verb phrases. The meaning of a transitive verb is often ambiguous and disambiguated by its arguments, i.e., subjects and objects. Investigation of transitive verb phrases should therefore provide insights into how composition models can capture such semantic interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One lim"
W15-4001,D14-1163,1,0.300115,"w composition models can capture such semantic interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One limitation of this approach is that the embeddings of subjectverb-object phrases are computed statically, i.e., the composition process and the embedding (or matrix) construction process are conducted separately. In the neural network-based methods, the embeddings of words and phrases can be learned jointly (Hashimoto et al., 2014). However, the strong interaction between ve"
W15-4001,Y14-1010,0,0.283935,"capture such semantic interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One limitation of this approach is that the embeddings of subjectverb-object phrases are computed statically, i.e., the composition process and the embedding (or matrix) construction process are conducted separately. In the neural network-based methods, the embeddings of words and phrases can be learned jointly (Hashimoto et al., 2014). However, the strong interaction between verbs and their argument"
W15-4001,P13-1088,0,0.0218094,"he-art methods. Our experimental results also show that adjuncts provide useful information in learning the meanings of verb phrases. 1 Introduction There is a growing interest in learning vectorspace representations of words and phrases using large training corpora in the field of Natural Language Processing (NLP) (Mikolov et al., 2013; Mitchell and Lapata, 2010). The phrase representations are usually computed by composition models that combine the meanings of words into the meanings of phrases. While some studies focus on representing entire phrases or sentences using syntactic structures (Hermann and Blunsom, 2013; Socher et al., 2011), others focus on representing the meaning of transitive verb phrases (Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis et al., 2012). 1 Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality (CVSC), pages 1–11, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Predicate make in for learning the embeddings of transitive verb phrases. We assume a three-mode tensor in which the value of each element represents the level of plausibility of a tuple of a predicate and its two argument"
W15-4001,D13-1166,0,0.0683819,"Missing"
W15-4001,C12-2054,0,0.220174,"d objects. Investigation of transitive verb phrases should therefore provide insights into how composition models can capture such semantic interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One limitation of this approach is that the embeddings of subjectverb-object phrases are computed statically, i.e., the composition process and the embedding (or matrix) construction process are conducted separately. In the neural network-based methods, the embeddings of words and phrases"
W15-4001,P13-1045,0,0.0520762,"for learning verb phrase embeddings. show the results of using the SVOPN data. In each setting, we used the enWiki data with d = 50. Table 6 clearly shows the difference between our method and the baseline method. In our method, the meaning of “make” becomes close to those of “earn”, “pay”, and “use” when taking “money”, “payment”, and “use”, respectively, as its object. By contrast, PAS-CLBLM simply emphasizes the head word “make”. In previous work, it is also reported that the weighed addition composition functions put more weight on head words (Hashimoto et al., 2014; Muraoka et al., 2014; Socher et al., 2013). As opposed to these previous methods, our method has the ability of selecting the meaning of transitive verbs according to their objects. Table 6 also shows that the phrase embeddings in our method are influenced by using the adjunct data (i.e., the SVOPN data). For example, in the example of “make money”, the results for using the SVO data include “use money” as the nearest neighbors. When using the SVOPN data, the focus seems to shift to the true meaning of “make money”. Effects of the training corpora. In previous work on learning and evaluating word embeddings, it is generally observed t"
W15-4001,A00-1023,0,0.00801643,"yo.ac.jp Abstract In this paper, we investigate vector-space representations of transitive verb phrases. The meaning of a transitive verb is often ambiguous and disambiguated by its arguments, i.e., subjects and objects. Investigation of transitive verb phrases should therefore provide insights into how composition models can capture such semantic interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One limitation of this approach is that the embeddings of subjectverb-object"
W15-4001,D13-1014,0,0.462358,"interactions between words. Moreover, in practice, capturing the meanings of transitive verb phrases should be useful in many real-world NLP applications such as semantic retrieval (Miyao et al., 2006) and question answering (Who did What to Whom?) (Srihari and Li, 2000). There are several approaches to representing transitive verb phrases in a vector space using large unannotated corpora. One is based on tensor calculus (Grefenstette and Sadrzadeh, 2011; Kartsaklis et al., 2012; Van de Cruys et al., 2013) and another is based on neural networks (Hashimoto et al., 2014; Muraoka et al., 2014; Tsubaki et al., 2013). In the tensor-based methods, transitive verbs are represented as matrices, and they are constructed by using the pre-trained word embeddings of their subjects and objects. One limitation of this approach is that the embeddings of subjectverb-object phrases are computed statically, i.e., the composition process and the embedding (or matrix) construction process are conducted separately. In the neural network-based methods, the embeddings of words and phrases can be learned jointly (Hashimoto et al., 2014). However, the strong interaction between verbs and their arguments is not fully captured"
W15-4001,N13-1134,0,0.107534,"Missing"
W15-4001,D14-1004,0,0.0865205,"Missing"
W16-1629,D16-1046,0,0.0235373,"sed and semi-supervised. Our focus is the supervised setting, where both of the source and target domain datasets are labeled. We would like to use the label information of the source domain to improve the performance on the target domain. Recently, Recurrent Neural Networks (RNNs) have been successfully applied to various tasks in the field of natural language processing (NLP), including language modeling (Mikolov et al., 2010), caption generation (Vinyals et al., 2015b) and parsing (Vinyals et al., 2015a). For neural networks, there are two standard methods for supervised domain adaptation (Mou et al., 2016). The first method is fine tuning: we first train the model with the source dataset and then tune it with the target domain dataset (Venugopalan et al., 2015; Kim, 2014). Since the objective function of neural network training is nonconvex, the performance of the trained model can depend on the initialization of the parameters. This is in contrast with the convex methods such as Support Vector Machines (SVMs). We expect that the first training gives a good initialization of the parameters, and therefore the latter training gives a good generalization even if the target domain dataset is small."
W16-1629,N15-1173,0,0.0367787,"e label information of the source domain to improve the performance on the target domain. Recently, Recurrent Neural Networks (RNNs) have been successfully applied to various tasks in the field of natural language processing (NLP), including language modeling (Mikolov et al., 2010), caption generation (Vinyals et al., 2015b) and parsing (Vinyals et al., 2015a). For neural networks, there are two standard methods for supervised domain adaptation (Mou et al., 2016). The first method is fine tuning: we first train the model with the source dataset and then tune it with the target domain dataset (Venugopalan et al., 2015; Kim, 2014). Since the objective function of neural network training is nonconvex, the performance of the trained model can depend on the initialization of the parameters. This is in contrast with the convex methods such as Support Vector Machines (SVMs). We expect that the first training gives a good initialization of the parameters, and therefore the latter training gives a good generalization even if the target domain dataset is small. The downside of this approach is the lack of the optimization objective. The other method is to design the neural network so that it has two outputs. The fi"
W16-1629,P07-1033,0,0.428321,"Missing"
W16-1629,D14-1181,0,0.00495062,"source domain to improve the performance on the target domain. Recently, Recurrent Neural Networks (RNNs) have been successfully applied to various tasks in the field of natural language processing (NLP), including language modeling (Mikolov et al., 2010), caption generation (Vinyals et al., 2015b) and parsing (Vinyals et al., 2015a). For neural networks, there are two standard methods for supervised domain adaptation (Mou et al., 2016). The first method is fine tuning: we first train the model with the source dataset and then tune it with the target domain dataset (Venugopalan et al., 2015; Kim, 2014). Since the objective function of neural network training is nonconvex, the performance of the trained model can depend on the initialization of the parameters. This is in contrast with the convex methods such as Support Vector Machines (SVMs). We expect that the first training gives a good initialization of the parameters, and therefore the latter training gives a good generalization even if the target domain dataset is small. The downside of this approach is the lack of the optimization objective. The other method is to design the neural network so that it has two outputs. The first output i"
W16-1629,N16-1015,0,0.0256997,"nce of a new (target) domain by using a dataset from the original (source) domain. Suppose that, as the source domain dataset, we have a captioning corpus, consisting of images of daily lives and each image has captions. Suppose also that we would like to generate captions for exotic cuisine, which are rare in the corpus. It is usually very costly to make a new corpus for the target domain, i.e., taking and captioning those 249 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 249–257, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics Wen et al. (2016) have proposed a procedure to generate natural language for multiple domains of spoken dialogue systems. They improve the fine tuning method by pre-trainig with synthesized data. However, the synthesis protocol is only applicable to the spoken dialogue system. In this paper, we focus on domain adaptation methods which can be applied without dataset-specific tricks. Yang et al. (2016) have conducted a series of experiments to investigate the transferability of neural networks for NLP. They compare the performance of two transfer methods called INIT and MULT, which correspond to the fine tuning"
W16-1629,Q14-1006,0,0.040563,"e training. Note that the CIDEr scores correlate with human evaluations better than BLEU and METOR scores (Vedantam et al., 2015). Generated captions for sample images are shown in Table 4. In the first example, A LL fails to identify the chocolate cake because there are birds in the source dataset which somehow look similar to chocolate cake. We argue that P ROPOSED learns birds by the source parameters and chocolate cakes by the target parameters, and thus succeeded in generating appropriate captions. is another captioning dataset, consisting of 30K images, and each image has five captions (Young et al., 2014). Although the formats of the datasets are almost the same, the model trained by the MS COCO dataset does not work well for the Flickr 30K dataset and vice versa. The word distributions of the captions are considerably different. If we ignore words with less than 30 counts, MS COCO has 3,655 words and Flicker30K has 2732 words; and only 1,486 words are shared. Also, the average lengths of captions are different. The average length of captions in Flickr30K is 12.3 while that of MS COCO is 10.5. The first result is the domain adaptation from MS COCO to Flickr30K, summarized in Table 5. Again, we"
W16-4605,P16-1078,1,0.902164,"verify that the attention-based unknown word replacement method is effective in improving translation scores in Chinese-to-Japanese machine translation. We further show results of manual analysis on the replaced unknown words. 1 Introduction End-to-end Neural Machine Translation (NMT) with Recurrent Neural Networks (RNNs) is attracting increasing attention (Sutskever et al., 2014). By incorporating attention mechanisms (Bahdanau et al., 2015), NMT models have achieved state-of-the-art results on several translation tasks, such as Englishto-German (Luong et al., 2015a) and English-to-Japanese (Eriguchi et al., 2016) tasks. Although NMT is attractive due to its translation quality and relatively simple architecture, it is known to have some serious problems including unknown (or rare) word problems (Luong et al., 2015b). Thus, there is still room for improvement in many aspects of NMT models. In our UT-KAY system that participated in the Workshop on Asian Translation 2016 (WAT 2016) (Nakazawa et al., 2016a), we investigate the following two issues: • adaptation with multiple domains, and • attention-based unknown word replacement. Our system is based on an Attention-based NMT (ANMT) model (Luong et al., 2"
W16-4605,2015.iwslt-evaluation.11,0,0.0385015,"s usually built as a single large neural network and trained using a large parallel corpus. Such a parallel corpus is, in general, constructed by collecting sentence pairs from a variety of domains 75 Proceedings of the 3rd Workshop on Asian Translation, pages 75–83, Osaka, Japan, December 11-17 2016. (or topics), such as computer science and biomedicine. Sentences in different domains have different word distributions, and it has been shown that domain adaption is an effective way of improving image captioning models, which perform sentence generation like NMT models (Watanabe et al., 2016). Luong and Manning (2015) proposed pre-training techniques using a large general domain corpus to perform domain adaptation for NMT models. However, both of these approaches assume that there are only two domains, i.e., the source and target domains. In practice, there exist multiple topics, and thus the explicit use of information about multiple domains in the NMT models is worth investigating. 2.2 Unknown Word Replacement in NMT Previous approaches to unknown word problems are roughly categorized into three types: characterbased, subword-based, and copy-based approaches. The character-based methods aim at building w"
W16-4605,P16-1100,0,0.0118793,"form domain adaptation for NMT models. However, both of these approaches assume that there are only two domains, i.e., the source and target domains. In practice, there exist multiple topics, and thus the explicit use of information about multiple domains in the NMT models is worth investigating. 2.2 Unknown Word Replacement in NMT Previous approaches to unknown word problems are roughly categorized into three types: characterbased, subword-based, and copy-based approaches. The character-based methods aim at building word representations for unknown words by using character-level information (Luong and Manning, 2016). The character-based methods can handle any words and has achieved better results than word-based methods. However, the computational cost grows rapidly. Recently, Sennrich et al. (2016) have shown that the use of subword units in NMT models is effective. The subword units can treat multiple levels of granularity existing in words and reduce the size of the vocabulary compared with the standard word-based models. However, the rules to use the subword units are built based on the training data, and thus there still remains the problem of treating an infinite number of the unknown words. The co"
W16-4605,D15-1166,0,0.145124,"n word replacement method. In experiments, we verify that the attention-based unknown word replacement method is effective in improving translation scores in Chinese-to-Japanese machine translation. We further show results of manual analysis on the replaced unknown words. 1 Introduction End-to-end Neural Machine Translation (NMT) with Recurrent Neural Networks (RNNs) is attracting increasing attention (Sutskever et al., 2014). By incorporating attention mechanisms (Bahdanau et al., 2015), NMT models have achieved state-of-the-art results on several translation tasks, such as Englishto-German (Luong et al., 2015a) and English-to-Japanese (Eriguchi et al., 2016) tasks. Although NMT is attractive due to its translation quality and relatively simple architecture, it is known to have some serious problems including unknown (or rare) word problems (Luong et al., 2015b). Thus, there is still room for improvement in many aspects of NMT models. In our UT-KAY system that participated in the Workshop on Asian Translation 2016 (WAT 2016) (Nakazawa et al., 2016a), we investigate the following two issues: • adaptation with multiple domains, and • attention-based unknown word replacement. Our system is based on an"
W16-4605,P15-1002,0,0.188649,"n word replacement method. In experiments, we verify that the attention-based unknown word replacement method is effective in improving translation scores in Chinese-to-Japanese machine translation. We further show results of manual analysis on the replaced unknown words. 1 Introduction End-to-end Neural Machine Translation (NMT) with Recurrent Neural Networks (RNNs) is attracting increasing attention (Sutskever et al., 2014). By incorporating attention mechanisms (Bahdanau et al., 2015), NMT models have achieved state-of-the-art results on several translation tasks, such as Englishto-German (Luong et al., 2015a) and English-to-Japanese (Eriguchi et al., 2016) tasks. Although NMT is attractive due to its translation quality and relatively simple architecture, it is known to have some serious problems including unknown (or rare) word problems (Luong et al., 2015b). Thus, there is still room for improvement in many aspects of NMT models. In our UT-KAY system that participated in the Workshop on Asian Translation 2016 (WAT 2016) (Nakazawa et al., 2016a), we investigate the following two issues: • adaptation with multiple domains, and • attention-based unknown word replacement. Our system is based on an"
W16-4605,L16-1350,0,0.0418869,"Missing"
W16-4605,W15-5003,0,0.022226,"the development data after each epoch.4 To generate the target sentences, we used the beam search strategy based on the statistics of the sentence lengths as in Eriguchi et al. (2016). We set the beam size to 20, and after generating the sentences using the beam search, we applied the attention-based unknown word replacement method to the unknown words output by the system. 5 Results and Discussion 5.1 Main Results Table 1 shows our experimental results in terms of BLEU and RIBES scores for the development and test data. In the table, the results of the best systems at WAT 2015 and WAT 2016, Neubig et al. (2015) and Kyoto-U, are also shown. These results are the Kytea-based evaluation results. First, we can see 1 http://lotus.kuee.kyoto-u.ac.jp/ASPEC/ The categorization was based on personal communication with the organizer of WAT 2016. 3 http://www.phontron.com/kytea/ and http://stanfordnlp.github.io/CoreNLP/. 4 Our system was implemented using our CPU-based neural network library: https://github.com/hassyGo/N3LP. 2 79 Method (1) ANMT (2) ANMT w/ UNK replacement (3) ANMT w/ domain adaptation (4) ANMT w/ domain adaptation and UNK replacement (5) Ensemble of (1) and (3) (6) Ensemble of (1) and (3) w/"
W16-4605,P16-1162,0,0.0456792,"s, and thus the explicit use of information about multiple domains in the NMT models is worth investigating. 2.2 Unknown Word Replacement in NMT Previous approaches to unknown word problems are roughly categorized into three types: characterbased, subword-based, and copy-based approaches. The character-based methods aim at building word representations for unknown words by using character-level information (Luong and Manning, 2016). The character-based methods can handle any words and has achieved better results than word-based methods. However, the computational cost grows rapidly. Recently, Sennrich et al. (2016) have shown that the use of subword units in NMT models is effective. The subword units can treat multiple levels of granularity existing in words and reduce the size of the vocabulary compared with the standard word-based models. However, the rules to use the subword units are built based on the training data, and thus there still remains the problem of treating an infinite number of the unknown words. The copy-based methods aim at copying relevant source words to replace unknown words. Some use existing alignment tools (Luong et al., 2015b), and others suggest that using attention scores in"
W16-4605,W16-1629,1,0.928914,"problems including unknown (or rare) word problems (Luong et al., 2015b). Thus, there is still room for improvement in many aspects of NMT models. In our UT-KAY system that participated in the Workshop on Asian Translation 2016 (WAT 2016) (Nakazawa et al., 2016a), we investigate the following two issues: • adaptation with multiple domains, and • attention-based unknown word replacement. Our system is based on an Attention-based NMT (ANMT) model (Luong et al., 2015a). To explicitly treat translation pairs from multiple domains, our system extends a domain adaptation method for neural networks (Watanabe et al., 2016), and apply it to the baseline ANMT model. To address the unknown word problems in translated sentences, we investigate the effectiveness of replacing each unknown word according to the attention scores output by the baseline ANMT model. In experiments, we apply our system to a Chinese-to-Japanese translation task of scientific text. Our experimental results show that the attention-based unknown word replacement method consistently improves the BLEU scores by about 1.0 for the baseline system, the domain adaptation system, and the ensemble of the two systems. Moreover, our manual analysis on t"
W16-4605,W15-3014,0,\N,Missing
W16-4617,W14-4012,0,0.251986,"Missing"
W16-4617,D14-1179,0,0.0645761,"Missing"
W16-4617,P16-1160,0,0.0772978,"ing (BPE) method to create a sub-word level vocabulary according to the frequencies of sub-word appearance in the corpus. They successfully replaced a large word vocabulary in German and Russian with a much smaller sub-word vocabulary. They have also shown that their sub-word-based NMT model gives better translations than the word-based NMT models. The smallest unit of a sequence of text data is a character. The character-based approach has attracted much attention in the field of NMT, because it enables an NMT model to handle all of the tokens in the corpus (Costa-juss`a and Fonollosa, 2016; Chung et al., 2016). A hybrid model of the word-based and the character-based model has also been proposed by Luong and Manning (2016). These studies reported the success and effectiveness in translating the out-of-vocabulary words. In this paper, we apply character-based decoding to a tree-based NMT model (Eriguchi et al., 2016). The existing character-based models focus only on the sequence-based NMT models. The objective of this paper is to analyze the results of the character-based decoding in the tree-based NMT model. We also enrich the tree-based encoder with syntactic features. Figure 1 shows an overview"
W16-4617,P16-2058,0,0.048184,"Missing"
W16-4617,P16-1078,1,0.293183,"nslations than the word-based NMT models. The smallest unit of a sequence of text data is a character. The character-based approach has attracted much attention in the field of NMT, because it enables an NMT model to handle all of the tokens in the corpus (Costa-juss`a and Fonollosa, 2016; Chung et al., 2016). A hybrid model of the word-based and the character-based model has also been proposed by Luong and Manning (2016). These studies reported the success and effectiveness in translating the out-of-vocabulary words. In this paper, we apply character-based decoding to a tree-based NMT model (Eriguchi et al., 2016). The existing character-based models focus only on the sequence-based NMT models. The objective of this paper is to analyze the results of the character-based decoding in the tree-based NMT model. We also enrich the tree-based encoder with syntactic features. Figure 1 shows an overview of our system. We conducted the English-to-Japanese translation task on the WAT’16 dataset. The results of our characterbased decoder model show that its translation accuracy is lower than that of the word-based decoder model by 1.34 BLEU scores, but the character-based decoder model needed much less time to ge"
W16-4617,D10-1092,0,0.128746,"beam search in order to obtain a proper translation sentence with the size of 20 and 5 in the word-based decoder and the character-based decoder, respectively. The maximum length of a generated sentence is set to 100 in the word-based decoder and to 300 in the character-based decoder. Cho et al. (2014a) reported that an RNN-based decoder generates a shorter sentence when using the original beam search. We used the beam search method proposed in Eriguchi et al. (2016) in order to output longer translations. We evaluated the models by the BLEU score (Papineni et al., 2002) and the RIBES score (Isozaki et al., 2010) employed as the official evaluation metrics in WAT’16. 4.2 Experimental Results Table 3 shows the experimental results of the character-based models, the word-based models and the baseline SMT systems. BP denotes the brevity penalty in the BLEU score. First, we can see small improvements in the RIBES score of the single tree-to-sequence ANMT models with the characterbased decoder using syntactic features, compared to our proposed baseline system. System 1 is one of our submitted systems. The translations are output by the ensemble of the three models, and we used a simple 2 3 http://lotus.kue"
W16-4617,P15-1001,0,0.120433,"た。 + label input (m = 64) 低損失フォルステライト磁器の開発概要を述べた。 Table 5: Translation examples of test data. tences, the ground truth target sentences, and the translated sentences by the word-based model, by the character-based model, and the character-based model using the syntactic features embedded with a dimension size of 64. The words in the same color semantically correspond to each other. In sentence A, we can see that the character-based models correctly translated the source word “micro” with the characters “マイクロ”, while the word-based decoder requires the unknown replacement (Luong et al., 2015b; Jean et al., 2015). When the word-based model outputs the target word “UNK”, the source phrase ”micro watt” has the highest attention score (α = 0.78) and the source word “micro” has the second highest attention score (α = 0.16). The word-based decoder model is successful in outputting the original number (“３８０”) in the source side to the target side, and both of the character-based decoder model has also succeeded in predicting a correct sequence of characters “３”, “８”, and “０” one by one. The training dataset includes the translation of the word “380” into the characters“３８０”, so the character-based model can"
W16-4617,D13-1176,0,0.102982,"in” into “ 低損失フォルステライト磁器”. The word-based decoder model generates two “UNK”s. The source word “forsterite” (“フォルステライト” in Japanese) has the highest attention score (α = 0.23) to the first “UNK”, and the phrase “forsterite porcelain” has the second highest attention score (α = 0.21). The second “UNK” is softly alined to the source word “porcelain” (“磁器” in Japanese) with the highest attention score (α = 0.26) and to the source phrase “forsterite porcelain” with the second highest attention score (α = 0.16). 6 Related Work There are many NMT architectures: a convolutional network-based encoder (Kalchbrenner and Blunsom, 2013), sequence-to-sequence models (Cho et al., 2014b; Sutskever et al., 2014) and a tree-based encoder (Eriguchi et al., 2016). The objective of these research efforts focused on how to encode the data in a language into a vector space and to decode the data in another language from the vector space. Sennrich and Haddow (2016) improved the vector space of NMT models by adding linguistic features. The text data is basically considered as the sequence of words. 180 The word-based NMT models cannot usually cover the whole vocabulary in the corpus. Rare words are mapped into “unknown” words when the N"
W16-4617,P16-1057,0,0.0229316,"Missing"
W16-4617,P16-1100,0,0.121572,"the corpus. They successfully replaced a large word vocabulary in German and Russian with a much smaller sub-word vocabulary. They have also shown that their sub-word-based NMT model gives better translations than the word-based NMT models. The smallest unit of a sequence of text data is a character. The character-based approach has attracted much attention in the field of NMT, because it enables an NMT model to handle all of the tokens in the corpus (Costa-juss`a and Fonollosa, 2016; Chung et al., 2016). A hybrid model of the word-based and the character-based model has also been proposed by Luong and Manning (2016). These studies reported the success and effectiveness in translating the out-of-vocabulary words. In this paper, we apply character-based decoding to a tree-based NMT model (Eriguchi et al., 2016). The existing character-based models focus only on the sequence-based NMT models. The objective of this paper is to analyze the results of the character-based decoding in the tree-based NMT model. We also enrich the tree-based encoder with syntactic features. Figure 1 shows an overview of our system. We conducted the English-to-Japanese translation task on the WAT’16 dataset. The results of our char"
W16-4617,D15-1166,0,0.139038,". Our model is based on the tree-to-sequence Attention-based NMT (ANMT) model proposed by Eriguchi et al. (2016). We submitted two ANMT systems: one with a word-based decoder and the other with a character-based decoder. Experimenting on the English-to-Japanese translation task, we have confirmed that the character-based decoder can cover almost the full vocabulary in the target language and generate translations much faster than the word-based model. 1 Introduction End-to-end Neural Machine Translation (NMT) models have recently achieved state-of-the-art results in several translation tasks (Luong et al., 2015a; Luong et al., 2015b). Those NMT models are based on the idea of sequence-to-sequence learning (Sutskever et al., 2014), where both of the source and the target sentences are considered as a sequence of symbols (e.g. words or characters) and they are directly converted via a vector space. The sequence of symbols on the source side is input into a vector space, and the sequence of symbols on the target side is output from the vector space. In the end-to-end NMT models, the above architectures are embodied by a single neural network. The optimal unit for NMT is an important research question d"
W16-4617,P15-1002,0,0.122376,". Our model is based on the tree-to-sequence Attention-based NMT (ANMT) model proposed by Eriguchi et al. (2016). We submitted two ANMT systems: one with a word-based decoder and the other with a character-based decoder. Experimenting on the English-to-Japanese translation task, we have confirmed that the character-based decoder can cover almost the full vocabulary in the target language and generate translations much faster than the word-based model. 1 Introduction End-to-end Neural Machine Translation (NMT) models have recently achieved state-of-the-art results in several translation tasks (Luong et al., 2015a; Luong et al., 2015b). Those NMT models are based on the idea of sequence-to-sequence learning (Sutskever et al., 2014), where both of the source and the target sentences are considered as a sequence of symbols (e.g. words or characters) and they are directly converted via a vector space. The sequence of symbols on the source side is input into a vector space, and the sequence of symbols on the target side is output from the vector space. In the end-to-end NMT models, the above architectures are embodied by a single neural network. The optimal unit for NMT is an important research question d"
W16-4617,J08-1002,0,0.052995,"5 million pairs of training sentences from train-1.txt and the first 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 177 Train dataset Dev. dataset Test dataset Sentences Parsed sentences 1,346,946 1,790 1,812 1,346,946 1,789 1,811 Vocabulary size |Vword |in English |Vword |in Japanese |Vcharacter |in Japanese 87,796 65,680 3,004 Table 1: The details of dataset in the ASPEC corpus. Table 2: Vocabulary sizes in the training models. half of train-2.txt. We removed the sentences whose lengths are greater than 50 words. In the tree-based encoder, binary trees of the source sentences were obtained by Enju (Miyao and Tsujii, 2008), which is a probabilistic HPSG parser. We used phrase category labels as the syntactic features in the proposed treebased encoder. There are 19 types of phrase category labels given by Enju. In the word-based decoder model, we employed KyTea (Neubig et al., 2011) as the segmentation tool for the Japanese sentences. We performed the preprocessing steps of the data as recommended in WAT’16.2 Table 1 and Table 2 show the details of the final dataset and the vocabulary sizes in our experiments. Each vocabulary includes the words and the characters whose frequencies exceed five or two in the train"
W16-4617,W15-5001,0,0.0807138,"core when the model predicts “EOS”. The BLEU score is sensitive to the value of BP, and we observe the same trend in that the character-based approaches generate a shorter sentence by the original beam search. As a result, each of the character-based models can generate longer translation by +0.09 BP scores at least than System 1 using the original beam search. The word-based tree-to-sequece decoder model shows slightly better performance than the wordbased sequence-to-sequence ANMT model (Luong et al., 2015a) in both of the scores. The results of the baseline systems are the ones reported in Nakazawa et al. (2015). Compared to these SMT baselines, each of the character-based models clearly outperforms the phrase-based system in both of the BLEU and RIBES scores. Although the hierarchical phrase-based SMT system and the tree-to-string SMT system outperforms the single character-based model without phrase label inputs by +1.04 and by +1.92 BLEU scores, respectively, our best ensemble of character-based models shows better performance (+5.65 RIBES scores) than the tree-to-string SMT system. All the submitted systems are evaluated by pairwise cloudsourcing. System 1 is ranked as the 9th out of the 10 submi"
W16-4617,W16-4601,0,0.0583037,"Missing"
W16-4617,L16-1350,0,0.0415398,"Missing"
W16-4617,P11-2093,0,0.081196,"n Japanese |Vcharacter |in Japanese 87,796 65,680 3,004 Table 1: The details of dataset in the ASPEC corpus. Table 2: Vocabulary sizes in the training models. half of train-2.txt. We removed the sentences whose lengths are greater than 50 words. In the tree-based encoder, binary trees of the source sentences were obtained by Enju (Miyao and Tsujii, 2008), which is a probabilistic HPSG parser. We used phrase category labels as the syntactic features in the proposed treebased encoder. There are 19 types of phrase category labels given by Enju. In the word-based decoder model, we employed KyTea (Neubig et al., 2011) as the segmentation tool for the Japanese sentences. We performed the preprocessing steps of the data as recommended in WAT’16.2 Table 1 and Table 2 show the details of the final dataset and the vocabulary sizes in our experiments. Each vocabulary includes the words and the characters whose frequencies exceed five or two in the training data, respectively. The out-of-vocabulary words are mapped into a special token i.e. “UNK”. As a result, the vocabulary size of the characters in Japanese is about 22 times smaller than that of the words. NMT models are often trained on a limited vocabulary, b"
W16-4617,P02-1040,0,0.0954831,"ascanu et al., 2012) is set to 3.0. We use a beam search in order to obtain a proper translation sentence with the size of 20 and 5 in the word-based decoder and the character-based decoder, respectively. The maximum length of a generated sentence is set to 100 in the word-based decoder and to 300 in the character-based decoder. Cho et al. (2014a) reported that an RNN-based decoder generates a shorter sentence when using the original beam search. We used the beam search method proposed in Eriguchi et al. (2016) in order to output longer translations. We evaluated the models by the BLEU score (Papineni et al., 2002) and the RIBES score (Isozaki et al., 2010) employed as the official evaluation metrics in WAT’16. 4.2 Experimental Results Table 3 shows the experimental results of the character-based models, the word-based models and the baseline SMT systems. BP denotes the brevity penalty in the BLEU score. First, we can see small improvements in the RIBES score of the single tree-to-sequence ANMT models with the characterbased decoder using syntactic features, compared to our proposed baseline system. System 1 is one of our submitted systems. The translations are output by the ensemble of the three models"
W16-4617,W16-2209,0,0.0352846,"ce word “porcelain” (“磁器” in Japanese) with the highest attention score (α = 0.26) and to the source phrase “forsterite porcelain” with the second highest attention score (α = 0.16). 6 Related Work There are many NMT architectures: a convolutional network-based encoder (Kalchbrenner and Blunsom, 2013), sequence-to-sequence models (Cho et al., 2014b; Sutskever et al., 2014) and a tree-based encoder (Eriguchi et al., 2016). The objective of these research efforts focused on how to encode the data in a language into a vector space and to decode the data in another language from the vector space. Sennrich and Haddow (2016) improved the vector space of NMT models by adding linguistic features. The text data is basically considered as the sequence of words. 180 The word-based NMT models cannot usually cover the whole vocabulary in the corpus. Rare words are mapped into “unknown” words when the NMT models are trained. Luong et al. (2015b) proposed an ex post facto replacement technique for such unknown words, and Jean et al. (2015) replace the unknown word with the source word which has the highest attention score to the unknown word. Sennrich et al. (2016) adopted a sub-word as a unit of the vocabulary for the NM"
W16-4617,P16-1162,0,0.323242,"both of the source and the target sentences are considered as a sequence of symbols (e.g. words or characters) and they are directly converted via a vector space. The sequence of symbols on the source side is input into a vector space, and the sequence of symbols on the target side is output from the vector space. In the end-to-end NMT models, the above architectures are embodied by a single neural network. The optimal unit for NMT is an important research question discussed in the community. Early NMT models employ a word as a unit of the sequence (Cho et al., 2014b; Sutskever et al., 2014). Sennrich et al. (2016) have used a Byte-Pair Encoding (BPE) method to create a sub-word level vocabulary according to the frequencies of sub-word appearance in the corpus. They successfully replaced a large word vocabulary in German and Russian with a much smaller sub-word vocabulary. They have also shown that their sub-word-based NMT model gives better translations than the word-based NMT models. The smallest unit of a sequence of text data is a character. The character-based approach has attracted much attention in the field of NMT, because it enables an NMT model to handle all of the tokens in the corpus (Costa-"
W16-4617,P15-1150,0,0.0646219,"of parallel sentence pairs. When training the model, the parameters θ are updated by Stochastic Gradient Descent (SGD). 3 Our systems: tree-to-character attention-based NMT model Our system is mostly based on the tree-to-sequence Attention-based NMT (ANMT) model described in Eriguchi et al. (2016) which has a tree-based encoder and a sequence-based decoder. They employed Long Short-Term Memory (LSTM) as the units of RNNs (Hochreiter and Schmidhuber, 1997; Gers et al., 2000). In their proposed tree-based encoder, the phrase vectors are computed from their child states by using Tree-LSTM units (Tai et al., 2015), following the phrase structure of a sentence. We incorporate (p) syntactic features into the tree-based encoder, and the k-th phrase vector hk ∈ Rd×1 in our system is computed as follows: (i) ik = σ(Ul hlk + Ur(i) hrk + W (i) zk + b(i) ), (fr ) l hk + Ur(fr ) hrk + W (fr ) zk + b(fr ) ), (˜ c) tanh(Ul hlk + Ur(˜c) hrk + W (o) zk + b(˜c) ), (p) hk = ok ⊙ tanh(ck ), fkr = σ(Ul c˜k = (f (fl ) l hk + Ur l) hrk + W (fl ) zk + b(fl ) ), (o) ok = σ(Ul hlk + Ur(o) hrk + W (o) zk + b(o) ), ck = ik ⊙ c˜k + fkl ⊙ clk + fkr ⊙ crk , fkl = σ(Ul (8) where each of ik , ok , c˜k , ck , clk , crk , fkl , and"
W16-4617,W15-5007,0,0.0767861,"et al., 2015a) to improve translation accuracy. The j-th hidden state of the decoder is computed in our systems as follows: sj = RN Ndecoder (Embed(yj−1 ), [sj−1 ; s˜j−1 ]), (9) where [sj−1 ; s˜j−1 ] ∈ R2d×1 denotes the concatenation of sj−1 and s˜j−1 . 4 Experiment in WAT’16 task 4.1 Experimental Setting We conducted experiments for our system using the 3rd Workshop of Asian Translation 2016 (WAT’16)1 English-to-Japanese translation task (Nakazawa et al., 2016a). The data set is the Asian Scientific Paper Excerpt Corpus (ASPEC) (Nakazawa et al., 2016b). The data setting followed the ones in Zhu (2015) and Eriguchi et al. (2016). We collected 1.5 million pairs of training sentences from train-1.txt and the first 1 http://lotus.kuee.kyoto-u.ac.jp/WAT/ 177 Train dataset Dev. dataset Test dataset Sentences Parsed sentences 1,346,946 1,790 1,812 1,346,946 1,789 1,811 Vocabulary size |Vword |in English |Vword |in Japanese |Vcharacter |in Japanese 87,796 65,680 3,004 Table 1: The details of dataset in the ASPEC corpus. Table 2: Vocabulary sizes in the training models. half of train-2.txt. We removed the sentences whose lengths are greater than 50 words. In the tree-based encoder, binary trees of"
