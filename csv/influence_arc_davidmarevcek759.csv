2008.eamt-1.16,bojar-prokopova-2006-czech,0,0.0283705,"Missing"
2008.eamt-1.16,J03-1002,0,0.011532,"ntent words). We show that the inter-annotator agreement on such t-layer links is SEnglishT set date yet get_back #Cor no table bargaining SCzechT stanovit datum žádný dosud n vrat st l vyjednávac  Fig. 2. Example of alignment on the t-layer (t-trees are simplified). 105 12th EAMT conference, 22-23 September 2008, Hamburg, Germany higher compared to that on word-layer. The main result achieved in the presented work is following: we show that the t-alignment produced by our feature-based structural t-aligner outperforms the t-alignment derived from the word-layer alignment provided by GIZA++ [3] in terms of their f-measure. This is probably caused by the fact that tectogrammatical representations of Czech and English sentences are much closer compared to the distance of their surface shapes. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not"
2008.eamt-1.16,P96-1018,0,0.0295322,"rom the word-layer alignment provided by GIZA++ [3] in terms of their f-measure. This is probably caused by the fact that tectogrammatical representations of Czech and English sentences are much closer compared to the distance of their surface shapes. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical represent"
2008.eamt-1.16,W01-1406,0,0.66403,"es. For example, most auxiliary words, whose alignment is notoriously problematic, are not represented as tectogrammatical nodes on their own and thus no artificial rules for their alignment are needed. Nodes of t-trees represent content words in sentences. Haruno and Yamazaki were engaged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical representation is based on the Functional Generative Description, developed by Petr Sgall and his collaborators since 1960s (see [8]). We use its implementation specified in Prague Dependency Treebank 2.0 as described in [1]. It consists of three interlinke"
2008.eamt-1.16,W04-3228,0,0.0244207,"aged in alignment of content words only for Japanese-English pair [4], with the motivation similar to ours: it is not feasible to align functional words in structurally very different languages; however, they did not use tree structures. Experiments with alignment of dependency trees are described for example in [5] and in [6], but in our opinion no quantitative comparison of these approaches with our approach is possible due to different experiment contexts. There is also a broad literature about aligning constituency trees; dependency and constituency approaches to alignment are compared in [7]. 2 Tectogrammatical representation The tectogrammatical representation is based on the Functional Generative Description, developed by Petr Sgall and his collaborators since 1960s (see [8]). We use its implementation specified in Prague Dependency Treebank 2.0 as described in [1]. It consists of three interlinked annotation layers: the morphological layer, the analytical layer (a-layer for short, describing the surface syntax) and the tectogrammatical layer (t-layer, describing the deep syntax – transition between syntax and semantics). On the t-layer, every sentence is represented as a roote"
2008.eamt-1.16,W08-0325,1,0.778306,"dinating conjunctions and prepositions are represented in the respective nodes in the form of their attributes. For example, there is no node representing auxiliary will at the t-layer, but its meaning is captured by attribute tense. Other attributes describe several cognitive, syntactic and morphological categories. The presence of an attribute in a node is determined by the node type. In this paper we will use the following attributes: – t-lemma – tectogrammatical lemma, – formeme – simplified description of the morphosyntactic form (how the tnode is expressed on the surface), introduced in [9] – deepord – describes the organization of words in a sentence according to their increasing communicative dynamism. It also determines the linear position of t-node in the tree. As for the alignment on t-layer we currently do not distinguish different arrow types. Every t-node is aligned with no, one or more t-nodes in the opposite 106 12th EAMT conference, 22-23 September 2008, Hamburg, Germany language. Sometimes it is necessary to align more Czech t-nodes with more English t-nodes. Then the arrows will lead from each such Czech t-node to all corresponding English t-nodes. This occurs infre"
2008.eamt-1.16,H05-1066,0,0.0871239,"Missing"
2008.eamt-1.16,A00-1031,0,0.00967893,"Missing"
2008.eamt-1.16,W02-1001,0,0.0175887,"Missing"
2008.eamt-1.16,2001.mtsummit-ebmt.4,0,\N,Missing
2008.eamt-1.16,J03-4003,0,\N,Missing
2020.findings-emnlp.245,I17-1001,0,0.0230053,"Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a small amount of data is sufficient for that purpose (§6.1). 2 Models and Data We analyze the uncased base BERT model for English, which we will refer to as enBERT, and the uncased multilingual BERT model, mBERT, for English, German, French, Czech, Finnish, Indonesian,"
2020.findings-emnlp.245,2020.acl-main.493,0,0.0295344,"et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a small amount of data is sufficient for that purpose (§6.1). 2 Models and Data We analyze the uncased base BERT model for English, which we will refer to as enBERT, and the uncased multilingual BERT model, mBERT, for English, German, French, Czech, Finnish, Indonesian, Turkish, Korean, and Japanese 1 . The code 1"
2020.findings-emnlp.245,P19-1493,0,0.143053,", Korean, Japanese) are significantly lower than for SVO languages (English, French, Czech, Finnish, Indonesian) in both Dependency Accuracy (14.7 pp) and the UAS (10.5 pp). Our methods outperform the baselines in the latter group by 17.2 pp to 25.4 pp for Dependency Accuracy and from 6.1 pp to 15.5 pp for UAS. The influence of Adjective and Noun order is less apparent. On average, the NA languages results are higher than for the AN languages by 2.4 pp in Dependency Accuracy and 2.7 pp in UAS. 2715 7.1 Cross-lingual intersections Representation of mBERT is language independent to some extent (Pires et al., 2019; Libovickỳ et al., 2019). Thus, a natural question is whether the same mBERT heads encode the same syntactic relations for different languages. In particular, subject relations tend to be encoded by similar heads in different languages, which rarely belong to an ensemble for other dependency labels. Again Japanese is an exception here, possibly due to different ObjectVerb order. For adjective modifiers, the French ensemble has two heads in common with the German and one with other considered languages, although the preferred order of adjective and noun is different. 2 4 3 4 1 4 1 2 1 1 2 2 1"
2020.findings-emnlp.245,W17-0412,0,0.033847,"Missing"
2020.findings-emnlp.245,W18-5431,0,0.307782,"s well across languages. 1 Introduction and Related Work In recent years, systems based on Transformer architecture achieved state-of-the-art results in language modeling (Devlin et al., 2019) and machine translation (Vaswani et al., 2017). Additionally, the contextual embeddings obtained from the intermediate representation of the model brought improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al.,"
2020.findings-emnlp.245,K17-3009,0,0.0439431,"ish, German, French, Czech, Finnish, Indonesian, Turkish, Korean, and Japanese 1 . The code 1 Pretrained models are available at https://github. com/google-research/bert 2710 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2710–2722 c November 16 - 20, 2020. 2020 Association for Computational Linguistics shared by Clark et al. (2019) 2 substantially helped us in extracting attention weights from BERT. To find syntactic heads, we use: 1000 EuroParl multi parallel sentences (Koehn, 2004) for five European languages, automatically annotated with UDPipe UD 2.0 models (Straka and Straková, 2017); Google Universal Dependency Treebanks (GSD) for Indonesian, Korean, and Japanese (McDonald et al., 2013); the UD Turkish Treebank (IMST-UD) (Sulubacak et al., 2016). We use another PUD treebanks from the CoNLL 2017 Shared Task for evaluation of mBERT in all languages (Nivre et al., 2017)3 . 3 Adapting UD to BERT Since the explicit dependency structure is not used in BERT training, syntactic dependencies captured in latent layers are expected to diverge from annotation guidelines. After initial experiments, we have observed that some of the differences are systematic (see Table 1). UD Modifie"
2020.findings-emnlp.245,W19-4808,0,0.0721454,"tation of the model brought improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few pa"
2020.findings-emnlp.245,P19-1580,0,0.163251,"improvements in various NLP tasks. Multiple recent works try to analyze such latent representations (Linzen et al., 2019), observe syntactic properties in some Transformer self-attention heads, and extract syntactic trees from the attentions matrices (Raganato and Tiedemann, 2018; Mareˇcek and Rosa, 2019; Clark et al., 2019; Jawahar et al., 2019). In our work, we focus on the comparative analysis of the syntactic structure, examining how the BERT self-attention weights correspond to Universal Dependencies (UD) syntax (Nivre et al., 2016). We confirm the findings of Vig and Belinkov (2019) and Voita et al. (2019) that in TransFinally, we apply our observations to improve the method of extracting dependency trees from attention (§5), and analyze the results both in a monolingual and a multilingual setting (§6). Our method crucially differs from probing (Belinkov et al., 2017; Hewitt and Manning, 2019; Chi et al., 2020; Kulmizev et al., 2020). We do not use treebank data to train a parser; rather, we extract dependency relations directly from selected attention heads. We only employ syntactically annotated data to select the heads; however, this means estimating relatively few parameters, and only a sma"
2020.findings-emnlp.245,L16-1262,0,\N,Missing
2020.findings-emnlp.245,N19-1419,0,\N,Missing
2020.findings-emnlp.245,P19-1526,0,\N,Missing
2020.findings-emnlp.245,W19-4828,0,\N,Missing
2021.acl-long.36,P17-1080,0,0.194257,"tic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization. 1 (a) Structural Probe (b) Orthogonal Structural Probe Figure 1: Comparison of the Structural Probe of Hewitt and Manning (2019) and the Orthogonal Structural Probe proposed by us. Introduction Latent representations of neural networks encode specific linguistic features. Recently, a lot of focus was devoted to interpret these representations and analyze structures captured by the deep models. One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019). The pre-trained model’s 1 parameters are fixed, and its latent states or outputs are then fed into a simple neural network optimized to solve an auxiliary task, e.g., semantic, syntactic parsing, anaphora resolution, morphosyntactic tagging, etc. The amount of language information stored in the representations can be evaluated by measuring the specific language task’s performance. Probing experiments usually involve classification tasks. Lately, Hewitt and Manning (2019) proposed Structural Probes, which use regression as an optim"
2021.acl-long.36,N19-1419,0,0.295813,"ction is decomposed into 1. isomorphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization. 1 (a) Structural Probe (b) Orthogonal Structural Probe Figure 1: Comparison of the Structural Probe of Hewitt and Manning (2019) and the Orthogonal Structural Probe proposed by us. Introduction Latent representations of neural networks encode specific linguistic features. Recently, a lot of focus was devoted to interpret these representations and analyze structures captured by the deep models. One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019). The pre-trained model’s 1 parameters are fixed, and its latent states or outputs are then fed into a simple neural network optimized to solve an auxiliary task, e.g., semantic, syntactic parsin"
2021.acl-long.36,2020.acl-main.375,0,0.20108,"Missing"
2021.acl-long.36,Q16-1037,0,0.0623385,"ations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization. 1 (a) Structural Probe (b) Orthogonal Structural Probe Figure 1: Comparison of the Structural Probe of Hewitt and Manning (2019) and the Orthogonal Structural Probe proposed by us. Introduction Latent representations of neural networks encode specific linguistic features. Recently, a lot of focus was devoted to interpret these representations and analyze structures captured by the deep models. One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019). The pre-trained model’s 1 parameters are fixed, and its latent states or outputs are then fed into a simple neural network optimized to solve an auxiliary task, e.g., semantic, syntactic parsing, anaphora resolution, morphosyntactic tagging, etc. The amount of language information stored in the representations can be evaluated by measuring the specific language task’s performance. Probing experiments usually involve classification tasks. Lately, Hewitt and Manning (2019) proposed Structural Probes, which use regression as an optimization objective. They train a linear proj"
2021.acl-long.36,N19-1112,0,0.125167,"orthogonal constraint makes the Structural Probes less vulnerable to memorization. 1 (a) Structural Probe (b) Orthogonal Structural Probe Figure 1: Comparison of the Structural Probe of Hewitt and Manning (2019) and the Orthogonal Structural Probe proposed by us. Introduction Latent representations of neural networks encode specific linguistic features. Recently, a lot of focus was devoted to interpret these representations and analyze structures captured by the deep models. One of the most popular analysis methods is probing (Belinkov et al., 2017; Blevins et al., 2018; Linzen et al., 2016; Liu et al., 2019). The pre-trained model’s 1 parameters are fixed, and its latent states or outputs are then fed into a simple neural network optimized to solve an auxiliary task, e.g., semantic, syntactic parsing, anaphora resolution, morphosyntactic tagging, etc. The amount of language information stored in the representations can be evaluated by measuring the specific language task’s performance. Probing experiments usually involve classification tasks. Lately, Hewitt and Manning (2019) proposed Structural Probes, which use regression as an optimization objective. They train a linear projection layer to app"
2021.blackboxnlp-1.20,P18-1198,0,0.0236119,"e performance on these tasks is not always up to par with supervised state of the art, prompting has the advantage of being unsupervised. However, various studies have found that task performance is highly dependent on the prompt used (Jiang et al., 2020; Reynolds and McDonell, 2021); these same studies propose automatic or handmade improvements to prompts to improve them. Supervised techniques have also been devised to automatically develop better prompts (Shin et al., 2020). Beyond downstream tasks, prompting is also useful for investigating language models as a test subject. While probing (Conneau et al., 2018; Tenney et al., 2019), for example, is useful, it involves training an auxiliary model. This auxiliary model can be troublesome; (Hewitt and Liang, 2019) show that the probes themselves can learn tasks independently of whether the model encodes linguistic structure. Moreover, a model can contain information within its internal representations without relying on that information to make predictions. In contrast, prompting requires no auxiliary models that might complicate matters. Ettinger (2020) uses prompting to compare BERT’s linguistic competencies and tendencies to that of humans; among t"
2021.blackboxnlp-1.20,N19-1423,0,0.0381276,"ssigned to each token that could fill in the blank; here, we would expect an answer such as “fruit”. While Ravichander et al. (2020) also approach BERT’s lexical semantics using a prompting task, they focus on simple prompts and examine only if BERT can predict a word’s canonical hypernym. In contrast, we explore how more complex prompts affect BERT’s ability to predict hypernyms, propose new methods of evaluating hypernym discovery that that take into account the fact that a word may have multiple hypernyms, and find effective prompts for hypernym discovery. Large pretrained language models (Devlin et al., 2019; Radford et al., 2019) have set new standards for performance on NLP tasks. Many of these tasks, such as question answering and natural language 2 Background: Prompting inference, might seem to require human-like syntactic and semantic capabilities to perform well, Although fine-tuning is the methodology with leading to many studies on this topic (Rogers et al., which LLMs shattered existing state-of-the-art on 2020). many downstream tasks, the use of prompting has also gained interest in recent years. Unlike fineHowever, evidence that LLMs have human-like semantic capabilities is mixed. With"
2021.blackboxnlp-1.20,2020.tacl-1.3,0,0.0196459,"prompting is also useful for investigating language models as a test subject. While probing (Conneau et al., 2018; Tenney et al., 2019), for example, is useful, it involves training an auxiliary model. This auxiliary model can be troublesome; (Hewitt and Liang, 2019) show that the probes themselves can learn tasks independently of whether the model encodes linguistic structure. Moreover, a model can contain information within its internal representations without relying on that information to make predictions. In contrast, prompting requires no auxiliary models that might complicate matters. Ettinger (2020) uses prompting to compare BERT’s linguistic competencies and tendencies to that of humans; among those investigated is hypernymy. They find that BERT easily identifies noun hypernyms, but fails to adapt to negated sentences: given the prompts “A hammer is a [MASK]” and “A hammer is not a [MASK]”, BERT predicts the words “hammer”, “tool”, and “weapon” for both. In a follow-up, Ravichander et al. (2020), investigate whether BERT has a systematic understanding of hypernymy. They find that BERT’s understanding of hypernymy is not systematic: BERT fails when tasked with finding the hypernym of a h"
2021.blackboxnlp-1.20,C92-2082,0,0.29915,"the singular are single tokens, hypernyms in the plural are often split into multiple tokens, and cannot be guessed using one [MASK]token. As a follow up, we run the type-of experiment, using the prompt “A(n) x is a type1 of [MASK].”. This is intended to allow for vowel hypernyms and clarify that hypernymy is the relation of interest. We also test two final simple prompts that reflect common real-world situations in which hypernyms might appear. The first, “a [MASK], such as x”, is a prompt first proposed as a template using which hypernym-hyponym pairs could be discovered in unlabeled text (Hearst, 1992). This such-as prompt is a fragment, rather than a natural sentence. The second prompt is “My favorite [MASK]is x.”. This “favorite” prompt both provides a more natural prompt, and puts the [MASK]token in an intermediate position. Next, we perform a series of contextualized queries, based on the hypothesis that some hypernyms are difficult for BERT to predict because there is insufficient information in the query, and our short, contextless prompts are too different from the training data. To resolve this we generate contexts in two ways. First, we create a handwritten context to be used for e"
2021.blackboxnlp-1.20,D19-1275,0,0.0118014,"ous studies have found that task performance is highly dependent on the prompt used (Jiang et al., 2020; Reynolds and McDonell, 2021); these same studies propose automatic or handmade improvements to prompts to improve them. Supervised techniques have also been devised to automatically develop better prompts (Shin et al., 2020). Beyond downstream tasks, prompting is also useful for investigating language models as a test subject. While probing (Conneau et al., 2018; Tenney et al., 2019), for example, is useful, it involves training an auxiliary model. This auxiliary model can be troublesome; (Hewitt and Liang, 2019) show that the probes themselves can learn tasks independently of whether the model encodes linguistic structure. Moreover, a model can contain information within its internal representations without relying on that information to make predictions. In contrast, prompting requires no auxiliary models that might complicate matters. Ettinger (2020) uses prompting to compare BERT’s linguistic competencies and tendencies to that of humans; among those investigated is hypernymy. They find that BERT easily identifies noun hypernyms, but fails to adapt to negated sentences: given the prompts “A hammer"
2021.blackboxnlp-1.20,2020.tacl-1.28,0,0.0351449,". Prompting has seen use for diverse tasks such as 275 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 275–282 Online, November 11, 2021. ©2021 Association for Computational Linguistics knowledge base completion (Petroni et al., 2019) as well as summarization and translation (Radford et al., 2019). While performance on these tasks is not always up to par with supervised state of the art, prompting has the advantage of being unsupervised. However, various studies have found that task performance is highly dependent on the prompt used (Jiang et al., 2020; Reynolds and McDonell, 2021); these same studies propose automatic or handmade improvements to prompts to improve them. Supervised techniques have also been devised to automatically develop better prompts (Shin et al., 2020). Beyond downstream tasks, prompting is also useful for investigating language models as a test subject. While probing (Conneau et al., 2018; Tenney et al., 2019), for example, is useful, it involves training an auxiliary model. This auxiliary model can be troublesome; (Hewitt and Liang, 2019) show that the probes themselves can learn tasks independently of whether the mo"
2021.blackboxnlp-1.20,W04-2710,0,0.127682,"Missing"
2021.blackboxnlp-1.20,D19-1250,0,0.0227109,") or the identity of one or more masked words (masked language modeling). The output of (2020) note that while polysemous words’ senses are linearly separable based on their BERT embed- the language model is then used as the response to the task at hand. dings, these embeddings do not form clusters based on their senses alone. Prompting has seen use for diverse tasks such as 275 Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 275–282 Online, November 11, 2021. ©2021 Association for Computational Linguistics knowledge base completion (Petroni et al., 2019) as well as summarization and translation (Radford et al., 2019). While performance on these tasks is not always up to par with supervised state of the art, prompting has the advantage of being unsupervised. However, various studies have found that task performance is highly dependent on the prompt used (Jiang et al., 2020; Reynolds and McDonell, 2021); these same studies propose automatic or handmade improvements to prompts to improve them. Supervised techniques have also been devised to automatically develop better prompts (Shin et al., 2020). Beyond downstream tasks, prompting is also usefu"
2021.blackboxnlp-1.20,2020.starsem-1.10,0,0.096348,"non-contextual embeddings on a lexical relation prediction task. That said, BERT’s knowledge of lexical semantics is not equal for all words: it struggles with rare words (Schick and Schütze, 2020). In this work, we further examine BERT’s knowledge of lexical semantics, more specifically that of hypernymy, using a Cloze / prompting methodology. In the most basic form of this framework, we simply run BERT on an input sentence such as “An apple is a [MASK].”, and extract the probabilities assigned to each token that could fill in the blank; here, we would expect an answer such as “fruit”. While Ravichander et al. (2020) also approach BERT’s lexical semantics using a prompting task, they focus on simple prompts and examine only if BERT can predict a word’s canonical hypernym. In contrast, we explore how more complex prompts affect BERT’s ability to predict hypernyms, propose new methods of evaluating hypernym discovery that that take into account the fact that a word may have multiple hypernyms, and find effective prompts for hypernym discovery. Large pretrained language models (Devlin et al., 2019; Radford et al., 2019) have set new standards for performance on NLP tasks. Many of these tasks, such as questio"
2021.blackboxnlp-1.20,2020.blackboxnlp-1.15,0,0.094551,"Missing"
2021.blackboxnlp-1.20,2020.tacl-1.54,0,0.0375852,"Missing"
2021.blackboxnlp-1.20,2020.emnlp-main.346,0,0.0766196,"Missing"
2021.blackboxnlp-1.20,2020.emnlp-main.586,0,0.0821131,"Missing"
2021.emnlp-main.376,P81-1022,0,0.555932,"Missing"
2021.emnlp-main.376,elkateb-etal-2006-building,0,0.0530039,"Missing"
2021.emnlp-main.376,E14-1049,0,0.0437569,"Cross-lingual embeddings There is an essential 1 branch of research studying relationships of emEnglish, Spanish, Slovene, Indonesian, Chinese, Finnish, Arabic, French, and Basque beddings across languages. Mikolov et al. (2013) 4589 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4589–4598 c November 7–11, 2021. 2021 Association for Computational Linguistics showed that distributions of the word vectors in different languages could be aligned in shared space. Following research analyzed various methods of aligning cross-lingual static embeddings (Faruqui and Dyer, 2014; Artetxe et al., 2016; Smith et al., 2017) and gradually dropped the requirement of parallel data for alignment (Artetxe et al., 2018; Zhang et al., 2017; Lample et al., 2018). Significant attention was also devoted to the analysis of multilingual and contextual embeddings of M BERT (Pires et al., 2019; Libovický et al., 2020). There is also no conclusive answer to whether the alignment of such representations is beneficial to cross-lingual transfer. Wang et al. (2019) show that the alignment facilitates zero-shot parsing, while results of Wu and Dredze (2020) for multiple tasks put in doubt"
2021.emnlp-main.376,2020.emnlp-main.363,0,0.0522048,"Missing"
2021.emnlp-main.376,2020.findings-emnlp.150,0,0.022239,"4589–4598 c November 7–11, 2021. 2021 Association for Computational Linguistics showed that distributions of the word vectors in different languages could be aligned in shared space. Following research analyzed various methods of aligning cross-lingual static embeddings (Faruqui and Dyer, 2014; Artetxe et al., 2016; Smith et al., 2017) and gradually dropped the requirement of parallel data for alignment (Artetxe et al., 2018; Zhang et al., 2017; Lample et al., 2018). Significant attention was also devoted to the analysis of multilingual and contextual embeddings of M BERT (Pires et al., 2019; Libovický et al., 2020). There is also no conclusive answer to whether the alignment of such representations is beneficial to cross-lingual transfer. Wang et al. (2019) show that the alignment facilitates zero-shot parsing, while results of Wu and Dredze (2020) for multiple tasks put in doubt the benefits of the alignment. 3 Method dB (hi , hj )2 = (B(hi − hj ))T (B(hi − hj )), (1) B is the Linear Transformation matrix and hi , hj are the vector representations of words at positions i and j. Another type of a probe is a Depth Probe, where the token’s depth in a dependency tree is approximated by the Euclidean norm o"
2021.emnlp-main.376,2021.acl-long.36,1,0.849714,"Missing"
2021.emnlp-main.376,Q16-1037,0,0.0277161,"rning an orthogonal transformation that maps the embeddings across languages based on monolingual linguistic information: dependency syntax and lexical hypernymy. This new capability allows us to test different probing scenarios. We measure how adding assumptions of isomorphism and uniformity of the representations across languages affect probing results to answer our research questions. 2 Related Work Probing It is a method of evaluating linguistic information encoded in pre-trained NLP models. Usually, a simple classifier for the probing task is trained on the frozen model’s representation (Linzen et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). The work of Hewitt and Manning (2019) introduced structural probes that linearly transform contextual embeddings to approximate the topology of dependency trees. Limisiewicz and Mareˇcek (2021) proposed new structural tasks and introduced orthogonal constraint allowing to decompose projected embeddings into parts correlated with specific linguistic features. Kulmizev et al. (2020) probed different languages to examine what type of syntactic dependency annotation is captured in an LM. Hall Maudslay et al. (2020) modify the loss function, improving"
2021.emnlp-main.376,2020.acl-main.375,0,0.0936023,"of evaluating linguistic information encoded in pre-trained NLP models. Usually, a simple classifier for the probing task is trained on the frozen model’s representation (Linzen et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). The work of Hewitt and Manning (2019) introduced structural probes that linearly transform contextual embeddings to approximate the topology of dependency trees. Limisiewicz and Mareˇcek (2021) proposed new structural tasks and introduced orthogonal constraint allowing to decompose projected embeddings into parts correlated with specific linguistic features. Kulmizev et al. (2020) probed different languages to examine what type of syntactic dependency annotation is captured in an LM. Hall Maudslay et al. (2020) modify the loss function, improving syntactic probes’ ability to parse. Cross-lingual embeddings There is an essential 1 branch of research studying relationships of emEnglish, Spanish, Slovene, Indonesian, Chinese, Finnish, Arabic, French, and Basque beddings across languages. Mikolov et al. (2013) 4589 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4589–4598 c November 7–11, 2021. 2021 Association for Computationa"
2021.emnlp-main.376,Y11-1027,0,0.034945,"Missing"
2021.emnlp-main.376,2020.lrec-1.497,0,0.0578937,"Missing"
2021.emnlp-main.376,N18-1202,0,0.0272471,"rix. The new formulation of an Orthogonal Distance Probe is2 : 2 ddV ¯ T (hi , hj ) (3) = (d¯ V T (hi − hj ))T (d¯ V T (hi − hj )), where V is an orthogonal matrix (Orthogonal Transformation) and d¯ is a Scaling Vector, which 2 4 Reformulation of an Orthogonal Depth Probe is analogiExperiments We examine vector representations obtained from multilingual cased BERT (Devlin et al., 2019). 4.1 The Structural Probe (Hewitt and Manning, 2019) is a gradient optimized linear projection of the contextual word representations produced by a pretrained neural model (e.g. BERT Devlin et al. (2019), ELM O Peters et al. (2018)). In a Distance Probe, the Euclidean distance between projected word vectors approximates the distance between words in a dependency tree: cal. can be changed during optimization for each task to allow multi-task joint probing. This procedure allowed optimizing a separate Scaling Vector d¯ for a specific objective, allowing probing for multiple linguistic tasks simultaneously. In this work, an individual Orthogonal Transformation V is trained for each language, facilitating multi-language probing. This approach assumes that the representations are isomorphic across languages; we examine this"
2021.emnlp-main.376,D17-1207,0,0.0264866,"c, French, and Basque beddings across languages. Mikolov et al. (2013) 4589 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4589–4598 c November 7–11, 2021. 2021 Association for Computational Linguistics showed that distributions of the word vectors in different languages could be aligned in shared space. Following research analyzed various methods of aligning cross-lingual static embeddings (Faruqui and Dyer, 2014; Artetxe et al., 2016; Smith et al., 2017) and gradually dropped the requirement of parallel data for alignment (Artetxe et al., 2018; Zhang et al., 2017; Lample et al., 2018). Significant attention was also devoted to the analysis of multilingual and contextual embeddings of M BERT (Pires et al., 2019; Libovický et al., 2020). There is also no conclusive answer to whether the alignment of such representations is beneficial to cross-lingual transfer. Wang et al. (2019) show that the alignment facilitates zero-shot parsing, while results of Wu and Dredze (2020) for multiple tasks put in doubt the benefits of the alignment. 3 Method dB (hi , hj )2 = (B(hi − hj ))T (B(hi − hj )), (1) B is the Linear Transformation matrix and hi , hj are the vecto"
bojar-etal-2012-joy,bojar-etal-2010-evaluating,1,\N,Missing
bojar-etal-2012-joy,C00-2163,0,\N,Missing
bojar-etal-2012-joy,W07-1709,0,\N,Missing
bojar-etal-2012-joy,P02-1040,0,\N,Missing
bojar-etal-2012-joy,H05-1066,0,\N,Missing
bojar-etal-2012-joy,P07-2045,1,\N,Missing
bojar-etal-2012-joy,W10-1703,0,\N,Missing
bojar-etal-2012-joy,W09-3939,1,\N,Missing
D12-1028,D10-1117,0,0.24525,"Missing"
D12-1028,A00-1031,0,0.170556,"arts of the treebanks (the files test.conll) for the dependency tree induction. As a source of the part-of-speech tags, we use the fine-grained gold PoS tags, which are in the fifth column in the CoNLL format. For obtaining reducibility scores, we used the W2C corpus9 of Wikipedia articles, which was ˇ downloaded by Majliˇs and Zabokrtsk´ y (2012). Their statistics across languages are shown in Table 4. To make them useful, the necessary preprocessing steps must have been done. The texts were first automatically segmented and tokenized10 and then they were part-of-speech tagged by TnT tagger (Brants, 2000), which was trained on the respective CoNLL training data (the files train.conll). The quality of such tagging is not very high, since we do not use any lexicons11 or pretrained models. However, it is sufficient for obtaining good reducibility scores. 8 We do not have appropriate Chinese segmenter that would segment Chinese texts in the same way as in CoNLL. 9 http://ufal.mff.cuni.cz/˜majlis/w2c/ 10 The segmentation to sentences and tokenization was perˇ formed using the TectoMT framework (Popel and Zabokrtsk´ y, 2010). 11 Using lexicons or another pretrained models for tagging means using oth"
D12-1028,W06-2920,0,0.178839,"o the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manual"
D12-1028,C96-1058,0,0.324093,"r location in the corpus. These counts are collected over the whole corpus with the collection-rate 0.01.7 When the samling is finished, we build final dependency trees based on the edge counts obtained during the sampling. We employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them; the weights of edges for computing MST correspond to the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking"
D12-1028,P07-1077,0,0.0248163,"e employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them; the weights of edges for computing MST correspond to the number of times they were present during the sampling. This averaging method ˇ was used also by Mareˇcek and Zabokrtsk´ y (2011). Other possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situ"
D12-1028,N09-1012,0,0.237092,"Missing"
D12-1028,P04-1061,0,0.905252,"briefly outlines the state of the art in unsupervised dependency parsing. Our measure of reducibility based on a large monolingual corpus is presented in Section 3. Section 4 shows our models which serve for generating probability estimates for edge sampling described in Section 5. Experimental parsing results for languages included in CoNLL shared task treebanks are summarized in Section 6. Section 7 concludes this article. 2 Related Work The most popular approach in unsupervised dependency parsing of the recent years is to employ Dependency Model with Valence (DMV), which was introduced by Klein and Manning (2004). The inference algorithm was further improved by Smith (2007) and Cohen et al. (2008). Headden, Johnson and McClosky (2009) introduced the Extended Valence Grammar (EVG) and added lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning larger dependency fragments. Unfortunately, many of these works show results only for English.1 However, the main feature of unsupervised methods should be their applicability across a wide range of languages. Such experiments were done by Spitkovsky (2011b; 2011c), where the parsing algorithm was evaluated on"
D12-1028,majlis-zabokrtsky-2012-language,1,0.869845,"Missing"
D12-1028,W11-3901,1,0.901282,"Missing"
D12-1028,D11-1006,0,0.0748374,"Missing"
D12-1028,D11-1118,0,0.375042,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
D12-1028,D11-1117,0,0.337233,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
D12-1028,W11-0303,0,0.394274,"possibilities for obtaining final dependency trees would be using Eisner’s projective algorithm (Eisner, 1996) or using annealing method (favoring more likely changes) at the end of the sampling. However, the general non-projective MST algorithm enable non-projective edges, which are by no means negligible in treebanks (Havelka, 2007). 6 Experiments and Evaluation We evaluate our parser on 20 treebanks (18 languages) included in CoNLL shared tasks 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007). Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011b), the tuning experiments were performed on English only. We used English for checking functionality of the individual models and for optimizing hyperparameter values. The best configuration of the parser achieved on English development data was then used for parsing all other languages. This simulates the situation in which we have only one treebank (English) on which we can tune our parser and we want to parse other languages for which we have no manually annotated treebanks. 7 After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 303 langua"
K18-2019,W17-0401,0,0.0905027,"Missing"
K18-2019,N13-1073,0,0.0190644,"s there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a prominent word form and is thus likely to be translated correctly even in this way. 6 https://unimorph.github.io/ 7 We have not evaluated the influence on delexicalized 2 http://www.opensubtitles.org/ http://opus.nlpl.eu/"
K18-2019,L16-1147,0,0.0188728,"icon, we change its tag (unless it is AUX), lemma, and morphological features according to the lexicon. Each feature that was mapped from UniMorph style to UD style is added to the features obtained by the tagger. In case it was there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a pro"
K18-2019,P15-2040,1,0.890588,"Missing"
K18-2019,W17-7615,1,0.859487,"Missing"
K18-2019,W17-1226,1,0.848863,"ost of the target languages; the specific setups used for each of the target languages are described in later sections. 2.1 6. now UDPipe can be trained in a standard way on the resulting pseudo-target treebank and applied to target texts Treebank translation using parallel data 2.2 Tiedemann (2014) introduced the approach of automatically translating the word forms in a source treebank into the target language, and then training a pseudo-target parser (and/or a tagger) on the resulting pseudo-target treebank. This approach was further investigated by Rosa ˇ et al. (2017), Rosa and Zabokrtsk´ y (2017) and Rosa (2018a), finding that the sophistication of the Machine Translation (MT) system plays a rather minor role in cross-lingual parsing, while there is a significant benefit in using word-based translation – this forces the translations to be more literal, and enables a trivial approach to annotation transfer. In this work, we use probably the simplest possible approach, based on extracting a dictionary from word-aligned data, and translating each source word into the target word most frequently aligned to it, ignoring any context or other information. While we had found that using state-"
K18-2019,N06-2033,0,0.118422,"s-lingual parsing of target languages without any training data, McDonald et al. (2013) showed that combining syntactic information from multiple source languages can lead to a more accurate parsing than when using only one source language. Moreover, this idea can be easily extended to target languages with small training data, combining the target language resources with larger resources for other close languages (Zhang and Barzilay, 2015). To combine the multilingual resources, we use the weighted parse tree combination method of ˇ Rosa and Zabokrtsk´ y (2015), which is based on the work of Sagae and Lavie (2006). It consists of training separate parsers on the source language treebanks (and also the target language treebank if it is available), applying them independently to the input sentence, and then combining the resulting dependency trees into a directed graph, with each edge weighted by a sum of weights of the parsers which produced this edge. The final parse tree is then obtained by applying the directed maximum spanning tree algorithm of Chu and Liu (1965) and Edmonds (1967) to the weighted graph. To make the source parser applicable to the target language sentences, we either use a translati"
K18-2019,L16-1680,0,0.0680977,"Missing"
K18-2019,tiedemann-2012-parallel,0,0.0605145,"ing to the lexicon. Each feature that was mapped from UniMorph style to UD style is added to the features obtained by the tagger. In case it was there but with a different value, the value is changed. We chose to post-correct the morphological annotation only after parsing. This way, the parser cannot benefit from the potentially better morphological annotation; however, the target parser seems to benefit from being applied to an annotation more similar to what it was trained on.7 1. obtain OpenSubtitles20182 (Lison and Tiedemann, 2016) sentence-aligned source-target parallel data from Opus3 (Tiedemann, 2012) 2. tokenize the parallel data with source and target UDPipe tokenizers 3. obtain intersection word-alignment with FastAlign4 (Dyer et al., 2013) 4. extract the translation table: for each source word, take the target word most frequently aligned to it, and store it as its translation 5. translate the source training treebank into the target language, replacing each word form 5 We translate lemmas using the dictionary extracted on forms, as we typically do not have another choice anyway. We assume that the lemma is a prominent word form and is thus likely to be translated correctly even in thi"
K18-2019,C14-1175,0,0.0492526,"019 and each lemma5 by its translation from the translation table (keep the word untranslated if it does not appear in the translation table) cross-lingual techniques, as an enrichment of the baseline approach to achieve better performance. In this section, we introduce several approaches that we apply to many or most of the target languages; the specific setups used for each of the target languages are described in later sections. 2.1 6. now UDPipe can be trained in a standard way on the resulting pseudo-target treebank and applied to target texts Treebank translation using parallel data 2.2 Tiedemann (2014) introduced the approach of automatically translating the word forms in a source treebank into the target language, and then training a pseudo-target parser (and/or a tagger) on the resulting pseudo-target treebank. This approach was further investigated by Rosa ˇ et al. (2017), Rosa and Zabokrtsk´ y (2017) and Rosa (2018a), finding that the sophistication of the Machine Translation (MT) system plays a rather minor role in cross-lingual parsing, while there is a significant benefit in using word-based translation – this forces the translations to be more literal, and enables a trivial approach"
K18-2019,K18-2001,0,0.108655,"Missing"
K18-2019,D15-1213,0,0.146359,"r, as opposed to dependency trees, there are no strict structural constraints, which means that instead of the spanning tree algorithm, we can use a simple weighted voting. For cross-lingual parsing of target languages without any training data, McDonald et al. (2013) showed that combining syntactic information from multiple source languages can lead to a more accurate parsing than when using only one source language. Moreover, this idea can be easily extended to target languages with small training data, combining the target language resources with larger resources for other close languages (Zhang and Barzilay, 2015). To combine the multilingual resources, we use the weighted parse tree combination method of ˇ Rosa and Zabokrtsk´ y (2015), which is based on the work of Sagae and Lavie (2006). It consists of training separate parsers on the source language treebanks (and also the target language treebank if it is available), applying them independently to the input sentence, and then combining the resulting dependency trees into a directed graph, with each edge weighted by a sum of weights of the parsers which produced this edge. The final parse tree is then obtained by applying the directed maximum spanni"
L16-1015,J92-4003,0,0.304047,"we say that something is an adverb in language X, we should be able to support such a claim by some measurable evidence rather than just by saying that it becomes an adverb if translated to English. 2. Related Work There is a body of literature about POS tagging of under-resourced languages. Most approaches rely on the existence of some form of parallel (or comparable) data. We will discuss only those approaches that attempt at using the same tagset across languages, and not those aiming at unsupervised induction, such as the well-known Brown clusters induced in a fully unsupervised fashion (Brown et al., 1992). An overview of such truly unsupervised approaches can be found in (Christodouloupoulos et al., 2010).2 • For some multilingual NLP tasks, such as unsupervised dependency parsing (or parser transfer), it might be more important to preprocess all languages under study as similarly as possible (including POS tagging), rather than to maximize accuracy with respect to highly different gold-standard data in individual languages. 1 However, we do not say that our method is completely language-independent. For instance, we rely on the existence of a meaningful tokenization in the target language. 2"
L16-1015,D10-1056,0,0.0699114,"Missing"
L16-1015,W02-2006,0,0.0990035,"Missing"
L16-1015,P11-1061,0,0.321767,"because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different method named delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained on annotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency, neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certain extent, serve as predictors of the part of speech, represented by the universal POS tag (Das and Petrov, 2011). Keywords: delexicalized tagging, HamdleDT 2.0, features expansion, classifier 1. Introduction languages); the model is independent of individual word forms. In delexicalized parsing, word form sequences are substituted by sequences of POS tags, which—of course— is not extendable to tagging. Instead, we substitute word forms by vectors of numerical features that can be computed using only unannotated monolingual texts. The background intuition is that the individual POS categories will tend to manifest similar statistical properties across languages (e.g., prepositions tend to be short, relat"
L16-1015,P13-2112,0,0.0359974,"Missing"
L16-1015,I05-1075,0,0.080472,"Missing"
L16-1015,majlis-zabokrtsky-2012-language,1,0.885772,"Missing"
L16-1015,L16-1262,1,0.852652,"Missing"
L16-1015,petrov-etal-2012-universal,0,0.0316314,"sulting classifier is used to assign POS tags to all words’ feature vectors in the target languages, X NN = f (y) y∈N ext(w) X − y∈N ext(w) 4. we evaluate our approach on the target languages for which there are labeled data available, and assume that reasonably similar accuracies are reached also for the other target languages. f (y) f (y) log NN NN 5. substituting word entropy X SN = f (y) y∈Subst(w) 3.2. Tagset X A prerequisite to our approach is a common tagset for both the source and the target languages. We use the same tagset as (Das and Petrov, 2011), the Google Universal POS tag set (Petrov et al., 2012). With just 12 tags it is fairly y∈Subst(w) 3 97 − f (y) f (y) log SN SN http://universaldependencies.org/ 3.4. 6. is number – binary value is number(w), In our approach, we need two types of data resources: 7. is punctuation – binary value is punctuation(w), • raw monolingual texts for both source and target languages; this data is used for extracting feature vectors for words in individual languages; we use W2C, a web-based corpus of 120 languages (Majliˇs and ˇ Zabokrtsk´ y, 2012), 8. relative frequency after number log |i : ci = w ∧ is number(ci−1 )| f (w) • POS-tagged data for source lang"
L16-1015,N01-1026,0,0.208375,"Missing"
L16-1015,I08-3008,1,0.770159,"n the existence of a meaningful tokenization in the target language. 2 There is a certain terminological confusion in this area: sometimes the word “unsupervised” is used also for situations in which there are no hand-tagged data available for the target language, but some manual annotation of the source language exists and is projected across parallel data like in (Das and Petrov, 2011). We prefer to avoid the term “unsupervised” when manual annotation is used in any language. We propose “delexicalized tagging”, a new method for under-resourced languages. In analogy to delexicalized parsing (Zeman and Resnik, 2008), we transfer a tagging model from a resource-rich language (or a set of 96 (Yarowsky and Ngai, 2001) project POS tags from English to French and Chinese via both automatic and gold alignment, and report substantial growth of accuracy after using de-noising postprocessing. (Fossum and Abney, 2005) extend this approach by projecting multiple source languages onto a target language. (Das and Petrov, 2011) use graph-based label propagation for cross-lingual knowledge transfer, and estimate emission distributions in the target language using a loglinear model. (Duong et al., 2013) choose only auto"
L16-1015,P15-2044,0,\N,Missing
P10-2016,W08-0309,0,0.065204,"based on the deep syntactic representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is"
P10-2016,W09-0401,0,0.0561799,"representation of the sentence performing very well for Czech as the target language. Aside from including dependency and n-gram relations in the scoring, we also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-indep"
P10-2016,2009.mtsummit-papers.3,0,0.368676,"o if the part of speech does not occur in the reference or the candidate set and otherwise it is computed as given in Equation 1 below. ˇ We use TectoMT (Zabokrtsk´ y and Bojar, 2008), http://ufal.mff.cuni.cz/tectomt/, for the linguistic pre-processing. While both our implementation of SemPOS as well as TectoMT are in principle freely available, a stable public version has yet to be released. Our plans include experiments with approximating the deep syntactic analysis with a simple tagger, which would also decrease the installation burden and computation costs, at the expense of accuracy. 5 3 Condon et al. (2009) identify similar issues when evaluating translation to Arabic and employ rule-based normalization of MT output to improve the correlation. It is beyond the scope of this paper to describe the rather different nature of morphological richness in Czech, Arabic and also other languages, e.g. German or Finnish. 4 The dataset with manually flagged errors is available at http://ufal.mff.cuni.cz/euromatrixplus/ 87 SRC REF cu-bojar pctrans Prague Stock Market falls to minus by the end of the trading day praˇzsk´a burza se ke konci obchodov´an´ı propadla do minusu praha stock market klesne k minus na"
P10-2016,W07-0738,0,0.0873662,"Missing"
P10-2016,P07-2045,1,0.010071,". for other languages) were found later, usually employing language-specific tools, see e.g. Przybocki et al. (2008) or Callison-Burch et al. (2009). The unbeaten advantage of BLEU is its simplicity. Figure 1 illustrates a very low correlation to human judgments when translating to Czech. We plot the official BLEU score against the rank established as the percentage of sentences where a system ranked no worse than all its competitors (Callison-Burch et al., 2009). The systems developed at Charles University (cu-) are described in Bojar et al. (2009), uedin is a vanilla configuration of Moses (Koehn et al., 2007) and the remaining ones are commercial MT systems. In a manual analysis, we identified the reasons for the low correlation: BLEU is overly sensitive to sequences and forms in the hypothesis matching ∗ This work has been supported by the grants EuroMatrixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of the Czech Republic), FP7-ICT-2009-4-247762 (Faust), GA201/09/H057, GAUK 1163/2010, and MSM 0021620838. We are grateful to the anonymous reviewers for further research suggestions. 1 http://nist.gov/speech/tests /metricsmatr/2008/results/ 2 http://www.statmt.org/wmt08 and wmt09 86 Proceedings"
P10-2016,P02-1040,0,0.0811214,"e also apply and evaluate SemPOS for English. Introduction Automatic metrics of machine translation (MT) quality are vital for research progress at a fast pace. Many automatic metrics of MT quality have been proposed and evaluated in terms of correlation with human judgments while various techniques of manual judging are being examined as well, see e.g. MetricsMATR08 (Przybocki et al., 2008)1 , WMT08 and WMT09 (Callison-Burch et al., 2008; Callison-Burch et al., 2009)2 . The contribution of this paper is twofold. Section 2 illustrates and explains severe problems of a widely used BLEU metric (Papineni et al., 2002) when applied to Czech as a representative of languages with rich morphology. We see this as an instance of the sparse data problem well known for MT itself: too much detail in the formal representation leading to low coverage of e.g. a translation dictionary. In MT evaluation, too much detail leads to the lack of comparable parts of the hypothesis and the reference. 2 Problems of BLEU BLEU (Papineni et al., 2002) is an established language-independent MT metric. Its correlation to human judgments was originally deemed high (for English) but better correlating metrics (esp. for other languages"
P10-2016,W09-0422,1,\N,Missing
P13-1028,W12-1912,0,0.0423489,"ive model where the grammar is expressed by two probability distributions: Pchoose (cd |ch , dir ), which generates a new child cd attached to the head ch in the direction dir (left or right), and Pstop (STOP |ch , dir , · · · ), which makes a decision whether to generate another child of ch in the direction dir or not.2 Such a grammar is then inferred using sampling or variational methods. Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed. The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. The estimation is done using the reducibility principle ˇ introduced in (Mareˇcek and Zabokrtsk´ y, 2012). The reduc"
P13-1028,D10-1117,0,0.374271,"h was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al.,"
P13-1028,A00-1031,0,0.055428,"we use the Wikipedia articles from W2C ˇ corpus (Majliˇs and Zabokrtsk´ y, 2012), which provide sufficient amount of data for our purposes. Statistics across languages are shown in Table 1. The Wikipedia texts were automatically tokenized and segmented to sentences so that their tokenization was similar to the one in the CoNLL evaluation treebanks. Unfortunately, we were not able to find any segmenter for Chinese that would produce a desired segmentation; therefore, we removed Chinese from evaluation. The next step was to provide the Wikipedia texts with POS tags. We employed the TnT tagger (Brants, 2000) which was trained on the re286 language red. seq. 546 645 1808 712 930 576 880 7603 1488 language tokens (mil.) Greek 20.9 Hungarian 26.3 Italian 39.7 Japanese 2.6 Portuguese 31.7 Slovenian 13.7 Spanish 53.4 Swedish 19.2 Turkish 16.5 red. seq. 1037 2237 723 31 4765 513 1156 481 5706 0.2 0 VBD VBD 0 Czech left-stop Czech right-stop AA 0.4 Db Z: RR NN VB Vf C= Vp Z: C= RR Vf 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 estimation computed on the treebank VVFIN NE VAFIN NN NN VVFIN 0.4 ADV ADJA ART APPR 0.2 0 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 estimation computed on the treebank Fc 0.6 sp aq vm nc np"
P13-1028,W06-2920,0,0.0241605,"hu and Liu, 1965). Therefore, the resulting trees consist of edges maximizing the sum of individual counts: X TM ST = arg max count(e) T e∈T It is important to note that the MST algorithm may produce non-projective trees. Even if we average the strictly projective dependency trees, some non-projective edges may appear in the result. This might be an advantage since correct non-projective edges can be predicted; however, this relaxation may introduce mistakes as well. 6 6.1 Experiments Data We use two types of resources in our experiments. The first type are CoNLL treebanks from the year 2006 (Buchholz and Marsi, 2006) and 2007 (Nivre et al., 2007), which we use for inference and for evaluation. As is the standard practice in unsupervised parsing evaluation, we removed all punctuation marks from the trees. In case a punctuation node was not a leaf, its children are attached to the parent of the removed node. For estimating the STOP probabilities (Section 3), we use the Wikipedia articles from W2C ˇ corpus (Majliˇs and Zabokrtsk´ y, 2012), which provide sufficient amount of data for our purposes. Statistics across languages are shown in Table 1. The Wikipedia texts were automatically tokenized and segmented"
P13-1028,W12-1913,0,0.0324347,"Missing"
P13-1028,D12-1028,1,0.865383,"Missing"
P13-1028,D11-1005,0,0.0279913,"d Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 3.1 Martin Fo"
P13-1028,D11-1006,0,0.0173326,"n of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 3.1 Martin Fourcade was sixth , maintaining his lead at the top of the overall World Cup standings , although Svendsen is now only 59 points away from the Frenchman in second . The next competition is this weekend at Lillehammer in Norway . Larinto saw oﬀ allcomers at Kuopio with jumps of 129.5 and 124m for a total 240.9 points , just 0.1 points ahead of compatriot Matti Hautamaeki , who landed eﬀorts of 127 and 129.5m . Third place went"
P13-1028,C08-1018,0,0.0124105,"ˇcek and Zabokrtsk´ y, 2012). Our dependency model contained a submodel which directly prioritized subtrees that form reducible sequences of POS tags. Reducibility scores of given POS tag sequences were estimated using a large corpus of Wikipedia articles. The weakness of this approach was the fact that longer sequences of POS tags are very sparse and no reducibility scores could be estimated for them. In this paper, we avoid this shortcoming by estimating the STOP probabilities for individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoot"
P13-1028,D10-1120,0,0.173438,"pitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain text. 3 3.1 Martin Fourcade was sixth , maintaining his lead at the top of the overall World Cup standings , although Svendsen is now only 59 points away from the Frenchman in second . The next competition is this weekend at Lillehammer in Norway . Larinto saw oﬀ allcomers at Kuopio with jumps of 129.5 and 124m for a total 240.9 poi"
P13-1028,W12-0701,0,0.0588436,"e based on the DMV, a generative model where the grammar is expressed by two probability distributions: Pchoose (cd |ch , dir ), which generates a new child cd attached to the head ch in the direction dir (left or right), and Pstop (STOP |ch , dir , · · · ), which makes a decision whether to generate another child of ch in the direction dir or not.2 Such a grammar is then inferred using sampling or variational methods. Unfortunately, there are still cases where the inferred grammar is very different from the grammar we would expect, e.g. verbs become leaves instead of governing the sentences. Rasooli and Faili (2012) and Bisk and Hockenmaier (2012) made some efforts to boost the verbocentricity of the inferred structures; however, both of the approaches require manual identification of the POS tags marking the verbs, which renders them useless when unsupervised POS tags are employed. The main contribution of this paper is a considerable improvement of unsupervised parsing quality by estimating the Pstop probabilities externally using a very large corpus, and employing this prior knowledge in the standard inference of DMV. The estimation is done using the reducibility principle ˇ introduced in (Mareˇcek an"
P13-1028,N09-1012,0,0.264895,"Missing"
P13-1028,W11-1109,0,0.0183889,"thing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rules such as Verb→Noun, Noun→Adjective. McDonald et al. (2011) identify the POS tags by a crosslingual transfer. Such approaches achieve better results; however, they are useless for grammar induction from plain"
P13-1028,P04-1061,0,0.926533,"rtcoming by estimating the STOP probabilities for individual POS tags only. Another task related to reducibility is sentence compression (Knight and Marcu, 2002; Cohn and Lapata, 2008), which was used for text summarization. The task is to shorten the sentences while retaining the most important pieces of information, using the knowledge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The not"
P13-1028,N10-1116,0,0.248634,"edge of the grammar. Conversely, our task is to induce the grammar using the sentences and their shortened versions. Dependency Model with Valence (DMV) has been the most popular approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. Fo"
P13-1028,D11-1118,0,0.158023,"ar approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) m"
P13-1028,W11-0303,0,0.169799,"ar approach to unsupervised dependency parsing in the recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) m"
P13-1028,D12-1063,0,0.776351,"recent years. It was introduced by Klein and Manning (2004) and further improved by Smith (2007) and Cohen et al. (2008). Headden III et al. (2009) introduce the Extended Valence Grammar and add lexicalization and smoothing. Blunsom and Cohn (2010) use tree substitution grammars, which allow learning of larger dependency fragments by employing the Pitman-Yor process. Spitkovsky et al. (2010) improve the inference using iterated learning of increasingly longer sentences. Further improvements were achieved by better dealing with punctuation (Spitkovsky et al., 2011b) and new “boundary” models (Spitkovsky et al., 2012). Related Work Reducibility: The notion of reducibility belongs to the traditional linguistic criteria for recogniz282 Other approaches to unsupervised dependency parsing were described e.g. in (Søgaard, 2011), (Cohen et al., 2011), and (Bisk and Hockenmaier, 2012). There also exist “less unsupervised” approaches that utilize an external knowledge of the POS tagset. For example, Rasooli and Faili (2012) identify the last verb in the sentence, minimize its probability of reduction and thus push it to the root position. Naseem et al. (2010) make use of manually-specified universal dependency rul"
P13-1028,majlis-zabokrtsky-2012-language,0,0.0343196,"Missing"
P13-1028,D07-1096,0,\N,Missing
P13-1051,C00-2143,0,0.133964,"rinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling"
P13-1051,W06-2920,0,0.0228052,"section, we identify the CS styles defined in the previous section as used in the primary treebank data sources; statistical observations (such as the amount of annotated shared modifiers) presented here, as well as experiments on CS-style convertibility presented in Section 5.2, are based on the normalized shapes of the treebanks as contained in the HamleDT 1.0 treebank collection (Zeman et al., 2012).15 Some of the treebanks were downloaded individually from the web, but most of them came from previously published collections for dependency parsing campaigns: six languages from CoNLL-2006 (Buchholz and Marsi, 2006), seven languages from CoNLL-2007 (Nivre et al., 2007), two languages from CoNLL-2009 (Hajiˇc and others, 2009), three languages from ICON-2010 (Husain et al., 2010). Obviously, there is a certain risk that the CS-related information contained in the source treebanks was slightly biased by the properties of the CoNLL format upon conversion. In addition, many of the treebanks were natively dependency-based (cf. the 2nd column of Table 1), but some were originally based on constituents and thus specific converters to the CoNLL format had to be created (for instance, the Spanish phrase-structure"
P13-1051,dzeroski-etal-2006-towards,0,0.138307,"CS head, or attaching shared modifiers below the nearest conjunct). Even if it does not make sense to create the full Cartesian product of all dimensions because some values cannot be combined, it allows to explore the space of possible CS styles systematically.9 One can find various arguments supporting the particular choices. MTT possesses a complex set of linguistic criteria for identifying the governor of a relation (see Mazziotta (2011) for an overview), which lead to MS. MS is preferred in a rule-based dependency parsing system of Lombardo and Lesmo (1998). PS is advocated by ˇ ep´anek (2006) who claims that it can represent Stˇ shared modifiers using a single additional binary attribute, while MS would require a more complex co-indexing attribute. An argumentation of Tratz and Hovy (2011) follows a similar direction: We would like to change our [MS] handling of coordinating conjunctions to treat the coordinating conjunction as the head [PS] because this has fewer ambiguities than [MS]. . . We conclude that the influence of the choice of coordination style is a well-known problem in dependency syntax. Nevertheless, published works usually focus only on a narrow ad-hoc selection of"
P13-1051,W12-0503,1,0.894933,"Missing"
P13-1051,afonso-etal-2002-floresta,0,0.0183801,"hers, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinatio"
P13-1051,W03-2405,0,0.0337781,"Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treebank (C˘al˘acean, 2008), Russian: Syntagrus (Boguslavsky et al., 2000), Slovene: Slovene Dependency Treebank (Dˇzeroski et al., 2006), Spanish: AnCora (Taul´e et al., 2008), Swedish: Talbanken05 (Nilsson et al., 2005), ˇ Tamil: TamilTB (Ramasamy and Zabokrtsk´ y, 2012), Turkish: METU-Sabanci Turkish Treebank (Atalay et al., 2003). 8 Edge labeling can be trivially converted to node labeling in tree structures. 9 The full Cartesian product of variants in Figure 1 would result in topological 216 variants, but only 126 are applicable (the inapplicable combinations are marked with “—” in Figure 1). Those 126 topological variants can be further combined with labeling variants defined in Section 3.2. Variations in representing coordination structures Our analysis of variations in representing coordination structures is based on observations from a set of dependency treebanks for 26 languages.7 5 We use the already establishe"
P13-1051,W09-1201,1,0.0605496,"Missing"
P13-1051,W12-3603,1,0.894647,"Missing"
P13-1051,ramasamy-zabokrtsky-2012-prague,1,0.894745,"Missing"
P13-1051,E09-1047,0,0.144276,"Missing"
P13-1051,W98-0502,0,0.490473,"tactic surface means of expressing coordination relations is fuzzy. Some languages can use enclitics instead of conjunctions/prepositions, e.g. Latin “Senatus Populusque Romanus”. Purely hypotactic surface means such as the preposition in “John with Mary” occur too.4 Related work Let us first recall the basic well-known characteristics of CSs. In the simplest case of a CS, a coordinating conjunction joins two (usually syntactically and semantically compatible) words or phrases called conjuncts. Even this simplest case is difficult to represent within a dependency tree because, in the words of Lombardo and Lesmo (1998): Dependency paradigms exhibit obvious difficulties with coordination because, differently from most linguistic structures, it is not possible to characterize the coordination construct with a general schema involving a head and some modifiers of it. Proper formal representation of CSs is further complicated by the following facts: • Careful semantic analysis of CSs discloses additional complications: if a node is modified by a CS, it might happen that it is the node itself (and not its modifiers) what should be semantically considered as a conjunct. Note the difference between “red and white"
P13-1051,J93-2004,0,0.0420156,"rent granularity of syntactic labels. 3 3.1 Topological variations We distinguish the following dimensions of topological variations of CS styles (see Figure 1): Family – configuration of conjuncts. We divide the topological variations into three main groups, labeled as Prague (fP), Moscow (fM), and vided by IXA Group) (Aduriz and others, 2003), Bulgarian: BulTreeBank (Simov and Osenova, 2005), Czech: Prague Dependency Treebank 2.0 (Hajiˇc et al., 2006), Danish: Danish Dependency Treebank (Kromann et al., 2004), Dutch: Alpino Treebank (van der Beek and others, 2002), English: Penn TreeBank 3 (Marcus et al., 1993), Finnish: Turku Dependency Treebank (Haverinen et al., 2010), German: Tiger Treebank (Brants et al., 2002), Greek (modern): Greek Dependency Treebank (Prokopidis et al., 2005), Hindi, Bengali and Telugu: Hyderabad Dependency Treebank (Husain et al., 2010), Hungarian: Szeged Treebank (Csendes et al., 2005), Italian: Italian Syntactic-Semantic Treebank (Montemagni and others, 2003), Latin: Latin Dependency Treebank (Bamman and Crane, 2011), Persian: Persian Dependency Treebank (Rasooli et al., 2011), Portuguese: Floresta sint´a(c)tica (Afonso et al., 2002), Romanian: Romanian Dependency Treeban"
P13-1051,D07-1013,0,0.158065,"Missing"
P13-1051,taule-etal-2008-ancora,0,0.0175746,"Missing"
P13-1051,zeman-etal-2012-hamledt,1,0.0729458,"Missing"
P13-1051,D11-1116,0,\N,Missing
P13-1051,J03-4003,0,\N,Missing
P13-1051,D07-1096,0,\N,Missing
P13-3025,W12-3132,1,0.855405,"Missing"
P13-3025,2011.mtsummit-papers.35,0,0.451304,"Missing"
P13-3025,W12-3130,0,0.3097,"B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source 3 Evaluation of Existing SPE Approaches First, we evaluated the utility of the approach of B´echara et al. (2011) for the English-Czech language pair. We used 1 million sentence pairs from CzEng 1.0 (Bojar et al., 2012b), a large EnglishCzech parallel corpus. Identically to the paper, we split the training data into 10 parts, trained 10 systems (each on nine tenths of the data) and used them to translate the remaining part. The second step was then trained on the concatenation of these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are report"
P13-3025,P07-2045,0,0.00639385,"successful research in statistical post-editing (SPE) of SMT (see Section 2). In our paper, we describe a statistical approach to correcting one particular type of English-toCzech SMT errors – errors in the verb-noun valency. The term valency stands for the way in which verbs and their arguments are used together, usually together with prepositions and morphological cases, and is described in Section 4. Several examples of the valency of the English verb ‘to go’ and the corresponding Czech verb ‘j´ıt’ are shown in Table 1. We conducted our experiments using a state-ofthe-art SMT system Moses (Koehn et al., 2007). An example of Moses making a valency error is translating the sentence ‘The government spends on the middle schools.’, adapted from our development data set. As shown in Table 2, Moses translates the sentence incorrectly, making an error in the valency of the ‘utr´acet – sˇkola’ (‘spend – school’) pair. The missing preposition changes the meaning dramatically, as the verb ‘utr´acet’ is pol172 Proceedings of the ACL Student Research Workshop, pages 172–179, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Direction en→cs cs→en ysemous and can mean ‘to spend ("
P13-3025,bojar-etal-2012-joy,1,0.896011,"Missing"
P13-3025,W04-3250,0,0.11784,"these translations and the target side of CzEng. We also implemented the contextual variant of SPE where words in the intermediate language are annotated with corresponding source words if the alignment strength is greater than a given threshold. We limited ourselves to the threshold value 0.8, for which the best results are reported in the paper. We tuned all systems on the dataset of WMT11 (CallisonBurch et al., 2011) and evaluated on the WMT12 dataset (Callison-Burch et al., 2012). Table 3 summarizes our results. The reported confidence intervals were estimated using bootstrap resampling (Koehn, 2004). SPE did not lead to any improvements of BLEU in our experiments. In fact, SPE even slightly decreased the score (but 1 Depfix (Rosa et al., 2012b) performs rule-based postediting on shallow-syntax dependency trees, while Deepfix (described in this paper) is a statistical post-editing system operating on deep-syntax dependency trees. 173 the difference is statistically insignificant in all cases). We conclude that this method does not improve English-Czech translation, possibly because our training data is too large for this method to bring any benefit. We therefore proceed with a more comple"
P13-3025,P05-1012,0,0.164008,"Missing"
P13-3025,J03-1002,0,0.00593089,"Missing"
P13-3025,W10-1703,0,0.0747674,"Missing"
P13-3025,W07-0704,0,0.368871,"to automatically determine the structure of each sentence, and to detect and correct valency errors using a simple statistical valency model. We describe our approach in detail in Section 5. We evaluate and discuss our experiments in Section 6. We then conclude the paper and propose areas to be researched in future in Section 7. 2 Baseline 10.85±0.47 17.20±0.53 SPE 10.70±0.44 17.11±0.52 Context SPE 10.73±0.49 17.18±0.54 Table 3: Results of SPE approach of B´echara et al. (2011) evaluated on English-Czech SMT. words, eventually reaching an improvement of 2.29 BLEU points. However, similarly to Oflazer and El-Kahlout (2007), the training data used are very small, and it is not clear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. Related Work The first reported results of au"
P13-3025,W11-2103,0,0.0479084,"Missing"
P13-3025,P02-1040,0,0.0879185,"nd reported that the SMT system alone performed worse than the post-edited rule-based system. They then tried to post-edit the bilingual SMT system with another monolingual instance of the same SMT system, but concluded that no improvement in quality was observed. The first known positive results in SPE of SMT are reported by Oflazer and El-Kahlout (2007) on English to Turkish machine translation. The authors followed a similar approach to Simard et al. (2007), training an SMT system to postedit its own output. They use two iterations of post-editing to get an improvement of 0.47 BLEU points (Papineni et al., 2002). The authors used a rather small training set and do not discuss the scalability of their approach. To the best of our knowledge, the best results reported so far for SPE of SMT are by B´echara et al. (2011) on French-to-English translation. The authors start by using a similar approach to Oflazer and El-Kahlout (2007), getting a statistically significant improvement of 0.65 BLEU points. They then further improve the performance of their system by adding information from the source side into the post-editing system by concatenating some of the translated words with their source 3 Evaluation o"
P13-3025,W12-3102,0,0.0733791,"Missing"
P13-3025,W12-4205,1,0.902315,"Missing"
P13-3025,W12-3146,1,0.908164,"Missing"
P13-3025,N07-1064,0,0.205241,"ear how their method scales on larger training data. In our previous work (Rosa et al., 2012b), we explored a related but substantially different area of rule-based post-editing of SMT. The resulting system, Depfix, manages to significantly improve the quality of several SMT systems outputs, using a set of hand-written rules that detect and correct grammatical errors, such as agreement violations. Depfix can be easily combined with Deepfix,1 as it is able to correct different types of errors. Related Work The first reported results of automatic post-editing of machine translation outputs are (Simard et al., 2007) where the authors successfully performed statistical post-editing (SPE) of rule-based machine translation outputs. To perform the postediting, they used a phrase-based SMT system in a monolingual setting, trained on the outputs of the rule-based system as the source and the humanprovided reference translations as the target, to achieve massive translation quality improvements. The authors also compared the performance of the post-edited rule-based system to directly using the SMT system in a bilingual setting, and reported that the SMT system alone performed worse than the post-edited rule-ba"
P13-3025,W07-1709,0,0.0764048,"Missing"
rosa-etal-2014-hamledt,de-marneffe-etal-2006-generating,0,\N,Missing
rosa-etal-2014-hamledt,zeman-2008-reusable,1,\N,Missing
rosa-etal-2014-hamledt,J93-2004,0,\N,Missing
rosa-etal-2014-hamledt,de-marneffe-etal-2014-universal,0,\N,Missing
rosa-etal-2014-hamledt,C00-2143,0,\N,Missing
rosa-etal-2014-hamledt,W08-1301,0,\N,Missing
rosa-etal-2014-hamledt,W13-3721,0,\N,Missing
rosa-etal-2014-hamledt,D11-1006,0,\N,Missing
rosa-etal-2014-hamledt,P13-1051,1,\N,Missing
rosa-etal-2014-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
rosa-etal-2014-hamledt,berovic-etal-2012-croatian,0,\N,Missing
rosa-etal-2014-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
rosa-etal-2014-hamledt,W03-2405,0,\N,Missing
rosa-etal-2014-hamledt,P13-2017,0,\N,Missing
rosa-etal-2014-hamledt,taule-etal-2008-ancora,0,\N,Missing
rosa-etal-2014-hamledt,W10-1819,0,\N,Missing
rosa-etal-2014-hamledt,afonso-etal-2002-floresta,0,\N,Missing
W09-0422,bojar-etal-2008-czeng,1,0.888991,"Missing"
W09-0422,W08-0309,0,0.0625391,"ion of prepositional group would be difficult otherwise. After the capitalization of the beginning of each sentence (and each named entity instance), we obtain the final translation by flattening the surface tree. Table 3 reports lowercase BLEU and NIST scores and preliminary manual ranks of our submissions in contrast with other systems participating in English→Czech translation, as evaluated on the official WMT09 unseen test set. Note that automatic metrics are known to correlate quite poorly with human judgements, see the best ranking but “lower scoring” PC Translator this year and also in Callison-Burch et al. (2008). System BLEU Moses T 14.24 Moses T+C 13.86 Google 13.59 U. of Edinburgh 13.55 Moses T+C+C&T+T+G 84k 10.01 Eurotran XP 09.51 PC Translator 09.42 TectoMT 07.29 NIST 5.175 5.110 4.964 5.039 4.360 4.381 4.335 4.173 Rank -3.02 (4) – -2.82 (3) -3.24 (5) -2.81 (2) -2.77 (1) -3.35 (6) Table 3: Automatic scores and preliminary human rank for English→Czech translation. Systems in italics are provided for comparison only. Best results in bold. Unfortunately, this preliminary evaluation suggests that simpler models perform better, partly 4.4 Preliminary Error Analysis because it is easier to tune them pr"
W09-0422,P05-1045,0,0.0043121,"ons: 15 33 43 . 5 Later, we found out that the grow-diag-final-and heuristic provides insignificantly superior results. 4 ˇ In some previous experiments (e.g.Zabokrtsk´ y et al. (2008)), we used phrase-structure parser Collins (1999) with subsequent constituency-dependency conversion. 2 126 with the option to resort to (2) an independent translation of lemma→lemma and tag→tag finished by a generation step that combines target-side lemma and tag to produce the final target-side form. One of the steps in the analysis of English is named entity recognition using Stanford Named Entity Recognizer (Finkel et al., 2005). The nodes in the English t-layer are grouped according to the detected named entities and they are assigned the type of entity (location, person, or organization). This information is preserved in the transfer of the deep English trees to the deep Czech trees to allow for the appropriate capitalization of the Czech translation. We use three language models in this setup (3-grams of forms, 3-grams of lemmas, and 10-grams of tags). Due to the increased complexity of the setup, we were able to train this model on 84k parallel sentences only (the Commentaries section) and we use the target-side"
W09-0422,P07-2045,1,0.010104,"naming conventions. However, we were unable to reliably determine the series number and the episode number from the file names. We employed a two-step procedure to automatically pair the TV series subtitle files. For every TV series: We describe two systems for English-toCzech machine translation that took part in the WMT09 translation task. One of the systems is a tuned phrase-based system and the other one is based on a linguistically motivated analysis-transfer-synthesis approach. 1 Introduction We participated in WMT09 with two very different systems: (1) a phrase-based MT based on Moses (Koehn et al., 2007) and tuned for English→Czech translation, and (2) a complex ˇ system in the TectoMT platform (Zabokrtsk´ y et al., 2008). 1. We clustered the files on both sides to remove duplicates 2. We found the best matching using a provisional translation dictionary. This proved to be a successful technique on a small sample of manually paired test data. The process was facilitated by the fact that the correct pairs of episodes usually share some named entities which the human translator chose to keep in the original English form. 2 Data 2.1 Monolingual Data Our Czech monolingual data consist of (1) the"
W09-0422,J03-1002,0,0.00228104,"0.4 Table 2: Czech-English data sizes and sources. ∗ The work on this project was supported by the grants MSM0021620838, 1ET201120505, 1ET101120503, GAUK ˇ ˇ LC536 and FP6-IST-5-034291-STP 52408/2008, MSMT CR (EuroMatrix). 1 www.opensubtitles.org and titulky.com Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 125–129, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 125 2.3 Data Preprocessing using TectoMT platform: Analysis and Alignment also uses word alignment generated from surface shapes of sentences by GIZA++ tool, Och and Ney (2003). We use acquired aligned tectogrammatical trees for training some models for the transfer. As analysis of such amounts of data is obviously computationally very demanding, we run it in parallel using Sun Grid Engine3 cluster of 40 4-CPU computers. For this purpose, we implemented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0"
W09-0422,W07-1709,0,0.170323,"Missing"
W09-0422,W08-0325,1,0.861517,"emented a rather generic tool that submits any TectoMT pipeline to the cluster. As we believe that various kinds of linguistically relevant information might be helpful in MT, we performed automatic analysis of the data. The data were analyzed using the layered annotation scheme of the Prague Dependency Treebank 2.0 (PDT 2.0, Hajiˇc and others (2006)), i.e. we used three layers of sentence representation: morphological layer, surface-syntax layer (called analytical (a-) layer), and deep-syntax layer (called tectogrammatical (t-) layer). The analysis was implemented using TectoMT, ˇ (Zabokrtsk´y et al., 2008). TectoMT is a highly modular software framework aimed at creating MT systems (focused, but by far not limited to translation using tectogrammatical transfer) and other NLP applications. Numerous existing NLP tools such as taggers, parsers, and named entity recognizers are already integrated in TectoMT, especially for (but again, not limited to) English and Czech. During the analysis of the large Czech monolingual data, we used Jan Hajiˇc’s Czech tagger shipped with PDT 2.0, Maximum Spanning Tree parser (McDonald et al., 2005) with optimized set ˇ of features as described in Nov´ak and Zabokrt"
W09-0422,J03-4003,0,\N,Missing
W09-0422,H05-1066,0,\N,Missing
W09-0422,W08-0319,1,\N,Missing
W09-0422,2008.eamt-1.16,1,\N,Missing
W09-4005,H05-1066,0,0.138988,"Missing"
W09-4005,C08-1081,0,0.0606465,"Missing"
W09-4005,W07-1709,0,0.0295975,"Missing"
W09-4005,W08-0325,0,\N,Missing
W10-1730,N07-1008,0,0.0186993,"(SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of th"
W10-1730,W04-3250,0,0.149325,"mes) instead of word forms. In particular, our target-language tree model (TreeLM) predicts the probability of node’s lemma and formeme given its parent’s lemma and formeme. The optimal (lemma and formeme) labeling is found by tree-modified Viterbi search; ˇ for details see (Zabokrtsk´ y and Popel, 2009). 4 Experiments When included into the above described translation scenario, the MaxEnt TM outperforms the baseline TM, be it used together with or without TreeLM. The results are summarized in Table 1. The improvement is statistically significant according to paired bootstrap resampling test (Koehn, 2004). In the configuration without TreeLM the improvement is greater (1.33 BLEU) than with TreeLM (0.81 BLEU), which confirms our hypothesis that MaxEnt TM captures some of the contextual dependencies resolved otherwise by language models. 205 5 Conclusions Jan Hajiˇc. 2004. Disambiguation of Rich Inflection – Computational Morphology of Czech. Charles University – The Karolinum Press, Prague. We have introduced a maximum entropy translation model in dependency-based MT which enables exploiting a large number of feature functions in order to obtain more accurate translations. The BLEU evaluation p"
W10-1730,H05-1066,0,0.157359,"Missing"
W10-1730,P02-1038,0,0.0667274,"antic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combin"
W10-1730,W03-0420,0,0.0157367,"el data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abstracts from language-specific means (inflection, agg"
W10-1730,J96-1002,0,0.0200957,"model (TM) p(t|s) is the probability that the string t from the target language is the translation of the string s from the source language. Typical approach in SMT is to use backward translation model p(s|t) according to Bayes’ rule and noisychannel model. However, in this paper we deal only with the forward (direct) model.1 The idea of using maximum entropy for constructing forward translation models is not new. It naturally allows to make use of various features potentially important for correct choice of targetlanguage expressions. Let us adopt a motivating example of such a feature from (Berger et al., 1996) (which contains the first usage of maxent translation model we are aware of): “If house appears within the next three words (e.g., the phrases in the house and in the red house), then dans might be a more likely [French] translation [of in].” Incorporating non-local features extracted from the source sentence into the standard noisychannel model in which only the backward translation model is available, is not possible. This drawback of the noisy-channel approach is typically compensated by using large target-language n-gram models, which can – in a result – play a role similar to that of a m"
W10-1730,W96-0213,0,0.208466,"wever, we expect that it would be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) t"
W10-1730,A00-2018,0,0.0559398,"d be more beneficial to exploit both the parallel data and the monolingual data in a more balance fashion, rather than extract only a reduced amount of information from the parallel data and compensate it by large language model on the target side. Introduction The principle of maximum entropy states that, given known constraints, the probability distribution which best represents the current state of knowledge is the one with the largest entropy. Maximum entropy models based on this principle have been widely used in Natural Language Processing, e.g. for tagging (Ratnaparkhi, 1996), parsing (Charniak, 2000), and named entity recognition (Bender et al., 2003). Maximum entropy models have the following form p(y|x) = X X 1 exp λi fi (x, y) Z(x) i 1 A backward translation model is used only for pruning training data in this paper. where fi is a feature function, λi is its weight, and 201 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 201–206, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics The intuition behind the decision to use tectogrammatics for MT is the following: we believe that (1) tectogrammatics largely abs"
W10-1730,W07-1709,0,0.179472,"Missing"
W10-1730,P00-1006,0,0.0343477,"v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), adj:attr (semantic adjective (SA) in attributive position), adj:compl (SA in complement position). We have implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the tran"
W10-1730,P09-2037,1,0.49307,"; coreference is also resolved. Collapsing edges are depicted by wider lines in the Figure 1a. 3 Training the two models In this section we describe two translation models used in the experiments: a baseline translation model based on maximum likelihood estimates (3.2), and a maximum entropy based model (3.3). Both models are trained using the same data (3.1). In addition, we describe a target-language tree model (3.4), which can be combined with both the translation models using the Hidden Tree Markov Model approach and tree-modified Viterbi ˇ search, similarly to the approach of (Zabokrtsk´ y and Popel, 2009). 5. The transfer phase follows, whose most difficult part consists in labeling the tree with target-side lemmas and formemes5 (changes of tree topology are required relatively infrequently). See Figure 1c. 3.1 6. Finally, surface sentence shape (Figure 1d) is synthesized from the tectogrammatical tree, which is basically a reverse operation for the Data preprocessing common for both models We used Czech-English parallel corpus CzEng 0.9 ˇ (Bojar and Zabokrtsk´ y, 2009) for training the translation models. CzEng 0.9 contains about 8 million sentence pairs, and also their tectogrammatical analy"
W10-1730,D09-1023,0,0.0237895,"e implemented our experiments in the TectoMT software framework, which already offers tool chains for analysis and synthesis of Czech ˇ and English sentences (Zabokrtsk´ y et al., 2008). The translation scenario proceeds as follows. A deeper discussion on the potential advantages of maximum entropy approach over the noisychannel approach can be found in (Foster, 2000) and (Och and Ney, 2002), in which another successful applications of maxent translation models are shown. Log-linear translation models (instead of MLE) with rich feature sets are used also in (Ittycheriah and Roukos, 2007) and (Gimpel and Smith, 2009); the idea can be traced back to (Papineni et al., 1997). What makes our approach different from the previously published works is that 1. we show how the maximum entropy translation model can be used in a dependency framework; we use deep-syntactic dependency trees (as defined in the Prague Dependency Treebank (Hajiˇc et al., 2006)) as the transfer layer, 2. we combine the maximum entropy translation model with target-language dependency tree model and use tree-modified Viterbi search for finding the optimal lemmas labeling of the target-tree nodes. The rest of the paper is structured as foll"
W10-1730,W08-0325,1,0.702701,"(1) tectogrammatics largely abstracts from language-specific means (inflection, agglutination, functional words etc.) of expressing non-lexical meanings and thus tectogrammatical trees are supposed to be highly similar across languages,2 (2) it enables a natural transfer factorization,3 (3) and local tree contexts in tectogrammatical trees carry more information (especially for lexical choice) than local linear contexts in the original sentences.4 In order to facilitate transfer of sentence ‘syntactization’, we work with tectogrammatical nodes ˇ enhanced with the formeme attribute (Zabokrtsk´ y et al., 2008), which captures the surface morphosyntactic form of a given tectogrammatical node in a compact fashion. For example, the value n:pˇred+4 is used to label semantic nouns that should appear in an accusative form in a prepositional group with the preposition pˇred in Czech. For English we use formemes such as n:subj (semantic noun (SN) in subject position), n:for+X (SN with preposition for), n:X+ago (SN with postposition ago), n:poss (possessive form of SN), v:because+fin (semantic verb (SV) as a subordinating finite clause introduced by because), v:without+ger (SV as a gerund after without), ad"
W11-2152,W10-1705,1,0.760025,"decimal and thousand separators in numbers. While there are language-specific conventions, they are not always followed and the normalization can in such cases confuse the order of magnitude by 3. 427 the output based on non-normalized test sets as our primary English-to-Czech submission. We invested much less effort into the submission called CU - BOJAR for Czech-to-English. The only interesting feature there is the use of alternative decoding paths to translate either from the Czech form or from the Czech lemma equipped with meaningbearing morphological properties, e.g. the number of nouns. Bojar and Kos (2010) used the same setup with simple lemmas in the fallback decoding path. The enriched lemmas perform marginally better. 2.3 Two-step translation Our two-step translation is essentially the same setup as detailed by Bojar and Kos (2010): (1) the English source is translated to simplified Czech, and (2) the simplified Czech is monotonically translated to fully inflected Czech. Both steps are simple phrase-based models. Instead of word forms, the simplified Czech uses lemmas enriched by a subset of morphological features selected manually to encode only properties overt both in English and Czech su"
W11-2152,H05-1066,0,0.0673478,"Missing"
W11-2152,J03-1002,0,0.00353942,"ical example of a correction is the agreement between the subject and the predicate: they should share the morphological number and gender. If they do not, we simply change the number and gender of the predicate in agreement with the subject.4 An example of such a changed predicate is in Figure 1. Apart from the dependency tree of the target sentence, we can also use the dependency tree of the source sentence. Source sentences are grammatically correct and the accuracy of the tagger and the parser is accordingly higher there. Words in the source and target sentences are aligned using GIZA++5 (Och and Ney, 2003) but verbose outputs of the original MT systems would be possibly a better option. The rules for fixing grammatical agreement between words can thus consider also the dependency relations and morphological caregories of their English counterparts in the input sentence. 4 In this case, we suppose that the number of the subject has a much higher chance to be correct. 5 GIZA++ was run on lemmatized texts in both directions and intersection symmetrization was used. 428 . AuxK came Pred people pl Sb Some pl Atr later Adv . AuxK přišel lidé Někteří Pred sg, m Sb pl Atr později Adv přišli Pred pl Fig"
W11-2152,W07-1709,0,0.109407,"Missing"
W11-2152,N07-1064,0,\N,Missing
W11-2153,P05-1022,0,0.248854,"g to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU."
W11-2153,W07-2416,0,0.584308,"might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 However, the parser bottleneck of the dependency-based MT approach was observed also by other researchers (R"
W11-2153,P03-1054,0,0.0229106,"have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence of the selected parsers on the MT quality in terms of BLEU. Section 5 concludes. 2 Howeve"
W11-2153,W10-1730,1,0.897432,"Missing"
W11-2153,E06-1011,0,0.0143919,"ion that, n:sb – semantic noun in a subject position, n:for+X – semantic noun in a prepositional group introduced with preposition for, adj:attr – semantic adjective in an attributive position. 435 Involved Parsers We performed experiments with parsers from three families: graph-based parsers, transitionbased parsers, and phrase-structure parsers (with constituency-to-dependency postprocessing). 3.1 Graph-based Parser In graph-based parsing, we learn a model for scoring graph edges, and we search for the highest-scoring tree composed of the graph’s edges. We used Maximum Spanning Tree parser (Mcdonald and Pereira, 2006) which is capable of incorporating second order features (MST for short). 3.2 Transition-based Parsers Transition-based parsers utilize the shift-reduce algorithm. Input words are put into a queue and consumed by shift-reduce actions, while the output parser is gradually built. Unlike graph-based parsers, transition-based parsers have linear time complexity and allow straightforward application of non-local features. We included two transition-based parsers into our experiments: • Malt – Malt parser introduced by Nivre et al. (2007) 6 5 http://ucnk.ff.cuni.cz We used stackeager algorithm, libl"
W11-2153,H05-1066,0,0.288682,"Missing"
W11-2153,W07-1709,0,0.0270164,"Missing"
W11-2153,P07-1031,0,0.0148678,"d be parsed independently of the rest of the sentence. This was shown to improve not only parsing accuracy of the parenthesed word sequence (which is forced to remain in one subtree), but also the rest of the sentence.10 In our experiments, SentChunk is used only in combination with the three genuine dependency parsers. 4 Experiments and Evaluation 4.1 Data for Parsers’ Training and Evaluation The dependency trees needed for training the parsers and evaluating their UAS were created from the Penn Treebank data (enriched first with internal noun phrase structure applied via scripts provided by Vadas and Curran (2007)) by Penn Converter (Johansson and Nugues, 2007) with the -conll2007 option (PennConv for short). All the parsers were evaluated on the same data – section 23. All the parsers were trained on sections 02–21, except for the Stanford parser which was trained on sections 01–21. We were able to retrain the parser models only for MST and Malt. For the other parsers we used pretrained models available on the Internet: CJ’s default model ec50spfinal, Stanford’s wsjPCFG.ser.gz model, and 10 Edge length is a common feature in dependency parsers, so “deleting” parenthesed words may give higher scores to"
W11-2153,P09-2037,1,0.946992,"ependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors have already been addressed by improvements based on Hidden Markov Tree Models for lexical and syntactic choice as shown by ˇ Zabokrtsk´ y and Popel (2009), and by context sensitive translation models based on maximum entropy as described by Mareˇcek et al. (2010). Our study proceeds along two directions. First, we train two state-of-the-art dependency parsers on training sets with varying size. Second, we use five parsers based on different parsing techniques. In both cases we document the relation between parsing accuracy (in terms of Unlabeled Attachment Score, UAS) and translation quality (estimated by the well known BLEU metric). The motivation behind the first set of experiments is that we can extrapolate the learning curve and try to pred"
W11-2153,W08-0325,1,0.950709,"study the relationship between parsing accuracy in terms of unlabeled attachment score and machine translation quality in terms of BLEU. 1 Introduction In the last years, statistical n-gram models dominated the field of Machine Translation (MT). However, their results are still far from perfect. Therefore we believe it makes sense to investigate alternative statistical approaches. This paper is focused on an analysis-transfer-synthesis translation system called TectoMT whose transfer representation has a shape of a deep-syntactic dependency tree. The system has ˇ been introduced by Zabokrtsk´ y et al. (2008). The translation direction under consideration is Englishto-Czech. It has been shown by Popel (2009) that the current accuracy of the dependency parser employed in this translation system is one of the limiting factors from the viewpoint of its output quality. In other words, the parsing phase is responsible for a large portion of translation errors. The biggest source of translation errors in the referred study was (and probably still is) the transfer phase, however the proportion has changed since and the relative importance of the parsing phase has grown, because the tranfer phase errors h"
W11-2153,P11-2033,0,0.167717,"of pronouns (personal and possessive pronouns referring to the clause’s subject should have reflexive forms in Czech). Thus it is obvious that the parser choice is important and that it might not be enough to choose a parser, for machine translation, only according to its UAS. Due to growing popularity of dependency syntax in the last years, there are a number of dependency parsers available. The present paper deals with five parsers evaluated within the translation framework: three genuine dependency parsers, namely the parsers described in (McDonald et al., 2005), (Nivre et al., 2007), and (Zhang and Nivre, 2011), and two constituency parsers (Charniak and Johnson, 2005) and (Klein and Manning, 2003), whose outputs were converted to dependency structures by Penn Converter (Johansson and Nugues, 2007). As for the related literature, there is no published study measuring the influence of dependency parsers on dependency-based MT to our knowledge.2 The remainder of this paper is structured as follows. The overall translation pipeline, within which the parsers are tested, is described in Section 2. Section 3 lists the parsers under consideration and their main features. Section 4 summarizes the influence"
W11-2153,zhang-etal-2004-interpreting,0,0.0936613,"Missing"
W11-3901,N09-1012,0,0.10418,"nguages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate"
W11-3901,P04-1061,0,0.226422,", stressing especially the treeness constraint. The best configuration is then applied to 19 languages from CoNLL-2006 and CoNLL-2007 shared tasks. Our best achieved results are comparable to the state of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buch"
W11-3901,H05-1066,0,0.1583,"Missing"
W11-3901,D10-1118,0,0.614386,"d Dependency Parsing ˇ David Mareˇcek and Zdenˇek Zabokrtsk´ y Charles University in Prague, Faculty of Mathematics and Physics Institute of Formal and Applied Linguistics {marecek,zabokrtsky}@ufal.mff.cuni.cz Abstract The attachment scores are very high for English, for which the methods seems to be optimized, but the scores are quite low for some other languages. In this paper, we describe our new approach to unsupervised dependency parsing. Unlike DMV, it is not based on constituency trees, which cannot handle non-projectivities. We have been inspired rather by the experiment described in (Brody, 2010), in which the dependency parsing task is formulated as a problem of word alignment; every sentence is aligned with itself with one constraint: no word can be attached to itself. However, unlike (Brody, 2010), where the output structures might not be trees and could contain cycles, we introduce a sampling method with the acyclicity constraint. Our approach attempts at optimizing the overall probability of tree structures given the corpus. We perform the optimization using Gibbs sampling (Gilks et al., 1996). We employ several ways of incorporating prior knowledge about dependency trees into th"
W11-3901,P11-1067,0,0.0193213,"was measured both for CoarsePOS and FinePOS tags and evaluated with all three measures. We can see that CoarsePOS tags work better if we do not use SingleRoot constraint or NounRoot model. Adding NounRoot model improves the UAS by 8 percent. We choose the settings of the experiment number 10 (which uses all our improvements and constraints) as the best configuration for Czech. It has the highest UUAS score and the values of the other scores are very close to the maximum achieved values. • UUAS - undirected UAS (edge direction is disregarded), 6.4 • NED - neutral edge direction, introduced in (Schwartz et al., 2011), which treats not only a node’s gold parent and child as the correct answer, but also its gold grandparent. Learning curves It is useful to draw learning curves in order to see how well the learning algorithm can exploit additional data. Figure 4 shows the speed of growth of UAS for our best unsupervised configuration in 5 n. Initialization Tags Models Baseline configuration: 1 Random CoarsePOS Dep+Dist 2 Random FinePOS Dep+Dist Parsing with Maximum spanning tree algorithm: 3 Random CoarsePOS Dep+Dist 4 Random FinePOS Dep+Dist Using tree-sampling: 5 RandomTree CoarsePOS Dep+Dist 6 RandomTree"
W11-3901,W06-2920,0,0.80465,"004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature selection – we use words’ distance and their POS tags as the main indicators for predicting a dependency relation, • hard constraints – some knowledge on dependency tree properties (such as acyclicity) is difficult to represent by local models, therefore we implement it as a hard constraint in the sampling procedure, • corpus initialization – we study the effect of different initializations of trees in the corpus, 1 Robust Uns"
W11-3901,D11-1117,0,0.152154,"ate of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature select"
W11-3901,W11-0303,0,0.148644,"ate of the art in dependency parsing and outperform the previously published results for many languages. 1 Introduction Unsupervised approaches receive considerably growing attention in NLP in the last years, and dependency parsing is not an exception. In recent years, quite a lot of works in unsupervised parsing (or grammar induction) was based on Dependency Model with Valence (DMV) introduced by (Klein and Manning, 2004); (Smith, 2007) and (Cohen et al., 2008) has focused on DMV variants, (Headden et al., 2009) introduced extended valency model (EVG) and added lexicalization and smoothing. (Spitkovsky et al., 2011b) used punctuation marks for splitting a sentence and impose parsing restrictions over its fragments. Gibbs sampling was used in (Naseem and Barzilay, 2011). Some of the papers focused on English only, but some presented the results across wide rage of languages. The last such paper was (Spitkovsky et al., 2011a), where the evaluation was done on all 19 languages included in CoNLL shared tasks (Buchholz and Marsi, 2006) and (Nivre et al., 2007). • independence assumptions – we approximate probability of a tree by a product of probabilities of dependency edges, • edge models and feature select"
W11-3901,J93-2003,0,\N,Missing
W11-3901,D07-1096,0,\N,Missing
W12-1911,N09-1012,0,0.137884,"Missing"
W12-1911,P04-1061,0,0.123137,"(4) where ti is part-of-speech tag of the word on the position i, c−i (“ti , fi ”) stands for the count of words with PoS tag ti and fertility fi in the history, and P0 is a prior probability for the given fertility which depends on the total number of node dependents denoted by |fi |(the sum of numbers of left and right dependents): P0 (fi ) = 1 2|fi |+1 (5) This prior probability has a nice property: for a given number of nodes, the product of fertility probabilities over all the nodes is equal for all possible dependency trees. This ensures balance of this model during inference. 5 In DMV (Klein and Manning, 2004) and in the extended model EVG (Headden III et al., 2009), there is a STOP sign indicating that no more dependents in a given direction will be generated. Given a certain head, all its dependents in left direction are generated first, then the STOP sign in that direction, then all its right dependents and then STOP in the other direction. This process continues recursively for all generated dependents. 6 For example, fertility “1-3” means that the node has one left and three right dependents, fertility “0-0” indicates that it is a leaf. 7 If a specific fertility has been frequent for a given P"
W12-1911,majlis-zabokrtsky-2012-language,1,0.867069,"Missing"
W12-1911,D11-1117,0,0.0206557,"e occurs at a particular location in the corpus. This counts are collected over the whole corpus with the collection-rate 0.01.12 When the sampling is finished, the final dependency trees are built using such edges that belonged to the most frequent ones during the sampling. We employ the maximum spanning tree (MST) algorithm (Chu and Liu, 1965) to find them.13 Tree projectivity is not guaranteed by the MST algorithm. 5 Experiments We evaluated our parser on 10 treebanks included in the WILS shared-task data. Similarly to some previous papers on unsupervised parsing (Gillenwater et al., 2011; Spitkovsky et al., 2011), the tuning experiments were performed on English only. We used 12 11 dog)(was) in ((the) dog) was (in)((the) dog) was) in ((the) dog) was (in ((the) dog) was (in ((the) After each small change is made, the edges from the whole corpus are collected with a probability 0.01. 13 The weights of edges needed in MST algorithm correspond to the number of times they were present during the sampling. language tokens (mil.) Arabic 19.7 Basque 14.1 Czech 20.3 Danish 15.9 Dutch 27.1 language tokens (mil.) English 85.0 Portuguese 31.7 Slovenian 13.7 Swedish 19.2 αe = 0.01, Table 2: Wikipedia texts statist"
W12-1911,A00-1031,0,\N,Missing
W12-3132,hajic-etal-2012-announcing,1,0.802661,"Missing"
W12-3132,W04-3250,0,0.264493,"Missing"
W12-3132,W06-1606,0,0.0610868,"Missing"
W12-3132,W10-1730,1,0.899159,"Missing"
W12-3132,W01-1406,0,0.0190747,"actic description, mainly within valency lexicons, starting probably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysi"
W12-3132,P02-1040,0,0.0829475,". 6.1 Czech Synthesis The synthesis phase of the TectoMT system relies heavily on the information included in formemes, as its rule-based blocks use solely formemes and grammar rules to gradually change a deep tree node into a surface subtree. To directly measure the suitability of our changes for the synthesis stage of the TectoMT system, we used a Czech-to-Czech round trip—deep analysis of Czech PDT 2.0 development set sentences using the CzEng 1.0 pipeline (Bojar et al., 2012b), followed directly by the synthesis part of the TectoMT system. The results were evaluated using the BLEU metric (Papineni et al., 2002) with the original sentences as reference; they indicate a higher suitability of the new formemes for deep Czech synthesis (see Table 2). 6.2 Version Original formemes Revised formemes BLEU 0.6818 0.7092 Table 2: A comparison of formeme versions in Czech-toCzech round trip. Version Original formemes Revised formemes BLEU 0.1190 0.1199 Table 3: A comparison of formeme versions in Englishto-Czech TectoMT translation on the WMT12 test set. two translation scenarios—one using the original formemes and the second using the revised formemes in the formeme-to-formeme translation model. Due to time re"
W12-3132,W11-2153,1,0.890651,"Missing"
W12-3132,P05-1034,0,0.0232767,"robably with the work by Helbig and Schenkel (1969). Perhaps the best one for Czech is PDT-VALLEX (Hajiˇc et al., 2003), listing all possible subtrees corresponding to valency arguments (Urešová, 2009). Žabokrtský (2005) gives an overview of works in this field. 267 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 267–274, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics This kind of information has been most exploited in structural MT systems, employing semantic relations (Menezes and Richardson, 2001) or surface tree substructures (Quirk et al., 2005; Marcu et al., 2006). Formemes, originally developed for Natural Language Generation (NLG) (Ptáˇcek and Žabokrtský, 2006), have been successfully applied to MT within the TectoMT system. Our revision of formeme annotation aims to improve the MT performance, keeping other possible applications in mind. 3 The TectoMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory"
W12-3132,W08-0325,1,0.921231,"Missing"
W12-3132,P09-2037,1,0.875964,"oMT English-Czech Machine Translation System The TectoMT system is a structural machine translation system with deep transfer, first introduced by Žabokrtský et al. (2008). It currently supports English-to-Czech translation. Its analysis stage follows the Prague tectogrammatics theory (Sgall, 1967; Sgall et al., 1986), proceeding over two layers of structural description, from shallow (analytical) to deep (tectogrammatical) (see Section 3.1). The transfer phase of the system is based on Maximum Entropy context-sensitive translation models (Mareˇcek et al., 2010) and Hidden Tree Markov Models (Žabokrtský and Popel, 2009). It is factorized into three subtasks: lemma, formeme and grammatemes translation (see Sections 3.2 and 3.3). The subsequent generation phase consists of rulebased components that gradually change the deep target language representation into a shallow one, which is then converted to text (cf. Section 6.1). The version of TectoMT submitted to WMT122 builds upon the WMT11 version. Several rule-based components were slightly refined. However, most of the effort was devoted to creating a better and bigger parallel treebank—CzEng 1.03 (Bojar et al., 2012b), and re-training the statistical componen"
W12-3132,W09-0422,1,\N,Missing
W12-3132,2001.mtsummit-ebmt.4,0,\N,Missing
W12-3132,bojar-etal-2012-joy,1,\N,Missing
W12-3146,hajic-etal-2012-announcing,0,0.0154183,"Missing"
W12-3146,W11-2103,0,0.036468,"ced the rule set used by the original DEPFIX system and measured the performance of the individual rules. We also modified the dependency parser of McDonald et al. (2005) in two ways to adjust it for the parsing of MT outputs. We show that our system is able to improve the quality of the state-of-the-art MT systems. 1 Introduction The today’s outputs of Machine Translation (MT) often contain serious grammatical errors. This is particularly apparent in statistical MT systems (SMT), which do not employ structural linguistic rules. These systems have been dominating the area in the recent years (Callison-Burch et al., 2011). Such errors make the translated text less fluent and may even lead to unintelligibility or misleading statements. The problem is more evident in languages with rich morphology, such as Czech, where morphological agreement is of a relatively high importance for the interpretation of syntactic relations. The DEPFIX system (Mareˇcek et al., 2011) attempts to correct some of the frequent SMT sys∗ This research has been supported by the European Union Seventh Framework Programme (FP7) under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310, GA201/09/H057 (Res-Informatica), and LH120"
W12-3146,2009.mtsummit-commercial.6,0,0.0396722,"Missing"
W12-3146,P07-2045,0,0.00498126,"atures computed over its aligned source word, if there is one. To address the differences between the gold standard training data and SMT outputs, we “worsen” the treebank used to train the parser, i.e. introduce errors similar to those found in target sentences: The trees retain their correct structure, only the word forms are modified to resemble SMT output. We have computed a “part-of-speech tag error model” on parallel sentences from the Prague Czech-English Dependency Treebank (PCEDT) 2.0 (Bojar et al., 2012), comparing the gold standard Czech translations to the output of an SMT system (Koehn et al., 2007) and estimating the Maximum Likelihood probabilities of errors for each part-ofspeech tag. We then applied this error model to the Czech PCEDT 2.0 sentences and used the resulting “worsened” treebank to train the parser. 4 Rules 2012 uses 20 hand-written rules, addressing various frequent errors in MT output. Each rule takes an analyzed target sentence as its input, often together with its analyzed source senDEPFIX 2 http://ufal.mff.cuni.cz/treex 363 tence, and attempts to correct any errors found – usually by changing morphosyntactic categories of a word (such as number, gender, case, person"
W12-3146,N09-2055,0,0.298447,"Missing"
W12-3146,H05-1066,0,0.00799007,"Missing"
W12-3146,J03-1002,0,0.0030145,"ors induced by this type of MT systems, it can be applied to virtually any MT system (Mareˇcek et al., 2011). 362 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 362–368, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics TectoMT/Treex NLP framework (Popel and ˇ Zabokrtsk´ y, 2010),2 using the Morˇce tagger (Spoustov´a et al., 2007) and the MST parser (McDonald et al., 2005) trained on the CoNLL 2007 Shared Task English data (Nivre et al., 2007) to analyze the source sentences. The source and target sentences are aligned using GIZA++ (Och and Ney, 2003). 3 Parsing The DEPFIX 2011 system used the MST parser (McDonald et al., 2005) with an improved feature set ˇ for Czech (Nov´ak and Zabokrtsk´ y, 2007) trained on the Prague Dependency Treebank (PDT) 2.0 (Hajiˇc and others, 2006) to analyze the target sentences. DEPFIX 2012 uses a reimplementation of the MST parser capable of utilizing parallel features from the source side in the parsing of the target sentence. The source text is usually grammatical and therefore is likely to be analyzed more reliably. The source structure obtained in this way can then provide hints for the target parser. We"
W12-3146,P02-1040,0,0.104954,"Missing"
W12-3146,W07-1709,0,0.0676744,"Missing"
W12-3146,W11-2152,1,\N,Missing
W12-3146,N07-1064,0,\N,Missing
W12-3146,D07-1096,0,\N,Missing
W12-4205,hajic-etal-2012-announcing,0,0.0852801,"Missing"
W12-4205,W06-2920,0,0.0367148,"targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in"
W12-4205,N10-1015,0,0.0755019,"011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,W10-1703,0,0.0245956,"parser training data, so that the training sentences resemble SMT output. We evaluate the modified parser on DEP FIX , a system that improves English-Czech SMT outputs using automatic rule-based corrections of grammatical mistakes which requires parsed SMT output sentences as its input. Both parser modifications led to improvements in BLEU score; their combination was evaluated manually, showing a statistically significant improvement of the translation quality. 1 Introduction The machine translation (MT) quality is on a steady rise, with mostly statistical systems (SMT) dominating the area (Callison-Burch et al., 2010; CallisonBurch et al., 2011). Most MT systems do not employ structural linguistic knowledge and even the stateof-the-art MT solutions are unable to avoid making serious grammatical errors in the output, which often leads to unintelligibility or to a risk of misinterpretations of the text by a reader. ∗ This research has been supported by the EU Seventh Framework Programme under grant agreement n◦ 247762 (Faust), and by the grants GAUK116310 and GA201/09/H057. This problem is particularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the"
W12-4205,W11-2103,0,0.0450256,"Missing"
W12-4205,P10-1003,0,0.0232916,"ek et al., 2011), which we outline in Section 5. We describe the experiments carried out and present the most important results in Section 6. Section 7 then concludes the paper and indicates more possibilities of further improvements. 2 Related Work Our approach to parsing with parallel features is similar to various works which seek to improve the parsing accuracy on parallel texts (“bitexts”) by using information from both languages. Huang et al. (2009) employ “bilingual constraints” in shiftreduce parsing to disambiguate difficult syntactic constructions and resolve shift-reduce conflicts. Chen et al. (2010) use similar subtree constraints to improve parser accuracy in a dependency scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT"
W12-4205,P99-1065,0,0.316623,"Missing"
W12-4205,P08-2056,0,0.0358,"Missing"
W12-4205,D09-1127,0,0.0558467,"Missing"
W12-4205,P07-2045,0,0.0114404,"g one, creating the respective alignment link from word A (in the reference) to word B (in the SMT output) and deleting all scores of links from A or to B, so that one-to-one alignments are enforced. The process is terminated when no links with a score higher than a given threshold are available; some words may thus remain unaligned. The score is computed as a linear combination of the following four features: • word form (or lemma if available) similarity based on Jaro-Winkler distance (Winkler, 1990), 1. We translated the English side of PCEDT5 to Czech using SMT (we chose the Moses system (Koehn et al., 2007) for our experiments) and tagged the resulting translations using the Morˇce tagger (Spoustov´a et al., 2007). • fine-grained morphological tag similarity, • similarity of the relative position in the sentence, 2. We aligned the Czech side of PCEDT, now serving as a reference translation, to the SMT output using our Monolingual Greedy Aligner (see Section 4.2). 3. Collecting the counts of individual errors, we estimated the Maximum Likelihood probabilities of changing a correct fine-grained morphological tag (of a word from the reference) into a possibly incorrect fine-grained morphological ta"
W12-4205,J93-2004,0,0.0469362,"er. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR"
W12-4205,H05-1066,0,0.30865,"Missing"
W12-4205,W06-2932,0,0.0233358,"entence first and include features computed over the parsed source sentence in the set of features used for parsing SMT output. We first align the source and SMT output sentences on the word level and then use alignment-wise local features – i.e. for each SMT output word, we add features computed over its aligned source word, if applicable (cf. Section 3.4 for a listing). 3.2 Parsers Used We have reimplemented the MST parser (McDonald et al., 2005) in order to provide for a simple insertion of the parallel features into the models. We also used the original implementation of the MST parser by McDonald et al. (2006) for comparison in our experiments. To distinguish the two variants used, we denote the original MST parser as M C D parser,2 and the new reimplementation as RUR parser. We trained RUR parser in a first-order nonprojective setting with single-best MIRA. Dependency labels are assigned in a second stage by a 2 M C D uses k-best MIRA, does first- and second-order parsing, both projectively and non-projectively, and can be obtained from http://sourceforge.net/projects/ mstparser. 41 MIRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the"
W12-4205,D11-1006,0,0.0305903,"010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and"
W12-4205,J03-1002,0,0.0141147,"IRA-based labeler, which has been implemented according to McDonald (2006) and Gimpel and Cohen (2007). We used the Prague Czech-English Dependency Treebank3 (PCEDT) 2.0 (Bojar et al., 2012) as the training data for RUR parser – a parallel treebank created from the Penn Treebank (Marcus et al., 1993) and its translation into Czech by human translators. The dependency trees on the English side were converted from the manually annotated phrasestructure trees in Penn Treebank, the Czech trees were created automatically using M C D. Words of the Czech and English sentences were aligned by GIZA++ (Och and Ney, 2003). We apply RUR parser only for SMT output parsing; for source parsing, we use M C D parser trained on the English CoNLL 2007 data (Nivre et al., 2007), as the performance of this parser is sufficient for this task. 3.3 Monolingual Features The set of monolingual features used in RUR parser follows those described by McDonald et al. (2005). For parsing, we use the features described below. The individual features are computed for both the parent node and the child node of an edge and conjoined in various ways. The coarse morphological tag and lemma are provided by the Morˇce tagger (Spoustov´a"
W12-4205,P02-1040,0,0.0832465,"Missing"
W12-4205,W07-1709,0,0.0974333,"Missing"
W12-4205,stymne-ahrenberg-2010-using,0,0.0223864,"icularly apparent in target languages with rich morphological inflection, such as Czech. As Czech often conveys the relations between individual words using morphological agreement instead of word order, together with the word order itself being relatively free, choosing the correct inflection becomes crucial. Since the output of phrase-based SMT shows frequent inflection errors (even in adjacent words) due to each word belonging to a different phrase, a possible way to address the grammaticality problem is a combination of statistical and structural approach, such as SMT output post-editing (Stymne and Ahrenberg, 2010; Mareˇcek et al., 2011). In this paper, we focus on improving SMT output parsing quality, as rule-based post-editing systems rely heavily on the quality of SMT output analysis. Parsers trained on gold standard parse trees often fail to produce the expected result when applied to SMT output with grammatical errors. This is partly caused by the fact that when parsing highly inflected free word-order languages the parsers have to rely on morphological agreement, which, as stated above, is often erroneous in SMT output. Training a parser specifically by creating a manually annotated treebank of M"
W12-4205,J97-3002,0,0.138024,"ation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual parsing aim to parse the parallel sentences directly using a grammar formalism fit for this purpose, such as Inversion Transduction Grammars (ITG) (Wu, 1997). Burkett et al. (2010) further include ITG parsing with wordalignment in a joint scenario. We concentrate here on using dependency parsers because of tools and training data availability for the examined language pair. Regarding treebank adaptation for parser robustness, Foster et al. (2008) introduce various kinds of artificial errors into the training data to make the final parser less sensitive to grammar errors. However, their approach concentrates on mistakes made by humans (such as misspellings, word repetition or omission etc.) and the error models used are handcrafted. Our work focuse"
W12-4205,P10-1062,0,0.0211229,"ncy scenario. Chen et al. (2011) then improve the method by obtaining a training parallel treebank via SMT. In recent work, Haulrich (2012) experiments with a setup very similar to ours: adding alignment-projected features to an originally monolingual parser. However, the main aim of all these works is to improve the parsing accuracy on correct parallel texts, i.e. human-translated. This paper applies similar methods, but with a different objective in mind – increasing the ability of the parser to process ungrammatical SMT output sentences and, ultimately, improve rule-based SMT post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDo"
W12-4205,I08-3008,0,0.0945642,"post-editing. Xiong et al. (2010) use SMT parsing in translation quality assessment, providing syntactic features to a classifier detecting erroneous words in SMT output, yet they do not concentrate on improving parsing accuracy – they employ a link grammar parser, which 1 The abbreviation “RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences"
W12-4205,P09-1007,0,0.0247054,"“RUR” parser stands for “Rudolph’s Universal Robust” parser. 40 is robust, but not tuned specifically to process ungrammatical input. There is also another related direction of research in parsing of parallel texts, which is targeted on parsing under-resourced languages, e.g. the works by Hwa et al. (2005), Zeman and Resnik (2008), and McDonald et al. (2011). They address the fact that parsers for the language of interest are of low quality or even non-existent, whereas there are highquality parsers for the other language. They exploit common properties of both languages and delexicalization. Zhao et al. (2009) uses information from word-by-word translated treebank to obtain additional training data and boost parser accuracy. This is different from our situation, as there exist high performance parsers for Czech (Buchholz and Marsi, 2006; Nivre et al., 2007; Hajiˇc et al., 2009). Boosting accuracy on correct sentences is not our primary goal and we do not intend to replace the Czech parser by an English parser; instead, we aim to increase the robustness of an already existing Czech parser by adding knowledge from the corresponding English source, parsed by an English parser. Other works in bilingual"
W12-4205,W11-2152,1,\N,Missing
W12-4205,D11-1007,0,\N,Missing
W12-4205,W09-1201,0,\N,Missing
W12-4205,D07-1096,0,\N,Missing
W12-4205,W10-1705,0,\N,Missing
W16-2318,H05-1098,0,0.0439267,"mpty>. All such nodes representing a single word without any counterpart are leaf nodes. This ensures that the merged tree structure can be simply divided into two monolingual trees, not including empty nodes. The two separated trees are also “internally” isomorfic, the only differences are in leaves. The annotation style of Universal Dependencies is suitable for the merged tree structures, since majority of function words are annotated as leaves there. Function words are the ones which often cannot be translated as one-to-one. For example, Introduction Tree-based machine translation systems (Chiang et al., 2005; Duˇsek et al., 2012; Sennrich and Haddow, 2015) are alternatives to the leading phrasebased MT systems (Koehn et al., 2007) and newly very progressive neural MT systems (Bahdanau et al., 2015). Our approach aims to produce bilingual dependency trees, in which both source and target sentences are encoded together. We adapt the Universal Dependencies annotation style (Nivre et al., 2016), in which the functional words1 are in 1 Functional words are determiners, prepositions, conjunctions, auxiliary verbs, particles, etc. 333 Proceedings of the First Conference on Machine Translation, Volume 2:"
W16-2318,W12-3132,1,0.898313,"Missing"
W16-2318,P07-2045,0,0.0145034,"Missing"
W16-2318,J97-3002,0,0.693375,"Missing"
W16-2318,J93-2004,0,0.0547743,"iliary verbs are also used differently across languages. 3 Data The parallel data we use in the experiments is the training part of the Czech-English parallel corpus CzEng 1.0 (Bojar et al., 2012). It consists of more than 15 million sentences, 206 million tokens on the Czech side and 232 million tokens on the English side. We extract the parallel sentences with original tokenization from the CzEng exportformat together with the part-of-speech (POS) tags and the word alignment. The original CzEng POS tags, Prague Dependency Treebank tags (Hajiˇc et al., 2006) for Czech and Penn Treebank tags (Marcus et al., 1993) for English, are mapped to the universal POS tagset developed for Universal Dependencies (Nivre et al., 2016). The simple 1-to-1 mapping was taken from the GitHub repository.2 The POS tags used in Universal Dependencies POS are listed in Table 1. 4 Input: enF, enT, csF, csT : arrays of forms and tags of the English and Czech sentence Input: en2csAlign, cs2enAlign: unidirectional alignment links between English and Czech Output: mrgF, mrgT : arrays of form and tags of the merged sentence k = 0; foreach i ∈ {1, · · · , |enF |} do used = 0; foreach j ∈ {1, · · · , |csF |} do if cs2enAlign[i] 6="
W16-2318,W10-1730,1,0.859169,"Missing"
W16-2318,P13-1028,1,0.8498,"Missing"
W16-2318,H05-1066,0,0.243213,"Missing"
W16-2318,L16-1262,0,0.0449116,"Missing"
W16-2318,J03-1002,0,0.0116703,"arrays of form and tags of the merged sentence k = 0; foreach i ∈ {1, · · · , |enF |} do used = 0; foreach j ∈ {1, · · · , |csF |} do if cs2enAlign[i] 6= j then continue; k++; if en2csAlign[j] = i then mrgF [k] = enF [i] + ’ ’ + csF [j]; mrgT [k] = enT [i] + ’ ’ + csT [j]; used = 1; else mrgF [k] = ’<empty> ’ + csF [j]; mrgT [k] = ’<empty> ’ + csT [j]; end end if used = 0 then k++; mrgF [k] = enF [i] + ’ <empty>’; mrgT [k] = enT [i] + ’ <empty>’; end end return mrgF, mrgT ; Figure 1: Merging algorithm pseudocode. word.3 These alignment links are direct outputs from GIZA++ word-alignment tool (Och and Ney, 2003) before symmetrization. The algorithm traverses through the source sentence and for each word, it collects all its target counterparts using the cs2enAlign4 The Czech word, where the cs2enAlign and en2csAlign intersect, creates the word pair with the English one. The other Czech words stay alone and are completed with the <empty> label. If there is no intersection counterpart for the English word, it is also completed with the <empty> label. Figure 2 shows one example of merging. The pairs of words connected by both cs2enAlign Merging algorithm Parallel sentences tagged by the universal POS ta"
W16-2318,P02-1040,0,0.0937911,"Missing"
W16-2318,D15-1248,0,0.0180628,"e word without any counterpart are leaf nodes. This ensures that the merged tree structure can be simply divided into two monolingual trees, not including empty nodes. The two separated trees are also “internally” isomorfic, the only differences are in leaves. The annotation style of Universal Dependencies is suitable for the merged tree structures, since majority of function words are annotated as leaves there. Function words are the ones which often cannot be translated as one-to-one. For example, Introduction Tree-based machine translation systems (Chiang et al., 2005; Duˇsek et al., 2012; Sennrich and Haddow, 2015) are alternatives to the leading phrasebased MT systems (Koehn et al., 2007) and newly very progressive neural MT systems (Bahdanau et al., 2015). Our approach aims to produce bilingual dependency trees, in which both source and target sentences are encoded together. We adapt the Universal Dependencies annotation style (Nivre et al., 2016), in which the functional words1 are in 1 Functional words are determiners, prepositions, conjunctions, auxiliary verbs, particles, etc. 333 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 333–338, c Berlin, Ger"
W16-6401,W10-1705,1,0.929434,"oses + Moses post-editing, simple Moses + Moses post-editing, TwoStep Google Translate + TectoMT post-editing Moses + TectoMT post-editing § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setup"
W16-6401,W15-3006,1,0.779365,"nslation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been"
W16-6401,W12-3130,1,0.860797,"s, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd Deep Machine Translation Workshop (DMTW 2016), pages 1–10, Lisbon, Portugal, 21 October 2016. Figure 1: TectoMT 2.1.1 Factored Moses In the more recent experiments that we report, the Moses system used is actually the Factored Moses of Bojar et al. (2012). It translates the source English text into a factored representation of Czech, where each word is represented by a tuple of a word form and a corresponding part-of-speech (PoS) tag. This enables Moses to use an additional language model which operates on PoS tags instead of word forms. This helps overcome data sparsity issues of the word-based language model and thus leads to a higher output quality, especially to its better grammaticality. Factored Moses is trained on parallel corpora pre-analyzed by Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically"
W16-6401,W13-2208,1,0.933672,"to perform forced translation of identified named entities, using a named entity recognizer and a bilingual gazetteer, as well as forced nontranslation of special structures (URLs, e-mail addresses, computer commands and filenames); Moses XML annotation is used to preserve the forcedly translated items.12 Apart from domain adaptation, simpler general Treex pre- and post-processing steps were also successfully used, such as projection of letter case in identical words from source to target. 3.7 Two-headed Chimera: Moses with Additional TectoMT Phrase-table The Two-headed Chimera or AddToTrain (Bojar et al., 2013b; Bojar and Tamchyna, 2015)is a combination of full TectoMT with full Moses (see Figure 9). First, the input is translated by TectoMT. TectoMT translations are then joined with the input to create a small synthetic parallel corpus, from which a secondary phrase table is extracted. This is then used together with the primary phrase table, extracted from the large training data, to train Moses. Finally, the input is translated by the resulting Moses system. This setup enables Moses to use parts of the TectoMT translation that it considers good, while still having the base large phrase table at"
W16-6401,W15-3009,1,0.853769,"y Treex. 2.2 Treex Treex2 (Popel and Žabokrtský, 2010; Žabokrtský, 2011) is a linguistically motivated NLP framework. It consists of a large number of smaller components performing a specific NLP-task (blocks), both Treex-specific as well as Treex-wrapped external tools, which can be flexibly combined into processing pipelines. Sentences are represented by surface and deep syntactic dependency trees, richly annotated with numerous linguistic attributes, similarly to the Prague Dependency Treebank (Hajiˇc, 1998). 2.2.1 TectoMT The main application of Treex is TectoMT3 (Žabokrtský et al., 2008; Dušek et al., 2015), a linguistically motivated hybrid machine translation system. Its pipieline consists of three main steps: analysis of each source sentence up to t-layer (a deep syntactic representation of the sentence in a labelled dependency t-tree), transfer of the source t-tree to the target t-tree (i.e., the translation per se), and generation of the target sentence from the target t-tree (see Figure 1). The transfer is performed by copying the t-tree structure and grammatemes4 (attributes describing grammatical meaning) from source, and predicting target lemmas and formemes5 (deep morphosyntactic attri"
W16-6401,W12-3132,1,0.90614,"Missing"
W16-6401,W13-2216,1,0.89877,"Missing"
W16-6401,P07-2045,1,0.0127177,"rtcomings cancel out. In our paper, we review a set of such attempts, performed with Moses, a prominent representative of the PB-SMT systems, and Treex, a linguistically motivated NLP framework, featuring, among other, a full-fledged deep syntactic MT system, TectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to othe"
W16-6401,W09-0424,0,0.0237791,"u-plain Moses output downloaded from http://matrix.statmt.org/systems/show/2807, test set downloaded from http://matrix.statmt.org/test_sets/list. 10 Figure 8: Moses with TectoMT pre- and post-processing 6 Figure 9: Two-headed Chimera Zeman (2010) used several pre-processing steps to make the source English text more similar to Czech, such as removing articles, marking subjects by artificial suffixes (“/Sb”), and reordering auxiliary verbs to neighbor their main verbs. Of course, the SMT system was also trained on texts preprocessed in that way; in these experiments, the Joshua PB-SMT system (Li et al., 2009) was used instead of Moses. This approach may seem too aggressive, prone to making the input noisier as well as being potentially lossy. However, the author showed that with careful selection and tuning of the pre-processing steps, a significant improvement of translation quality can be achieved; moreover, this was also confirmed on English-to-Hindi translation. Rosa et al. (2016) successfully apply Treex pre-processing and post-processing to Moses, but this time with the main objective being an adaptation of Moses trained on general-domain data to a specific domain (namely the domain of Infor"
W16-6401,J03-1002,0,0.0116228,"ectoMT. As Treex and TectoMT have been primarily developed to process Czech language and to perform English-to-Czech translation, most of the existing system combination experiments have been performed on the English-to-Czech language pair.1 Therefore, we limit ourselves to this setting in our work. 2 Individual Systems 2.1 Moses Moses (Koehn et al., 2007) is a standard PB-SMT system. It features simple rule-based tokenization and true-casing scripts, which are sometimes language-specific, but the core of the decoder is purely statistical and oblivious of any linguistics. It relies on GIZA++ (Och and Ney, 2003) to compute word alignment of the training parallel corpus, used to extract lexicons and phrase tables that provide the knowledge of translation options to the decoder. A word-based language model is used to score possible translations, so that a fluent one can be produced as the output. 1 A few combinations have been also applied to other translation pairs. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the 2nd De"
W16-6401,W07-0704,0,0.0827717,"Missing"
W16-6401,W12-3146,1,0.888935,"Missing"
W16-6401,W16-2334,1,0.910577,"ng § 3.5 Moses + Depfix post-editing § 3.6 Joshua + Treex pre-processing Moses + Treex pre-/post-processing § 3.7 Two-headed Chimera: Moses + TectoMT § 3.8 Chimera: Moses + TectoMT + Depfix ∆ BLEU versus base Moses TectoMT −2.2 +2.7 +3.2 −0.1 −0.1 *−0.9 −2.4 +2.4 +0.1 +0.1 +0.4 **+0.5 +0.4 +4.7 +0.6 +5.4 +5.5 +1.1 +5.3 +1.6 +1.3 +6.1 +5.0 +0.5 +5.3 +5.7 +1.2 +5.4 +1.5 +6.3 +1.1 Reference Popel (2015) Bojar et al. (2013a) Galušˇcáková et al. (2013) Rosa (2013) Bojar and Kos (2010) Majliš (2009) Section 3.4 & Bojar et al. (2016) Mareˇcek et al. (2011) Rosa et al. (2012) Rosa (2013) Zeman (2010) Rosa et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar and Tamchyna (2015) Bojar et al. (2016) Bojar et al. (2013a) Bojar et al. (2013b) Bojar et al. (2014) Bojar et al. (2015) Bojar et al. (2016) Tamchyna et al. (2016) Table 1: System combinations. Difference in BLEU versus the Moses and/or TectoMT base system; * versus Google Translate, ** versus Joshua. Figure 2: TectoMoses: TectoMT with Moses Transfer While most of the setups have been properly described and evaluated in a peer-reviewed publication, others, especially some of the unsuccessful ones, were ne"
W16-6401,N07-1064,0,0.0327481,"Majliš (2009) Zeman (2010) Table 2: Base systems. Figure 3: PhraseFix translating one t-node with two or more t-nodes or deleting some t-nodes.8 It also uses MERT tuning and it should scale with more training data. In the experiments with two factors (Popel, 2013), two language models were used: one for lemmas and one for formemes. Unfortunately, the TectoMoses experiment brought negative results, presumably due to additional noise introduced by the added transformations. 3.2 PhraseFix: TectoMT with Moses Post-editing The PhraseFix system of Galušˇcáková et al. (2013) is based on the work of Simard et al. (2007), who introduced the idea of automatically post-editing a first-stage MT system by a second-stage MT system, trained to “translate” the output of the first-stage system into a reference translation. This has been shown to be particularly beneficial for conceptually different MT systems. In PhraseFix, the source English side of the CzEng parallel corpus of Bojar and Žabokrtský (2009) is translated by TectoMT into Czech, and Moses is then trained in a monolingual setting to translate the TectoMT-Czech into reference-Czech, i.e., the target side of CzEng (see Figure 3). Evaluation shows that this"
W16-6401,W07-1709,0,0.0726314,"Missing"
W16-6401,W16-2325,1,0.836879,"parts of the TectoMT translation that it considers good, while still having the base large phrase table at its disposal. This has been shown to have a positive effect, e.g., in choosing the correct inflection of a word when the language model encounters an unknown context, or in generating a translation for a word that constitutes an out-of-vocabulary item for Moses (as TectoMT can abstract from word forms to lemmas and beyond, which Moses cannot). 3.8 Chimera: Moses with Additional TectoMT Phrase-table and Depfix Post-editing The Three-headed Chimera, or simply Chimera (Bojar et al., 2013b; Tamchyna et al., 2016), is a combination of TectoMT and Moses, as in Section 3.7, complemented by a final post-editing step performed by Depfix, as in Section 3.5 (see Figure 10). It has been repeatedly confirmed as the best system by both automatic and manual evaluations, not only among the ones reported in this paper, but also in general, 12 http://www.statmt.org/moses/?n=Advanced.Hybrid 7 Figure 10: Three-headed Chimera being the winner of the WMT English-to-Czech translation task in the years 2013, 2014 and 2015 (Bojar et al., 2013a; Bojar et al., 2014; Bojar et al., 2015). 4 Conclusion We reviewed a range of e"
W16-6401,W08-0325,0,0.0651825,"Missing"
W17-1226,C16-1012,0,0.0121703,"ctly, as there are various spelling and morpho211 proach is documented by T¨ackstr¨om et al. (2013). logical differences even between very close languages. Using such shared features allows a parser that was trained on a source treebank to be used directly on target texts; i.e. the source-target “transfer” of the parser is trivial, compared to a sourcetarget transfer of the treebank as described in §2.1. The common abstraction features used by the parser can be linguistically motivated, or induced by mathematical methods such as clustering and vector space representation: 2.3 Other variations Aufrant et al. (2016) combines both main strategies described above by adapting the word order in source sentences to be more similar to that of the target language, e.g. by swapping the order of an attribute and its nominal head; the information about these configurations was extracted from the WALS World Atlas of Language Structures (Dryer and Haspelmath, 2013). Such processing of source language trees fits to the first family of approaches, as it resembles a (very limited) MT preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a"
W17-1226,J92-4003,0,0.101154,"gs: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question"
W17-1226,W06-2920,0,0.108524,"ions whether a tree (and what kind of tree) is a reasonable representation for a sentence structure, and whether all languages do really share their structural properties to such an extent that a single type of representation is viable for all of them. Though such issues deserve intensive attention, and perhaps even more so now when UD have gained such a fascinating momentum, we take the two assumptions simply for granted. Neither do we present the genesis of the current UD collection, preceded by HamleDT treebank collection by Zeman et al. (2014), going back to the CoNLL 2006 and 2007 tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), and to earlier POS standardization efforts. In this overview, we limit ourselves to the scope outlined by the VarDial shared task, whose goal is to develop a parser for a (virtually) underresourced language closely related to a resourcerich language.2 We believe that most of the published approaches could be classified into two broad families which we call tree-transfer-based methods and common-abstraction-based methods. The former project individual dependency trees across the language boundary prior to training a target parser. The latter methods transfer a parser mode"
W17-1226,J03-1002,0,0.00596816,"translation. To reduce the OOV rate, two backoff layers are also stored, the first disregarding the morpho feats, and the second also disregarding the UPOS. An option that we leave for future research is to use the alignment scores provided by the MGA when constructing the translation table. For simplicity, we create only one joint translation table for translating DS into NO. Word-alignment Since the source and target languages in our task are very close to each other, we decided to use the heuristic Monolingual Greedy Aligner (MGA) of Rosa et al. (2012),9 rather than e.g. the usual Giza++ (Och and Ney, 2003) – most standard word aligners ignore word similarity, which we believe to be useful and important in our setting. MGA utilizes the word, lemma, and tag similarity based on Jaro-Winkler distance (Winkler, 1990), and the similarity of relative positions in the sentences, to devise a score for each potential alignment link as a linear combination of these, weighted by pre-set weights. The iterative alignment process then greedily chooses the currently highest scoring pair of words to align in each step; each word can only be aligned once. The process stops when one of the sides is fully aligned,"
W17-1226,P99-1065,0,0.341674,"Missing"
W17-1226,D15-1039,0,0.0140857,"inks. In addition, such alignment typically has a higher amount of one-to-one word alignments, which facilitates tree projection; in case of extremely close languages, as in this paper, the MT system can be constrained to produce only 1:1 translations. There are two additional advantages of the treetransfer-based approach: • the feature set used by the target language parser is independent of the features that are applicable to the source language, • we can easily use only sentence pairs (or tree fragments) with a reasonably high correspondence between source and target structures, as done by Rasooli and Collins (2015). Tree-transfer-based approaches In the tree-transfer-based approaches, a synthetic pseudo-target treebank is created by some sort of projection of individual source trees into the target language. Then a standard monolingual parser can be trained using the pseudo-target treebank in a more or less standard way. As it is quite unlikely that a manually annotated source treebank 2.2 2 Crosslingual transfer is not used only in truly underresourced scenarios, but also in situations in which it is hoped that features explicitly manifested in one language (such as morphological agreement) could boost"
W17-1226,K15-1012,0,0.0123173,"n since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obvious trade-off that appears with this family of methods is associated with the specificity/generality of the shared abstract representation of words. For example, in the case of delexicalization by a common POS tagset, the question arises what is the best granularity of shared tags. The more simplified tags, the more languageuniversal information is captured, but the more information is lost at the same time. Moreover, even if two languages share a particular morphological category, e.g. pronoun reflexivity, it is hard to predict whether adding this distinction into the shared tagset"
W17-1226,P15-2040,1,0.922144,"Missing"
W17-1226,W12-4205,1,0.895855,"Missing"
W17-1226,D11-1006,0,0.0460896,"preprocessing; but after this step, a POSdelexicalized parser transfer is used, which fits the second family. When processing more than a few underresourced languages, choosing the best source language should be ideally automatized too. One could rely on language phylogenetic trees or on linguistic information available e.g. in WALS, or on more mechanized measures, such as KullbackLeibler divergence of POS trigram distributions ˇ (Rosa and Zabokrtsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harm"
W17-1226,L16-1680,0,0.150363,"Missing"
W17-1226,N12-1052,0,0.242999,"Missing"
W17-1226,N13-1126,0,0.0549572,"Missing"
W17-1226,C14-1175,0,0.407656,"et parser. The latter methods transfer a parser model trained directly on the source treebank, but limited only to abstract features shared by both languages. 2.1 with high-quality human-made target translations and high-quality alignment exists, one or more of the necessary components must be approximated. And even if all these data components existed, the task of dependency tree projection would inevitably lead to collisions that have to be resolved heuristically, especially in the case of many-to-one or many-to-many alignments, as investigated e.g. by Hwa et al. (2005) and more recently by Tiedemann (2014) or Ramasamy et al. (2014). This family embraces the following approaches: • using a parallel corpus and projecting the trees through word-alignment links, with authentic texts in both languages but an automatically parsed source side, • using a machine-translated parallel corpus, with only one side containing authentic texts and the other being created by MT; both translation directions have pros and cons: – source-to-target MT allows for using a gold treebank on the source side, – target-to-source MT allows the parser to learn to work with real texts in the target language, for which, in add"
W17-1226,W17-1216,0,0.0957939,"Missing"
W17-1226,N01-1026,0,0.132429,"er is trained on monolingually predicted tags, as explained in §4.1. We have found source-xtag to work well for heterogeneous source data, such as the DS mixture. Conversely, target-xtag proved useful for SK, where the source treebank is much larger than the target data used to train the target tagger. A tagger trained on the large source treebank provides much better tags, which in turn boosts the parsing accuracy, despite the noise from MT and xtag. Note that if no target tagger is available, we must either use target-xtag, or we may project a tagger across the parallel data in the style of Yarowsky and Ngai (2001) and use the resulting tagger in our baseline or source-xtag scenarios.12 We also experimented with cross-tagging of only the UPOS or only the morpho feats, with different setups being useful for different languages. Although the UDPipe tagger can also be trained to perform lemmatization, we have not found any way to obtain and utilize lemmas that would improve the cross-lingual parsing.13 12 Our approach still needs a target tagger to perform the word alignment, but we believe that for very close languages, the word forms alone might be sufficient to obtain a goodenough alignment; or, a diffe"
W17-1226,W17-1201,0,0.0972651,"Missing"
W17-1226,I08-3008,1,0.743945,"tsk´ y, 2015). In addition, we might want to combine information from more source languages, like in the case of multi-source transfer introduced by McDonald et al. (2011). Choosing source language weights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual pa"
W17-1226,zeman-2008-reusable,1,0.792983,"eights to be used as mixing coefficients becomes quite intricate then as we face a trade-off between similarity of the source languages to the target language and the size of resources available for them. • Unified POS tags: a POS tagset simplified and unified to the extent that it was usable for both source and target languages was behind one of the first experiments with delexicalized parsing by Zeman and Resnik (2008). The advantage of such approaches lies in their linguistic interpretability. On the other hand, in spite of the substantial progress in tagset harmonization since the work of Zeman (2008), this approach can end up in a very limited intersection of morphological categories in case of more distant languages. • Word clusters have been successfully applied in many NLP fields, with the clusters of Brown et al. (1992) being probably the most prominent representative. T¨ackstr¨om et al. (2012) showed that cross-lingually induced clusters can serve as the common abstract features for cross-lingual parsing. • Word embeddings, if induced with some cross-lingual constraints and mapped into a shared low-dimensional space, can also be used, as shown e.g. by Duong et al. (2015). 3 An obviou"
W17-2806,N16-1089,0,0.0261629,"ectly interpreted as predicted target location. 2.4 2.5 Using recurrent output layers We also tested a variant of the previous architecture in which the feed-forward output layers are substituted by recurrent 128-dimensional LSTM layers. The new architecture is shown in Figure 2. We also tried similar models for predicting the source blocks. They have bidirectional recurrent layer, followed by single output layer, which is feed-forward for one model and recurrent 64dimensional LSTM for the other one. Predicting reference and relative position Our second model is similar to the one proposed by Bisk et al. (2016b). It does not predict directly the target location, but a meaning representation of the command, which is then interpreted based on the world state to get the final predicted target location. Our representation is composed of 20 weights representing how much each block is used as a reference, and 2-dimensional vector representing the relative position from the reference block. Let w = (w1 , w2 , ...w20 )T represent the weights of individual reference blocks, d = (d1 , d2 )T represent 3 Results The experiment results are compared in Table 3. We report improvement over the previous results for"
W17-2806,E17-2079,0,0.0534743,"Missing"
W17-4720,P11-2031,0,0.0359814,"or the future to be able to draw better conclusions. 5.3 • Neural Monkey – the output of the system described in Section 4.2 using greedy decoding, • Neural Monkey 1 – decoding with beam search of 50 and taking only the first candidate translation to the phrase table, • Neural Monkey 50 – decoding with beam search of 50 and taking all 50 candidate translations to the phrase table, All combinations we have experimented with are shown in Table 4. The last column “Average BLEU” was calculated the same way as it was done in Section 4.3. Also the same 5 MERT runs were used for MultEval evaluation (Clark et al., 2011). Basically, Table 4 confirms the well-know saying “more data helps”. Using translations from different systems as additional phrase tables gave on average a 2.5 BLEU score boost, if we compare rows 1 or 2 and row 14. We also see that using more than three phrase tables might lead to a lower BLEU score: Consider the system in the row 7 with four separate phrase tables (Avg. BLEU 23.7) and the system in the row 3 where three of the tables were first merged into one (Avg. BLEU 23.9). Moreover, Multeval comparison showed no significant difference between systems from rows 7 and 8, despite the eff"
W17-4720,P13-2071,0,0.0171315,"he standard encoder-decoder architecture with attention as preposed by Bahdanau et al. (2015). (Attempts to combine MT systems with Neural Monkey are described in Section 5.2 below.) We use the following model parameters which fit into 8GB GPU memory of NVIDIA GeForce GTX 1080. The encoder uses embeddings of size 600 and the hidden state of size 600. • Synthetic phrase table extracted from the main training data, ie. either or both of NematusNews and MosesNews as listed in Table 1. • In-domain phrase table extracted from either or both of XenCNews and XenCMonoNews. • Operation Sequence Model (Durrani et al., 2013) trained on the NematusNews corpus. 8 While dropout is useful for small datasets, Sennrich et al. (2016a) observed no gain from dropout with 8M training sentence pairs. Our training data is more than 7× larger. 9 In contrast to what Tu et al. (2017, Table 1) observe for other implementations of the Bahdanau et al. (2015) model, Neural Monkey does not exhibit degradation of the quality of the top candidate with increasing beam size. We have thus no reason to keep beam size as small as usual. 6 http://data.statmt.org/rsennrich/ wmt16_systems 7 http://ufal.mff.cuni.cz/neuralmonkey 250 Phrase Tabl"
W17-4720,P07-2045,1,0.0105599,"ased on a simple perplexity computation utilizing only one side of the corpora so that monolingual corpora are sufficient and the second mode is based on the bilingual crossentropy difference as described by Axelrod et al. (2011). We took two different corpora as our in-domain data: 334 322k 103 193k 55k 67k Table 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 5,5 taking just the single-best output. The Moses-based system used only a single phrase table translating from word form to word forms and twelve 10-gram language models built on individual years of English mononews. We took all Czech mononews corpora available thi"
W17-4720,D11-1033,0,0.0298072,"MT16 submission (Tamchyna et al., 2016). Instead, we used the XenC toolkit (Rousseau, 2013) to extract domain-specific data from the whole corpus (referred to as “out-of-domain”, in the following). We used two modes of XenC. Both of these modes estimate two language models from in-domain and out-of-domain corpora, using SRILM toolkit (Stolcke, 2002). The first mode is a filtering process based on a simple perplexity computation utilizing only one side of the corpora so that monolingual corpora are sufficient and the second mode is based on the bilingual crossentropy difference as described by Axelrod et al. (2011). We took two different corpora as our in-domain data: 334 322k 103 193k 55k 67k Table 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used onl"
W17-4720,Q17-1026,0,0.0588988,"Missing"
W17-4720,P17-2031,0,0.0633694,"Missing"
W17-4720,P14-5003,0,0.143231,"Missing"
W17-4720,W16-2325,1,0.900635,"1. Use available monolingual data and last year’s systems to prepare a synthetic parallel corpus using “back translation” (Section 3). 2. Train “individual forward systems” on this synthetic corpus (Section 4). Introduction 3. Apply individual forward systems to the source side of the genuine parallel data. The paper describes CUNI submissions for English-to-Czech WMT 2017 News Translation Task. We experimented with several neural machine translation (NMT) systems and we further developed our phrase-based statistical machine translation system Chimera, which was our primary system last year (Tamchyna et al., 2016). This year, we planned our setup in a way that would allow us to experiment with neural system combination. To this end, we reserved the provided English-Czech parallel data for the training of the system combination and trained our “individual forward systems” on almost only synthetic data. The structure of the paper is the following. In Section 2, we provide an overview of the relatively complex setup. Section 3 details how the training data for all the systems were prepared, including the description of MT systems used for backtranslation. Section 4 is devoted to our individual forward tra"
W17-4720,C16-1172,0,0.0483656,"Missing"
W17-4720,P03-1021,0,0.00938203,"ce side of the development and test sets. to 20M sentence pairs instead of 59M synthetic sentences. Selecting the genuine parallel sentences both bilingually and monolingually (XenCNews) works usually better than selecting them only monolingually (XenCMonoNews), but there is a significant difference in corpus size so the numbers are not directly comparable. The common components for all the tested systems are language models, which were taken from CUNI’s last year submission. For some experiments we have used up to 4 phrase tables separately as Moses alternative decoding paths, trusting MERT (Och, 2003) to estimate weights. Alternatively (or when the number of the phrase tables would be even higher), we used the standard Moses phrase table mixing technique with uniform weights. Phrase tables mixed into one before MERT are listed as “Mix(table1, table2, ...)” in the following. MERT was done using the WMT2015 test set, and our internal evaluation was performed on WMT2016 test set, but with a different tokenization so the scores reported here are not directly comparable to the results at http://matrix. statmt.org/. We report the results in Table 2, listing the used phrase tables and optionally"
W17-4720,W12-3146,1,0.904593,"Missing"
W17-4720,W08-0325,0,0.045346,"Missing"
W17-4720,P17-2060,0,0.0446737,"Missing"
W17-4720,E17-3017,0,0.050928,"Missing"
W17-4720,P16-1162,0,0.751947,"able 1: Datasets 3.1 Back-Translated Data To create back-translated data, we used the CzEng 1.6 Czech-English parallel corpus (Bojar et al., 2016) and the Czech News Crawl articles released for WMT20171 (called “mononews” for short). We used two different back-translation systems: Moses (Koehn et al., 2007) trained by ourselves, and Marian2 (known as AmuNMT before it included NMT training; Junczys-Dowmunt et al., 2016) using the pretrained Nematus (Sennrich et al., 2017) models3 from WMT16 News Task.4 We used only the non-ensembled left-to-right run (i.e. no right-to-left rescoring as done by Sennrich et al., 2016a) with beam size of 5,5 taking just the single-best output. The Moses-based system used only a single phrase table translating from word form to word forms and twelve 10-gram language models built on individual years of English mononews. We took all Czech mononews corpora available this year, concatenated and translated them using both systems described above and thus created two back-translated corpora on which we planned to train our forward systems. The “Synthetic corpora” section of Table 1 shows the numbers of sentences and tokens of the resulting corpora. Despite having started with the"
W17-4769,D16-1134,1,0.825917,"rst one works only on Czech and uses many semantic features based of rich Czech tectogrammatical annotation (B¨ohmov´a et al., 2003). The second one uses much fewer features, however, it is language universal and needs only a dependency parsing model available. 2.1 AutoDA Using Czech Tectogrammatics This metric automatically parses the Czech translation candidate and the reference translation and uses various semantic features to compute the final score. 1. AutoDA: A linear regression model using semantic features trained on WMT Direct Assessment scores (Bojar et al., 2016) or HUMEseg scores (Birch et al., 2016). 2.1.1 Word Alignment AutoDA relies on automatic alignment between the translation candidate and the reference trans2. TreeAggreg: N-gram based metric computed over aligned syntactic structures instead of 604 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 604–611 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics Method AutoDA TreeAggreg NMTScorer Resource Type Monolingual/Bilingual* Monolingual Bilingual Trainable Yes No Yes Metric Type Segment-level Linear Regression Tree Segment-level ChrF** Segment-le"
W17-4769,L16-1262,0,0.102773,"Missing"
W17-4769,C00-2163,0,0.158333,"sible. TreeAggreg can use any string-level metric for score computation instead of ChrF (**). Dataset WMT16 DAseg WMT15 DAseg WMT16 HUMEseg Source TR/FI/CS/RO/RU/DE EN DE/RU/FI/CS EN EN Target EN RU EN RU CS/DE/PL/RO # Sentences 560 500 ∼350 Table 2: Overview of the available data for training AutoDA. • functor: the semantic value of the syntactic dependency relation. Functors express the functions of individual modifications in the sentence, e.g. ACT (Actor), PAT (Patient), ADDR (Addressee), LOC (Location), MANN (Manner), lation. The easiest way of obtaining word alignments is to run GIZA++ (Och and Ney, 2000) on the set of sentence pairs. GIZA++ was designed to align documents in two languages and it can obviously also align documents in a single language, although it does not benefit in any way from the fact that many words are identical in the aligned sentences. GIZA++ works well if the input corpus is sufficiently large, to allow for extraction of reliable word co-occurrence statistics. While the test sets alone are too small, we have a corpus of paraphrases for Czech (Bojar et al., 2013). We thus run GIZA++ on all possible paraphrase combinations together with the reference-translation pairs w"
W17-4769,W16-2301,1,0.86698,"Missing"
W17-4769,P02-1040,0,0.114911,"ranslation “Jako kofeinov´y n´apoj, alkohol v tˇele zabraˇnuje vstˇreb´av´an´ı kalcia z potravy.” metric aligned-tnode-tlemma-exact-match aligned-tnode-formeme-match aligned-tnode-functor-match aligned-tnode-sempos-match lexrf-form-exact-match lexrf-lemma-exact-match BLEU on forms BLEU on lemmas chrF3 AutoDA (87 features) AutoDA (selected 23 features) 2.1.4 Linear Regression Training We collect 83 various features based on matching tectogrammatical attributes computed on all nodes or a subsets defined by particular semantic part-of-speech tags. To this set of features, we add two BLEU scores (Papineni et al., 2002) computed on forms and on lemmas and two chrF3 scores (Popovic, 2015) computed on trigrams and sixgrams, so we have 87 features in total. We train a linear regression model to obtain a weighted mix of features that fits best the WMT16 HUMEseg scores. Since the amount of annotated data available is low, we use the jackknife strategy: en-cs 0.449 0.429 0.391 0.416 0.372 0.436 0.361 0.395 0.540 0.625 0.659 Table 3: Selected Czech deep-syntactic features and their correlation against WMT16 HUMEseg dataset. Comparison with BLEU, chrF3, and our trainable AutoDA (using chrF3 as well). • We split the"
W17-4769,W15-3049,0,0.139982,"Missing"
W17-4769,W12-4205,1,0.901962,"Missing"
W17-4769,L16-1680,0,0.0491449,"Missing"
W18-5444,P18-2003,0,0.0767232,"Missing"
W18-5444,Q16-1037,0,0.170228,"Missing"
W18-5444,J93-2004,0,0.0617662,"syntactic theories and annotations. We would also like to discuss results across various languages and natural language processing (NLP) tasks. In this abstract, we present our preliminary results, analyzing the encoder in English-to-German NMT within the NeuralMonkey toolkit (Helcl and Libovick´y, 2017). We introduce aggregation of self-attention through layers to get a distribution over the input tokens for each encoder position and layer (Section 2). We then propose algorithms for constructing two types of syntactic trees (Sections 3 and 4), apply them to 42 sentences sampled from PennTB (Marcus et al., 1993), and compare the resulting structures to established syntax annotation styles, such as that of PennTB, UD (Nivre et al., 2016), or PDT (B¨ohmov´a et al., 2003). ._ apart_ ped_ rip markets_ stock_ and_ es_ futur the_ between_ link_ the_ ,_ a_ Introduction as_ 1 result_ David Mareˇcek and Rudolf Rosa Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics Charles University, Prague, Czechia {marecek,rosa}@ufal.mff.cuni.cz as_ a_ result_ ,_ the_ link_ between_ the_ futur es_ and_ stock_ markets_ rip ped_ apart_ ._ Figure 1: Aggregated encoder’s self-attentions after the 6t"
W18-5444,L16-1262,0,0.0440475,"Missing"
W18-5444,D16-1159,0,0.21182,"Missing"
W18-6326,W11-2107,0,0.0434034,"t (Q, K1:n , V1:n ) = Ah (Q, Kflat , Vflat ) (6) Hierarchical. In the hierarchical combination (Figure 1d), we first compute the attention independently over each input. The resulting contexts are then treated as states of another input and the 1 255 http://github.com/ufal/neuralmonkey set the dimension of the hidden layers in the feedforward sub-layers to 4096. We use 16 heads in the attention layers. 5 During the evaluation, we follow the preprocessing used in WMT Multimodal Translation Shared Task (Specia et al., 2016). We evaluate the results using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) as implemented in MultEval. 2 The results of the MMT task are tabulated in Table 1. The results of the multi-source MT are shown in Table 2. Conclusions of previous work show (Elliott and K´ad´ar, 2017) that the improved performance of the multimodal models compared to textual models can come from improving the input representation. In order to test whether it is also the case with our models or the models explicitly use the visual input, we perform an adversarial evaluation similar to Elliott (2018). We evaluate the model while providinng a random image and observe how it affects the score a"
W18-6326,D18-1329,0,0.0470984,"l., 2016). We evaluate the results using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) as implemented in MultEval. 2 The results of the MMT task are tabulated in Table 1. The results of the multi-source MT are shown in Table 2. Conclusions of previous work show (Elliott and K´ad´ar, 2017) that the improved performance of the multimodal models compared to textual models can come from improving the input representation. In order to test whether it is also the case with our models or the models explicitly use the visual input, we perform an adversarial evaluation similar to Elliott (2018). We evaluate the model while providinng a random image and observe how it affects the score and observe whether their quality drops. 4.2 Results In MMT, the input combination significantly surpassed the text-only baseline in English-toFrench translation. The performance in other target languages is only slightly better than the textual baseline. The only worse score was achieved by the flat combination strategy. We hypothesize this might be because the optimization failed to find a common representation of the input modalities that could be used to compute the joint distribution. Multi-Source"
W18-6326,W16-2358,0,0.0531282,"Missing"
W18-6326,W18-2711,0,0.0690653,"y, achieving similar results that our models achieved using only the Multi30k dataset. To our knowledge, multi-source MT has also been studied only using the RNN-based models. Dabre et al. (2017) use simple concatenation of source sentences in various languages and process them with a single multilingual encoder. Zoph and Knight (2016) try context concatenation and hierarchical gating method for combining context vectors in attention models with multiple inputs encoded by separate encoders. In all of their experiments, the multi-source methods significantly surpass the single-source baseline. Nishimura et al. (2018) extend the former approach for situations when of the source languages is missing, so that the translation system does not overly rely on a single source language like some of the models presented in this work. 0.0 Figure 2: Attention over contexts in the hiearchical strategy over the decoder layers. 6 Related Work MMT was so far solved only within the RNNbased architectures. Elliott et al. (2015) report significant improvements with a non-attentive model. With attentive models (Bahdanau et al., 2014), the additional visual information usually did not improve the models significantly (Caglaya"
W18-6326,N16-1004,0,0.0438769,"anged randomly. 1.0 0.8 layer 2 0.6 layer 3 0.4 layer 4 0.2 layer 5 Except for using the image features direct input to the model, they can be used as an auxiliary objective (Elliott and K´ad´ar, 2017). In this setup, the visually grounded representation, improves the MMT significantly, achieving similar results that our models achieved using only the Multi30k dataset. To our knowledge, multi-source MT has also been studied only using the RNN-based models. Dabre et al. (2017) use simple concatenation of source sentences in various languages and process them with a single multilingual encoder. Zoph and Knight (2016) try context concatenation and hierarchical gating method for combining context vectors in attention models with multiple inputs encoded by separate encoders. In all of their experiments, the multi-source methods significantly surpass the single-source baseline. Nishimura et al. (2018) extend the former approach for situations when of the source languages is missing, so that the translation system does not overly rely on a single source language like some of the models presented in this work. 0.0 Figure 2: Attention over contexts in the hiearchical strategy over the decoder layers. 6 Related W"
W18-6326,P02-1040,0,0.100801,"a (Q, K1:n , V1:n ) (7) h (5) Ahflat (Q, K1:n , V1:n ) = Ah (Q, Kflat , Vflat ) (6) Hierarchical. In the hierarchical combination (Figure 1d), we first compute the attention independently over each input. The resulting contexts are then treated as states of another input and the 1 255 http://github.com/ufal/neuralmonkey set the dimension of the hidden layers in the feedforward sub-layers to 4096. We use 16 heads in the attention layers. 5 During the evaluation, we follow the preprocessing used in WMT Multimodal Translation Shared Task (Specia et al., 2016). We evaluate the results using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2011) as implemented in MultEval. 2 The results of the MMT task are tabulated in Table 1. The results of the multi-source MT are shown in Table 2. Conclusions of previous work show (Elliott and K´ad´ar, 2017) that the improved performance of the multimodal models compared to textual models can come from improving the input representation. In order to test whether it is also the case with our models or the models explicitly use the visual input, we perform an adversarial evaluation similar to Elliott (2018). We evaluate the model while providinng a random image"
W18-6326,E17-2025,0,0.149132,"a test set of 1,000. We experiment with all language pairs available in this dataset. We extract image feature using the last convolutional layer of the ResNet network (He et al., 2016) trained for ImageNet classification. We apply a linear projection into 512 dimensions on the image representation, so it has the same dimension as the rest of the model. For each language pair, we create a shared wordpiece-based vocabulary of approximately 40k subwords. We share the embedding matrices across the languages and we use the transposed embedding matrix as the output projection matrix as proposed by Press and Wolf (2017). We use 6 layers in the textual encoder and decoder, and set the model dimension to 512. We Ah (Q, Ki , Vi ) (4) Flat. The encoder-decoder attention in the flat combination strategy (Figure 1c) uses all the states of all input encoders as a single set of keys and values. Thus, the attention models a joint distribution over a flattened set of all encoder states. Unlike the approach taken in the recurrent setup (Libovick´y and Helcl, 2017), where the flat combination strategy requires an explicit projection of the encoder states to a shared vector space, in the Transformer models, the vector sp"
W18-6326,W16-2346,0,0.100989,"Missing"
W18-6326,tiedemann-2012-parallel,0,0.0427355,"f the input modalities that could be used to compute the joint distribution. Multi-Source MT The adversarial evaluation with randomly selected input images shows that all our models rely on both inputs while generating the target sentence and that providing incorrect visual input harms the model performance. The modality gating in the hierarchical attention combination seems to make the models more robust to noisy visual input. In this set of experiment, we attempt to generate a sentence in a target language, given equivalent sentences in multiple source languages. We use the Europarl corpus (Tiedemann, 2012) for training and testing the MSMT. We use Spanish, French, German, and English as source languages and Czech as a target language. We selected an intersection of the bilingual sub-corpora using English as a pivot language. Our dataset contains 511k 5-tuples of sentences for training, 1k for validation and another 1k for testing. In the multi-source translation task, all the proposed strategies perform better than single-source translation from English to Czech. Among the combination strategies, the best-scoring is the serial stacking of the attentions. In multimodal translation, the flat comb"
W18-6326,P18-1117,0,0.0750023,"Missing"
W18-6326,I17-1014,0,\N,Missing
W19-4818,Q18-1003,0,0.0320863,"Missing"
W19-4818,D17-1193,0,0.0685565,"Missing"
W19-4818,N16-2002,0,0.0175901,"mation process which creates new words from existing ones by 2 Related work We have not found any prior work aimed specifically at derivational relations in word embeddings. Cotterell and Sch¨utze (2018) present a model of the semantics and structure of derivationally complex words. Our work differs in that we are examining how are derivational relations represented in preexisting applications. 173 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 173–180 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Gladkova et al. (2016) detect morphological and semantic relations (including some derivational relations) with word embeddings. Their approach is analogy-based and they conclude that their “experiments show that derivational and lexicographic relations remain a major challenge”. G´abor et al. (2017) explore vector spaces for semantic relations, using unsupervised clustering. They evaluate the clustering on 9 semantic relation classes. Our approach is similar, but we focus on derivational relations. Soricut and Och (2015) use word embeddings to induce morphological segmentation in an unsupervised manner. Some of th"
W19-4818,W18-1816,1,0.84593,"Missing"
W19-4818,D07-1043,0,0.106832,", 1967),4 • agg: Hierarchical agglomerative clustering using Euclidean distance and Ward’s linkage criterion (Joe H. Ward, 1963),5 • agg (cos): The same hierarchical agglomerative clustering, but using cosine distance instead of Euclidean. For each word pair W1 and W2 , where W1 is the derivational parent of W2 and their embeddings v1 and v2 , the clustering algorithm only gets the differience vector d = v2 − v1 . The information about the word forms and their derivation type is only used in evaluation. We evaluate the clustering quality by homogeneity (H), completeness (C) and V-measure (V) (Rosenberg and Hirschberg, 2007). These are entropy based methods, which can be compared across any number of clusters. Homogeneity is a measure of the ratio of instances of a single class pertaining to a single cluster. Completeness measures the ratio of the member of a given class that is assigned to the same cluster. V-measure is computed as the harmonic mean of homogeneity and completeness scores. Following G´abor et al. (2017), we also report the accuracy (A) that would be achieved by the clustering if we assigned every cluster to the class that is most frequent in this cluster and then used the clustering as a classifi"
W19-4818,sevcikova-zabokrtsky-2014-word,0,0.39342,"Missing"
W19-4818,hnatkova-etal-2014-syn,0,0.0436715,"Missing"
W19-4818,N15-1186,0,0.0200143,"ages 173–180 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Gladkova et al. (2016) detect morphological and semantic relations (including some derivational relations) with word embeddings. Their approach is analogy-based and they conclude that their “experiments show that derivational and lexicographic relations remain a major challenge”. G´abor et al. (2017) explore vector spaces for semantic relations, using unsupervised clustering. They evaluate the clustering on 9 semantic relation classes. Our approach is similar, but we focus on derivational relations. Soricut and Och (2015) use word embeddings to induce morphological segmentation in an unsupervised manner. Some of the relations between words that this approach implicitly uses are derivational. to separate from each other (see e.g. ten Hacken, 2014). Both change base words using affixes, but they differ in the type of the outcome: derivation creates new words, inflection only creates forms of the base word. DeriNet differentiates derivation from inflection the same way the Czech morphological tool MorphoDiTa (Strakov´a et al., 2014) does – it considers the processes handled by the MorphoDiTa tool to be inflection"
W19-4818,P14-5003,0,0.0794127,"Missing"
W19-4818,C14-1163,0,0.0721948,"Missing"
W19-4827,N19-1112,0,0.0157325,"of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions, but rather looking for structures explicitly presen"
W19-4827,P14-5010,0,0.00462413,"requently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrasestructure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall. 1 Introduction The classical approach to Natural Language Processing used to be complex pipelines, e.g. (Popel ˇ and Zabokrtsk´ y, 2010; Manning et al., 2014; Forcada et al., 2011), consisting of multiple steps of linguistically motivated analyses, such as partof-speech tagging or syntactic parsing, using explicit intermediate representations (e.g. dependency trees) to abstract over the underlying texts. In recent years, this has changed with the introduction of deep neural end-to-end models, which take raw text as input and produce the desired output directly. Any intermediate representations of the text may emerge during the training of the neural network, and are hidden to us. We focus on the encoder part of the Transformer architecture (Vaswan"
W19-4827,J93-2004,0,0.0660621,"Missing"
W19-4827,P18-2003,0,0.0178098,"es or anaphora links. In this work, we analyze the syntactic properties of the self-attention heads both qualitatively 2 Related Work Initial analyses of syntax captured by neural networks focused on RNNs. Shi et al. (2016) examine how much syntax is learned by RNN encoder by freezing its weights and using a decoder to predict syntactic trees. Adi et al. (2016) examine sentence vector representations by training auxiliary classifiers to take sentence encodings and predict attributes like word order. Linzen et al. (2016) assess the ability of LSTMs to learn syntax by predicting verbal numbers. Blevins et al. (2018) measure the amount of syntax in RNNs by predicting part-of-speech tags and constituent labels. In the last year, related studies appeared also for the Transformer architecture. Tang et al. (2018) show the Transformer networks perform better 263 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 263–275 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics to the states on the previous layer (input states), passed through a feed-forward layer. This may allow the encoder to do more advanced multi-step processin"
W19-4827,W18-5444,1,0.902996,"Missing"
W19-4827,N18-1202,0,0.0231646,"ource subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions,"
W19-4827,W18-1816,0,0.0585938,"Missing"
W19-4827,W18-5431,0,0.177451,"cluding syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by the network, but because it can be predicted from features preserved from the input, as has been already noted e.g. by Belinkov and Glass (2018). In our work, we try to avoid that risk by not using annotated data for the predictions, but rather looking for structures explicitly present in the network representations. In a study closely related to ours, Raganato and Tiedemann (2018) also observe syntax-like patterns in Transformer encoder self-attentions, and try to extract syntactic trees without using annotated data (except for taking the root node from the gold annotation). However, they construct dependency trees, while we observe phrase-like rather than dependency-like structures. Moreover, their findings are somewhat inconclusive, as the accuracy of the resulting trees is close to the baseline, while our results are clearly positive. A similar approach was already suggested (but not evaluated) in (Mareˇcek and Rosa, 2018). 3 3.1 Encoder Self-Attention Visualization"
W19-4827,N19-1419,0,0.0479625,"he source subword embeddings forward, bypassing the self-attention mechanism, and get averaged with the outputs of the self-attention. This ensures that the output state at each position retains a significant amount of the corresponding source subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing models on annotated data (e.g. treebanks). However, such a model may learn to predict the linguistic structure not because it is captured by"
W19-4827,P16-1162,0,0.0245687,"ce-target language pairs (en-fr, en-de, fr-en, fr-de, de-en, de-fr).4 From the Europarl corpus, we take first 1,000 sentences as development data, last 1,000 sentences as evaluation data, and the remaining 486,272 sentences for training. Table 1 lists the BLEU scores of the systems. All inspections and evaluations, both manual and automatic, have been performed on the evaluation data. The data are tokenized by the Stanford Tokenizer5 to make the tokens consistent with the constituency trees with which we will compare our results. We then build a shared dictionary of 100,000 BPE subword units (Sennrich et al., 2016) on the concatenated training data of all three languages, append an EOS symbol to each sentence, and train the translation model. 4 4.1 Diagonals Especially at the first encoder layer, there often appear various simple diagonal heads. Typically, each output state attends to the input state at the same position. This may serve to pass the subword information to the higher layers. In some cases, most of the output states attend to the corresponding input states, but some of them attend elsewhere. The role of such partial diagonal may be looking for a specific phenomenon that only occurs for som"
W19-4827,2005.mtsummit-papers.11,0,0.0300562,"ads concentrate nearly all of the attention at each output state onto just one input state. In the following subsections, we list all of the distinctive patterns that we have identified.6 An important thing to note is that typically, a head behaves consistently across all sentences, i.e., for a given head on a given layer of a given trained Transformer encoder, we typically see the same attention patterns across all sentences. Table 1: BLEU scores measured on the test data. French (fr), and German (de). We selected those particular languages because they are available in the Europarl corpus1 (Koehn, 2005) comprising large high-quality multiparallel data, and because constituency syntax parse trees can be obtained for them by the Stanford parser (Klein and Manning, 2003) out-of-the-box.2 As we want to explore a state-of-the-art setup, we use the Transformer model (Vaswani et al., 2017) as reimplemented by Helcl et al. (2018) in the Neural Monkey framework3 in standard setting: 6 encoder and decoder layers, 16 attention heads, embedding size of 512, hidden-layers’ size of 4096, dropout 0.9, and batch size 30. We train the translator for all 6 source-target language pairs (en-fr, en-de, fr-en, fr"
W19-4827,D16-1159,0,0.0332131,"ut directly. Any intermediate representations of the text may emerge during the training of the neural network, and are hidden to us. We focus on the encoder part of the Transformer architecture (Vaswani et al., 2017), applied to neural machine translation (NMT), as visualizations presented by the authors suggest that its attention heads capture various phenomena such as syntax, semantic roles or anaphora links. In this work, we analyze the syntactic properties of the self-attention heads both qualitatively 2 Related Work Initial analyses of syntax captured by neural networks focused on RNNs. Shi et al. (2016) examine how much syntax is learned by RNN encoder by freezing its weights and using a decoder to predict syntactic trees. Adi et al. (2016) examine sentence vector representations by training auxiliary classifiers to take sentence encodings and predict attributes like word order. Linzen et al. (2016) assess the ability of LSTMs to learn syntax by predicting verbal numbers. Blevins et al. (2018) measure the amount of syntax in RNNs by predicting part-of-speech tags and constituent labels. In the last year, related studies appeared also for the Transformer architecture. Tang et al. (2018) show"
W19-4827,D18-1458,0,0.0579193,"Missing"
W19-4827,W18-5448,0,0.0141338,"osition on the higher layers. Another notable feature of the Transformer encoder is the use of residual connections, which transport the source subword embeddings forward, bypassing the self-attention mechanism, and get averaged with the outputs of the self-attention. This ensures that the output state at each position retains a significant amount of the corresponding source subword embedding, supporting the usual shortcut of assuming that the hidden states can be thought of as representations of the underlying subwords (in the context of the sentence). than RNNs on word sense disambiguation. Zhang and Bowman (2018) show that language models use more syntactic and morphological information than translation models. Recently, Hewitt and Manning (2019) tried to find syntactic structures in contextual word representations by training simple models on annotated parse trees, concluding that syntactic trees are embedded both in BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models. This is also supported by Liu et al. (2019), who successfully trained probes to extract linguistic structures, including syntactic dependencies, from various trained neural networks. Most existing works train probing model"
W19-4827,Q16-1037,0,\N,Missing
W19-4827,Q19-1004,0,\N,Missing
W19-4827,N19-1423,0,\N,Missing
Y16-2018,I05-1075,0,\N,Missing
Y16-2018,N01-1026,0,\N,Missing
Y16-2018,D10-1056,0,\N,Missing
Y16-2018,petrov-etal-2012-universal,0,\N,Missing
Y16-2018,I08-3008,1,\N,Missing
Y16-2018,W02-2006,0,\N,Missing
Y16-2018,D11-1006,0,\N,Missing
Y16-2018,P13-2112,0,\N,Missing
Y16-2018,P15-2044,0,\N,Missing
Y16-2018,L16-1262,1,\N,Missing
Y16-2018,L16-1497,0,\N,Missing
zeman-etal-2012-hamledt,zeman-2008-reusable,1,\N,Missing
zeman-etal-2012-hamledt,bosco-etal-2010-comparing,0,\N,Missing
zeman-etal-2012-hamledt,W08-2121,0,\N,Missing
zeman-etal-2012-hamledt,C00-2143,0,\N,Missing
zeman-etal-2012-hamledt,P06-1033,0,\N,Missing
zeman-etal-2012-hamledt,W08-0325,1,\N,Missing
zeman-etal-2012-hamledt,D11-1036,0,\N,Missing
zeman-etal-2012-hamledt,D11-1006,0,\N,Missing
zeman-etal-2012-hamledt,ramasamy-zabokrtsky-2012-prague,1,\N,Missing
zeman-etal-2012-hamledt,R09-1007,0,\N,Missing
zeman-etal-2012-hamledt,dzeroski-etal-2006-towards,0,\N,Missing
zeman-etal-2012-hamledt,taule-etal-2008-ancora,0,\N,Missing
zeman-etal-2012-hamledt,afonso-etal-2002-floresta,0,\N,Missing
