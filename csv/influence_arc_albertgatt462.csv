2007.mtsummit-ucnlg.14,P89-1009,0,0.466714,"do peer attribute sets uniquely describe the target referent? Minimality: A minimal attribute set is defined as an attribute set which uniquely identifies the referent such that there is no smaller attribute set which uniquely identifies the referent. For example, a minimal description of e1 in the above example is {COLOUR:red}, since it is the only red object. There may be more than one minimal attribute set. As an aggregate measure, we computed the proportion of minimal distinguishing outputs produced by peer systems. Minimality has frequently been cited as a desideratum for GRE algorithms (Dale, 1989; Gardent, 2002). 2. Minimality: are peer attribute sets of minimal size? 3. Humanlikeness: are peer attribute sets similar to reference attribute sets? 4. Identification Accuracy: do peer attribute sets enable people to identify the target referent accurately? 5. Identification Speed: do peer attribute sets enable people to identify a referent quickly? 4.1 Automatic Evaluation Methods Humanlikeness: We measured the similarity between the peer attribute sets and (human-produced) reference attribute sets, because (Grice’s maxim of Clarity notwithstanding) humans choose to overspecify and unders"
2007.mtsummit-ucnlg.14,W07-2307,1,\N,Missing
2007.mtsummit-ucnlg.21,W07-2307,1,0.863657,"Missing"
2007.mtsummit-ucnlg.21,J07-2004,1,0.878071,"Missing"
2020.evalnlgeval-1.5,W05-0909,0,0.0610457,"ed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content"
2020.evalnlgeval-1.5,E09-1014,0,0.0607084,"Missing"
2020.evalnlgeval-1.5,2020.lrec-1.828,1,0.810698,"Missing"
2020.evalnlgeval-1.5,E14-1074,0,0.0600624,"Missing"
2020.evalnlgeval-1.5,P17-4012,0,0.0122863,"ifferent frameworks with different takes on the same problem: (a) as a true translation task, where given a headline in one style, the model learns to generate a new headline in the target style; (b) as a summarisation task, where headlines are viewed as an extreme case of summarisation and generated from the article. We exploit article-headline generators trained on opposite sources to do the transfer. This approach does not in principle require parallel data for training. For the translation approach (S2S), we train a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as"
2020.evalnlgeval-1.5,W19-8643,1,0.892211,"Missing"
2020.evalnlgeval-1.5,W04-1013,0,0.0256706,", 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evalua"
2020.evalnlgeval-1.5,N19-1049,0,0.0483085,"Missing"
2020.evalnlgeval-1.5,N18-2012,0,0.0472247,"Missing"
2020.evalnlgeval-1.5,D17-1238,0,0.0311446,"Missing"
2020.evalnlgeval-1.5,P02-1040,0,0.114378,"nsfer (Fu et al., 2018; Mir et al., 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’"
2020.evalnlgeval-1.5,N18-1012,0,0.0198065,"summarisation system SUM does better at content preservation (HH and AH) than S2S1. However, its scores on the main classifier are worse in both transfer directions, as well as on average. The average compliancy score is higher for S2S1. In summary, for data which is not strongly aligned, our methods suggest that style transfer is better when conceived as a translation task. BLEU is higher for SUM, but the overall extremely low scores across the board suggest that it might not be a very informative metric for this setup, although commonly used to assess content preservation in style transfer (Rao and Tetreault, 2018). Our HH and AH classifiers appear more indicative in this respect, and ROUGE scores seem to correlate a bit more with them, when compared to BLEU. It remains to be investigated whether BLEU, ROUGE, and our content-checking classifiers do in fact measure something similar or not. With better-aligned data (bottom panel), the picture is more nuanced. Here, the main comparison is between two systems trained on strongly aligned data, one of which (S2S2) has additional, weakly aligned data. The overall compliancy score suggests that this improves style transfer (and this system is also the top perf"
2020.evalnlgeval-1.5,J18-3002,0,0.0235647,"Missing"
2020.evalnlgeval-1.5,J09-4008,0,0.0932183,"Missing"
2020.evalnlgeval-1.5,W02-2113,0,0.189512,"Missing"
2020.evalnlgeval-1.5,P17-1099,0,0.0166246,"ain a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as pick them from a fixed vocabulary, thereby allowing better handling of out-of-vocabulary words. ability to reproduce novel words. One model is trained on the la Repubblica portion of the training set, the other on Il Giornale. In a style transfer setting we use these models as follows: Given a headline from Il Giornale, for example, the model trained on la Repubblica can be run over the corresponding article from Il Giornale to generate a headline in the style of la Repubblica, and vice versa. To train the model"
2020.evalnlgeval-1.5,2020.acl-main.704,0,0.0162999,". We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content assessment, and are carried out"
2020.gebnlp-1.1,S18-2005,0,0.0293931,"th mitigating bias in English ELMo (Zhao et al., 2019) and in embeddings of morphologically rich languages (Zmigrod et al., 2019). 3 Data In line with previous research (Kurita et al., 2019; Zhao et al., 2019; Basta et al., 2019), we measure gender bias in BERT using sentence templates. For this purpose we create the Bias Evaluation Corpus with Professions (BEC-Pro), containing English and German sentences built from templates (Section 3.2). We also use two previously existing corpora, which are described in Section 3.1. 3.1 Existing Corpora The Equity Evaluation Corpus (EEC) was developed by Kiritchenko and Mohammad (2018) as a benchmark corpus for testing gender and racial bias in NLP systems in connection with emotions. It contains 8,640 sentences constructed using 11 sentence templates with the variables <person>, which is instantiated by a male- or female-denoting NP; and <emotion word>, whose values can be one of the basic emotions. We use this corpus for preliminary bias assessment.2 This corpus also inspired the structure of the BEC-Pro, and we borrow from it the person words used in our templates. The GAP corpus (Webster et al., 2018) was developed as a benchmark for measuring gender bias in coreference"
2020.gebnlp-1.1,W19-3823,0,0.128092,"e), December 13, 2020. Contributions This work makes the following contributions: (i) We present and release the Bias Evaluation Corpus with Professions (BEC-Pro), a template-based corpus in English and German, which we created to measure gender bias with respect to different profession groups. We make the dataset and code for all experiments publicly available at https://github.com/marionbartl/gender-bias-BERT. (ii) Through a more diverse sentence context in our corpus than in previous research, we confirm that the method of querying BERT’s underlying MLM (Masked Language Model), proposed by Kurita et al. (2019), can be used for bias detection in contextualized word embeddings. (iii) We test our bias analysis on BERT against actual U.S. workforce statistics, which helps us to observe that the BERT language model does not only encode biases that reflect real-world data, but also those that are based on stereotypes. For bias mitigation, (iv) we show the success of a technique on BERT, which was previously applied on ELMo (Peters et al., 2018; Zhao et al., 2019). Finally, (v) we attempt the cross-lingual transfer of a bias measuring method proposed for English, and show how this method is impaired by th"
2020.inlg-1.45,P02-1040,0,0.117646,"w that different kinds of errors elicit significantly different evaluation scores, even though all erroneous descriptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. Th"
2020.inlg-1.45,J18-3002,0,0.250965,"riptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of"
2020.inlg-1.45,2020.acl-main.704,0,0.0241616,"n as more or less severe. 1 1.1 Introduction Recent years have seen a growing discomfort with the use of automatic metrics like BLEU (Papineni et al., 2002) for the evaluation of natural language generation (NLG) systems (e.g., Sulem et al. 2018; Reiter 2018; Mathur et al. 2020). Much of the criticism centers around the fact that these metrics show poor agreement with human judgments. While many researchers have tried to develop new metrics that are better suited to evaluate NLG systems (e.g. tailored to the domain like SPICE (Anderson et al., 2016) or with intensive pre-training like BLEURT; Sellam et al. 2020), we are not aware of any studies attempting to explain why we see such a poor correlation between human judges and automatic metrics. This paper aims to explore this hypothesis, Motivation Image description systems make different kinds of mistakes, and these mistakes are likely to be of different importance for a ‘correct’ interpretation of the relevant image. Consider Figure 1, which shows multiple human reference descriptions, and a description generated by Li et al.’s (2018) system (all in Chinese, with English glosses). This system makes three different mistakes, which are shown separatel"
2020.inlg-1.45,P17-1024,0,0.0222989,"hese studies show that while some errors may make users abandon a product, other errors may not be judged as harshly. In fact, Mirnig et al. found that people may even like a robot more if it occasionally makes a mistake. But, as Abdolrahmani et al. note: this all depends on the context of use. 399 Our study asks how we can systematically study the impact of different kinds of errors in automatic image descriptions. Several studies have proposed different categorizations of these errors. We will discuss those studies below. 2.2 Weaknesses in system competence Hodosh and Hockenmaier (2016) and Shekhar et al. (2017) both manipulate existing image descriptions to generate flawed descriptions, which they use to see if automatic image description systems can recognize those flaws. For example, given a sentence like (2), Hodosh and Hockenmaier swap the existing scene description for another one (2→2a), and ask systems to identify the correct description. Shekhar et al. change an entity with another entity falling under the same supercategory (e.g. VEHI CLE , 2→2b), and ask systems to identify the flaw in the description. (2) Ref: A man is riding a bicycle down the street. a. A man is riding a bicycle on the"
2020.lrec-1.626,W18-5102,0,0.0258496,"Missing"
2020.lrec-1.626,N13-1037,0,0.0155689,"or one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding better interannotator agreement results is established more concretely. In this regard, a line of future work, briefly discussed in Section 3., is to further validate the multi-level annotation schemes. Specifically, reliability needs to be estimated on the basis of larger and more diverse samples. Furthermore, we have already identified the question of whether, having conducted a micro-analysis of user-gene"
2020.lrec-1.626,C12-2029,0,0.0160199,"comments have to be annotated to find a considerable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed"
2020.lrec-1.626,W14-3901,0,0.0131572,"iderable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding better interannotator ag"
2020.lrec-1.626,gao-huang-2017-detecting,0,0.0132488,"tions, “overt prejudicial bias has been transformed into subtle and increasingly covert expressions” (Leets, 2003, p.146). Therefore, while clearly indicative of a negative attitude, the use of derogatory terms, as in (9), cannot account for all instances of hate speech: (8) [username] sure NO! The majority of Maltese people are against ” ILLEGALS”! Do not mix racism with illegals please! (9) That’s because we’re not simply importing destitute people. We’re importing a discredited, disheveled and destructive culture. 5091 In this respect, it is often acknowledged (Warner and Hirschberg, 2012; Gao and Huang, 2017; Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018; Sanguinetti et al., 2018; Watanabe et al., 2018) that discourse context plays a crucial role in evaluating remarks, as there are several indirect strategies for expressing discriminatory hatred, which our in-depth analysis enabled us to additionally unearth. The most pertinent example of this is the use of metaphor, which has for long been emphasised as a popular strategy for communicating discrimination, since it typically involves “mappings from a conceptual ‘source domain’ to a ‘target domain’ with resulting conceptual ‘blends’ that help"
2020.lrec-1.626,W19-3512,0,0.0119109,"parable annotation tasks. Discussing this issue within the more general context of detecting abusive language, Waseem et al. (2017) identify two sources for the resulting annotation confusion: (a) the existence of abusive language directed towards some generalised outgroup, as opposed to a specific individual; and (b) the implicitness with which an abusive attitude can often be communicated. Despite targeting abusive language in general, the resulting two-fold typology has been taken to apply to the more particular discussion of hate speech too (ElSherief et al., 2018; MacAvaney et al., 2019; Mulki et al., 2019; Rizos et al., 2019), rendering the lack of an explicit insult/threat towards an individual target in terms of their membership to a protected group more difficult to classify as hate speech than a remark that explicitly incites to discriminatory hatred. This much seems to be further corroborated by a recent study by Salminen et al. (2019) which revealed that, when evaluating the hatefulness of online comments on a scale of 1 (not hateful at all) to 4 (very hateful), annotators agree more on the two extremes than in the middle ground. Quite justifiably, this suggests that it is easier to clas"
2020.lrec-1.626,W14-5904,0,0.0119121,"tated to find a considerable number of hate speech instances” (Schmidt and Wiegand, 2017, p.7), MaNeCo is well suited for addressing the challenge of building a set that is balanced between hate speech and non-hate speech data. Clearly, this does not mean that we will not have additional challenges to face before we end up with a dataset that could potentially be used for training and testing pur5094 poses. For one, being a corpus of user-generated content in the bilingual setting of Malta, it contains extensive codeswitching and code-mixing (Rosner and Farrugia, 2007; Elfardy and Diab, 2012; Sadat et al., 2014; Eskander et al., 2014), which is further complicated by the inconsistent use of Maltese spelling online, particularly in relation to the specific Maltese graphemes [˙c], [˙g], [g¯h], [¯h], and [˙z], for which diacritics are often omitted in online discourse. Then, given the casual nature of most communication on social media, it is full of non-canonical written text (Baldwin et al., 2013; Eisenstein, 2013) exhibiting unconventional orthography, use of arbitrary abbreviations and so on. Even so, these are challenges that can be faced after the effectiveness of our proposed scheme in yielding"
2020.lrec-1.626,L18-1443,0,0.158396,"ot hate speech, or neither offensive nor hate speech. Along similar lines, Zampieri et al. (2019) introduce an explicitly hierarchical annotation scheme that requested annotators to code tweets on three consecutive levels: (a) on whether they contain offensive language; (b) on whether the insult/threat is targeted to some individual or group; and (c) on whether the target is an individual, a group or another type of entity (e.g. an organization or event). Finally, an annotation framework which, like the one proposed here, takes into account the intricate nature of hate speech was developed by Sanguinetti et al. (2018). Here, annotators were asked to not only provide a binary classification of Italian tweets as hate speech or not, but also to grade their intensity on a scale from 0 to 4, and indicate whether each tweet contains ironical statements or stereotypical representations as well as how it fares in terms of aggressiveness and offensiveness. Despite the apparently increasing interest in the area and the development of all the more sophisticated annotation methodologies, a major cause for concern when it comes to annotations used for model training and testing is that reliability scores are consistent"
2020.lrec-1.626,W17-1101,0,0.264416,"it is an instance of hate speech or not. This makes the problem a case of binary classification (±hate speech), which in turn makes it amenable to treatment using a variety of classification methods. These supervised learning techniques require pre-labelled training data, consisting of manually annotated positive and negative examples of the class(es) to be identified, to learn a model which, given a new instance, can predict the label with some degree of probability. In this setting, the most common features traditionally used for hate speech classifiers are lexical and grammatical features (Schmidt and Wiegand, 2017; Fortuna and Nunes, 2018), with more recent approaches making use of neural network models relying on word embeddings (Badjatiya et al., 2017; Agrawal and Awekar, 2018). In this paper, we concentrate on the particular issues that one has to take into consideration when annotating Web 2.0 data for hate speech. The unique angle of our perspective is that it is informed by data-driven research in the field of Critical Discourse Analysis (CDA), a strand of applied linguistics that has for long dealt with the ways in which language is used to express ideologically-charged attitudes, especially in"
2020.lrec-1.626,W12-2103,0,0.686826,"model. Even so, previous studies “remain fairly vague when it comes to the annotation guidelines their annotators were given for their work” (Schmidt and Wiegand, 2017, p.8). A review of the relevant literature reveals that the majority of previous attempts to annotate Web 2.0 data for hate speech involves simple binary classification into hate speech and non-hate speech (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015; Nobata et al., 2016). There are of course notable exceptions where the annotation scheme involved was more or less hierarchical in nature. For example, in Warner and Hirschberg (2012), annotators were tasked with classifying texts on the basis of whether they constitute hate speech or not, but were additionally asked to specify the target of said speech in the interest of distinguishing between seven different domains of hatred (e.g. sexism, xenophobia, homophobia, etc). Then, acknowledging that hate speech is a subtype of the more general category of offensive language and can thus often be conflated with it, Davidson et al. (2017) asked annotators to label tweets in terms of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speec"
2020.lrec-1.626,N16-2013,0,0.0265639,"is strikingly evident when one takes into account that most of the studies reviewed in NLP state-of-the-art reports on hate speech detection (including our discussion so far) formulate the question at hand using different – albeit interrelated – terms, which apart from hate speech variously include harmful speech, offensive and abusive language, verbal attacks, hostility or even cyber-bullying, among others. In this vein, as Waseem et al. (2017, p.78) observe and exemplify, this “lack of consensus has resulted in contradictory annotation guidelines – some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017).” The apparent confusion as to how to adequately define hate speech is of course not exclusive to NLP research, but extends to social scientific (Gagliardone et al., 2015) and even legal treatments of the theme (Bleich, 2014; Sellars, 2016). Given this general confusion, it is certainly no surprise that annotators, especially ones who do not have domainspecific knowledge on the matter, will be prone to disagreeing as to how to classify some text in the relevant task, as opposed to other comparable"
2020.lrec-1.626,W17-3012,0,0.0140427,"e taken to incite to discriminatory hatred, hate speech is now often used as an umbrella label for all sorts of hateful/insulting/abusive content (Brown, 2017). This much is strikingly evident when one takes into account that most of the studies reviewed in NLP state-of-the-art reports on hate speech detection (including our discussion so far) formulate the question at hand using different – albeit interrelated – terms, which apart from hate speech variously include harmful speech, offensive and abusive language, verbal attacks, hostility or even cyber-bullying, among others. In this vein, as Waseem et al. (2017, p.78) observe and exemplify, this “lack of consensus has resulted in contradictory annotation guidelines – some messages considered as hate speech by Waseem and Hovy (2016) are only considered derogatory and offensive by Nobata et al. (2016) and Davidson et al. (2017).” The apparent confusion as to how to adequately define hate speech is of course not exclusive to NLP research, but extends to social scientific (Gagliardone et al., 2015) and even legal treatments of the theme (Bleich, 2014; Sellars, 2016). Given this general confusion, it is certainly no surprise that annotators, especially o"
2020.lrec-1.626,W16-5618,0,0.276041,"ntensity on a scale from 0 to 4, and indicate whether each tweet contains ironical statements or stereotypical representations as well as how it fares in terms of aggressiveness and offensiveness. Despite the apparently increasing interest in the area and the development of all the more sophisticated annotation methodologies, a major cause for concern when it comes to annotations used for model training and testing is that reliability scores are consistently found to be low pretty much across the board (Warner and Hirschberg, 2012; Nobata et al., 2016; Ross et al., 2016; Tulkens et al., 2016; Waseem, 2016; Bretschneider and Peters, 2017; Schmidt and Wiegand, 2017; de Gibert et al., 2018). This effectively suggests that, even in the presence of more detailed guidelines (Ross et al., 2016; Malmasi and Zampieri, 2018; 2 Samples of this corpus can be made available upon request. A full release is expected in future, pending licensing agreements with the donors of the MaNeCo data, which will need to cover sensitive data such as comments written by online users, but deleted by the moderators of the newspaper portal. Sanguinetti et al., 2018), annotators often fail to develop an intersubjective under"
2020.lrec-1.626,N19-1144,0,0.0148192,"with classifying texts on the basis of whether they constitute hate speech or not, but were additionally asked to specify the target of said speech in the interest of distinguishing between seven different domains of hatred (e.g. sexism, xenophobia, homophobia, etc). Then, acknowledging that hate speech is a subtype of the more general category of offensive language and can thus often be conflated with it, Davidson et al. (2017) asked annotators to label tweets in terms of three categories: hate speech, offensive but not hate speech, or neither offensive nor hate speech. Along similar lines, Zampieri et al. (2019) introduce an explicitly hierarchical annotation scheme that requested annotators to code tweets on three consecutive levels: (a) on whether they contain offensive language; (b) on whether the insult/threat is targeted to some individual or group; and (c) on whether the target is an individual, a group or another type of entity (e.g. an organization or event). Finally, an annotation framework which, like the one proposed here, takes into account the intricate nature of hate speech was developed by Sanguinetti et al. (2018). Here, annotators were asked to not only provide a binary classificatio"
2020.lrec-1.784,W17-1304,1,0.827867,"unsupported. As a matter of fact, digital support for the Maltese language has improved drastically since the publication of the METANET white paper (and this is likely true for a number of other languages covered by the white paper series). To take some examples, large annotated corpora for Maltese are now available, with accompanying tools for segmentation and labelling, including tokenisation and partˇ epl¨o, 2013).2 . Advances of-speech annotation (Gatt and C´ have been made in the development of electronic lexicons (Camilleri, 2013) and in automatic morphological analysis and labelling (Borg and Gatt, 2017; Ravishankar et al., 2017) as well as dependency parsing (Tiedemann and van der Plas, 2016; Zammit, 2018). Advances in speech technology for Maltese have however been comparatively limited. While there have been successful attempts to build speech synthesis systems using concatenative techniques (Micallef, 1997; Borg et al., 2011), no tools currently exist for Automatic Speech Recognition (ASR). This is partly due to a substantial data bottleneck where resources for speech engineering are concerned. 1.2. Aims of the present paper The present paper addresses this gap, presenting a new corpus f"
2020.nl4xai-1.5,P18-1151,0,0.0279074,"ates are lists of ordered triples divided into sentences. Castro Ferreira et al. (2019) first order the input triples in the way they will be expressed and then divides this ordered list into sentences and paragraphs. This ordering of triples and segmentation into sentences is studied with different models : two rulebased baselines (which apply either random selection of triples or most frequent order seen on the training set) and two neural models (GRU and Input structure encoding Some approaches use the structure of the input to constrain the order in which input units are verbalised. Thus, Distiawan et al. (2018) capture the inter and intra RDF triples relationships using a graph-based encoder (GRT-LSTM). It then combines topological sort and breadth-first traversal algorithms to determine in which order the vertices of the GRT-LSTM will be input with data during training thereby performing content planning. Dedicated Attention mechanisms Instead of encoding input structure, some of the approaches use attention mechanisms to make their model focus on specific aspects of the data structure. Sha et al. (2018) take advantage of the information given by table field names and by relations between table fie"
2020.nl4xai-1.5,W18-6505,0,0.332608,"where a template is a sequence of latent variables (transitions) learned by the model on the training data. Decoding (emissions) is then conditioned on both the input and the template latent variables. Intuitively, the approach learns an alignment between input tokens, latent variables and output text segments (cf. Table 2). A key feature of this approach is that this learned alignment can be used both to control (by generating from different templates) and to explain (by examining the mapping between input data and output text mediated by the latent variable) the generation model. Similarly, Gehrmann et al. (2018) develop a mixture of models where each model learns a latent sentence template style based on a subset of the Similarly Shao et al. (2019) decompose text generation into a sequence of sentence generation subtasks where a planning latent variable is learned based on the encoded input data. Using this latent variable, the generation is made hierarchically with a sentence decoder and a word decoder. The plan decoder specifies the content of each output sentence. The sentence decoder also improves highlevel planning of the text. Indeed this model helps capture inter-sentence dependencies in parti"
2020.nl4xai-1.5,W19-8645,0,0.330563,"on unseen data. Zhao et al. (2020) model a plan as a sequence of RDF properties which, before decoding, is enriched with its input subject and object. A Graph Convolutional Network (GCN) encodes the graph input and a Feed Forward Network is used to predict a plan which is then encoded by an LSTM. The LSTM decoder takes as input the hidden states from both encoders. In this approach the document structuring sub-task is tackled by an additional plan encoder. Explicit Content Structuring using Supervised Learning. Other approaches explicitly generate content plans using supervised learning. In (Moryossef et al., 2019b), a text plan is a sequence of sentence plans where each sentence plan is an ordered tree. Linearisation is then given by a pre-order traversal of the sentence trees. The authors adopt an overgenerate-and-rank approach where the text plans are generated using symbolic methods and ranked using a product of expert model integrating different probabilities such as the relation direction probability (e.g. the probability that the triple {A, manager, B} is expressed as “A is the manager of B” or, in reverse order, as “B is managed by A”) or the relation transition probability (which relations are"
2020.nl4xai-1.5,N19-1236,0,0.344474,"on unseen data. Zhao et al. (2020) model a plan as a sequence of RDF properties which, before decoding, is enriched with its input subject and object. A Graph Convolutional Network (GCN) encodes the graph input and a Feed Forward Network is used to predict a plan which is then encoded by an LSTM. The LSTM decoder takes as input the hidden states from both encoders. In this approach the document structuring sub-task is tackled by an additional plan encoder. Explicit Content Structuring using Supervised Learning. Other approaches explicitly generate content plans using supervised learning. In (Moryossef et al., 2019b), a text plan is a sequence of sentence plans where each sentence plan is an ordered tree. Linearisation is then given by a pre-order traversal of the sentence trees. The authors adopt an overgenerate-and-rank approach where the text plans are generated using symbolic methods and ranked using a product of expert model integrating different probabilities such as the relation direction probability (e.g. the probability that the triple {A, manager, B} is expressed as “A is the manager of B” or, in reverse order, as “B is managed by A”) or the relation transition probability (which relations are"
2020.nl4xai-1.5,P17-1099,0,0.0344407,"ty is model analysis such as supported e.g., by the AllenNLP Interpret toolkit (Wallace et al., 2019) which provides two alternative means for interpreting neural models. Gradient-based methods explain a model’s prediction by identifying the importance of input tokens based on the gradient of the loss with respect to the tokens (Simonyan et al., 2014) Lexicalisation Lexicalisation maps input symbols to words. In neural approach, lexicalisation is mostly driven by the decoder which produces a distribution over the next word, from which a lexical choice is made. The copy mechanism introduced by See et al. (2017) is also widely used as it allows copying from the input (Sha et al., 2018; Moryossef et al., 2019b; Laha et al., 2020). At each decoding step, a learned “switch variable” is computed to decide whether the next word should be generated by the S2S model or simply copied from the input. Inspecting the value of the switch variable permits assessing how much lexicalisation tends to copy vs to generate and can provide some explainability in the lexicalisation sub-task. Finally, a few approaches use lexicons and rule-based mapping. In particular, Castro Ferreira et al. (2019) use a rulebased model t"
2020.nl4xai-1.5,D19-1052,0,0.0376709,"Missing"
2020.nl4xai-1.5,P18-1182,0,0.0402737,"Missing"
2020.nl4xai-1.5,2020.acl-main.641,0,0.0185632,"previously selected content is ordered and divided into sentences and paragraphs. The goal of this task is to produce a text plan. Many approaches choose to model document structuring. Four main types of approaches can be distinguished depending on whether the content plan is determined by latent variables, explicit content structuring, based on the input structure or guided by a dedicated attention mechanism. Latent variable approaches have also been proposed for so-called hierarchical approaches where the generation of text segments, generally sentences, is conditioned on a text plan. Thus, Shen et al. (2020) propose a model where, given a set of input records, the model first selects a data record based on a transition probability which takes into account previously selected data records and second, generates tokens based on the word generation probability and attending only to the selected data record. This “strong attention” mechanism allows control of the output structure. It also reduces hallucination by using the constraints that all data records must be used only once. The model automatically learns the optimal content planning by exploring exponentially many segmentation/correspondence pos"
2020.nl4xai-1.5,D19-1054,0,0.254302,"ning Macroplanning is the first subtask of the traditional pre-neural NLG pipeline. It answers the “what to say” question and can be decomposed into selecting and organising the content that should be expressed in the generated text. 2.1 Motivation Content Determination Content determination is the task of selecting information in the input data that should be expressed in the output text. The importance of this subtask depends on the goal of a generation model. In the papers surveyed, papers which verbalise RDF or Meaning Representations (MR) input do not perform content determination, while Shen et al. (2019), who generate headlines from source text, do. In this approach, content selection is viewed as a sequence labelling task where masking binary latent variables are applied to the input. Texts are generated by first sampling from the input to decide which content to cover, then decoding by conditioning on the selected content. The proposed content selector has a ratio of selected tokens that can be adjusted, bringing controllability in the content selection. It should also be noted that in template-based approaches such as (Wiseman et al., 2018), which use templates for text structuring (cf. Se"
2020.nl4xai-1.5,D19-3002,0,0.0274646,"Table 2 which shows examples of latent templates used to generate from the input, latent variables provide a natural means to explain the model’s behaviour i.e., to understand which part of the input licenses which part of the output. They are also domain agnostic and, in contrast to the explicit pipeline models mentioned in the previous paragraph, they do not require the additional creation of labelled data which often relies on complex, domain specific, heuristics. A third alternative way to support explainability is model analysis such as supported e.g., by the AllenNLP Interpret toolkit (Wallace et al., 2019) which provides two alternative means for interpreting neural models. Gradient-based methods explain a model’s prediction by identifying the importance of input tokens based on the gradient of the loss with respect to the tokens (Simonyan et al., 2014) Lexicalisation Lexicalisation maps input symbols to words. In neural approach, lexicalisation is mostly driven by the decoder which produces a distribution over the next word, from which a lexical choice is made. The copy mechanism introduced by See et al. (2017) is also widely used as it allows copying from the input (Sha et al., 2018; Moryosse"
2020.nl4xai-1.5,D18-1356,0,0.356395,"R) input do not perform content determination, while Shen et al. (2019), who generate headlines from source text, do. In this approach, content selection is viewed as a sequence labelling task where masking binary latent variables are applied to the input. Texts are generated by first sampling from the input to decide which content to cover, then decoding by conditioning on the selected content. The proposed content selector has a ratio of selected tokens that can be adjusted, bringing controllability in the content selection. It should also be noted that in template-based approaches such as (Wiseman et al., 2018), which use templates for text structuring (cf. Sec. 2.2), the template choice determines the structure of the output text but also has an influence on the content selection since some templates will not express some of the input information. For instance, the output 2 in The end-to-end encoder-decoder is a popular neural approach that is efficient to generate fluent texts. However it has often been shown to face some adequacy problems such as hallucination, repetition or omission of information. As the end-to-end encoder-decoder approaches are often “black box” approaches, such adequacy probl"
2020.nl4xai-1.5,2020.acl-main.224,0,0.123487,"d attention mechanisms in the sentence decoder. Remark. Learning a template can cover different NLG subtasks at once. For instance Gehrmann et al. (2018) use sentence templates, which determine the order in which the selected content is expressed (document structuring), define aggregation and for some cases encourage the use of referring expressions and of some turns of phrase (usually included in the lexicalisation sub-task) and defines to some extent the surface realization. Transformer). They show that neural models perform better on the seen data but do not generalize well on unseen data. Zhao et al. (2020) model a plan as a sequence of RDF properties which, before decoding, is enriched with its input subject and object. A Graph Convolutional Network (GCN) encodes the graph input and a Feed Forward Network is used to predict a plan which is then encoded by an LSTM. The LSTM decoder takes as input the hidden states from both encoders. In this approach the document structuring sub-task is tackled by an additional plan encoder. Explicit Content Structuring using Supervised Learning. Other approaches explicitly generate content plans using supervised learning. In (Moryossef et al., 2019b), a text pl"
2020.nl4xai-1.6,2020.nl4xai-1.0,0,0.256406,"Missing"
2020.nl4xai-1.6,W09-0613,1,0.64928,"atural language and measuring an increased benefit for the end-user. NLG-based approaches fall into two broad categories: template-based and end-to-end generation. 3.1 Template-based Generation By leveraging knowledge about the kind of explanation produced about the system it is possible to structure templates that present the output in textual form. The popular LIME method (Ribeiro et al., 2016), which provides a linear approximation of the feature contribution to the output, can be presented in natural language using paragraphs (Forrest et al., 2018), for example with the SimpleNLG toolbox (Gatt and Reiter, 2009). ExpliClas (Alonso and Bugarin, 2019) is a web-service that provides local and global explanations for black boxes by leveraging post-hoc techniques (such as gray model surrogates) in natural language using the NLG pipeline proposed by Reiter and Dale (2000). In the medical domain, a fracture-detecting model has been extended to produce a textual explanation that follows a limited vocabulary and a fixed sentence length (Gale et al., 2018). The authors measured a significant increase in the trustworthiness from a medical population for the textual modality over the visual. While output with te"
2020.nl4xai-1.6,E06-1040,0,0.0371008,"ustworthiness from a medical population for the textual modality over the visual. While output with templates is easier to control, its static nature someEvaluating Explanation Systems There is an ongoing discussion in the XAI community on how to evaluate explanation systems. Human assessment is deemed the most relevant, and care should be given in measuring the goodness of an explanation in terms of whether the user understands the model better after the explanation was given (Hoffman et al., 2018). The work of 24 times produces sentences that are non-natural and lack variation. 3.2 quality, Belz and Reiter (2006), Reiter and Belz (2009) and Reiter (2018) point out that these metrics might not adequately measure quality of content. In addition, Post (2018) shows how different libraries have different default values for the parameters used in computing automatic metrics, thus making comparisons across different publications more difficult. More importantly, automatic metrics have been observed to not correlate with human evaluations (Novikova et al., 2017). That said, while human evaluation remains the gold standard for the general assessment of overall system quality, using it at every step of the deve"
2020.nl4xai-1.6,W18-6319,0,0.0126423,"aluating Explanation Systems There is an ongoing discussion in the XAI community on how to evaluate explanation systems. Human assessment is deemed the most relevant, and care should be given in measuring the goodness of an explanation in terms of whether the user understands the model better after the explanation was given (Hoffman et al., 2018). The work of 24 times produces sentences that are non-natural and lack variation. 3.2 quality, Belz and Reiter (2006), Reiter and Belz (2009) and Reiter (2018) point out that these metrics might not adequately measure quality of content. In addition, Post (2018) shows how different libraries have different default values for the parameters used in computing automatic metrics, thus making comparisons across different publications more difficult. More importantly, automatic metrics have been observed to not correlate with human evaluations (Novikova et al., 2017). That said, while human evaluation remains the gold standard for the general assessment of overall system quality, using it at every step of the development process would be too expensive and slow (van der Lee et al., 2019). So, goodness of text generated is a prerequisite but is not enough in"
2020.nl4xai-1.6,J18-3002,0,0.0120549,"tual modality over the visual. While output with templates is easier to control, its static nature someEvaluating Explanation Systems There is an ongoing discussion in the XAI community on how to evaluate explanation systems. Human assessment is deemed the most relevant, and care should be given in measuring the goodness of an explanation in terms of whether the user understands the model better after the explanation was given (Hoffman et al., 2018). The work of 24 times produces sentences that are non-natural and lack variation. 3.2 quality, Belz and Reiter (2006), Reiter and Belz (2009) and Reiter (2018) point out that these metrics might not adequately measure quality of content. In addition, Post (2018) shows how different libraries have different default values for the parameters used in computing automatic metrics, thus making comparisons across different publications more difficult. More importantly, automatic metrics have been observed to not correlate with human evaluations (Novikova et al., 2017). That said, while human evaluation remains the gold standard for the general assessment of overall system quality, using it at every step of the development process would be too expensive and"
2020.nl4xai-1.6,J09-4008,0,0.0307405,"ical population for the textual modality over the visual. While output with templates is easier to control, its static nature someEvaluating Explanation Systems There is an ongoing discussion in the XAI community on how to evaluate explanation systems. Human assessment is deemed the most relevant, and care should be given in measuring the goodness of an explanation in terms of whether the user understands the model better after the explanation was given (Hoffman et al., 2018). The work of 24 times produces sentences that are non-natural and lack variation. 3.2 quality, Belz and Reiter (2006), Reiter and Belz (2009) and Reiter (2018) point out that these metrics might not adequately measure quality of content. In addition, Post (2018) shows how different libraries have different default values for the parameters used in computing automatic metrics, thus making comparisons across different publications more difficult. More importantly, automatic metrics have been observed to not correlate with human evaluations (Novikova et al., 2017). That said, while human evaluation remains the gold standard for the general assessment of overall system quality, using it at every step of the development process would be"
2020.nl4xai-1.6,W19-8643,1,0.889315,"Missing"
2020.nl4xai-1.6,N16-3020,0,0.0544309,"an” 2.3 3 Explaining with Natural Language An explanation can be laid out using different modalities. The general trend in the literature is to represent results in a graphical visual form, but some researchers are using natural language and measuring an increased benefit for the end-user. NLG-based approaches fall into two broad categories: template-based and end-to-end generation. 3.1 Template-based Generation By leveraging knowledge about the kind of explanation produced about the system it is possible to structure templates that present the output in textual form. The popular LIME method (Ribeiro et al., 2016), which provides a linear approximation of the feature contribution to the output, can be presented in natural language using paragraphs (Forrest et al., 2018), for example with the SimpleNLG toolbox (Gatt and Reiter, 2009). ExpliClas (Alonso and Bugarin, 2019) is a web-service that provides local and global explanations for black boxes by leveraging post-hoc techniques (such as gray model surrogates) in natural language using the NLG pipeline proposed by Reiter and Dale (2000). In the medical domain, a fracture-detecting model has been extended to produce a textual explanation that follows a"
2020.nl4xai-1.6,D17-1238,0,0.0437985,"Missing"
2021.blackboxnlp-1.15,2020.acl-main.747,0,0.094691,"Missing"
2021.blackboxnlp-1.15,2020.blackboxnlp-1.5,0,0.0304814,"on, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results have motivated researchers to try and disentangle the language-specific and languageneutral components of mBERT (e.g. Libovick´y et al., 2020; Gonen et al., 2020). This background provides the motivation for the work presented in this paper. We focus on the relationship between language-specific and languageneutral representations in mBERT. However, our main goal is to study the impact of fine-tuning on the balance between these two types of representations. More specifically, we measure the Recent work has shown evidence that the knowledge acquired by multilingual BERT (mBERT) has two components: a languagespecific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks – POS tagging an"
2021.blackboxnlp-1.15,2021.naacl-main.282,0,0.0536609,"Missing"
2021.blackboxnlp-1.15,2020.emnlp-main.363,0,0.0387014,"Missing"
2021.blackboxnlp-1.15,2020.findings-emnlp.150,0,0.0314188,"Missing"
2021.blackboxnlp-1.15,N18-1101,0,0.0173193,"form cross-lingual zero-shot learning, we fine-tune mBERT on English only and evalu215 guages (Conneau et al., 2018). label classifier UDPOS For POS tagging, we use data from the Universal Dependencies Treebank (UDPOS; Marneffe et al., 2020) v2.7, using the train/dev/test splits provided. A validation set is randomly sampled from the training set. Since we are interested in cross-lingual zero-shot learning, we removed all non-English data from the train/val splits. mBERT language classifier Figure 1: The basic model architecture. XNLI For NLI, we use the monolingual English MultiNLI data set (Williams et al., 2018) as a training set, and the Cross-lingual Natural Language Inference data (XNLI; Conneau et al., 2018) for the development set and test set. Again, a validation set is randomly sampled from the training set. the test data from the target task (which is also labelled by language). Unless otherwise specified, gradients from the language classifier are not propagated to the pretrained mBERT model. Thus, the mBERT model parameters are only fine-tuned on the target task data set whilst the language classifier is finetuned in isolation. This allows us to monitor how much language-specific informatio"
2021.blackboxnlp-1.15,2020.aacl-main.56,0,0.0690634,"Missing"
2021.blackboxnlp-1.15,D19-1077,0,0.021936,"a line of earlier work which sought to achieve transferable multilingual representations using recurrent networkbased methods (e.g. Artetxe et al., 2019, inter alia), as well as work on developing multilingual embedding representations (Ruder et al., 2017). The considerable capacity of these multilingual models and their success in cross-lingual tasks has motivated a lot of research into the nature of the representations learned during pretraining. On the one hand, there is a significant amount of research suggesting that models such as mBERT acquire robust language-specific representations (Wu and Dredze, 2019; Libovick´y et al., 2020; Choenni and Shutova, 2020). On the other hand, it has been suggested that in addition to language-specific information, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results"
2021.blackboxnlp-1.15,P19-1493,0,0.318385,"ing that models such as mBERT acquire robust language-specific representations (Wu and Dredze, 2019; Libovick´y et al., 2020; Choenni and Shutova, 2020). On the other hand, it has been suggested that in addition to language-specific information, models like mBERT also have language-neutral representations, which cut across linguistic distinctions and enable the model to handle aspects of meaning language-independently. This also allows the model to be fine-tuned on a monolingual labelled data set and achieve good results in other languages, a process known as cross-lingual zero-shot learning (Pires et al., 2019; Libovick´y et al., 2020; Conneau et al., 2018; Hu et al., 2020). These results have motivated researchers to try and disentangle the language-specific and languageneutral components of mBERT (e.g. Libovick´y et al., 2020; Gonen et al., 2020). This background provides the motivation for the work presented in this paper. We focus on the relationship between language-specific and languageneutral representations in mBERT. However, our main goal is to study the impact of fine-tuning on the balance between these two types of representations. More specifically, we measure the Recent work has shown"
2021.blackboxnlp-1.15,2020.coling-main.105,0,0.0843735,"Missing"
2021.blackboxnlp-1.15,D07-1043,0,0.223036,"Missing"
2021.findings-emnlp.132,W05-0909,0,0.382325,"Missing"
2021.findings-emnlp.132,2020.webnlg-1.7,1,0.84159,"Missing"
2021.findings-emnlp.132,W18-6521,0,0.156526,"ects entityevaluation setup. This suggests that in order based semantic adequacy. These metrics rely on an to measure the entity-based adequacy of generalgorithm designed to automatically detect whether ated texts, an automatic metric such as the one proposed here might be more reliable, as less an entity present in the input graph has a corresubjective and more focused on correct verbalisponding mention in the output text. We evaluate sation of the input, than human evaluation meathis algorithm on a corpus of 25,173 (RDF, Text) sures. pairs with manually annotated entity mentions from Castro Ferreira et al. (2018) and show that our al1 Introduction gorithm has a recall of 0.74 and a precision of 0.75. We apply these metrics to the output of 25 RDF With the introduction of pretrained models, the fluency of text generation systems has improved. verbalisers developed for the WebNLG 2017 and However, semantic adequacy (faithfulness to the in- 2020 challenges and show that some of the systems which rank highest in terms of BLEU scores put) remains an unsolved issue. It remains difficult actually rank in the lower half with respect to entityto ensure that the generated text faithfully captures the input (Wis"
2021.findings-emnlp.132,P19-1483,0,0.0228599,"find that the correlation with human scores varies with the specifics of the human evaluation setup. This suggests that our automatic metric might be a more reliable means of identifying models with low entity-based semantic adequacy than human evaluation. We are publicly releasing our source code. 1 captions and computes an F-score over the semantic propositions in the scene graph (Anderson et al., 2016). Similarly, the MEANT metric applies Semantic Role Labelling to generated and reference texts and computes similarity by matching the resulting semantic frames. In data-to-text generation, (Dhingra et al., 2019) uses custom entailment models to determine whether an n-gram in the generated text is entailed by the input and computes an F-score based on these n-grams. In text sum2 Related work marisation, Goodrich et al. (2019) compare relation Various methods have been proposed to evaluate tuples extracted from a ground-truth summary and the semantic adequacy of generated texts. a generated one using either a Named entity RecogCommonly-used metrics are surface-based (ei- nition and a Relation Classifier or an end-to-end ther word- or character-based) such as BLEU (Pa- Transformer model to extract these"
2021.findings-emnlp.132,2020.inlg-1.19,0,0.0309734,"ne graph reference or to human judgments, but with respect encoding the objects and relations present in these to the input. Wiseman et al. (2017) define Re1 lation Generation score as the precision of input https://gitlab.nl4xai.eu/juliette. faille/entity-based-semantic-adequacy relations found in the output texts (the relation ex1531 traction is performed by a neural model). Reed et al. (2018) define information extraction patterns to measure the occurrence of the input attributes and their values in the outputs and compute semantic adequacy using the Slot Error Rate. Ribeiro et al. (2020); Dušek and Kasner (2020) use natural language inference (NLI) to detect two way entailment between the generated text and the input. Sulem et al. (2020) introduce SAMSA which assesses simplification quality by comparing the predicate/argument structures contained in the input with those contained in the output summary. Similarly, we evaluate the semantic adequacy of a generated text by comparing it with the input. We focus on entities however and provide a detailed assessment of both the reliability of our metrics and its correlation with human and with automatic metrics. 3 Defining E-Based Semantic Adequacy We assum"
2021.findings-emnlp.132,P02-1040,0,0.108961,"Missing"
2021.findings-emnlp.132,W15-3049,0,0.03869,"Missing"
2021.findings-emnlp.132,2020.acl-demos.14,0,0.0130607,"he Appendix. 1532 the dictionary was developed by updating it with entities for which no mention was detected in an evaluated text; these were manually included in the dictionary. This dictionary provides a symbolic means to improve entity mention detection and more generally, to adapt the algorithm to a new domain. However, it should be noted that, on the WebNLG 2017 dataset, adding this dictionary only slightly improves entity mention detection and is not essential. Pronominal entity mentions In order to detect which input entity a pronoun refers to, we use two methods. We first use Stanza (Qi et al., 2020) to compute co-reference chains in our texts, keeping only the pronominal mentions. For the pronouns that were not detected by the previous method, we used a simple heuristic. In the WebNLG corpus, RDF graphs are created with a single entity as &quot;root&quot;. Other entities are meant to describe and provide information about this root. As the texts are quite short we assume that most pronominal anaphors refer to the &quot;root&quot; of the graph. We therefore associate all remaining pronouns to the root entity of the RDF graph. Dates We use the python library dateparser to normalise dates both in the text and"
2021.findings-emnlp.132,W18-6535,0,0.0127083,"f propositional content. In comClosest to our approach are metrics which evalputer vision for instance, SPICE transforms both uate the generated output, not with respect to the generated and reference captions into a scene graph reference or to human judgments, but with respect encoding the objects and relations present in these to the input. Wiseman et al. (2017) define Re1 lation Generation score as the precision of input https://gitlab.nl4xai.eu/juliette. faille/entity-based-semantic-adequacy relations found in the output texts (the relation ex1531 traction is performed by a neural model). Reed et al. (2018) define information extraction patterns to measure the occurrence of the input attributes and their values in the outputs and compute semantic adequacy using the Slot Error Rate. Ribeiro et al. (2020); Dušek and Kasner (2020) use natural language inference (NLI) to detect two way entailment between the generated text and the input. Sulem et al. (2020) introduce SAMSA which assesses simplification quality by comparing the predicate/argument structures contained in the input with those contained in the output summary. Similarly, we evaluate the semantic adequacy of a generated text by comparing"
2021.findings-emnlp.132,2020.tacl-1.38,1,0.834984,"nce captions into a scene graph reference or to human judgments, but with respect encoding the objects and relations present in these to the input. Wiseman et al. (2017) define Re1 lation Generation score as the precision of input https://gitlab.nl4xai.eu/juliette. faille/entity-based-semantic-adequacy relations found in the output texts (the relation ex1531 traction is performed by a neural model). Reed et al. (2018) define information extraction patterns to measure the occurrence of the input attributes and their values in the outputs and compute semantic adequacy using the Slot Error Rate. Ribeiro et al. (2020); Dušek and Kasner (2020) use natural language inference (NLI) to detect two way entailment between the generated text and the input. Sulem et al. (2020) introduce SAMSA which assesses simplification quality by comparing the predicate/argument structures contained in the input with those contained in the output summary. Similarly, we evaluate the semantic adequacy of a generated text by comparing it with the input. We focus on entities however and provide a detailed assessment of both the reliability of our metrics and its correlation with human and with automatic metrics. 3 Defining E-Based S"
2021.findings-emnlp.132,2020.acl-main.704,0,0.0246783,"ed (ei- nition and a Relation Classifier or an end-to-end ther word- or character-based) such as BLEU (Pa- Transformer model to extract these tuples. pineni et al., 2002), TER (Snover et al., 2006) or Rather than abstract over the lexical content of chrF (Popovi´c, 2015). As these methods fail to the generated and reference text, other work has account for paraphrases, alternative metrics have focused on developing metrics which model hubeen proposed such as METEOR (Banerjee and man judgement in particular, judgments of semanLavie, 2005), which measures n-gram overlap but tic similarity. Thus Sellam et al. (2020) introduced integrates synonyms and BERTscore, a trained met- BLEURT, an automatic metric pre-trained on synric based on word-embeddings similarity (Zhang* thetic and automatically rated data and fine-tuned et al., 2020). Semantic similarity has also been on human judgments. modeled in terms of propositional content. In comClosest to our approach are metrics which evalputer vision for instance, SPICE transforms both uate the generated output, not with respect to the generated and reference captions into a scene graph reference or to human judgments, but with respect encoding the objects and re"
2021.findings-emnlp.132,2006.amta-papers.25,0,0.157658,"termine whether an n-gram in the generated text is entailed by the input and computes an F-score based on these n-grams. In text sum2 Related work marisation, Goodrich et al. (2019) compare relation Various methods have been proposed to evaluate tuples extracted from a ground-truth summary and the semantic adequacy of generated texts. a generated one using either a Named entity RecogCommonly-used metrics are surface-based (ei- nition and a Relation Classifier or an end-to-end ther word- or character-based) such as BLEU (Pa- Transformer model to extract these tuples. pineni et al., 2002), TER (Snover et al., 2006) or Rather than abstract over the lexical content of chrF (Popovi´c, 2015). As these methods fail to the generated and reference text, other work has account for paraphrases, alternative metrics have focused on developing metrics which model hubeen proposed such as METEOR (Banerjee and man judgement in particular, judgments of semanLavie, 2005), which measures n-gram overlap but tic similarity. Thus Sellam et al. (2020) introduced integrates synonyms and BERTscore, a trained met- BLEURT, an automatic metric pre-trained on synric based on word-embeddings similarity (Zhang* thetic and automatica"
2021.findings-emnlp.132,2020.starsem-1.6,0,0.0373707,"et al. (2017) define Re1 lation Generation score as the precision of input https://gitlab.nl4xai.eu/juliette. faille/entity-based-semantic-adequacy relations found in the output texts (the relation ex1531 traction is performed by a neural model). Reed et al. (2018) define information extraction patterns to measure the occurrence of the input attributes and their values in the outputs and compute semantic adequacy using the Slot Error Rate. Ribeiro et al. (2020); Dušek and Kasner (2020) use natural language inference (NLI) to detect two way entailment between the generated text and the input. Sulem et al. (2020) introduce SAMSA which assesses simplification quality by comparing the predicate/argument structures contained in the input with those contained in the output summary. Similarly, we evaluate the semantic adequacy of a generated text by comparing it with the input. We focus on entities however and provide a detailed assessment of both the reliability of our metrics and its correlation with human and with automatic metrics. 3 Defining E-Based Semantic Adequacy We assume a corpus of (R, T ) instances where R is an RDF graph (a set of RDF triples) and T is a text verbalising that graph. RDF tripl"
belz-gatt-2012-repository,passonneau-2006-measuring,0,\N,Missing
belz-gatt-2012-repository,W09-2816,1,\N,Missing
belz-gatt-2012-repository,W07-2315,0,\N,Missing
belz-gatt-2012-repository,P02-1040,0,\N,Missing
belz-gatt-2012-repository,P89-1009,0,\N,Missing
borg-gatt-2014-crowd,N07-1020,0,\N,Missing
borg-gatt-2014-crowd,W02-0606,0,\N,Missing
borg-gatt-2014-crowd,W04-0106,0,\N,Missing
borg-gatt-2014-crowd,N01-1024,0,\N,Missing
borg-gatt-2014-crowd,P00-1027,0,\N,Missing
borg-gatt-2014-crowd,P05-1071,0,\N,Missing
borg-gatt-2014-crowd,J01-2001,0,\N,Missing
borg-gatt-2014-crowd,P08-1084,0,\N,Missing
borg-gatt-2014-crowd,W02-0603,0,\N,Missing
borg-gatt-2014-crowd,W00-0712,0,\N,Missing
borg-gatt-2014-crowd,J08-4004,0,\N,Missing
borg-gatt-2014-crowd,P10-4006,0,\N,Missing
borg-gatt-2014-crowd,E12-1067,0,\N,Missing
C14-1189,P89-1009,0,0.107003,"sed in automatic multimodal generation. We show that the decision to use a pointing gesture depends on features of the accompanying description (especially whether it contains spatial information), and on visual properties, especially distance or separation of a referent from its previous referent. 1 Introduction The automatic generation of multimodal referring actions is a relatively under-studied phenomenon in Natural Language Generation (NLG). While there has been extensive research on Referring Expression Generation (REG) focusing on the choice of content in expressions such as (1) below (Dale, 1989; Dale and Reiter, 1995; Krahmer and van Deemter, 2012), their multimodal counterpart – exemplified in (2) – raises questions that go beyond these choices. (1) the group of five large red circles (2) there’s a group of five large red ones [+pointing gesture with arm extended] One important question concerns the appropriateness of a pointing gesture under different conditions. The relevant conditions here include both the physical or perceptual common ground shared by interlocutors (for example, what other objects are in the vicinity of the target referent, and therefore potentially confusable"
C14-1189,W13-2109,1,0.185471,"pointing gesture. If this fails, a pointing gesture that is less precise may be generated, together with descriptive features of an object. Both of these approaches assume that the choice of modality in a referring action ultimately hinges on a trade-off between what can be said and what is easiest to produce, a view that has some empirical support (Beun and Cremers, 1998; Bangerter, 2004; Piwek, 2007). On the other hand de Ruiter et al. (2012) found that likelihood of pointing was unaffected by the difficulty of using descriptive features. From a computational perspective, our earlier work (Gatt and Paggio, 2013) also found evidence, based on a machine-learning study on dialogue data, for the co-occurrence of pointing with descriptive (especially spatial) features, suggesting that pointing gestures may be planned in tandem (and not in competition) with these features. The present paper uses the same corpus data as Gatt and Paggio (2013); however, that paper focused on the relationship between descriptive features (in the spoken part of the utterance) and pointing. In contrast, here we take a much broader view, also addressing the impact of the physical/perceptual features of the objects under discussi"
C14-1189,J12-1006,0,0.0399991,"Missing"
C14-1189,J03-1003,0,0.0401755,"Missing"
C14-1189,W05-1608,0,0.0247626,"te utterances (Enfield, 2009). This view has also influenced recent work in multimodal NLG. For example, Kopp et al. (2008) use ‘multimodal concepts’, combining propositional and gestural or perceptual information. In the case of referring expressions, pointing has been treated as a property, on a par with an object’s colour or size. Thus, van der Sluis and Krahmer (2007) propose an algorithm in a graph-based framework (Krahmer et al., 2003) which selects pointing gestures of varying degrees of precision based on their cost when compared to other linguistically realisable features. Similarly, Kranstedt and Wachsmuth (2005) propose an extension of Dale and Reiter’s (1995) Incremental Algorithm, which initially considers the possibility of producing an unambiguous pointing gesture. If this fails, a pointing gesture that is less precise may be generated, together with descriptive features of an object. Both of these approaches assume that the choice of modality in a referring action ultimately hinges on a trade-off between what can be said and what is easiest to produce, a view that has some empirical support (Beun and Cremers, 1998; Bangerter, 2004; Piwek, 2007). On the other hand de Ruiter et al. (2012) found th"
C18-1199,D15-1075,0,0.293942,"(interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant “world” or “situation”). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare “blind” and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing “grounding” in an optimal fashion. 1 Introduction Evaluating the ability to infer information from a text is a crucial test of the capability of models to grasp meaning. As a result, the computational linguistics community has invested huge efforts into developing textual entailment (TE) datasets. After formal semanticists developed FraCas in the mid ’90 (Cooper e"
C18-1199,W17-6809,0,0.0391296,"Missing"
C18-1199,N18-2017,0,0.0195464,"ng point the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), the largest natural language inference dataset available with sentence pairs labelled with entailment, contradiction and neutral relations. We augmented this dataset with images. It has been shown very recently that SNLI contains language bias, such that a simple classifier can achieve high accuracy in predicting the three classes just by having as input the hypothesis sentence. A subset of the SNLI test set with ‘hard’ cases, where such a simplistic classifier fails (hereafter SNLIhard ) has been released (Gururangan et al., 2018). Hence, in this paper we will report our results on both the full dataset and the hard 2356 test set, but then zoom in on SNLIhard to understand the models’ behaviour. We briefly introduce SNLI and the new test set and compare them through our annotation of linguistic phenomena. 3.1 Dataset construction SNLI and SNLIhard test set The SNLI dataset (Bowman et al., 2015) was built through Amazon Mechanical Turk. Workers were shown captions of photographs without the photo and were asked to write a new caption that (a) is definitely a true description of the photo (entailment); (b) might be a tru"
C18-1199,D17-1305,0,0.0112777,"l vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks. We share with this work the idea that the image can be taken as a possible world. However, we don’t use sets of images to obtain the visual denotation of text in order to check whether entailment is logically valid/highly likely. Rather, we take the image to be"
C18-1199,E17-1068,0,0.0142869,"on between linguistic and visual vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks. We share with this work the idea that the image can be taken as a possible world. However, we don’t use sets of images to obtain the visual denotation of text in order to check whether entailment is logically valid/highly likely. Rather, we"
C18-1199,J93-2004,0,0.0604657,"otation of a subset of the SNLI test set. Linguistic phenomena Following the error analysis approach described in recent work (Nangia et al., 2017; Williams et al., 2018), we compiled a new list of linguistic features that can be of interest when contrasting SNLI and SNLIhard , as well as for evaluating RTE models. Some of these were detected automatically, while others were assigned manually. Automatic tags included S YNONYM and A NTONYM, which were detected using WordNet (Miller, 1995). Q UANTIFIER, P RONOUN, D IFF T ENSE, S UPERLATIVE and BARE NP were identified using Penn treebank labels (Marcus et al., 1993), while labels such as N EGATION were found with a straightforward keyword search. The tag L ONG has been assigned to sentence pairs with a premise containing more than 30 tokens, or a hypothesis with more than 16 tokens. Details about the tags used in the manual annotation are presented in Table 3. We examined the differences in the tags distributions between the SNLI and SNLIhard test sets (Table 4). Interestingly, the hard sentence pairs from our random sample include proportionately more antonyms but fewer pronouns, as well as examples with different verb tenses in the premise and hypothes"
C18-1199,marelli-etal-2014-sick,1,0.892995,"the computational linguistics community has invested huge efforts into developing textual entailment (TE) datasets. After formal semanticists developed FraCas in the mid ’90 (Cooper et al., 1996), an increase in statistical approaches to computational semantics gave rise to the need for suitable evaluation datasets. Hence, Recognizing Textual Entailment (RTE) shared tasks have been organized regularly (Sammons et al., 2012). Recent work on compositional distributional models has motivated the development of the SICK dataset of sentence pairs in entailment relations for evaluating such models (Marelli et al., 2014). Further advances with Neural Networks (NNs) have once more motivated efforts to develop a large natural language inference dataset, SNLI (Bowman et al., 2015), since NNs need to be trained on big data. However, meaning is not something we obtain just from text and the ability to reason is not unimodal either. The importance of enriching meaning representations with other modalities has been advocated by cognitive scientists, (e.g., (Andrews et al., 2009; Barsalou, 2010)) and computational linguists (e.g., (Glavaˇs et al., 2017)). While efforts have been put into developing multimodal dataset"
C18-1199,passonneau-etal-2006-inter,0,0.025145,"Missing"
C18-1199,D14-1162,0,0.0809103,"Missing"
C18-1199,P17-2034,0,0.0230488,"d. A suitable GTE model therefore has to perform two sub-tasks: (a) it needs to ground its linguistic representations of P, H or both in non-linguistic (visual) data; (b) it needs to reason about the possible relationship between P and H (modulo the visual information). 2 Related Work Grounding language through vision has recently become the focus of several tasks, including Image Captioning (IC, e.g. (Hodosh et al., 2013; Xu et al., 2015)) and Visual Question Answering (VQA, eg. (Malinowski and Fritz, 2014; Antol et al., 2015)), and even more recently, Visual Reasoning (Johnson et al., 2017; Suhr et al., 2017) and Visual Dialog (Das et al., 2017). Our focus is on Grounded Textual Entailment (GTE). While the literature on TE is rather vast, GTE is still rather unexplored territory. Textual Entailment Throughout the history of Computational Linguistics various datasets have been built to evaluate Computational Semantics models on the TE task. Usually they contain data divided into entailment, contradiction or unknown classes. The “unknown” label has sometimes been replaced with the “unrelated” or “neutral” label, capturing slightly different types of phenomena. Interestingly, the “entailment” and “co"
C18-1199,N18-1101,0,0.0129406,"e book is old. P: A woman is applying lip makeup to another woman, H: The man is ready to fight. P: A group of people are taking a fun train ride, H: People ride the train. P: A crowd gathered on either side of a Soap Box Derby, H: The people are at the race. P: Kids being walked by an adult, H: An adult is escorting some children. P: A woman walks in front of a giant clock, H: The clock walks in front of the woman. Table 3: Tags used in manual annotation of a subset of the SNLI test set. Linguistic phenomena Following the error analysis approach described in recent work (Nangia et al., 2017; Williams et al., 2018), we compiled a new list of linguistic features that can be of interest when contrasting SNLI and SNLIhard , as well as for evaluating RTE models. Some of these were detected automatically, while others were assigned manually. Automatic tags included S YNONYM and A NTONYM, which were detected using WordNet (Miller, 1995). Q UANTIFIER, P RONOUN, D IFF T ENSE, S UPERLATIVE and BARE NP were identified using Penn treebank labels (Marcus et al., 1993), while labels such as N EGATION were found with a straightforward keyword search. The tag L ONG has been assigned to sentence pairs with a premise co"
C18-1199,Q14-1006,0,0.0547872,"given an encoding of the premise (Kolesnyk et al., 2016; Starc and Mladeni´c, 2017). Vision and Textual Entailment In recent years, several models have been proposed to integrate the language and vision modalities; usually the integration is operationalized by element-wise multiplication between linguistic and visual vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spa"
C18-1199,W17-5301,0,\N,Missing
D07-1011,P89-1009,0,0.287637,"description: (1) hORIENTATION : backi ∧ hSIZE : smalli This description is overspecified, because ORI ENTATION is not strictly necessary to distinguish the referents (hSIZE : smalli suffices). Moreover, the description does not include TYPE, though it has been argued that this is always required, as it maps to the head noun of an NP (Dale and Reiter, 1995). We will adopt this assumption here, for reasons explained below. Due to its hillclimbing nature, the IA avoids combinatorial search, unlike some predecessors which searched exhaustively for the briefest possible description of a referent (Dale, 1989), based on a strict interpretation of the Gricean Maxim of Quantity (Grice, 1975). Given that, under the view proposed by Olson (1970) among others, the function of a referential NP is to identify, a strict Gricean interpretation holds that it should contain no more information than necessary to achieve this goal. The Incremental Algorithm constitutes a departure from this view given that it can overspecify through its use of a PO. This has been justified on psycholinguistic grounds. Speakers overspecify their descriptions because they begin their formulation of a reference without exhaustivel"
D07-1011,P02-1013,0,0.71859,"em to the description. This procedure has three consequences: 1. Efficiency: Searching through disjunctive combinations results in a combinatorial explosion (van Deemter, 2002). 2. Gestalts and content: The notion of a ‘preferred attribute’ is obscured, since it is difficult to apply the same reasoning that motivated the PO in the IA to combinations like (COLOUR ∨ SIZE). 1 Note that logical disjunction is usually rendered as linguistic coordination using and. Thus, the table and the desk is the union of things which are desks or tables. 3. Form: Descriptions can become logically very complex (Gardent, 2002; Horacek, 2004). Proposals to deal with (3) include Gardent’s (2002) non-incremental, constraint-based algorithm to generate the briefest available description of a set, an approach extended in Gardent et al. (2004). An alternative, by Horacek (2004), combines bestfirst search with optimisation to reduce logical complexity. Neither approach benefits from empirical grounding, and both leave open the question of whether previous psycholinguistic research on singular reference is applicable to plurals. This paper reports a corpus-based analysis of plural descriptions elicited in well-defined dom"
D07-1011,W07-2307,1,0.857995,"Missing"
D07-1011,J02-1003,1,0.938162,"Missing"
D07-1011,W06-1410,0,0.05089,"xtended in Gardent et al. (2004). An alternative, by Horacek (2004), combines bestfirst search with optimisation to reduce logical complexity. Neither approach benefits from empirical grounding, and both leave open the question of whether previous psycholinguistic research on singular reference is applicable to plurals. This paper reports a corpus-based analysis of plural descriptions elicited in well-defined domains, of which Table 1 is an example. This study falls within a recent trend in which empirical issues in GRE have begun to be tackled (Gupta and Stent, 2005; Jordan and Walker, 2005; Viethen and Dale, 2006). We then propose an efficient algorithm for the generation of references to arbitrary sets, which combines corpusderived heuristics and a partitioning-based procedure, comparing this to IAbool . Unlike van Deemter (2002), we only focus on disjunction, leaving negation aside. Our starting point is the assumption that plurals, like singulars, evince preferences for certain attributes as predicted by the Conceptual Gestalts Principle. Based on previous work in Gestalt perception (Wertheimer, 1938; Rock, 1983), we propose an extension of this to sets, whereby plural descriptions are preferred if"
D07-1011,W06-1420,1,\N,Missing
E06-1041,P89-1009,0,0.776876,"uate its performance. 1 Introduction The problem of Generating Referring Expressions (GRE) can be summed up as a search for the properties in a knowledge base (KB) whose combination uniquely distinguishes a set of referents from their distractors. The content determination strategy adopted in such algorithms is usually based on the assumption (made explicit in Reiter (1990)) that the space of possible descriptions is partially ordered with respect to some principle(s) which determine their adequacy. Traditionally, these principles have been defined via an interpretation of the Gricean maxims (Dale, 1989; Reiter, 1990; Dale and Reiter, 1995; van Deemter, 2002)1. However, little attention has been paid to contextual or intentional influences on attribute selection (but cf. Jordan and Walker (2000); Krahmer and Theune (2002)). Furthermore, it is often assumed that all relevant knowledge about domain objects is represented in the database in a format (e.g. attribute-value pairs) that requires no further processing. This paper is concerned with two scenarios which raise problems for such an approach to GRE: 1. Real-valued attributes, e.g. size or spatial coordinates, which represent continuous di"
E06-1041,E89-1022,0,0.0600853,"icature that the entity’s academic role or profession is somehow relevant to the current discourse. When two entities are described using contrasting properties, say the student and the italian, the listener may find it harder to work out the relevance of the contrast. In a related vein, Aloni (2002) formalises the appropriateness of an answer to a question of the form Wh x? with reference to the ‘conceptual covers’ or perspectives under which x can be conceptualised, not all of which are equally relevant given the hearer’s information state and the discourse context. With respect to plurals, Eschenbach et al. (1989) argue that the generation of a plural anaphor with a split antecedent is more felicitous when the antecedents have something in common, such as their ontological category. This constraint has been shown to hold psycholinguistically (Kaup et al., 2002; Koh and Clifton, 2002; Moxey et al., 2004). Gatt and van Deemter (2005a) have shown that people’s perception of the adequacy of plural descriptions of the form, the N1 and (the) N2 is significantly correlated with the semantic similarity of N1 and N2 , while singular descriptions are more likely to be aggregated into a plural if semantically sim"
E06-1041,P00-1024,0,0.238234,"uniquely distinguishes a set of referents from their distractors. The content determination strategy adopted in such algorithms is usually based on the assumption (made explicit in Reiter (1990)) that the space of possible descriptions is partially ordered with respect to some principle(s) which determine their adequacy. Traditionally, these principles have been defined via an interpretation of the Gricean maxims (Dale, 1989; Reiter, 1990; Dale and Reiter, 1995; van Deemter, 2002)1. However, little attention has been paid to contextual or intentional influences on attribute selection (but cf. Jordan and Walker (2000); Krahmer and Theune (2002)). Furthermore, it is often assumed that all relevant knowledge about domain objects is represented in the database in a format (e.g. attribute-value pairs) that requires no further processing. This paper is concerned with two scenarios which raise problems for such an approach to GRE: 1. Real-valued attributes, e.g. size or spatial coordinates, which represent continuous dimensions. The utility of such attributes depends on whether a set of referents have values that are ‘sufficiently 1 For example, the Gricean Brevity maxim (Grice, 1975) has been interpreted as a d"
E06-1041,P89-1008,0,0.396609,"larity’ is not guaranteed simply by attempting to use values of the same attribute(s) for a given set of referents. The description the student 322 and the chef for {e1 , e3 } is relatively odd compared to the alternative the englishman and the greek. In both kinds of scenarios, a GRE algorithm that relied on a rigid preference order could not guarantee that a coherent description would be generated every time it was available. The issues raised here have never been systematically addressed in the GRE literature, although support for the underlying intuitions can be found in various quarters. Kronfeld (1989) distinguishes between functionally and conversationally relevant descriptions. A description is functionally relevant if it succeeds in distinguishing the intended referent(s), but conversational relevance arises in part from implicatures carried by the use of attributes in context. For example, describing e1 as the student carries the (Gricean) implicature that the entity’s academic role or profession is somehow relevant to the current discourse. When two entities are described using contrasting properties, say the student and the italian, the listener may find it harder to work out the rele"
E06-1041,P90-1013,0,0.0702919,"resolved if some structure is imposed on the available knowledge prior to content determination. We describe a clustering algorithm which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance. 1 Introduction The problem of Generating Referring Expressions (GRE) can be summed up as a search for the properties in a knowledge base (KB) whose combination uniquely distinguishes a set of referents from their distractors. The content determination strategy adopted in such algorithms is usually based on the assumption (made explicit in Reiter (1990)) that the space of possible descriptions is partially ordered with respect to some principle(s) which determine their adequacy. Traditionally, these principles have been defined via an interpretation of the Gricean maxims (Dale, 1989; Reiter, 1990; Dale and Reiter, 1995; van Deemter, 2002)1. However, little attention has been paid to contextual or intentional influences on attribute selection (but cf. Jordan and Walker (2000); Krahmer and Theune (2002)). Furthermore, it is often assumed that all relevant knowledge about domain objects is represented in the database in a format (e.g. attribute"
E06-1041,W00-1424,0,0.0755711,"Missing"
E06-1041,J02-1003,0,0.421072,"Missing"
L18-1525,P15-2017,0,0.0326605,"ni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a significant amount of descriptions that can be found as-is in the training set (Devlin et al., 2015, Tanti et al., 2018). This suggests that the datasets on 3324 which they are trained are very repetitive and lack diversity. State of the art image captioning requires large datasets for training and testing. While such datasets do exist for scene descriptions, no data is currently available for the face description task, despite the existence of annotated image datasets. In the following section we describe how we addressed this lacuna, initiating an ongoing crowd-sourcing exercise to create a large dataset of face descriptions, paired with images which are annotated with physical features."
L18-1525,D13-1128,0,0.0226852,"n a preliminary version of the corpus, focussing on how it was collected and evaluated.1 2. Background Automatic image description research can rely on a wide range of image-description datasets. Such datasets consist of images depicting various objects and actions, and associated descriptions, typically collected through crowdsourcing. The descriptions verbalise the objects and events or relations shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), mentioning what is visible, while minimising inferences that can be drawn from the visual information. Other datasets are somewhat more specialised. For example, the CaltechUCSD Birds and Oxford Flowers-102 contain fine-grained 1 The corpus will shortly be released to the public. The current version is available upon request. visual descriptions of images of birds and flowers respectively (Reed et al., 2016). Some datasets also conta"
L18-1525,W16-3210,0,0.0352719,"Missing"
L18-1525,P12-1038,0,0.0304027,"n generation are based either on caption retrieval or direct generation (Bernardi et al., 2016). In the generation-by-retrieval approach, human authored descriptions for similar images are stored in a database of image-description pairs. Given an input image that is to be described, the database is queried to find the most similar images to the input image and the descriptions of these images are returned. The descriptions are then either copied directly (which assumes that descriptions can be reused as-is with similar images) or synthesized from extracted phrases. (Ordonez et al., 2011) and (Kuznetsova et al., 2012) are examples of retrieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct generation attempts to generate novel descriptions using natural language generation techniques. Traditionally, this was achieved by using computer vision (CV) detectors which are applied to generate a list of image content (e.g objects and their attributes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarn"
L18-1525,E12-1076,0,0.0415758,"ieval in visual space; other approaches rely on retrieval in multimodal space (Hodosh et al., 2013, Socher et al., 2014). On the other hand, direct generation attempts to generate novel descriptions using natural language generation techniques. Traditionally, this was achieved by using computer vision (CV) detectors which are applied to generate a list of image content (e.g objects and their attributes, spatial relationships, and actions). These are fed into a classical NLG pipeline that produces a textual description, verbalising the salient aspects of the image. (Kulkarni et al., 2011) and (Mitchell et al., 2012) are early examples of such systems. The state of the art in image description makes use of deep learning approaches, usually relying on a neural language model to generate descriptions based on image analysis conducted via a pre-trained convolutional network (Vinyals et al., 2015, Mao et al., 2015, Xu et al., 2015, Rennie et al., 2016). While these systems are currently the state of the art, they suffer from a tendency to generate repetitive descriptions by generating a significant amount of descriptions that can be found as-is in the training set (Devlin et al., 2015, Tanti et al., 2018). Th"
L18-1525,Q14-1006,0,0.11024,"scriptions. Here we report on a preliminary version of the corpus, focussing on how it was collected and evaluated.1 2. Background Automatic image description research can rely on a wide range of image-description datasets. Such datasets consist of images depicting various objects and actions, and associated descriptions, typically collected through crowdsourcing. The descriptions verbalise the objects and events or relations shown in the images with different degrees of granularity. For example, the most widely-used image captioning datsets, such as Flickr8k (Hodosh et al., 2013), Flickr30K (Young et al., 2014), VLT2K (Elliott and Keller, 2013), and MS COCO (Lin et al., 2014), contain images of familiar scenes, and the descriptions are restricted to the ‘concrete conceptual’ level (Hodosh et al., 2013), mentioning what is visible, while minimising inferences that can be drawn from the visual information. Other datasets are somewhat more specialised. For example, the CaltechUCSD Birds and Oxford Flowers-102 contain fine-grained 1 The corpus will shortly be released to the public. The current version is available upon request. visual descriptions of images of birds and flowers respectively (Reed et al"
P06-2033,E06-1041,1,0.789342,"y by assuming a one-to-one mapping between properties and words. Another requirement is to distinguish between type properties (the set T ), and non-types (M )3 . The Thesaurus is used to find pairwise similarity of types in order to group them into related clusters. Word Sketches are used to find, for each type, the modifiers in the KB that are appropriate to the type, on the basis of the associated salience values. For example, in Table 3, e3 has plump as the value for girth, which combines more felicitously with man, than with biologist. Types are clustered using the algorithm described in Gatt (2006). For each type t, the algorithm finds its nearest neighbour nt in semantic space. Clusters are then found by recursively grouping elements with their nearest neighbours. If t, t′ have a common nearest neighbour n, then {t, t′ , n} is a cluster. Clearly, the resulting sets are convex in the sense of Definition 1. Each modifier is assigned to a cluster by finding in its Word Sketch the type with which it co-occurs with the greatest salience value. Thus, a cluster is a pair Definition 2. Maximal coherence A description D is maximally coherent iff there is no description D ′ coextensive with D su"
P06-2033,P00-1019,0,0.0485914,"Missing"
P06-2033,P89-1008,0,0.115753,"the top row, IA would yield the postgraduate and the chef, which is fine in case occupation is the relevant attribute in the discourse, but otherwise is arguably worse than an alternative like the italian and the maltese, because it is more difficult to see what a postgraduate and a chef have in common. Related issues have been raised in the formal semantics literature. Aloni (2002) argues that an appropriate answer to a question of the form ‘Wh x?’ must conceptualise the different instantiations of x using a perspective which is relevant given the hearer’s information state and the context. Kronfeld (1989) distinguishes a description’s functional relevance – i.e. its success in distinguishing a referent – from its conversational relevance, which arises in part from implicatures. In our example, describing e1 as the postgraduate carries the implicature that the entity’s academic role is relevant. When two entities are described using contrasting properties, say the student and the italian, the contrast may be misleading for the listener. Any attempt to port these observations to the GRE scenario must do so without sacrificing logical completeness. While a GRE algorithm should attempt to find the"
P06-2033,E99-1005,0,0.0128217,"r’ = 1,2,3; ‘dissimilar’ = 4) showed a significant difference in proportions for the nominal category (χ2 = 4.78, p = .03), but not the modifier category. Pairwise comparisons showed a significantly larger proportion of 1 (Z = 2.7, p = .007) and 2 responses (Z = 2.54, p = .01) in the nominal compared to the modifier condition. id base type occupation specialisation girth e1 woman professor physicist plump e2 woman lecturer geologist thin e3 man lecturer biologist plump e4 man chemist thin Table 2: An example knowledge base restrictions on the plausibility of adjective-noun combinations exist (Lapata et al., 1999), and that using unlikely combinations (e.g. the immaculate kitchen rather than the spotless kitchen) impacts processing in online tasks (Murphy, 1984). Unlike types, which have a categorisation function, modifiers have the role of adding information about an element of a category. This would partially explain the experimental results: When elements of a plurality have identical types (as in the modifier version of our experiment), the CC is already satisfied, and selection of modifiers would presumably depend on respecting adjective-noun combination restrictions. Further research is required"
P06-2033,J90-1003,0,0.0661115,"Missing"
P06-2033,P89-1009,0,0.614083,"en the avoidance of redundancy and achieving coherence. It is to an investigation of this tension that we now turn. Definition 3. Local coherence A description D is locally coherent iff: a. either D is maximally coherent or b. there is no D ′ coextensive with D, obtained by replacing types from some perspective in PD with types from another perspective such that w(D) &gt; w(D ′ ). 4 Evaluation It has been known at least since Dale and Reiter (1995) that the best distinguishing description is not always the shortest one. Yet, brevity plays a part in all GRE algorithms, sometimes in a strict form (Dale, 1989), or by letting the algorithm approximate the shortest description (for example, in the Dale and Reiter’s IA). This is also true of references to sets, the clearest example being Gardent’s constraint based approach, which always finds the description with the smallest number of logical operators. Such proposals do not take coherence (in our sense of the word) into account. This raises obvious questions about the relative importance of brevity and coherence in reference to sets. The evaluation took the form of an experiment to compare the output of our Coherence Model with the family of algorit"
P06-2033,P02-1013,0,0.439447,"ence Constraint (CC): As far as possible, describe objects using related properties. 1 Introduction Algorithms for the Generation of Referring Expressions (GRE) seek a set of properties that distinguish an intended referent from its distractors in a knowledge base. Much of the GRE literature has focused on developing efficient content determination strategies that output the best available description according to some interpretation of the Gricean maxims (Dale and Reiter, 1995), especially Brevity. Work on reference to sets has also proceeded within this general framework (van Deemter, 2002; Gardent, 2002; Horacek, 2004). One problem that has not received much attention is that of conceptual coherence in the generation of plural references, i.e. the ascription of related properties to elements of a set, so that the resulting description constitutes a coherent cover for the plurality. As an example, consider a reference to {e1 , e3 } in Table 1 using the Incremental Algorithm (IA) (Dale and Reiter, 1995). IA searches along an ordered list of attributes, selecting properties of the intended referents that remove some distractors. Assuming the ordering in the top row, IA would yield the postgradu"
P06-2033,P04-1052,0,0.0505963,"Missing"
P06-2033,J02-1003,1,0.918351,"Missing"
P08-2050,W00-1401,0,0.0648136,"the similarity between a peer attribute set A1 and a (human-produced) reference 1 ∩A2 | attribute set A2 as 2×|A |A1 |+|A2 |. MASI (Passonneau, 2006) is similar but biased in favour of similarity where one set is a subset of the other. 4. String-similarity measures: In order to apply string-similarity metrics, peer and reference outputs were converted to word-strings by the method described under 1 above. String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion cost of 1. We also used the version of string-edit distance (‘SEB’) of Bangalore et al. (2000) which normalises for length. BLEU computes the proportion of word ngrams (n ≤ 4 is standard) that a peer output shares with several reference outputs. The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU which gives more importance to less frequent (hence more informative) n-grams. We also used two versions of the ROUGE metric (Lin and Hovy, 2003), ROUGE-2 and ROUGE-SU4 (based on non-contiguous, or ‘skip’, n-grams), which were official scores in the DUC 2005 summarization task. 4 Results Results for all evaluation measures and all systems are shown in Table 1. Uniqueness"
P08-2050,2007.mtsummit-ucnlg.14,1,0.910466,"a and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse."
P08-2050,E06-1040,1,0.684901,"task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 2 Task, Data and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this"
P08-2050,P89-1009,0,0.360234,"ents in particular) are dominated by intrinsic evaluation methodologies, in contrast to the more extrinsic evaluation traditions of NLG . In this paper, we present research in which we applied both intrinsic and extrinsic evaluation methods to the same task, in order to shed light on how Referring expression generation (REG) is concerned with the generation of expressions that describe entities in a given piece of discourse. REG research goes back at least to the 1980s (Appelt, Grosz, Joshi, McDonald and others), but the field as it is today was shaped in particular by Dale and Reiter’s work (Dale, 1989; Dale and Reiter, 1995). REG tends to be divided into the stages of attribute selection (selecting properties of entities) and realisation (converting selected properties into word strings). Attribute selection in its standard formulation was the shared task in the ASGRE Challenge: given an intended referent (‘target’) and the other domain entities (‘distractors’) each with possible attributes, select a set of attributes for the target referent. The ASGRE data (which is now publicly available) consists of all 780 singular items in the TUNA corpus (Gatt et al., 2007) in two subdomains, consist"
P08-2050,W07-2307,1,0.613693,"Missing"
P08-2050,W02-2103,0,0.0151408,"current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task. 2 Task, Data and Systems 1 Introduction In recent years, NLG evaluation has taken on a more comparative character. NLG now has evaluation results for comparable, but independently developed systems, including results for systems that regenerate the Penn Treebank (Langkilde, 2002) and systems that generate weather forecasts (Belz and Reiter, 2006). The growing interest in comparative evaluation has also resulted in a tentative interest in shared-task evaluation events, which led to the first such event for NLG (the Attribute Selection for Generation of Referring Expressions, or ASGRE, Challenge) in 2007 (Belz and Gatt, 2007), with a second event (the Referring Expression Generation, or REG, Challenge) currently underway. In HLT in general, comparative evaluations (and shared-task evaluation events in particular) are dominated by intrinsic evaluation methodologies, in c"
P08-2050,N03-1020,0,0.107985,"Missing"
P08-2050,passonneau-2006-measuring,0,0.111661,"Missing"
P19-1246,K16-1018,0,0.0141984,"determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics"
P19-1246,P16-2051,0,0.0387176,"Missing"
P19-1246,P18-2061,1,0.832182,"Missing"
P19-1246,D15-1162,0,0.0809055,"hese non-lexical feature representations, we use a Convolutional Neural Network (CNN) classifier (LeCun et al., 1995), rather than rely on sparse models as we did for our lexical baseline.4 The model consists of a single convolutional layer coupled with a sum-pooling operation; a Multi-Layer Perceptron on top improves discrimination performance between classes. We use the Adam optimizer (Kingma and Ba, 2015) with a fixed learning rate (0.001) and L2 regularization (Ng, 2004); a dropout layer (0.2) (Srivastava et al., 2014) helps to prevent overfitting. For the implementation we rely on spaCy (Honnibal and Johnson, 2015). 6.1 Bleached representation Recently, van der Goot et al. (2018) introduced a language-independent representation termed bleaching for capturing gender differences in writing style, while abstracting away from lexical information. Bleaching preserves surface information while obfuscating lexical content. This allows a focus on lexical variation as a function of personal style, while reducing the possible influence of topic as a determining factor. We experiment with this idea under the assumption that authors belonging to different groups will show a difference in the formality of their writ"
P19-1246,P15-2079,0,0.0240695,"butes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervision, exploiting occupation information in Twitter p"
P19-1246,K15-1011,0,0.128815,"price/prestige range, under the assumption that customers (and salespersons) of these establishments would belong to different social strata. ""[Labov’s study] was designed to test two ideas [. . . ]: first, that the variable (r) is a social differentiator in all levels of New York City speech; and second, that casual and anonymous speech events could be used as the basis for a systematic study of language."" (Labov, 2006, p. 40. Italics ours.) Inspired by Labov’s work and the recent surge of interest in computational social science (CioffiRevilla, 2016) and computational sociolinguistics (e.g. Johannsen et al., 2015), we set out to investigate whether and to what extent variations in writing style, analysed in terms of several linguistic variables, are influenced by socio-economic status (RQ1; see below). To do so, we use user-generated restaurant reviews on social media. User-generated content bears important similarities to Labov’s ""casual and anonymous speech events"" on at least two fronts: 1) anonymity is here still preserved since we are not including personal information about the authors; furthermore 2) social media are now recognised in the literature as a source of naturally (i.e. casual) occurri"
P19-1246,P12-3005,0,0.0415768,"ains more than 5 million documents, from over 1 million authors, with a Zipfian distribution: a small number of authors publish most of the reviews, while most of the authors only leave one review. Grouping reviews per author and filtering out authors with only one review reduces the final dataset to fewer than a thousand authors, though this set of reviews is large and allows us to infer demographic information about the reviewers (see also Hovy et al., 2015). Language The Yelp! dataset contains reviews written in multiple languages, though the vast majority are in English. We use langid.py (Lui and Baldwin, 2012) to automatically detect and filter out non-English instances. The need for both good parsing performance and large quantity of text limits us from working with data from other languages. rant as a proxy for socio-economic status. The average price of a meal in a restaurant is encoded by four labels: $, $$, $$$, $$$$. As a first, coarse step, we accept this representation and divide our population into four groups. We group all of the reviews per author and represent each author as a vector, where each element is the price range of a restaurant reviewed by the user. We compute the mode of this"
P19-1246,W17-5007,0,0.0279096,"computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text. While lexical information is highly predictive, it is restricted to topic. In contrast, syntactic information is almost as predictive and is a much better signal for stylistic varia2590 tion. From a methodological point of view, we can draw two conclusions from this work. First, as has been noted (Plank et al., 2016), neural networks can perform well with relatively small datasets, in this case proving competitive with the sparse models that are usually favoured in author profiling (Malmasi et al., 2017; Basile et al., 2018). Second, distant supervision with proxy labels for socio-economic status yields useful insights and is validated externally via readability scores. This is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings. Nevertheless, there are limitations of distant labelling and social media data — with issues related specifically to the language of food (Askalidis and Malthouse, 2016) — that we will take into account in future work. First, we wish to investigate the role of additional variables (such as age and"
P19-1246,petrov-etal-2012-universal,0,0.0194458,"Missing"
P19-1246,C14-1168,0,0.0281915,"ic status influences language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general"
P19-1246,P16-2067,0,0.015995,"2014) finds an interesting correlation between the price range of a restaurant and the lengths of food names on its menu. 9 Conclusion Inspired by Labov and encouraged by recent interest in computational sociolinguistics, we developed accurate neural models to predict socioeconomic status from text. While lexical information is highly predictive, it is restricted to topic. In contrast, syntactic information is almost as predictive and is a much better signal for stylistic varia2590 tion. From a methodological point of view, we can draw two conclusions from this work. First, as has been noted (Plank et al., 2016), neural networks can perform well with relatively small datasets, in this case proving competitive with the sparse models that are usually favoured in author profiling (Malmasi et al., 2017; Basile et al., 2018). Second, distant supervision with proxy labels for socio-economic status yields useful insights and is validated externally via readability scores. This is encouraging for further studies in computational social science in ecologically valid and relatively labour-free settings. Nevertheless, there are limitations of distant labelling and social media data — with issues related specifi"
P19-1246,W16-4304,1,0.859234,"language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computatio"
P19-1246,W14-5307,0,0.0243786,"Missing"
P19-1246,P05-2008,0,0.100302,"el). 8 Related Work The idea that socio-economic status influences language use and is a determinant of language variation has been central to sociolinguistic theory for a long time (Bernstein, 1960; Labov, 1972, 2006). Labov’s work could be viewed as an early form of distant supervision, exploiting established categories (e.g. the price and status of establishments such as department stores) to draw inferences about variables related to social stratification. The work presented here takes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Ver"
P19-1246,D17-1035,0,0.0487601,"Missing"
P19-1246,W11-0310,0,0.0280438,"kes inspiration from this paradigm, and contributes to the growing literature on distant supervision in NLP (Read, 2005), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervisio"
P19-1246,L16-1258,0,0.025161,"05), especially in social media (e.g. Plank et al., 2014; Pool and Nissim, 2016; Fang and Cohn, 2016; Basile et al., 2017; Klinger, 2017, inter alia). Computational work on style – i.e. linguistic features characteristic of an individual or group (Biber, 1988) – has focussed on demographic or personal variables, ranging from geographical location and dialect (Zampieri et al., 2014; Han et al., 2014; Eisenstein, 2013) to age and gender (Argamon et al., 2007; Newman et al., 2008; Sarawgi et al., 2011; Johannsen et al., 2015; Hovy and Søgaard, 2015), as well as personality (Argamon et al., 2005; Verhoeven et al., 2016; Youyou et al., 2015). An general overview of computational sociolinguistics can be found in Nguyen et al. (2016). By contrast, there has been relatively little work on socio-economic status. Flekova et al. (2016) show that textual features can predict income, demonstrating a relationship between this and age. Lampos et al. (2016) also report good results on inferring the socio-economic status of social media users from text. Like the present work, they use distant supervision, exploiting occupation information in Twitter profiles. Our work differs from these precedents in that we investigate"
P19-1246,N16-1174,0,0.0138096,"th labels which denote the social class of the authors we adopt the paradigm of distant supervision. We take the price range of the restau(c) Balance: the raw data is highly skewed towards class $$ (Figure 2), but for our experiments we want equally represented classes to avoid any size-related effects. 1 The repository contains all code and models which can be run by acquiring the freely available Yelp dataset. 2 This data is released within the context of the Yelp! Challenge, a multi-domain shared task which has attracted attention in NLP primarily for benchmarking text classification (e.g. Yang et al., 2016)). We use the dataset released for Round 11. Y $ of reviewed restaurant In order to address (a), we employ an entropybased strategy to filter out noisier data points. This 2584 Table 2 shows the final label and token distribution, after filtering and downsampling. In Figure 3, we show two sample reviews, one from class $ and one from class $$$$. $$ $ $$$ $$$$ Figure 2: Author distribution before filtering. While users belonging to class $$$$ might visit cheaper places, the same is not true in the opposite direction: this explains the small size of class $$$$. is described below. For the size-"
P19-1246,J16-3007,0,\N,Missing
P19-1246,W13-1706,0,\N,Missing
P19-1246,W13-1102,0,\N,Missing
R09-1021,E06-1040,0,0.0141581,"ons of morphosyntactic realisers [11, 3]. Extrinsic, task-based methods are also widespread [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human judgements, suggesting that they are measuring different aspects of output quality [2]. Cautionary notes have also been sounded in Machine Translation [5] and summarisation [6], with some recent work in NLG showing that intrinsic measures also correlate poorly with task-based measures NLG [1]. On the other hand, the relationship between task-based measures and textual characteristics bears on several important questions. Task-based evaluations tend to yield global scores from which it is often hard to extract specific indicators of a system’s weaknesses. While judgement elicitation studies may be better suited to this purpose, these do not necessarily reflect a system’s utility"
R09-1021,E06-1032,0,0.048697,"ods are also widespread [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human judgements, suggesting that they are measuring different aspects of output quality [2]. Cautionary notes have also been sounded in Machine Translation [5] and summarisation [6], with some recent work in NLG showing that intrinsic measures also correlate poorly with task-based measures NLG [1]. On the other hand, the relationship between task-based measures and textual characteristics bears on several important questions. Task-based evaluations tend to yield global scores from which it is often hard to extract specific indicators of a system’s weaknesses. While judgement elicitation studies may be better suited to this purpose, these do not necessarily reflect a system’s utility in a task, while corpus-based studies necessarily depend on a finit"
R09-1021,W05-0901,0,0.251444,"d [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human judgements, suggesting that they are measuring different aspects of output quality [2]. Cautionary notes have also been sounded in Machine Translation [5] and summarisation [6], with some recent work in NLG showing that intrinsic measures also correlate poorly with task-based measures NLG [1]. On the other hand, the relationship between task-based measures and textual characteristics bears on several important questions. Task-based evaluations tend to yield global scores from which it is often hard to extract specific indicators of a system’s weaknesses. While judgement elicitation studies may be better suited to this purpose, these do not necessarily reflect a system’s utility in a task, while corpus-based studies necessarily depend on a finite number of reference"
R09-1021,W08-1113,0,0.450122,"ated text – particularly its informativeness and relevance – and its utility for decision-making. Second, we propose a novel intrinsic evaluation method. Rather than comparing BT-45 texts to a gold standard based on surface characteristics, such as matching n−grams, we make explicit use of domain knowledge in the form of an ontology and attempt to quantify the differences in the content selection strategies underlying the BT-45 and the gold standard texts. Background Intrinsic evaluation in NLG has often relied on human input, typically in the form of ratings of or responses to questionnaires [12, 4, 7]. Automatic intrinsic methods exploiting corpora have mostly been used in evaluations of morphosyntactic realisers [11, 3]. Extrinsic, task-based methods are also widespread [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human j"
R09-1021,P08-2050,1,0.844307,"a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human judgements, suggesting that they are measuring different aspects of output quality [2]. Cautionary notes have also been sounded in Machine Translation [5] and summarisation [6], with some recent work in NLG showing that intrinsic measures also correlate poorly with task-based measures NLG [1]. On the other hand, the relationship between task-based measures and textual characteristics bears on several important questions. Task-based evaluations tend to yield global scores from which it is often hard to extract specific indicators of a system’s weaknesses. While judgement elicitation studies may be better suited to this purpose, these do not necessarily reflect a system’s utility in a task, while corpus-based studies necessarily depend on a finite number of reference outputs against which to compare a system, which do not exhaust the space of possibilities. 3 The BT-45 system and ev"
R09-1021,W02-2103,0,0.187289,"intrinsic evaluation method. Rather than comparing BT-45 texts to a gold standard based on surface characteristics, such as matching n−grams, we make explicit use of domain knowledge in the form of an ontology and attempt to quantify the differences in the content selection strategies underlying the BT-45 and the gold standard texts. Background Intrinsic evaluation in NLG has often relied on human input, typically in the form of ratings of or responses to questionnaires [12, 4, 7]. Automatic intrinsic methods exploiting corpora have mostly been used in evaluations of morphosyntactic realisers [11, 3]. Extrinsic, task-based methods are also widespread [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human judgements, suggesting that they are measuring different aspects of output quality [2]. Cautionary notes have also been sou"
R09-1021,J97-1004,0,0.501307,"ated text – particularly its informativeness and relevance – and its utility for decision-making. Second, we propose a novel intrinsic evaluation method. Rather than comparing BT-45 texts to a gold standard based on surface characteristics, such as matching n−grams, we make explicit use of domain knowledge in the form of an ontology and attempt to quantify the differences in the content selection strategies underlying the BT-45 and the gold standard texts. Background Intrinsic evaluation in NLG has often relied on human input, typically in the form of ratings of or responses to questionnaires [12, 4, 7]. Automatic intrinsic methods exploiting corpora have mostly been used in evaluations of morphosyntactic realisers [11, 3]. Extrinsic, task-based methods are also widespread [14, 10, 15]. While such studies tend to be more expensive and labour-intensive, extrinsic evaluation criteria give a reliable assessment of the system’s utility in doing what it was designed to do. The relationship between these different classes of evaluation methods is not straightforward. Recent work has shown that corpus-based intrinsic methods do not correlate with the results of intrinsic evaluation based on human j"
R09-1021,karasimos-isard-2004-multi,0,\N,Missing
rosner-etal-2012-incorporating,reynaert-2008-errors,0,\N,Missing
rosner-etal-2012-incorporating,C90-2036,0,\N,Missing
W06-1420,J02-1003,1,0.640807,"Missing"
W06-1420,P00-1024,0,0.140798,"Missing"
W06-1420,P90-1013,0,0.431905,"Missing"
W07-2307,P02-1013,0,0.716767,"Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and Reiter’s (1995) contribution, adapted to also deal with pluralities and gradable properties. A"
W07-2307,P97-1027,0,0.0435983,"the Generation of Referring Expressions using a balanced corpus Albert Gatt and Ielka van der Sluis and Kees van Deemter Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that fo"
W07-2307,P06-1131,0,0.148513,"of Referring Expressions using a balanced corpus Albert Gatt and Ielka van der Sluis and Kees van Deemter Department of Computing Science University of Aberdeen {agatt,ivdsluis,kvdeemte}@csd.abdn.ac.uk ter match to human referential behaviour.1 This was due in part to the way the IA searches for a distinguishing description by performing gradient descent along a predetermined list of domain attributes, called the preference order, whose ranking reflects general or domain-specific preferences (see §4.1). The Incremental Algorithm has served as a starting point for later models (Horacek, 1997; Kelleher and Kruijff, 2006), and has also served as a yardstick against which to compare other approaches (Gardent, 2002; Jordan and Walker, 2005). Despite its influence, few empirical evaluations have focused on the IA. Evaluation is even more desirable given the dependency of the algorithm on a preference order, which can radically change its behaviour, so that in a domain with n attributes, there are in principle n! different algorithms. This paper is concerned with applying a corpusbased methodology to evaluate content determination for GRE, comparing the three classic algorithms that formed the basis for Dale and R"
W07-2307,P04-1052,0,0.0140927,"ub-corpus described in §3 was conducted, using the same overall setup as the present study. In this section, we briefly discuss our main findings in this sub-corpus. A more detailed comparison of the evaluation results on the furniture domain with parallel results on the people domain is reported elsewhere. The targets in the people corpus differ from their distractors only in whether they had a beard (HAS - Conclusions In recent years, GRE has extended considerably beyond what was seen as its remit a decade ago, for example by taking linguistic context into account (Krahmer and Theune, 2002; Siddharthan and Copestake, 2004). We have been conservative by focusing on three classic algorithms discussed in Dale and Reiter (1995), with straightforward extensions to plurals and gradables. We tested the Incremental Algorithm’s match against speaker behaviour compared to other models using a a balanced, semantically and pragmatically transparent corpus. It turns out that performance depends on the preference order of the attributes that are used by the IA. Preliminary indications from a study on a more complex sub-corpus 55 support this view. This evaluation took a speakeroriented perspective. A reader-oriented perspect"
W07-2307,W00-1416,0,0.0698807,"erents, which had identical values on the MD attributes. For example, both targets might be blue in a domain where the minimally distinguishing description consisted of COLOUR. Plural/Dissimilar (PD): In the remaining 7 Plural trials, the targets had different values of the minimally distinguishing attributes. Plural referents were taken into account because plurality is pervasive in NL discourse. The literature (e.g. Gardent (2002)) suggests that they can be treated adequately by minor variations of the classic GRE algorithms (as long as the descriptions in question refer distributively, cf. Stone (2000)), which is something we considered worth testing. 3.2 Corpus annotation The XML annotation scheme (van der Sluis et al., 2006) pairs each corpus description with a representation of the domain in which it was produced. The domain representation, exemplified in Figure 1(a)), indicates which entities are target referents or distractors, and what combination of the attributes and values in Table 1 they have, as well as their numeric X - DIM and Y- DIM values (row and column numbers). 3 This was manipulated as a second, between-subjects factor. Participants were randomly placed in groups which va"
W07-2307,W06-1420,1,0.577592,"Missing"
W07-2307,J02-1003,1,0.838349,"Missing"
W07-2307,W06-1410,0,0.447421,"Missing"
W07-2307,P89-1009,0,0.411171,"lgorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict. 1 Introduction The current state of the art in the Generation of Referring Expressions (GRE) is dominated by versions of the Incremental Algorithm (IA) of Dale and Reiter (1995). Focusing on the generation of “first-mention” definite descriptions, Dale and Reiter compared the IA to a number of its predecessors, including a Full Brevity (FB) algorithm, which generates descriptions of minimal length, and a Greedy algorithm (GR), which approximates Full Brevity (Dale, 1989). In doing so, the authors focused on Content Determination (CD, which is the purely semantic part of GRE), and on a description’s ability to identify a referent for a hearer. Under this problem definition, GRE algorithms take as input a Knowledge Base (KB), which lists domain entities and their properties (often represented as attribute-value pairs), together with a set of intended referents, R. The output of CD is a distinguishing description of R, that is, a logical form which distinguishes this set from its distractors. Dale and Reiter argued that the IA was a superior model, and predicted"
W07-2307,J06-2002,1,\N,Missing
W07-2307,W02-2113,0,\N,Missing
W08-1108,W07-2307,1,0.897826,"Missing"
W08-1108,karasimos-isard-2004-multi,0,0.0292183,"w these relate to algorithmic properties (Section 4.1 and 4.2). Finally we look at how intrinsic and extrinsic evaluations correlate with each other (Section 4.3). ASGRE and Evaluation Though ASGRE evaluations have been carried out (Gupta and Stent, 2005; Viethen and Dale, 2006; Gatt et al., 2007), these have focused on ‘classic’ algorithms, and have been corpus-based. The absence of task-performance evaluations is surprising, considering the well-defined nature of the ASGRE task, and the predominance of task-performance studies elsewhere in the NLG evaluation literature (Reiter et al., 2003; Karasimos and Isard, 2004). Given the widespread agreement on task definition and input/output specifications, ASGRE was an ideal candidate for the first NLG shared task evaluation challenge. The challenge was first discussed 2 The ASGRE Challenge The ASGRE Challenge used the TUNA Corpus (Gatt et al., 2007), a set of human-produced referring expressions (REs) for entities in visual domains of pictures of furniture or people. The corpus was collected during an online elicitation experiment in which subjects typed descriptions of a target referent in a DOMAIN in which there were also 6 other entities (‘distractors’). Eac"
W08-1108,P06-1131,0,0.061127,"z Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the"
W08-1108,passonneau-2006-measuring,0,0.0571199,"s to the ASGRE Challenge. Properties are indicated in the first column (abbreviations are explained in the table caption). The version of the IS - FBS system that was originally submitted to ASGRE contained a bug and did not actually output minimal attribute sets (but added an arbitrary attribute to each set). Unlike the ASGRE Challenge task-performance evaluation, the analysis presented in this paper uses the corrected version of this system. 3 Intrinsic measures 2 × |DS ∩ DH | |DS |+ |DH | (1) For this paper, we also computed MASI, a version of the Jaccard similarity coefficient proposed by Passonneau (2006) which multiplies the similarity value by a monotonicity coefficient, biasing the measure towards those cases where DS and DH have an empty set difference. Intuitively, this means that those system-produced descriptions are preferred which do not include attributes that are omitted by a human. Thus, two of our intrinsic measures assess Humanlikeness (Dice and MASI), while Minimality reflects the extent to which an algorithm conforms to brevity, one of the principles that has emerged from the ASGRE literature. Evaluation methods 3.2 Evaluation methods can be characterised as either intrinsic or"
W08-1108,P90-1013,0,0.082054,"sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribute set that uniquely refers to the intended referent (Dale, 1989). Another frequent property of ASGRE algorithms is Incrementality which involves selection of attributes one at a time (rather than exhaustive search for a distinguishing set),"
W08-1108,J02-1003,0,0.047089,"Missing"
W08-1108,J06-2002,0,0.0703273,"Missing"
W08-1108,W06-1410,0,0.0312618,"orithmic properties of the participating systems. We thus focus on two issues in ASGRE and its evaluation. We examine the similarities and differences among the systems submitted to the ASGRE Challenge and compare them to classic approaches (Section 2.1). We look at the results of intrinsic and extrinsic evaluations (Section 4) and examine how these relate to algorithmic properties (Section 4.1 and 4.2). Finally we look at how intrinsic and extrinsic evaluations correlate with each other (Section 4.3). ASGRE and Evaluation Though ASGRE evaluations have been carried out (Gupta and Stent, 2005; Viethen and Dale, 2006; Gatt et al., 2007), these have focused on ‘classic’ algorithms, and have been corpus-based. The absence of task-performance evaluations is surprising, considering the well-defined nature of the ASGRE task, and the predominance of task-performance studies elsewhere in the NLG evaluation literature (Reiter et al., 2003; Karasimos and Isard, 2004). Given the widespread agreement on task definition and input/output specifications, ASGRE was an ideal candidate for the first NLG shared task evaluation challenge. The challenge was first discussed 2 The ASGRE Challenge The ASGRE Challenge used the T"
W08-1108,2007.mtsummit-ucnlg.14,1,0.835716,", that is, it bases selection on the extent to which an attribute helps distinguish an entity from its distractors. In the remainder of this paper, we uniformly use the term ’algorithmic property’ for the selection criteria and other properties of ASGRE algorithms described above, and refer to them by the following short forms: Full Brevity, Uniqueness, Discriminatory Power, Hardwired Type Selection, Human Preference Modelling and Incrementality.1 1.2 during a workshop held at Arlington, Va. (Dale and White, 2007), and eventually organised as part of the UCNLG + MT Workshop in September 2007 (Belz and Gatt, 2007). The ASGRE Shared Task provided an opportunity to (a) assess the extent to which the field has diversified since its inception; (b) carry out a comparative evaluation involving both automatic methods and human task-performance methods. 1.3 Overview In the ASGRE Challenge report (Belz and Gatt, 2007) we presented the results of the ASGRE Challenge evaluations objectively and with little interpretation. In this paper, we present the results of a new task-performance evaluation and a new intrinsic measure involving the same 15 systems and compare the new results with the earlier ones. We also ex"
W08-1108,P08-2050,1,0.858454,"possibly arising from the use of attributes (such as SIZE) which impose a greater cognitive load on the reader. The very strong correlation (0.97) between Dice and MASI is to be expected, given the similarity in the way they are defined. Another unambiguous result emerges: none of the similarity-based metrics covary significantly with any of the task-performance measures. An extended analysis involving a larger range of intrinsic metrics confirmed this lack of significant covariation for string-based similarity metrics as well as setsimilarity metrics across two task-performance experiments (Belz and Gatt, 2008). This indicates that at least for some areas of HLT, task-performance evaluation is vital: without the external reality check provided by extrinsic evaluations, intrinsic evaluations may end up being too self-contained and disconnected from notions of usefulness to provide a meaningful assessment of systems’ quality. 5 Acknowledgements We gratefully acknowledge the contribution made to the evaluations by the faculty and staff at Brighton University who participated in the identification experiments. The biggest contribution was, of course, made by the participants in the ASGRE Challenge who c"
W08-1108,E91-1028,0,0.0873793,"gatt@abdn.ac.uk Anja Belz Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a c"
W08-1108,P89-1009,0,0.51902,"neration: New Algorithms and Evaluation Methods Albert Gatt Department of Computing Science University of Aberdeen Aberdeen AB24 3UE, UK a.gatt@abdn.ac.uk Anja Belz Natural Language Technology Group University of Brighton Brighton BN2 4GJ, UK a.s.belz@brighton.ac.uk Abstract focus has increasingly been on definite descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds t"
W08-1108,P02-1013,0,0.136631,"te descriptions and identification, where the set of attributes selected should uniquely distinguish the intended referent from other entities (its ‘distractors’). Unique Reference in this sense is a dominant criterion for selecting attribute sets in classic ASGRE algorithms. Following Dale (1989), and especially Dale and Reiter (1995), several contributions have extended the remit of ASGRE algorithms to handle relations (Dale and Haddock, 1991; Kelleher and Kruijff, 2006) and gradable attributes (van Deemter, 2006); and also to guarantee logical completeness of algorithms (van Deemter, 2002; Gardent, 2002; Horacek, 2004; Gatt and van Deemter, 2007). Much of this work has incorporated the principle of brevity. Based on the Gricean Quantity maxim (Grice, 1975), and originally discussed by Appelt (1985), and further by Dale (1989), Reiter (1990) and Gardent (2002), this principle holds that descriptions should contain no more information than is necessary to distinguish an intended referent. In ASGRE, this has been translated into a criterion which determines the adequacy of an attribute set, implemented in its most straightforward form in Full Brevity algorithms which select the smallest attribu"
W08-1108,D07-1011,1,0.796767,"Missing"
W08-1119,E06-1040,1,0.50769,"t al., 2005). Most of these systems have generated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decisions based on the generated texts, and measuring the quality of these decisions. 2 BabyTalk and BT-45 Law et al. (2005) showed that human-written textual summaries were effective decision-support aids in NICU , but of course it is not practical to expect medical professionals to routinely write such summaries, especially considering that the summaries used by Law et al. i"
W08-1119,J88-2003,0,0.0454049,"was inserted successfully. In other words, the time given would still be the time that the abstract event started; but this is misleading, because readers of the above text expect that the stated time is the time of the successful insertion (14.08), not the time at which the sequence of insert/remove events started. We need a much better model of how to communicate time, and how this communication depends on the semantics and linguistic expression of the events being described. An obvious first step, which we are currently working on, is to include a linguisticallymotivated temporal ontology (Moens and Steedman, 1988), which will be separate from the existing domain ontology. We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order (Oberlander and Lascarides, 1992). 6 Discussion Two discourse analysts from Edinburgh University, Dr. Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts. Their top-level comment was that the human texts had much better narrative structures than the BT-45 texts. They use the term ‘narrative’ in 153 the sense of Labov (1972, Chapter 9); that i"
W08-1119,C92-2108,0,0.0578845,"rtion (14.08), not the time at which the sequence of insert/remove events started. We need a much better model of how to communicate time, and how this communication depends on the semantics and linguistic expression of the events being described. An obvious first step, which we are currently working on, is to include a linguisticallymotivated temporal ontology (Moens and Steedman, 1988), which will be separate from the existing domain ontology. We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order (Oberlander and Lascarides, 1992). 6 Discussion Two discourse analysts from Edinburgh University, Dr. Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts. Their top-level comment was that the human texts had much better narrative structures than the BT-45 texts. They use the term ‘narrative’ in 153 the sense of Labov (1972, Chapter 9); that is storylike structures which describe real experiences, and which go beyond just describing the events and include information that helps listeners make sense of what happened, such as abstracts, evaluatives, correlatives, and explicatives. D"
W08-1119,passonneau-2006-measuring,0,0.0161877,"ation is not possible. But it also suggests that if we can improve the technology so that computer-generated texts are as effective as human texts, we should have a very effective decisionsupport technology. 4 Quantitative Comparison of BT-45 and Corpus Texts In addition to the task-based evaluation described above, we also quantitatively compared the BT-45 and human texts, and qualitatively analysed problems in the BT-45 texts. Quantitative comparison was done by annotating the BT-45 and human texts to identify which events they mentioned. For each scenario, we computed the MASI coefficient (Passonneau, 2006) between the set of events mentioned in the BT-45 and human texts. The average MASI score was 0.21 (SD = 0.13), which is low; this suggests that BT-45 and the human writers choose different content. We also checked whether similar human and BT-45 texts (as judged by MASI score) obtained similar evaluation scores; in fact there was no significant correlation between MASI similarity of human and BT-45 texts and the difference between their evaluation scores. We performed a second analysis based on comparing the structure (e.g., number and size of paragraphs) of the BT-45 and human texts, using a"
W08-1119,W07-2315,1,0.734978,"ation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems. 1 Introduction Data-to-text NLG systems produce textual output based on the analysis and interpretation of nonlinguistic data (Reiter, 2007). Systems which produce short summaries of small amounts of data, such as weather-forecast generators (Reiter et al., 2005), have been one of the most successful applications of NLG , and there is growing interest in creating systems which produce longer summaries of larger data sets. We have recently carried out an evaluation of one such system, BT-45 (Portet et al., 2007), which generates multi-paragraph summaries of clinical data from a Neonatal Intensive Care Unit (NICU). The summaries cover a period of roughly 45 minutes, and describe both sensor data (heart rate, blood oxygen saturation,"
W08-1119,W05-1615,1,0.787348,"erated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decisions based on the generated texts, and measuring the quality of these decisions. 2 BabyTalk and BT-45 Law et al. (2005) showed that human-written textual summaries were effective decision-support aids in NICU , but of course it is not practical to expect medical professionals to routinely write such summaries, especially considering that the summaries used by Law et al. in some cases took several hours to write. T"
W08-1119,W08-1104,1,0.812292,"which showed that medical professionals were more likely to make the correct treatment deci147 sion when shown a human-written textual summary of the data than when they were shown a graphical visualisation of the data. A number of data-to-text systems have been developed and indeed fielded, especially in the domain of weather forecasts (Goldberg et al., 1994; Reiter et al., 2005). Most of these systems have generated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decis"
W08-1127,W00-1401,0,0.0392065,"s) is that an RE is not good or bad in its own right, but depends on the other MSRs in the same text.4 String Accuracy: This is defined just like REG 08-Type Accuracy, except here what is determined is identity between REFEX word strings (the MSREs themselves), not between REG08-Types. String-edit distance metrics: String-edit distance (SE) is straightforward Levenshtein distance with a substitution cost of 2 and insertion/deletion 4 This definition is also slightly different from the one given in the Participants’ Pack. cost of 1. We also used the version of string-edit distance described by Bangalore et al. (2000) which normalises for length. This version is denoted ‘SEB’ below. For the single-RE test sets, the global score is simply the average of all RE-level scores. For Test Set C-2, we used an approach analogous to that described above for REG08-Type Accuracy. We first computed the best string-edit distance at the text level (here, just the sum of RE-level distances) and then obtained the global distance by dividing the sum of best text-level distances by the number of REFs in all the texts. Other metrics: BLEU is a precision metric from MT that assesses the quality of a peer translation in terms o"
W08-1127,W07-2302,1,0.820701,"The immediate motivating application context for the GREC Task is the improvement of referential clarity and coherence in extractive summarisation by regenerating referring expressions in summaries. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GREC Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC Task Corpus is an extension of GREC 1.0 which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). for the purpose of the REG’08 GREC Task, we obtained an additional 1,000 texts in the new subdomain of mountain texts and developed a new XML annotation scheme (Section 2.2). Five teams from four countries registered for the GREC Task, of which three teams eventually submitted 6 systems. We also used the corpus texts themselves as ‘system’ outputs, and created four baseline systems. We evaluated the resulting 10 systems using a range of intrinsic and extrinsic evaluation methods. This report presents the results of all evaluations (Section 6), along with descriptions of the GREC data and t"
W08-1127,H05-1004,0,0.0209919,"ctation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their average. 5 Systems Base-rand, Base-freq, Base-1st, Base-name: We created four baseline systems. Base-rand selects on"
W08-1127,I08-1016,0,0.0250676,"of possible referring expressions for selection. As this is a new referring expression generation (REG) task, the shared task definition was kept fairly simple and the aim for participating systems was to select the appropriate type of referring expression (more specifically, its REG08-TYPE, full details below). The immediate motivating application context for the GREC Task is the improvement of referential clarity and coherence in extractive summarisation by regenerating referring expressions in summaries. There has recently been a small flurry of work in this area (Steinberger et al., 2007; Nenkova, 2008). In the longer term, the GREC Task is intended to be a step in the direction of the more general task of generating referential expressions in discourse context. The GREC Task Corpus is an extension of GREC 1.0 which had about 1,000 texts in the subdomains of cities, countries, rivers and people (Belz and Varges, 2007a). for the purpose of the REG’08 GREC Task, we obtained an additional 1,000 texts in the new subdomain of mountain texts and developed a new XML annotation scheme (Section 2.2). Five teams from four countries registered for the GREC Task, of which three teams eventually submitte"
W08-1127,qiu-etal-2004-public,0,0.0300994,"similar to that in the humanbased experiments described above: badly chosen reference chains seem likely to affect the reader’s ability to resolve REs. In the automatic version, the role of the reader is played by an automatic coreference resolution tool and the expectation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Re"
W08-1127,M95-1005,0,0.0328313,"resolution tool and the expectation is that the tool performs worse (are less able to identify coreference chains correctly) with worse MSR reference chains. To counteract the potential problem of results being a function of a specific coreference resolution algorithm or tool, we decided to use three different resolvers—those included in LingPipe,5 JavaRap (Qiu et al., 2004) and OpenNLP (Morton, 2005)— and to average results. There does not appear to be a single standard eval5 http://alias-i.com/lingpipe/ uation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B - CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their average. 5 Systems Base-rand, Base-freq, Base-1st, Base-name: We created four baseline systems. Ba"
W08-1131,W08-1108,1,0.841081,"point at which a participant called up the next screen via mouse click; (b) identification time (IT), measured from the point at which pictures (the visual domain) were presented on the screen to the point where a participant identified a referent by clicking on it; (c) error rate (ER), the proportion of times the wrong referent was identified. This design differs from that used in the 2007 ASGRE Challenge, in which descriptions and visual domains were presented in a single phase (on the same screen), so that RT and IT were conflated. The new experiment replicates the methodology reported in Gatt and Belz (2008), in a follow-up study on the ASGRE 2007 data. Another difference between the two experiments is that the current one is based on peer outputs which are themselves realisations, whereas the ASGRE experiment involved attribute sets which had to be realised before they could be used. Design: We used a Repeated Latin Squares design, in which each combination of SYSTEM3 and test set item is allocated one trial. Since there were 12 levels of SYSTEM, but 112 test set items, 8 randomly selected items (4 furniture and 4 people) were duplicated, yielding 120 items and 10 12 × 12 latin squares. The item"
W08-1131,W07-2307,1,0.766492,"Missing"
W08-1131,P02-1040,0,0.0984351,"MAIN. String-edit distance (TUNA - R, TUNA - REG): This is the classic Levenshtein distance measure, used to compare the difference between a peer output and a reference output in the corpus, as the minimal number of insertions, deletions and/or substitutions of words required to transform one string into another. The cost for insertions and deletions was set to 1, that for substitutions to 2. Edit distance is an integer bounded by the length of the longest description in the pair being compared. (TUNA - R, TUNA - REG): This is an n-gram based string comparison measure, originally proposed by Papineni et al. (2002) for evaluation of Machine Translation systems. It evaluates a system based on the proportion of word n-grams (considering all n-grams of length n ≤ 4 is standard) that it shares with several reference translations. Unlike Dice, MASI and String-edit, BLEU is by definition an aggregate measure (i.e. a single BLEU score is obtained for a system based on the entire set of items to be compared, and this is generally not equal to the average of BLEU scores for individual items). BLEU ranges between 0 and 1. BLEU NIST ( TUNA - R , TUNA - REG ): This is a version of BLEU , which gives more importance"
W08-1131,passonneau-2006-measuring,0,0.141439,"Missing"
W08-1131,2007.mtsummit-ucnlg.14,1,0.759345,"ction for referring expressions (TUNA - AS), realisation (TUNA - R) and end-toend referring expression generation (TUNA REG ). 8 teams submitted a total of 33 systems to the three tasks, with an additional submission to the Open Track. The evaluation used a range of automatically computed measures. In addition, an evaluation experiment was carried out using the peer outputs for the TUNA REG task. This report describes each task and the evaluation methods used, and presents the evaluation results. 1 Introduction The TUNA Challenge 2008 built on the foundations laid in the ASGRE 2007 Challenge (Belz and Gatt, 2007), which consisted of a single shared task, based on a subset of the TUNA Corpus (Gatt et al., 2007). The TUNA Corpus is a collection of human-authored descriptions of a referent, paired with a representation of the domain in which that description was elicited. The 2008 Challenge expanded the scope of the previous edition in a variety of ways. This year, there were three shared tasks. TUNA - AS is the Attribute Selection task piloted in the 2007 ASGRE Challenge, which involves the selection of a set of attributes which are true of a target referent, and help to distinguish it from its distract"
W09-0613,W96-0507,0,0.137301,"st interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and noncanned) representations. 1 2. Since the tactical problem involves search through a space of linguistic choices, the broader the coverage, the more efficiency may be compromised. Where real-time deployment is a goal, this may be an obstacle. Introduction Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as R EAL P RO (Lavoie and Rambow, 1997), A LETH G EN (Coch, 1996), KPML (Bateman, 1997), FUF / SURGE (Elhadad and Robin, 1996), H ALO GEN (Langkilde, 2000), YAG (McRoy et al., 2000), and O PEN CCG (White, 2006). Realisation involves two logically distinguishable tasks. Tactical generation involves making appropriate linguistic choices given the semantic input. However, once tactical decisions have been taken, building a syntactic representation, applying the right morphological operations, and linearising the sentence as a string are comparatively mechanical tasks. With the possible exception of template-based realisers, such as YAG, existing wide-coverage"
W09-0613,W96-0501,0,0.0303404,"nd linearise them. The library is also flexible in allowing the use of mixed (canned and noncanned) representations. 1 2. Since the tactical problem involves search through a space of linguistic choices, the broader the coverage, the more efficiency may be compromised. Where real-time deployment is a goal, this may be an obstacle. Introduction Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as R EAL P RO (Lavoie and Rambow, 1997), A LETH G EN (Coch, 1996), KPML (Bateman, 1997), FUF / SURGE (Elhadad and Robin, 1996), H ALO GEN (Langkilde, 2000), YAG (McRoy et al., 2000), and O PEN CCG (White, 2006). Realisation involves two logically distinguishable tasks. Tactical generation involves making appropriate linguistic choices given the semantic input. However, once tactical decisions have been taken, building a syntactic representation, applying the right morphological operations, and linearising the sentence as a string are comparatively mechanical tasks. With the possible exception of template-based realisers, such as YAG, existing wide-coverage realisers usually carry out both tasks. By contrast, a realis"
W09-0613,W07-2315,1,0.599809,"patient information (Portet et al., to appear) differs from standard usage, and does not always allow variation to the same extent. Since realisers don’t typically address such requirements, their use in a particular application may require the alteration of the realiser’s rule-base or, in the case of statistical realisers, re-training on large volumes of appropruately annotated data. This paper describes SimpleNLG, a realisation engine which grew out of recent experiences in building large-scale data-to-text NLG systems, whose goal is to summarise large volumes of numeric and symbolic data (Reiter, 2007). Sublanguage requirements and efficiency are important considerations in such systems. Although meeting these requirements was the initial motivation behind SimpleNLG, it has since been developed into an engine with significant coverage of English syntax and morphology, while at the same time providing a simple API that offers users direct programmatic control over the realisation process. Proceedings of the 12th European Workshop on Natural Language Generation, pages 90–93, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 90 lexical phrasal Feature A DJ P"
W09-0613,A00-2023,0,\N,Missing
W09-0613,W00-1437,0,\N,Missing
W09-0613,W06-1403,0,\N,Missing
W09-0613,A97-1039,0,\N,Missing
W09-0615,C08-1055,1,0.891826,"Missing"
W09-0615,W06-1419,0,0.020107,"roduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our"
W09-0615,P04-1052,0,0.0190968,"and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is In order to study specific data, we have focussed on the construction illustrated in Section 1 above: potentially ambiguous Noun Phrases of the general form the Adj Nouni and Nounj . For such phrases, there are potentially two interpretations: wide scope (Adj modifies both Nouni and Nounj"
W09-0615,W06-1421,0,0.0195059,"describe an ongoing study with human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2"
W09-0615,W98-1419,0,0.0281273,"mter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce referring expressions that are of optimal benefit to a hearer. That is, we are interested in generating descriptions that are easy to read and understand. But the readability and intelligibility of a description can crucially depend on the way in which it is In order to study specific data, we have focussed on the construction illustrated in Section 1 above: potentially ambiguous Noun Phrases of the general form the Adj Nouni and Nounj . For such phrases, there are potentially two in"
W09-0615,E06-1040,0,0.027936,"ith human readers. 1 Introduction In recent years, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Co"
W09-0615,W06-1420,1,0.826323,"Missing"
W09-0615,W08-1108,1,0.810867,"as seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algorithms that produce ref"
W09-0615,U06-1017,0,0.0608209,"ears, the NLG community has seen a substantial number of studies to evaluate Generation of Referring Expressions (GRE) algorithms, but it is still far from clear what would constitute an optimal evaluation method. Two limitations stand out in the bulk of existing work. Firstly, most existing evaluations are essentially speakeroriented, focussing on the degree of “humanlikeness” of the generated descriptions, disregarding their effectiveness (e.g. Mellish and Dale (1998), Gupta and Stent (2005), van Deemter et al. (2006), Belz and Kilgarriff (2006), Belz and Reiter (2006), Paris et al. (2006), Viethen and Dale (2006), Gatt and Belz (2008)). The limited number of exceptions to this rule indicate that the differences between the two approaches to evaluation can be substantial (Gatt and Belz, 2008). Secondly, most evaluations have focussed on the semantic content of the generated descriptions, as produced by the Content Determination stage of a GRE algorithm; this means that linguistic realisation (i.e. the choice of words and linguistic constructions) is usually not addressed (exceptions are: Stone and Webber (1998), Krahmer and Theune (2002), Siddharthan and Copestake (2004)). Our aim is to build GRE algor"
W09-0615,W06-1409,1,\N,Missing
W09-0629,2007.mtsummit-ucnlg.14,1,0.776163,"−LOC condition, they were discouraged from doing so, though not prevented. The XML format we have been using in the TUNA - REG STEC s, shown in Figure 1, is a variant of the original format of the TUNA corpus. The root TRIAL node has a unique ID and an indication of the +/ − LOC experimental condi1 Introduction This year’s run of the TUNA - REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions. The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge (Belz and Gatt, 2007) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between September 2007 and May 2008 (Gatt et al., 2008). This year’s TUNA Task replicates one of the three tasks from REG’08, the TUNA - REG Task. It uses the same test data, to enable direct comparison against the 2008 results. Four participating teams submitted 6 different systems this year; teams and their affiliations are shown in Table 1. 1 http://www.csd.abdn.ac.uk/research/tuna/ http://genpsylab-wexlist.unizh.ch 3 The elicitation e"
W09-0629,P08-2050,1,0.603169,"0.48 Extrinsic ID Acc. ID Speed 0.50 -0.89* 0.95** -0.65 1 -0.39 -0.39 1 0.68 -0.79 -0.01 0.68 0.49 -0.51 0.60 0.06 Auto-assessed, intrinsic Acc. SE BLEU NIST .85* -0.57 0.66 0.30 .83* -0.29 0.60 0.48 0.68 -0.01 0.49 0.60 -0.79 0.68 -0.51 0.06 1.00 -0.68 .859* 0.49 -0.68 1 -0.75 -0.07 .86* -0.75 1 0.71 0.49 -0.07 0.71 1 Table 10: Correlations (Pearson’s r) between all evaluation measures. (∗ significant at p ≤ .05; at p ≤ .01) ∗∗ significant Fluency and Identification Speed, implying that more fluent descriptions led to faster identification. While these results differ from previous findings (Belz and Gatt, 2008), in which no significant correlations were found between extrinsic measures and automatic intrinsic metrics, it is worth noting that significance in the results reported here was only observed between human-assessed intrinsic measures and the extrinsic ones. tions of the evaluation methods we have developed. Among such experiments will be direct comparisons between the results of the three variants of the identification experiment we have tried out, and a direct comparison between different designs for human-assessed intrinsic evaluations (e.g. comparing the slider design reported here to pre"
W09-0629,W08-1131,1,0.808649,"root TRIAL node has a unique ID and an indication of the +/ − LOC experimental condi1 Introduction This year’s run of the TUNA - REG Shared-Task Evaluation Competition (STEC) is the third, and final, competition to involve the TUNA Corpus of referring expressions. The TUNA Corpus was first used in the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge (Belz and Gatt, 2007) which took place between May and September 2007; and again for three of the shared tasks in Referring Expression Generation (REG) Challenges 2008, which ran between September 2007 and May 2008 (Gatt et al., 2008). This year’s TUNA Task replicates one of the three tasks from REG’08, the TUNA - REG Task. It uses the same test data, to enable direct comparison against the 2008 results. Four participating teams submitted 6 different systems this year; teams and their affiliations are shown in Table 1. 1 http://www.csd.abdn.ac.uk/research/tuna/ http://genpsylab-wexlist.unizh.ch 3 The elicitation experiment had an additional independent variable, manipulating whether descriptions were elicited in a ‘fault-critical’ or ‘non-fault-critical’ condition. For the shared tasks this was ignored by collapsing all th"
W09-0629,2001.mtsummit-papers.68,0,0.030421,"erally equal to the average of scores for individual items). Because the test data has two human-authored reference descriptions per domain, the Accuracy and SE scores had to be computed slightly differently to obtain test data scores (whereas BLEU and NIST are designed for multiple reference texts). For the test data only, therefore, Accuracy expresses the percentage of a system’s outputs that match at least one of the reference outputs, and SE is the average of the two pairwise scores against the reference outputs. BLEU -x is an n-gram based string comparison measure, originally proposed by Papineni et al. (2001; 2002) for evaluation of Machine Translation systems. It computes the proportion of word n-grams of length x and less that a system output shares with several reference outputs. Setting x = 4 (i.e. considering all n-grams of length ≤ 4) is standard, but because many of the TUNA descriptions are shorter than 4 tokens, we compute BLEU -3 instead. BLEU ranges from 0 to 1. Results: Table 4 is an overview of the selfreported scores on the development set included in the participants’ reports (not all participants report Accuracy scores). The corresponding scores for the test data set as well as NI"
W09-0629,P02-1040,0,0.11671,"Missing"
W09-0629,passonneau-2006-measuring,0,\N,Missing
W09-0629,qiu-etal-2004-public,0,\N,Missing
W09-0629,M95-1005,0,\N,Missing
W09-0629,W07-2307,1,\N,Missing
W09-0629,W07-2302,1,\N,Missing
W09-0629,W08-1108,1,\N,Missing
W09-0629,H05-1004,0,\N,Missing
W09-0629,I08-1016,0,\N,Missing
W09-0629,W00-1401,0,\N,Missing
W09-2816,H05-1004,0,0.0246037,"that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected items from Test Set"
W09-2816,I08-1016,0,0.0410324,"Missing"
W09-2816,qiu-etal-2004-public,0,0.0697036,"Missing"
W09-2816,M95-1005,0,0.142828,"formance.6 The basic idea is that it seems likely that badly chosen reference chains affect the ability to resolve REs in automatic coreference resolution tools which will tend to perform worse with poorly selected MSR reference chains. To counteract the possibility of results being a function of a specific coreference resolution algorithm or tool, we used two different resolvers— those included in LingPipe7 and OpenNLP (Morton, 2005)—and averaged results. There does not appear to be a single standard evaluation metric in the coreference resolution community, so we opted to use three: MUC -6 (Vilain et al., 1995), CEAF (Luo, 2005), and B CUBED (Bagga and Baldwin, 1998), which seem to be the most widely accepted metrics. All three metrics compute Recall, Precision and F-Scores on aligned gold-standard and resolver-tool coreference chains. They differ in how the alignment is obtained and what components of coreference chains are counted for calculating scores. Results for the automatic extrinsic evaluations are reported below in terms of the F-Scores from these three metrics, as well as in terms of their mean. 4.3 Human intrinsic evaluation The intrinsic human evaluation involved 24 randomly selected it"
W09-2816,W07-2302,1,\N,Missing
W10-4206,E06-1032,0,0.20199,"erts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches."
W10-4206,W05-0901,0,0.114002,"d metrics exploiting corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with"
W10-4206,W08-1113,0,0.16982,"rrelate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 1 Introduction Evaluation methodology in NLG has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit ("
W10-4206,R09-1021,1,0.739142,"es medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Franc¸ois Portet Laboratoire d’Informatique de Grenoble Grenoble Institute of Technology francois.portet@imag.fr Meulen et al., 2009). This gives us the opportunity to directly compare the textual characteristics of generated and human-written summaries and their relationship to decision-making performance. The present work uses data from an earlier study (Gatt and Portet, 2009), which presented some preliminary results along these lines for the system in question. We extend this work in a number of ways. Our principal aim is to test the validity not only of general-purpose metrics which measure surface properties of text, but also of metrics which make use of domain knowledge, in the sense that they attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontolog"
W10-4206,karasimos-isard-2004-multi,0,0.0306284,"ledge, in the sense that they attempt to relate the ‘deep semantics’ of the texts to extrinsic factors, based on an ontology for the BT-45 domain. After an overview of related work in section 2, the BT-45 system, its domain ontology and the extrinsic evaluation are described in section 3. The ontology plays an important role in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosy"
W10-4206,W02-2103,0,0.018887,"7). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on t"
W10-4206,J97-1004,0,0.0251561,"ole in the evaluation metrics presented in Section 5. Finally, the evaluation of the methods is presented in Section 6, before discussing and concluding in Section 7. 2 Related Work In NLG evaluation, extrinsic, task-based methods play a significant role (Reiter et al., 2003; Karasimos and Isard, 2004; Stock et al., 2007). Depending on the study design, these studies often leave open the question of precisely which aspects of a system (and of the text it generates) contribute to success or failure. Intrinsic NLG evaluations often involve ratings of text quality or responses to questionnaires (Lester and Porter, 1997; Callaway and Lester, 2002; Foster, 2008), with some studies using post-editing by human experts (Reiter et al., 2005). Automatically computed metrics exploiting corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact"
W10-4206,N03-1020,0,0.0756433,"Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Franc¸ois Portet Laboratoire d’Informatique de Grenoble Grenoble Institute of Technology francois.portet@imag.fr Meulen et al., 2009). This gives us the opport"
W10-4206,P08-2051,0,0.0236987,"g corpora, such as BLEU, NIST and ROUGE , have mainly been used in evaluations of the coverage and quality of morphosyntactic realisers (Langkilde-Geary, 2002; Callaway, 2003), though they have recently also been used for subtasks such as Referring Expression Generation (Gatt and Belz, to appear) as well as end-toend weather forecasting systems (Reiter and Belz, 2009). The widespread use of these metrics in NLP partly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Fo"
W10-4206,N04-1019,0,0.0876184,"tly rests on the fact that they are quick and cheap, but there is controversy about their reliability both in MT (Calliston-Burch et al., 2006) and summarisation (Dorr et al., 2005; Liu and Liu, 2008). As noted in Section 1, similar questions have been raised in NLG. One of the problems associated with these metrics is that they rely on the notion of a ‘gold standard’, which is not always precisely definable given multiple solutions to the same generation, summarisation or translation task. These observations underlie recent developments in Summarisation evaluation such as the Pyramid method (Nenkova and Passonneau, 2004), which in addition also emphasises content overlap with a set of reference summaries, rather than n-gram matches. It is interesting to note that, with some exceptions (Foster, 2008), most of the methodological studies on intrinsic evaluation cited here have focused on ‘generic’ metrics (corpus-based automatic measures being foremost among them), none of which use domain knowledge to quantify those aspects of a text related to its content. There is some work in Summarisation that suggests that incorporating more knowledge improves results. For example, Yoo and Song (Yoo et al., 2007) used the"
W10-4206,P02-1040,0,0.0787001,"has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Portet et al., 2009), which summarises medical data for decision support purposes in a Neonatal Intensive Care Unit (NICU). Our extrinsic data comes from an experiment involving complex medical decision making based on automatically generated and human-authored texts (van der Franc¸ois Portet Laboratoire d’Informatique de Grenoble Grenoble Institute of Technology francois.portet@imag.fr Meulen et al.,"
W10-4206,J09-4008,0,0.135633,"deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance. 1 Introduction Evaluation methodology in NLG has generated a lot of interest. Some recent work suggested that the relationship between various intrinsic and extrinsic evaluation methods (Sp¨arck-Jones and Galliers, 1996) is not straightforward (Reiter and Belz, 2009; Gatt and Belz, to appear), leading to some arguments for more domain-specific intrinsic metrics (Foster, 2008). One reason why these issues are important is that reliable intrinsic evaluation metrics that correlate with performance in an extrinsic, task-based setting can inform system development. Indeed, this is often the stated purpose of evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Hovy, 2003), which were originally characterised as evaluation ‘understudies’. In this paper we take up these questions in the context of a knowledge-based NLG system, BT-45 (Porte"
W10-4206,W08-1119,1,0.896917,"Missing"
W11-2804,P02-1040,0,0.100706,"Missing"
W11-2804,J09-4008,1,0.825783,"performance, human opinions on Likert-like scales, and/or similarity to a goldstandard corpus. While such evaluations are essential, we believe there is also a role for qualitative evaluations, especially when the goal of the evaluation is formative that is, assessing weaknesses and identifying how the NLG system could be improved. In this paper we describe how we used two qualitative methodologies, content analysis and discourse analysis, to evaluate texts produced by Background 2.1 Evaluation in NLG The great majority of published evaluations of NLG systems are quantitative: as described by Reiter and Belz (2009), they either measure the impact of a generated text on task performance, ask human subjects to rate generated texts on a Likert-like scale, or compare the similarity of generated texts to corpus texts using automatic metrics such as BLEU (Papineni et al., 2002). Reiter and Belz point out that many human-based quantitative NLG evaluations also solicit free-text comments from their subjects, and these are very helpful in diagnosing and fixing problems in generated texts. Soliciting such comments, however is usually a secondary goal of evaluations of NLG systems, the primary goal being quantitat"
W11-2804,W08-1119,1,\N,Missing
W11-2812,W05-1622,0,0.0297734,"methodology illustrated in the following sections is not restricted to particular languages. Neither of the two questions we have raised – that of representing and quantifying uncertainty, and that of mapping from this to the right modal expression in a particular language – has been treated 93 exhaustively in the NLG literature. To our knowledge, the only recent approach to handling modals in NLG is Klabunde (2007), who focuses on the generation of deontic modals (those related to obligation, rather than epistemic certainty) in the CAN system, which advises students about university courses (Klabunde, 2005; Klabunde, 2007). Klabunde’s approach is based on the influential possible worlds framework proposed by Kratzer (Kratzer, 1977; Kratzer, 1981; Portner, 2009), in which the truth of a modalised proposition is evaluated against a (contextually determined) set of relevant possible worlds or situations, ordered by their accessibility from the current world or situation. In an epistemic context, this set contains the worlds which are compatible to some degree with the propositions which constitute the underlying ‘evidence’ for the statement. Most semantic work on modality has been based on this fr"
W11-2812,W07-2311,0,0.0825301,"ed proposition is simply asserted (thereby presupposing certainty about the matter); its modalised counterpart is not, or only with some qualification as to the degree of evidence that the speaker has for it. We are primarily interested in how the resources that a language makes available to express epistemic modality can be harnessed to express temporal uncertainty in data-to-text systems, thus avoiding misleading the reader. While the importance of this problem has been pointed out in recent work (Portet et al., 2009; Gatt et al., 2009), modality lacks a principled treatment in NLG (but see Klabunde (2007)). As Klabunde notes, NLG systems which use modals in their output (Elhadad, 1995; Reiter et al., 2003) do not seem to select these expressions in a principled way. The following example illustrates some of the difficulties in dealing with epistemic modality, especially from a cross-linguistic perspective: (1) A bank robbery occurred yesterday afternoon. An investigator is trying to reconstruct the scene from eye-witness reports. He knows for certain that the robbers were inside the bank for no more than 45 minutes. He also knows for certain that the police took 30 minutes to arrive on the sce"
W13-2109,W08-1132,0,0.027424,"mers (1998) note that certain speakers in their corpus explicitly stated that they had attempted to perform the task in their dialogues without pointing, in spite of their having been told that they could point. Recent data-driven experiments on referential descriptions by Dale and Viethen (Dale and Viethen, 2010), In a domain quite similar to the one used here, suggest that speakers do indeed cluster according to their preferred referential strategy. Similar assumptions have informed REG algorithms trained on the TUNA Corpus, in the context of the Generation Challenges (Gatt and Belz, 2010) (Bohnet, 2008; Di Fabbrizio et al., 2008). In future work, we plan to address this question in a multimodal context, where results by Piwek (2007) have already suggested that such individual strategies may play an important role. The hypothesis that specific combinations of pointing and linguistic descriptions (for example, an object’s colour or size) can be excluded, is clearly not borne out by the data. There is, however, a tendency for locative features to act as stronger predictors of pointing gestures. Although the trend is not very strong, it is an interesting one since it confirms the experimental r"
W13-2109,P02-1012,0,0.10484,"Missing"
W13-2109,P89-1009,0,0.760975,"interact with descriptive strategies. In particular, pointing gestures are strongly associated with the use of locative features in referring expressions. 1 Introduction Referring Expression Generation (REG) is considered a core task in many NLG systems (Krahmer and van Deemter, 2012). Typically, the REG task is defined in terms of identification: a referent needs to be unambiguously identified in a discourse, enabling the reader or listener to pick it out from among its potential distractors. Most work in this area has focused on algorithms that select the content for definite descriptions (Dale, 1989; Dale and 82 Proceedings of the 14th European Workshop on Natural Language Generation, pages 82–91, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics example, de Ruiter (2000) proposes that the two modalities are planned together at early stages of conceptualisation during speech production, while ¨ urek (2003) suggest that gestures are Kita and Ozy¨ planned by spatio-motoric processes which differ from the planning of speech production, but interact with it at particular points. Recent computational work has also taken these ideas on board. For example, Kopp"
W13-2109,W08-2120,0,0.0478788,"Missing"
W13-2109,J12-1006,0,0.0730641,"Missing"
W13-2109,J03-1003,0,0.271393,"Missing"
W13-2109,W05-1608,0,0.705347,"g might proceed hand in hand, so that In this case, the pointing gesture further contributes to the communicative aim of identifying the cluster of five objects, in tandem with the visual features mentioned in the description. McNeill’s proposal (McNeill and Duncan, 2000) is that speech and gesture should be considered as the joint outcome of the language production process, rather than as outcomes of separate processes. Various models have been proposed which are more or less congruent with this view. For 83 2007) assume a trade-off between speech and gesture. A similar assumption is made by Kranstedt and Wachsmuth (2005), who view pointing gesturs as mainly concerned with the ‘where’ of an object. Their algorithm, which underlies the planning of multimodal references by a virtual agent, extends the Incremental Algorithm (Dale and Reiter, 1995) as follows. Given an object in a 3D space, the algorithm first considers the possibility of producing an unambiguous pointing gesture; failing this, a pointing gesture covering the intended referent and some of its surrounding distractors may be planned. In the latter case, the algorithm then integrates other features of the object (e.g. its colour), in an effort to exc"
W13-2109,W99-0108,0,0.155115,"Missing"
W17-1304,E14-1060,0,0.0313172,"Missing"
W17-1304,W02-0606,0,0.352683,"ve. As seen in the previous section, most computational approaches deal with either Semitic morphology (as one would for Arabic or its varieties), or with a system based on stems and affixes (as in Italian). Therefore, we might expect that certain methods will perform differently depending on which component we look at. Indeed, overall accuracy figures may mask interesting differences among the different components. The main motivation behind this analysis is that Maltese words of Semitic origin tend to have considerable stem variation (non-concatenative), Schone and Jurafsky (2000; 2001) and Baroni et al. (2002) use both orthographic and semantic similarity to detect morphologically related word pairs, arguing that neither is sufficient on its own to determine a morphological relation. Yarowsky and Wicentowski (2000) use a combination of alignment models with the aim of pairing inflected 27 whilst the word formation from Romance/English origin words would generally leave stems whole (concatenative)2 . Maltese provides an ideal scenario for this type of analysis due to its mixed morphology. Often, clustering techniques would either be sensitive to a particular language, such as catering for weak conso"
W17-1304,N07-1020,0,0.0769438,"Missing"
W17-1304,C96-1017,0,0.356756,"features which can be classified in an optimal sequence to provide a final complex label. Once again, the focus of the analysis is on the hybridity of the language and whether a single technique is appropriate for a mixed morphology such as that found in Maltese. 2 Related Work Computational morphology can be viewed as having three separate subtasks — segmentation, clustering related words, and labelling (see Hammarstr¨om and Borin (2011)). Various approaches are used for each of the tasks, ranging from rulebased techniques, such as finite state transducers for Arabic morphological analysis (Beesley, 1996; Habash et al., 2005), to various unsupervised, semi- or fully-supervised techniques which would generally deal with one or two of the subtasks. For most of the techniques described, it is difficult to directly compare results due to difference in the data used and the evaluation setting itself. For instance, the results achieved by segmentation techniques are then evaluated in an information retrieval task. The majority of works dealing with unsupervised morphology focus on English and assume that the morphological processes are concatenative (Hammarstr¨om and Borin, 2011). Goldsmith (2001)"
W17-1304,P07-1116,0,0.0274467,"5; 2007) use Maximum a Posteriori approaches to segment words from unannotated texts, and have become part of the baseline and standard evaluation in the Morpho Challenge series of competitions (Kurimo et al., 2010). Kohonen et al. (2010) extends this work by introducing semi- and supervised approaches to the model learning for segmentation. This is done by introducing a discriminative weighting scheme that gives preference to the segmentations within the labelled data. Transitional probabilities are used to determine potential word boundaries (Keshava and Pitler, 2006; Dasgupta and Ng, 2007; Demberg, 2007). The technique is very intuitive, and posits that the 1 http://mlrs.research.um.edu.mt/ resources/gabra/ 26 words. However this technique relies on part-ofspeech, affix and stem information. Can and Manandhar (2012) create a hierarchical clustering of morphologically related words using both affixes and stems to combine words in the same clusters. Ahlberg et al. (2014) produce inflection tables by obtaining generalisations over a small number of samples through a semi-supervised approach. The system takes a group of words and assumes that the similar elements that are shared by the different"
W17-1304,borg-gatt-2014-crowd,1,0.8465,"F t-e˙zamina t-igdeb 1P L n-e˙zamina-w n-igdb-u 2P L t-e˙zamina-w t-igdb-u 3P L j-e˙zamina-w j-igdb-u To date, there still is no complete morphological analyser for Maltese. In a first attempt at a 25 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 25–34, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics computational treatment of Maltese morphology, Farrugia (2008) used a neural network and focused solely on broken plural for nouns (Schembri, 2006). The only work treating computational morphology for Maltese in general was by Borg and Gatt (2014), who used unsupervised techniques to group together morphologically related words. A theoretical analysis of the templatic verbs (Spagnol, 2011) was used by Camilleri (2013), who created a computational grammar for Maltese for the Resource Grammar Library (Ranta, 2011), with a particular focus on inflectional verbal morphology. The grammar produced the full paradigm of a verb on the basis of its root, which can consist of over 1,400 inflective forms per derived verbal form, of which traditional grammars usually list 10. This resource is known ˙ ˙ as Gabra and is available online1 . Gabra is,"
W17-1304,N13-1138,0,0.0314411,"Missing"
W17-1304,E12-1067,0,0.0518275,"Missing"
W17-1304,P02-1065,0,0.111346,"Missing"
W17-1304,P05-1071,0,0.196553,"Missing"
W17-1304,W00-0712,0,0.0322352,"ms, concatenative and non-concatenative. As seen in the previous section, most computational approaches deal with either Semitic morphology (as one would for Arabic or its varieties), or with a system based on stems and affixes (as in Italian). Therefore, we might expect that certain methods will perform differently depending on which component we look at. Indeed, overall accuracy figures may mask interesting differences among the different components. The main motivation behind this analysis is that Maltese words of Semitic origin tend to have considerable stem variation (non-concatenative), Schone and Jurafsky (2000; 2001) and Baroni et al. (2002) use both orthographic and semantic similarity to detect morphologically related word pairs, arguing that neither is sufficient on its own to determine a morphological relation. Yarowsky and Wicentowski (2000) use a combination of alignment models with the aim of pairing inflected 27 whilst the word formation from Romance/English origin words would generally leave stems whole (concatenative)2 . Maltese provides an ideal scenario for this type of analysis due to its mixed morphology. Often, clustering techniques would either be sensitive to a particular language,"
W17-1304,N01-1024,0,0.0214099,"uing that neither is sufficient on its own to determine a morphological relation. Yarowsky and Wicentowski (2000) use a combination of alignment models with the aim of pairing inflected 27 whilst the word formation from Romance/English origin words would generally leave stems whole (concatenative)2 . Maltese provides an ideal scenario for this type of analysis due to its mixed morphology. Often, clustering techniques would either be sensitive to a particular language, such as catering for weak consonants in Arabic (de Roeck and Al-Fares, 2000), or focus solely on English or Romance languages (Schone and Jurafsky, 2001; Yarowsky and Wicentowski, 2000; Baroni et al., 2002) where stem variation is not widespread. The analysis below uses a dataset of clusters produced by Borg and Gatt (2014), who employed an unsupervised technique using several interim steps to cluster words together. First, potential affixes are identified using transitional probabilities in a similar fashion to (Keshava and Pitler, 2006; Dasgupta and Ng, 2007). Words are then clustered on the basis of common stems. Clusters are improved using measures of orthographic and semantic similarity, in a similar vein to (Schone and Jurafsky, 2001; B"
W17-1304,Q13-1021,0,0.0360201,"Missing"
W17-1304,J11-2002,0,0.0528093,"Missing"
W17-1304,P08-1084,0,0.0765942,"Missing"
W17-1304,W10-2210,0,0.0212537,"ling with unsupervised morphology focus on English and assume that the morphological processes are concatenative (Hammarstr¨om and Borin, 2011). Goldsmith (2001) uses the minimum description length algorithm, which aims to represent a language in the most compact way possible by grouping together words that take on the same set of suffixes. In a similar vein, Creutz and Lagus (2005; 2007) use Maximum a Posteriori approaches to segment words from unannotated texts, and have become part of the baseline and standard evaluation in the Morpho Challenge series of competitions (Kurimo et al., 2010). Kohonen et al. (2010) extends this work by introducing semi- and supervised approaches to the model learning for segmentation. This is done by introducing a discriminative weighting scheme that gives preference to the segmentations within the labelled data. Transitional probabilities are used to determine potential word boundaries (Keshava and Pitler, 2006; Dasgupta and Ng, 2007; Demberg, 2007). The technique is very intuitive, and posits that the 1 http://mlrs.research.um.edu.mt/ resources/gabra/ 26 words. However this technique relies on part-ofspeech, affix and stem information. Can and Manandhar (2012) create"
W17-1304,P99-1037,0,0.134065,"Missing"
W17-1304,W10-2211,0,0.0332621,"majority of works dealing with unsupervised morphology focus on English and assume that the morphological processes are concatenative (Hammarstr¨om and Borin, 2011). Goldsmith (2001) uses the minimum description length algorithm, which aims to represent a language in the most compact way possible by grouping together words that take on the same set of suffixes. In a similar vein, Creutz and Lagus (2005; 2007) use Maximum a Posteriori approaches to segment words from unannotated texts, and have become part of the baseline and standard evaluation in the Morpho Challenge series of competitions (Kurimo et al., 2010). Kohonen et al. (2010) extends this work by introducing semi- and supervised approaches to the model learning for segmentation. This is done by introducing a discriminative weighting scheme that gives preference to the segmentations within the labelled data. Transitional probabilities are used to determine potential word boundaries (Keshava and Pitler, 2006; Dasgupta and Ng, 2007; Demberg, 2007). The technique is very intuitive, and posits that the 1 http://mlrs.research.um.edu.mt/ resources/gabra/ 26 words. However this technique relies on part-ofspeech, affix and stem information. Can and M"
W17-1304,P00-1027,0,0.0369126,"Italian). Therefore, we might expect that certain methods will perform differently depending on which component we look at. Indeed, overall accuracy figures may mask interesting differences among the different components. The main motivation behind this analysis is that Maltese words of Semitic origin tend to have considerable stem variation (non-concatenative), Schone and Jurafsky (2000; 2001) and Baroni et al. (2002) use both orthographic and semantic similarity to detect morphologically related word pairs, arguing that neither is sufficient on its own to determine a morphological relation. Yarowsky and Wicentowski (2000) use a combination of alignment models with the aim of pairing inflected 27 whilst the word formation from Romance/English origin words would generally leave stems whole (concatenative)2 . Maltese provides an ideal scenario for this type of analysis due to its mixed morphology. Often, clustering techniques would either be sensitive to a particular language, such as catering for weak consonants in Arabic (de Roeck and Al-Fares, 2000), or focus solely on English or Romance languages (Schone and Jurafsky, 2001; Yarowsky and Wicentowski, 2000; Baroni et al., 2002) where stem variation is not wides"
W17-1304,Q15-1012,0,0.024213,"Missing"
W17-1304,N09-1024,0,0.0517079,"Missing"
W17-1304,J01-2001,0,\N,Missing
W17-1304,P06-1086,0,\N,Missing
W17-3506,W05-0909,0,0.0618924,"after each training epoch). Initialization of weights was done using Xavier initialization (Glorot and Bengio, 2010) and biases were set to zero. Each architecture was trained three separate times; the results reported below are averages over these three separate runs. To evaluate the trained models we generated captions for images in the test set using beam search 55 with a beam width of 3 and a clipped maximum length of 20 words. The MSCOCO evaluation code3 was used to measure the quality of the captions by using the standard evaluation metrics BLEU(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004). We also calculated the percentage of word types that were actually used in the generated captions out of the vocabulary of available word types. This measure indicates how well each architecture exploits the vocabulary it is trained on. The code used for the experiments was implemented with TensorFlow and is available online4 . 4 Results Table 1 reports means and standard deviations over the three runs of all the MSCOCO measures and the vocabulary usage. Since the point is to compare the effects of the architectures rather than"
W17-3506,D13-1128,0,0.0478638,"Missing"
W17-3506,W15-2807,0,0.0277694,"Missing"
W17-3506,N16-1147,0,0.0151079,"ators. 1 Introduction Image captioning (Bernardi et al., 2016) has emerged as an important testbed for solutions to the fundamental AI challenge of grounding symbolic or linguistic information in perceptual data (Harnad, 1990; Roy and Reiter, 2005). Most captioning systems focus on what Hodosh et al. (2013) refer to as concrete conceptual descriptions, that is, captions that describe what is strictly within the image, although recently, there has been growing interest in moving beyond this, with research on visual question-answering (Antol et al., 2015) and imagegrounded narrative generation (Huang et al., 2016) among others. Approaches to image captioning can be divided into three main classes (Bernardi et al., 2016): 2. Systems that frame the task as a retrieval problem, where a caption, or parts thereof, is identified by computing the proximity/relevance of strings in the training data to a given image. This is done by exploiting either a unimodal (Ordonez et al., 2011; Gupta et al., 2012; Mason and Charniak, ) or multimodal (Hodosh et al., 2013; Socher et al., 2014) space. Many retrieval-based approaches rely on neural models to handle both image features and linguistic information (Ordonez et al"
W17-3506,P04-1077,0,0.0152941,"vier initialization (Glorot and Bengio, 2010) and biases were set to zero. Each architecture was trained three separate times; the results reported below are averages over these three separate runs. To evaluate the trained models we generated captions for images in the test set using beam search 55 with a beam width of 3 and a clipped maximum length of 20 words. The MSCOCO evaluation code3 was used to measure the quality of the captions by using the standard evaluation metrics BLEU(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004). We also calculated the percentage of word types that were actually used in the generated captions out of the vocabulary of available word types. This measure indicates how well each architecture exploits the vocabulary it is trained on. The code used for the experiments was implemented with TensorFlow and is available online4 . 4 Results Table 1 reports means and standard deviations over the three runs of all the MSCOCO measures and the vocabulary usage. Since the point is to compare the effects of the architectures rather than to reach state-of-the-art performance, we do not include results"
W17-3506,E12-1076,0,0.104799,"Missing"
W17-3506,P02-1040,0,0.0977357,"idation performance is measured after each training epoch). Initialization of weights was done using Xavier initialization (Glorot and Bengio, 2010) and biases were set to zero. Each architecture was trained three separate times; the results reported below are averages over these three separate runs. To evaluate the trained models we generated captions for images in the test set using beam search 55 with a beam width of 3 and a clipped maximum length of 20 words. The MSCOCO evaluation code3 was used to measure the quality of the captions by using the standard evaluation metrics BLEU(1,2,3,4) (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), and ROUGE-L (Lin and Och, 2004). We also calculated the percentage of word types that were actually used in the generated captions out of the vocabulary of available word types. This measure indicates how well each architecture exploits the vocabulary it is trained on. The code used for the experiments was implemented with TensorFlow and is available online4 . 4 Results Table 1 reports means and standard deviations over the three runs of all the MSCOCO measures and the vocabulary usage. Since the point is to compare the effect"
W17-3506,D17-1039,0,0.0290282,"Missing"
W17-3506,Q14-1006,0,0.0123919,"entations encoded in an RNN could be pre-trained and re-used for a variety of tasks and/or image captioning datasets, with domain-specific training only required for the final feedforward layer, where the tuning required to make perceptually grounded predictions is carried out. We return to this point in Section 6.1. In the following sections, we describe some experiments to conduct such a comparison. 3 Experiments To evaluate the performance of the inject and merge architectures, and thus the roles of the RNN, we trained and evaluated them on the Flickr8k (Hodosh et al., 2013) and Flickr30k (Young et al., 2014) datasets of image-caption pairs. For the purposes of these experiments, we used the version of the datasets distributed by Karpathy and Fei-Fei (2015)2 . The dataset splits are identical to that used by Karpathy and Fei-Fei (2015): Flickr8k is split into 6,000 images for training, 1,000 for validation, and 1,000 for testing whilst Flickr30k is split into 29,000 images for training, 1,014 images for validation, and 1,000 images for testing. Each image 2 http://cs.stanford.edu/people/karpathy/ deepimagesent/ 54 (a) The merge architecture. (b) The inject architecture. Figure 3: An illustration o"
W17-3506,Q14-1017,0,\N,Missing
W17-5311,D15-1075,0,0.0167851,"etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 56–60, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 3 Our Approach dependencies across multiple words. As such, dependency parsing provides vital information on the sentence’s structure. Hence, we"
W17-5311,P17-1152,0,0.0404133,"distributed sentence representations, typically based on existing word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The baseline models at the shared task use GloVe vectors and present three approaches to obtaining sentence embeddings (Williams et al., 2017): a) taking the sum of the embeddings of all the words in the sentence (continous bag of words, CBOW); b) taking the average of the hidden state outputs of a bidirectional LSTM (BiLSTM; Hochreiter and Schmidhuber 1997) across all the words; and c) the Enhanced Sequential Inference Model (ESIM) by (Chen et al., 2017), which, however, relies on cross-sentence attention, which submissions to the shared task may not make use of. Introduction The RepEval 2017 Shared Task aims to evaluate fixed-length vector representations (or embeddings) of sentences on the basis of a natural language understanding task, viz. natural language inference (NLI), also known as recognising textual entailments. Given two sentences, the first being the premise and the second the hypothesis, the goal of NLI is to train a classifier to predict whether the relation of the hypothesis to the premise is one of entailment, contradiction o"
W17-5311,D17-1070,0,0.11498,"pus (see Williams et al. (2017) for details). Task participants are provided with both training and development datasets, where parts of the development data match the training data in terms of genre, topic etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 56–60, c Copenhagen, Denmark, Septembe"
W17-5311,D14-1181,0,0.00547889,"Missing"
W17-5311,D14-1162,0,0.0802714,"ared Task on natural language inference. Our system is a simple system based on a standard BiLSTM architecture, using as input GloVe word embeddings augmented with further linguistic information. We use max pooling on the BiLSTM outputs to obtain embeddings for sentences. On both the matched and the mismatched test sets, our system clearly beats the shared task’s BiLSTM baseline model. 1 2 Related Work Various works in recent years have dealt with the creation of distributed sentence representations, typically based on existing word embeddings such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). The baseline models at the shared task use GloVe vectors and present three approaches to obtaining sentence embeddings (Williams et al., 2017): a) taking the sum of the embeddings of all the words in the sentence (continous bag of words, CBOW); b) taking the average of the hidden state outputs of a bidirectional LSTM (BiLSTM; Hochreiter and Schmidhuber 1997) across all the words; and c) the Enhanced Sequential Inference Model (ESIM) by (Chen et al., 2017), which, however, relies on cross-sentence attention, which submissions to the shared task may not make use of. Introduction The RepEval 20"
W17-5311,L16-1680,0,0.0618536,"Missing"
W17-5311,P15-1150,0,0.219484,"g and test data for this 3-way classification task at RepEval 2017 are drawn from the Multi-Genre NLI, or MultiNLI corpus (see Williams et al. (2017) for details). Task participants are provided with both training and development datasets, where parts of the development data match the training data in terms of genre, topic etc. (referred to as matched examples) and other parts do not (referred to as mismatched examples). This paper presents Team LCT-MALTA’s submission to the shared task. In line with previous research, we obtain a single vector which is the Instead of the BiLSTM architecture, Tai et al. (2015) propose a tree-structured LSTM to capture the hierarchical structure of natural language sentences. Conneau et al. (2017) use BiLSTM with max pooling and achieve state-of-art results when testing their sentence representations on an NLI task based on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). Lin et al. (2017) introduce a self-attention mechanism with multiple hops of attention on top of BiLSTM, where the different hops attend to different parts of the input sentence. Their approach represents sentence embeddings as 2-D matrices instead of vectors. 56 Procee"
W17-5311,P16-2022,0,\N,Missing
W18-6551,W08-1104,1,0.845298,"Missing"
W18-6551,W17-5525,0,0.0638373,"Missing"
W18-6551,W15-4723,1,0.903448,"Missing"
W18-6551,W07-2315,1,0.567353,"of 7 descriptors (including cardinal points, coast, inland, and a proper name). Figure 2 shows a representation of the answers given by the students for “Northern Galicia” and a contour map that illustrates the percentages of overlapping answers. The second survey was addressed to meteorologists in the Galician Weather Agency (MeteoGalicia, 2018). Its purpose was to gather data to create Language grounding, i.e., understanding how words and expressions are anchored in data, is one of the initial tasks that are essential for the conception of a data-to-text (D2T) system (Roy and Reiter, 2005; Reiter, 2007). This can be achieved through different means, such as using heuristics or machine learning algorithms on an available parallel corpora of text and data (Novikova et al., 2017) to obtain a mapping between the expressions of interest and the underlying data (Reiter et al., 2005), getting experts to provide these mappings, or running surveys on writers or readers that provide enough data for the application of mapping algorithms (Ramos-Soto et al., 2017). Performing language grounding ensures that generated texts include words whose meaning is aligned with what writers understand or what reader"
W18-6551,W02-2113,1,0.63836,"Missing"
W18-6551,W09-0607,1,\N,Missing
W18-6562,J06-2002,0,0.771708,"Missing"
W18-6562,W17-3511,0,0.0622242,"ndence and graduality. This has become increasingly evident in work that has sought solutions to the REG problem in naturalistic scenes, as part of a broader research focus on the visionlanguage interface (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016). However, context dependence is also a central concern for approaches to REG that assume a more structured input representation where entities and their properties are available, but the extent to which a property applies to a referent is not necessarily an allor-none decision (Horacek, 2005; van Deemter, 2006; Turner et al., 2008; Williams and Scheutz, 2017). Under these conditions, it is no longer possible to assume that properties are crisp or Boolean, or even that both sender and receiver necessarily assume the same semantics for those properties. For example, it may not be realistic to assume that all objects are red to the same degree, or that both sender and receiver have the same model of what counts as ‘red’. Furthermore, the utility of the term ‘red’ in identifying the referent will depend in part on context, that is, whether there are any other red entities, and whether they are red to the same extent. The above issues affect how refere"
W18-6562,W05-1606,0,0.0592265,"ify it. One source of complexity for REG is contextdependence and graduality. This has become increasingly evident in work that has sought solutions to the REG problem in naturalistic scenes, as part of a broader research focus on the visionlanguage interface (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016). However, context dependence is also a central concern for approaches to REG that assume a more structured input representation where entities and their properties are available, but the extent to which a property applies to a referent is not necessarily an allor-none decision (Horacek, 2005; van Deemter, 2006; Turner et al., 2008; Williams and Scheutz, 2017). Under these conditions, it is no longer possible to assume that properties are crisp or Boolean, or even that both sender and receiver necessarily assume the same semantics for those properties. For example, it may not be realistic to assume that all objects are red to the same degree, or that both sender and receiver have the same model of what counts as ‘red’. Furthermore, the utility of the term ‘red’ in identifying the referent will depend in part on context, that is, whether there are any other red entities, and whethe"
W18-6562,D14-1086,0,0.0318209,"versity of Granada nicm@decsai.ugr.es Gustavo Rivas-Gervilla Dept. of Computer Science & AI University of Granada griger@decsai.ugr.es Daniel S´anchez Dept. of Computer Science & AI University of Granada daniel@decsai.ugr.es Abstract context, using natural language, so that another receiving user is able to precisely and univocally identify it. One source of complexity for REG is contextdependence and graduality. This has become increasingly evident in work that has sought solutions to the REG problem in naturalistic scenes, as part of a broader research focus on the visionlanguage interface (Kazemzadeh et al., 2014; Mao et al., 2016; Yu et al., 2016). However, context dependence is also a central concern for approaches to REG that assume a more structured input representation where entities and their properties are available, but the extent to which a property applies to a referent is not necessarily an allor-none decision (Horacek, 2005; van Deemter, 2006; Turner et al., 2008; Williams and Scheutz, 2017). Under these conditions, it is no longer possible to assume that properties are crisp or Boolean, or even that both sender and receiver necessarily assume the same semantics for those properties. For e"
W19-8625,D15-1075,0,0.466446,"on entailment generation (Kolesnyk et al., 2016; Starc and Mladeni´c, 2017), we explore the additional benefits derived from grounding textual premises in image data. We therefore assume that the data consists of triples hP, I, Qi, such that premises are accompanied by their corresponding images (I) as well as the hypotheses, as in Figure 1. We compare multimodal models to a unimodal setup, with a view to determining to what extent image data helps in generating entailments. Data-driven NLI has received a boost from the availability of large datasets such as SICK (Marelli et al., 2014), SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Some recent work has also investigated multimodal NLI, whereby the classification of the entailment relationship is done on the basis of image features (Xie et al., 2019; Lai, 2018), or a combination of image and textual features (Vu et al., 2018). In particular, Vu et al. (2018) exploited the fact that the main portion of SNLI was created by reusing image captions from the Flickr30k dataset (Young et al., 2014) as premises, for which entailments, contradictions and neutral hypotheses were subsequently crowdsourced via Amazon Mechanical Turk (Bowman et al"
W19-8625,W07-0734,0,0.0212499,"el is based on a standard image captioning setup (Bernardi et al., 2016) in which the entailed hypothesis is based on the image features only, with no premise sentence. This architecture leaves out the encoder RNN completely. Note that, while this is a standard image captioning setup, it is put to a somewhat different use here, since we do not train the model to generate captions (which correspond to the premises in V-SNLI) but the hypotheses (which are taken to follow from the premises). 3.3 Results Evaluation metrics We employed the evaluation metrics BLEU-1 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and CIDEr (Vedantam et al., 2015) to compare generated entailments against the gold outputs in the model. We also compare the model perplexity on the test set. As noted earlier, we adopted a multi-reference approach to evaluation, exploiting the fact that in our dataset we grouped all reference hypotheses corresponding to each premise (and its corre4.2 Human evaluation We conducted a human evaluation experiment, designed to address the question whether better en182 Model Unimodal T+I-Init T+I-Merge IC BLEU-1 0.695 0.634 0.686 0.474 METEOR 0.267 0.239 0.271 0.16 CIDEr 0.938 0.763 0.955 0.235 P"
W19-8625,P15-2017,0,0.0233024,"ding image). Thus, evaluation metrics are calculated by comparing the generated hypotheses to a group of reference hypotheses. This is advantageous, since n-gram based metrics such as BLEU and METEOR are known to yield more reliable results when multiple reference texts are available. used as the representation of the premise P. 2. Multimodal, text+image input, init-inject (T+I-Init): A multimodal model in which the image features are incorporated at the encoding stage, that is, image features are used to initialise the RNN encoder. This architecture, which is widely used in image captioning (Devlin et al., 2015; Liu et al., 2016) is referred to as init-inject, following Tanti et al. (2018), on the grounds that image features are directly injected into the RNN. 4 4.1 Metric-based evaluation The results obtained by all models are shown in Table 1. The T+I-Merge architecture outperforms all other models on CIDEr and METEOR, while the unimodal model, relying only on textual premises, is marginally better on BLEU and has slightly lower perplexity. The lower perplexity of the unimodal model is is unsurprising given that in the unimodal model, the decoder is only conditioned on textual features without ima"
W19-8625,marelli-etal-2014-sick,0,0.0352773,"In contrast to previous work on entailment generation (Kolesnyk et al., 2016; Starc and Mladeni´c, 2017), we explore the additional benefits derived from grounding textual premises in image data. We therefore assume that the data consists of triples hP, I, Qi, such that premises are accompanied by their corresponding images (I) as well as the hypotheses, as in Figure 1. We compare multimodal models to a unimodal setup, with a view to determining to what extent image data helps in generating entailments. Data-driven NLI has received a boost from the availability of large datasets such as SICK (Marelli et al., 2014), SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Some recent work has also investigated multimodal NLI, whereby the classification of the entailment relationship is done on the basis of image features (Xie et al., 2019; Lai, 2018), or a combination of image and textual features (Vu et al., 2018). In particular, Vu et al. (2018) exploited the fact that the main portion of SNLI was created by reusing image captions from the Flickr30k dataset (Young et al., 2014) as premises, for which entailments, contradictions and neutral hypotheses were subsequently crowdsourced via Amazon M"
W19-8625,P19-1334,0,0.0149582,"e last example in Table 2). If textual similarity is indeed playing a role, then we would expect a model to generate entailments (Qgen ) with a better CIDEr score, in those cases where there is a high degree of overlap between the premise and the reference hypothesis (Qref ). We operationalised overlap in terms of the Dice Linguistic biases Recently, a number of authors have expressed concerns that NLI models may be learning heuristics based on superficial syntactic features rather than classifying entailment relationships based on a deeper ‘understanding’ of the semantics of the input texts (McCoy et al., 2019). Indeed, studies on the SNLI dataset have shown that it contains several linguistic biases (Gururangan et al., 2018), such that the semantic relationship (entailment/contradiction/neutral) be185 (a) Histogram of Dice overlap values (b) CIDEr scores for Low vs High overlap test cases Figure 5: Dice overlap vs CIDEr scores (T+I-Merge) coefficient3 , computed over the sets of words in P or Qref , after stop word removal.4 As shown by the histogram in Figure 5a, a significant proportion of the P -Qref pairs have a relatively high Dice coefficient ranging from 0.4 to 0.8. We divided test set insta"
W19-8625,N18-1198,0,0.155043,"There are at least two important motivations for this: The rest of this paper is structured as follows. In Section 2 we discuss further motivations for viewing NLI and entailment generation as multimodal tasks. Section 3 describes the dataset and architectures used; Section 4 presents experimental results, including a human evaluation. Our conclusion (Section 5) is that it is feasible to frame NLI as a generation task and that incorporating non-linguistic information is potentially profitable. However, in line with recent research evaluating Vision-Language (VL) models (Shekhar et al., 2017; Wang et al., 2018; Vu et al., 2018; Tanti et al., 2019), we also find that current architectures are unable to ground textual representations in image data sufficiently. 179 shortcomings in the way VL models adequately utilise visual information, a focus on multimodal entailment generation is especially timely, since it permits direct comparison between models utilising unimodal and multimodal input. As Figure 1 suggests, given the triple hP, I, Qi, the hypothesis Q could be generated from the premise P only, from the image I, or from a combination of P + I. Our question is whether we can generate better entai"
W19-8625,P02-1040,0,0.104042,"image input only (IC): This model is based on a standard image captioning setup (Bernardi et al., 2016) in which the entailed hypothesis is based on the image features only, with no premise sentence. This architecture leaves out the encoder RNN completely. Note that, while this is a standard image captioning setup, it is put to a somewhat different use here, since we do not train the model to generate captions (which correspond to the premises in V-SNLI) but the hypotheses (which are taken to follow from the premises). 3.3 Results Evaluation metrics We employed the evaluation metrics BLEU-1 (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and CIDEr (Vedantam et al., 2015) to compare generated entailments against the gold outputs in the model. We also compare the model perplexity on the test set. As noted earlier, we adopted a multi-reference approach to evaluation, exploiting the fact that in our dataset we grouped all reference hypotheses corresponding to each premise (and its corre4.2 Human evaluation We conducted a human evaluation experiment, designed to address the question whether better en182 Model Unimodal T+I-Init T+I-Merge IC BLEU-1 0.695 0.634 0.686 0.474 METEOR 0.267 0.239 0.271 0."
W19-8625,N18-1101,0,0.0151026,"et al., 2016; Starc and Mladeni´c, 2017), we explore the additional benefits derived from grounding textual premises in image data. We therefore assume that the data consists of triples hP, I, Qi, such that premises are accompanied by their corresponding images (I) as well as the hypotheses, as in Figure 1. We compare multimodal models to a unimodal setup, with a view to determining to what extent image data helps in generating entailments. Data-driven NLI has received a boost from the availability of large datasets such as SICK (Marelli et al., 2014), SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Some recent work has also investigated multimodal NLI, whereby the classification of the entailment relationship is done on the basis of image features (Xie et al., 2019; Lai, 2018), or a combination of image and textual features (Vu et al., 2018). In particular, Vu et al. (2018) exploited the fact that the main portion of SNLI was created by reusing image captions from the Flickr30k dataset (Young et al., 2014) as premises, for which entailments, contradictions and neutral hypotheses were subsequently crowdsourced via Amazon Mechanical Turk (Bowman et al., 2015). This makes it possible to p"
W19-8625,Q14-1006,0,0.716548,"generating entailments. Data-driven NLI has received a boost from the availability of large datasets such as SICK (Marelli et al., 2014), SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018). Some recent work has also investigated multimodal NLI, whereby the classification of the entailment relationship is done on the basis of image features (Xie et al., 2019; Lai, 2018), or a combination of image and textual features (Vu et al., 2018). In particular, Vu et al. (2018) exploited the fact that the main portion of SNLI was created by reusing image captions from the Flickr30k dataset (Young et al., 2014) as premises, for which entailments, contradictions and neutral hypotheses were subsequently crowdsourced via Amazon Mechanical Turk (Bowman et al., 2015). This makes it possible to pair premises with the images for which they were originally written as descriptive captions, thereby reformulating the NLI problem as a Vision-Language task. There are at least two important motivations for this: The rest of this paper is structured as follows. In Section 2 we discuss further motivations for viewing NLI and entailment generation as multimodal tasks. Section 3 describes the dataset and architecture"
W19-8643,J08-4004,0,0.365011,"m, in that different criteria may overlap or be inter-definable. As Gatt and Belz (2010) and Hastie and Belz (2014) suggest, common and shared evaluation guidelines should be developed for each task, and efforts should be made to standardise criteria and naming conventions. In the absence of such guidelines, care should be taken to explicitly define the criteria measured and highlight possible overlaps between them. 4.2 Evaluator agreement The varying opinions of judges are also reflected in low Inter-Annotator Agreement (IAA), where adequate thresholds also tend to be open to interpretation (Artstein and Poesio, 2008). Amidei et al. (2018b) argue that, given the variable nature of natural language, it is undesirable to use restrictive thresholds, since an ostensibly low IAA score could be due to a host of factors, including personal bias. The authors therefore suggest reporting IAA statistics with confidence intervals. However, narrower confidence intervals (suggesting a more precise IAA score) would normally be expected with large samples (e.g., 1000 or more comparisons McHugh, 2012), which are well beyond most sizes reported in our overview (§ 3.4). When the goal of an evaluation is to identify potential"
W19-8643,D19-1052,1,0.784368,"Missing"
W19-8643,W13-5608,0,0.0303571,"Missing"
W19-8643,W15-4708,0,0.124554,"et al., 2017; Sulem et al., 2018; Reiter, 2018, and the discussion in Section 2). Previous studies have also provided overviews of evaluation methods. Gkatzia and Mahamood (2015) focused on NLG papers from 2005-2014; Amidei et al. (2018a) provided a 2013-2018 overview of evaluation in question generation; and Gatt and Krahmer (2018) provided a more general survey of the state-of-the-art in NLG. However, the aim of these papers was to give a structured overview of existing methods, rather than discuss shortcomings and best practices. Moreover, they did not focus on human evaluation. Following Gkatzia and Mahamood (2015), Section 3 provides an overview of current evaluation practices, based on papers from INLG and ACL in 2018. Apart from the broad range of methods used, we also observe that evaluation practices have changed since 2015: for example, there is a significant decrease in the number of papers featuring extrinsic evaluation. This may be caused by the current focus on smaller, decontextualized tasks, which do not take users into account. Building on findings from NLG, but also statistics and the behavioral sciences, Section 4 provides a set of recommendations and best practices for human evaluation i"
W19-8643,W11-2308,0,0.0212041,"Missing"
W19-8643,W08-1120,0,0.042439,"Missing"
W19-8643,hastie-belz-2014-comparative,0,0.0294109,"the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is currently lacking for NLG. One obstacle to addressing the difficulties identified in this section is the lack of a standardised nomenclature for different text quality criteria. This presents a practical problem, in that it is hard to compare evaluation results to previously reported work; but it also presents a theoretical problem, in that different criteria may overlap or be inter-definable. As Gatt and Belz (2010) and Hastie and Belz (2014) suggest, common and shared evaluation guidelines should be developed for each task, and efforts should be made to standardise criteria and naming conventions. In the absence of such guidelines, care should be taken to explicitly define the criteria measured and highlight possible overlaps between them. 4.2 Evaluator agreement The varying opinions of judges are also reflected in low Inter-Annotator Agreement (IAA), where adequate thresholds also tend to be open to interpretation (Artstein and Poesio, 2008). Amidei et al. (2018b) argue that, given the variable nature of natural language, it is"
W19-8643,W17-3904,0,0.0279316,"a continuous (0-100) scale (Graham et al., 2017; Bojar et al., 2016b), similar to Magnitude Estimation (Bard et al., 1996). Zarrieß et al. (2015) used a mouse contingent reading paradigm in an evaluation study of generated text, finding that features recorded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit generated texts. Then processing these edits using post-editing distance measures like Translation Edit Rate (Snover et al., 2006), might be a viable method to investigate the time and cost associated with using a system. While more commonly seen in Machine Translation, authors have explored the use of such metrics in 361 want to compare various versions of their own novel system (e.g. with or without output variation, or relying on different word embedding models, to give j"
W19-8643,W04-3250,0,0.431351,"Missing"
W19-8643,W17-3503,1,0.907118,"Missing"
W19-8643,C18-1082,1,0.915049,"Missing"
W19-8643,W11-2804,1,0.812177,"instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion in the use of single-item scales, unless the construct in question is very simple, clear and onedimensional. Under most conditions, multi-item scales have much higher predictive validity. Using multiple items may well make the"
W19-8643,D17-1238,0,0.261633,"Missing"
W19-8643,N18-2012,0,0.15926,"Missing"
W19-8643,D08-1020,0,0.060535,"This may introduce sampling biases of the kind that have been critiqued in psychology in recent years, where experimental results based on samples of WEIRD (Western, Educated, Industrialised, Rich and Developed) populations may well have given rise to biased models (see, for example, Henrich et al., 2010). evaluation are usually treated as subjective (as in the case of judgments of fluency, adequacy and the like). It is also conceivable that these criteria can be assessed using more objective measures, similar to existing readability measures (e.g., Ambati et al., 2016; Kincaid et al., 1975; Pitler and Nenkova, 2008; Vajjala and Meurers, 2014), where objective text metrics (e.g. average word length, average parse tree height, average number of nouns) are used in a formula, or as features in a regression model, to obtain a score for a text criterion. Similarly, it may be possible to use separate subjective criteria as features in a regression model to calculate overall text quality scores. This would also provide information about the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is cu"
W19-8643,W18-6319,0,0.07533,"Missing"
W19-8643,2006.amta-papers.25,0,0.0928781,"orded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit generated texts. Then processing these edits using post-editing distance measures like Translation Edit Rate (Snover et al., 2006), might be a viable method to investigate the time and cost associated with using a system. While more commonly seen in Machine Translation, authors have explored the use of such metrics in 361 want to compare various versions of their own novel system (e.g. with or without output variation, or relying on different word embedding models, to give just two more or less random examples) to compare them to each other, to some other (‘state-of-the-art’) systems, and/or with respect to one or more baselines. Notice that this quickly gives rise to a rather complex statistical design with multiple fac"
W19-8643,W05-1615,0,0.0446487,"on the task itself, 7-point scales (with clear verbal anchoring) seem best for most tasks. Most of the experimental literature’s findings found that 7-point scales maximise reliability, validity and discriminative power (for instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion"
W19-8643,J09-4008,0,0.43907,"ost of the experimental literature’s findings found that 7-point scales maximise reliability, validity and discriminative power (for instance, Miller, 1956; Green and Rao, 1970; Jones, 1968; Cicchetti et al., 1985; Lissitz and Green, 1975; Preston and Colman, 2000). These studies discourage smaller scales, and adding more response points than 7 also does not increase reliability according to these studies. While Likert scales are the most popular scale 360 NLG (Bernhard et al., 2012; Han et al., 2017; Sripada et al., 2005). Finally, some remarks on qualitative evaluation methods are in order. Reiter and Belz (2009) note that free-text comments can be beneficial to diagnose potential problems of an NLG system. Furthermore, Sambaraju et al. (2011) argue the added value of content analysis and discourse analysis for evaluation. Such qualitative analyses can find potential blind spots of quantitative analyses. At the same time, the subjectivity that is often inherent in studies based on discourse analysis, such as Sambaraju et al. (2011) would need to be offset by data from larger-scale, quantitative studies. tion in the use of single-item scales, unless the construct in question is very simple, clear and o"
W19-8643,D18-1081,0,0.0869369,"ilburg University c.vdrlee@uvt.nl Albert Gatt University of Malta albert.gatt@um.edu.mt Sander Wubben Tilburg University s.wubben@uvt.nl Emiel Krahmer Tilburg University e.j.krahmer@uvt.nl Abstract the evaluation of NLG systems (see Ananthakrishnan et al., 2007; Novikova et al., 2017; Sulem et al., 2018; Reiter, 2018, and the discussion in Section 2). Previous studies have also provided overviews of evaluation methods. Gkatzia and Mahamood (2015) focused on NLG papers from 2005-2014; Amidei et al. (2018a) provided a 2013-2018 overview of evaluation in question generation; and Gatt and Krahmer (2018) provided a more general survey of the state-of-the-art in NLG. However, the aim of these papers was to give a structured overview of existing methods, rather than discuss shortcomings and best practices. Moreover, they did not focus on human evaluation. Following Gkatzia and Mahamood (2015), Section 3 provides an overview of current evaluation practices, based on papers from INLG and ACL in 2018. Apart from the broad range of methods used, we also observe that evaluation practices have changed since 2015: for example, there is a significant decrease in the number of papers featuring extrinsic"
W19-8643,2003.mtsummit-papers.51,0,0.199911,"es not explain the decline in (relative) frequency. That might be because of the set-up of the tasks we see nowadays. Extrinsic evaluations require that the system is embedded in its target use context (or a suitable simulation thereof), which in turn requires that the system addresses a specific purpose. In practice, this often means the system follows the ‘traditional’ NLG 1 In theory this correlation might increase when more reference texts are used, since this allows for more variety in the generated texts. However, in contrast to what this theory would predict, both Doddington (2002) and Turian et al. (2003) report that correlations between metrics and human judgments in machine translation do not improve substantially as the number of reference texts increases. Similarly, Choshen and Abend (2018) found that reliability issues of reference-based evaluation due to low-coverage reference sets cannot be overcome by attainably increasing references. 2 For the ACL papers, we focused on the following tracks: Machine Translation, Summarization, Question Answering, and Generation. See Supplementary Materials for a detailed overview of the investigated papers and their evaluation characteristics 356 Crite"
W19-8643,E14-1031,0,0.0155286,"ng biases of the kind that have been critiqued in psychology in recent years, where experimental results based on samples of WEIRD (Western, Educated, Industrialised, Rich and Developed) populations may well have given rise to biased models (see, for example, Henrich et al., 2010). evaluation are usually treated as subjective (as in the case of judgments of fluency, adequacy and the like). It is also conceivable that these criteria can be assessed using more objective measures, similar to existing readability measures (e.g., Ambati et al., 2016; Kincaid et al., 1975; Pitler and Nenkova, 2008; Vajjala and Meurers, 2014), where objective text metrics (e.g. average word length, average parse tree height, average number of nouns) are used in a formula, or as features in a regression model, to obtain a score for a text criterion. Similarly, it may be possible to use separate subjective criteria as features in a regression model to calculate overall text quality scores. This would also provide information about the importance of the subjective criteria for overall text quality judgments. However, such research on the relationship between subjective criteria and objective measures is currently lacking for NLG. One"
W19-8643,W15-4705,0,0.0209534,"ssing data, or to remove participants that failed ‘attention checks’ (or related checks e.g. instructional manipulation checks, or trap questions) from the sample. However, the use of attention checks is Alternative evaluation instruments should not be ruled out either. Ever since a pilot in 2016 (Bojar et al., 2016a), recent editions of the Conference on Machine Translation (WMT), have used Direct Assessment, whereby participants compare an output to a reference text on a continuous (0-100) scale (Graham et al., 2017; Bojar et al., 2016b), similar to Magnitude Estimation (Bard et al., 1996). Zarrieß et al. (2015) used a mouse contingent reading paradigm in an evaluation study of generated text, finding that features recorded using this paradigm (e.g. reading time) provided valuable information to gauge text quality levels. It should also be noted that most metrics used in NLG are reader-focused. However, in many real-world scenarios, especially ‘creative’ NLG applications, NLG systems and human writers work alongside each other in some way (see Maher, 2012; Manjavacas et al., 2017). With such a collaboration in mind, it makes sense to also investigate writer-focused methods. Having participants edit g"
