2021.acl-long.405,Lower Perplexity is Not Always Human-Like,2021,-1,-1,5,0,9471,tatsuki kuribayashi,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization {---}\textit{the lower perplexity a language model has, the more human-like the language model is}{---} in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models."
2020.repl4nlp-1.8,Adversarial Training for Commonsense Inference,2020,17,0,4,0,1228,lis pereira,Proceedings of the 5th Workshop on Representation Learning for NLP,0,"We apply small perturbations to word embeddings and minimize the resultant adversarial risk to regularize the model. We exploit a novel combination of two different approaches to estimate these perturbations: 1) using the true label and 2) using the model prediction. Without relying on any human-crafted features, knowledge bases, or additional datasets other than the target datasets, our model boosts the fine-tuning performance of RoBERTa, achieving competitive results on multiple reading comprehension datasets that require commonsense inference."
2020.paclic-1.15,Generation and Evaluation of Concept Embeddings Via Fine-Tuning Using Automatically Tagged Corpus,2020,-1,-1,3,0.591704,15816,kanako komiya,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.46,Composing Word Vectors for {J}apanese Compound Words Using Bilingual Word Embeddings,2020,-1,-1,3,0,15890,teruo hirabayashi,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.24,Design of {BCCWJ}-{EEG}: Balanced Corpus with Human Electroencephalography,2020,-1,-1,2,0.75,8398,yohei oseki,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The past decade has witnessed the happy marriage between natural language processing (NLP) and the cognitive science of language. Moreover, given the historical relationship between biological and artificial neural networks, the advent of deep learning has re-sparked strong interests in the fusion of NLP and the neuroscience of language. Importantly, this inter-fertilization between NLP, on one hand, and the cognitive (neuro)science of language, on the other, has been driven by the language resources annotated with human language processing data. However, there remain several limitations with those language resources on annotations, genres, languages, etc. In this paper, we describe the design of a novel language resource called BCCWJ-EEG, the Balanced Corpus of Contemporary Written Japanese (BCCWJ) experimentally annotated with human electroencephalography (EEG). Specifically, after extensively reviewing the language resources currently available in the literature with special focus on eye-tracking and EEG, we summarize the details concerning (i) participants, (ii) stimuli, (iii) procedure, (iv) data preprocessing, (v) corpus evaluation, (vi) resource release, and (vii) compilation schedule. In addition, potential applications of BCCWJ-EEG to neuroscience and NLP will also be discussed."
2020.lrec-1.875,{KOTONOHA}: A Corpus Concordance System for Skewer-Searching {NINJAL} Corpora,2020,-1,-1,5,0,18339,teruaki oka,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The National Institute for Japanese Language and Linguistics, Japan (NINJAL, Japan), has developed several types of corpora. For each corpus NINJAL provided an online search environment, {`}Chunagon{'}, which is a morphological-information-annotation-based concordance system made publicly available in 2011. NINJAL has now provided a skewer-search system {`}Kotonoha{'} based on the {`}Chunagon{'} systems. This system enables querying of multiple corpora by certain categories, such as register type and period."
2020.findings-emnlp.121,Dynamically Updating Event Representations for Temporal Relation Classification with Multi-category Learning,2020,-1,-1,2,0,1612,fei cheng,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Temporal relation classification is the pair-wise task for identifying the relation of a temporal link (TLINKs) between two mentions, i.e. event, time and document creation time (DCT). It leads to two crucial limits: 1) Two TLINKs involving a common mention do not share information. 2) Existing models with independent classifiers for each TLINK category (E2E, E2T and E2D) hinder from using the whole data. This paper presents an event centric model that allows to manage dynamic event representations across multiple TLINKs. Our model deals with three TLINK categories with multi-task learning to leverage the full size of data. The experimental results show that our proposal outperforms state-of-the-art models and two strong transfer learning baselines on both the English and Japanese data."
2020.bucc-1.4,Automatic Creation of Correspondence Table of Meaning Tags from Two Dictionaries in One Language Using Bilingual Word Embedding,2020,-1,-1,3,0,15890,teruo hirabayashi,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"In this paper, we show how to use bilingual word embeddings (BWE) to automatically create a corresponding table of meaning tags from two dictionaries in one language and examine the effectiveness of the method. To do this, we had a problem: the meaning tags do not always correspond one-to-one because the granularities of the word senses and the concepts are different from each other. Therefore, we regarded the concept tag that corresponds to a word sense the most as the correct concept tag corresponding the word sense. We used two BWE methods, a linear transformation matrix and VecMap. We evaluated the most frequent sense (MFS) method and the corpus concatenation method for comparison. The accuracies of the proposed methods were higher than the accuracy of the random baseline but lower than those of the MFS and corpus concatenation methods. However, because our method utilized the embedding vectors of the word senses, the relations of the sense tags corresponding to concept tags could be examined by mapping the sense embeddings to the vector space of the concept tags. Also, our methods could be performed when we have only concept or word sense embeddings whereas the MFS method requires a parallel corpus and the corpus concatenation method needs two tagged corpora."
D19-5902,Word Familiarity Rate Estimation Using a {B}ayesian Linear Mixed Model,2019,-1,-1,1,1,13282,masayuki asahara,Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP,0,"This paper presents research on word familiarity rate estimation using the {`}Word List by Semantic Principles{'}. We collected rating information on 96,557 words in the {`}Word List by Semantic Principles{'} via Yahoo! crowdsourcing. We asked 3,392 subject participants to use their introspection to rate the familiarity of words based on the five perspectives of {`}KNOW{'}, {`}WRITE{'}, {`}READ{'}, {`}SPEAK{'}, and {`}LISTEN{'}, and each word was rated by at least 16 subject participants. We used Bayesian linear mixed models to estimate the word familiarity rates. We also explored the ratings with the semantic labels used in the {`}Word List by Semantic Principles{'}."
Y18-1003,Between Reading Time and Clause Boundaries in {J}apanese - Wrap-up Effect in a Head-Final Language,2018,-1,-1,1,1,13282,masayuki asahara,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1029,Annotation of {`}Word List by Semantic Principles{'} Labels for the {B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese,2018,0,0,2,0,27499,sachi kato,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-6009,Coordinate Structures in {U}niversal {D}ependencies for Head-final Languages,2018,0,0,3,0,1260,hiroshi kanayama,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept in head-final languages. This paper describes the status in the current Japanese and Korean corpora and proposes alternative designs suitable for these languages."
W18-6014,{UD}-{J}apanese {BCCWJ}: {U}niversal {D}ependencies Annotation for the {B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese,2018,0,0,2,0,27836,mai omura,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"In this paper, we describe a corpus UD Japanese-BCCWJ that was created by converting the Balanced Corpus of Contemporary Written Japanese (BCCWJ), a Japanese language corpus, to adhere to the UD annotation schema. The BCCWJ already assigns dependency information at the level of the bunsetsu (a Japanese syntactic unit comparable to the phrase). We developed a program to convert the BCCWJ to UD based on this dependency structure, and this corpus is the result of completely automatic conversion using the program. UD Japanese-BCCWJ is the largest-scale UD Japanese corpus and the second-largest of all UD corpora, including 1,980 documents, 57,109 sentences, and 1,273k words across six distinct domains."
W18-2805,Predicting {J}apanese Word Order in Double Object Constructions,2018,0,1,1,1,13282,masayuki asahara,Proceedings of the Eight Workshop on Cognitive Aspects of Computational Language Learning and Processing,0,This paper presents a statistical model to predict Japanese word order in the double object constructions. We employed a Bayesian linear mixed model with manually annotated predicate-argument structure data. The findings from the refined corpus analysis confirmed the effects of information status of an NP as {`}givennew ordering{'} in addition to the effects of {`}long-before-short{'} as a tendency of the general Japanese word order.
L18-1162,All-words Word Sense Disambiguation Using Concept Embeddings,2018,0,2,3,0,29674,rui suzuki,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1287,{U}niversal {D}ependencies Version 2 for {J}apanese,2018,0,1,1,1,13282,masayuki asahara,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
Y17-1006,Between Reading Time and Information Structure,2017,17,1,1,1,13282,masayuki asahara,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
I17-1041,Between Reading Time and Syntactic/Semantic Categories,2017,15,0,1,1,13282,masayuki asahara,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"This article presents a contrastive analysis between reading time and syntactic/semantic categories in Japanese. We overlaid the reading time annotation of BCCWJ-EyeTrack and a syntactic/semantic category information annotation on the {`}Balanced Corpus of Contemporary Written Japanese{'}. Statistical analysis based on a mixed linear model showed that verbal phrases tend to have shorter reading times than adjectives, adverbial phrases, or nominal phrases. The results suggest that the preceding phrases associated with the presenting phrases promote the reading process to shorten the gazing time."
W16-5406,{BCCWJ}-{D}ep{P}ara: A Syntactic Annotation Treebank on the {`}{B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese{'},2016,0,4,1,1,13282,masayuki asahara,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Paratactic syntactic structures are difficult to represent in syntactic dependency tree structures. As such, we propose an annotation schema for syntactic dependency annotation of Japanese, in which coordinate structures are split from and overlaid on bunsetsu-based (base phrase unit) dependency. The schema represents nested coordinate structures, non-constituent conjuncts, and forward sharing as the set of regions. The annotation was performed on the core data of {`}Balanced Corpus of Contemporary Written Japanese{'}, which comprised about one million words and 1980 samples from six registers, such as newspapers, books, magazines, and web texts."
L16-1261,{U}niversal {D}ependencies for {J}apanese,2016,2,2,3,0,29830,takaaki tanaka,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present an attempt to port the international syntactic annotation scheme, Universal Dependencies, to the Japanese language in this paper. Since the Japanese syntactic structure is usually annotated on the basis of unique chunk-based dependencies, we first introduce word-based dependencies by using a word unit called the Short Unit Word, which usually corresponds to an entry in the lexicon UniDic. Porting is done by mapping the part-of-speech tagset in UniDic to the universal part-of-speech tagset, and converting a constituent-based treebank to a typed dependency tree. The conversion is not straightforward, and we discuss the problems that arose in the conversion and the current solutions. A treebank consisting of 10,000 sentences was built by converting the existent resources and currently released to the public."
C16-2006,{`}{B}on{T}en{'} {--} Corpus Concordance System for {`}{NINJAL} Web {J}apanese Corpus{'},2016,0,0,1,1,13282,masayuki asahara,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"The National Institute for Japanese Language and Linguistics, Japan (NINJAL) has undertaken a corpus compilation project to construct a web corpus for linguistic research comprising ten billion words. The project is divided into four parts: page collection, linguistic analysis, development of the corpus concordance system, and preservation. This article presents the corpus concordance system named {`}BonTen{'} which enables the ten-billion-scaled corpus to be queried by string, a sequence of morphological information or a subtree of the syntactic dependency structure."
C16-2011,Demonstration of {C}ha{K}i.{NET} {--} beyond the corpus search system,2016,0,0,1,1,13282,masayuki asahara,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"ChaKi.NET is a corpus management system for dependency structure annotated corpora. After more than 10 years of continuous development, the system is now usable not only for corpus search, but also for visualization, annotation, labelling, and formatting for statistical analysis. This paper describes the various functions included in the current ChaKi.NET system."
C16-1066,Reading-Time Annotations for {``}{B}alanced {C}orpus of {C}ontemporary {W}ritten {J}apanese{''},2016,10,3,1,1,13282,masayuki asahara,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"The Dundee Eyetracking Corpus contains eyetracking data collected while native speakers of English and French read newspaper editorial articles. Similar resources for other languages are still rare, especially for languages in which words are not overtly delimited with spaces. This is a report on a project to build an eyetracking corpus for Japanese. Measurements were collected while 24 native speakers of Japanese read excerpts from the Balanced Corpus of Contemporary Written Japanese Texts were presented with or without segmentation (i.e. with or without space at the boundaries between bunsetsu segmentations) and with two types of methodologies (eyetracking and self-paced reading presentation). Readers{'} background information including vocabulary-size estimation and Japanese reading-span score were also collected. As an example of the possible uses for the corpus, we also report analyses investigating the phenomena of anti-locality."
O14-4001,{BCCWJ}-{T}ime{B}ank: Temporal and Event Information Annotation on {J}apanese Text,2014,0,1,1,1,13282,masayuki asahara,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 3, September 2014",0,None
Y13-1019,{BCCWJ}-{T}ime{B}ank: Temporal and Event Information Annotation on {J}apanese Text,2013,20,3,1,1,13282,masayuki asahara,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Temporal information extraction can be divided into the following tasks: temporal expression extraction, time normalisation, and temporal ordering relation resolution. The first task is a subtask of a named entity and numeral expression extraction. The second task is often performed by rewriting systems. The third task consists of event anchoring. This paper proposes a Japanese temporal ordering annotation scheme that is used to annotate expressions by referring to 'the' Balanced Corpus of Contemporary Written Japanese' (BCCWJ). We extracted verbal and adjective event expressions as xefxbcx9cEVENTxefxbcx9e in a subset of BCCWJ and annotated a temporal ordering relation xefxbcx9cTLINKxefxbcx9e on the pairs of these event expressions and time expressions obtained from a previous study. The recognition of temporal ordering by language recipients tends to disagree with the normalisation of time expressions. Nevertheless, we should not strive for unique gold annotation data in such a situation. Rather, we should evaluate the degree of inter-annotator discrepancies among subjects in an experiment. This study analysed inter-annotator discrepancies across three annotators performing temporal ordering annotation. The results show that the annotators exhibit little agreement for time segment boundaries, whereas a high level of agreement is exhibited for the annotation of temporal relative ordering tendencies."
P12-1069,Head-driven Transition-based Parsing with Top-down Prediction,2012,28,6,3,1,12296,katsuhiko hayashi,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a novel top-down head-driven parsing algorithm for data-driven projective dependency analysis. This algorithm handles global structures, such as clause and coordination, better than shift-reduce or other bottom-up algorithms. Experiments on the English Penn Treebank data and the Chinese CoNLL-06 data show that the proposed algorithm achieves comparable results with other data-driven dependency parsing algorithms."
C12-2134,Identifying Temporal Relations by Sentence and Document Optimizations,2012,17,0,2,1,32762,katsumasa yoshikawa,Proceedings of {COLING} 2012: Posters,0,"This paper presents a temporal relation identification method optimizing relations at sentence and document levels. Temporal relation identification is to identify temporal orders between events and time expressions. Various approaches of this task have been studied through the shared tasks TempEval (Verhagen et al., 2007, 2010). Not only identifying each temporal relation independently, some works also try to find multiple temporal relations jointly by logical constraints in Integer Linear Programming (Chambers and Jurafsky, 2008; Do et al., 2012) or Markov Logic Networks (Yoshikawa et al., 2009; Ling and Weld, 2010; Ha et al., 2010). Though previous joint approaches optimize temporal relations in an entire document, we first optimize our model at sentence level and then extend it to document level. We consider that different types of temporal relations require different types of optimizations. By evaluating our sentence and document optimized model on the TempEval-2 data, we show that our approaches can achieve competitive performance in comparison to other state-of-the-art systems. We find that the sentence and document optimized model has strong tasks in TempEval-2, respectively."
W11-3504,Different Input Systems for Different Devices,2011,5,0,3,0,44087,asad habib,Proceedings of the Workshop on Advances in Text Input Methods ({WTIM} 2011),0,"We live in the age of touch screen gadgets. The future trends also show promising growth for them. Currently available input systems developed for standard PCs have room for improvement in efficiency, visibility and usability etc. particularly for Perso-Arabic scripts e.g., Urdu. In addition, small touch screen devices expose users to health hazards. We put forth Ergonomics in prime focus to reduce potential health hazards. We proposed distinct touch-screen keypads for different devices that are practically applicable for fast, correct and easy composing. We computed the estimated input time and tapcounts using automated procedure to compare contemporary keypads with our proposed keypads. Our experiments on a considerably large Urdu corpus reveal results of ample significance. Our optimization technique for arrangement of alphabets and unique interface for data input is extendable and equally applicable to other natural languages."
I11-1126,Jointly Extracting {J}apanese Predicate-Argument Relation with {M}arkov {L}ogic,2011,17,10,2,1,32762,katsumasa yoshikawa,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"This paper describes a new Markov Logic approach for Japanese Predicate-Argument (PA) relation extraction. Most previous work built separated classifiers corresponding to each case role and independently identified the PA relations, neglecting dependencies (constraints) between two or more PA relations. We propose a method which collectively extracts PA relations by optimizing all argument candidates in a sentence. Our method can jointly consider dependency between multiple PA relations and find the most probable combination of predicates and their arguments in a sentence. In addition, our model involves new constraints to avoid considering inappropriate candidates for arguments and identify correct PA relations effectively. Compared to the state-of-the-art, our method achieves competitive results without largescale data."
D11-1137,Third-order Variational Reranking on Packed-Shared Dependency Forests,2011,24,12,3,1,12296,katsuhiko hayashi,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel forest reranking algorithm for discriminative dependency parsing based on a variant of Eisner's generative model. In our framework, we define two kinds of generative model for reranking. One is learned from training data offline and the other from a forest generated by a baseline parser on the fly. The final prediction in the reranking stage is performed using linear interpolation of these models and discriminative model. In order to efficiently train the model from and decode on a hypergraph data structure representing a forest, we apply extended inside/outside and Viterbi algorithms. Experimental results show that our proposed forest reranking algorithm achieves significant improvement when compared with conventional approaches."
P10-2018,A Structured Model for Joint Learning of Argument Roles and Predicate Senses,2010,14,22,2,1,5308,yotaro watanabe,Proceedings of the {ACL} 2010 Conference Short Papers,0,"In predicate-argument structure analysis, it is important to capture non-local dependencies among arguments and inter-dependencies between the sense of a predicate and the semantic roles of its arguments. However, no existing approach explicitly handles both non-local dependencies and semantic dependencies between predicates and arguments. In this paper we propose a structured model that overcomes the limitation of existing approaches; the model captures both types of dependencies simultaneously by introducing four types of factors including a global factor type capturing non-local dependencies among arguments and a pairwise factor type capturing local dependencies between a predicate and an argument. In experiments the proposed model achieved competitive results compared to the state-of-the-art systems without applying any feature selection procedure."
W09-1218,Multilingual Syntactic-Semantic Dependency Parsing with Three-Stage Approximate Max-Margin Linear Models,2009,21,1,2,1,5308,yotaro watanabe,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes a system for syntactic-semantic dependency parsing for multiple languages. The system consists of three parts: a state-of-the-art higher-order projective dependency parser for syntactic dependency parsing, a predicate classifier, and an argument classifier for semantic dependency parsing. For semantic dependency parsing, we explore use of global features. All components are trained with an approximate max-margin learning algorithm.n n In the closed challenge of the CoNLL-2009 Shared Task (Hajic et al., 2009), our system achieved the 3rd best performances for English and Czech, and the 4th best performance for Japanese."
P09-1046,Jointly Identifying Temporal Relations with {M}arkov {L}ogic,2009,14,103,3,1,32762,katsumasa yoshikawa,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"Recent work on temporal relation identification has focused on three types of relations between events: temporal relations between an event and a time expression, between a pair of events and between an event and the document creation time. These types of relations have mostly been identified in isolation by event pairwise comparison. However, this approach neglects logical constraints between temporal relations of different types that we believe to be helpful. We therefore propose a Markov Logic model that jointly identifies relations of all three relation types simultaneously. By evaluating our model on the TempEval data we show that this approach leads to about 2% higher accuracy for all three types of relations ---and to the best results for the task when compared to those of other machine learning based systems."
W08-2132,A Pipeline Approach for Syntactic and Semantic Dependency Parsing,2008,13,3,3,1,5308,yotaro watanabe,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper describes our system for syntactic and semantic dependency parsing to participate the shared task of CoNLL-2008. We use a pipeline approach, in which syntactic dependency parsing, word sense disambiguation, and semantic role labeling are performed separately: Syntactic dependency parsing is performed by a tournament model with a support vector machine; word sense disambiguation is performed by a nearest neighbour method in a compressed feature space by probabilistic latent semantic indexing; and semantic role labeling is performed by a an online passive-aggressive algorithm. The submitted result was 79.10 macro-average F1 for the joint task, 87.18% syntactic dependencies LAS, and 70.84 semantic dependencies F1. After the deadline, we constructed the other configuration, which achieved 80.89 F1 for the joint task, and 74.53 semantic dependencies F1. The result shows that the configuration of pipeline is a crucial issue in the task."
O08-4003,Constructing a Temporal Relation Tagged Corpus of {C}hinese Based on Dependency Structure Analysis,2008,-1,-1,2,1,38079,yuchang cheng,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 2, June 2008",0,None
I08-4005,Use of Event Types for Temporal Relation Identification in {C}hinese Text,2008,6,3,2,1,38079,yuchang cheng,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper investigates a machine learning approach for identification of temporal relation between events in Chinese text. We proposed a temporal relation annotation guideline (Cheng, 2007) and constructed temporal information annotated corpora. However, our previous criteria did not deal with various uses of Chinese verbs. For supplementing the previous version of our criteria, we introduce attributes of verbs that describe event types. We illustrate the attributes by the different examples of verb usages. We perform an experiment to evaluate the effect of our event type attributes in the temporal relation identification. As far as we know, this is the first work of temporal relation identification between verbs in Chinese texts. The result shows that the use of the attributes of verbs can improve the annotation accuracy."
I08-4008,Analyzing {C}hinese Synthetic Words with Tree-based Information and a Survey on {C}hinese Morphologically Derived Words,2008,7,5,2,0,48567,jia lu,Proceedings of the Sixth {SIGHAN} Workshop on {C}hinese Language Processing,0,"The lack of internal information of Chinese synthetic words has become a crucial problem for Chinese morphological analysis systems which will face various needs of segmentation standards for upper NLP applications in the future. In this paper, we first categorize Chinese synthetic words into several types according to their inside semantic and syntactic structure, and then propose a method to represent these inside information of word by applying a tree-based structure. Then we try to automatically identify the inner morphological structure of 3-character synthetic words by using a large corpus and try to add syntactic tags to their internal structure. We believe that this tree-based word internal information could be useful in specifying a Chinese synthetic word segmentation standard."
I08-1062,{J}apanese-{S}panish Thesaurus Construction Using {E}nglish as a Pivot,2008,7,6,2,0,48679,jessica ramirez,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We present the results of research with the goal of automatically creating a multilingual thesaurus based on the freely available resources of Wikipedia and WordNet. Our goal is to increase resources for natural language processing tasks such as machine translation targeting the Japanese-Spanish language pair. Given the scarcity of resources, we use existing English resources as a pivot for creating a trilingual JapaneseSpanish-English thesaurus. Our approach consists of extracting the translation tuples from Wikipedia, disambiguating them by mapping them to WordNet word senses. We present results comparing two methods of disambiguation, the first using VSM on Wikipedia article texts and WordNet definitions, and the second using categorical information extracted from Wikipedia, We find that mixing the two methods produces favorable results. Using the proposed method, we have constructed a multilingual Spanish-Japanese-English thesaurus consisting of 25,375 entries. The same method can be applied to any pair of languages that are linked to English in Wikipedia."
C08-1046,{J}apanese Dependency Parsing Using a Tournament Model,2008,14,3,2,0,44088,masakazu iwatate,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"In Japanese dependency parsing, Kudo's relative preference-based method (Kudo and Matsumoto, 2005) outperforms both deterministic and probabilistic CKY-based parsing methods. In Kudo's method, for each dependent word (or chunk) a log-linear model estimates relative preference of all other candidate words (or chunks) for being as its head. This cannot be considered in the deterministic parsing methods. We propose an algorithm based on a tournament model, in which the relative preferences are directly modeled by one-on-one games in a step-ladder tournament. In an evaluation experiment with Kyoto Text Corpus Version 4.0, the proposed method outperforms previous approaches, including the relative preference-based method."
S07-1052,{NAIST}.{J}apan: Temporal Relation Identification Using Dependency Parsed Tree,2007,8,26,2,1,38079,yuchang cheng,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"In this paper, we attempt to use a sequence labeling model with features from dependency parsed tree for temporal relation identification. In the sequence labeling model, the relations of contextual pairs can be used as features for relation identification of the current pair. Head-modifier relations between pairs of words within one sentence can be also used as the features. In our preliminary experiments, these features are effective for the temporal relation identification tasks."
D07-1068,A Graph-Based Approach to Named Entity Categorization in {W}ikipedia Using Conditional Random Fields,2007,22,45,2,1,5308,yotaro watanabe,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper presents a method for categorizing named entities in Wikipedia. In Wikipedia, an anchor text is glossed in a linked HTML text. We formalize named entity categorization as a task of categorizing anchor texts with linked HTML texts which glosses a named entity. Using this representation, we introduce a graph structure in which anchor texts are regarded as nodes. In order to incorporate HTML structure on the graph, three types of cliques are defined based on the HTML tree structure. We propose a method with Conditional Random Fields (CRFs) to categorize the nodes on the graph. Since the defined graph may include cycles, the exact inference of CRFs is computationally expensive. We introduce an approximate inference method using Treebased Reparameterization (TRP) to reduce computational cost. In experiments, our proposed model obtained significant improvements compare to baseline models that use Support Vector Machines."
Y06-1044,The Construction of a Dictionary for a Two-layer {C}hinese Morphological Analyzer,2006,11,2,4,1,44966,chooiling goh,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"We built a morphological analyzer, which can be freely used by anyone for research purpose. In order to build a pratical system, a dictionary with reasonable size is necessary. The initial dictionary is built from the Penn Chinese Treebank corpus v4.0 and contains only 33,438 entries. Since the initial dictionary is quite small, unknown word detection methods are applied to a huge raw text in order to extract new words to be added into the system dictionary. We have successfully constructed a dictionary with 120,769 entries. Finally, we propose a two-layer morphological analyzer to cater for two sets of outputs. The first layer produces the minimal segmentation units defined by us, and the second layer transforms the output of the first layer to the original segmentation units defined by Penn Chinese Treebank."
W06-2927,Multi-lingual Dependency Parsing at {NAIST},2006,14,16,2,1,38079,yuchang cheng,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"In this paper, we present a framework for multi-lingual dependency parsing. Our bottom-up deterministic parser adopts Nivre's algorithm (Nivre, 2004) with a preprocessor. Support Vector Machines (SVMs) are utilized to determine the word dependency attachments. Then, a maximum entropy method (MaxEnt) is used for determining the label of the dependency relation. To improve the performance of the parser, we construct a tagger based on SVMs to find neighboring attachment as a preprocessor. Experimental evaluation shows that the proposed extension improves the parsing accuracy of our base parser in 9 languages. (Hajic et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; Bohmova et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dzeroski et al., 2006; Civit and Marti, 2002; Nilsson et al., 2005; Oflazer et al., 2003; Atalay et al., 2003)."
matsumoto-etal-2006-annotated,An Annotated Corpus Management Tool: {C}ha{K}i,2006,3,2,2,0,991,yuji matsumoto,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Large scale annotated corpora are very important not only inlinguistic research but also in practical natural language processingtasks since a number of practical tools such as Part-of-speech (POS) taggers and syntactic parsers are now corpus-based or machine learning-based systems which require some amount of accurately annotated corpora. This article presents an annotated corpus management tool that provides various functions that include flexible search, statistic calculation, and error correction for linguistically annotated corpora. The target of annotation covers POS tags, base phrase chunks and syntactic dependency structures. This tool aims at helping development of consistent construction of lexicon and annotated corpora to be used by researchers both in linguists and language processing communities."
O05-4005,{C}hinese Word Segmentation by Classification of Characters,2005,-1,-1,2,1,44966,chooiling goh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 3, September 2005: Special Issue on Selected Papers from {ROCLING} {XVI}",0,None
I05-3003,{C}hinese Deterministic Dependency Analyzer: Examining Effects of Global Features and Root Node Finder,2005,0,2,2,1,38079,yuchang cheng,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,None
I05-3018,Combination of Machine Learning Methods for Optimum {C}hinese Word Segmentation,2005,10,21,1,1,13282,masayuki asahara,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This article presents our recent work for participation in the Second International Chinese Word Segmentation Bakeoff. Our system performs two procedures: Out-ofvocabulary extraction and word segmentation. We compose three out-of-vocabulary extraction modules: Character-based tagging with different classifiers xe2x80x93 maximum entropy, support vector machines, and conditional random fields. We also compose three word segmentation modules xe2x80x93 character-based tagging by maximum entropy classifier, maximum entropy markov model, and conditional random fields. All modules are based on previously proposed methods. We submitted three systems which are different combination of the modules."
I05-1050,Automatic Extraction of Fixed Multiword Expressions,2005,0,0,2,0,51064,campbell hore,Second International Joint Conference on Natural Language Processing: Full Papers,0,None
I05-1059,Building a {J}apanese-{C}hinese Dictionary Using Kanji/Hanzi Conversion,2005,7,17,2,1,44966,chooiling goh,Second International Joint Conference on Natural Language Processing: Full Papers,0,"A new bilingual dictionary can be built using two existing bilingual dictionaries, such as Japanese-English and English-Chinese to build Japanese-Chinese dictionary. However, Japanese and Chinese are nearer languages than English, there should be a more direct way of doing this. Since a lot of Japanese words are composed of kanji, which are similar to hanzi in Chinese, we attempt to build a dictionary for kanji words by simple conversion from kanji to hanzi. Our survey shows that around 2/3 of the nouns and verbal nouns in Japanese are kanji words, and more than 1/3 of them can be translated into Chinese directly. The accuracy of conversion is 97%. Besides, we obtain translation candidates for 24% of the Japanese words using English as a pivot language with 77% accuracy. By adding the kanji/hanzi conversion method, we increase the candidates by 9%, to 33%, with better quality candidates."
Y04-1014,Pruning False Unknown Words to Improve {C}hinese Word Segmentation,2004,10,6,2,1,44966,chooiling goh,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"During the process of unknown word detection in Chinese word segmentation, many detected word candidates are invalid. These false unknown word candidates deteriorate the overall segmentation accuracy, as it will affect the segmentation accuracy of known words. Therefore, we propose to eliminate as many invalid word candidates as possible by a pruning process. Our experiments show that by cutting down the invalid unknown word candidates, we improve the segmentation accuracy of known words and hence that of the overall segmentation accuracy."
W04-1109,{C}hinese Word Segmentation by Classification of Characters,2004,0,7,2,1,44966,chooiling goh,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,None
C04-1066,{J}apanese Unknown Word Identification by Character-based Chunking,2004,17,20,1,1,13282,masayuki asahara,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We introduce a character-based chunking for unknown word identification in Japanese text. A major advantage of our method is an ability to detect low frequency unknown words of unrestricted character type patterns. The method is built upon SVM-based chunking, by use of character n-gram and surrounding context of n-best word segmentation candidates from statistical morphological analysis as features. It is applied to newspapers and patent texts, achieving 95% precision and 55-70% recall for newspapers and more than 85% precision for patent texts."
W03-1720,Combining Segmenter and Chunker for {C}hinese Word Segmentation,2003,3,21,1,1,13282,masayuki asahara,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Our proposed method is to use a Hidden Markov Model-based word segmenter and a Support Vector Machine-based chunker for Chinese word segmentation. Firstly, input sentences are analyzed by the Hidden Markov Model-based word segmenter. The word segmenter produces n-best word candidates together with some class information and confidence measures. Secondly, the extracted words are broken into character units and each character is annotated with the possible word class and the position in the word, which are then used as the features for the chunker. Finally, the Support Vector Machine-based chunker brings character units together into words so as to determine the word boundaries."
P03-2039,{C}hinese Unknown Word Identification Using Character-based Tagging and Chunking,2003,3,43,2,0,46757,chooi goh,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,"Since written Chinese has no space to delimit words, segmenting Chinese texts becomes an essential task. During this task, the problem of unknown word occurs. It is impossible to register all words in a dictionary as new words can always be created by combining characters. We propose a unified solution to detect unknown words in Chinese texts. First, a morphological analysis is done to obtain initial segmentation and POS tags and then a chunker is used to detect unknown words."
N03-1002,{J}apanese Named Entity Extraction with Redundant Morphological Analysis,2003,6,140,1,1,13282,masayuki asahara,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method."
asahara-etal-2002-use,Use of {XML} and Relational Databases for Consistent Development and Maintenance of Lexicons and Annotated Corpora,2002,4,0,1,1,13282,masayuki asahara,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"Abstract In this paper, we present a use of XML and relational database for developing and maintaining Japanese linguistic resources. In languages that do not provide word delimitation in texts (e.g. Chinese and Japanese), consistent delimitation definition of words in a lexicon is a critical issue to build POS tagged corpora. When we change the definition of word delimitation in the lexicon, we need to modify the tagged corpora to make them consistent with the lexicon. We propose a use of relational database to perform these modifications in tandem. Hence, in the Japanese language, there are several standards for word delimitation definition. To accommodate more than one definition of word delimitation, we compose a compounding word lexicon in the database. The compounding word lexicon includes dependency structures of compounding words."
C00-1004,Extended Models and Tools for High-performance Part-of-speech,2000,2,3,1,1,13282,masayuki asahara,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,None
