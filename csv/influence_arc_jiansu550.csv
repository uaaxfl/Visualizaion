C04-1033,P95-1017,0,0.602569,"d coreferential clusters. Compared with individual NPs, coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usua"
C04-1033,P98-1012,0,0.0814212,"9) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the frame"
C04-1033,W99-0611,0,0.0941913,"ll and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Ba"
C04-1033,N04-1002,0,0.0128346,"rvised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selection. Besides, the idea of clustering can be seen in the research of cross-document coreference, where NPs with high context similarity would be chained together based on certain clustering methods (Bagga and Biermann, 1998; Gooi and Allan, 2004). 6 Conclusion In this paper we have proposed a supervised learning-based approach to coreference resolution. Rather than mining the coreferential relationship between NP pairs as in conventional approaches, our approach does resolution by exploring the relationships between an NP and the coreferential clusters. Compared to individual NPs, coreferential clusters provide more information for rules learning and reference determination. In the paper, we first introduced the conventional NP-NP based approach and analyzed its limitation. Then we described in details the framework of our NP-Cluster"
C04-1033,N01-1008,0,0.0893674,"Missing"
C04-1033,J94-4002,0,0.569485,"t, the Best-First strategy was applied. As illustrated in the table, we could observe that: 1. Without the three features, the system is equivalent to the baseline system in terms of the same recall and precision. 2. Cluster StrSim (f23 ) is the most effective as it contributes most to the system performance. Simply using this feature boosts 5 Related work To our knowledge, our work is the first supervised-learning based attempt to do coreference resolution by exploring the relationship between an NP and coreferential clusters. In the heuristic salience-based algorithm for pronoun resolution, Lappin and Leass (1994) introduce a procedure for identifying anaphorically linked NP as a cluster for which a global salience value is computed as the sum of the salience values of its elements. Cardie and Wagstaff (1999) have proposed an unsupervised approach which also incorporates cluster information into consideration. Their approach uses hard constraints to preclude the link of an NP to a cluster mismatching the number, gender or semantic agreements, while our approach takes these agreements together with other features (e.g. cluster-length, string-matching degree,etc) as preference factors for cluster selecti"
C04-1033,W02-1008,0,0.0936705,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,P02-1014,0,0.267893,"ters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its referred entity. Con"
C04-1033,W03-1307,1,0.587032,"Missing"
C04-1033,J01-4004,0,0.870843,", coreferential clusters could provide richer information of the entities for better rules learning and reference determination. The evaluation done on MEDLINE data set shows that our approach outperforms the baseline NP-NP based approach in both recall and precision. 1 Introduction Coreference resolution is the process of linking as a cluster1 multiple expressions which refer to the same entities in a document. In recent years, supervised machine learning approaches have been applied to this problem and achieved considerable success (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002b)). The main idea of most supervised learning approaches is to recast this task as a binary classification problem. Specifically, a classifier is learned and then used to determine whether or not two NPs in a document are co-referring. Clusters are formed by linking coreferential NP pairs according to a certain selection strategy. In this way, the identification of coreferential clusters in text is reduced to the identification of coreferential NP pairs. One problem of such reduction, however, is that the individual NP usually lacks adequate descriptive information of its"
C04-1033,W02-1040,0,0.120755,"ion 4 reports and discusses the experimental results. Section 5 describes related research work. Finally, conclusion is given in Section 6. 2 Baseline: the NP-NP based approach 2.1 Framework description We built a baseline coreference resolution system, which adopts the common NP-NP based learning framework as employed in (Soon et al., 2001). Each instance in this approach takes the form of i {NPj , NPi }, which is associated with a feature vector consisting of 18 features (f1 ∼ f18 ) as described in Table 2. Most of the features come from Soon et al. (2001)’s system. Inspired by the work of (Strube et al., 2002) and (Yang et al., 2004), we use two features, StrSim1 (f17 ) and StrSim2 (f18 ), to measure the string-matching degree of NPj and NPi . Given the following similarity function: Str Simlarity(Str1 , Str2 ) = 100 × |Str1 ∩ Str2 | Str1 StrSim1 and StrSim2 are computed using Str Similarity(SN Pj , SN Pi ) and Str Similarity(SN Pi , SN Pj ), respectively. Here SN P is the token list of NP, which is obtained by applying word stemming, stopword removal and acronym expansion to the original string as described in Yang et al. (2004)’s work. During training, for each anaphor NPj in a given text, a posi"
C04-1033,M95-1005,0,0.468812,"Missing"
C04-1033,P02-1060,1,0.804783,"Missing"
C04-1033,C98-1012,0,\N,Missing
C04-1075,P87-1022,0,0.660741,"Missing"
C04-1075,C88-1021,0,0.40473,"nformation extraction and question answering. In particular, information extraction systems like those built in the DARPA Message Understanding Conferences (MUC) have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command co"
C04-1075,C90-3063,0,0.0314584,"e revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard"
C04-1075,J86-3001,0,0.0235999,"nts, c-command constraints, semantic consistency), or “preferences” which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are de"
C04-1075,P98-2143,0,0.0616325,"ch a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard invalid ones from the set of possib"
C04-1075,P02-1014,0,0.718957,"tion extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and number agreements, c-command constraints, semantic consistency, syntactic parallelism, semantic parallelism, salience, proximity, etc. These factors can be either “constraints” which discard invalid ones from the set of possible candidates (such as gender and number agr"
C04-1075,A88-1003,0,0.301296,"n, text summarization, information extraction and question answering. In particular, information extraction systems like those built in the DARPA Message Understanding Conferences (MUC) have revealed that coreference resolution is such a crucial component of an information extraction system that a separate coreference task has been defined and evaluated in MUC-6 (1995) and MUC-7 (1998). There is a long tradition of work on coreference resolution within computational linguistics. Many of the earlier works in coreference resolution heavily exploited domain and linguistic knowledge (Carter 1987; Rich and LuperFoy 1988; Carbonell and Brown 1988). However, the pressing need for the development of robust and inexpensive solutions encouraged the drive toward knowledge-poor strategies (Dagan and Itai 1990; Lappin and Leass 1994; Mitkov 1998; Soon, Ng and Lim 2001; Ng and Cardie 2002), which was further motivated by the emergence of cheaper and more reliable corpusbased NLP tools such as part-of-speech taggers and shallow parsers alongside the increasing availability of corpora and other resources (e.g. ontology). Approaches to coreference resolution usually rely on a set of factors which include gender and numb"
C04-1075,J01-4004,0,0.540354,"Missing"
C04-1075,P98-2204,0,0.0232425,"erences” which gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge. Fi"
C04-1075,J01-4003,0,0.224416,"h gives more preference to certain candidates and less to others (such as syntactic parallelism, semantic parallelism, salience, proximity). While a number of approaches use a similar set of factors, the computational strategies (the way antecedents are determined, i.e. the algorithm and formula for assigning antecedents) may differ, i.e. from simple co-occurrence rules (Dagan and Itai 1990) to decision trees (Soon, Ng and Lim 2001; Ng and Cardie 2002) to pattern induced rules (Ng and Cardie 2002) to centering algorithms (Grosz and Sidner 1986; Brennan, Friedman and Pollard 1987; Strube 1998; Tetreault 2001). This paper proposes a simple constraint-based multi-agent system to coreference resolution of general noun phrases in unrestricted English text. For a given anaphor and all the preceding referring expressions as the antecedent candidates, a common constraint agent is first presented to filter out invalid antecedent candidates using various kinds of general knowledge. Then, according to the type of the anaphor, a special constraint agent is proposed to filter out more invalid antecedent candidates using constraints which are derived from various kinds of special knowledge. Finally, a simple p"
C04-1075,W00-1309,1,0.873037,"Missing"
C04-1075,P02-1060,1,0.838301,"Missing"
C04-1075,J94-4002,0,\N,Missing
C04-1075,C98-2138,0,\N,Missing
C04-1075,C98-2199,0,\N,Missing
C04-1103,W03-0317,0,0.138161,"Missing"
C04-1103,P98-2220,0,0.225691,"Missing"
C04-1103,kang-choi-2000-automatic,0,\N,Missing
C04-1103,C02-1099,0,\N,Missing
C04-1103,C00-1056,0,\N,Missing
C04-1103,W03-1508,0,\N,Missing
C04-1103,C98-2215,0,\N,Missing
C04-1103,P02-1051,0,\N,Missing
C08-1016,W03-1023,0,\N,Missing
C08-1016,J05-3004,0,\N,Missing
C08-1016,J06-1005,0,\N,Missing
C08-1016,P02-1014,0,\N,Missing
C08-1016,J01-4004,0,\N,Missing
C10-1022,J01-4004,0,0.277206,"on training instance, the various features useful for event pronoun resolution and SVM classifier with adjustment of hyper-plane. Twin-candidate model is further introduced to capture the preferences among candidates. Section 3 presents in details the structural syntactic feature and the kernel functions to incorporate such a feature in the resolution. Section 4 presents the experiment results and some discussion. Section 5 concludes the paper. 2 The Resolution Framework Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al., 2001) and (Ng and Cardie, 2002a). 2.1 Training and Testing instance In the learning framework, training or testing instance of the resolution system has a form of where is the ith candidate of the antecedent of anaphor . An instance is labeled as positive if is the antecedent of , or negative if is not the antecedent of . An instance is associated with a feature vector which records different properties and relations between and . The features used in our system will be discussed later in this paper. During training, for each event pronoun, we consider the preceding verbs in its current and previou"
C10-1022,P02-1011,0,0.646355,"Missing"
C10-1022,C02-1139,0,0.176584,"e various features useful for event pronoun resolution and SVM classifier with adjustment of hyper-plane. Twin-candidate model is further introduced to capture the preferences among candidates. Section 3 presents in details the structural syntactic feature and the kernel functions to incorporate such a feature in the resolution. Section 4 presents the experiment results and some discussion. Section 5 concludes the paper. 2 The Resolution Framework Our event-anaphora resolution system adopts the common learning-based model for object anaphora resolution, as employed by (Soon et al., 2001) and (Ng and Cardie, 2002a). 2.1 Training and Testing instance In the learning framework, training or testing instance of the resolution system has a form of where is the ith candidate of the antecedent of anaphor . An instance is labeled as positive if is the antecedent of , or negative if is not the antecedent of . An instance is associated with a feature vector which records different properties and relations between and . The features used in our system will be discussed later in this paper. During training, for each event pronoun, we consider the preceding verbs in its current and previous two sentences as its an"
C10-1022,P03-1054,0,0.0134325,"Missing"
C10-1022,P03-1023,1,0.917703,"Missing"
C10-1022,P04-1043,0,0.30271,"e or after the pronoun. date and the pronoun3. Such a feature keeps the most information related to the pronoun and candidate pair. Figure 3 shows the structure for feature full-expansion for instance {invaded, it}. As illustrated, the “NP” node for “perhaps different groups” is further expanded to the POS level. All its child nodes are included in the full-expansion tree except the surface words. 3.2 Convolution Parse Tree Kernel and Composite Kernel To calculate the similarity between two structured features, we use the convolution tree kernel that is defined by Collins and Duffy (2002) and Moschitti (2004). Given two trees, the kernel will enumerate all their sub-trees and use the number of common sub-trees as the measure of similarity between two trees. The above tree kernel only aims for the structured feature. We also need a composite kernel to combine the structured feature and the flat features from section 2.2. In our study we define the composite kernel as follows: where is the convolution tree kernel defined for the structured feature, and is the kernel applied on the flat features. Both kernels are divided by their respective length4 for normalization. The new composite kernel , define"
C10-1022,P04-1017,1,0.906771,"to an equivalent level with the minority class samples which is described in (Kubat and Matwin, 1997) and (Estabrooks et al, 2004). In (Ng and Cardie, 2002b), they proposed a negative sample selection scheme which included only negative instances found in between an anaphor and its antecedent. However, in our event pronoun resolution, we are distinguishing the event-anaphoric from non-event anaphoric which is different from (Ng and Cardie, 2002b). 2.2 Feature Space In a conventional pronoun resolution, a set of syntactic and semantic knowledge has been reported as in (Strube and Müller, 2003; Yang et al, 2004;2005a;2006). These features include number agreement, gender agreement and many others. However, most of these features are not useful for our task, as our antecedents are inflectional verbs instead of noun phrases. Thus we have conducted a study on effectiveness of potential positional, lexical and syntactic features. The lexical knowledge is mainly collected from corpus statistics. The syntactic features are mainly from intuitions. These features are purposely engineered to be highly correlated with positive instances. Therefore such kind of features will contribute to a high precision clas"
C10-1022,P05-1021,1,0.89395,"Missing"
C10-1022,I05-1063,1,0.905347,"Missing"
C10-1022,N06-2015,0,0.0827532,"Missing"
C10-1022,P06-1006,1,0.877191,"ine (Vapnik, 1995) to allow the use of kernels to incorporate the structure feature. One advantage of SVM is that we can use tree kernel approach to capture syntactic parse tree information in a particular high-dimension space. Suppose a training set consists of labeled vectors , where is the feature vector of a training instance and is its class label. The classifier learned by SVM is: where is the learned parameter for a support vector . An instance is classified as positive if . Otherwise, is negative.  Adjust Hyper-plane with Development Data Previous works on pronoun resolution such as (Yang et al, 2006) used the default setting for hyper-plane which sets . And an instance is positive if and negative otherwise. In our study, we look into a method of adjusting the hyper-plane’s position using development data to improve the classifier’s performance. Considering a default model setting for SVM as shown in Figure 2(for illustration purpose, we use a 2-D example). Figure 2: 2-D SVM Illustration The objective of SVM learning process is to find a set of weight vector which maximizes the margin (defined as ) with constraints defined 191 by support vectors. The separating hyper-plane is given by as b"
C10-1022,E06-1015,0,0.123054,"Missing"
C10-1022,P07-1103,0,0.613617,"Missing"
C10-1022,J08-3002,1,\N,Missing
C10-1022,P03-1022,0,\N,Missing
C10-1022,P08-1096,1,\N,Missing
C10-1022,P02-1034,0,\N,Missing
C10-1022,P02-1014,0,\N,Missing
C10-1145,S07-1012,0,0.0325864,"Wen Ting Wang‡ † ‡ School of Computing National University of Singapore {z-wei, tancl} @comp.nus.edu.sg people search, knowledge base population (KBP), and information extraction, because an entity (such as Abbott Laboratories, a diversified pharmaceuticals health care company) can be referred to by multiple mentions (e.g. “ABT” and “Abbott”), and a mention (e.g. “Abbott”) can be shared by different entities (e.g. Abbott Texas: a city in United States; Bud Abbott, an American actor; and Abbott Laboratories, a diversified pharmaceutical health care company). Both Web People Search (WePS) task (Artiles et al. 2007) and Global Entity Detection & Recognition task (GEDR) in Automatic Content Extraction 2008 (ACE08) disambiguate entity mentions by clustering documents with these mentions. Each cluster then represents a unique entity. Recently entity linking has been proposed in this field. However, it is quite different from the previous tasks. Given a knowledge base, a document collection, entity linking task as defined by KBP-091 (McNamee and Dang, 2009) is to determine for each name string and the document it appears, which knowledge base entity is being referred to, or if the entity is a new entity whic"
C10-1145,D09-1056,0,0.00891313,"ular learning algorithm. During disambiguation, (query, entity) is presented to the classifier which then returns a class label. Each (query, entity) pair is represented by the feature vector using different features and similarity metrics. We chose the following three classes of features as they represent a wide range of information - lexical features, word-category pair, NE type - that have been proved to be effective in previous works and tasks. We now discuss the three categories of features used in our framework in details. Lexical features. For Bag of Words feature in Web People Search, Artiles et al. (2009) illustrated that noun phrase and n-grams longer than 2 were not effective in comparison with tokenbased features and using bi-grams gives the best 1294 results only reaching recall 0.7. Thus, we use token-based features. The similarity metric we choose is cosine (using standard tf.idf weighting). Furthermore, we also take into account the co-occurring NEs and represent it in the form of token-based features. Then, the single cosine similarity feature is based on Co-occurring NEs and Bag of Words. Word Category Pair. Bunescu (2007) demonstrated that word-category pairs extracted from the docum"
C10-1145,P07-1033,0,0.00976239,"Missing"
C10-1145,N04-1002,0,0.026506,"h the highest rank is chosen. Bag of words and co-occurring NEs are represented in the form of token-based feature vectors. Then tf.idf is employed to calculate similarity between feature vectors. To make the baseline system with tokenbased features state-of-the-art, we conduct a series of experiments. Table 1 lists the performances of our token-based ranking systems. In our experiment, local tokens are text segments generated by a text window centered on the mention. We set the window size to 55, which is the value that was observed to give optimum performance for the disambiguation problem (Gooi and Allan, 2004). Full tokens and NE are all the tokens and named entities co-occurring in the text respectively. We notice that tokens of the full text as well as the co-occurring named entity produce the best baseline performance, which we use for the further experiment. http://download.wikipedia.org 1295 local tokens local tokens + NE full tokens + NE Micro-averaged Accuracy 60.0 60.6 61.9 Table 1: Results of the ranking methods 4.3 Experiment and Result As discussed in Section 3.1, we exploit two more knowledge sources in Wikipedia: “did you mean” (DYM) and “Wikipedia search engine” (SE) for name variatio"
C10-1145,zesch-etal-2008-extracting,0,0.00970901,"uation, which links an entity mention with the real world entity it refers to. 3.1 Name Variation The aim for Name Variation is to build a Knowledge Repository of entities that contains vast amount of world knowledge of entities like name variations, acronyms, confusable names, spelling variations, nick names etc. We use Wikipedia to build our knowledge repository since Wikipedia is the largest encyclopedia in the world and surpasses other knowledge bases in its coverage of concepts and up-to-date content. We obtain useful information from Wikipedia by the tool named Java Wikipedia Library 2 (Zesch et al. 2008), which allows to access all information contained in Wikipedia. Cucerzan (2007) extracts the name variations of an entity by leveraging four knowledge sources in Wikipedia: “entity pages”, “disambiguation pages” “redirect pages” and “anchor text”. Entity page in Wikipedia is uniquely identified by its title – a sequence of words, with the first word always capitalized. The title of Entity Page represents an unambiguous name variation for the entity. A redirect page in Wikipedia is an aid to navigation. When a page in Wikipedia is redirected, it means that those set of pages are referring to t"
C10-1145,D07-1074,0,\N,Missing
C10-2172,W05-0613,0,0.0392846,"sohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an impli"
C10-2172,W01-1605,0,0.643567,"Missing"
C10-2172,W03-1210,0,0.0480087,"Missing"
C10-2172,N04-1020,0,0.0089935,"rns as features to recognize implicit relations between adjacent sentences in a Japanese corpus. They showed that phrasal patterns extracted from a text span pair provide useful evidence in the relation classiﬁcation. (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracies on synthetic data. (Blair-Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by reﬁning the training and classiﬁcation process using parameter optimization, topic segmentation and syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers wit"
C10-2172,P02-1047,0,0.383456,"e a part of many natural language processing systems, e.g., text summarization system, question answering system. If there are discourse connectives between textual units to explicitly mark their relations, the recognition task on these texts is deﬁned as explicit discourse relation recognition. Otherwise it is deﬁned as implicit discourse relation recognition. However, for implicit relations, there are no connectives to explicitly mark the relations, which makes the recognition task quite difﬁcult. Some of existing works attempt to perform relation recognition without hand-annotated corpora (Marcu and Echihabi, 2002), (Sporleder and Lascarides, 2008) and (Blair-Goldensohn, 2007). They use unambiguous patterns such as [Arg1, but Arg2] to create synthetic examples of implicit relations and then use [Arg1, Arg2] as an training example of an implicit relation. Another research line is to exploit various linguistically informed features under the framework of supervised models, (Pitler et al., 2009a) and (Lin et al., 2009), e.g., polarity features, semantic classes, tense, production rules of parse trees of arguments, etc. Our study on PDTB test data shows that the average f-score for the most general 4 senses"
C10-2172,P09-1077,0,0.648462,"ional features We predict implicit connectives on both training set and test set. Then we can use the predicted implicit connectives as additional features for supervised implicit relation recognition. Previous works exploited various linguistically informed features under the framework of supervised models. In this paper, we include 9 types of features in our system due to their superior performance in previous studies, e.g., polarity features, semantic classes of verbs, contextual sense, modality, inquirer tags of words, ﬁrst-last words of arguments, cross-argument word pairs, ever used in (Pitler et al., 2009a), production rules of parse trees of arguments used in (Lin et al., 2009), and intra-argument word pairs inspired by the work of (Saito et al., 2006). Here we provide the details of the 9 features, shown as follows: Verbs: Similar to the work in (Pitler et al., 2009a), the verb features consist of the number of pairs of verbs in Arg1 and Arg2 if they are from the same class based on their highest Levin verb class level (Dorr, 2001). In addition, the average length of verb phrase and the part of speech tags of main verb are also included as verb features. Context: If the immediately preceding"
C10-2172,P09-2004,0,0.281099,"Missing"
C10-2172,prasad-etal-2008-penn,0,0.418208,"syntactic parsing. (Lapata and Lascarides, 2004) dealt with temporal links between main and subordinate clauses by inferring the temporal markers linking them. They extracted clause pairs with explicit temporal markers from BLLIP corpus as training data. Another research line is to use humanannotated corpora as training data, e.g., the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (?), (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008) beneﬁts the researchers with a large discourse annotated corpora, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classiﬁcation on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classiﬁcation baseline. (Lin et al., 2009) presented an implicit discourse relation classiﬁer in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and crossargument w"
C10-2172,N06-2034,0,0.0171666,"Missing"
C10-2172,W06-1317,0,0.205213,"Missing"
C10-2172,D09-1036,0,\N,Missing
C10-2172,N03-1030,0,\N,Missing
C12-1189,W06-2911,0,0.165659,"learn the shared information from the “name"" group instead of individual instance. As shown in Figure 1, the data set M for entity linking usually has a certain number of names (e.g.“Hoffman"",“Chad Johnson"", etc.), each with some labeled instances. Then, we treat each “name” and its associated instances in M as a prediction problem of structural learning. Besides, the queried name (e.g. “AZ"" in Figure 1) with auto-labeled instances Aq is our target prediction problem, which is the problem we are aiming to solve. According to the applications of structural learning in other tasks, such as WSD (Ando, 2006), structural learning assumes that there exists a predictive structure shared by multiple related problems. In order to learn the predictive structure Θ shared by M and Aq , we need to (a) select relevant prediction problems (i.e. relevant names) from M. That is, they should share a certain predictive structure with the target problem; (b) select useful features from the feature set shown in Table 2. The relevant prediction problems may only has shared structure with target problem over certain features. In this paper, we use a set of experiments including feature split and data set M 3097 par"
C12-1189,P05-1001,0,0.150011,"e.g. feature “acronym"" is true), certain surface features effective for linking to “Communist Party of China"" may be also effective for disambiguating “NY"", and vice versa However, with the gap in other aspects between the distributions of Aq and M shown in Section 1, directly adding M to our training set will produce a lot of noise with respect to the queried name. Thus, instead of using all the distribution knowledge in M, we propose to only incorporate the shared knowledge with Aq from M into u estimation based on structural learning. The Structural Learning Algorithm. Structural learning (Ando and Zhang, 2005b) is a multitask learning algorithm that takes advantage of the low-dimensional predictive structure shared by multiple related problems. Let us assume that we have K prediction problems indexed by l ∈ {1, .., K}, each with n(l) instances (Xli , Yli ). Each Xli is a feature vector of dimension p. Let Θ be an orthonormal h×p (h is a parameter) matrix, that captures the predictive structure shared by all the 3096 K problems. Then, we decompose the weight vector ul for problem l into two parts: one part that models the distribution knowledge specific to each problem l and one part that models th"
C12-1189,S07-1012,0,0.0166509,"do not refer the company. Amigo et al. (2010) concluded that it was not viable to train separate system for each of the companies, as the system must immediately react to any imaginable company name. Thus, in this benchmark, the set of company names in the training and test corpora are different. However, the lazy learning approach proposed in this paper demonstrates that it is feasible to train separate system for each company, and the system can immediately react to any company name without manually labeling new corpora. More generally, resolving ambiguous names in Web People Search (WePS) (Artiles et al., 2007) and Cross-document Coreference (Bagga and Baldwin, 1998) disambiguates names by clustering the articles according to the entity mentioned. This differs significantly from entity linking, which has a given entity list (i.e. the KB) to which we disambiguate the mentions. 3 Candidate Generation Because the knowledge base usually contains millions of entries, it is time-consuming to apply the disambiguation algorithm to the entire knowledge base. Thus, the following pre-processing process is conducted to filter out irrelevant KB entries and select only a set of candidates that are potentially the"
C12-1189,P98-1012,0,0.143468,"d that it was not viable to train separate system for each of the companies, as the system must immediately react to any imaginable company name. Thus, in this benchmark, the set of company names in the training and test corpora are different. However, the lazy learning approach proposed in this paper demonstrates that it is feasible to train separate system for each company, and the system can immediately react to any company name without manually labeling new corpora. More generally, resolving ambiguous names in Web People Search (WePS) (Artiles et al., 2007) and Cross-document Coreference (Bagga and Baldwin, 1998) disambiguates names by clustering the articles according to the entity mentioned. This differs significantly from entity linking, which has a given entity list (i.e. the KB) to which we disambiguate the mentions. 3 Candidate Generation Because the knowledge base usually contains millions of entries, it is time-consuming to apply the disambiguation algorithm to the entire knowledge base. Thus, the following pre-processing process is conducted to filter out irrelevant KB entries and select only a set of candidates that are potentially the correct match to the given query (a query consists of a"
C12-1189,E06-1002,0,0.0256635,"we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used Wikipedia collection and news stories as the development data, and all of the three Wikification work (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011) generalized their ranker on the data generated from Wikipedia without considering the knowledge of the queried name, th"
C12-1189,D11-1071,0,0.0121384,"retrieve the possible KB entries 3091 for a given mention. Section 4 presents our lazy learning for entity linking with query-specific information. Section 5 discusses a special case - NIL mentions . The experiments are shown in Section 6. Section 7 concludes the paper. 2 Related Work As we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used W"
C12-1189,D07-1074,0,0.0346276,"of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; Han and Sun, 2011) fall into the traditional entity linking framework shown in Figure 1. Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts. This differs from our method where we use the selective knowledge from unlabeled data. In some other work, entity linking is also called named entity disambiguation using Wikipedia (Bunescu and Pasca, 2006; Cucerzan, 2007) or Wikification (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011). These two similar tasks link expressions in text to their referent Wikipedia pages. However, since Bunescu and Pasca (2006) used Wikipedia hyperlinks to train the SVM kernel, Cucerzan (2007) used Wikipedia collection and news stories as the development data, and all of the three Wikification work (Mihalcea and Csomai, 2007; Milne and Witten, 2008; Ratinov et al., 2011) generalized their ranker on the data generated from Wikipedia without considering the knowledge of the queried name, they also fall into"
C12-1189,C10-1032,0,0.0754017,"s task are name variation and name ambiguity. Name variation refers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible t"
C12-1189,D11-1074,0,0.0677527,"acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontotext.com/kim 4 http://nlp.cs.qc.cuny.edu/kbp/2011/ 2 3"
C12-1189,P11-1095,0,0.0684377,"entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontotext.com/kim 4 http://nlp.cs.qc.cuny.edu/kbp/2011/ 2 3 3090 data set for e"
C12-1189,P11-1115,0,0.046208,"ta sets of queried and other names by mining their shared predictive structure. Keywords: Entity Linking, Lazy Learning, Query-Specific Information. Proceedings of COLING 2012: Technical Papers, pages 3089–3104, COLING 2012, Mumbai, December 2012. 3089 1 Introduction Recently, more and more knowledge bases (KB) which contain rich knowledge about the world’s entities such as Wikipedia 1 , OpenCyc 2 and KIM 3 (Popov et al., 2004) have become available. These knowledge bases have been shown to form a valuable component for many natural language processing tasks such as knowledge base population (Ji and Grishman, 2011), text classification (Wang and Domeniconi, 2008), and cross-document coreference (Finin et al., 2009). However, to be able to utilize or enrich these KB resources, the applications usually require linking the mentions of entities in text to their corresponding entries in the knowledge bases, which is called entity linking task and has been proposed and studied in Text Analysis Conference (TAC) since 2009 (McNamee and Dang, 2009). Given a mention of an entity in text and a KB, entity linking is to link the mention to its corresponding entry in KB. The major challenges of this task are name var"
C12-1189,P11-3004,0,0.045958,"t more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opency"
C12-1189,P11-1138,0,0.0581951,"ne name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http://www.opencyc.org/ http://www.ontot"
C12-1189,C10-1145,1,0.940304,"fers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www.wikipedia.org/ http"
C12-1189,N10-1072,0,0.167552,"y. Name variation refers to the case that more than one name variation such as alias, misspelling and acronym refers to the same entity. For example, both “48 th State” and “The Grand Canyon State” refer to state of Arizona, U.S.. Name ambiguity refers to the case that more than one entity shares the same name. For example, “AZ” may refer to state of Arizona, the Italian airline Alitalia, the country Azerbaijan, or other entries in KB that have the same name. Most previous studies on entity linking used annotated data to learn a classifier or ranker (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Ratinov et al., 2011) or to estimate parameters (Gottipati and Jiang, 2011; Han and Sun, 2011). Besides, from the analysis by Ji et al. (2011), all of the top systems from the participants in the shared task of TAC-114 used supervised learning approaches to solve this disambiguation problem. Figure 1: The System Architecture for Traditional Approaches. (M contains a certain number of names. “Hoffman” and “Chad Johnson” are two examples of them.) However, as there are infinite number of entity names, it is impossible to manually create the labeled 1 http://www"
C12-1189,C98-1012,0,\N,Missing
D10-1085,C10-1022,1,\N,Missing
D10-1085,W02-1008,0,\N,Missing
D10-1085,N06-2015,0,\N,Missing
D10-1085,J08-3002,1,\N,Missing
D10-1085,P03-1054,0,\N,Missing
D10-1085,P04-1043,0,\N,Missing
D10-1085,P06-1006,1,\N,Missing
D10-1085,P02-1034,0,\N,Missing
D10-1085,P04-1018,0,\N,Missing
D10-1085,I05-1063,1,\N,Missing
D10-1085,P07-1103,0,\N,Missing
D10-1085,P02-1014,0,\N,Missing
D10-1085,J01-4004,0,\N,Missing
D10-1085,P02-1011,0,\N,Missing
D10-1085,E06-1015,0,\N,Missing
D10-1085,P03-1023,1,\N,Missing
D13-1002,S07-1025,0,0.587881,"and show that these discourse features are superior. While we are just focusing on E-E temporal classification, our work can complement other approaches such as the joint inference approach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal r"
D13-1002,I11-1012,1,0.868561,"Missing"
D13-1002,W04-3205,0,0.0658355,"4) relative position of events in article, (1) (2) (3) (4) (5) System D O 2012 BASE BASE + R ST + P DTB + T OPIC S EG BASE + R ST + P DTB + T OPIC S EG + C OREF BASE + O-R ST + P DTB + O-T OPIC S EG + O-C OREF Precision 43.86 59.55 71.89 75.23 78.35 Recall 52.65 38.14 41.99 43.58 54.24 F1 47.46 46.50 53.01 55.19 64.10 Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive row is statistically significant, but a comparison is not possible between rows (1) and (2). 5) the number of sentences between the target events and 6) VerbOcean (Chklovski and Pantel, 2004) relations between events. This baseline system, and the subsequent systems we will describe, comprises of three separate one-vs-all classifiers for each of the temporal classes. The result obtained by our baseline is shown in Row 2 (i.e. BASE) in Table 2. We note that our baseline is competitive and performs similarly to the results obtained by Do et al. (2012). However as we do not have the raw judgements from Do’s system, we cannot test for statistical significance. We also implemented our proposed features and show the results obtained in the remaining rows of Table 2. In Row 3, R ST denot"
D13-1002,D12-1062,0,0.350012,"s happened before reports from the officials. Being able to infer these temporal relationships allows us to build up a better understanding of the text in question, and can aid several natural language understanding tasks such as information extraction and text summarization. For example, we can build up a temporal characterization of an article by constructing a temporal graph denoting the relationships between all events within an article (Verhagen et al., 2009). This can then be used to help construct an event timeline which layouts sequentially event mentions in the order they take place (Do et al., 2012). The temporal graph can also be used in text summarization, where temporal order can be used to improve sentence ordering and thereby the eventual generated summary (Barzilay et al., 2002). Given the importance and value of temporal relations, the community has organized shared tasks 1 From article AFP ENG 20030304.0250 of the ACE 2005 corpus (ACE, 2005). 12 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 12–23, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics to spur research efforts in this area, inc"
D13-1002,N13-1112,0,0.176803,"Missing"
D13-1002,P12-1007,0,0.02672,"eneralized. Convolution kernels had also previously been shown to work well for the related problem of E-T temporal classification (Ng and Kan, 2012), where the features adopted are similarly structural in nature. We now describe our use of the discourse analysis frameworks to generate appropriate representations for input to the convolution kernel. RST Discourse Framework. Recall that the RST framework provides us with a discourse tree for an entire input article. In recent years several automatic RST discourse parsers have been made available. In our work, we first make use of the parser by Feng and Hirst (2012) to obtain a discourse tree representation of our input. To represent the meaningful portion of the resultant tree, we encode path information between the two sentences of interest. We illustrate this procedure using the example discourse tree illustrated in Figure 3. EDUs including EDU 1 to EDU 3 form the vertices while discourse relations r1 and r2 between the EDUs form the edges. For a E-E pair, {A, B}, we can obtain a feature structure by first locating the EDUs within which A and B are found. A is found inside EDU 1 and B is found within EDU 3. We trace the shortest path between EDU 1 and"
D13-1002,S10-1076,0,0.0175359,"ments to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance are obtained. However this gain is attributed to the joint inference model they had developed, making use of similar surface features. To the best of our knowledge,"
D13-1002,P94-1002,0,0.212361,"tence pairs. This is a problem when we need to do an article-wide analysis. The RST framework does not suffer from this limitation however as we can build up a discourse 15 tree connecting all the text within a given article. Topical Text Segmentation. A third complementary type of inter-sentential analysis is topical text segmentation. This form of segmentation separates a piece of text into non-overlapping segments, each of which can span several sentences. Each segment represents passages or topics, and provides a coarsegrained study of the linear structure of the text (Skorochod’Ko, 1972; Hearst, 1994). The transition between segments can represent possible topic shifts which can provide useful information about temporal relationships. Referring to Example 32 , we have delimited the different lines of text into segments with parentheses along with a subscript. Segment (1) talks about the casualty numbers seen at a medical centre, while Segment (2) provides background information that informs us a bomb explosion had taken place. The segment boundary signals to us a possible temporal shift and can help us to infer that the bombing event took place BEFORE the deaths and injuries had occurred."
D13-1002,D11-1026,0,0.0175528,"eans...”, and its discourse structure is shown in the bottom half of Figure 9. As with the previous example, the fragment suggests (correctly) that there should be a OVERLAP relationship for the “requested – said” event pair. [A] Milosevic and his wife wielded enormous power in Yugoslavia for more than a decade before he was swept out of power after a popular revolt in October 2000. [B] The court order was requested by Jack Welch’s attorney, Daniel K. Webb, who said Welch would likely be asked about his business dealings, his health and entries in his personal diary. (4) favoured over recall (Kazantseva and Szpakowicz, 2011, p. 292). As such there is just an average of between two to three identified segments per article. This makes the feature more generalizable despite making use of actual segment numbers. 2. The style of writing in newswire articles which we are experimenting on generally follows common journalistic guidelines. The semantics behind the transitions across the coarse-grained segments that were identified are thus likely to be of a similar nature across many different articles. temporal Milosevic … wielded… a decade temporal before.. swept out.. power after a… October 2000. manner-means The cour"
D13-1002,S10-1077,0,0.01603,"ach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance ar"
D13-1002,P06-1095,0,0.0720035,"ss based on sentence gap. Experiments. The work done in Do et al. (2012) is highly related to our experiments, and so we have reported the relevant results for local E-E classification in Row 1 of Table 2 as a reference. While largely comparable, note that a direct comparison is not possible because 1) the number of E-E instances we have is slightly different from what was reported, and 2) we do not have access to the exact partitions they have created for 5-fold cross-validation. As such, we have implemented a baseline adopting similar surface lexico-syntactic features used in previous work (Mani et al., 2006; Bethard and Martin, 2007; Ng and Kan, 2012; Do et al., 2012), including 1) part-of-speech tags, 2) tenses, 3) dependency parses, 4) relative position of events in article, (1) (2) (3) (4) (5) System D O 2012 BASE BASE + R ST + P DTB + T OPIC S EG BASE + R ST + P DTB + T OPIC S EG + C OREF BASE + O-R ST + P DTB + O-T OPIC S EG + O-C OREF Precision 43.86 59.55 71.89 75.23 78.35 Recall 52.65 38.14 41.99 43.58 54.24 F1 47.46 46.50 53.01 55.19 64.10 Table 2: Macro-averaged results obtained from our experiments. The difference in F1 scores between each successive row is statistically significant,"
D13-1002,W97-0713,0,0.112293,"Missing"
D13-1002,C12-1129,1,0.906833,"ropose the use of support vector machines (SVM), adopting a convolution kernel (Collins and Duffy, 2001) for its kernel function (Vapnik, 1999; Moschitti, 2006). The use of convolution kernels allows us to do away with the extensive feature engineering typically required to generate flat vectorized representations of features. This process is time consuming and demands specialized knowledge to achieve representations that are discriminative, yet are sufficiently generalized. Convolution kernels had also previously been shown to work well for the related problem of E-T temporal classification (Ng and Kan, 2012), where the features adopted are similarly structural in nature. We now describe our use of the discourse analysis frameworks to generate appropriate representations for input to the convolution kernel. RST Discourse Framework. Recall that the RST framework provides us with a discourse tree for an entire input article. In recent years several automatic RST discourse parsers have been made available. In our work, we first make use of the parser by Feng and Hirst (2012) to obtain a discourse tree representation of our input. To represent the meaningful portion of the resultant tree, we encode pa"
D13-1002,W10-2926,0,0.0157175,"Table 1. However we see that many of the relations do not follow this distribution. For example, we observe that several relations such as the RST “Condition” and PDTB “Cause” relations are almost exclusively found within AFTER and BEFORE event pairs only, while the RST “Manner-means” and PDTB “Synchrony” relations occur in a disproportionately large number of OVERLAP event pairs. These relations are likely useful in disambiguating between the different temporal classes. To verify this, we examine the convolution tree fragments that lie on the support vector of our SVM classifier. The work of Pighin and Moschitti (2010) 19 in linearizing kernel functions allows us to take a look at these tree fragments. Applying the linearization process leads to a different classifier from the one we have used. The identified tree fragments are therefore just an approximation to those actually employed by our classifier. However, this analysis still offers an introspection as to what relations are most influential for classification. B1 B2 B3 B4 B5 BEFORE (Temporal ... (Temporal (Elaboration ... (Condition (Explanation ... (Condition (Attribution ... (Elaboration (Bckgrnd ... O1 OVERLAP (Manner-means ... Table 4: Subset of"
D13-1002,prasad-etal-2008-penn,0,0.0380832,"they do not explain how sentences are combined together, and thus are unable to properly differentiate between the different temporal classifications. Supporting our argument is the work of Smith (2010), where she argued that syntax cannot fully account for the underlying semantics beneath surface text. D’Souza and Ng (2013) found out as much, and showed that adopting richer linguistic features such as lexical relations from curated dictionaries (e.g. Webster and WordNet) as well as discourse relations help temporal classification. They had shown that the Penn Discourse TreeBank (PDTB) style (Prasad et al., 2008) discourse relations are useful. We expand on their study to assess the utility of adopting additional discourse frameworks as alternative and complementary views. 3 Making Use of Discourse To highlight the deficiencies of surface features, we quote here an example from Lascarides and Asher (1993): [A] Max opened the door. The room was pitch dark. [B] Max switched off the light. The room was pitch dark. (2) The two lines of text A and B in Example 2 have similar syntactic structure. Given only syntactic features, we may be drawn to conclude that they share similar temporal relationships. Howev"
D13-1002,S10-1062,0,0.0190375,"tems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CRF) based learner making use of similar features. Other researchers including Uzzaman and Allen (2010) and Ha et al. (2010) made use of Markov Logic Networks (MLN). By leveraging on the transitivity properties of temporal relationships (Setzer et al., 2003), they found that MLNs are useful in inferring new temporal relationships from known ones. Recognizing that the temporal relationships between event pairs and time expressions are related, Yoshikawa et al. (2009) proposed the use of a joint inference model and showed that improvements in performance are obtained. However this gain is attributed to the joint inference model they had developed, making use of similar surface features. To the be"
D13-1002,P09-1046,0,0.0211269,"n. That is, we want to be able to determine the temporal relationship between two events located anywhere within an article. The main contribution of our work is going beyond the surface lexical and syntactic features commonly adopted by existing state-of-the-art approaches. We suggest making use of semantically 13 motivated features derived from discourse analysis instead, and show that these discourse features are superior. While we are just focusing on E-E temporal classification, our work can complement other approaches such as the joint inference approach proposed by Do et al. (2012) and Yoshikawa et al. (2009) which builds on top of event-timex (E-T) and E-E temporal classification systems. We believe that improvements to the underlying E-T and E-E classification systems will help with global inference. 2 Related Work Many researchers have worked on the E-E temporal classification problem, especially as part of the TempEval series of evaluation workshops. Bethard and Martin (2007) presented one of the earliest supervised machine learning systems, making use of support vector machines (SVM) with a variety of lexical and syntactic features. Kolya et al. (2010) described a conditional random field (CR"
D13-1002,S10-1010,0,\N,Missing
D18-1381,P14-2009,0,0.0668333,"Missing"
D18-1381,N15-1184,0,0.0249048,"utions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong et al., 2014; Tang et al., 2016; Tai et al., 2015; Ren et al., 2016; Zhang et al., 2016; Teng and Zhang, 2016). This ranges from learning sentiment-specific word embeddings (Tang et al., 2014b; Faruqui et al., 2015) to end-to-end neural architectures (Teng et al., 2016; Angelidis and Lapata, 2017). The winning solution of SemEval 2016 (Deriu et al., 2016) utilized ensembles of convolutional neural networks (CNN). Recurrent-based models such as the bidirectional long short-term memory (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013) are popular and standard strong baselines for many opinion mining tasks including sentiment analysis (Tay et al., 2017) and sarcasm detection (Tay et al., 2018c). These neural models such as the BiLSTM are capable of modeling semantic compositionality and produ"
D18-1381,W11-0705,0,0.0122895,"der), a new attention-based neural architecture that exploits sentiment lexicons for learning to compose an auxiliary sentence embedding. Our model achieves state-of-the-art performance on several benchmark datasets. Finally, our AGLR, a single neural model, also achieves competitive performance with respect to top teams in SemEval runs which are mostly comprised of extensively engineered ensembles. 2 Related Work Sentiment lexicons have a rich traditional in sentiment analysis research and have been exploited in many statistical methods across the years (Hu and Liu, 2004; Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Tang et al., 2014b,a; Teng et al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural mode"
D18-1381,C04-1200,0,0.703576,"ing model performance. To this end, our model employs two distinctly unique components, i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments, we show that our approach outperforms many other neural baselines on sentiment classification tasks on multiple benchmark datasets. 1 Introduction Across the rich history of sentiment analysis research (Kim and Hovy, 2004; Liu, 2012; Pang et al., 2008), sentiment lexicons have been extensively used as features for sentiment classification tasks. Lexicons, either handcrafted or algorithmically generated, consist of words and their associated polarity score. For instance, lexicons assign a high positive score for the word ‘excellent’ but a negative score for the word ‘terrible’. Traditionally, the summation of lexicon scores has been treated as a reasonable heuristic estimate (or feature) that is capable of supporting opinion mining applications. Throughout the years, plenty of lexicon lists have been built for"
D18-1381,D14-1181,0,0.0150385,"Missing"
D18-1381,D15-1168,0,0.0588086,"Missing"
D18-1381,D15-1166,0,0.0565765,"et al., 2017) and sarcasm detection (Tay et al., 2018c). These neural models such as the BiLSTM are capable of modeling semantic compositionality and produce a feature vector which can be used for classification. To integrate the information of lexicon inside Lexicon RNN model, Teng et al. (2016) proposed to use the hidden representations from a BiLSTM to influence the lexicon score, i.e., learning context-sensitive lexicon features. However, our method can be considered as a vastly different paradigm and instead learns a d-dimensional embedding using neural attention (Bahdanau et al., 2014; Luong et al., 2015) instead of a lexicon score. The key idea of neural attention is that it allows neural networks to look (or attend) to certain words in a sequence. This concept has indeed profoundly impacted the fields of NLP, giving rise to many variant architectures including end-to-end memory networks (Sukhbaatar et al., 2015; Li et al., 2017). Our approach draws inspiration from memory networks and co-attentive models for machine comprehension (Xiong et al., 2016; Seo et al., 2016). In fact, the auxiliary network can be interpreted as a form of multi-layered attention which draws connection to vanilla mem"
D18-1381,D13-1170,0,0.0191662,"hammad et al., 2013; Tang et al., 2014b,a; Teng et al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong et al., 2014; Tang et al., 2016; Tai et al., 2015; Ren et al., 2016; Zhang et al., 2016; Teng and Zhang, 2016). This ranges from learning sentiment-specific word embeddings (Tang et al., 2014b; Faruqui et al., 2015) to end-to-end neural architectures (Teng et al., 2016; Angelidis and Lapata, 2017). The winning solution of SemEval 2016 (Deriu et al., 2016) utilized ensembles of convolutional neural networks (CNN). Recurrent-based models such as the bidirectional long short-term memory (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013) are popular and standard str"
D18-1381,S14-2111,0,0.0716835,"top teams in SemEval runs which are mostly comprised of extensively engineered ensembles. 2 Related Work Sentiment lexicons have a rich traditional in sentiment analysis research and have been exploited in many statistical methods across the years (Hu and Liu, 2004; Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Tang et al., 2014b,a; Teng et al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong et al., 2014; Tang et al., 2016; Tai et al., 2015; Ren et al., 2016; Zhang et al., 2016; Teng and Zhang, 2016). This ranges from learning sentiment-specific word embeddings (Tang et al., 2014b; Faruqui et al., 2015) to end-to-end neural architectures (Teng et al., 2016;"
D18-1381,S13-2053,0,0.605356,"tensively used as features for sentiment classification tasks. Lexicons, either handcrafted or algorithmically generated, consist of words and their associated polarity score. For instance, lexicons assign a high positive score for the word ‘excellent’ but a negative score for the word ‘terrible’. Traditionally, the summation of lexicon scores has been treated as a reasonable heuristic estimate (or feature) that is capable of supporting opinion mining applications. Throughout the years, plenty of lexicon lists have been built for various specific domains or general purposes (Hu and Liu, 2004; Mohammad et al., 2013; Wilson et al., 2005). They are indeed valuable resources that should be exploited. ∗ Denotes equal contribution. However, sentiment lexicons are in reality hardly useful without context. After all, the complexity and ambiguity of natural language pose great challenges for the crude bag-of-words generalization of lexicons. Firstly, the concept of semantic compositionality is non-existent in simple lexicon approaches which raises problems when handling flipping negation (not happy), content word negation (ameliorates pain) or unbounded dependencies (no body passed the exam). Secondly, lexicons"
D18-1381,P15-1150,0,0.182516,"ies, e.g., the lexicon polarity score of ‘Thanks for making this uncomfortable situation more comfortable’ becomes negative because uncomfortable has a higher negative lexicon score over the positive score of the word comfortable. Lastly, strongly positive or negative words may occur in neutral context which forces an inclination of predictions towards a nonneutral polarity. As such, the exploitation of readily available lexicon lists is an inherently challenging task. Deep learning has demonstrated incredibly competitive performance in many NLP tasks (Liu et al., 2015; Bradbury et al., 2016; Tai et al., 2015). With no exception, the task of sentiment analysis is recently also dominated by neural architectures. It has been proven from the fact that the top systems from SemEval Sentiment analysis challenges (e.g., notably 2016 and 2017) have mainly leveraged the effectiveness of deep learning models. The main advantage of deep learning approach is that it is effective in exploring both linguistic and semantic relations between words, thus can overcomes the problems of lexicon-based approach. However, current deep learning approach 3443 Proceedings of the 2018 Conference on Empirical Methods in Natur"
D18-1381,S13-2052,0,0.0295352,"output of the softmax layer and R = λ kψk22 is the L2 regularization. 4 • BiLSTM (Bidirectional Long Short-Term Memory) is a standard strong neural baseline for many NLP tasks. The size of the LSTM is set to 150. Empirical Evaluation This section describes our empirical experiments. 4.1 • AT-BiLSTM (Attention-based BiLSTM) is an extension of the BiLSTM model with neural attention. Evaluation Procedure In this section, we describe the datasets used, evaluation metric and implementation details. Datasets We conduct our experiments2 on subsets of sentiment analysis benchmarks from SemEval 2013 (Nakov et al., 2013), SemEval 2014 (Rosenthal et al., 2014) and SemEval 2016 (Nakov et al., 2016). More specifically, we focus on the sentence level of sentiment analysis and evaluate on the datasets of SemEval 2013 task 2, SemEval 2014 task 9 and SemEval 2016 task 4, which we will name as SemEval13, SemEval14 and SemEval16 respectively in this section. For fair comparison, we use the same setting of training, development and testing as in SemEval competitions. To further evaluate the performance of methods when data is limited, for SemEval16, we experiment on two different training settings. The first, TRAIN, us"
D18-1381,C16-1311,0,0.0145291,"al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong et al., 2014; Tang et al., 2016; Tai et al., 2015; Ren et al., 2016; Zhang et al., 2016; Teng and Zhang, 2016). This ranges from learning sentiment-specific word embeddings (Tang et al., 2014b; Faruqui et al., 2015) to end-to-end neural architectures (Teng et al., 2016; Angelidis and Lapata, 2017). The winning solution of SemEval 2016 (Deriu et al., 2016) utilized ensembles of convolutional neural networks (CNN). Recurrent-based models such as the bidirectional long short-term memory (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013) are popular and standard strong baselines for many opinion mining tasks inclu"
D18-1381,C14-1018,0,0.152718,"re that exploits sentiment lexicons for learning to compose an auxiliary sentence embedding. Our model achieves state-of-the-art performance on several benchmark datasets. Finally, our AGLR, a single neural model, also achieves competitive performance with respect to top teams in SemEval runs which are mostly comprised of extensively engineered ensembles. 2 Related Work Sentiment lexicons have a rich traditional in sentiment analysis research and have been exploited in many statistical methods across the years (Hu and Liu, 2004; Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Tang et al., 2014b,a; Teng et al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong e"
D18-1381,D14-1162,0,0.0837017,"Missing"
D18-1381,P14-1146,0,0.0338022,"re that exploits sentiment lexicons for learning to compose an auxiliary sentence embedding. Our model achieves state-of-the-art performance on several benchmark datasets. Finally, our AGLR, a single neural model, also achieves competitive performance with respect to top teams in SemEval runs which are mostly comprised of extensively engineered ensembles. 2 Related Work Sentiment lexicons have a rich traditional in sentiment analysis research and have been exploited in many statistical methods across the years (Hu and Liu, 2004; Kim and Hovy, 2004; Agarwal et al., 2011; Mohammad et al., 2013; Tang et al., 2014b,a; Teng et al., 2016). It is easy to see how sentiment lexicons are able to benefit opinion mining applications. More specifically, sentiment lexicons form an integral role in the winning solutions of SemEval 2013 (Mohammad et al., 2013) and 2014 (Miura et al., 2014). In many of these these approaches, standard machine learning classifiers (such as Support Vector Machines) are trained on discrete features partly derived from resources such as sentiment lexicon. In recent years, we see a shift of the state-of-theart from discrete models to neural models (Socher et al., 2013; Kim, 2014; Dong e"
D18-1381,D13-1066,0,0.0715594,"Missing"
D18-1381,S14-2009,0,0.0607306,"Missing"
D18-1381,P18-1093,1,0.882428,"Missing"
D18-1381,D16-1169,0,0.111653,"2016) utilized ensembles of convolutional neural networks (CNN). Recurrent-based models such as the bidirectional long short-term memory (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013) are popular and standard strong baselines for many opinion mining tasks including sentiment analysis (Tay et al., 2017) and sarcasm detection (Tay et al., 2018c). These neural models such as the BiLSTM are capable of modeling semantic compositionality and produce a feature vector which can be used for classification. To integrate the information of lexicon inside Lexicon RNN model, Teng et al. (2016) proposed to use the hidden representations from a BiLSTM to influence the lexicon score, i.e., learning context-sensitive lexicon features. However, our method can be considered as a vastly different paradigm and instead learns a d-dimensional embedding using neural attention (Bahdanau et al., 2014; Luong et al., 2015) instead of a lexicon score. The key idea of neural attention is that it allows neural networks to look (or attend) to certain words in a sequence. This concept has indeed profoundly impacted the fields of NLP, giving rise to many variant architectures including end-to-end memor"
D18-1381,H05-1044,0,0.136323,"res for sentiment classification tasks. Lexicons, either handcrafted or algorithmically generated, consist of words and their associated polarity score. For instance, lexicons assign a high positive score for the word ‘excellent’ but a negative score for the word ‘terrible’. Traditionally, the summation of lexicon scores has been treated as a reasonable heuristic estimate (or feature) that is capable of supporting opinion mining applications. Throughout the years, plenty of lexicon lists have been built for various specific domains or general purposes (Hu and Liu, 2004; Mohammad et al., 2013; Wilson et al., 2005). They are indeed valuable resources that should be exploited. ∗ Denotes equal contribution. However, sentiment lexicons are in reality hardly useful without context. After all, the complexity and ambiguity of natural language pose great challenges for the crude bag-of-words generalization of lexicons. Firstly, the concept of semantic compositionality is non-existent in simple lexicon approaches which raises problems when handling flipping negation (not happy), content word negation (ameliorates pain) or unbounded dependencies (no body passed the exam). Secondly, lexicons also do not handle wo"
I05-1034,A00-2030,0,0.0584531,"stness in handling large-scale or new domain data due to two reasons. First, rules have to be rewritten for different tasks or when porting to different domains. Second, generating rules manually is quite labor- and time-consuming. 1 GPE is an acronym introduced by the ACE (2004) program to represent a Geo-Political Entity --- an entity with land and a government. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 378 – 389, 2005. © Springer-Verlag Berlin Heidelberg 2005 Discovering Relations Between Named Entities from a Large Raw Corpus 379 Since then, various supervised learning approaches [2,3,4,5] have been explored extensively in relation extraction. These approaches automatically learn relation patterns or models from a large annotated corpus. To decrease the corpus annotation requirement, some researchers turned to weakly supervised learning approaches [6,7], which rely on a small set of initial seeds instead of a large annotated corpus. However, there is no systematic way in selecting initial seeds and deciding an “optimal” number of them. Alternatively, Hasegawa et al. [8] proposed a cosine similarity-based unsupervised learning approach for extracting relations from a large raw c"
I05-1034,P03-1005,0,0.0243155,"Missing"
I05-1034,P04-1043,0,0.255007,"Missing"
I05-1034,sekine-etal-2002-extended,0,0.0148849,"ht, NP], [sold, NP, yesterday]) = 1 + K ( bought, sold ) + K (NP, NP) = 1+0.25+0.25+K c ([a, red, car], [the, flat]) = 1.5 + K (a, the) + K (car, flat ) =2 The above similarity score is more than one. This is because we did not normalize the score using Formula (6). 2.3 Tree Similarity Based Unsupervised Learning Our method consists of five steps: 1) Named Entity (NE) tagging and sentence parsing: Detailed and accurate NE types provide more effective information for relation discovery. Here we use Sekine’s NE tagger [20], where 150 hierarchical types and subtypes of Named Entities are defined [21]. This NE tagger has also been adopted by Hasegawa et al. [8]. Besides, Collin’s parser [18] is adopted to generate shallow parse trees. 2) Similarity calculation: The similarity between two relation instances is defined between two parse trees. However, the state-of-the-art of parser is always error-prone. Therefore, we only use the minimum span parse tree including the NE pairs when calculating the similarity function [4]. Please note that the two entities may not be the leftmost or rightmost node in the sub-tree. 3) NE pairs clustering: Clustering of NE pairs is based on the similarity scor"
I05-1034,J03-4003,0,\N,Missing
I05-1034,P04-1054,0,\N,Missing
I05-1034,P04-1053,0,\N,Missing
I05-1034,P02-1034,0,\N,Missing
I05-1034,P04-1016,0,\N,Missing
I05-1053,W03-1501,0,0.402766,"l at the phrase level. We also present a twostep search to decode the best result from the models. Our proposed model is evaluated on the LDC Chinese-English NE translation corpus. The experiment results show that our proposed model is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and"
I05-1053,C02-1099,0,0.106662,"experiment results show that our proposed model is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to"
I05-1053,P04-1021,1,0.870234,"del is high effective for NE translation. 1 Introduction A Named Entity (NE) is essentially a proper noun phrase. Automatic NE translation is an indispensable component of cross-lingual applications such as machine translation and cross-lingual information retrieval and extraction. NE is translated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE t"
I05-1053,P02-1051,0,0.181878,"ated by a combination of meaning translation and/or phoneme transliteration [1]. NE transliteration has been given much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT. Its challenges lie in not only the ambiguity in lexical mapping such as <副(Fu),Deputy> and <副(Fu),Vice> in Fig.1 in the next page, but also the pos"
I05-1053,N04-1036,0,0.0681695,"much attention in the literature. Many attempts, including phoneme and grapheme-based methods, various machine learning and rule-based algorithms [2,3] and Joint Source-Channel Model (JSCM) [4], have been made recently to tackle the issue of NE transliteration. However, only a few works have been reported in NE translation. Chen et al. [1] proposed a frequency-based approach to learn formulation and transformation rules for multilingual Named Entities (NEs). Al-Onaizan and Knight [5] investigated the translation of Arabic NEs to English using monolingual and bilingual resources. Huang et al. [6] described an approach to translate rarely occurring NEs by combining phonetic and semantic similarities. In this paper, we pay special attention to the issue of NE translation. Although NE translation is less sophisticated than machine translation (MT) in general, to some extent, the issues in NE translation are similar to those in MT. Its challenges lie in not only the ambiguity in lexical mapping such as <副(Fu),Deputy> and <副(Fu),Vice> in Fig.1 in the next page, but also the position permutation and fertility of words. Fig.1 illustrates two excerpts of NE translation from the LDC corpus [7]"
I05-1053,2002.tmi-tutorials.2,0,0.0491198,"our study, we enhance the LMM with the PM to account for the word reordering issue in NE translation, so our model is capable of modeling the non-monotone problem. In contrast, JSCM only models the monotone problem. Both rule-based [1] and statistical model-based [5,6] methods have been proposed to address the NE translation problem. The model-based methods mostly are based on conditional probability under the noisy-channel framework [8]. Now let’s review the different modeling methods: 1) 2) 3) As far as lexical choice issue is concerned, the noisy-channel model, represented by IBM Model 1-5 [8], models lexical dependency using a context-free conditional probability. Marcu and Wong [10] proposed a phrase-based context-free joint probability model for lexical mapping. In contrast, our LMM models lexical dependency using n-order bilingual contextual information. Another characteristic of our method lies in its modeling and search strategy. NE translation and MT are usually viewed as a non-monotone search problem and it is well-known that a non-monotone search is exponentially more complex than a monotone search. Thus, we propose the two separated models and the two-step search, so that"
I05-1053,W02-1018,0,0.0349964,"nslation, so our model is capable of modeling the non-monotone problem. In contrast, JSCM only models the monotone problem. Both rule-based [1] and statistical model-based [5,6] methods have been proposed to address the NE translation problem. The model-based methods mostly are based on conditional probability under the noisy-channel framework [8]. Now let’s review the different modeling methods: 1) 2) 3) As far as lexical choice issue is concerned, the noisy-channel model, represented by IBM Model 1-5 [8], models lexical dependency using a context-free conditional probability. Marcu and Wong [10] proposed a phrase-based context-free joint probability model for lexical mapping. In contrast, our LMM models lexical dependency using n-order bilingual contextual information. Another characteristic of our method lies in its modeling and search strategy. NE translation and MT are usually viewed as a non-monotone search problem and it is well-known that a non-monotone search is exponentially more complex than a monotone search. Thus, we propose the two separated models and the two-step search, so that the lexical mapping issue can be resolved by monotone search. This results in a large improv"
I05-1053,W99-0604,0,0.114464,"Missing"
I05-1053,N03-1017,0,0.0566621,"hrase-based MT research, to carry out the same NE translation experiments as reference cases. All the experiments conducted in this paper are listed as follow: 1) 3 4 IBM method C: word-based IBM Model 4 trained by GIZA++3 [15] and ISI Decoder4 [14,16]; http://www.fjoch.com/ http://www.isi.edu/natural-language/software/decoder/manual.html A Phrase-Based Context-Dependent Joint Probability Model 2) 3) 4) 607 IBM method D: phrase-based IBM Model 4 trained by GIZA++ on phrasealigned corpus and ISI Decoder working on phrase-segmented testing corpus. Koehn method: Koehn et al.’s phrase-based model [12] and PHARAOH5 decoder6; Our method: phrase-based bi-gram LMM and bi-gram PM, and our two-step decoder. To make an accurate comparison, all the above three phrase-based models are trained on the same phrase-segmented and aligned corpus, and tested on the same phrase-segmented corpus. ISI Decoder carries out a greedy search, and PHARAOH is a beam-search stack decoder. To optimize their performances, the two decoders are allowed to do unlimited reordering without penalty. We train trigram language models in the first three experiments and bi-gram models in the forth experiment. 5.2 NE Translation"
I05-1053,P01-1030,0,0.0646095,"Missing"
I05-1053,J03-1002,0,0.00868264,"ntext-dependent joint probability model. 3 Training Following the modeling strategy discussed above, the training process consists of three steps: phrase alignment, reordering of corpus, and learning statistical parameters for lexical mapping and permutation models. 3.1 Acquiring Phrase Pairs To reduce vocabulary size and avoid sparseness, we constrain the phrase length to up to three words and the lower-frequency phrase pairs are pruned out for accurate 604 M. Zhang et al. phrase-alignment1. Given a word alignment corpus which can be obtained by means of the publicly available GIZA++ toolkit [15], it is very straightforward to construct the phrase-alignment corpus by incrementally traversing the word-aligned NE from left to right2. The set of resulting phrase pairs forms a lexical mapping table. 3.2 Reordering Corpus The context-dependent lexical mapping model assumes monotonic alignment in the bilingual training corpus. Thus, the phrase aligned corpus needs to be reordered so that it is in either source-ordered or target-ordered alignment. We choose to reorder the target phrases to follow the source order. Only in this way can we use the lexical mapping model to describe the monotoni"
I05-1053,N03-1010,0,0.0245356,"Missing"
I05-1053,J03-1005,0,0.0554023,"Missing"
I05-1053,2001.mtsummit-papers.68,0,0.0298051,"Missing"
I05-1053,W00-0508,0,0.0570397,"et us discuss the major differences between them: 610 1) 2) M. Zhang et al. Our LMM models the lexical mapping and target word selection using a context-dependent joint probability while IBM Model 1 using a contextindependent conditional probability and a target n-gram language model. Our LMM carries out the target word selection and our PM only models the target word connectivity while the language model in IBM Model 1 performs the function of target word selection. Alternatively, finite-state automata (FSA) for statistical MT were previous suggested for decoding using contextual information [21,22]. Bangalore and Riccardi [21] proposed a phrase-based variable length n-gram model followed by a reordering scheme for spoken language translation. However, their re-ordering scheme was not evaluated by empirical experiments. 7 Conclusions In this paper, we propose a new model for NE translation. We present the training and decoding methods for the proposed model. We also compare the proposed method with related work. Empirical experiments show that our method outperforms the previous methods significantly in all test cases. We conclude that our method works more effectively and efficiently in"
I05-1053,P04-1065,0,0.0254848,"et us discuss the major differences between them: 610 1) 2) M. Zhang et al. Our LMM models the lexical mapping and target word selection using a context-dependent joint probability while IBM Model 1 using a contextindependent conditional probability and a target n-gram language model. Our LMM carries out the target word selection and our PM only models the target word connectivity while the language model in IBM Model 1 performs the function of target word selection. Alternatively, finite-state automata (FSA) for statistical MT were previous suggested for decoding using contextual information [21,22]. Bangalore and Riccardi [21] proposed a phrase-based variable length n-gram model followed by a reordering scheme for spoken language translation. However, their re-ordering scheme was not evaluated by empirical experiments. 7 Conclusions In this paper, we propose a new model for NE translation. We present the training and decoding methods for the proposed model. We also compare the proposed method with related work. Empirical experiments show that our method outperforms the previous methods significantly in all test cases. We conclude that our method works more effectively and efficiently in"
I05-1053,C04-1030,0,\N,Missing
I05-1053,J93-2003,0,\N,Missing
I05-1053,P02-1040,0,\N,Missing
I05-1053,N04-1033,0,\N,Missing
I05-1053,J98-4003,0,\N,Missing
I05-1063,J01-4004,0,0.461959,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,P02-1014,0,0.307638,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,P03-1023,1,0.949019,"ning instances, which leads to a classiﬁer capable of identifying the cases of non-anaphors during resolution. In this way, the twin-candidate model itself could avoid the resolution of non-anaphors, and thus could be directly deployed to coreference resolution. The evaluation done on newswire domain shows that the twin-candidate based system with our modiﬁed framework achieves better and more reliable performance than those with other solutions. 1 Introduction In recent years supervised learning approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between ca"
I05-1063,W03-2604,0,0.0524874,"ng approaches have been widely used in coreference resolution task and achieved considerable success [1,2,3,4,5]. Most of these approaches adopt the single-candidate learning model, in which coreference relation is determined between a possible anaphor and one individual candidate at a time [1,3,4]. However, it has been claimed that the reference between an anaphor and its candidate is often subject to the other competing candidates [5]. Such information is nevertheless diﬃcult to be captured in the single-candidate model. As an alternative, several researchers proposed a twin-candidate model [2,5,6]. Instead of directly determining coreference relations, this model would judge the preference between candidates and then select the most preferred one as the antecedent. The previous work has reported that such a model can eﬀectively help antecedent determination for anaphors [5,6]. However, one problem exits with the twin-candidate model. For every encountered NP during resolution, the model would always pick out a “best” candidate as the antecedent, even if the current NP is not an anaphor. The twin-candidate R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 719–730, 2005. c Springer-Verl"
I05-1063,C02-1139,0,0.500844,"Missing"
I05-1063,M95-1005,0,0.135699,"tion and Discussion Experiment Setup The experiments were done on the newswire domain, using MUC coreference data set (Wall Street Journal articles). For MUC-6 [8] and MUC-7 [9], 30 “dryrun” documents were used for training as well as 20-30 documents for testing. In addition, another 100 annotated documents from MUC-6 corpus were also prepared for the purpose of deeper system analysis. Throughout the experiments, C5 was used as the learning algorithm [10]. The recall and precision rates of the coreference resolution systems were calculated based on the scoring scheme proposed by Vilain et al. [11]. A Twin-Candidate Model of Coreference Resolution 725 Table 2. Features for coreference resolution using the twin-candidate model Features describing the new markable M : 1 if M is a deﬁnite NP; else 0 1. M DefNP 1 if M is an indeﬁnite NP; else 0 2. M IndefNP 1 if M is a proper noun; else 0 3. M ProperNP 1 if M is a pronoun; else 0 4. M Pronoun Features describing the candidate, C1 or C2 , of M 1 if C1 (C2 ) is a deﬁnite NP; else 0 5. candi DefNp 1(2) 6. candi IndefNp 1(2) 1 if C1 (C2 ) is an indeﬁnite NP; else 0 7. candi ProperNp 1(2) 1 if C1 (C2 ) is a proper noun; else 0 8. candi Pronoun 1"
I08-1046,P07-1073,0,0.12223,"his paper, we detail the ways one can fruitfully employ relation specific sentences retrieved from the Web with a semisupervised labeling approach. Most importantly we show how the output from such an approach can be combined with existing knowledge gleaned from supervised learning to improve the performance of relation extraction significantly. 2 Related Work and Differences To our knowledge, there is no previous work that exploits the information from a large raw text corpus like the Web to improve supervised relation extraction. In the spirit of the work done by (Shinyama and Sekine, 2003; Bunescu and Mooney, 2007), we are trying to collect clusters of paraphrases for given relation mentions. Briefly, since the same relation can be expressed in many ways, the information we may learn about that relation in any single sentence is very limited. The idea then is to alleviate this bias by collecting many paraphrases of the same relation instance into clusters when we train our system. Shinyama generalizes the expressions using partof-speech information and dependency tree similarity into generic templates. Bunescu’s work uses a relation kernel on subsquences of words developed in (Bunescu and Mooney, 2005)."
I08-1046,P06-1017,0,0.0533632,"Missing"
I08-1046,J03-4003,0,0.0067741,"n that using Web based information found by our semi-supervised algorithm. We constructed our fully supervised learner according to the specifications for the system developed by (Zhou et al., 2005). It utilizes a feature based framework with a Support Vector Machine (SVM) classifier (Cortes and Vapnik, 1995). We support the same set of features as Zhou, namely: local word position features, entity type, base phrase chunking features, dependency and parse tree features as well as semantic information like country names and a list of trigger words. In our current work, we use Michael Collins’ (Collins, 2003) parser for syntactic information. Sentence boundary detection, chunking, and parsing are done as preprocessing steps before we begin our learning. Given a sentence with the relation mention instance, the semi-supervised method goes through the following five stages: 4. Cluster the sentences found using k-medoids, filter out noise and retain only the cluster that the original relation mention would be assigned to. 5. Collate all clusters by relation type and generate a skip-bigram index for each relation type. Extracting entity mentions and gathering hypernyms The process starts when we receiv"
I08-1046,doddington-etal-2004-automatic,0,0.0283751,"Missing"
I08-1046,W06-2201,0,0.0461474,"entions for the same entity. In this example, it might result in the four pairs show in Table 1. 352 Table 1: Examples of entity pairs after hypernym expansion President Sotheby’s Chief Executive Sotheby’s Decision maker Sotheby’s leader Sotheby’s Table 2: Examples of extracted text from Google The 40 year old former president travels incognitio to Sotheby’s Brooks was named president of Sotheby’s Inc. Subsidiary President at Sotheby’s... Bill Ruprecht, chief executive of Sotheby’s, agreed that September 11 had been... The Duke will also remain leader of Sotheby’s Germany... 3.1.2 Web search (Geleijnse and Korst, 2006) use Google as their search engine for extracting surface patterns from Web documents. We use the same procedure here to find our paraphrases. For each pair of arguments, we create a boolean query string em1 ∗ em2 , and submit it to Google. The query will find documents where mentions of em1 are separated from em2 by any number of words. We restrict the language to English. Google returns a two line extract from the documents that match our boolean query. The extracts are generally those lines where the key query terms are most densely collocated. These are parsed and obviously nonsensical sen"
I08-1046,W06-2200,0,0.0755951,"Missing"
I08-1046,C04-1072,0,0.222142,"subsquences of words developed in (Bunescu and Mooney, 2005). We observed that both approaches suffer from low recall despite the attempts to generalize the subsequences and templates probably because they rely on local context only. Based on our observation, we looked for a way to use our clusters without losing non-local information about the sentences. Bag-of-words or unigram representations of our paraphrase clusters are easy to compute, but information about word ordering is lost. Hence, we settled on the use of a skip-bigram representation of our relation clusters instead. Skip-bigrams (Lin and Och, 2004) are pairs of words in sentence order allowing for gaps in between. Longer gaps capture far-flung associations between words, while short gaps of between 1 and 4 capture local structure. In using them, we do not restrict ourselves to context centered around the entity mentions. Another advantage of using skip-bigrams is that we can capture some extra-sentential information since we are no longer restricted by the ability to generate dependency structures within a single sentence as Shinyama is. Using skip-bigrams, we can assess the similarity of a particular new relation mention instance again"
I08-1046,W03-1609,0,0.201736,"lable, for regular IE. In this paper, we detail the ways one can fruitfully employ relation specific sentences retrieved from the Web with a semisupervised labeling approach. Most importantly we show how the output from such an approach can be combined with existing knowledge gleaned from supervised learning to improve the performance of relation extraction significantly. 2 Related Work and Differences To our knowledge, there is no previous work that exploits the information from a large raw text corpus like the Web to improve supervised relation extraction. In the spirit of the work done by (Shinyama and Sekine, 2003; Bunescu and Mooney, 2007), we are trying to collect clusters of paraphrases for given relation mentions. Briefly, since the same relation can be expressed in many ways, the information we may learn about that relation in any single sentence is very limited. The idea then is to alleviate this bias by collecting many paraphrases of the same relation instance into clusters when we train our system. Shinyama generalizes the expressions using partof-speech information and dependency tree similarity into generic templates. Bunescu’s work uses a relation kernel on subsquences of words developed in"
I08-1046,I05-1034,1,0.903217,"Missing"
I08-1046,P05-1053,1,0.958737,"further restrict our consideration to those relationships clearly supported by evidence in the scope of the same document. The ACE’s annotators mark all mentions of relations where there is a direct syntactic connection between the entities, i.e. when one entity mention modifies another one, or when two entity mentions are arguments of the same event. Relations between entities that are implied in the text but which do not satisfy either requirement are considered to be implicit, and are marked only once. Our work sits squarely in the realm of work on regular IE done by (Zelenko et al., 2003; Zhou et al., 2005; Chen et al., 2006). Here, the corpus of interest is a well defined set of texts, such as news articles, and we have to detect and classify all appearances of relations from a set of given relation types in the documents. In line with assumptions in the related work, we assert that the differences in the markup for implicit and explicit relations does not significantly affect our performance. Supervised learning methods have proved to be some of the most effective for regular IE. They do however, need large volumes of expensively annotated examples to perform robustly. As a result, even the l"
I11-1012,P02-1011,0,0.0205233,"results with discussions. Last section will wrap up with a conclusion and future research directions. 2 Previous Work Although event coreference resolution is an important task, it has not attracted much attention. There is only a limited number of previous works related to this task. In (Asher, 1993) chapter 6, a method to resolve references to abstract entities using discourse representation theory is discussed. However, no computational system was proposed. Besides linguistic studies, there are only a few previous works attempting to tackle subproblems of the event coreference resolution. (Byron, 2002; Müller, 2007; Chen et al, 2010a) attempted event pronoun resolution. (Chen et al, 2010b) attempted resolving noun phrases to verb mentions. All these works only focused on identifying pairs of coreferent event mentions in their targeted phenomena. The ultimate goal, which is extracting event chain, is lack of attention. (Pradhan, et al, 2007) applied a conventional co-reference resolution system to OntoNotes1.0 corpus using the same set of features for object coreference resolution. However, there is no specific performance reported on event coreference. As (Chen et al, 2010b) pointed out, t"
I11-1012,C10-1017,0,0.0557971,"coreference, we would like to revisit the two-step resolution framework to understand some of its weaknesses. Most of previous coreference resolution system employs a two-steps approach as in (Soon et al, 2001; Nicolae & Nicolae, 2006) and many others. The first step identifies all the pairs of coreferent mentions. The second step forms coreference chains using the coreferent pairs identified from the first step. 1 Event roles refer to the arguments of the event such as actuator, patient, time, location and etc. 103 Although a handful of single-step frameworks were proposed recently such as (Cai & Strube, 2010), two-step framework is still widely in use because it has been well-studied. Conceptually, the two-step framework adopts a divide-andconquer strategy which in turn, allows us to focus on different sub-problems at different stages. The mention-pair detection step allows us to employ many features associated with strong linguistic intuitions which have been proven useful in the previous linguistic study. The chain formation step allows us to leverage on efficient and robust graph partitioning algorithms such spectral partitioning used in this paper. Practically, the two-step framework is also m"
I11-1012,N06-2015,0,0.014611,"Missing"
I11-1012,W02-1008,0,0.0782086,"Missing"
I11-1012,W06-1633,0,0.0301428,"solution. However, there is no specific performance reported on event coreference. As (Chen et al, 2010b) pointed out, the conventional features do not function properly on event coreference problem. Thus, a thorough investigation on event coreference phenomena is required for a better understanding of the problem. 3 Resolution Framework Before we introduce our proposed system to event coreference, we would like to revisit the two-step resolution framework to understand some of its weaknesses. Most of previous coreference resolution system employs a two-steps approach as in (Soon et al, 2001; Nicolae & Nicolae, 2006) and many others. The first step identifies all the pairs of coreferent mentions. The second step forms coreference chains using the coreferent pairs identified from the first step. 1 Event roles refer to the arguments of the event such as actuator, patient, time, location and etc. 103 Although a handful of single-step frameworks were proposed recently such as (Cai & Strube, 2010), two-step framework is still widely in use because it has been well-studied. Conceptually, the two-step framework adopts a divide-andconquer strategy which in turn, allows us to focus on different sub-problems at dif"
I11-1012,D08-1068,0,0.0762522,"Missing"
I11-1012,J01-4004,0,0.932843,"ntroduction Coreference resolution, the task of resolving and linking different mentions of the same object/event in a text, is important for an intelligent text processing system. The resolved coreferent mentions form a coreference chain representing a particular object/event. Following the natural order in the texts, any two consecutive mentions in a coreference chain form an anaphoric pair with the latter mention referring back to the prior one. The latter mention is called the anaphor while the prior one is named as the antecedent. Most of previous works on coreference resolution such as (Soon et al, 2001; Yang et al, 2006), aimed at object coreference which both the anaphor and its antecedent are mentions of the same 4 National University of Singapore 4 tancl@comp.nus.edu.sg real world object such as person, location and organization. In contrast, an event coreference as defined in (Asher, 1993) is an anaphoric reference to an event, fact, and proposition which is representative of eventuality and abstract entities. In the following example: “Israel has [fired] missiles on the offices of the Palestinian Authority. [It] has caused 7 deaths with many injuries… Israel helicopter gunships [fired]"
I11-1012,P10-2029,0,0.00998812,"which in turn, allows us to focus on different sub-problems at different stages. The mention-pair detection step allows us to employ many features associated with strong linguistic intuitions which have been proven useful in the previous linguistic study. The chain formation step allows us to leverage on efficient and robust graph partitioning algorithms such spectral partitioning used in this paper. Practically, the two-step framework is also more mature for practical uses and has been implemented as a number of standard coreference resolution toolkits widely available such as RECONCILE in (Stoyanov et al, 2010) and BART in (Versley et al, 2008). Performance-wise, two-step approaches also show comparable performance to single step approaches on some benchmark datasets2. In this paper, we are exploiting a brand new type of coreference phenomenon with merely previous attempts. Therefore, we employed the much matured two-step framework with innovative extensions to accommodate complicated event coreference phenomena. Such a divideand-conquer strategy will provide us more insight for further advancements as well. 3.1 Mention-Pair Resolution Models Most of mention-pair models adopt the wellknown machine l"
I11-1012,P08-4003,0,0.064892,"Missing"
I11-1012,P06-1006,1,0.645464,"rence resolution, the task of resolving and linking different mentions of the same object/event in a text, is important for an intelligent text processing system. The resolved coreferent mentions form a coreference chain representing a particular object/event. Following the natural order in the texts, any two consecutive mentions in a coreference chain form an anaphoric pair with the latter mention referring back to the prior one. The latter mention is called the anaphor while the prior one is named as the antecedent. Most of previous works on coreference resolution such as (Soon et al, 2001; Yang et al, 2006), aimed at object coreference which both the anaphor and its antecedent are mentions of the same 4 National University of Singapore 4 tancl@comp.nus.edu.sg real world object such as person, location and organization. In contrast, an event coreference as defined in (Asher, 1993) is an anaphoric reference to an event, fact, and proposition which is representative of eventuality and abstract entities. In the following example: “Israel has [fired] missiles on the offices of the Palestinian Authority. [It] has caused 7 deaths with many injuries… Israel helicopter gunships [fired] across the Gaza St"
I11-1012,C10-1022,1,\N,Missing
I11-1012,P07-1103,0,\N,Missing
I11-1063,P98-1012,0,0.0820438,"large collection to populate a knowledge base (KB) (e.g. Wikipedia). This requires either linking entity mentions in the documents with entries in the KB or highlighting these mentions as new entries to current KB. Entity linking (McNamee and Dang, 2009) involves both finding name variants (e.g. both “George H. W. Bush” and “George Bush Senior” refer to the 41st U.S. president) and name disambiguation (e.g. given “George Bush” and 1 http://nlp.cs.qc.cuny.edu/kbp/2010/ its context, we should be able to disambiguate which president it is referring to). Compared with Cross-Document Coreference (Bagga and Baldwin, 1998) which clusters the articles according to the entity mentioned, entity linking has a given entity list (i.e. the reference KB) to which we disambiguate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the co"
I11-1063,P99-1016,0,0.0717705,"upercategories of c, SC={sc1, sc2, …,sck }. If the head lemma of one of cpi matches the head lemma of scj, label the relation between c and scj as is-a. (2) assign is-a label to the link between two categories if a Wikipedia article is redundantly categorized under both of them. For example, “Internet” is categorized under both “Computer networks” and “Computing” and there is a link between “Computer networks” and “Computing”. Then this link is assigned is-a. Next, we use lexical-syntactic patterns in a corpus. This method uses two sets of patterns. One set is used to identify is-a relations (Caraballo, 1999; Hearst, 1992), for example “such NP1 as NP2”, NP1 and NP2 are the values of categories and their subcategories respectively. The second set is used to identify not-is-a relations. For example “NP1 has NP2”, where the link between NP1 and NP2 will be assigned not-is-a. These patterns are used with a corpus built from Wikipedia articles, and separately with the Tipster corpus (Harman and Liberman, 1993). The label is assigned by majority voting between the frequency counts for the two types of patterns. Finally, we assign is-a labels to links based on transitive closures - all categories along"
I11-1063,C10-1032,0,0.0914314,"guate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the comparison of the weighted literal term vectors (Varma et al., 2009; Li et al., 2009; McNamee et al., 2009; Zhang et al., 2010; Zheng et al., 2010; Dredze et al., 2010). Such literal matching suffers from sparseness issue. For example, consider the following four observations of Michael Jordan without term match: 1) Michael Jordan is a leading researcher in machine learning and artificial intelligence. 2) Michael Jordan is currently a full professor at the University of California, Berkeley. 3) Michael Jordan (born February, 1963) is a former American professional basketball player. 4) Michael Jordan wins NBA MVP of 91-92 season. To measure the similarity of these contexts, the semantic knowledge underlying the words is needed. Furthermore, current state-of-"
I11-1063,C92-2082,0,0.0817485,"c, SC={sc1, sc2, …,sck }. If the head lemma of one of cpi matches the head lemma of scj, label the relation between c and scj as is-a. (2) assign is-a label to the link between two categories if a Wikipedia article is redundantly categorized under both of them. For example, “Internet” is categorized under both “Computer networks” and “Computing” and there is a link between “Computer networks” and “Computing”. Then this link is assigned is-a. Next, we use lexical-syntactic patterns in a corpus. This method uses two sets of patterns. One set is used to identify is-a relations (Caraballo, 1999; Hearst, 1992), for example “such NP1 as NP2”, NP1 and NP2 are the values of categories and their subcategories respectively. The second set is used to identify not-is-a relations. For example “NP1 has NP2”, where the link between NP1 and NP2 will be assigned not-is-a. These patterns are used with a corpus built from Wikipedia articles, and separately with the Tipster corpus (Harman and Liberman, 1993). The label is assigned by majority voting between the frequency counts for the two types of patterns. Finally, we assign is-a labels to links based on transitive closures - all categories along an is-a chain"
I11-1063,D09-1026,0,0.052334,"contributors to assign categories to each article, which are defined as “major topics that are likely to be useful to someone reading the article”. Thus, Wikipedia can serve as a document collection with multiple topical labels, where we can learn the posterior distribution over words for each topical label (i.e. Wikipedia category). Then, from the observed words in the mention’s context and KB entry, we can estimate the distribution of the contexts over the Wikipedia categories. To obtain this distribution, we use a supervised Latent Dirichlet Allocation (LDA) model – labeled LDA defined by Ramage et al. (2009), which represents state-of-the-art method for multi-labeled text classification. It performs better on collections with more semantically diverse labels, which we need in order to leverage on the large semantically diverse categories from Wikipedia as the topical labels. Figure 1 shows us a graphical representation of the labeled LDA for the multi-labeled document collection. Labeled LDA is a three level hierarchical Bayesian model. β is the multinomial distribution over words for a Wikipedia category, which has a Dirichlet prior with hyperparameter η. Both the category set Λ as well as the t"
I11-1063,P04-1075,1,0.746954,"o other entries like “Bud Abbott” “Abbott Texas”, etc. Thus, we need an instance selection approach to reduce the effect of this distribution problem. However, the traditional instance selection approaches (Brighton and Mellish, 2002; Liu and Motoda, 2002) only can solve two problems: 1) a large dataset causes response-time to become slow 2) the noisy instances affect accuracy, which are different from our needs here. We thus propose an instance selection approach to select a more balanced subset from the autoannotated instances. This instance selection strategy is similar to active learning (Shen et al., 2004; Brinker, 2003) for reducing the manual annotation effort on training instances through proposing only the useful candidates to annotators. As we already have a large set of autogenerated training instances, the selection here is a fully automatic process to get a useful and more balanced subset instead. We use the SVM classifier mentioned in Section 2.2 to select the instances from the large dataset. The initial classifier can be trained on a set of initial training instances, which can be a small part of the whole auto-generated data, or the limited manual annotated training instances avail"
I11-1063,C10-1145,1,0.862013,"Missing"
I11-1063,N10-1072,0,0.0765975,"to which we disambiguate the entity mentions. Moreover, in the articles, there are new entities not present in KB. For name disambiguation in entity linking, there has been much previous work which demonstrates modeling context is an important part of measuring document similarity. However, the traditional approach for entity linking treats the context as a bag of words, n-grams, noun phrases or/and co-occurring named entities, and measures context similarity by the comparison of the weighted literal term vectors (Varma et al., 2009; Li et al., 2009; McNamee et al., 2009; Zhang et al., 2010; Zheng et al., 2010; Dredze et al., 2010). Such literal matching suffers from sparseness issue. For example, consider the following four observations of Michael Jordan without term match: 1) Michael Jordan is a leading researcher in machine learning and artificial intelligence. 2) Michael Jordan is currently a full professor at the University of California, Berkeley. 3) Michael Jordan (born February, 1963) is a former American professional basketball player. 4) Michael Jordan wins NBA MVP of 91-92 season. To measure the similarity of these contexts, the semantic knowledge underlying the words is needed. Furtherm"
I11-1063,C98-1012,0,\N,Missing
I13-1004,P98-1012,0,0.0177845,"Missing"
I13-1004,D07-1086,0,0.139347,"ultiple local transformations in the form of ‘wi wi+1 7→ wi |wi+1 ’ or ‘wi |wi+1 7→ wi wi+1 ’. ‘wi wi+1 7→ wi |wi+1 ’ means that S a does not include a segment boundary between tokens wi and wi+1 and S b does; Similarly, ‘wi |wi+1 7→ wi wi+1 ’ means the reverse. For example, for the first query in Table 1, we can have the local transformations ‘download adobe 7→ download |adobe’ and ‘adobe |writer 7→ adobe writer’. The proposed model estimates the score of every local transformation using a binary classifier and then aggregates the individual scores to reach its final decision. 2 Related Work Bergsma and Wang (2007) considered the decision to segment or not between each pair of adjacent words as a binary classification problem. Guo et al. (2008), Yu and Shi (2009), and Kiseleva et al. (2010) used methods based on CRF. As the cost of obtaining labeled data is high, they are usually not feasible to develop a set of labeled data covering all the domains on the web and then train a scalable QS model for web search. The work for web-scale QS are usually unsupervised and utilized various statistics such as mutual information (MI) and frequency count collected from various sources such as web data, query logs,"
I13-1004,P98-2186,0,0.00911503,"arg max x∈T (Sq1 7→Sqj ) f (x) as Contextual Features: POS Tag. Bergsma et al. (2007) show that part-of-speech (POS) tags are useful in their segmentation classification. We also exploit the POS tag pair of wi0 and wi0 +1 as features. For example, intuitively, “NN NN 7→ NN |NN” is more likely to occur than “JJ NN 7→ JJ |NN”. The POS tags that we consider include all types of POS tags. Note that this is different from Bergsma et al. (2007). As their segmentation model only takes care of noun phrase queries, their POS tags are restricted to determiners, adjectives, and nouns. The POS tagger by (Roth and Zelenko, 1998) is used in this paper. Mutual Information (MI). Following previous work (Section 2), we also adopt MI between wi0 and wi0 +1 as our feature. The work (Bergsma and Wang, 2007) also considered the case of a noun phrase with multiple modifiers (e.g. “female bus driver”). To make the segmentation decision between ‘female’ and ‘bus’, M I(‘female’, ‘driver’) is more suitable to represent the information of not separating them than M I(‘female’, ‘bus’). Thus, we also incorporate M I(wi0 −1 , wi0 +1 ) and M I(wi0 , wi0 +2 ) into our feature set. 1&lt;j≤n its index to replace the top-1 segmentation; Othe"
I13-1004,P09-2047,0,\N,Missing
I13-1004,C98-2181,0,\N,Missing
J08-3002,P95-1017,0,0.208093,"Missing"
J08-3002,J96-1002,0,0.0574936,"Missing"
J08-3002,W98-1119,0,0.0464981,"Missing"
J08-3002,J95-2003,0,0.0598271,"Missing"
J08-3002,W03-2604,0,0.499617,"Missing"
J08-3002,J00-4006,0,0.109052,"f the twin-candidate model for anaphora resolution. Further, we explore how to deploy the model in the more complicated coreference resolution task. We evaluate the twin-candidate model in different domains using the Automatic Content Extraction data sets. The experimental results indicate that our twin-candidate model is superior to the single-candidate model for the task of pronominal anaphora resolution. For the task of coreference resolution, it also performs equally well, or better. 1. Introduction Anaphora is reference to an entity that has been previously introduced into the discourse (Jurafsky and Martin 2000). The referring expression used is called the anaphor and the expression being referred to is its antecedent. The anaphor is usually used to refer to the same entity as the antecedent; hence, they are coreferential with each other. The process of determining the antecedent of an anaphor is called anaphora resolution. As a key problem in discourse and language understanding, anaphora resolution is crucial in many natural language applications, such as machine translation, text summarization, question answering, information extraction, and so on. In recent ∗ 21 Heng Mui Keng Terrace, Singapore,"
J08-3002,W97-0319,0,0.0860925,"r.a-star.edu.sg. ∗∗ 21 Heng Mui Keng Terrace, Singapore, 119613. E-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given a"
J08-3002,N04-1037,0,0.0285086,". . . , Cn . The singlecandidate model assumes that the probability that Ck is the antecedent is only dependent on the anaphor ana and Ck , and independent of all the other candidates. That is: p (ante(Ck ) |ana, C1 , C2 , . . . , Cn ) = p (ante(Ck ) |ana, Ck ) (1) Thus, the probability of a candidate Ck being the antecedent can be approximated using the classiﬁcation result on the instance describing the anaphor and Ck alone. The single-candidate model is widely used in most anaphora resolution systems (Aone and Bennett 1995; Ge, Hale, and Charniak 1998; Preiss 2001; Strube and Mueller 2003; Kehler et al. 2004; Ng et al. 2005). In our study, we also build as the 329 Computational Linguistics Volume 34, Number 3 Table 1 A sample text for anaphora resolution. [1 Those ﬁgures] are almost exactly what [2 the government] proposed to [3 legislators] in [4 September]. If [5 the government] can stick with [6 them], [7 it] will be able to halve this year’s 120 billion ruble (US $193 billion) deﬁcit. Table 2 Training instances generated under the single-candidate model for anaphora resolution. Anaphor [6 them] [7 it] Training Instance i{[6 i{[6 i{[6 i{[6 i{[6 them] , [1 them] , [2 them] , [3 them] , [4 them]"
J08-3002,P04-1018,0,0.190663,"rive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with features describing"
J08-3002,W97-1310,0,0.0741564,"Missing"
J08-3002,P05-1020,0,0.0356148,"Missing"
J08-3002,W02-1008,0,0.590151,"-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each o"
J08-3002,P02-1014,0,0.910798,"-mail: sujian@i2r.a-star.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each o"
J08-3002,J01-4004,0,0.959003,"Missing"
J08-3002,P03-1022,0,0.112416,"tar.edu.sg. † 3 Science Drive 2, Singapore 117543. E-mail: tancl@comp.nus.edu.sg. Submission received: 25 October 2005; revised submission received: 2 February 2007; accepted for publication: 5 May 2007. © 2008 Association for Computational Linguistics Computational Linguistics Volume 34, Number 3 years, supervised learning approaches have been widely applied to anaphora resolution, and they have achieved considerable success (Aone and Bennett 1995; McCarthy and Lehnert 1995; Connolly, Burger, and Day 1997; Kehler 1997; Ge, Hale, and Charniak 1998; Soon, Ng, and Lim 2001; Ng and Cardie 2002b; Strube and Mueller 2003; Luo et al. 2004; Ng et al. 2005). The strength of learning-based anaphora resolution is that resolution regularities can be automatically learned from annotated data. Traditionally, learning-based approaches to anaphora resolution adopt the single-candidate model, in which the potential antecedents (i.e., antecedent candidates) are considered in isolation for both learning and resolution. In such a model, the purpose of classiﬁcation is to determine if a candidate is the antecedent of a given anaphor. A training or testing instance is formed by an anaphor and each of its candidates, with fea"
J08-3002,M95-1005,0,0.32181,"Missing"
J08-3002,H89-1033,0,0.0396633,"gher preference to be referred to in later utterances. The forward-looking centers can be ranked based on grammatical roles or other factors. Distance-based factors: Pronouns prefer candidates in the previous sentence compared with those two or more sentences back (Clark and Sengul 1979). As a matter of fact, “eliminating” factors could also be considered “preferential” if we think of the act of eliminating candidates as giving them low preference. Preference-based strategies are also widely seen in earlier manual approaches to pronominal anaphora resolution. For example, the SHRDLU system by Winograd (1972) prefers antecedent candidates in the subject position over those in the object position. The system by Wilks (1973) prefers candidates that satisfy selectional restrictions with the anaphor. Hobbs’s algorithm (Hobbs 1978) prefers candidates that are closer to the anaphor in the syntax tree, and the RAP algorithm (Lappin and Leass 1994) prefers candidates that have a high salience value computed by aggregating the weights of different factors. During resolution, the single-candidate model does select an antecedent based on preference by using classiﬁcation conﬁdence for candidates; that is, th"
J08-3002,P05-1021,1,0.894387,"Missing"
J08-3002,W00-1309,1,0.856944,"Missing"
J08-3002,P02-1060,1,0.413866,"Missing"
J08-3002,J94-4002,0,\N,Missing
N06-1037,P01-1017,0,0.057162,"Missing"
N06-1037,A00-2030,0,\N,Missing
N06-1037,P05-1053,1,\N,Missing
N06-1037,P04-1043,0,\N,Missing
N06-1037,P04-1054,0,\N,Missing
N06-1037,P03-1005,0,\N,Missing
N06-1037,H05-1091,0,\N,Missing
N06-1037,P05-1052,0,\N,Missing
P02-1060,M98-1015,0,0.00617687,"Missing"
P02-1060,E99-1001,0,0.0189434,"Missing"
P02-1060,1993.eamt-1.1,0,0.0634917,"Missing"
P02-1060,W00-0737,1,0.410295,"Missing"
P02-1060,W00-1309,1,0.468852,"Missing"
P02-1060,M98-1004,0,\N,Missing
P02-1060,M98-1012,0,\N,Missing
P02-1060,M98-1014,0,\N,Missing
P02-1060,M98-1021,0,\N,Missing
P02-1060,W97-0312,0,\N,Missing
P02-1060,M98-1018,0,\N,Missing
P02-1060,M98-1019,0,\N,Missing
P02-1060,M98-1020,0,\N,Missing
P02-1060,M95-1012,0,\N,Missing
P02-1060,W00-0726,0,\N,Missing
P02-1060,J95-4004,0,\N,Missing
P02-1060,M98-1009,0,\N,Missing
P03-1023,P95-1017,0,0.890131,"candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, h"
P03-1023,P99-1048,0,0.113874,"Missing"
P03-1023,P87-1022,0,0.240326,"be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition"
P03-1023,P98-2143,0,0.285496,"ce resolution is the process of linking together multiple expressions of a given entity. The key to solve this problem is to determine the antecedent for each referring expression in a document. In coreference resolution, it is common that two or more candidates compete to be the antecedent of an anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 200"
P03-1023,P02-1014,0,0.894344,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,C02-1139,0,0.844771,"hod (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for the antecedent candidates. For example, the “Best-First” selection algorithms (Aone and Bennett, 1995; Ng and Cardie, 2002a) link the anaphor to the candidate with the maximal confidence value (above 0.5). One problem of the single-candidate model, however, is that it only takes into account the relations"
P03-1023,P98-2204,0,0.0157449,"n anaphor (Mitkov, 1999). Whether a candidate is coreferential to an anaphor is often determined by the competition among all the candidates. So far, various algorithms have been proposed to determine the preference relationship between two candidates. Mitkov’s knowledge-poor pronoun resolution method (Mitkov, 1998), for example, uses the scores from a set of antecedent indicators Jian Su* Chew Lim Tan + + Department of Computer Science, National University of Singapore, Singapore 117543 + (yangxiao,tancl)@comp.nus.edu.sg to rank the candidates. And centering algorithms (Brennan et al., 1987; Strube, 1998; Tetreault, 2001), sort the antecedent candidates based on the ranking of the forward-looking or backwardlooking centers. In recent years, supervised machine learning approaches have been widely used in coreference resolution (Aone and Bennett, 1995; McCarthy, 1996; Soon et al., 2001; Ng and Cardie, 2002a), and have achieved significant success. Normally, these approaches adopt a single-candidate model in which the classifier judges whether an antecedent candidate is coreferential to an anaphor with a confidence value. The confidence values are generally used as the competition criterion for"
P03-1023,M95-1005,0,0.609474,"ables). Without candidate filtering, the recall may rise as the correct antecedents would not be eliminated wrongly. Nevertheless, the precision drops largely due to the numerous invalid NPs in the candidate set. As a result, a significantly low Fmeasure is obtained in their approach. Table 4 summarizes the overall performance of different approaches to coreference resolution. Different from Table 2 and 3, here we focus on whether a coreferential chain could be correctly identified. For this purpose, we obtain the recall, the precision and the F-measure using the standard MUC scoring program (Vilain et al. 1995) for the coreference resolution task. Here the recall means the correct resolved chains over the whole coreferential chains in the data set, and precision means the correct resolved chains over the whole resolved chains. In line with the previous experiments, we see reasonable improvement in the performance of the coreference resolution: compared with the baseline approach based on the single-candidate model, the F-measure of approach increases from 69.4 to 71.3 for MUC-6, and from 58.7 to 60.2 for MUC-7. 6 Related Work A similar twin-candidate model was adopted in the anaphoric resolution sys"
P03-1023,W00-1309,1,0.849053,"Missing"
P03-1023,P02-1060,1,0.566195,"Missing"
P03-1023,J01-4003,0,\N,Missing
P03-1023,J01-4004,0,\N,Missing
P03-1023,C98-2138,0,\N,Missing
P03-1023,C98-2199,0,\N,Missing
P04-1017,P87-1022,0,0.522925,"uns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates. In the algorithms based on the centering theory (Brennan et al., 1987; Grosz et al., 1995), if a candidate and its antecedent are the backwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN. In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a trainable model. For each candidate, we take into consideration the properties of its antecedents in terms of features (henceforth backward features), and use the supervised learning method to explore their"
P04-1017,W98-1119,0,0.0510693,"eferential information of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedent"
P04-1017,P83-1007,0,0.509301,"tter performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model"
P04-1017,J95-2003,0,0.907422,"n the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for"
P04-1017,W03-2604,0,0.0339131,"on. Although the algorithm provided no further 0 0 refinement for DTpron , we can use DTpron , as suggested in Section 5.2, to calculate backward features and classify instances by running 0 PRON-RESOLVE(DTnon−pron , DTpron ). The results of such a system, RealResolve-4, are listed in the last line of Table 4. For both MUC6 and MUC-7, RealResolve-4 obtains exactly the same performance as RealResolve-3. 6 Related Work To our knowledge, our work is the first effort that systematically explores the influence of coreferential information of candidates on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity,"
P04-1017,P98-2143,0,0.153326,"ion of candidates on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity, the coreferential length only reflects its global salience in the whole text(s), instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution. Moreover, during resolution, the found coreferential length of an entity is often incomplete, and thus the obtained length value is usually inaccurate for the salience evaluation. 7 Conclusion and Future Work In this paper we have proposed a model which incorporates coreferential information of candidates to improve pronoun resolu"
P04-1017,P02-1014,0,0.294769,"into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the cent"
P04-1017,W99-0207,0,0.0143157,"tes on pronoun resolution in learning-based ways. Iida et al. (2003) also take into consideration the contextual clues in their coreference resolution system, by using two features to reflect the ranking order of a candidate in Salience Reference List (SRL). However, similar to common centering models, in their system the ranking of entities in SRL is also heuristic-based. The coreferential chain length of a candidate, or its variants such as occurrence frequency and TFIDF, has been used as a salience factor in some learning-based reference resolution systems (Iida et al., 2003; Mitkov, 1998; Paul et al., 1999; Strube and Muller, 2003). However, for an entity, the coreferential length only reflects its global salience in the whole text(s), instead of the local salience in a discourse segment which is nevertheless more informative for pronoun resolution. Moreover, during resolution, the found coreferential length of an entity is often incomplete, and thus the obtained length value is usually inaccurate for the salience evaluation. 7 Conclusion and Future Work In this paper we have proposed a model which incorporates coreferential information of candidates to improve pronoun resolution. When evaluati"
P04-1017,J81-4001,0,0.440311,"ll achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased syste"
P04-1017,J01-4004,0,0.860656,"ation of candidates into pronoun resolution. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usual"
P04-1017,P03-1022,0,0.175942,"ion. Preliminary experiments show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughou"
P04-1017,P98-2204,0,0.294892,"Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes that a co-referring candidate is a hearer-old discourse entity and is preferred to other hearer-new candidates. In the algorithms based on the centering theory (Brennan et al., 1987; Grosz et al., 1995), if a candidate and its antecedent are the backwardlooking centers of two subsequent utterances respectively, the candidate would be the most preferred since the CONTINUE transition is always ranked higher than SHIFT or RETAIN. In this paper, we present a supervised learning-based pronoun resolution system which incorporates coreferential information of candidates in a tr"
P04-1017,J01-4003,0,0.132668,"troduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discourse segment (Mitkov, 1999). To determine the salience of a candidate in the local context, we may need to check the coreferential information of the candidate, such as the existence and properties of its antecedents. In fact, such information has been used for pronoun resolution in many heuristicbased systems. The S-List model (Strube, 1998), for example, assumes"
P04-1017,M95-1005,0,0.0794761,"Missing"
P04-1017,P03-1023,1,0.735862,"nts show that our model will boost the resolution performance given the right antecedents of the candidates. We further discuss how to apply our model in real resolution where the antecedents of the candidate are found by a separate noun phrase resolution module. The experimental results show that our model still achieves better performance than the baseline. 1 Introduction In recent years, supervised machine learning approaches have been widely explored in reference resolution and achieved considerable success (Ge et al., 1998; Soon et al., 2001; Ng and Cardie, 2002; Strube and Muller, 2003; Yang et al., 2003). Most learning-based pronoun resolution systems determine the reference relationship between an anaphor and its antecedent candidate only from the properties of the pair. The knowledge about the context of anaphor and antecedent is nevertheless ignored. However, research in centering theory (Sidner, 1981; Grosz et al., 1983; Grosz et al., 1995; Tetreault, 2001) has revealed that the local focusing (or centering) also has a great effect on the processing of pronominal expressions. The choices of the antecedents of pronouns usually depend on the center of attention throughout the local discours"
P04-1017,C98-2138,0,\N,Missing
P04-1017,C98-2199,0,\N,Missing
P04-1021,W03-1508,0,0.797183,"Missing"
P04-1021,C00-1056,0,\N,Missing
P04-1075,W02-0301,0,0.00997657,"he named entity recognizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist o"
P04-1075,W03-1305,0,0.0392232,"gnizer. The contributions not only come from the above measures, but also the two sample selection strategies which effectively incorporate informativeness, representativeness and diversity criteria. To our knowledge, it is the first work on considering the three criteria all together for active learning. Furthermore, such measures and strategies can be easily adapted to other active learning tasks as well. 2 Multi-criteria for NER Active Learning Support Vector Machines (SVM) is a powerful machine learning method, which has been applied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special hea"
P04-1075,P00-1016,0,0.432559,"ted corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, called batchedbased sample sele ction (Lewis and C"
P04-1075,W03-1307,1,0.3891,"ied successfully in NER tasks, such as (Kazama et al. 2002; Lee et al. 2003). In this paper, we apply active learning methods to a simple and effective SVM model to recognize one class of names at a time, such as protein names, person names, etc. In NER, SVM is to classify a word into positive class “1”indicating that the word is a part of an entity, or negative class “-1” indicating that the word is not a part of an entity. Each word in SVM is represented as a high-dimensional feature vector including surface word information, orthographic features, POS feature and semantic trigger features (Shen et al. 2003). The semantic trigger features consist of some special head nouns for an entity class which is supplied by users. Furthermore, a window (size = 7), which represents the local context of the target word w, is also used to classify w. However, for active learning in NER, it is not reasonable to select a single word without context for human to label. Even if we require human to label a single word, he has to make an addition effort to refer to the context of the word. In our active learning process, we select a word sequence which consists of a machine-annotated named entity and its context rat"
P04-1075,N03-1031,0,0.0302235,"models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are selected at a time, cal"
P04-1075,P02-1016,0,0.784918,"processing (NLP), models are generally trained on large annotated corpus. However, annotating such corpus is expensive and timeconsuming, which makes it difficult to adapt an existing model to a new domain. In order to overcome this difficulty, active learning (sample sele ction) has been studied in more and more NLP applications such as POS tagging (Engelson and Dagan 1999), information extraction (Thompson et al. 1999), text classif ication (Lewis and Catlett 1994; McCallum and Nigam 1998; Schohn and Cohn 2000; Tong and Koller 2000; Brinker 2003), statistical parsing (Thompson et al. 1999; Tang et al. 2002; Steedman et al. 2003), noun phrase chunking (Ngai and Yarowsky 2000), etc. Active learning is based on the assumption that 1 a small number of annotated examples and a large number of unannotated examples are available. This assumption is valid in most NLP tasks. Different from supervised learning in which the entire corpus are labeled manually, active learning is to select the most useful example for labe ling and add the labeled example to training set to retrain model. This procedure is repeated until the model achieves a certain level of performance. Practically, a batch of examples are"
P04-1075,W02-2029,0,\N,Missing
P04-1075,W00-1306,0,\N,Missing
P05-1021,N04-1038,0,0.0950612,"sis that the government said it1 was going to collect it2 . For anaphor it1 , the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well us"
P05-1021,C90-3063,0,0.721766,"y incorporated in the twin-candidate learning model and significantly improve the resolution of neutral pronouns. 1 Introduction Semantic compatibility is an important factor for pronoun resolution. Since pronouns, especially neutral pronouns, carry little semantics of their own, the compatibility between an anaphor and its antecedent candidate is commonly evaluated by examining the relationships between the candidate and the anaphor’s context, based on the statistics that the corresponding predicate-argument tuples occur in a particular large corpus. Consider the example given in the work of Dagan and Itai (1990): (1) They know full well that companies held tax money aside for collection later on the basis that the government said it1 was going to collect it2 . For anaphor it1 , the candidate government should have higher semantic compatibility than money because government collect is supposed to occur more frequently than money collect in a large corpus. A similar pattern could also be observed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It"
P05-1021,N04-1037,0,0.237545,"ed for it2 . So far, the corpus-based semantic knowledge has been successfully employed in several anaphora resolution systems. Dagan and Itai (1990) proposed a heuristics-based approach to pronoun resolution. It determined the preference of candidates based on predicate-argument frequencies. Recently, Bean and Riloff (2004) presented an unsupervised approach to coreference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic comp"
P05-1021,J03-3005,0,0.222477,"nowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics web. It is believed that the size of the web is thousands of times larger than normal large corpora, and the counts obtained from the web are highly correlated with the counts from large balanced corpora for predicate-argument bi-grams (Keller and Lapata, 2003). So far the web has been utilized in nominal anaphora resolution (Modjeska et al., 2003; Poesio et al., 2004) to determine the semantic relation between an anaphor and candidate pair. However, to our knowledge, using the web to help pronoun resolution still remains unexplored. Learning framework. Commonly, the predicateargument statistics is incorporated into anaphora resolution systems as a feature. What kind of learning framework is suitable for this feature? Previous approaches to anaphora resolution adopt the singlecandidate model, in which the resolution is done on an anaphor and one can"
P05-1021,P98-2143,0,0.202456,"reference resolution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, p"
P05-1021,W03-1023,0,0.0886949,"Missing"
P05-1021,P02-1014,0,0.225064,"bability (Eq. 2) metric. In such a situation, count(candi, ana) is the hit number of the inflected queries returned by the search engine, while count(candi) is the hit number of the query formed with only the head of the candidate (i.e.,“the + candi”). 3 Applying the Semantic Compatibility In this section, we discuss how to incorporate the statistics-based semantic compatibility for pronoun resolution, in a machine learning framework. 3.1 The Single-Candidate Model One way to utilize the semantic compatibility is to take it as a feature under the single-candidate learning model as employed by Ng and Cardie (2002). In such a learning model, each training or testing instance takes the form of i{C, ana}, where ana is the possible anaphor and C is its antecedent candidate. An instance is associated with a feature vector to describe their relationships. During training, for each anaphor in a given text, a positive instance is created by pairing the anaphor and its closest antecedent. Also a set of negative instances is formed by pairing the anaphor and each of the intervening candidates. Based on the training instances, a binary classifier is generated using a certain learning algorithm, like C5 (Quinlan,"
P05-1021,P04-1019,0,0.0809532,"Missing"
P05-1021,J01-4004,0,0.279715,"ution, which mined the co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann"
P05-1021,P03-1022,0,0.0887334,"he co-referring NP pairs with similar predicatearguments from a large corpus using a bootstrapping method. However, the utility of the corpus-based semantics for pronoun resolution is often argued. Kehler et al. (2004), for example, explored the usage of the corpus-based statistics in supervised learning based systems, and found that such information did not produce apparent improvement for the overall pronoun resolution. Indeed, existing learning-based approaches to anaphor resolution have performed reasonably well using limited and shallow knowledge (e.g., Mitkov (1998), Soon et al. (2001), Strube and Muller (2003)). Could the relatively noisy semantic knowledge give us further system improvement? In this paper we focus on improving pronominal anaphora resolution using automatically computed semantic compatibility information. We propose to enhance the utility of the statistics-based knowledge from two aspects: Statistics source. Corpus-based knowledge usually suffers from data sparseness problem. That is, many predicate-argument tuples would be unseen even in a large corpus. A possible solution is the 165 Proceedings of the 43rd Annual Meeting of the ACL, pages 165–172, c Ann Arbor, June 2005. 2005 Ass"
P05-1021,P98-2204,0,0.0217787,"ility of the candidate the semantic compatibility difference between two competing candidates Table 1: Feature set for our pronoun resolution system(*ed feature is only for the single-candidate model while **ed feature is only for the twin-candidate mode) computational cost but with high reliability. Table 1 summarizes the features with their respective possible values. The first three features represent the lexical properties of a candidate. The POS properties could indicate whether a candidate refers to a hearerold entity that would have a higher preference to be selected as the antecedent (Strube, 1998). SameSent and NearestNP mark the distance relationships between an anaphor and the candidate, which would significantly affect the candidate selection (Hobbs, 1978). FirstNP aims to capture the salience of the candidate in the local discourse segment. ParalStuct marks whether a candidate and an anaphor have similar surrounding words, which is also a salience factor for the candidate evaluation (Mitkov, 1998). Feature StatSem records the statistics-based semantic compatibility computed, from the corpus or the web, by either frequency or probability metric, as described in the previous section."
P05-1021,P03-1023,1,0.770302,"way to tackle this problem is to normalize the feature by the frequencies of the anaphor’s context, e.g., “count(collected)” and “count(said)”. This, however, would require extra calculation. In fact, as candidates of a specific anaphor share the same anaphor context, we can just normalize the semantic feature of a candidate by that of its competitor: StatSemN (C, ana) = StatSem(C, ana) max StatSem(ci , ana) ci ∈candi set(ana) The value (0 ∼ 1) represents the rank of the semantic compatibility of the candidate C among candi set(ana), the current candidates of ana. 3.3 The Twin-Candidate Model Yang et al. (2003) proposed an alternative twincandidate model for anaphora resolution task. The strength of such a model is that unlike the singlecandidate model, it could capture the preference relationships between competing candidates. In the model, candidates for an anaphor are paired and features from two competing candidates are put together for consideration. This property could nicely deal with the above mentioned training problem of different anaphor contexts, because the semantic feature would be considered under the current candidate set only. In fact, as semantic compatibility is a preference-based"
P05-1021,P02-1060,1,0.513413,"Missing"
P05-1021,W00-0737,1,0.882222,"Missing"
P05-1021,C98-2138,0,\N,Missing
P05-1021,C98-2199,0,\N,Missing
P05-1053,P04-1054,0,0.650545,"Missing"
P05-1053,A00-2030,0,0.0346955,"tion on the 5 ACE relation types. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 and Section 4 describe our approach and various features employed respectively. Finally, we present experimental setting and results in Section 5 and conclude with some general observations in relation extraction in Section 6. 2 Related Work The relation extraction task was formulated at the 7th Message Understanding Conference (MUC-7 1998) and is starting to be addressed more and more within the natural language processing and machine learning communities. Miller et al (2000) augmented syntactic full parse trees with semantic information corresponding to entities and relations, and built generative models for the augmented trees. Zelenko et al (2003) proposed extracting relations by computing kernel functions between parse trees. Culotta et al (2004) extended this work to estimate kernel functions between augmented dependency trees and achieved 63.2 F-measure in relation detection and 45.8 F-measure in relation detection and classification on the 5 ACE relation types. Kambhatla (2004) employed Maximum Entropy models for relation extraction with features derived fr"
P05-1053,C02-1151,0,\N,Missing
P05-1053,J03-4003,0,\N,Missing
P06-1006,J03-3005,0,0.0214287,"Missing"
P06-1006,C96-1021,0,0.41745,"Missing"
P06-1006,J94-4002,0,0.917584,"the sentence distance between the candidate and the pronominal anaphor. Closeness: whether the candidate is the candidate closest to the pronominal anaphor. FirstNP: whether the candidate is the first noun phrase in the current sentence. Parallelism: whether the candidate has an identical collocation pattern with the pronominal anaphor. Table 1: Feature set for the baseline pronoun resolution system salience measures have to be assigned manually. Luo and Zitouni (2005) proposed a coreference resolution approach which also explores the information from the syntactic parse trees. Different from Lappin and Leass (1994)’s algorithm, they employed a maximum entropy based model to automatically compute the importance (in terms of weights) of the features extracted from the trees. In their work, the selection of their features is mainly inspired by the government and binding theory, aiming to capture the c-command relationships between the pronoun and its antecedent candidate. By contrast, our approach simply utilizes the parse trees as a structured feature, and lets the learning algorithm discover all possible embedded information that is necessary for pronoun resolution. Related Work One of the early work on"
P06-1006,H05-1083,0,0.0665107,"ct of a sentence, a subject of a clause, or not. Object: whether the candidate is an object of a verb, an object of a preposition, or not. Distance: the sentence distance between the candidate and the pronominal anaphor. Closeness: whether the candidate is the candidate closest to the pronominal anaphor. FirstNP: whether the candidate is the first noun phrase in the current sentence. Parallelism: whether the candidate has an identical collocation pattern with the pronominal anaphor. Table 1: Feature set for the baseline pronoun resolution system salience measures have to be assigned manually. Luo and Zitouni (2005) proposed a coreference resolution approach which also explores the information from the syntactic parse trees. Different from Lappin and Leass (1994)’s algorithm, they employed a maximum entropy based model to automatically compute the importance (in terms of weights) of the features extracted from the trees. In their work, the selection of their features is mainly inspired by the government and binding theory, aiming to capture the c-command relationships between the pronoun and its antecedent candidate. By contrast, our approach simply utilizes the parse trees as a structured feature, and l"
P06-1006,P98-2143,0,0.186048,"Missing"
P06-1006,P04-1043,0,0.366702,"ilize the syntactic parse trees to help learning-based pronoun resolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of Syntactic knowled"
P06-1006,P02-1014,0,0.144265,"ance of the algorithm would rely heavily on the accuracy of the parsing results. Lappin and Leass (1994) reported a pronoun resolution algorithm which uses the syntactic representation output by McCord’s Slot Grammar parser. A set of salience measures (e.g. Subject, Object or Accusative emphasis) is derived from the syntactic structure. The candidate with the highest salience score would be selected as the antecedent. In their algorithm, the weights of 3 The Resolution Framework Our pronoun resolution system adopts the common learning-based framework similar to those by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance is formed by a pronoun and one of its antecedent candidate. During training, for each pronominal anaphor encountered, a positive instance is created by paring the anaphor and its closest antecedent. Also a set of negative instances is formed by paring the anaphor with each of the 42 will discuss how to use kernels to incorporate the more complex structured feature. non-coreferential candidates. Based on the training instances, a binary classifier is generated using a particular learning algorithm. During resolution, a pronominal anaph"
P06-1006,J01-4004,0,0.75231,"arse trees, the performance of the algorithm would rely heavily on the accuracy of the parsing results. Lappin and Leass (1994) reported a pronoun resolution algorithm which uses the syntactic representation output by McCord’s Slot Grammar parser. A set of salience measures (e.g. Subject, Object or Accusative emphasis) is derived from the syntactic structure. The candidate with the highest salience score would be selected as the antecedent. In their algorithm, the weights of 3 The Resolution Framework Our pronoun resolution system adopts the common learning-based framework similar to those by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance is formed by a pronoun and one of its antecedent candidate. During training, for each pronominal anaphor encountered, a positive instance is created by paring the anaphor and its closest antecedent. Also a set of negative instances is formed by paring the anaphor with each of the 42 will discuss how to use kernels to incorporate the more complex structured feature. non-coreferential candidates. Based on the training instances, a binary classifier is generated using a particular learning algorithm. During resol"
P06-1006,P95-1017,0,0.514578,"Missing"
P06-1006,A00-2018,0,0.00378114,"we focussed on the third-person pronominal anaphora resolution. All the experiments were done on the ACE-2 V1.0 corpus (NIST, 2003), which contain two data sets, training and devtest, used for training and testing respectively. Each of these sets is further divided into three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). An input raw text was preprocessed automatically by a pipeline of NLP components, including sentence boundary detection, POS-tagging, Text Chunking and Named-Entity Recognition. The texts were parsed using the maximum-entropybased Charniak parser (Charniak, 2000), based on which the structured features were computed automatically. For learning, the SVM-Light software (Joachims, 1999) was employed with the convolution tree kernel implemented by Moschitti (2004). All classifiers were trained with default learning parameters. The performance was evaluated based on the metric success, the ratio of the number of correctly resolved5 anaphor over the number of all anaphors. For each anaphor, the NPs occurring within the current and previous two sentences were taken as the initial antecedent candidates. Those with mismatched number and gender agreements were"
P06-1006,P04-1017,1,0.914439,"Missing"
P06-1006,P02-1034,0,0.575113,"we will explore how to utilize the syntactic parse trees to help learning-based pronoun resolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of"
P06-1006,P05-1052,0,0.021097,"esolution. Specifically, we directly utilize the parse trees as a structured feature, and then use a kernel-based method to automatically mine the knowledge embedded in the parse trees. The structured syntactic feature, together with other normal features, is incorporated in a trainable model based on Support Vector Machine (SVM) (Vapnik, 1995) to learn the decision classifier for resolution. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2002; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhao and Grishman, 2005). However, to our knowledge, the application of such a technique to the pronoun resolution task still remains unexplored. Compared with previous work, our approach has several advantages: (1) The approach utilizes the parse trees as a structured feature, which avoids the efforts of decoding the parse trees into a set of syntactic features in a heuristic manner. (2) The approach is able to put together the structured feature and the normal flat features in a trainable model, which allows different types of Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic inf"
P06-1006,J03-4003,0,\N,Missing
P06-1006,C98-2138,0,\N,Missing
P06-1016,P04-1054,0,0.168458,"he previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typic"
P06-1016,A00-2030,0,0.0727802,"rchical strategy outperforms the previous best-reported system. The rest of this paper is organized as follows. Section 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome t"
P06-1016,I05-1034,1,0.93572,"tion 2 presents related work. Section 3 describes the hierarchical learning strategy using the perceptron algorithm. Finally, we present experimentation in Section 4 and conclude this paper in Section 5. 2 Related Work The relation extraction task was formulated at MUC-7(1998). With the increasing popularity of ACE, this task is starting to attract more and more researchers within the natural language processing and machine learning communities. Typical works include Miller et al (2000), Zelenko et al (2003), Culotta and Sorensen (2004), Bunescu and Mooney (2005a), Bunescu and Mooney (2005b), Zhang et al (2005), Roth and Yih (2002), Kambhatla (2004), Zhao and Grisman (2005) and Zhou et al (2005). Miller et al (2000) augmented syntactic full parse trees with semantic information of entities and relations, and built generative models to integrate various tasks such as POS tagging, named entity recognition, template element extraction and relation extraction. The problem is that such integration may impose big challenges, e.g. the need of a large annotated corpus. To overcome the data sparseness problem, generative models typically applied some smoothing techniques to integrate different scales of cont"
P06-1016,P05-1052,0,0.412762,"ve steady performance given the current corpus size. Given the relative large size of this corpus, it will be time-consuming and very expensive to further expand the corpus with a reasonable gain in performance. Even if we can somehow expend the corpus and achieve steady performance on major relation subtypes, it will be still far beyond practice for those minor subtypes given the much unevenly distribution among different relation subtypes. While various machine learning approaches, such as generative modeling (Miller et al 2000), maximum entropy (Kambhatla 2004) and support vector machines (Zhao and Grisman 2005; Zhou et al 2005), have been applied in the relation extraction task, no explicit learning strategy is proposed to deal with the inherent data sparseness problem caused by the much uneven distribution among different relations. This paper proposes a novel hierarchical learning strategy to deal with the data sparseness problem by modeling the commonality among related classes. Through organizing various classes hierarchically, a linear discriminative function is determined for each class in a topdown way using a perceptron algorithm with the lower-level weight vector derived from the upper-lev"
P06-1016,P02-1060,1,0.825946,"Missing"
P06-1016,P05-1053,1,0.864337,"Missing"
P06-1016,C02-1151,0,\N,Missing
P06-1016,J03-4003,0,\N,Missing
P06-1016,P04-1053,0,\N,Missing
P06-1016,H05-1091,0,\N,Missing
P06-1104,P01-1017,0,0.0536781,"Missing"
P06-1104,A00-2030,0,\N,Missing
P06-1104,P05-1053,1,\N,Missing
P06-1104,P04-1043,0,\N,Missing
P06-1104,P04-1054,0,\N,Missing
P06-1104,P03-1005,0,\N,Missing
P06-1104,H05-1091,0,\N,Missing
P06-1104,P05-1052,0,\N,Missing
P06-2005,P00-1037,0,0.383464,"ese SMS translation system using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv"
P06-2005,J93-2003,0,0.0118108,"≈ max {P(T |e1N )i P( s1K |e1K )} (1) = arg max {P( s1M |e1N )i P( e1N )} T e1N This is the basic function of the channel model for the phrase-based SMS normalization model, where we used the maximum approximation for the sum over all segmentations. Then we further decompose the probability P( s1K |e1K ) using a Assuming that one SMS word is mapped exactly to one English word in the channel model P ( s |e) under an alignment A , we need to consider only two types of probabilities: the alignment probabilities denoted by P(m |am ) and the lexicon mapping probabilities denoted by P ( sm |eam ) (Brown et al. 1993). The channel phrase alignment A as done in the previous word-based model. P( s1K |e1K ) = ∑ P( s1K , A |e1K ) model can be written as in the following equation where m is the position of a word in s and am its alignment in e . A = ∑{P( A |e1K )i P( s1K |A, e1K )} A P( s1M |e1N ) = ∑ P( s1M , A |e1N )  K  (4) = ∑  ∏ P( k |ak )i P( sk |s1k −1 , e aa1k )  A  k =1  { A = ∑ P( A |e1N )i P( s1M |A, e1N ) (2) }  K  ≈ ∑  ∏ P( k |ak )i P( sk |eak )   A  k =1 { A  M  ≈ ∑  ∏ P( m |am )i P( sm |eam )  A  m =1  { (3) = ∑ P(T |e1N )i P( s1K |e1K ) = arg max {P( e |s )} N 1 } } We are now"
P06-2005,C02-1134,0,0.127259,"tem using a wordgroup model. In addition, in most of the commercial SMS translation applications 2 , SMS lingo (i.e., SMS short form) dictionary is provided to replace SMS short-forms with normal English words. Most of the systems do not handle OOV (out-of-vocabulary) items and ambiguous inputs. Following compares SMS text normalization with other similar or related applications. 2.1 Gale, 1991) that mostly model the edit operations using distance measures (Damerau 1964; Levenshtein 1966), specific word set confusions (Golding and Roth, 1999) and pronunciation modeling (Brill and Moore, 2000; Toutanova and Moore, 2002). These models are mostly character-based or string-based without considering the context. In addition, the author might not be aware of the errors in the word introduced during the edit operations, as most errors are due to mistype of characters near to each other on the keyboard or homophones, such as “poor” or “pour”. In SMS, errors are not isolated within word and are usually not surrounded by clean context. Words are altered deliberately to reflect sender’s distinct creation and idiosyncrasies. A character can be deleted on purpose, such as “wat” (what) and “hv” (have). It also consists o"
P06-2005,C90-2036,0,\N,Missing
P06-2005,P02-1040,0,\N,Missing
P06-2005,P01-1008,0,\N,Missing
P06-2005,P02-1019,0,\N,Missing
P06-2005,N03-1017,0,\N,Missing
P06-2005,shimohata-sumita-2002-automatic,0,\N,Missing
P07-1067,N04-1038,0,0.122008,"to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Instead of leveraging existing lexicons, many researchers have investigated corpus-based ap529 proaches to mine semantic relations. Garera and Yarowsky (2006) propose an unsupervised model which extracts hypernym relation for resloving definite NPs. Their model assumes that a definite NP and its hypernym words usually co-occur in texts. Thus, for a definite-NP anaphor, a preceding NP that has a high co-occurrence statistics in a large corpus is preferred for the antecedent. Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic position, e.g. “murder of <NP>”, “killed <patient>”. From the caseframes, they derive different types of contextual role knowledge for resolution, for example, whether an anaphor and an antecedent candidate can be filled into co-occurring caseframes, or whether they are substitutable for each other in their caseframes. Different from their sys"
P07-1067,W06-2906,0,0.127998,"knowledge. In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. In (Harabagiu et al., 2001), the path patterns in WordNet are utilized to compute the semantic consistency between NPs. Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Instead of leveraging existing lexicons, many researchers have investigated corpus-based ap529 proaches to mine semantic relations. Garera and Yarowsky (2006) propose an unsupervised model which extracts hypernym relation for resloving definite NPs. Their model assumes that a definite NP and its hypernym words usually co-occur in texts. Thus, for a definite-NP anaphor, a preceding NP that has a high co-occurrence statistics in a large corpus is preferred for the antecedent. Bean and Riloff (2004) present a system called BABAR that uses contextual role knowledge to do coreference resolution. They apply an IE component to unannotated texts to generate a set of extraction caseframes. Each caseframe represents a linguistic expression and a syntactic po"
P07-1067,N01-1008,0,0.759518,"e variety Semantic relatedness is a very important factor for of semantic relations, not just is-a, by which coreference resolution, as noun phrases used to recoreference relationship is realized? For exfer to the same entity should have a certain semantic ample, in some annotation schemes like ACE, relation. To obtain this semantic information, previ“Beijing:China” are coreferential as the capital ous work on reference resolution usually leverages and the country could be used to represent the a semantic lexicon like WordNet (Vieira and Poegovernment. The pattern for the common “issio, 2000; Harabagiu et al., 2001; Soon et al., 2001; a” relation will fail to identify the NP pairs of Ng and Cardie, 2002). However, the drawback of such a “capital-country” relation. WordNet is that many expressions (especially for proper names), word senses and semantic relations To deal with this problem, in this paper we proare not available from the database (Vieira and Poe- pose an approach which can automatically discover sio, 2000). In recent years, increasing interest has effective patterns to represent the semantic relations 528 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics"
P07-1067,W03-2606,0,0.365835,"loit the patterns to obtain the semantic relatedness information. The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution. 1 Introduction been seen in mining semantic relations from large text corpora. One common solution is to utilize a pattern that can represent a specific semantic relation (e.g., “X such as Y” for is-a relation, and “X and other Y” for other-relation). Instantiated with two given noun phrases, the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness (Markert et al., 2003; Modjeska et al., 2003; Poesio et al., 2004). However, in the previous pattern based approaches, the selection of the patterns to represent a specific semantic relation is done in an ad hoc way, usually by linguistic intuition. The manually selected patterns, nevertheless, are not necessarily the most effective ones for coreference resolution from the following two concerns: • Accuracy. Can the patterns (e.g., “X such as Y”) find as many NP pairs of the specific semantic relation (e.g. is-a) as possible, with a high precision? • Breadth. Can the patterns cover a wide variety Semantic relatedn"
P07-1067,W03-1023,0,0.152772,"btain the semantic relatedness information. The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution. 1 Introduction been seen in mining semantic relations from large text corpora. One common solution is to utilize a pattern that can represent a specific semantic relation (e.g., “X such as Y” for is-a relation, and “X and other Y” for other-relation). Instantiated with two given noun phrases, the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness (Markert et al., 2003; Modjeska et al., 2003; Poesio et al., 2004). However, in the previous pattern based approaches, the selection of the patterns to represent a specific semantic relation is done in an ad hoc way, usually by linguistic intuition. The manually selected patterns, nevertheless, are not necessarily the most effective ones for coreference resolution from the following two concerns: • Accuracy. Can the patterns (e.g., “X such as Y”) find as many NP pairs of the specific semantic relation (e.g. is-a) as possible, with a high precision? • Breadth. Can the patterns cover a wide variety Semantic relatedness is a very important"
P07-1067,P02-1014,0,0.649845,"is-a, by which coreference resolution, as noun phrases used to recoreference relationship is realized? For exfer to the same entity should have a certain semantic ample, in some annotation schemes like ACE, relation. To obtain this semantic information, previ“Beijing:China” are coreferential as the capital ous work on reference resolution usually leverages and the country could be used to represent the a semantic lexicon like WordNet (Vieira and Poegovernment. The pattern for the common “issio, 2000; Harabagiu et al., 2001; Soon et al., 2001; a” relation will fail to identify the NP pairs of Ng and Cardie, 2002). However, the drawback of such a “capital-country” relation. WordNet is that many expressions (especially for proper names), word senses and semantic relations To deal with this problem, in this paper we proare not available from the database (Vieira and Poe- pose an approach which can automatically discover sio, 2000). In recent years, increasing interest has effective patterns to represent the semantic relations 528 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528–535, c Prague, Czech Republic, June 2007. 2007 Association for Computational Li"
P07-1067,P05-1020,0,0.0357118,"Missing"
P07-1067,P06-1015,0,0.0593648,"and forms a query. The query is submitted to the Google searching engine, and the returned hit number is utilized to compute the semantic relatedness between the two NPs. In their work, the semantic information is used as a feature for the learner. Markert et al. (2003) and Poesio et al. (2004) adopt a similar strategy for the bridging anaphora resolution. In (Hearst, 1998), the author also proposes to discover new patterns instead of using the manually designed ones. She employs a bootstrapping algorithm to learn new patterns from the word pairs with a known relation. Based on Hearst’s work, Pantel and Pennacchiotti (2006) further give a method which measures the reliability of the patterns based on the strength of association between patterns and instances, employing the pointwise mutual information (PMI). 3 Framework of Coreference Resolution Our coreference resolution system adopts the common learning-based framework as employed by Soon et al. (2001) and Ng and Cardie (2002). In the learning framework, a training or testing instance has the form of i{N Pi , N Pj }, in which N Pj is a possible anaphor and N Pi is one of its antecedent candidates. An instance is associated with a vector of features, which is u"
P07-1067,P04-1019,0,0.589082,"tedness information. The evaluation on ACE data set shows that the pattern based semantic information is helpful for coreference resolution. 1 Introduction been seen in mining semantic relations from large text corpora. One common solution is to utilize a pattern that can represent a specific semantic relation (e.g., “X such as Y” for is-a relation, and “X and other Y” for other-relation). Instantiated with two given noun phrases, the pattern is searched in a large corpus and the occurrence number is used as a measure of their semantic relatedness (Markert et al., 2003; Modjeska et al., 2003; Poesio et al., 2004). However, in the previous pattern based approaches, the selection of the patterns to represent a specific semantic relation is done in an ad hoc way, usually by linguistic intuition. The manually selected patterns, nevertheless, are not necessarily the most effective ones for coreference resolution from the following two concerns: • Accuracy. Can the patterns (e.g., “X such as Y”) find as many NP pairs of the specific semantic relation (e.g. is-a) as possible, with a high precision? • Breadth. Can the patterns cover a wide variety Semantic relatedness is a very important factor for of semanti"
P07-1067,N06-1025,0,0.293002,"resents the model to obtain the patternbased semantic relatedness information. Section 5 discusses the experimental results. Finally, Section 6 summarizes the conclusions. 2 Related Work Earlier work on coreference resolution commonly relies on semantic lexicons for semantic relatedness knowledge. In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. In (Harabagiu et al., 2001), the path patterns in WordNet are utilized to compute the semantic consistency between NPs. Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Instead of leveraging existing lexicons, many researchers have investigated corpus-based ap529 proaches to mine semantic relations. Garera and Yarowsky (2006) propose an unsupervised model which extracts hypernym relation for resloving definite NPs. Their model assumes that a definite NP and its hypernym words usually co-occur in texts. Thus, for a definite-NP anaphor, a preceding NP that has a high co-occurrence statistics in a large corpus is preferred for the ante"
P07-1067,J01-4004,0,0.729419,"edness is a very important factor for of semantic relations, not just is-a, by which coreference resolution, as noun phrases used to recoreference relationship is realized? For exfer to the same entity should have a certain semantic ample, in some annotation schemes like ACE, relation. To obtain this semantic information, previ“Beijing:China” are coreferential as the capital ous work on reference resolution usually leverages and the country could be used to represent the a semantic lexicon like WordNet (Vieira and Poegovernment. The pattern for the common “issio, 2000; Harabagiu et al., 2001; Soon et al., 2001; a” relation will fail to identify the NP pairs of Ng and Cardie, 2002). However, the drawback of such a “capital-country” relation. WordNet is that many expressions (especially for proper names), word senses and semantic relations To deal with this problem, in this paper we proare not available from the database (Vieira and Poe- pose an approach which can automatically discover sio, 2000). In recent years, increasing interest has effective patterns to represent the semantic relations 528 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 528–535, c"
P07-1067,J00-4003,0,0.0272672,"perimental results show that the pattern based semantic relatedness information is helpful for the coreference resolution. The remainder of the paper is organized as follows. Section 2 gives some related work. Section 3 introduces the framework for coreference resolution. Section 4 presents the model to obtain the patternbased semantic relatedness information. Section 5 discusses the experimental results. Finally, Section 6 summarizes the conclusions. 2 Related Work Earlier work on coreference resolution commonly relies on semantic lexicons for semantic relatedness knowledge. In the system by Vieira and Poesio (2000), for example, WordNet is consulted to obtain the synonymy, hypernymy and meronymy relations for resolving the definite anaphora. In (Harabagiu et al., 2001), the path patterns in WordNet are utilized to compute the semantic consistency between NPs. Recently, Ponzetto and Strube (2006) suggest to mine semantic relatedness from Wikipedia, which can deal with the data sparseness problem suffered by using WordNet. Instead of leveraging existing lexicons, many researchers have investigated corpus-based ap529 proaches to mine semantic relations. Garera and Yarowsky (2006) propose an unsupervised mo"
P07-1067,M95-1005,0,0.0185159,"contains two data set, training and devtest, used for training and testing respectively. Each of these sets is further divided by three domains: newswire (NWire), newspaper (NPaper), and broadcast news (BNews). An input raw text was preprocessed automatically by a pipeline of NLP components, including sentence boundary detection, POS-tagging, Text Chunking and Named-Entity Recognition. Two different classifiers were learned respectively for resolving pronouns and non-pronouns. As mentioned, the pattern based semantic information was only applied to the non-pronoun resolution. For evaluation, Vilain et al. (1995)’s scoring algorithm was adopted to compute the recall and precision of the whole coreference resolution. For pattern extraction and feature computing, we used Wikipedia, a web-based free-content encyclopedia, as the text corpus. We collected the English Wikipedia database dump in November 2006 (refer to http://download.wikimedia.org/). After all the hyperlinks and other html tags were removed, the whole pure text contains about 220 Million words. 5.2 Results and Discussion Table 1 lists the performance of different coreference resolution systems. The first line of the table shows the baseline"
P07-1067,J06-1005,0,\N,Missing
P08-1096,P95-1017,0,0.282338,"plicitly express relations between an entity and the contained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et"
P08-1096,N07-1011,0,0.682128,"n (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an entity and record their information in a single feature vector, as it would make the feature space too large. Even worse, the number of mentions in an entity is not fixed, which would result in variant-length feature vectors and make trouble for normal machine learning algorithms. A solution seen in previous work (Luo et al., 2004; Culotta et al., 2007) is to design a set of first-order features summarizing the information of the mentions in an entity, for example, “whether the entity has any mention that is a name alias of the active mention?” or “whether most of the mentions in the entity have the same head word as the active mention?” These features, nevertheless, are designed in an ad-hoc manner and lack the capability of describing each individual mention in an entity. In this paper, we present a more expressive entity843 Proceedings of ACL-08: HLT, pages 843–851, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Ling"
P08-1096,N07-1030,0,0.213821,"Missing"
P08-1096,P04-1018,0,0.879059,". Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all"
P08-1096,P02-1014,0,0.959589,"nd automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, th"
P08-1096,P05-1020,0,0.108939,"Missing"
P08-1096,P07-1068,0,0.0587945,"Missing"
P08-1096,J01-4004,0,0.901524,"ontained mentions, and automatically learn first-order rules important for coreference decision. The evaluation on the ACE data set shows that the ILP based entity-mention model is effective for the coreference resolution task. 1 Introduction Coreference resolution is the process of linking multiple mentions that refer to the same entity. Most of previous work adopts the mention-pair model, which recasts coreference resolution to a binary classification problem of determining whether or not two mentions in a document are co-referring (e.g. Aone and Bennett (1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional menti"
P08-1096,P07-1006,0,0.0425872,"ch other. The simplest model conditions coreference on mention pairs, but enforces dependency by calculating the distance of a node to a partition (i.e., the probability that an active mention belongs to an entity) based on the sum of its distances to all the nodes in the partition (i.e., the sum of the probability of the active mention co-referring with the mentions in the entity). Inductive Logic Programming (ILP) has been applied to some natural language processing tasks, including parsing (Mooney, 1997), POS disambiguation (Cussens, 1996), lexicon construction (Claveau et al., 2003), WSD (Specia et al., 2007), and so on. However, to our knowledge, our work is the first effort to adopt this technique for the coreference resolution task. 3 Modelling Coreference Resolution Suppose we have a document containing n mentions {mj : 1 &lt; j &lt; n}, in which mj is the jth mention occurring in the document. Let ei be the ith entity in the document. We define P (L|ei , mj ), (1) the probability that a mention belongs to an entity. Here the random variable L takes a binary value and is 1 if mj is a mention of ei . By assuming that mentions occurring after mj have no influence on the decision of linking mj to an en"
P08-1096,M95-1005,0,0.818946,"Missing"
P08-1096,P07-1067,1,0.315086,"Missing"
P08-1096,P04-1017,1,0.929248,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,C04-1033,1,0.933005,"(1995); McCarthy and Lehnert (1995); Soon et al. (2001); Ng and Cardie (2002)). Although having achieved reasonable success, the mention-pair model has a limitation that information beyond mention pairs is ignored for training and testing. As an individual mention usually lacks adequate descriptive information of the referred entity, it is often difficult to judge whether or not two mentions are talking about the same entity simply from the pair alone. An alternative learning model that can overcome this problem performs coreference resolution based on entity-mention pairs (Luo et al., 2004; Yang et al., 2004b). Compared with the traditional mentionpair counterpart, the entity-mention model aims to make coreference decision at an entity level. Classification is done to determine whether a mention is a referent of a partially found entity. A mention to be resolved (called active mention henceforth) is linked to an appropriate entity chain (if any), based on classification results. One problem that arises with the entity-mention model is how to represent the knowledge related to an entity. In a document, an entity may have more than one mention. It is impractical to enumerate all the mentions in an"
P08-1096,W00-1309,1,0.821131,"Missing"
P08-1096,P02-1060,1,0.450274,"Missing"
P10-1073,D07-1010,0,0.14342,"classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-leve"
P10-1073,N06-2034,0,0.147621,"ing identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et a"
P10-1073,N06-1037,1,0.528018,"o objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. ?? - ?? ) are covered in production rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Especially, when there are enough training data, this is similar to the"
P10-1073,D09-1036,0,0.426661,"important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for discourse analysis. The more recent work from Lin et al. (2009) uses 2-level production rules"
P10-1073,P02-1047,0,0.105277,"of recognizing such relations between text units including identifying and classifying provides important information to other natural language processing systems, such as language generation, document summarization, and question answering. For example, Causal relation can be used to answer more sophisticated, non-factoid ‘Why’ questions. Lee et al. (2006) demonstrates that modeling discourse structure requires prior linguistic analysis on syntax. This shows the importance of syntactic knowledge to discourse analysis. However, most of previous work only deploys lexical and semantic features (Marcu and Echihabi, 2002; Pettibone and PonBarry, 2003; Saito et al., 2006; Ben and James, 2007; Lin et al., 2009; Pitler et al., 2009) with only two exceptions (Ben and James, 2007; Lin et al., 2009). Nevertheless, Ben and James (2007) only uses flat syntactic path connecting connective and arguments in the parse tree. The hierarchical structured information in the trees is not well preserved in their flat syntactic path features. Besides, such a syntactic feature selected and defined according to linguistic intuition has its limitation, as it remains unclear what kinds of syntactic heuristics are effective for disc"
P10-1073,P04-1043,0,0.0228654,"ploying a kernel function to calculate the similarity between two objects directly. In particular, the kernel methods could be very effective at reducing the burden of feature engineering for structured objects in NLP research (Culotta and Sorensen, 2004). This is because a kernel can measure the similarity between two discrete structured objects by directly using the original representation of the objects instead of explicitly enumerating their features. Indeed, using kernel methods to mine structural knowledge has shown success in some NLP applications like parsing (Collins and Duffy, 2001; Moschitti, 2004) and relation extraction (Zelenko et al., 2003; Zhang et al., 2006). However, to our knowledge, the application of such a technique to discourse relation recognition still remains unexplored. Lin et al. (2009) has explored the 2-level production rules for discourse analysis. However, Figure 1 shows that only 2-level sub-tree structures (e.g. ?? - ?? ) are covered in production rules. Other sub-trees beyond 2-level (e.g. ?? - ?? ) are only captured in the tree kernel, which allows tree kernel to further leverage on information from higher dimension space for possible better discrimination. Espe"
P10-1073,prasad-etal-2008-penn,0,0.334151,"from the parse trees for discourse analysis, applying kernel function to the parse tree structures directly. These structural syntactic features, together with other flat features are then incorporated into our composite kernel to capture diverse knowledge for simultaneous discourse identification and classification. The experiment shows that tree kernel is able to effectively incorporate syntactic structural information and produce statistical significant improvements over flat syntactic path feature for the recognition of both explicit and implicit relation in Penn Discourse Treebank (PDTB; Prasad et al., 2008). We also illustrate that tree kernel approach covers more structure information than the production rules, which allows tree kernel to further work on a higher dimensional space for possible better discrimination. Besides, inspired by the linguistic study on tense and discourse anaphor (Webber, 1988), we further propose to incorporate temporal ordering information to constrain the interpretation of discourse relation, which also demonstrates statistical significant improvements for discourse relation recognition on PDTB v2.0 for both explicit and implicit relations. The organization of the re"
P10-1073,P09-1077,0,\N,Missing
P10-1073,P04-1054,0,\N,Missing
P10-1073,J88-2006,0,\N,Missing
P10-1073,P02-1034,0,\N,Missing
P18-1093,K16-1017,0,0.197986,"ep Learning for Sarcasm Detection Deep learning based methods have recently garnered considerable interest in many areas of NLP research. In our problem domain, (Zhang et al., 2016) proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. (Ghosh and Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that exploit personality information (Ghosh and Veale, 2017) and user context (Amir et al., 2016). Novel methods for sarcasm detection such as gaze / cognitive features (Mishra et al., 2016, 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at"
P18-1093,W14-2609,0,0.137106,"e forward, presenting an algorithm strongly based on the intuition that sarcasm arises from a juxtaposition of positive and negative situations. 2.1 Sarcasm Detection Naturally, many works in this area have treated the sarcasm detection task as a standard text classification problem. An extremely comprehensive overview can be found at (Joshi et al., 2017). Feature engineering approaches were highly popular, exploiting a wide diverse range of features such as syntactic patterns (Tsur et al., 2010), sentiment lexicons (Gonz´alez-Ib´anez et al., 2011), ngram (Reyes et al., 2013), word frequency (Barbieri et al., 2014), word shape and pointedness features (Pt´acˇ ek et al., 2014), readability and flips (Rajadesingan et al., 2015), etc. Notably, there have been quite a reasonable number of works that propose features based on similarity and contrast. (Hern´andez-Far´ıas et al., 2015) measured the Wordnet based semantic similarity between words. (Joshi et al., 2015) proposed a framework based on explicit and implicit incongruity, utilizing features based on positive-negative patterns. (Joshi et al., 2016) proposed similarity features based on word embeddings. 1011 2.2 3 Deep Learning for Sarcasm Detection Dee"
P18-1093,D16-1171,0,0.0252525,"o non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment classification (Chen et al., 2016; Yang et al., 2016), aspect-level sentiment analysis (Tay et al., 2018s, 2017b; Chen et al., 2017) and entailment classification (Rockt¨aschel et al., 2015). Co-attention / Bi-Attention (Xiong et al., 2016; Seo et al., 2016) is a form of pairwise attention mechanism that was proposed to model query-document pairs. Intraattention can be interpreted as a self-targetted coattention and is seeing a lot promising results in many recent works (Vaswani et al., 2017; Parikh et al., 2016; Tay et al., 2017a; Shen et al., 2017). The key idea is to model a sequence against itself, learning to attend whil"
P18-1093,D17-1047,0,0.0291043,"sed distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment classification (Chen et al., 2016; Yang et al., 2016), aspect-level sentiment analysis (Tay et al., 2018s, 2017b; Chen et al., 2017) and entailment classification (Rockt¨aschel et al., 2015). Co-attention / Bi-Attention (Xiong et al., 2016; Seo et al., 2016) is a form of pairwise attention mechanism that was proposed to model query-document pairs. Intraattention can be interpreted as a self-targetted coattention and is seeing a lot promising results in many recent works (Vaswani et al., 2017; Parikh et al., 2016; Tay et al., 2017a; Shen et al., 2017). The key idea is to model a sequence against itself, learning to attend while capturing long term dependencies and word-word level interactions. To the best of our knowledge,"
P18-1093,D17-1169,0,0.288157,"d Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that exploit personality information (Ghosh and Veale, 2017) and user context (Amir et al., 2016). Novel methods for sarcasm detection such as gaze / cognitive features (Mishra et al., 2016, 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment classification (Chen et al., 2016; Yang et al., 2016)"
P18-1093,W16-0425,0,0.488709,"en phrases {love, ignored}, {best, drilling} and {movie, asleep} (in the examples above) richly characterize the nature of sarcasm conveyed, i.e., word pairs tend to be contradictory and more often than not, express a juxtaposition of positive and negative terms. This concept is also explored in (Joshi et al., 2015) in which the authors refer to this phenomena as ‘incongruity’. Hence, it would be useful to capture the relationships between selected word pairs in a sentence, i.e., looking in-between. State-of-the-art sarcasm detection systems mainly rely on deep and sequential neural networks (Ghosh and Veale, 2016; Zhang et al., 2016). In these works, compositional encoders such as gated recurrent units (GRU) (Cho et al., 2014) or long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) are often employed, with the input document being parsed one word at a time. This has several shortcomings for the sarcasm detection task. Firstly, there is 1010 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1010–1020 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics no explicit interaction between word pairs,"
P18-1093,D17-1050,0,0.749307,"s based on word embeddings. 1011 2.2 3 Deep Learning for Sarcasm Detection Deep learning based methods have recently garnered considerable interest in many areas of NLP research. In our problem domain, (Zhang et al., 2016) proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. (Ghosh and Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that exploit personality information (Ghosh and Veale, 2017) and user context (Amir et al., 2016). Novel methods for sarcasm detection such as gaze / cognitive features (Mishra et al., 2016, 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on thei"
P18-1093,P11-2102,0,0.764026,"Missing"
P18-1093,P15-2124,0,0.748369,"effective sarcasm detectors can bring about numerous benefits to opinion mining applications. Given the examples, we make a crucial observation - Sarcasm relies a lot on the semantic relationships (and contrast) between individual words and phrases in a sentence. For instance, the relationships between phrases {love, ignored}, {best, drilling} and {movie, asleep} (in the examples above) richly characterize the nature of sarcasm conveyed, i.e., word pairs tend to be contradictory and more often than not, express a juxtaposition of positive and negative terms. This concept is also explored in (Joshi et al., 2015) in which the authors refer to this phenomena as ‘incongruity’. Hence, it would be useful to capture the relationships between selected word pairs in a sentence, i.e., looking in-between. State-of-the-art sarcasm detection systems mainly rely on deep and sequential neural networks (Ghosh and Veale, 2016; Zhang et al., 2016). In these works, compositional encoders such as gated recurrent units (GRU) (Cho et al., 2014) or long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) are often employed, with the input document being parsed one word at a time. This has several shortcomings for"
P18-1093,D16-1104,0,0.152325,"010), sentiment lexicons (Gonz´alez-Ib´anez et al., 2011), ngram (Reyes et al., 2013), word frequency (Barbieri et al., 2014), word shape and pointedness features (Pt´acˇ ek et al., 2014), readability and flips (Rajadesingan et al., 2015), etc. Notably, there have been quite a reasonable number of works that propose features based on similarity and contrast. (Hern´andez-Far´ıas et al., 2015) measured the Wordnet based semantic similarity between words. (Joshi et al., 2015) proposed a framework based on explicit and implicit incongruity, utilizing features based on positive-negative patterns. (Joshi et al., 2016) proposed similarity features based on word embeddings. 1011 2.2 3 Deep Learning for Sarcasm Detection Deep learning based methods have recently garnered considerable interest in many areas of NLP research. In our problem domain, (Zhang et al., 2016) proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. (Ghosh and Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that explo"
P18-1093,D15-1166,0,0.0360348,", 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment classification (Chen et al., 2016; Yang et al., 2016), aspect-level sentiment analysis (Tay et al., 2018s, 2017b; Chen et al., 2017) and entailment classification (Rockt¨aschel et al., 2015). Co-attention / Bi-Attention (Xiong et al., 2016; Seo et al., 2016) is a form of pairwise attention mechanism that was proposed to model query-document pairs. Intraattention can be interpreted as a self-targetted coattention and is seeing a lot promising results in many recent work"
P18-1093,P17-1035,0,0.225618,"Missing"
P18-1093,P16-1104,0,0.339859,"erable interest in many areas of NLP research. In our problem domain, (Zhang et al., 2016) proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. (Ghosh and Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that exploit personality information (Ghosh and Veale, 2017) and user context (Amir et al., 2016). Novel methods for sarcasm detection such as gaze / cognitive features (Mishra et al., 2016, 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation ("
P18-1093,D16-1244,0,0.0925133,"Missing"
P18-1093,P17-1155,0,0.125163,"problem domain, (Zhang et al., 2016) proposed a recurrent-based model with a gated pooling mechanism for sarcasm detection on Twitter. (Ghosh and Veale, 2016) proposed a convolutional long-short-term memory network (CNN-LSTM-DNN) that achieves state-of-the-art performance. While our work focuses on document-only sarcasm detection, several notable works have proposed models that exploit personality information (Ghosh and Veale, 2017) and user context (Amir et al., 2016). Novel methods for sarcasm detection such as gaze / cognitive features (Mishra et al., 2016, 2017) have also been explored. (Peled and Reichart, 2017) proposed a novel framework based on neural machine translation to convert a sequence from sarcastic to non-sarcastic. (Felbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning seq"
P18-1093,D14-1162,0,0.0810531,"ith NLTK5 ’s Tweet tokenizer. Words that only appear once in the entire corpus are removed and marked with the UNK token. Document lengths are truncated at 40, 20, 80 tokens for Twitter, Reddit and Debates dataset respectively. Mentions of other users on the Twitter dataset are replaced by ‘@USER’. Documents with URLs (i.e., containing ‘http’) are removed from the corpus. Documents with less than 5 tokens are also removed. The learning optimizer used is the RMSProp with an initial learning rate of 0.001. The L2 regularization is set to 10−8 . We initialize the word embedding layer with GloVe (Pennington et al., 2014). We use the GloVe model trained on 2B Tweets for the Tweets and Reddit dataset. The Glove model trained on Common Crawl is used for the Debates corpus. The size of the word embeddings is fixed at d = 100 and are fine-tuned during training. In all experiments, we use a development set to select the best hyperparameters. Each model is trained for a total of 30 epochs and the model is saved each time the performance 1015 5 https://nltk.org Model NBOW Vanilla CNN Vanilla LSTM Attention LSTM GRNN (Zhang et al.) CNN-LSTM-DNN (Ghosh and Veale) SIARN (this paper) MIARN (this paper) Tweets (Pt´acˇ ek"
P18-1093,C14-1022,0,0.413329,"Missing"
P18-1093,D13-1066,0,0.651883,"Missing"
P18-1093,N16-1174,0,0.313459,"elbo et al., 2017) proposed a layer-wise training scheme that utilizes emoji-based distant supervision for sentiment analysis and sarcasm detection tasks. 2.3 Attention Models for NLP In the context of NLP, the key idea of neural attention is to soft select a sequence of words based on their relative importance to the task at hand. Early innovations in attentional paradigms mainly involve neural machine translation (Luong et al., 2015; Bahdanau et al., 2014) for aligning sequence pairs. Attention is also commonplace in many NLP applications such as sentiment classification (Chen et al., 2016; Yang et al., 2016), aspect-level sentiment analysis (Tay et al., 2018s, 2017b; Chen et al., 2017) and entailment classification (Rockt¨aschel et al., 2015). Co-attention / Bi-Attention (Xiong et al., 2016; Seo et al., 2016) is a form of pairwise attention mechanism that was proposed to model query-document pairs. Intraattention can be interpreted as a self-targetted coattention and is seeing a lot promising results in many recent works (Vaswani et al., 2017; Parikh et al., 2016; Tay et al., 2017a; Shen et al., 2017). The key idea is to model a sequence against itself, learning to attend while capturing long ter"
P18-1093,C16-1231,0,0.788244,"Missing"
S10-1050,S10-1006,0,0.0938248,"Missing"
S10-1050,P08-1027,0,0.0146298,"uite different in definition of relations and granularities of various applications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practi"
S10-1050,S07-1081,0,0.0168329,"relations and granularities of various applications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practical NLP settings."
S10-1050,P08-1052,0,0.0293068,"ications. That is, there is little agreement on relation inventories. SemEval 2010 Task 8 (Hendrickx et al., 2008) provides a new standard benchmark for semantic relation classification to a wider community, where it defines 9 relations including C AUSE -E FFECT, C OMPONENT-W HOLE, C ONTENT-C ONTAINER, E NTITY-D ESTINATION, E NTITY-O RIGIN, I NSTRUMENT-AGENCY, M EMBER -C OLLECTION, M ESSAGE -T OPIC, 2. Most previous work at SemEval 2007 Task 4 leveraged on external theauri or corpora (whether unannotated or annotated) (Davidov and Rappoport, 2008), (Costello, 2007), (Beamer et al., 2007) and (Nakov and Hearst, 2008) that make the task adaption to different domains and languages more difficult, since they would not have such manually classified or annotated corpus available. From a practical point of view, our system would make use of less resources. 3. Most previous work at Semeval 2007 Task 4 constructed several local classifiers on different algorithms or different feature subsets, one for each relation (Hendrickx et al., 2007) and (Davidov and Rappoport, 2008). Our approach is to build a global classifier for all relations in practical NLP settings. 226 Proceedings of the 5th International Workshop on"
S10-1050,S07-1039,0,\N,Missing
S10-1050,S07-1085,0,\N,Missing
S15-2083,P07-1056,0,0.0540338,"first list, we collect and keep only high frequent opinion targets. For the second list, we consider the counts of individual words in the opinion targets and keep those words that frequently occur as part of an opinion target in the training set. 2.1.4 Head Word From the sentence parse tree, we extract the head word of each word and use it as a feature. 2.1.5 Word Cluster We induce Brown clusters and K-means clusters from two different sources of unlabeled dataset: the Multi-Domain Sentiment Dataset that contains 1 http://nlp.stanford.edu/software/lex-parser.shtml 497 Amazon product reviews (Blitzer et al., 2007)2 , and the Yelp Phoenix Academic Dataset that contains user reviews3 . We also experiment using a third dataset that is created by combining the initial two datasets into one. For Brown clusters4 , we experiment with different datasets, cluster sizes ({100, 200, 500, 1000}), minimum occurrences ({1, 2, 3}) and binary prefix lengths. The best settings to use are determined using 5-fold cross validation. K-means clusters are induced using the word2vec tool (Mikolov et al., 2013)5 . Similarly, among different datasets, word vector sizes ({50, 100, 200, 500, 1000}), cluster sizes ({50, 100, 200,"
S15-2083,N13-1090,0,0.00246909,"omain Sentiment Dataset that contains 1 http://nlp.stanford.edu/software/lex-parser.shtml 497 Amazon product reviews (Blitzer et al., 2007)2 , and the Yelp Phoenix Academic Dataset that contains user reviews3 . We also experiment using a third dataset that is created by combining the initial two datasets into one. For Brown clusters4 , we experiment with different datasets, cluster sizes ({100, 200, 500, 1000}), minimum occurrences ({1, 2, 3}) and binary prefix lengths. The best settings to use are determined using 5-fold cross validation. K-means clusters are induced using the word2vec tool (Mikolov et al., 2013)5 . Similarly, among different datasets, word vector sizes ({50, 100, 200, 500, 1000}), cluster sizes ({50, 100, 200, 500, 1000}), and sub-sampling thresholds ({0.00001, 0.001}), we use 5-fold cross validation to select the best settings. 2.1.6 Name List Generated using Double Propagation For the restaurant domain, we generate a name list of possible opinion targets using the Double Propagation (DP) algorithm (Qiu et al., 2011). The propagation rules are modified from the logic rules presented in Liu et al. (2013), where we write our rules in Prolog and use SWI-Prolog6 as the solver. As the ru"
S15-2083,S14-2004,0,0.0907622,"targets correctly (Slot 1 & 2). Introduction The amount of user-generated content on the web has grown rapidly in recent years, prompting increasing interests in the research area of sentiment analysis and opinion mining. Most previous work is concerned with detecting the overall polarity of a sentence or paragraph, regardless of the target entities (e.g. restaurants) and their aspects (e.g. food). By contrast, the Aspect Based Sentiment Analysis task of SemEval 2014 (SE-ABSA14) is concerned with identifying the aspects of given target entities and the sentiment expressed towards each aspect (Pontiki et al., 2014). The SemEval-2015 Aspect Based Sentiment Analysis (SE-ABSA15) task is a continuation of SE-ABSA14 (Pontiki et al., 2015). The SEABSA15 task features a number of changes that For Slot 1, we model the problem as a multi-class classification problem where binary classifiers are trained to predict the aspect categories. We follow the one-vs-all strategy and train a binary classifier for each category in the training set. Each classifier is trained using sigmoidal feedforward network with 1 hidden layer. For Slot 2, we follow the approach of Toh and Wang (2014) by modeling the problem as a sequent"
S15-2083,S15-2082,0,0.283504,"Missing"
S15-2083,J11-1002,0,0.125277,"Missing"
S15-2083,E99-1023,0,0.0185637,"the probability threshold where we regard the classifier output as positive outcome. Any classifier that returns a probability score greater than the threshold will be added to the output set of categories. The tuned parameter values used are shown in Table 1. Table 2 shows the features used for the restaurant and laptop domain, as well as the 5-fold crossvalidation performances after adding each feature group. 2.2.2 Opinion Target Extraction (Slot 2) Opinion target extraction is modeled as a sequential labeling task, where each word in the sentence is assigned a label using the IOB2 scheme (Sang and Veenstra, 1999). The classifier is trained using Conditional Random Fields (CRF), which has shown to achieve state-of-the-art performances in previous work (Toh and Wang, 2014; Chernyshevich, 2014). We use the CRFsuite tool (Okazaki, 2007) for CRF training and enable negative state and transition features (-p feature.possible states=1 7 https://github.com/JohnLangford/vowpal wabbit/wiki 498 Restaurant Feature F1 Word 0.6245 + Bigram 0.6423 + Name List 0.6608 + Head Word 0.6660 + Word Cluster 0.7038 Laptop Feature F1 Word 0.4520 + Bigram 0.4611 + Head Word 0.4721 + Word Cluster 0.4841 Table 2: 5-fold cross-va"
S15-2083,S14-2038,1,0.886539,"ent expressed towards each aspect (Pontiki et al., 2014). The SemEval-2015 Aspect Based Sentiment Analysis (SE-ABSA15) task is a continuation of SE-ABSA14 (Pontiki et al., 2015). The SEABSA15 task features a number of changes that For Slot 1, we model the problem as a multi-class classification problem where binary classifiers are trained to predict the aspect categories. We follow the one-vs-all strategy and train a binary classifier for each category in the training set. Each classifier is trained using sigmoidal feedforward network with 1 hidden layer. For Slot 2, we follow the approach of Toh and Wang (2014) by modeling the problem as a sequential labeling task, using Conditional Random Fields (CRF) as the training algorithm. For Slot 1 & 2, we perform a simple combination of Slot 1 predictions and Slot 2 predictions. The remainder of this paper is structured as follows. In Section 2, we describe our system in detail, including the feature description and approaches. In Section 3, the official results are presented. Feature ablation results are shown in Section 4. Finally, Section 5 summarizes our work. 496 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages"
S16-1045,P07-1056,0,0.0184585,"omain. One list contains opinion targets that frequently occur in the training data. The other list contains words that often occur as part of an opinion target in the training data. 2.3 Head Word For each word, the head word is extracted from the sentence parse tree and is used as a feature. 2.4 Word Embeddings Word embeddings have shown previously to be beneficial to opinion target extraction, requiring only minimal feature engineering effort (Liu et al., 2015). We trained word embeddings from two unlabeled datasets: the Multi-Domain Sentiment Dataset containing product reviews from Amazon (Blitzer et al., 2007)1 , and the user reviews found in the Yelp Phoenix Academic Dataset2 . Additional word embeddings are also generated from the concatenation of the above two datasets. Two different approaches are used to train the word embeddings. The first approach uses the gensim3 implementation of the word2vec tool (Mikolov et al., 2013)4 . We experiment with different vector sizes, window sizes, minimum occurrences and subsampling thresholds. Besides using the training data to generate name lists, we used the unsupervised Double Propagation (DP) algorithm (Qiu et al., 2011) to generate candidate opinion ta"
S16-1045,D15-1168,0,0.264446,"d using 5-fold cross validation. 2.2 2.6 2.1 Word Name List Two name lists of opinion targets are generated from the training data of the restaurant domain. One list contains opinion targets that frequently occur in the training data. The other list contains words that often occur as part of an opinion target in the training data. 2.3 Head Word For each word, the head word is extracted from the sentence parse tree and is used as a feature. 2.4 Word Embeddings Word embeddings have shown previously to be beneficial to opinion target extraction, requiring only minimal feature engineering effort (Liu et al., 2015). We trained word embeddings from two unlabeled datasets: the Multi-Domain Sentiment Dataset containing product reviews from Amazon (Blitzer et al., 2007)1 , and the user reviews found in the Yelp Phoenix Academic Dataset2 . Additional word embeddings are also generated from the concatenation of the above two datasets. Two different approaches are used to train the word embeddings. The first approach uses the gensim3 implementation of the word2vec tool (Mikolov et al., 2013)4 . We experiment with different vector sizes, window sizes, minimum occurrences and subsampling thresholds. Besides usin"
S16-1045,N13-1090,0,0.0168726,"ings have shown previously to be beneficial to opinion target extraction, requiring only minimal feature engineering effort (Liu et al., 2015). We trained word embeddings from two unlabeled datasets: the Multi-Domain Sentiment Dataset containing product reviews from Amazon (Blitzer et al., 2007)1 , and the user reviews found in the Yelp Phoenix Academic Dataset2 . Additional word embeddings are also generated from the concatenation of the above two datasets. Two different approaches are used to train the word embeddings. The first approach uses the gensim3 implementation of the word2vec tool (Mikolov et al., 2013)4 . We experiment with different vector sizes, window sizes, minimum occurrences and subsampling thresholds. Besides using the training data to generate name lists, we used the unsupervised Double Propagation (DP) algorithm (Qiu et al., 2011) to generate candidate opinion targets and collect them into a list. We adjust the logical rules stated in Liu et al. (2013) to derive our own propagation rules written in Prolog. The SWI-Prolog7 is used as the solver. One issue with our rules is that it can only identify single-word targets. Thus, we check each identified target and include any consecutiv"
S16-1045,D14-1162,0,0.0989405,"our system are described. Section 3 presents the detailed machine learning approaches. Section 4 and Section 5 show the official evaluation results and feature ablation results respectively. Finally, Section 6 summa282 Proceedings of SemEval-2016, pages 282–288, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics rizes our work. 2 Features Our system used a variety of features which are briefly described in the following subsections. Most of the features used are the same as the features used in Toh and Su (2015). The second approach uses the GloVe tool (Pennington et al., 2014)5 . By varying the minimum count, window size and vector size, different embedding files are generated. The best embedding files to use are selected using 5-fold cross validation. 2.5 Word Cluster Each word in a sentence is used as a feature. Additional word context is used for different slots: for Slot 1, all word bigram context found in a sentence are also used; for Slot 2, the previous word and next word context are also used. We further processed the embedding files described in Section 2.4 by generating K-means clusters from them. Specifically, the K-means clusters are generated using the"
S16-1045,S15-2082,0,0.0770637,"Missing"
S16-1045,J11-1002,0,0.15495,"product reviews from Amazon (Blitzer et al., 2007)1 , and the user reviews found in the Yelp Phoenix Academic Dataset2 . Additional word embeddings are also generated from the concatenation of the above two datasets. Two different approaches are used to train the word embeddings. The first approach uses the gensim3 implementation of the word2vec tool (Mikolov et al., 2013)4 . We experiment with different vector sizes, window sizes, minimum occurrences and subsampling thresholds. Besides using the training data to generate name lists, we used the unsupervised Double Propagation (DP) algorithm (Qiu et al., 2011) to generate candidate opinion targets and collect them into a list. We adjust the logical rules stated in Liu et al. (2013) to derive our own propagation rules written in Prolog. The SWI-Prolog7 is used as the solver. One issue with our rules is that it can only identify single-word targets. Thus, we check each identified target and include any consecutive noun words right before the target. 3 Approaches This section describes our approaches used to generate the predictions for the different slots. The machine learning system is based on our previous work (Toh and Su, 2015) and is extended to"
S16-1045,S15-2079,0,0.066775,"Missing"
S16-1045,S15-2083,1,0.903382,"timent of a sentence or paragraph. However, such approach is unable to handle conflicting sentiment for different aspects of the same entity. Hence, a more fine-grained approach, known as Aspect-Based Sentiment Analysis (ABSA), is proposed. The goal is to correctly identify the aspects of entities and the polarity expressed for each aspect. The SemEval-2016 Aspect Based Sentiment Analysis (SE-ABSA16) task is a continuation of the same task in 2015 (Pontiki et al., 2015). Besides sentence-level ABSA (Subtask 1), it provides Our work is based on our previous machine learning system described in Toh and Su (2015), enhanced using additional features learned from neural networks. For Slot 1, we treat the problem as a multi-class classification problem where aspect categories are predicted via a set of binary classifiers. The one-vs-all strategy is used to train a binary classifier for each category found in the training data. Each classifier is trained using a single layer feedforward network. We enhance the system by adding neural network features learned from a Deep Convolutional Neural Network system. For Slot 2, we treat the problem as a sequential labeling task, where sequential labeling classifier"
S16-1045,S16-1002,0,\N,Missing
W00-0737,P98-1010,0,0.0353167,"Missing"
W00-0737,C92-3126,0,0.0394535,"Missing"
W00-0737,A88-1019,0,0.200947,"Missing"
W00-0737,W96-0102,0,0.0702374,"Missing"
W00-0737,W99-0707,0,0.077773,"Missing"
W00-0737,W95-0107,0,0.17493,"Missing"
W00-0737,C98-1010,0,\N,Missing
W00-1309,P98-1010,0,0.0393795,"Missing"
W00-1309,C92-3126,0,0.0219637,"Missing"
W00-1309,C92-3150,0,0.105101,"Missing"
W00-1309,W99-0707,0,0.0376181,"Missing"
W00-1309,J93-2004,0,0.0451101,"le and efficient processing algorithms. Recently, many researchers have looked at text chunking in two different ways: Some 71 entries and make it possible to further improve the accuracy by merging different contextdependent lexicons into one after automatic analysis of the chunking errors. Finally, the conclusion is given. Tin and the given token sequence G~. By assuming that the mutual information between G~ and T1~ is equal to the summation of mutual information between G~ and the individual tag ti(l_&lt;i_&lt;n ) : The data used for all our experiments is extracted from the PENN&quot; WSJ Treebank (Marcus et al. 1993) by the program provided by Sabine Buchholz from Tilbug University. We use sections 00-19 as the training data and 20-24 as test data. Therefore, the performance is on large scale task instead of small scale task on CoNLL-2000 with the same evaluation program. log /3 2. precision + recall 1 = ~ log e(Tln ). P(G~) i=1 P(t,, G~) P(t,). P(G? ) or n MI(T~ ~ , G~n ) = ~ MI(t,, G? ) , i=l For evaluation of our results, we use the precision and recall measures. Precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actua"
W00-1309,W96-0102,0,\N,Missing
W00-1309,W99-0629,0,\N,Missing
W00-1309,P93-1003,0,\N,Missing
W00-1309,C98-1010,0,\N,Missing
W03-1307,W02-2025,0,0.0528396,"Missing"
W03-1307,W00-0904,0,0.0241239,"Missing"
W03-1307,W02-2029,0,0.0190907,"bution based on the state transition probabilities. Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). 3 3.1 Feature Set Simple Deterministic Features (Fsd) The purpose of simple deterministic features is to capture the capitalization, digitalization and word formation information. This kind of features have been widely used in both newswire NER system, such as (Zhou and Su 2002), and biomedical NER system, such as (Nobata et al. 1999; Gaizauskas et al. 2000; Collier et al. 2000; Takeuchi and Collier 2002; Kazama et al. 2002). Based on the characteristics of biomedical NEs, we designed simple deterministic features manually. Table 1 shows the simple deterministic features with descending order of priority. Fsd Name Comma Dot LRB RRB LSB RSB RomanDigit GreekLetter StopWord ATCGsequence OneDigit AllDigits DigitCommaDigit DigitDotDigit OneCap AllCaps CapLowAlpha CapMixAlpha LowMixAlpha AlphaDigitAlpha AlphaDigit DigitAlphaDigit DigitAlpha Example , . ( ) [ ] II Beta in, at AACAAAG 5 60 1,25 0.5 T CSF All IgM kDa H2A T4 6C2 19D Table 1: Simple deterministic features From Table 1, we can find that:"
W03-1307,C00-1030,0,0.300876,"on As the research in biomedical domain has grown rapidly in recent years, a huge amount of nature language resources have been developed and become a rich knowledge base. The technique of named entity (NE) recognition (NER) is strongly demanded to be applied in biomedical domain. Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998). However, compared with those in newswire domain, these systems haven’t got high performance. It is probably because of the following factors of biomedical NE (Zhang et al. 2003): 1. Some modifiers are often before basic NEs, e.g. activated B cell lines, and sometimes biomedical NEs are very long, e.g. 47 kDa sterol regulatory element binding factor. This kind of factor highlights the difficulty for identifying the boundary of NE. 2. Two or more NEs share one head noun by using conjunction or disjunction construction, e.g. 91 and"
W03-1307,P02-1060,1,0.692033,". Our experiments on GENIA V3.0 and GENIA V1.1 achieve the 66.1 and 62.5 F-measure respectively, which outperform the previous best published results by 8.1 F-measure when using the same training and testing data. 1 Introduction As the research in biomedical domain has grown rapidly in recent years, a huge amount of nature language resources have been developed and become a rich knowledge base. The technique of named entity (NE) recognition (NER) is strongly demanded to be applied in biomedical domain. Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998). However, compared with those in newswire domain, these systems haven’t got high performance. It is probably because of the following factors of biomedical NE (Zhang et al. 2003): 1. Some modifiers are often before basic NEs, e.g. activated B cell lines, and sometimes biomedical NEs are very long, e.g. 47"
W03-1307,W02-0301,0,\N,Missing
W03-1711,H91-1060,0,0.0191495,"Missing"
W03-1711,P97-1003,0,0.0280652,"Missing"
W03-1711,W00-0726,0,0.0803629,"Missing"
W03-1711,W00-0737,1,0.888538,"Missing"
W03-1711,W00-1309,1,\N,Missing
W04-1219,P02-1060,1,0.55395,"Missing"
W04-1219,W03-1307,1,\N,Missing
W10-4326,W05-0613,0,0.0143873,". Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 200"
W10-4326,W01-1605,0,0.141734,"Missing"
W10-4326,W03-1210,0,0.0256562,"lation recognition. Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random clas"
W10-4326,N04-1020,0,0.0389146,"Missing"
W10-4326,D09-1036,0,0.101988,"Missing"
W10-4326,P02-1047,0,0.288883,"tal results showed that the two methods achieved comparable F-scores to the state-of-art methods. It indicates that the method using language model to predict connectives is very useful in solving this task. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 corpora, their approaches are not very useful in the real word. Another line of research is to use the unsupervised methods on unhuman-annotated corpus. (Marcu and Echihabi, 2002) used several patterns to extract instances of discourse relations such as contrast and elaboration from unlabeled corpora. Then they used word-pairs between arguments as features for building classification models and tested their model on artificial data for implicit relations. Subsequently other studies attempt to extend the work of (Marcu and Echihabi, 2002). (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracy on synthetic data. (Goldensohn, 2007) extended the work of (Marcu and"
W10-4326,P09-1077,0,0.308125,"method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and cross-argument word pairs. Although both of two methods achieved the state of the art performance for automatical recognition of implicit discourse relations, due to lack"
W10-4326,P09-2004,0,0.0519904,"Missing"
W10-4326,W06-0305,0,0.0185618,"mplicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse featur"
W10-4326,prasad-etal-2008-penn,0,0.288617,"Missing"
W10-4326,N06-2034,0,0.103242,"Missing"
W10-4326,N03-1030,0,0.0457007,"ves predicted using a language model, for implicit relation recognition. Moreover, this method can be applied in both supervised and unsupervised ways by generating features on labeled and unlabeled training data and then performing implicit discourse connectives recognition. Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing"
W10-4326,W06-1317,0,0.310693,"Missing"
W10-4326,C08-2022,0,\N,Missing
W15-4321,N15-1075,0,0.0465002,"ch time and select the best cluster file from the subset to create a new cluster feature. The procedure is repeated for a new subset of cluster files, until no (or negligible) improvement is obtained. Our final settings use one Brown cluster feature and six K-means cluster features (for both 10types and notypes settings). Features This section briefly describes the features used in our system. Besides the features commonly used in traditional NER systems, we focus on the use of word cluster features that have shown to be effective in previous work (Ratinov and Roth, 2009; Turian et al., 2010; Cherry and Guo, 2015). 3.1 Word Feature The current word and its lowercase format are used as features. To provide additional context information, the previous word and next word (in original format) are also used. 3.2 Orthographic Features Orthographic features based on regular expressions are often used in NER systems. We only use the following two orthographic features: InitialCap ([A-Z][a-z].*) and AllCaps ([A-Z]+). In addition, the first character and last two characters of each word are used as features. 3.3 Gazetteer Feature The current word is matched with entries in the Freebase entity lists and the featu"
W15-4321,P12-3005,0,0.0600623,"ACL 2015 Workshop on Noisy User-generated Text, pages 141–145, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 2014)1 , (2) English Gigaword Fifth Edition2 , and (3) raw tweets collected between the period of March 2015 and April 2015. For English Gigaword, all articles of story type are collected and tokenized. Further preprocessing is performed by following the cleaning step described in Turian et al. (2010). This results in a corpus consisting of 76 million sentences. The collected raw tweets are tokenized3 and non-English tweets are removed using langid.py (Lui and Baldwin, 2012), resulting in a total of 14 million tweets. 3 Brown clusters are generated using the implementation by Percy Liang4 . We experiment with different cluster sizes ({100, 200, 500, 1000}), resulting in different cluster files for each of the corpora. For each cluster file, different minimum occurrences ({5, 10, 20}) and binary prefix lengths ({4, 6, · · · , 14, 16}) are tested. For each word in the tweet, its corresponding binary prefix string representation is used as the feature value. K-means clusters are generated using two different methods. The first method uses the word2vec tool (Mikolov"
W15-4321,N13-1090,0,0.0204607,"n, 2012), resulting in a total of 14 million tweets. 3 Brown clusters are generated using the implementation by Percy Liang4 . We experiment with different cluster sizes ({100, 200, 500, 1000}), resulting in different cluster files for each of the corpora. For each cluster file, different minimum occurrences ({5, 10, 20}) and binary prefix lengths ({4, 6, · · · , 14, 16}) are tested. For each word in the tweet, its corresponding binary prefix string representation is used as the feature value. K-means clusters are generated using two different methods. The first method uses the word2vec tool (Mikolov et al., 2013)5 . By varying the minimum occurrences ({5, 10, 20}), word vector size ({50, 100, 200, 500, 1000}), cluster size ({50, 100, 200, 500, 1000}) and sub-sampling threshold ({0.00001, 0.001}), different cluster files are generated and tested. Similar to the Brown cluster feature, the name of the cluster that each word belongs to is used as the feature value. The second method uses the GloVe tool to generate global vectors for word representation6 . As the GloVe tool does not output any form of clusters, K-mean clusters are generated from the global vectors using the K-means implementation from Apac"
W15-4321,D14-1162,0,0.0776075,"Missing"
W15-4321,W09-1119,0,0.0250597,"only test a random subset of cluster files each time and select the best cluster file from the subset to create a new cluster feature. The procedure is repeated for a new subset of cluster files, until no (or negligible) improvement is obtained. Our final settings use one Brown cluster feature and six K-means cluster features (for both 10types and notypes settings). Features This section briefly describes the features used in our system. Besides the features commonly used in traditional NER systems, we focus on the use of word cluster features that have shown to be effective in previous work (Ratinov and Roth, 2009; Turian et al., 2010; Cherry and Guo, 2015). 3.1 Word Feature The current word and its lowercase format are used as features. To provide additional context information, the previous word and next word (in original format) are also used. 3.2 Orthographic Features Orthographic features based on regular expressions are often used in NER systems. We only use the following two orthographic features: InitialCap ([A-Z][a-z].*) and AllCaps ([A-Z]+). In addition, the first character and last two characters of each word are used as features. 3.3 Gazetteer Feature The current word is matched with entrie"
W15-4321,D11-1141,0,0.299437,"nning of the new millennium, user-generated content from the social media websites such as Twitter and Weibo presents a huge compilation of informative but noisy and informal text. This rapidly growing text collection becomes more and more important for NLP tasks such as sentiment analysis and emerging topic detection. However, standard NER system trained on formal text does not work well on this new and challenging style of text. Therefore, adapting the NER system to the new and challenging Twitter 2 External Resources External resources have shown to improve the performances of Twitter NER (Ritter et al., 2011). Our system uses a variety of external resources, either publicly available, or collected and preprocessed by us. 2.1 Freebase Entity Lists We use the Freebase entity lists provided by the task organizers. For some of the lists that are not provided (e.g. a list of sports facilities), we manually collect them by calling the appropriate Freebase API. 2.2 Unlabeled Corpora We gather unlabeled corpora from three different sources: (1) Pre-trained word vectors generated using the GloVe tool (Pennington et al., 141 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 141–145, c"
W15-4321,P10-1040,0,0.36632,"beled Corpora We gather unlabeled corpora from three different sources: (1) Pre-trained word vectors generated using the GloVe tool (Pennington et al., 141 Proceedings of the ACL 2015 Workshop on Noisy User-generated Text, pages 141–145, c Beijing, China, July 31, 2015. 2015 Association for Computational Linguistics 2014)1 , (2) English Gigaword Fifth Edition2 , and (3) raw tweets collected between the period of March 2015 and April 2015. For English Gigaword, all articles of story type are collected and tokenized. Further preprocessing is performed by following the cleaning step described in Turian et al. (2010). This results in a corpus consisting of 76 million sentences. The collected raw tweets are tokenized3 and non-English tweets are removed using langid.py (Lui and Baldwin, 2012), resulting in a total of 14 million tweets. 3 Brown clusters are generated using the implementation by Percy Liang4 . We experiment with different cluster sizes ({100, 200, 500, 1000}), resulting in different cluster files for each of the corpora. For each cluster file, different minimum occurrences ({5, 10, 20}) and binary prefix lengths ({4, 6, · · · , 14, 16}) are tested. For each word in the tweet, its correspondin"
