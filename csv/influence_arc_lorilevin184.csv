1995.tmi-1.13,P95-1016,0,0.062519,"rman, and English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing."
1995.tmi-1.13,E95-1026,0,0.0140163,"English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing. Thus, comp"
1995.tmi-1.13,1995.tmi-1.15,0,0.606961,"agram is shown in Figure 2.1 Processing starts with speech input in the source language. Recognition of the speech signal is done with acoustic modeling methods, constrained by a language model. The output of speech recognition is a word lattice. We prefer working with word lattices rather than the more common approach of processing N-best lists of hypotheses. An N-best list may be largely redundant and can be efficiently represented in the form of a lattice. Using a lattice parser can thus reduce time and space 1 Another approach being pursued in parallel in the Janus project is described in [10] 174 175 complexity relative to parsing a corresponding N-best list. Selection of the correct path through the lattice is accomplished during parsing when more information is available. Lattices, however, are potentially inefficient because of their size. We apply four steps to make them more tractable ([11]). The first step involves cleaning the lattice by mapping all non-human noises and pauses into a generic pause. Consecutive pauses are then adjoined to one long pause. The resulting lattice contains only linguistically meaningful information. The lattice is then broken at points where no h"
1995.tmi-1.13,1993.iwpt-1.12,1,0.826786,"et of sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice ar"
1995.tmi-1.13,P94-1045,1,0.890553,"sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are comb"
1995.tmi-1.13,P92-1025,0,0.0443447,"a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are combined, yielding a list of ILT sequences that represent the possible sentences in a full multi-sentence turn. An ILT n-gram is applied to each such list to determine the probability of each sequence of sentences. The discourse processor, based on Lambert’s work ([14, 15]), disambiguates the speech act of each sentence, normalizes temporal expressions, and incorporates the sentence into a discourse plan tree. The discourse processor's focusing heuristics and plan operators eliminate some ambiguity by filtering out hypotheses that do not fit into the current discourse context. The discourse component also updates a calendar in the dynamic discourse memory to keep track of what the speakers have said about their schedules. As processing continues, the N-best hypotheses for sequences of ILTs in a multisentence turn are sent to the generator. The generation output"
1996.amta-1.30,1993.iwpt-1.12,1,0.894049,"Missing"
1996.amta-1.30,P94-1045,1,0.845094,"Missing"
1996.amta-1.30,P81-1022,0,0.0225358,"Missing"
2001.mtsummit-road.7,J90-2002,0,0.143628,"Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using expone"
2001.mtsummit-road.7,C96-1030,1,0.787634,"rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are using the EBMT engine that was developed for the Diplomat and Tongues systems (Brown, 1996)1. In addition, we plan to develop statistical techniques for robust MT with sparse data using exponential models a"
2001.mtsummit-road.7,jones-havrilla-1998-twisted,0,0.0712645,"ng indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. For corpus-based MT, we are u"
2001.mtsummit-road.7,P98-2160,0,0.151102,"the internet without requiring indigenous people to give up their languages. MT additionally facilitates the design of educational programs in endangered languages, which can be a tool for their documentation and preservation. Several low-cost or rapid deployment MT methods have been proposed, most of which are data-intensive, depending on the existence of large corpora (Somers 1997; Al-Onaizan et al., 1999). An alternative approach for low-density languages is to learn MT rules or statistics from a smaller amount of data that is systematically elicited from a native speaker (Nirenburg, 1998; Nirenburg and Raskin, 1998; Jones and Havrilla, 1998). The NICE project (Native language Interpretation and Communication Environment) plans to combine into a multi-engine system both corpus-based MT (Al-Onaizan, et al. 1999; Brown, et al. 1990; Brown, 1996) and a new elicitation-based approach for automatic inference of transfer rules when a corpus is not available. Our vision for MT of the future includes an MT system that is omnivorous in the sense that it will use whatever resources (texts, linguists, native speakers) are most readily available and, in the extreme case, can be trained easily by a native speaker. Fo"
2001.mtsummit-road.7,sheremetyeva-nirenburg-2000-towards,0,0.0663845,"systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-density languages as a way of providing access to government, education, healthc"
2001.mtsummit-road.7,1997.tc-1.13,0,0.046319,"gorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages. Keywords Low-density languages, minor languages, feature detection, version space learning 1. Introduction and Motivation Recently, efforts in machine translation have spread in two directions. Long-term, high-cost development cycles have given way to research on how to build MT systems for new languages quickly and cheaply (Somers, 1997; Nirenburg, 1998; Nirenburg & Raskin 1998; Sherematyeva & Nirenburg, 2000, Jones & Havrilla 1998). Rapid deployment of MT can be useful in situations such as crises in which time and money are short, but more importantly, lowering the cost of MT has, in turn, opened the option of building MT systems for languages that do not have enough speakers to financially support a costly development process. (See Frederking, to appear; Frederking, Rudnicky, Hogan 1997; and the SALTMIL discussion group, http://193.2.100.60/SALTMIL). There is now increasing awareness of the importance of MT for low-densit"
2001.mtsummit-road.7,C98-2155,0,\N,Missing
2002.tmi-papers.17,jones-havrilla-1998-twisted,0,0.0505006,"r rules for the desired language pair. 1 Introduction In recent years, much of machine translation research has focused on two issues: portability to new languages, and developing machine translation systems rapidly. Since human expertise on rare languages may be in short supply, and human development time can be lengthy, automated learning of statistics or rules has been critical to both language portability and rapid development. Automated methods have typically been trained on uncontrolled parallel corpora. However, a minority of projects (Nirenburg (1998); Sherematyeva & Nirenburg (2000); Jones & Havrilla (1998)) have addressed automated learning of translation rules from a controlled corpus of carefully elicited sentences. Learning from controlled elicitation, like any method of automatic learning, is useful for rare languages that are not spoken by any computational linguists 1 . Native speaker informants provide data as required by the learning algorithm, but do not need to have technical knowledge. The resulting systems are automatically learned, but also consist of human-readable rules that can be extended and modified. In this paper we will review the challenges of automated rule learning from"
2002.tmi-papers.17,P98-2160,0,0.0276122,"Native speaker informants provide data as required by the learning algorithm, but do not need to have technical knowledge. The resulting systems are automatically learned, but also consist of human-readable rules that can be extended and modified. In this paper we will review the challenges of automated rule learning from controlled corpora in the context of our AVENUE machine translation project. While some of the issues we encounter are specific to our project, others are common to all systems that automatically elicit a controlled corpus. Other work in this area includes Nirenburg (1998), Nirenburg & Raskin (1998), and Sherematyeva & Nirenburg (2000), which describe the construction of a controlled corpus based on linguistic research across languages. They compiled a list of linguistic features (such as number) 1 Controlled elicitation systems can also be used for major languages in order to learn an MT system from a small amount of data. together with the possible values these features take on across languages (such as singular, dual, plural, and paucal). A bilingual speaker then provides the system with necessary information regarding these features, namely what values a certain feature can take in t"
2002.tmi-papers.17,sheremetyeva-nirenburg-2000-towards,0,0.0368235,"semi-automatically infer transfer rules for the desired language pair. 1 Introduction In recent years, much of machine translation research has focused on two issues: portability to new languages, and developing machine translation systems rapidly. Since human expertise on rare languages may be in short supply, and human development time can be lengthy, automated learning of statistics or rules has been critical to both language portability and rapid development. Automated methods have typically been trained on uncontrolled parallel corpora. However, a minority of projects (Nirenburg (1998); Sherematyeva & Nirenburg (2000); Jones & Havrilla (1998)) have addressed automated learning of translation rules from a controlled corpus of carefully elicited sentences. Learning from controlled elicitation, like any method of automatic learning, is useful for rare languages that are not spoken by any computational linguists 1 . Native speaker informants provide data as required by the learning algorithm, but do not need to have technical knowledge. The resulting systems are automatically learned, but also consist of human-readable rules that can be extended and modified. In this paper we will review the challenges of auto"
2002.tmi-papers.19,W00-0308,0,0.0224097,"rees to fit L2 sentences Insert missing lexical items and correct linear ordering of branches in the L2 trees Print grammar rules from L2 trees Tasks Done by Grammar Writer Tasks Done by Existing MT system Tasks Done by Native Speaker Tasks Done by Grammar Tools Proposed in this Paper Figure 1: Grammar Adaptation Process information. Finally, grammar induction tools link these two products together to form a forest of semantic parse trees and a grammar in the new language can be read from it. In the context of induction tools at large, our approach lies closer to the interactive techniques of Gavalda (2000) than to unsupervised approaches such as de Marcken’s (1995) and Lee’s (1996). In Section 2 we discuss the steps of this grammar development process in detail. Section 3 describes the tools that are used to induce grammar rules from translation examples and the Machine Translation system that we use in our experiments. Although our research into this topic is continuing, we have used these tools to develop a small grammar for Polish and we describe the experimental results of this feasibility study in Section 4. Section 5 concludes the paper. 2 Approach The grammar development process can be c"
2002.tmi-papers.19,H01-1018,1,0.838352,"Missing"
2002.tmi-papers.19,W95-0102,0,0.0602163,"Missing"
2004.eamt-1.14,J90-2002,0,0.23859,"ransfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpu"
2004.eamt-1.14,J93-2003,0,0.0115785,"encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods requi"
2004.eamt-1.14,1997.tmi-1.13,0,0.0783073,"stem to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel text for the two languages – on the order of magnitude of a million words or more. Such resources are curre"
2004.eamt-1.14,P02-1040,0,0.0790722,"Missing"
2004.eamt-1.14,2001.mtsummit-road.7,1,0.788397,"al of learning compositional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures"
2004.eamt-1.14,2002.tmi-papers.17,1,0.666028,"tional syntactic transfer rules. For example, simple noun phrases are elicited before prepositional phrases and simple sentences, so that during rule learning, the system can detect cases where transfer rules for NPs can serve as components within higher-level transfer rules for PPs and sentence structures. The current controlled elicitation corpus contains about 2000 phrases and sentences. It is by design very limited in vocabulary. A more detailed description of the elicitation corpus, the elicitation process and the interface tool used for elicitation can be found in (Probst et al, 2001), (Probst and Levin, 2002). 4. Automatic Transfer Rule Learning The rule learning system takes the elicited, wordaligned data as input. Based on this information, it then infers syntactic transfer rules. The learning system also learns the composition of transfer rules. In the compositionality learning stage, the learning system identifies cases where transfer rules for “lower-level” constituents (such as NPs) can serve as components within “higher-level” transfer rules (such as PPs and sentence structures). This process generalizes the applicability of the learned transfer rules and captures the compositional makeup o"
2004.eamt-1.14,2003.mtsummit-papers.53,1,0.816612,"ich four English reference translations are available. The following systems were evaluated in the experiment: 1. Three versions of the Hindi-to-English XFER system: 1a. XFER with No Grammar: the XFER system with no syntactic transfer rules (i.e. only lexical phrase-to-phrase matches and word-toword lexical transfer rules, with and without morphology). 1b. XFER with Learned Grammar: The XFER system with automatically learned syntactic transfer rules. 1c. XFER with Manual Grammar: The XFER system with the manually developed syntactic transfer rules. 2. SMT: The CMU Statistical MT (SMT) system (Vogel et al, 2003), trained on the limited-data parallel text resources. 3. EBMT: The CMU Example-based MT (EBMT) system (Brown, 1997), trained on the limiteddata parallel text resources. 4. MEMT: A “multi-engine” version that combines the lattices produced by the SMT system, and the XFER system with manual grammar. The decoder then selects an output from the joint lattice. Performance of the systems was measured using the NIST scoring metric (Doddington, 2002), as well as the BLEU score (Papineni et al, 2002). In order to validate the statistical significance of the differences in NIST and BLEU scores, we appl"
2004.eamt-1.14,P01-1067,0,0.0309867,"he source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources. 1. Introduction Corpus-based Machine Translation (MT) approaches such as Statistical Machine Translation (SMT) (Brown et al, 1990), (Brown et al, 1993), (Vogel and Tribble, 2002), (Yamada and Knight, 2001), (Papineni et al, 1998), (Och and Ney, 2002) and Example-based Machine Translation (EBMT) (Brown, 1997), (Sato and Nagao, 1990) have received much attention in recent years, and have significantly improved the state-of-the-art of Machine Translation for a number of different language pairs. These approaches are attractive because they are fully automated, and require orders of magnitude less human labor than traditional rulebased MT approaches. However, to achieve reasonable levels of translation performance, the corpus-based methods require very large volumes of sentence-aligned parallel tex"
2004.eamt-1.14,C90-3044,0,\N,Missing
2004.eamt-1.14,P02-1038,0,\N,Missing
2005.mtsummit-posters.10,W04-0107,1,0.823896,"igure 1: The elicitation tool is used by the informant for translation and alignment. Sentences are presented individually and can be annotated with context information when necessary. for quick design and implementation of elicitation corpora. Furthermore, we will look at a use of these methods to create a specific kind of corpus called a typological-functional corpus. This type of corpus is designed to elicit a range of language features (for example, tense, person, number) and explore the way those features are manifested in a target language. 2 and morphological paradigms is described in (Monson et al. 2004). Rule refinement via interaction with a consultant is described in (Font-Llitjos et al. 2005). At run time, the AVENUE system consists of a transfer engine and a decoder. The transfer engine encompases analysis, transfer, and generation and produces a large lattice of possible translations. The decoder uses statistical techniques to zero in on the best scoring hypothesis. (Lavie et al. 2003) The AVENUE Project 3 The work described in this paper takes place in the context of the AVENUE machine translation project1. AVENUE is focused on the development of machine translation systems for low-res"
2005.mtsummit-posters.10,2001.mtsummit-road.7,1,0.88585,"Missing"
2005.mtsummit-posters.10,2005.eamt-1.13,0,0.0327725,"Missing"
2007.tmi-papers.1,N06-2002,1,0.909934,"of data if the data is highly structured (Lavie et al. 2003). The elicitation corpus is therefore designed to produce highly structured data. Each sentence is designed to elicit a specific morphosyntactic property of the language, and sentences are organized into minimal pairs (e.g., A tree is falling and A tree fell) to compare the effects of changing one grammatical feature at a time. Probst (2005) describes automatic rule learning from elicited data. A small sample of elicitation sentences is included in the list below. A more detailed description of the elicitation corpus can be found in Alvarez et al, (2006). • • • • • • • • • Mary is writing a book for John. Who let him eat the sandwich? Who had the machine crush the car? They did not make the policeman run. Our brothers did not destroy files. He said that there is not a manual. The teacher who wrote a textbook left. The policeman chased the man who was a thief. Mary began to work. Each sentence in the elicitation corpus is associated with a set of featurevalue pairs, which represent the meaning elements that may be reflected in the 2 morphosyntax of the language. Figure 1 shows an example of an elicitation sentence and its feature structure. As"
2007.tmi-papers.1,I05-2035,0,0.0264337,"variety of reasons, it was not practical to train our statistical transfer system on this data. We therefore assessed the impact of these elicitation errors by training two EBMT systems on our Thai data. One trained on our original unsupervised corpus and the other trained on a corpus corrected of elicitation errors. This evaluation is described in section 6. 3 Related Work Two other projects that we know of formulate grammars based on elicited data. In addition to the Boas system mentioned above, which attempts to train naïve informants to provide linguistic information, the Grammar Matrix (Bender and Flickinger, 2005) collects facts like the existence of subject-verb agreement from a field worker and then automatically produces an HPSG grammar for the language. Both of these use knowledge that a trained human has put into technical linguistic form. In contrast, our approach analyzes translations of elicitation corpus sentences, and the underlying feature structures they represent, to derive the linguistic facts about the language automatically. 3 The Corpus and Support Materials Our elicitation corpus is a monolingual corpus of 3124 English sentences. We designed it to be translated into any human language"
2007.tmi-papers.1,C96-1030,0,0.0338486,"ill conclude by discussing the implications of using linguistically naïve consultants as a resource for building MT systems. 2 Background The AVENUE project has two related foci: building MT systems in lowresource scenarios, and making robust, hybrid MT systems using combinations of deep linguistic knowledge and statistical techniques. The hybrid system is a statistical transfer system (Lavie et al. 2004), which makes use of transfer rules as well as a statistical decoder. The rules can be written by hand, or learned automatically (Probst 2005). The AVENUE system also includes an EBMT system (Brown 1996), in order to use any pre-existing parallel texts that do happen to be available. One hypothesis of the AVENUE work for low-resource scenarios is that MT systems can be learned from small amounts of data if the data is highly structured (Lavie et al. 2003). The elicitation corpus is therefore designed to produce highly structured data. Each sentence is designed to elicit a specific morphosyntactic property of the language, and sentences are organized into minimal pairs (e.g., A tree is falling and A tree fell) to compare the effects of changing one grammatical feature at a time. Probst (2005)"
2009.eamt-1.2,W09-2301,0,0.0132665,"ource and low resource languages, including two indigenous Western Hemisphere languages, Mapudungun (Chile) and I˜nupiaq (Alaska). The full AVENUE framework includes several steps: (1) elicitation of data from native speakers, (2) automatic learning of transfer rules in a unification based synchronous grammar formalism based on the elicited data, (3)optional hand written transfer rules, (4) decoding, and (5) translation correction (as described above). For high resource languages, other techniques may be used such as statistical word alignment and extraction of syntactic phrases (Lavie, 2008; Hanneman and Lavie, 2009). These steps have not all been implemented for Mapudungun and I˜nupiaq, but work is in progress. 2.1 Data Collection Mapudungun and I˜nupiaq are both low-resource languages in the sense that large corpora and dictionaries in electronic form are not available. (Although more resources are becoming available for Mapudungun.) Both languages have descriptive grammars, however, and there are native speakers who are linguists and language experts. Our partners include Edna MacLean and Larry Kaplan for I˜nupiaq and Eliseo Ca˜nulef and Rosendo Huisca for Mapudungun. The partner institutions were the"
2009.eamt-1.2,monson-etal-2004-data,1,0.812164,"stitutions were the Alaska Native Language Center (ANLC) at the University of Alaska at Fairbanks, the Universidad de la Frontera (UFRO) in Temuco, Chile, and the Chilean Ministry of Education. We have proceeded with data collection in very different ways for Mapudungun and I˜nupiaq based on resources that were available. Because we had a reasonable amount of funding for our initial work on Mapudungun, the UFRO team along http://news.bbc.co.uk/1/hi/wales/4777933.stm http://www.abair.tcd.ie 9 with Rodolfo Vega from Carnegie Mellon (CMU) collected and transcribed 170 hours of spoken Mapudungun (Monson et al., 2004). For I˜nupiaq we have been pursuing other methods for acquiring data. The AVENUE elicitation corpus (Levin et al., 2006) consists of 3000 simple sentences illustrating grammatical features such as person, number, tense, aspect, animacy, and definiteness, as well as constructions such as relative clauses and questions. Edna MacLean translated the sentences into I˜nupiaq and provided interlinear glosses. Some scanned texts were collected from ANLC and were typed by CMU undergraduates7 resulting in a small corpus of 126K bytes. In addition, Shinjae Yoo at CMU is pursuing OCR with character n-gra"
2010.amta-papers.7,baker-etal-2010-modality,1,0.741353,"es of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now (deontic possibility). Many semanticists (Kratzer, 2009; von Fintel and Iatridou, 2009) define modality as quantification over possible worlds. John might go means that there exist some possible worlds in which John goes. Another view of modality relates more to a speaker’s attitude toward a proposition (Nirenburg and McShane, 2008; McShane et al., 2004). Modality resources built for this purpose have been described previously (Baker et al., 2010). This paper will focus on a tree-grafting mechanism used to enrich the machine-translation output and on the resulting improvements to translation quality when the training process for the machinetranslation systems included tagging of named entities and modality. The next section provides the motivation behind the SIMT approach. Section 3 presents implementation details of the semantically-informed syntactic system. Section 4 describes the tree grafting algorithm. Section 5 provides the results of this work. Standard Hierarchical Rules Syntactic Enhancements Semantically Informed Rules Figur"
2010.amta-papers.7,P05-1033,0,0.172966,"duEnglish test set, and the blind test set was NIST09. Tree-Grafting to refine translation grammars with semantic categories We use synchronous context free grammars (SCFGs) as the underlying formalism for our statistical models of translation. SCFGs provide a convenient and theoretically grounded way of incorporating linguistic information into statistical models of translation, by specifying grammar rules with syntactic non-terminals in the source and target languages. We refine the set of non-terminal symbols so that they not only include syntactic categories, but also semantic categories. Chiang (2005) re-popularized the use of SCFGs for machine translation, with the introduction of his hierarchical phrase-based machine translation system, Hiero. Hiero uses grammars with a single nonterminal symbol “X” rather than using linguistically informed non-terminal symbols. When moving to linguistic grammars, we use the Syntax Augmented Machine Translation (SAMT) developed by Venugopal et al. (2007). In SAMT the “X” symbols in translation grammars are replaced with nonterminal categories derived from parse trees that label the English side of the Urdu-English parallel corpus.1 We refine the syntacti"
2010.amta-papers.7,C96-1079,0,0.0150098,"ut has been accused of assam that the power of this on a large region has taken in the affairs and one Place, vice Center of which he is also Figure 1: An example of Urdu-English translation. Shown are an Urdu source document, a reference translation produced by a professional human translator, and machine translation output from a phrase-based model (Moses) without linguistic information, which is representative of state-of-the-art MT quality before the SIMT effort. Named entities have been the focus of information extraction research since the Message Understanding Conferences of the 1980s (Grishman and Sundheim, 1996). Automatic taggers identify semantic types such as person, organization, location, date, facility, etc. In this research effort we tagged English documents using an HMM-based tagger derived from Identifinder (Bikel et al., 1999). Modality is an extra-propositional component of meaning. In John may go to NY, the basic proposition is John go to NY and the word may indicates modality. Van der Auwera and Amman (2005) define core cases of modality: John must go to NY (epistemic necessity), John might go to NY (epistemic possibility), John has to leave now (deontic necessity) and John may leave now"
2010.amta-papers.7,N06-1031,0,0.0612688,"The semantic annotations were done manually by students following a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP"
2010.amta-papers.7,P03-1054,0,0.00597936,"he training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsing domain, the work of (Petrov and Klein, 2007) is related to the current work. Petrov and Klein use a technique of rule splitting and rule merging in order to refine parse trees during machine learning. Hierarchical splitting leads to the creation of learned categories that have linguistic relevance, such as a brea"
2010.amta-papers.7,P07-2045,1,0.00864032,"erstand the challenges of translating important semantic entities when working with a lowresource language pair. Figure 1 shows an example taken from the 2008 NIST Urdu-English translation task, and illustrates the translation quality of a stateof-the-art Urdu-English system (prior to the SIMT effort). The small amount of training data for this language pair (see Table 1) results in significantly degraded translation quality compared, e.g., to an Arabic-English system that has more than 100 times the amount of training data. The machine translation output in Figure 1 was produced using Moses (Koehn et al., 2007), a stateof-the-art phrase-based machine translation system that by default does not incorporate any linguistic information (e.g., syntax or morphology or translit3 Figure 3: Workflow for producing semantically-grafted parse trees. The English side of the parallel corpus is automatically parsed, and also tagged with modality and named-entity markers. These tags are then grafted onto the syntactic parse trees. The relation finder was designed for additional tagging but was not implemented in the current work. (Future work will test relations as another component of meaning that may contribute t"
2010.amta-papers.7,W09-0424,1,0.845363,"note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that were incorporated during the SIMT effort. Once the semantically-grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by an aligner such as GIZA++), to the rule extraction software to extract synchronous grammar rules that are both syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al., 2009), the SAMT grammar extraction software (Venugopal and Zollmann, 2009), and special purpose-built tree-grafting software. Figure 5 shows example semantic rules that are used by the decoder. The noun-phrase rules are augmented with named entities, and the verb phrase rules are augmented with modalities. The semantic categories are listed in Table 2 and Table 3. Because these get marked on the Urdu source as well as the English translation, semantically enriched grammars also act as very simple named entity or modality taggers for Urdu. However, only entities and modalities that occurred in the p"
2010.amta-papers.7,J93-2004,0,0.0365987,"with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes (Huang and Knight, 2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus et al., 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning, 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. In the parsi"
2010.amta-papers.7,A00-2030,1,0.678482,"Missing"
2010.amta-papers.7,P08-1001,0,0.0522391,"the Palestinian Authority fired several missiles GPE weapon GPE GPE Figure 4: A sentence on the English side of the bilingual parallel training corpus is parsed with a syntactic parser, and also tagged with a named entity tagger. The tags are then grafted onto the syntactic parse tree to form new categories like NP-GPE and NP-weapon. Grafting happens prior to extracting translation rules, which happens normally except for the use of the augmented trees. context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are named-entity-tagged by the Phoenix tagger (Richman and Schone, 2008) and modality-tagged by the system described in (Baker et al., 2010). 3. The named entities and modalities are grafted onto the syntactic parse trees using a treegrafting procedure. The grafting procedure was implemented as a part of the SIMT effort. Details are spelled out further in Section 4. The workflow for producing semantically-grafted trees is illustrated in Figure 3. Figure 4 illustrates how named-entity tags are grafted onto a parse tree. We note that while our framework is general, we focus the discussion here on the particular semantic elements (named entities and modalities) that"
2010.amta-papers.7,P99-1039,0,0.237079,"Missing"
2020.coling-main.471,D19-1091,1,0.831926,"or each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin scri"
2020.coling-main.471,D09-1031,0,0.0389939,"word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in gloss generation. These models e"
2020.coling-main.471,W14-2206,0,0.0700873,"Missing"
2020.coling-main.471,C14-1096,0,0.0299487,", to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that prop"
2020.coling-main.471,I17-1004,0,0.0405263,"Missing"
2020.coling-main.471,L18-1533,0,0.0282245,"quiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose an automated system which"
2020.coling-main.471,L18-1657,0,0.0629911,"Missing"
2020.coling-main.471,W17-0102,0,0.108775,"tems unseen during training. In this work we propose an automated system which creates the hard-to-obtain gloss from an easyto-obtain parallel corpus. We use deep neural models which have driven recent impressive advances in all facets of modern natural language processing (NLP). Our model for automatic gloss generation uses multi-source transformer models, combining information from the transcription and the translation, significantly outperforming previous state-of-the-art results on three challenging datasets in Lezgian, Tsez, and Arapaho (Arkhangelskiy, 2012; Abdulaev and Abdullaev, 2010; Kazeminejad et al., 2017). Importantly, our approach does not rely on any additional annotations other than plain transcription and translation, also making no assumptions about the gloss tag space. We further extend our training recipes to include necessary improvements that deal with data paucity (utilizing cross-lingual transfer from similar languages) and with the specific characteristics of the glossing task (presenting solutions for output length control). Our contributions are three-fold: 1. We apply multi-source transformers on the gloss generation task and significantly outperform previous state-of-the-art st"
2020.coling-main.471,D19-3019,0,0.0172569,"itten in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian from Cyrillic script to Latin script, and transliterated Tsez from Latin script to Cyrillic script.5 With the original and the transliterated versions of the training data at hand, we combine them during training into a single training set for the L ANGUAGE T RANSFER Model. The evaluation is of course performed on the original test sets with the original corresponding scripts. 5.1 Implementation We base our implementation on the Joey-NMT toolkit6 (Kreutzer et al., 2019), which we extended to support multi-source transformer models.7 The transcription and translation input sentences can be represented at different granularities: either at the word level or at the more recently popular sub-word level. For simplicity we leave this detail out of the results tables, reporting results with the better-performing option in each case. It is worth noting, though, that for Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word an"
2020.coling-main.471,W19-4226,0,0.0153372,"correct stems or tags for each segment. Accordingly, we perform the evaluation at the morpheme level: the first two evaluation units from the Figure 1 example would be “you&quot; and &quot;-GEN1&quot;, separated. This setting will provide somewhat of an oracle score, that would be achievable if a linguist or the community provide correct segmentations for the transcriptions, or if a morphological segmentation tool is available for that language. Transliteration Cross-lingual training between typologically related languages has shown promising results in several NLP tasks especially in low-resource settings (McCarthy et al., 2019; Anastasopoulos and Neubig, 2019). Two of our evaluation languages, namely Lezgian and Tsez are fairly similar as they are both members of the Nakho-Daghestanian language family, and as such are ideal for crosslingual transfer. However, Anastasopoulos and Neubig (2019) pointed out that cross-lingual learning can be inversely impeded if the languages do not use the same script even if they are closely genealogically related languages. Lezgian is written in Cyrillic script while Tsez is written in Latin script. To maximally exploit the power of cross-lingual training, we transliterated Lezgian"
2020.coling-main.471,2020.scil-1.42,0,0.106078,"or This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and POS-tagging information (McMillan-Major, 2020). In contrast, our approach is, to our knowledge, the first one to show that modern neural systems are a viable solution for the automatic glossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying"
2020.coling-main.471,D16-1096,0,0.026883,"er text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mapping of the output gl"
2020.coling-main.471,W18-4809,0,0.271751,"lly manifest a yawning gap between the amount of material recorded and archived and the amount of data that is thoroughly analyzed with morphological segmentation and gloss (Seifart et al., 2018). This gap can be filled using automatic approaches, which could at least accelerate the annotation process by providing high-quality first-pass annotations. Previous approaches to automatic gloss generation include manual rule crafting and deep rule-based analysis (Bender et al., 2014; Snoek et al., 2014), treating the glossing task as a classification problem focusing only on the morphological tags (Moeller and Hulden, 2018) and requiring a lexicon for This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 5397 Proceedings of the 28th International Conference on Computational Linguistics, pages 5397–5408 Barcelona, Spain (Online), December 8-13, 2020 Figure 1: A Tsez IGT example. Combining information from both the transcription and the translation can aid in deriving the information in the analysis. stems (Samardži´c et al., 2015), and using models based on Conditional Random Fields (CRF) integrated with translation and"
2020.coling-main.471,W09-1905,0,0.0375284,"BL SUPER . ESS ’, one word before the correct destination Musa ‘Musa’. Upon inspection, we found that the input word Musaňin is segmented into Mus-aňin after BPE, and the occurrences of aňin in the training set are overwhelmingly combinations of -a and ňin ‘QUOT’, where -a is a verbal suffix. In contrast, Musa ‘Musa’ is a proper name. The model cannot deduce that ňin is its own morpheme and may appear after nouns, when it has only seen it in tandem with a verbal suffix which, obviously, appears only after verbs. 5405 7 Related Work Several works have studied the automated IGT generation task (Palmer et al., 2009; Baldridge and Palmer, 2009; Samardži´c et al., 2015; Moeller and Hulden, 2018; McMillan-Major, 2020). They mainly used machine learning methods such as CRF and SVM to generate gloss and proposed a series of heuristic post-editing algorithms to improve the performance. Among them, Palmer et al. (2009), Baldridge and Palmer (2009) combined machine labeling and active learning for creating IGT. Moeller and Hulden (2018) tested LSTMs to predict the morphological labels within glosses, but underperformed against CRF models in that task. McMillan-Major (2020) exploited parallel information in glos"
2020.coling-main.471,P02-1040,0,0.109072,"veraging over all heads. 5400 Language Translation L EZGIAN T SEZ A RAPAHO English English/Russian English Training Examples Test Examples 951 1584 25208 119 198 3151 Table 1: Dataset information for the languages in our experiments. 4 Evaluating Gloss Generation The characteristics of gloss generation require special care rather than blindly using metrics established for other tasks like machine translation. Previous work uses: • Accuracy: percentage of correct (full) analyses for each token. It is the main metric used in previous work (Samardži´c et al., 2015; McMillan-Major, 2020). • BLEU (Papineni et al., 2002): an average of n-gram precision along with a brevity penalty, BLEU is perhaps the most popular reference-based machine translation method. Since our models are inspired from MT, we use it as another indication of quality as it captures accuracy/precision over n-grams, even though the rest of the metrics are more suitable to the automatic glossing task. • Precision/Recall: We further break down the evaluation to focus separately on lemmas and tags. Several previous works prioritize precision over recall, especially by not outputting tags if items are not seen during training, e.g. (Moeller and"
2020.coling-main.471,L18-1674,0,0.0242646,"ossing task, without requiring any additional components or making unrealistic assumptions regarding data or NLP tool availability for low-resource languages. We rely on the observation that parallel corpora with transcription and translation are likely to be available for many low-resource languages, since the knowledge of the two languages is sufficient for translating the corpus without the need of linguistic training. Documentation approaches relying on parallel audio collection (Bird et al., 2014) are in fact already underway in the Americas (Jimerson and Prud’hommeaux, 2018) and Africa (Rialland et al., 2018; Hamlaoui et al., 2018), among other places. An additional advantage of parallel corpora is that they contain rich information that can be beneficial for gloss generation. As Figure 1 outlines, the stems/lemmas in the analysis are often hiding in the translation, while the grammatical tags could be derived from the segments in the transcription. We hypothesize that the information from the translation can further ground the gloss generation, and especially allow a system that properly takes into account to generalize to produce lemmas or stems unseen during training. In this work we propose a"
2020.coling-main.471,2020.sigmorphon-1.21,0,0.0361868,"ansformer for Gloss Generation Problem Formulation and Model Our model is built upon the transformer model (Vaswani et al., 2017), a self-attention-based sequenceto-sequence (seq2seq) neural model. Compared to the CRF model used in McMillan-Major (2020), which can only capture local dependencies, a self-attention model can produce context-sensitive hidden representations that take the whole input into account. Moreover, unlike other recurrent (seq2seq) models such as bidirectional LSTM, the Transformer model shows more robust performance in morphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to inco"
2020.coling-main.471,W15-3710,0,0.03569,"Missing"
2020.coling-main.471,P16-1162,0,0.0448655,"Tsez and Arapaho the sub-word representations (obtained using byte-pair-encoding (BPE)8 ) always lead to better results. For the much smaller Lezgian dataset, we saw no difference between sub-word and word-level models, but this lack of difference can be explained by the overall very small size of the vocabulary for the Lezgian dataset. 5 We use the transliterator provided by https://pypi.org/project/transliterate/. https://github.com/joeynmt/joeynmt 7 Our code will be open-sourced at https://github.com/yukiyakiZ/Automatic_Glossing. 8 We use the sentencepiece implementation of the BPE method (Sennrich et al., 2016) with vocab size of 2000 for Lezgian, 2500 for Tsez, and 10000 for Arapaho) 6 5402 For training all Lezgian and Tsez models and the Arapaho model with the subsampled 2,000 training sentences, we use 2 layers for both encoders and the decoder and 2 attention heads. All the embedding and hidden state dimensions are set to 128. We use a batch size of 20. For training the Arapaho model on the original larger dataset, we use 4 layers for all encoders and decoder, with 4 attention heads. The embedding and hidden state dimension are 256, and batch size is 50. For all models, learning rate is initiali"
2020.coling-main.471,W14-2205,0,0.815954,"Missing"
2020.coling-main.471,P16-1008,0,0.0252577,"ontrol Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other models, we need to be able to produce the exact mappin"
2020.coling-main.471,P19-1148,0,0.0226311,"the particular nature of the gloss generation task. Length control Unlike other text generation tasks where the generated text can be relatively free in word order, the gloss must map to the transcription morpheme-by-morpheme or word-by-word, dependent on the intended granularity. One drawback of using a seq2seq model for the gloss generation task compared to e.g. a CRF-based approach like (McMillan-Major, 2020) is that a hard constraint of “one output per input” is not enforced by or hard-coded in the model. Even though structural biases (Cohn et al., 2016) such as hard monotonic attention (Wu and Cotterell, 2019), or source-side coverage mechanisms (Tu et al., 2016; Mi et al., 2016) could remedy this potential issue, we found that there was little need for them, as a simple mechanism to control the final output length during inference was sufficient.1 The intuition lies in the observation that the length of the output gloss should match that of the input transcription exactly. Hence, during inference we set a minimum desired length of the output sequence, and disallow any candidates shorter than that.2 Alignment between gloss and transcription To ensure fair evaluation against the baseline and other m"
2020.coling-main.471,N16-1004,0,0.0285687,"rphologyrelated tasks under low-resource settings (Ryan and Hulden, 2020). Our architecture choice is also motivated by the promising performance of the model along with its computational efficiency. The original Transformer is composed of a single encoder and a decoder, each with several layers. Each encoder layer consists of a multi-head self-attention layer and a fully connected feed-forward network, while decoder layers are additionally augmented with multi-head attention over the output of the encoder stack. Our model adds a second encoder to create a multi-source transformer similar to (Zoph and Knight, 2016; Anastasopoulos and Chiang, 2018), in order to incorporate the secondary information from the translation. A visual depiction of our model is outlined in Figure 2. Let X1 = x11 . . . x1N be a sequence of transcription words, X2 = x21 . . . x2M a sequence of translation words, and Y = y1 . . . yK be a sequence of the target gloss. A single-source gloss generation model attempts to model P (Y |X1 ). A multi-source model can jointly model P (Y |X1 , X2 ), and thus we need two encoders (see Figure 2b). One encoder transforms the input transcription sequence x11 . . . x1N into a sequence of input"
2020.emnlp-main.360,P13-1133,0,0.0775001,"Missing"
2020.emnlp-main.360,J17-4005,0,0.0543881,"Missing"
2020.emnlp-main.360,J19-1001,0,0.0803245,"dependency parsing. A specific problem may be that underfitting to the training data in order to better handle unseen words in the test set hinders downstream tasks that rely on words from the training dictionary (Zhang et al., 2020). In this study, we primarily examine the transferrability of MWEs in a word translation task, although it is possible that the better treatment of MWEs is also effective in downstream tasks. 2.3 Multi-word Expressions MWEs have been studied in the context of syntactic analysis (Ros´en et al., 2016; Kahane et al., 2017) and semantic analysis (Tratz and Hovy, 2010; Cordeiro et al., 2019). The discovery and identification of MWEs in corpora are important problems in this area (Sag et al., 2002), and much effort has been devoted to the development of methods (Constant et al., 2017) and annotated resources (Losnegaard et al., 2016). The universal dependencies (UD) project (Nivre et al., 2016) covers a wide range of languages but uses just a few dependency relations to annotate MWEs, namely fixed, flat, and compound. The DiMSUM shared task (Schneider et al., 2016) aims to detect English MWEs in texts. The PARSEME project (Savary et al., 2017; Ramisch et al., 2018) targets verbal"
2020.emnlp-main.360,P19-1070,0,0.0415094,"Missing"
2020.emnlp-main.360,gonzalez-agirre-etal-2012-multilingual,0,0.0197299,"st entries are nominals, but this resource also contains other types of MWEs like verbal phrases and connectives. parseme: Parseme is multilingual corpus in which Verbal MWEs are annotated for the PARSEME shared task 1.1 (Ramisch et al., 2018). Types of verbal MWEs include light verb constructions (e.g., give a speech), verb-particle constructions (e.g., wake up), verbal idioms, etc. They can be commonly observed in many languages even though 3 We use subsets of Arabic (Elkateb and Black, 2006), Chinese (Wang and Bond, 2013), English (Fellbaum, 1998), Japanese (Isahara et al., 2008), Spanish (Gonzalez-Agirre et al., 2012), Bulgarian, Russian, German, Hebrew, Hindi, and Turkish (Bond and Foster, 2013). eomw ar ja(i) ja(u) ru zh 1,608 5,006 3,897 3,887 6,927 bg de en es he hi tr eomw parseme 1,022 1,092 8,552 3,079 934 454 1,959 3,255 2,705 8,982 4,485 2,454 878 4,240 Table 2: MWE lists (lemma) used for MWE identification. eomw=Extended Multilingual Open Wordnet, i=IPADIC u=UniDic the category distributions vary from language to language. Table 2 shows the sizes of our lexicons. Note that not all MWEs in our lists are included in our word embeddings as some of them do not exist in our training corpora. 3.2 Bilin"
2020.emnlp-main.360,L18-1550,0,0.0250919,"a proper transformation matrix based on single word dictionaries, it should also be able to transform MWE embeddings to the shared vector space properly. Task 2: We also study whether the inclusion of MWEs in cross-lingual embedding space adversely affects the alignments between single words. We use MUSE for training and evaluation in Task 2. For each language pair, we train and test crosslingual mappings by the first 5k and next 1.5k unique source words10 in the bilingual dictionary, respectively. Parameters: We trained CBOW fastText models of 300 dimensions with the parameters suggested by Grave et al. (2018). We used the implementation by (Conneau et al., 2018) to align monolingual embeddings by the method described in §4.11 To fairly compare between the baseline (tokenization without MWEs) and the experimental condition (tokenization with MWEs), we uses the same set of candidate words from which we are going to pick the k best. The candidate set does not include 10 Source words are sorted by frequencies by Conneau et al. We also experimented with VecMap (Artetxe et al., 2018) and observed a similar result (Appendix E). 11 MWEs in Task 2. For the baseline in Task 1, MWEs are represented by the av"
2020.emnlp-main.360,D18-1043,0,0.0134642,"dictionaries. In particular, we show characteristic examples in English-Japanese (IPADIC) translations in Table 7. The first example illustrates a common construction using -nin (person), which is segmented into two words. The benchmark tends to contain transcriptions of foreign words like shefu as they are often single tokens. The second example shows verbalization, which is again segmented into noun + suru (do). These examples exemplify the limitation of evaluations restricted by single words, and may explain the difficulty of EnglishJapanese word translations reported in a previous study (Hoshen and Wolf, 2018). 6 Conclusion We studied the impact of pre-tokenizing MWEs on cross-lingual alignments of word embeddings. We found that simple lexicon-based tokenizations can align embeddings of MWEs at a high precision without breaking alignments of single-words. We believe our results will motivate researchers to pay more attention to the existence of MWEs and how they are aligned across languages. Acknowledgements We thank David R. Mortensen, Jong Hyuk Park, and anonymous reviewers for their helpful feedback. References Waleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah"
2020.emnlp-main.360,isahara-etal-2008-development,0,0.0605909,"mmon Locale Data Repository.3 Most entries are nominals, but this resource also contains other types of MWEs like verbal phrases and connectives. parseme: Parseme is multilingual corpus in which Verbal MWEs are annotated for the PARSEME shared task 1.1 (Ramisch et al., 2018). Types of verbal MWEs include light verb constructions (e.g., give a speech), verb-particle constructions (e.g., wake up), verbal idioms, etc. They can be commonly observed in many languages even though 3 We use subsets of Arabic (Elkateb and Black, 2006), Chinese (Wang and Bond, 2013), English (Fellbaum, 1998), Japanese (Isahara et al., 2008), Spanish (Gonzalez-Agirre et al., 2012), Bulgarian, Russian, German, Hebrew, Hindi, and Turkish (Bond and Foster, 2013). eomw ar ja(i) ja(u) ru zh 1,608 5,006 3,897 3,887 6,927 bg de en es he hi tr eomw parseme 1,022 1,092 8,552 3,079 934 454 1,959 3,255 2,705 8,982 4,485 2,454 878 4,240 Table 2: MWE lists (lemma) used for MWE identification. eomw=Extended Multilingual Open Wordnet, i=IPADIC u=UniDic the category distributions vary from language to language. Table 2 shows the sizes of our lexicons. Note that not all MWEs in our lists are included in our word embeddings as some of them do not"
2020.emnlp-main.360,P15-1162,0,0.0306878,"Missing"
2020.emnlp-main.360,P19-1492,0,0.0194995,"even without any supervision. Another stream of studies on CWEs adopts a joint approach: word embeddings on multiple languages are trained at one time using parallel corpora (Luong et al., 2015; Gouws et al., 2015). It is an interesting future direction to explore how MWEs affect joint detection of CWEs. 2.2 The limitations of CWEs Besides the problem of word units, several limitations of CWEs have been pointed out in the literature. The majority of such work focuses on the statistical characteristics of word embeddings rather than their linguistic nature. Some studies (Søgaard et al., 2018; Ormazabal et al., 2019) claim that the accuracy of cross-lingual alignments depends on the similarity of word embeddings spaces of different languages, and this similarity in turn depends 2 Available at https://github.com/llab-cmu/ emnlp2020-mwe-pretokenization. on the similarity between the training corpora. Kementchedjhieva et al. (2019), illustrating an issue related to evaluation of CWEs, argues that proper nouns constitute a quarter of the MUSE dataset, rendering it not ideal for word translation. Using a word translation task for the intrinsic evaluation of CWEs presupposes a correlation between its performanc"
2020.emnlp-main.360,W12-3301,0,0.0528896,"Missing"
2020.emnlp-main.360,W19-5110,0,0.013271,"et al., 2016) covers a wide range of languages but uses just a few dependency relations to annotate MWEs, namely fixed, flat, and compound. The DiMSUM shared task (Schneider et al., 2016) aims to detect English MWEs in texts. The PARSEME project (Savary et al., 2017; Ramisch et al., 2018) targets verbal MWEs and has constructed benchmark datasets in several–mostly European–languages for training automatic MWE taggers. However, such training resources are available only in a limited number of languages, and even with such resources, the automatic analysis of MWEs is known to be very difficult. Savary et al. (2019) argues the importance of syntactic MWE lexicons for further development in this area. Another line of work analyzes the interpretation of MWEs such as noun compounds (Tratz 4453 and Hovy, 2010). Some studies exploit word embeddings to build a classifier (e.g., Shwartz and Waterson, 2018). Several studies tokenize MWEs before training word embeddings (Baldwin et al., 2003; Salehi et al., 2015; Cordeiro et al., 2019). Although the major target of these studies is monolingual, our focus is on the cross-lingual mapping of MWEs by CWEs. 3 Data Creation This section describes the methods we used fo"
2020.emnlp-main.360,Q14-1016,0,0.0716657,"Missing"
2020.emnlp-main.360,S16-1084,0,0.0179873,"context of syntactic analysis (Ros´en et al., 2016; Kahane et al., 2017) and semantic analysis (Tratz and Hovy, 2010; Cordeiro et al., 2019). The discovery and identification of MWEs in corpora are important problems in this area (Sag et al., 2002), and much effort has been devoted to the development of methods (Constant et al., 2017) and annotated resources (Losnegaard et al., 2016). The universal dependencies (UD) project (Nivre et al., 2016) covers a wide range of languages but uses just a few dependency relations to annotate MWEs, namely fixed, flat, and compound. The DiMSUM shared task (Schneider et al., 2016) aims to detect English MWEs in texts. The PARSEME project (Savary et al., 2017; Ramisch et al., 2018) targets verbal MWEs and has constructed benchmark datasets in several–mostly European–languages for training automatic MWE taggers. However, such training resources are available only in a limited number of languages, and even with such resources, the automatic analysis of MWEs is known to be very difficult. Savary et al. (2019) argues the importance of syntactic MWE lexicons for further development in this area. Another line of work analyzes the interpretation of MWEs such as noun compounds"
2020.emnlp-main.360,P18-1041,0,0.0174525,"ponents, a deficiency carried into CWE construction. In this position paper, we argue that the token units of word embeddings should be discussed more carefully, and, in particular, that MWEs should be recognized as single units before training and evaluating word embeddings. In cross-lingual applications, MWEs are particularly important. A single token in one language is often translated into an MWE in another language. So, failure to tokenize MWEs is a critical flaw of CWEs in the task of word translation and presumably in other cross-lingual tasks as well. Some studies (Iyyer et al., 2015; Shen et al., 2018) have suggested representing phrase and sentence embeddings by taking the average or sum of their component word vectors. However, such a simple approach is not sufficient, as the meaning 4451 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4451–4464, c November 16–20, 2020. 2020 Association for Computational Linguistics average 联合 美国 Single → +MWE 联合united 州states 美国U.S. united states united states .32 → .40 .32 → .24 .37 → .38 .19 → .10 .16 → .10 .20 → .18 .57 → .41 .63 → .44 .69 → .82 Table 1: Cosine similarities between English and Chinese word"
2020.emnlp-main.360,L16-1368,0,0.0673814,"Missing"
2020.emnlp-main.360,N18-2035,0,0.0147046,", 2018) targets verbal MWEs and has constructed benchmark datasets in several–mostly European–languages for training automatic MWE taggers. However, such training resources are available only in a limited number of languages, and even with such resources, the automatic analysis of MWEs is known to be very difficult. Savary et al. (2019) argues the importance of syntactic MWE lexicons for further development in this area. Another line of work analyzes the interpretation of MWEs such as noun compounds (Tratz 4453 and Hovy, 2010). Some studies exploit word embeddings to build a classifier (e.g., Shwartz and Waterson, 2018). Several studies tokenize MWEs before training word embeddings (Baldwin et al., 2003; Salehi et al., 2015; Cordeiro et al., 2019). Although the major target of these studies is monolingual, our focus is on the cross-lingual mapping of MWEs by CWEs. 3 Data Creation This section describes the methods we used for creating the data that we are releasing with this paper: (1) monolingual lists of MWEs in eleven languages for pre-tokenizing MWEs in corpora and (2) bilingual dictionaries (ten languages each paired with English) for evaluating the resulting MWE embeddings in the word translation task."
2020.emnlp-main.360,W13-4302,0,0.0232523,"dditional synsets drawn from Wiktionary and the Unicode Common Locale Data Repository.3 Most entries are nominals, but this resource also contains other types of MWEs like verbal phrases and connectives. parseme: Parseme is multilingual corpus in which Verbal MWEs are annotated for the PARSEME shared task 1.1 (Ramisch et al., 2018). Types of verbal MWEs include light verb constructions (e.g., give a speech), verb-particle constructions (e.g., wake up), verbal idioms, etc. They can be commonly observed in many languages even though 3 We use subsets of Arabic (Elkateb and Black, 2006), Chinese (Wang and Bond, 2013), English (Fellbaum, 1998), Japanese (Isahara et al., 2008), Spanish (Gonzalez-Agirre et al., 2012), Bulgarian, Russian, German, Hebrew, Hindi, and Turkish (Bond and Foster, 2013). eomw ar ja(i) ja(u) ru zh 1,608 5,006 3,897 3,887 6,927 bg de en es he hi tr eomw parseme 1,022 1,092 8,552 3,079 934 454 1,959 3,255 2,705 8,982 4,485 2,454 878 4,240 Table 2: MWE lists (lemma) used for MWE identification. eomw=Extended Multilingual Open Wordnet, i=IPADIC u=UniDic the category distributions vary from language to language. Table 2 shows the sizes of our lexicons. Note that not all MWEs in our lists"
2020.emnlp-main.360,N15-1104,0,0.0353048,"Missing"
2020.emnlp-main.360,P17-1179,0,0.0272043,"raphic tokens.2 2 2.1 Related Work Cross-lingual Word Embeddings In this study, we experiment with one of the major approaches of learning CWEs, where monolingual embeddings trained in each language are mapped using cross-lingual supervision. Early work by Mikolov et al. (2013) showed that a linear transformation of word embeddings across languages can be trained by a bilingual dictionary. Smith et al. (2017) reported that the linear mapping becomes more accurate and computationally efficient by setting an orthogonal constraint on a transformation matrix. Recent studies (Artetxe et al., 2017; Zhang et al., 2017; Conneau et al., 2018) have further demonstrated that a transformation matrix can be learned by a very small amount of seed translations and even without any supervision. Another stream of studies on CWEs adopts a joint approach: word embeddings on multiple languages are trained at one time using parallel corpora (Luong et al., 2015; Gouws et al., 2015). It is an interesting future direction to explore how MWEs affect joint detection of CWEs. 2.2 The limitations of CWEs Besides the problem of word units, several limitations of CWEs have been pointed out in the literature. The majority of such"
2020.emnlp-main.360,2020.acl-main.201,0,0.0205335,"n of CWEs presupposes a correlation between its performance with the performance of CWEs in downstream tasks, which has been questioned by several studies. Ammar et al. (2016), Glavaˇs et al. (2019) and Fujinuma et al. (2019) show low correlation between word translation accuracy and the performance of downstream tasks such as document classification, natural language inference, and dependency parsing. A specific problem may be that underfitting to the training data in order to better handle unseen words in the test set hinders downstream tasks that rely on words from the training dictionary (Zhang et al., 2020). In this study, we primarily examine the transferrability of MWEs in a word translation task, although it is possible that the better treatment of MWEs is also effective in downstream tasks. 2.3 Multi-word Expressions MWEs have been studied in the context of syntactic analysis (Ros´en et al., 2016; Kahane et al., 2017) and semantic analysis (Tratz and Hovy, 2010; Cordeiro et al., 2019). The discovery and identification of MWEs in corpora are important problems in this area (Sag et al., 2002), and much effort has been devoted to the development of methods (Constant et al., 2017) and annotated"
2020.lrec-1.350,N19-1009,0,0.0408851,"Missing"
2020.lrec-1.350,L16-1632,0,0.0416335,"Missing"
2020.lrec-1.350,W17-0102,0,0.0279178,") developed hybrid ruleand phrase-based Statistical Machine Translation systems. Naturally, similar works in collecting corpora in Indigenous languages of Latin America are abundant, but very few, if any, have the scale and potential of our resource to be useful in many downstream language-specific and interdisciplinary applications. A general overview of the state of NLP for the under-represented languages of the Americas can be found at (Mager et al., 2018). To name a few of the many notable works, (Monta˜no et al., 2019) created a parallel Mixtec-Spanish corpus for Machine Translation and (Kazeminejad et al., 2017) created lexical resources ´ for Arapaho, while (Cardenas et al., 2018) and (Cavar et al., 2016) focused on building speech corpora for Southern Quechua and Chatino respectively. 4. The Resource 1 The resource is comprised of 142 hours of spoken Mapudungun that was recorded during the AVENUE project (Levin et al., 2002) in 2001 to 2005. The data was recorded under a partnership between the AVENUE project, funded by the US National Science Foundation at Carnegie Mellon University, the Chilean Ministry of Education (Mineduc), and the Instituto de Estudios Ind´ıgenas at Universidad de La Frontera"
2020.lrec-1.350,monson-etal-2004-data,1,0.674738,"Missing"
2020.lrec-1.350,monson-etal-2008-linguistic,1,0.46873,"Missing"
2020.lrec-1.350,W19-3650,0,0.033218,"Missing"
2020.lrec-1.350,P02-1040,0,0.110151,"Missing"
2020.lrec-1.350,W15-3049,0,0.0486746,"Missing"
2020.lrec-1.350,W18-6319,0,0.011561,"ity as well as label smoothing set to 0.1. We train with the Adam optimizer (Kingma and Ba, 2014) for up to 200 epochs using learning decay with a patience of six epochs. The baseline results using different portions of the training set (10k, 50k, 100k, and all (220k) parallel sentences) on both translation directions are presented in Table 3, using detokenized BLEU (Papineni et al., 2002) (a standard MT 2875 metric) and chrF (Popovi´c, 2015) (a metric that we consider to be more appropriate for polysynthetic languages, as it does not rely on word n-grams) computed with the sacreBLEU toolkit (Post, 2018). It it worth noting the difference in quality between the two directions, with translation into Spanish reaching 20.4 (almost 21) BLEU points in the development set, while the opposite direction (translating into Mapudungun) shows about a 7 BLEU points worse performance. This is most likely due to Mapudungun being a polysynthetic language, with its complicated morphology posing a challenge for proper generation. 7. Conclusion With this work we present a resource that will be extremely useful for building language systems in an endangered, under-represented language, Mapudungun. We benchmark N"
2020.lrec-1.350,P16-1162,0,0.0325849,"Missing"
baker-etal-2010-modality,levy-andrew-2006-tregex,0,\N,Missing
baker-etal-2010-modality,W09-0424,0,\N,Missing
baker-etal-2010-modality,P02-1040,0,\N,Missing
bhatia-etal-2014-unified,J98-2001,0,\N,Missing
bhatia-etal-2014-unified,C88-1044,0,\N,Missing
bhatia-etal-2014-unified,W13-2234,1,\N,Missing
C14-1100,bhatia-etal-2014-unified,1,0.727454,"es or clitics, as in Arabic. Sometimes it is expressed with other constructions, as in Chinese (a language without articles), where the existential construction can be used to express indefinite subjects and the ba- construction can be used to express definite direct objects (Chen, 2004). Aside from this variation in the form of (in)definite NPs within and across languages, there is also variability in the mapping between semantic, pragmatic, and discourse functions of NPs and the (in)definites expressing these functions. We refer to these as communicative functions of definiteness, following Bhatia et al. (2014). Croft (2003, pp. 6–7) shows that even when two languages have access to the same morphosyntactic forms of definiteness, the conditions under which an NP is marked as definite or indefinite (or not at all) are language-specific. He illustrates this by contrasting English and French translations (both languages use definite as well as indefinite articles) such as: (1) He showed extreme care. (unmarked) Il montra un soin extrême. (indef.) (2) I love artichokes and asparagus. (unmarked) J’aime les artichauts et les asperges. (def.) (3) His brother became a soldier. (indef.) Son frère est devenu"
C14-1100,D13-1174,1,0.849382,"ness. After preprocessing the text with a dependency parser and coreference resolver, which is described in §6.1, we extract several kinds of percepts for each NP. 4.2.1 Basic Words of interest. These are the head within the NP, all of its dependents, and its governor (external to the NP). We are also interested in the attached verb, which is the first verb one encounters when traversing the dependency path upward from the head. For each of these words, we have separate percepts capturing: the token, the part-of-speech (POS) tag, the lemma, the dependency relation, and (for the head only) a 3 Chahuneau et al. (2013) use a similar parametrization for their model of morphological inflection. As is standard practice with these models, bias parameters (which capture the overall frequency of percepts/attributes) are excluded from regularization. 5 See Theorem 1.2 in Breiman (2001) for details. 4 1063 binary indicator of plurality (determined from the POS tag). As there may be multiple dependents, we have additional features specific to the first and the last one. Moreover, to better capture tense, aspect and modality, we collect the attached verb’s auxiliaries. We also make note of the negative particle (with"
C14-1100,P05-1066,0,0.0178084,"rom some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000),"
C14-1100,C88-1044,0,0.577504,"to predict leaf labels (the non-bold faced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listen"
C14-1100,2007.mtsummit-papers.29,0,0.0509283,"ic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identifica"
C14-1100,T87-1035,0,0.537601,"aced labels in fig. 1); the evaluation measures (§5) include one that exploits these label groupings to award partial credit according to relatedness. §6 presents experiments comparing several models and discussing their strengths and weaknesses; computational work and applications related to definiteness are addressed in §7. 1060 2 Annotation scheme The literature on definiteness describes functions such as uniqueness, familiarity, identifiability, anaphoricity, specificity, and referentiality (Birner and Ward, 1994; Condoravdi, 1992; Evans, 1977, 1980; Gundel et al., 1988, 1993; Heim, 1990; Kadmon, 1987, 1990; Lyons, 1999; Prince, 1992; Roberts, 2003; Russell, 1905, inter alia) as being related to definiteness. Reductionist approaches to definiteness try to define it in terms of one or two of the aforementioned communicative functions. For example, Roberts (2003) proposes that the combination of uniqueness and a presupposition of familiarity underlie all definite descriptions. However, possessive definite descriptions (John’s daughter) and the weak definites (the son of Queen Juliana of the Netherlands) are neither unique nor necessarily familiar to the listener before they are spoken. In co"
C14-1100,C10-1068,0,0.0128025,"se, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that re"
C14-1100,P06-1077,0,0.0109354,"matical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While defini"
C14-1100,W00-0708,0,0.0373875,"(Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distingu"
C14-1100,C02-1139,0,0.0446422,"dded or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling of syntactic constructions such as dative shift and the expression of possession with “of” or “’s”. Bresnan and Ford (2010) used logistic regression with semantic features to predict syntactic constructions. Although we are doing the opposite (using syntactic features to predict semantic categories), we share"
C14-1100,C00-2162,0,0.0170846,"s made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of"
C14-1100,W12-3807,1,0.89797,"Missing"
C14-1100,N13-1071,0,0.0612652,"Missing"
C14-1100,D10-1032,0,0.0256045,"gh we are doing the opposite (using syntactic features to predict semantic categories), we share the assumption that reductionist approaches (as mentioned earlier) are not able to capture all the nuances of a linguistic phenomenon. Following Hopper and Traugott (2003) we observe that grammaticalization is accompanied by function drift, resulting in multiple communicative functions for each grammatical construction. Other attempts have also been made to capture, using classifiers, (propositional as well as non propositional) aspects of meaning that have been grammaticalized: see, for instance, Reichart and Rappoport (2010) for tense sense disambiguation, Prabhakaran et al. (2012) for modality tagging, and Srikumar and Roth (2013) for semantics expressed by prepositions. 8 Conclusion We have presented a data-driven approach to modeling the relationship between universal communicative functions associated with (in)definiteness and their lexical/grammatical realization in a particular language. Our feature-rich classifiers can give insights into this relationship as well as predict communicative functions for the benefit of NLP systems. Exploiting the higher-level semantic attributes, our log-linear classifier com"
C14-1100,P10-1005,0,0.108554,"tediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which a coreference resolution system can resolve the bridging relations. Also in the context of coreference resolution, Ng and Cardie (2002) and Kong et al. (2010) have examined anaphoricity detection. To the best of our knowledge, no studies have been conducted on automatic prediction of semantic and pragmatic communicative functions of definiteness more broadly. Our work is related to research in linguistics on the modeling o"
C14-1100,N10-1018,0,0.0157261,"(b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly studied in computational linguistics, 1067 studies on additional, more complex aspects of definiteness are limited. Reiter and Frank (2010) exploit linguistically-motivated features in a supervised approach to distinguish between generic and specific NPs. Hendrickx et al. (2011) investigated the extent to which"
C14-1100,P13-1045,0,0.0784612,"Missing"
C14-1100,W13-2234,1,0.798697,"s input to (or jointly with) the coreference task. Applications such as information extraction and dialogue processing could be expected to benefit not only from coreference information, but also from some of the semantic distinctions made in our framework, including specificity and genericity. Better computational processing of definiteness in different languages stands to help machine translation systems. It has been noted that machine translation systems face problems when the source and the target language use different grammatical strategies to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of"
C14-1100,P02-1039,0,0.00839924,"to express the same information (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme predictio"
C14-1100,2007.iwslt-1.3,0,0.0123911,"rmation (Stymne, 2009; Tsvetkov et al., 2013). Previous work on machine translation has attempted to deal with this in terms of either (a) preprocessing the source language to make it look more like the target language (Collins et al., 2005; Habash, 2007; Nießen and Ney, 2000; Stymne, 2009, inter alia); or (b) post-processing the machine translation output to match the target language, (e.g., Popovi´c et al., 2006). Attempts have also been made to use syntax on the source and/or the target sides to capture the syntactic differences between languages (Liu et al., 2006; Yamada and Knight, 2002; Zhang et al., 2007). Automated prediction of (in)definite articles has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and identification and correction of ESL errors (Han et al., 2006; Rozovskaya and Roth, 2010). More recently, Tsvetkov et al. (2013) trained a classifier to predict where English articles might plausibly be added or removed in a phrase, and used this classifier to improve the quality of statistical machine translation. While definiteness morpheme prediction has been thoroughly"
C16-1095,J92-4003,0,0.0814094,"Missing"
C16-1095,J93-1003,0,0.0211791,"gorithms, smoothed via additive (dirichlet) smoothing. Our pilot experiments showed that this resulted in better performance when compared to the performance with random initializations. The runtime (O(token ∗ iterations ∗ C 2 )) is higher than that of Brown algorithm (O(types ∗ C 2 + tokens)) because we estimate all the distributions of the probabilistic HMM using dynamic programming (Rabiner and Juang, 1986). Hence, subsequent models used interpolated stochastic batch updates (Liang and Klein, 2009) instead of batch updates so that the convergence is faster. We used a likelihood ratio test (Dunning, 1993), which is a form of hypothesis testing that decides whether the second word in the bigram is unusually associated with the first word of the bigram or not, to determine an initial list of possible NE collocations. For both Sorani and Tajik, this measure results in desirable LR score graphs for bigrams with a distinct “elbow” for both the languages. However, determining the threshold, below which a collocation should not be considered genuine for the purposes of further steps, was done manually by a human linguist looking at IPA representations of the collocations. This is a judgment that need"
C16-1095,P13-2054,0,0.0895686,"ence strategy in greater detail in Littell et al. (2016). For Tajik, we constructed a more traditional gazetteer using Tajik’s relatively extensive Wikipedia. Since Wikipedia titles are linked between Wikipedias in different languages, we had parallel English and Tajik titles; we filtered the English titles by heuristics including capitalization, and used the corresponding Tajik titles as the gazetteer entries. 3.2 Unsupervised morphology induction The Western Iranian languages are morphologically rich, with Sorani in particular having a high degree of morphological complexity (Walther, 2011; Esmaili and Salavati, 2013). In such languages, the presence of certain morphemes is strongly correlated with certain grammatical functions being present, which could be informative for the problem of identifying and discriminating named entities.1 While unsupervised induction of morphological grammars is a long-standing problem, the inferred morphological analyses typically diverge from conventional linguistic analyses rather substantially. We addressed this shortcoming by using feedback from human linguists using a modification of the interactive learning paradigm proposed by Hu et al. (2011). While superficially rela"
C16-1095,P11-1026,0,0.0114994,"Walther, 2011; Esmaili and Salavati, 2013). In such languages, the presence of certain morphemes is strongly correlated with certain grammatical functions being present, which could be informative for the problem of identifying and discriminating named entities.1 While unsupervised induction of morphological grammars is a long-standing problem, the inferred morphological analyses typically diverge from conventional linguistic analyses rather substantially. We addressed this shortcoming by using feedback from human linguists using a modification of the interactive learning paradigm proposed by Hu et al. (2011). While superficially related to active learning (Settles, 1 While Western Iranian languages also utilize prefixes and root modification, we concentrated here on suffixes alone; this simplifies the model and concentrates on those morphological alternations we believe more likely to be relevant to NER. 1000 Higher quality affixes will probably be higher in the list, but please go through the entire list. Examples of analyses containing each affix are listed for informational purposes. It is important to remember that you are not judging these anaylses, i.e., whether the affix in question is act"
C16-1095,N09-1069,0,0.0106249,"e expected to be identified as collocations. We warm-started with the distributions obtained by the Brown cluster algorithms, smoothed via additive (dirichlet) smoothing. Our pilot experiments showed that this resulted in better performance when compared to the performance with random initializations. The runtime (O(token ∗ iterations ∗ C 2 )) is higher than that of Brown algorithm (O(types ∗ C 2 + tokens)) because we estimate all the distributions of the probabilistic HMM using dynamic programming (Rabiner and Juang, 1986). Hence, subsequent models used interpolated stochastic batch updates (Liang and Klein, 2009) instead of batch updates so that the convergence is faster. We used a likelihood ratio test (Dunning, 1993), which is a form of hypothesis testing that decides whether the second word in the bigram is unusually associated with the first word of the bigram or not, to determine an initial list of possible NE collocations. For both Sorani and Tajik, this measure results in desirable LR score graphs for bigrams with a distinct “elbow” for both the languages. However, determining the threshold, below which a collocation should not be considered genuine for the purposes of further steps, was done m"
C16-1095,W03-0430,0,0.0445735,"of related languages; we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used for training NER systems: • • • • • • • • • • • Current token and Current tag Previous token and Current tag Next token and Current tag Current+previous token and current tag Current+next token and current tag First five features but with previous tag First five features conjoined with both current and previous tags Contains foreign script characters Indicator features for tokens containing digits Features about capitalization information Prefix features However, given the paucity of training data, these features"
C16-1095,C16-1328,1,0.762765,"t languages and scripts could be more directly compared, and so that judgments about the data could be made rapidly by linguists without native proficiency in the Perso-Arabic and Cyrillic writing systems, we produced representations of the Sorani, Kurmanji, and Tajik data in the International Phonetic Alphabet (IPA). To disambiguate ambiguous Sorani forms, we used a conditional random field (CRF) (Lafferty et al., 2001) that utilized a combination of human judgments, universal phonetic features, and language models of related languages; we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used"
C16-1095,N03-1028,0,0.00672678,"we describe this “IPAization” process in more detail in Mortensen et al. (2016) and Littell et al. (2016). 3 Named Entity Recognition For the NER task, we focused on identifying mentions of persons (PER), locations (LOC), and organizations (ORG) in the textual data. Our core system is a CRF based system with L1-regularization, where x is the input sequence and output y is the appropriate tag sequence, and no features look beyond a history of length 1. 999 f (y |x) = |x| ∑ f (x, yi , yi−1 ). (1) i=1 Based on previous work in the area (Tjong Kim Sang and De Meulder, 2003; McCallum and Li, 2003; Sha and Pereira, 2003), we begin with a standard set of features commonly used for training NER systems: • • • • • • • • • • • Current token and Current tag Previous token and Current tag Next token and Current tag Current+previous token and current tag Current+next token and current tag First five features but with previous tag First five features conjoined with both current and previous tags Contains foreign script characters Indicator features for tokens containing digits Features about capitalization information Prefix features However, given the paucity of training data, these features are numerous and sparse,"
C16-1095,W03-0419,0,0.0542807,"Missing"
C16-1328,D16-1153,1,0.877003,"ning data for a task like this. 9 The consonant/vowel feature should have been largely equivalent to the feature [±syllabic]. 3480 Features Accuracy CER Basic Basic+phon. Basic+Kurmanji Basic+Kurmanji+phon. Basic+Tajik Basic+Tajik+phon. All features 0.635 0.669 0.701 0.721 0.661 0.664 0.721 0.237 0.234 0.223 0.221 0.231 0.228 0.221 Table 3: IPA prediction for Sorani, trained on 244 tokens, tested on 402 tokens 4.2 NER with Phonologically-Aware Neural Models We subsequently experimented with PanPhon—and its sister package, Epitran—in performing NER with a character-based LSTM-CRF architecture (Bharadwaj et al., 2016). We made this architecture phonologically-aware by substituting phonological feature vectors from PanPhon for characters. We used the resulting features in a series of NER experiments in both monolingual and transfer scenarios. As a baseline, we employed a character based LSTM-CRF NER system with features from pre-trained word vectors. In a series of monolingual experiment using CoNLL 2002 data from Spanish (see Tab. 4, adapted from (Bharadwaj et al., 2016)) it was found that substituting PanPhon and phonological attention features for orthographic and orthographic attention features (in a mo"
C16-1328,N07-2004,0,0.022074,"ographies” are ambiguous writing systems that lose segmental information present in the speech stream. In the linguistic subfield of phonology, features are often represented in square brackets with the name on the right and the value presented as +, −, or ± (indicating that the feature is binary but that the value is not known). 5 Note that alternate feature systems use the feature [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) use"
C16-1328,P10-4002,1,0.793303,"aphs) in the script. 2. Human linguists identify the possible IPA representations of each orthographic unit, using knowledge of the language and the writing system. 3. The system generates all possible hypotheses for a subset of tokens of the language using the mapping developed in the previous step. 4. Human linguists generate training data using a grammar and lexicon (Thackston, 2006) by picking one or more valid pronunciations (or, if unknown, one or more likely hypotheses) for a selection of tokens. 5. Character-level chain conditional random field (CRF) is trained (Lafferty et al., 2001; Dyer et al., 2010) on resulting data. 3479 The hypothesis space within which the CRF operates was determined by the symbol-level IPA map developed in the first step of our work-flow. It is important to note that we allowed many-to-many mapping between orthographic input character sequences and IPA output character sequences in the sense that a single input character can be mapped to multiple IPA symbols and an input multigraph (consisting of multiple orthographic characters) can be mapped to a single IPA symbol. PanPhon feature vectors were used to create one set of features that were consumed by the CRF. This"
C16-1328,J96-4003,0,0.0356167,"e [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but w"
C16-1328,W06-1107,0,0.0408455,"ogical rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonological similarity measures based, in some cases, on a kind of phonological feature system (but with a multivalued place feature unlike the binary and ternary features integral to PanPhon). 3 PanPhon and Its Functionality PanPhon facilitates further experimentation with phonological (specifically, articulatory) features. Our goal in implementing PanPhon was not to implement a state-of-the-art feature system (from the standpoint of linguistic theory) but to develop a methodologically solid resource that would be useful for NLP researchers. Contemporary feature theories posit hier"
C16-1328,N16-1030,1,0.569259,"Missing"
C16-1328,L16-1529,1,0.912655,"o two classes of tasks: orthography-to-IPA character transduction and named entity recognition (NER). While the second set of experiments (on NER) were prompted by a need to test a particular class of model—phonologically aware LSTM-CRFs—the first set were motivated by the need to solve a particular problem: how best to convert Sorani Kurdish (a Northwestern Iranian language of Iraq and Iran) from orthographic to IPA representation. 4.1 Orthography-IPA Character Transduction As part of a NER system for the low-resource Sorani Kurdish language, we developed a Soraniorthography-to-IPA converter Littell et al. (2016). This was challenging because the Sorani orthography, like many Perso-Arabic scripts, badly underdetermines the equivalent phonetic representation. The following steps summarize the workflow behind building the Sorani-to-IPA converter: 1. Human linguists identify the orthographic units (i.e., characters and multigraphs) in the script. 2. Human linguists identify the possible IPA representations of each orthographic unit, using knowledge of the language and the writing system. 3. The system generates all possible hypotheses for a subset of tokens of the language using the mapping developed in"
C16-1328,N07-2030,0,0.0345232,"s writing systems that lose segmental information present in the speech stream. In the linguistic subfield of phonology, features are often represented in square brackets with the name on the right and the value presented as +, −, or ± (indicating that the feature is binary but that the value is not known). 5 Note that alternate feature systems use the feature [±front] to account for this contrast. 4 3476 and innate feature inventory (Mielke, 2008). Likewise, in the speech sciences and speech technology, phonological features have been the subject of widespread inquiry (Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted"
C16-1328,W06-1630,0,0.0104035,"Bromberg et al., 2007; Metze, 2007). This contrasts with NLP, where phonological features have been subject to less experimentation, perhaps because of the perceived lower relevance of phonology than morphology, syntax, and semantics to NLP tasks. However, some promising NLP results have been achieved using phonological features. In one of the more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonologica"
C16-1328,P07-1015,0,0.0272154,"more widely-cited cases of this kind, Gildea and Jurafsky (1996) added phonological biases—stated in terms of phonological features—to aid the OSTIA (the Onward Subsequential Transducer Inferance Algorithm) in learning phonological rules. Subsequently , Tao et al. (2006) used handweighted articulatory feature edit distance (augmented with “pseudofeatures”) to facilitate the transliteration of named entities. This feature-based system outperformed a temporal algorithm on a English/Hindi language pair and contributed to the performance of the best model the authors tested. In a successor study, Yoon et al. (2007) employed the model of Tao et al. (2006) but with features weighted by the winnow algorithm rather than by hand; they achieved comparable results without engineered weights. In a related strain of research, Kondrak and Sherif (2006) explored phonological similarity measures based, in some cases, on a kind of phonological feature system (but with a multivalued place feature unlike the binary and ternary features integral to PanPhon). 3 PanPhon and Its Functionality PanPhon facilitates further experimentation with phonological (specifically, articulatory) features. Our goal in implementing PanPh"
C94-1057,W91-0202,1,\N,Missing
C94-1057,J94-4004,0,\N,Missing
C96-1075,P94-1045,1,0.888768,"Missing"
C96-1075,P95-1005,1,0.822394,"Missing"
C96-1075,J87-1004,0,0.258485,"signed to be language-independent in the sense that they 442 S • . • iSource n Language &gt; J I OU ~ rI "" FGenKit enerator [ . . I s °oo. Figure 1: T h e J A N U S expressions, and incorporates the sentence into a discourse plan tree. &apos;the discourse processor also updates a calendar which keeps track of what the speakers haw&apos;~ said about their schedules. The discourse processor is described in greater detail else.where (R,osd et 31. 1995). 3 The QLR Translation Module The (]LR.* parser (Lavie and Tomita 11993; I,avie 1994) is a parsing system based on Tomita&apos;s Generalized LI~ parsing algorithm (Tomita 1987). The parser skips parts of the utterance that it cannot incorporate into a well-formed sentence structure. Thus it is well-suited to doinains ill which nongrammaticality is c o a l i t i o n . T h e parser conducts a search for the maximal subset of the original input that is covered by the grammar. This is done using a beam search heuristic that limits tile combinations of skipped words considered by the parser, and ensures that it operates within feasible time and space bonnds. The GI,R* parser was implemented as an extension to the G LR parsing system, a unificationI&gt;ased practical natural"
C96-1075,C90-1012,0,\N,Missing
C98-2180,C96-1075,1,0.897687,"Missing"
C98-2180,W97-0303,1,0.891754,"Missing"
C98-2180,W94-0113,1,0.866328,"Missing"
C98-2180,P95-1005,1,0.891709,"Missing"
carbonell-etal-2002-automatic,2001.mtsummit-road.7,1,\N,Missing
clark-etal-2008-toward,carbonell-etal-2002-automatic,1,\N,Missing
clark-etal-2008-toward,probst-lavie-2004-structurally,0,\N,Missing
clark-etal-2008-toward,N06-2002,1,\N,Missing
clark-etal-2008-toward,P07-1009,0,\N,Missing
clark-etal-2008-toward,W04-2706,0,\N,Missing
D18-1196,S07-1018,0,0.0381527,"eir POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, a"
D18-1196,W04-2412,0,0.0590212,"cantly outperforms prior construction-based work on predicting causal frames (§7). Finally, we discuss how the transition system and tagger model could be adapted to more difficult SCL tasks (§8). 1691 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1691–1701 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 2.1 Background and related work Shallow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-"
D18-1196,W05-0620,0,0.124058,"Missing"
D18-1196,C14-1078,0,0.0477612,"Missing"
D18-1196,P11-2121,0,0.0145517,"transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al"
D18-1196,W11-0906,0,0.700939,"transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al"
D18-1196,J14-1002,0,0.0192042,"pular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, adverbs, conjunction"
D18-1196,Q17-1009,1,0.799106,"er, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic forms. For instance, so X that Y (example 3) is characterized by a single construction, where the form is so hadjective Xi hfinite clausal complement Y i and the meaning is X to an extreme that causes Y . Following Dunietz et al. (2017a,b), we borrow two core insights of CxG: first, that morphemes, words, MWEs, and grammar are all on equal footing as “learned pairings of form and function” (Goldberg, 2013); and second, that constructions pair patterns of surface forms directly with meanings. Thus, we can tag any surface realizations of constructions as meaning-bearing triggers (hence “surface construction labeling”). 2.3 Causal language To test the SCL approach, we examine causal language, which conveys essential semantic information and is especially rich in constructional triggers. Our data representation for causal langu"
D18-1196,W17-0812,1,0.836546,"er, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic forms. For instance, so X that Y (example 3) is characterized by a single construction, where the form is so hadjective Xi hfinite clausal complement Y i and the meaning is X to an extreme that causes Y . Following Dunietz et al. (2017a,b), we borrow two core insights of CxG: first, that morphemes, words, MWEs, and grammar are all on equal footing as “learned pairings of form and function” (Goldberg, 2013); and second, that constructions pair patterns of surface forms directly with meanings. Thus, we can tag any surface realizations of constructions as meaning-bearing triggers (hence “surface construction labeling”). 2.3 Causal language To test the SCL approach, we examine causal language, which conveys essential semantic information and is especially rich in constructional triggers. Our data representation for causal langu"
D18-1196,P15-1033,0,0.473103,"ased semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and co"
D18-1196,D15-1112,0,0.0129848,"oth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 2015; Roth, 2016). Systems in both paradigms are constrained by their underlying representations. PropBank covers only verbs and certain nominal and adjectival predicates. FrameNet’s frame-evoking elements are broader, including verbs, prepositions, adverbs, conjunctions, and even some MWEs, but must still be single words or MWEs that act like words. As Table 1 demonstrates, some semantic domains, such as causality, demand a more flexible approach. 2.2 Construction grammar CxG, which posits that the fundamental units of language are CONSTRUCTIONS—pairings of meanings with arbitrary linguistic form"
D18-1196,S15-1033,0,0.0166454,"ow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to sc"
D18-1196,W09-1201,0,0.0669418,"Missing"
D18-1196,W08-2122,0,0.0401636,"d feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Pa"
D18-1196,P17-1104,0,0.0263383,"STM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defines a novel transition system for PropBank parsing. Our similar scheme for parsing causal constructions builds on this one, extending it for cases where the spans are not contiguous. 3 The SCL task for causal language An SCL task closely resembles an SSP task, except that the triggers can be complex constructions. As a corollary, the arguments can also be discontinuous and/or overlap with each ot"
D18-1196,P14-5010,0,0.00441969,"Missing"
D18-1196,H94-1020,0,0.418482,".1 5.2.4 Embedding the action history During training, DeepCx learns vector representations of each action. To embed the action history, these action embeddings are fed as inputs into yet another LSTM cell. This LSTM’s output is the embedding of the history thus far. 5.3 Implementation details DeepCx is implemented using a refactored version of the LSTM parser codebase that performs identically to the original.2 The neural network framework, which also underlies the LSTM parser, is an early version of DyNet (Neubig et al., 2017). The LSTM parser model is pretrained on the usual Penn Treebank (Marcus et al., 1994) sections (training: 02–21; development: 22). For w, ˜ we use the same “structured skip ngram” word embeddings as the LSTM parser. See Dyer et al. (2015) for details about the embedding approach, hyperparameters, and training corpora. DeepCx gives no special treatment to out-of-vocabulary items, other than using the 0 vector for words not included in the pretrained embeddings. 1 This embedding is similar to that proposed by Roth and Lapata (2016). However, their dependency paths include the words encountered along the way and their POS tags. We experimented with adding these elements to our de"
D18-1196,W08-2121,0,0.0384448,"ion-based work on predicting causal frames (§7). Finally, we discuss how the transition system and tagger model could be adapted to more difficult SCL tasks (§8). 1691 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1691–1701 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 2.1 Background and related work Shallow semantic parsing SCL of course inherits from SSP, which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficu"
D18-1196,K16-1019,0,0.0122846,"ansition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defi"
D18-1196,Q15-1003,0,0.0155725,"which has a venerable tagging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 201"
D18-1196,N18-2023,0,0.0136457,"-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsing actions. Google’s SLING (Ringgaard et al., 2017) applies a custom-designed transition scheme for frame-based parsing and coreference resolution. Vilares and Gómez-Rodríguez (2018) develop a transition system for Abstract Meaning Representation parsing, and TUPA (Hershcovich et al., 2017) does the same for Universal Conceptual Cognitive Annotation. Both can handle discontinuous or reentrant graph structures. Most directly relevant to DeepCx is Choi and Palmer’s (2011b) work, which defines a novel transition system for PropBank parsing. Our similar scheme for parsing causal constructions builds on this one, extending it for cases where the spans are not contiguous. 3 The SCL task for causal language An SCL task closely resembles an SSP task, except that the triggers can"
D18-1196,J08-4003,0,0.00915874,"icles and Congressional hearing transcripts fully annotated for causal language. The only prior work on construction-based semantic parsing that we know of is Causeway (Dunietz et al., 2017a), also based on the B ECAUSE corpus. Causeway detects causal connectives using lexico-syntactic patterns, then applies heuristics and classifiers to tag arguments and remove false positives. It achieves moderate performance, but requires extensive tuning and feature engineering. 2.4 Transition-based systems Transition-based systems have primarily been used for dependency parsing (e.g., Nivre et al., 2007; Nivre, 2008; Chen et al., 2014; Choi and Palmer, 2011a). Indeed, our system borrows many implementation elements from Dyer et al. (2015), who describe a shift-reduce parser that embeds the stack and buffer as LSTMs. This parser employs the novel STACK LSTM data structure—an LSTM augmented with a stack pointer, enabling it to be rewound to a previous state. Transition systems have been developed for semantic tasks, as well. Titov et al. (2009), Henderson et al. (2008), and Swayamdipta et al. (2016) explore extensions of dependency parsing that interleave semantic parsing actions with syntactic 1692 parsin"
D18-1196,P16-1113,0,0.0739347,"gging tradition. For PropBank data, dozens of taggers have been developed (see Carreras and Màrquez, 2004, 2005; Surdeanu et al., 2008; Hajiˇc et al., 2009). These typically focus on argument tagging, since PropBank triggers are readily identified by their POS tags. One popular design is a multistage pipeline that identifies argument spans and then labels them. Another alternative is BIO-style classification of argument words, either with conventional classifiers or with neural networks (e.g., Collobert et al., 2011; Foland and Martin, 2015). More recent systems (e.g., Täckström et al., 2015; Roth and Lapata, 2016) use neural networks to score and label possible argument spans or heads. FrameNet-based tagging is more difficult, as triggers must be identified and disambiguated. Many FrameNet taggers have taken a pipeline approach (see, e.g., Baker et al., 2007; Das et al., 2014) in which targets are first identified with a whitelist or simple rules. They are then assigned frames, which determine the available frame elements, and finally the frame elements are identified and labeled. Again, neural networks have also been used to score argument spans and heads (Täckström et al., 2015; FitzGerald et al., 20"
D18-1366,E17-1088,1,0.86445,"iment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, whi"
D18-1366,E17-2067,0,0.445484,"haracter ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other than tags might seem to be of no use in closely-related languages with diﬀerent scri"
D18-1366,D16-1153,1,0.878386,"., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncert"
D18-1366,D16-1136,0,0.0268979,"tations for the downstream task. These two regimes are described below: CT-Joint: This model explicitly maps the word representations of the two languages into the same space by training simultaneously on both. This is achieved simply by combining the corpora of both the high-resource and the low-resource language and training jointly using the skip-gram objective, discussed above. The central intuition is as follows: once two related languages are placed in the same phonological and morphological space, they will share many subword units in common and this will make joint training proﬁtable. Duong et al. (2016) and Gouws et al. (2015) have previously shown the advantages of joint training and we observe this to be true in our case as well. CT-FineTune: This model implicitly maps the word representations of the two languages into the same space. The model attempts this by taking the learned continuous representations of the high resource subword units, referred to by x?? ??? , and uses them to initialize the model for the low resource language. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword uni"
D18-1366,P08-1088,0,0.016977,"et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown considerable performance gains across diﬀerent tasks in the low resource setting by transferring knowledge from related high-resource languages. Most existing approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of lim"
D18-1366,W17-4110,0,0.0301722,"s. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword informa"
D18-1366,D17-1302,0,0.026782,"ive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do not require morphological analyzers, but we have found that even a morphological analyzer built in 2-3 weeks can boost performance an"
D18-1366,Q15-1016,0,0.0420122,"t tokens, within a speciﬁed window of the focus word ?? and ?(?|?? ) is the probability of observing context word ? given focus word ?? . The skipgram was originally deﬁned using the softmax function: ?(?|?? ) = ??(?,?? ) ?(?? ,?) ∑? ?=1 ? (2) where ? is a scoring function mapping ? and ?? to ℝ. The summation in the denominator is over the entire vocabulary ? which makes this formulation computationally ineﬃcient as cost of gradient computation is proportional to ? which is quite large (∼ 106 ). Mikolov et al. (2013b) hence employ negative sampling to make this computation eﬃcient and robust (Levy et al., 2015) and give better representations for infrequent words4 , which is crucial for the low resource settings. Negative sampling represents the above objective function (Equation 1) using a binary logistic loss as shown below: ? 3. We produce continuous representations for each subword unit, giving researchers the ability to use them in their own tasks as they see ﬁt. The code 1 for training word embeddings and the embeddings 2 which produced the best results are publicly available. We also release morphological analyzers for Hindi and Bengali3 . 2 Skipgram Objective https://github.com/Aditi138/Embe"
D18-1366,D15-1176,0,0.0397207,"g approaches for learning cross-lingual word embeddings (Ruder, 2017) either extend the monolingual objective function by adding a cross-lingual regularization objective which is then jointly optimized or use mapping-based approaches to align similar words across languages. These post-hoc coordination methods rely on bilingual lexicons or parallel corpora, which are typically of limited quantity and uncertain quality. In this paper, we take a diﬀerent task: focusing instead on the similarity of the surface forms, phonology, or morphology of the two transfer languages. Speciﬁcally, inspired by Ling et al. (2015), who demonstrate the eﬀectiveness of character-level modeling for knowledge sharing in multilingual scenarios, we propose two approaches to transfer word embeddings using diﬀerent types of linguistically-inspired subword-level information. Both approaches focus on mapping the low resource language embeddings closer to those of the high resource language and are executed using two diﬀerent training regimes. We explore the eﬀect of diﬀerent subword units— characters, lemmas, inﬂectional properties, and phonemes— as each one oﬀers a unique linguistic insight, discussed more in Section 3. Our pro"
D18-1366,W13-3512,0,0.534869,"haracter-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embeddings are trained on morphological representations (Luong et al., 2013; Botha and Blunsom, 2014; Cotterell and Schütze, 2015). Avraham and Goldberg (2017) explicitly model lemmas (stems or citation forms) and morphological properties (the sets of which are sometimes called “tags”) for training the word embeddings. Lemmas capture information about the lexical identity of a word and are closely correlated with the semantics of a word; tags capture information about the syntactic context of a word. See Figure 1 for an example. We take inspiration from the above work in adapting these subword units for cross-lingual transfer. Phonological units: Subword units other"
D18-1366,P16-1101,0,0.272374,"r approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high depen"
D18-1366,I13-1136,0,0.066857,"Missing"
D18-1366,D17-1269,0,0.120208,"Missing"
D18-1366,L18-1429,1,0.883211,"Missing"
D18-1366,W18-1818,1,0.81319,"l trained with charngrams+lemma+morph ngrams+lemma+morph and monolingual: char-ngram+lemma+morph). The diﬀerence is striking—in the monolingual condition, the NEs are widely dispersed, but in the bilingual condition, the NEs cluster together. This suggests that phonologically-mediated transfer through Turkish is resulting in embeddings in which NEs are close to one another, relative to monolingual Uyghur embeddings. 5.4 Machine Translation Task In addition to NER, we test the performance of our proposed approaches on the MT task to test generality of our conclusions. We use XNMT toolkit 3292 (Neubig et al., 2018) to translate sentences from the low-resource language to English. We run similar transfer and monolingual experiments as done for NER. Due to space limitations, we use select subword combinations for the experiments, details of which can be found in Appendix. BLEU is used as the evaluation metric. From Table 6, we observe that the combination of character-ngrams and lemma performs the best for Uyghur (+0.1) and the combination of character-ngrams, lemma and morph gives the best performance for Bengali (+1.7), over the word baseline, which demonstrates the importance of subword units for low-r"
D18-1366,D14-1162,0,0.084187,"that of NER. We hypothesize that this is because the MT models were trained on a training set that did not have translation pairs from the high resource language. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment a"
D18-1366,N18-1202,0,0.0322711,"ge. As Qi et al. (2018) note, when training MT systems on a single language pair, it is less necessary for the embeddings to be coordinated across the languages. 6 Related Work Word Embedding Models: Most algorithms for learning embeddings take inspiration from language modeling (Bengio et al., 2003), motivated by distributional hypothesis (Harris, 1954), and employ a shallow neural network to map the words into a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word represe"
D18-1366,C16-1018,0,0.0442983,"Missing"
D18-1366,K16-1022,0,0.0265297,"t al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al. (2016) employ a language-independent method for NER by grounding non-English phrases to English Wikipedia. Interestingly, Kim et al. (2017) use separate encoders for modeling language-speciﬁc and languageagnostic features for part-of-speech (POS) tagging, and make use of no cross-lingual resources. 7 Conclusion In this paper, we explored two simple methods for cross-lingual transfer, both of which are taskindependent and use transfer learning for leveraging subword information from resource-rich languages, especially through phonological and morphological representations. CT-Joint and CTFineTune do"
D18-1366,D16-1157,0,0.0138983,"⟨⟩قارىيالمايدۇ /qarijalmajdu/ /qari-jal-ma-jdu/ qari+Verb+Pot+Neg+Pres+A3sg ‘s/he can’t care for’ Figure 1: Representations of a word in Uyghur morphologically rich languages such as Turkish, Uyghur, Hindi, and Bengali. Although, given a large enough training corpus, most or all morphological forms of a lexeme (of which there may be many) could theoretically learn to have similar vector representations, it will be vastly more data efﬁcient if we can take into account regularities of their form to model morphology explicitly. We explore the following methods for doing so: Orthographic units: Wieting et al. (2016) and Bojanowski et al. (2016) show the utility of character-level modeling by representing the focus word ?? as a set of its character ngrams, denoted 1 ∑ by u?? = |?| ?∈? x? , where ? is the set of character ngrams and x? is the vector representation of ngram ?. Such representations capture morphological information in a brute-force but principled fashion—words that share the same morpheme are more likely to share the same character ngrams than words that do not. Morphological units: Previous work has found that morphological relationships between words can be captured more directly if embedd"
D18-1366,N16-1119,0,0.0259393,"a low dimensional space. Pennington et al. (2014) built over the above local context window model by combining it with global matrix factorization (Levy and Goldberg, 2014). Recently, Peters et al. (2018) show signiﬁcant gains across various tasks by learning word vectors as hidden states of a deep bi-directional language model. This was originally conceived for resource-abundant languages, hence it is as-of-yet unclear how generalizable they are to low-resource settings. Modeling subword information: Various methods have validated the importance of modeling subword units in downstream tasks. Xu et al. (2016); Chen et al. (2015) experiment at the character level whereas Luong et al. (2013) use morphemes as a basic unit in recursive neural network (RNN) to get morphologically-aware word representations. Xu and Liu (2017) incorporate the morphemes’ meanings as part of the word representation to implicitly model the morphological knowledge. Transfer learning: Most recent works using transfer in low resource setting are coupled tightly with the downstream task. Jin and Kann (2017) use morpheme units for cross-lingual transfer in a paradigm completion task using sequence-tosequence models. Tsai et al."
D18-1366,D16-1163,0,0.0326908,"guage. The model is ﬁrst trained using all subword units on the high resource language and the learned representations are then used for initializing the subword units for the low resource language. To elucidate which pretrained subword helped the most on the low resource language, we use the same model for diﬀerent experiments, which is trained using all subword units—phoneme-ngrams, lemma and morphological properties. The linguistic intuition behind CTFineTune is similar to that behind CT-Joint. This idea of transferring parameters from high resource language has been previously explored by Zoph et al. (2016) for low resource neural machine translation which showed considerable improvement. 5 Evaluation In this section, we ﬁrst describe the model setup for training word embeddings followed by details on NER and MT experiments. 5.1 Implementation details We base our model on the C++ implementation of fasttext5 (Bojanowski et al., 2016) with modiﬁcations as described above. Data: We represent a word in the training corpus using the format presented by Avraham and Goldberg (2017). For instance, the Uyghur word in Figure 1 is represented as follows: phoneme ipa: qarijalmajdu, lemma l:qari, and morphol"
D18-1366,N18-2084,1,0.921313,"ource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU. 1 Introduction Continuous word representations have demonstrated utility in state-of-the-art neural models for several NLP tasks, such as named entity recognition (NER; Ma and Hovy (2016)), machine reading (Tan et al., 2017), sentiment analysis (Tang et al., 2016; Yu et al., 2018), and machine translation (MT; Qi et al. (2018)). While the training of these word vectors does not rely on explicit human supervision, their quality is highly contingent on the size and quality of the unlabeled corpora available. There are over 7000 languages in the world (Hammarström et al., 2018), and corpora with suﬃcient size and coverage are available for just a handful, making it unclear how these methods will perform in the more common low-resource setting. Disheartening though this high dependence on resources sounds, several eﬀorts (Adams et al., 2017; Haghighi et al., 2008; Bharadwaj et al., 2016; Mayhew et al., 2017) have shown"
D18-1366,N15-1140,0,\N,Missing
E17-2002,E14-1049,0,0.0442429,"have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors. 1 2 Motivation The recent success of “polyglot” models (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Ammar et al., 2016; Tsvetkov et al., 2016; Daiber et al., 2016), in which a language model is trained on multiple languages and shares representations across languages, represents a promising avenue for NLP, especially for less-resourced languages, as these models appear to be able to learn useful patterns from better-resourced languages even when training data in the target language is limited. Just as neural NLP raises many questions about the best representations of words and sentences, these models raise the question of the representation of languages. Tsvetkov et al. (2016) shows that v"
E17-2002,Q16-1031,0,0.160226,"d consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors. 1 2 Motivation The recent success of “polyglot” models (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Ammar et al., 2016; Tsvetkov et al., 2016; Daiber et al., 2016), in which a language model is trained on multiple languages and shares representations across languages, represents a promising avenue for NLP, especially for less-resourced languages, as these models appear to be able to learn useful patterns from better-resourced languages even when training data in the target language is limited. Just as neural NLP raises many questions about the best representations of words and sentences, these models raise the question of the representation of languages. Tsvetkov et al. (2016) shows that vectors that represen"
E17-2002,D16-1153,1,0.813035,"ta in the target language is limited. Just as neural NLP raises many questions about the best representations of words and sentences, these models raise the question of the representation of languages. Tsvetkov et al. (2016) shows that vectors that represent information about the language outperform a simple “one-hot” representation where each language is represented by a 1 in a single dimension. This result parallels the results of other recent work in sound/character representation, in which vectors of linguistically-aware features outperform one-hot character representations on some tasks (Bharadwaj et al., 2016; Introduction This article introduces lang2vec1 , a database and utility representing languages as informationrich typological, phylogenetic, and geographical vectors. lang2vec feature primarily represent binary language facts (e.g., that negation precedes the verb or is represented as a suffix, that the language is part of the Germanic family, etc.) and are sourced and predicted from a variety of linguistic resources including WALS (Dryer and Haspelmath, 2013), PHOIBLE (Moran et al., 2014), Ethnologue (Lewis et al., 2015), and Glottolog (Hammarstr¨om et al., 2015). 1 www.cs.cmu.edu/˜dmortens"
E17-2002,P14-1006,0,0.020734,"ases that are normalized to have straightforward and consistent formats, naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors. 1 2 Motivation The recent success of “polyglot” models (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Ammar et al., 2016; Tsvetkov et al., 2016; Daiber et al., 2016), in which a language model is trained on multiple languages and shares representations across languages, represents a promising avenue for NLP, especially for less-resourced languages, as these models appear to be able to learn useful patterns from better-resourced languages even when training data in the target language is limited. Just as neural NLP raises many questions about the best representations of words and sentences, these models raise the question of the representation of languages. Tsvetkov et"
E17-2002,C16-1298,0,0.104361,"Missing"
E17-2002,P07-1009,0,0.22219,"Missing"
E17-2002,C16-1328,1,0.902006,"Missing"
E17-2002,N09-1067,0,0.110911,"Missing"
E17-2002,P16-1038,0,0.030048,"t sets of languages in different levels of detail, has different formats and semantics (ranging from binary features to trees to English prose descriptions), uses different identifiers for languages and different names for features, etc. It does not take long in collecting a “polyglot” experiment like those in Ammar et al. (2016), While lang2vec was originally conceived as providing rich language representations to “polyglot” models, it can be utilized in a variety of kinds of research projects (O’Horan et al., 2016): helping to choose “bridge” or “pivot” languages for cross-lingual transfer (Deri and Knight, 2016), directly providing feature values to systems interested in those specific features, or acting as a dataset for the prediction of unknown or un9 3.1 recorded language facts (Daum´e III and Campbell, 2007; Daum´e III, 2009; Coke et al., 2016). By normalizing information from a variety of data sources, it can also allow the comparison of resources, due to format and semantic differences, that were difficult to compare directly before, and help to quantify knowledge gaps concerning world languages. 3 Typological vectors The syntax features are adapted (after conversion to binary features) from t"
E17-2002,C16-1123,0,0.0697139,"Missing"
E17-2002,C16-1097,0,0.0281671,"ation for the URIEL knowledge base and the lang2vec utility is to make such research easier, allowing different sources of information to be easily used together or as different experimental conditions (e.g., is it better to provide this model information about the syntactic features of the language, or the phylogenetic relationships between the languages?). Standardizing the use of this kind of information also makes it easier to replicate and expand on previous work, without needing to know how the authors processed, for example, WALS feature classes or PHOIBLE inventories into model input. Rama, 2016). Sample results from Tsvetkov et al. (2016) are reproduced in Table 2, measuring the perplexity of monolingual and polyglot models, trained on pronunciation dictionaries in several languages and tested on Italian and Hindi. We can see that training on a set of three similar languages, and a set of four similar and dissimilar languages, raises perplexity above the baseline monolingual model, even when the language is identified to the model by a one-hot (id) vector. However, perplexity is lowered by the introduction of phonological feature vectors for each language (the phonology and inventory"
E17-2002,N16-1161,1,0.881222,", naming, and semantics. The goal of URIEL and lang2vec is to enable multilingual NLP, especially on less-resourced languages and make possible types of experiments (especially but not exclusively related to NLP tasks) that are otherwise difficult or impossible due to the sparsity and incommensurability of the data sources. lang2vec vectors have been shown to reduce perplexity in multilingual language modeling, when compared to one-hot language identification vectors. 1 2 Motivation The recent success of “polyglot” models (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Ammar et al., 2016; Tsvetkov et al., 2016; Daiber et al., 2016), in which a language model is trained on multiple languages and shares representations across languages, represents a promising avenue for NLP, especially for less-resourced languages, as these models appear to be able to learn useful patterns from better-resourced languages even when training data in the target language is limited. Just as neural NLP raises many questions about the best representations of words and sentences, these models raise the question of the representation of languages. Tsvetkov et al. (2016) shows that vectors that represent information about the"
E89-1010,P80-1024,0,0.0158659,"odel employs the direct memory *E-mail address is hiroaki@a.nl.cs.cmu.edu. Also with NEC Corporation. - 72 - Limitations of Current Methods of Ambiguity Resolution Traditional syntactic parsers have been using attachment preferences and local syntactic and semantic constraints for resolving lexical and structural ambiguities. ([17], [28], [2], [7], [26], [11], [5]) However, these methods cannot select one interpretation from several plausible interpretations because they do not incorporate the discourse context of the sentences being parsed ([81, [4]). Connectionist-type approaches as seen in [18], [25], and [8] essentially stick to semantic restrictions and associations. However, [18], [25], [24] only provide local interactions, omitting interaction with contexL Moreover, difficulties regarding variable-binding and embedded sentences should be noticed. In [8], world knowledge is used through testing referential success and other sequential tests. However, this method does not provide a uniform model of parsing: lexical ambiguities are resolved by marker passing and structural disambiguations are resolved by applying separate sequential tests. An approach by [15] is similar to our mode"
E89-1010,C88-2137,1,0.796925,"agmatics at real-time); however the parsing (LFG, and ease-frame based) and contextual inferences (marker-passing) are not under an uniform architecture. Past generations of DMTRANS ([19], [23]) have not incorporated cost-based structural ambiguity resolution schemes. 3 3.1 Overview of DMTRANS PLUS Memory Access Parsing DMTRANS PLUS is a second generation DMA system based upon DMTRANS ([19]) with new methods of ambiguity resolution based on costs. Unlike most natural language systems, which are based on the &quot;Build-and-Store&quot; model, our system employs a &quot;Recognize-and-Record&quot; model ([14],[19], [21]). Understanding of an input sentence (or speech input in ~/iDMTRANSPLUS) is defined as changes made in a memory network. Parsing and natural language understanding in these systems are considered to be memory-access processes, identifying existent knowledge in memory with the current input. Sentences are always parsed in context, i.e., through utilizing the existing and (currently acquired) knowledge about the world. In other words, during parsing, relevant discourse entities in memory are constantly being remembered. The model behind DMTRANS PLUS is a simulation of such a process. The memory"
E89-1010,T75-2013,0,\N,Missing
feely-etal-2014-cmu,W12-5205,0,\N,Missing
feely-etal-2014-cmu,N13-1031,0,\N,Missing
feely-etal-2014-cmu,levin-etal-2014-resources,1,\N,Missing
feely-etal-2014-cmu,seraji-etal-2012-basic,0,\N,Missing
H01-1018,W00-0308,0,0.139736,"Missing"
H01-1018,W00-0203,1,0.842507,"Missing"
H01-1018,woszczcyna-etal-1998-modular,1,0.742909,"Missing"
H01-1018,H01-1003,1,\N,Missing
J12-2006,P98-1013,0,0.198951,"Missing"
J12-2006,2010.amta-papers.7,1,0.863358,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,baker-etal-2010-modality,1,0.821878,"s produced by the taggers described here. The resulting system signiﬁcantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu–English test set. This ﬁnding supports the hypothesis that both syntactic and semantic information can improve translation quality. 1. Introduction This article describes the resource- and system-building efforts of an 8-week Johns Hopkins Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT) (Baker et al. 2010a, 2010b, 2010c, 2010d). Speciﬁcally, we describe our modality/negation (MN) annotation scheme, a (publicly available) MN lexicon, and two automated MN taggers that were built using the lexicon and annotation scheme. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). Two examples of MN tagging are shown in Figure 1. Note that modality and negation are uniﬁed into single MN tags (e.g., the “Able” modality tag is combin"
J12-2006,P05-1033,0,0.0907455,"-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the parsing domain, the work of Petrov and Klein (2007) is related to the current work. In their work, rule splitting and rule merging are applied to reﬁne parse trees during machine learning. Hierarchical splitting leads to the creation of learned categor"
J12-2006,W09-3012,1,0.662316,"ators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The authors use a previously annotated corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and"
J12-2006,W10-3001,0,0.0447283,"Missing"
J12-2006,N06-1031,0,0.116492,"one manually by students who were provided a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities, modality, and negation automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added beneﬁt of the extracted translation rules is that they are capable of producing semantically tagged Urdu parses, despite the fact that the training data were processed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making"
J12-2006,P03-1054,0,0.00282903,"ed by only an English parser and tagger. Related work in syntax-based MT includes that of Huang and Knight (2006), where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank (Marcus, Marcinkiewicz, and Santorini 1993) is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic p"
J12-2006,W04-3250,0,0.0152888,"dually, each of these made modest improvements over the syntactically informed system alone. Grafting named entities onto the parse trees improved the Bleu score by 0.2 points. Modality/negation improved it by 0.3 points. Doing both simultaneously had an additive effect and resulted in a 0.5 Bleu score improvement over syntax alone. This improvement was the largest improvement that we got from anything other than the move from linguistically naive models to syntactically informed models. We used bootstrap resampling to test whether the differences in Bleu scores were statistically signiﬁcant (Koehn 2004). All of the results were a signiﬁcant improvement over Hiero (at p ≤ 0.01). The difference between the syntactic system and the syntactic system with named entities is not signiﬁcant (p = 0.38). The differences between the 5 These experiments were conducted on the devtest set, containing 883 Urdu sentences (21,623 Urdu words) and four reference translations per sentence. The BLEU score for these experiments is measured on uncased output. 432 Baker et al. Modality and Negation in SIMT syntactic system and the syntactic system with MN, and between the syntactic system and the syntactic system w"
J12-2006,P07-2045,1,0.0084265,"s in this article including the training, development (dev), incremental test set (devtest), and blind test set (test). The dev/devtest was a split of the NIST08 Urdu–English test set, and the blind test set was NIST09. Urdu set training dev devtest test English lines tokens types tokens types 202k 981 883 1,792 1.7M 21k 22k 42k 56k 4k 4k 6k 1.7M 19k 19–20k 38–41k 51k 4k 4k 5k Table 1) results in signiﬁcantly degraded translation quality compared, for example, to an Arabic–English system that has more than 100 times the amount of training data. The output in Figure 2 was produced using Moses (Koehn et al. 2007), a state-ofthe-art phrase-based MT system that by default does not incorporate any linguistic information (e.g., syntax or morphology or transliteration knowledge). As a result, words that were not directly observed in the bilingual training data were untranslatable. Names, in particular, are problematic. For example, the lack of translation for Nagaland and Nagas induces multiple omissions throughout the translated text, thus producing several instances where the holder of a claim (or belief ) is missing. This is because out-ofvocabulary words are deleted from the Moses output. We use syntac"
J12-2006,W09-0424,1,0.73648,"elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single instance of the word “not”—is very important for a correct representation of events and likewi"
J12-2006,J93-2004,0,0.0409281,"Missing"
J12-2006,A00-2030,1,0.384287,"Missing"
J12-2006,Y05-1014,0,0.102007,"rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree-grafting and machine tra"
J12-2006,W06-3907,0,0.0592182,"Missing"
J12-2006,J05-1004,0,0.125993,"Missing"
J12-2006,P02-1040,0,0.088684,"ed to the source language (in our case, Urdu) during a process of syntactic alignment. These semantic elements are subsequently used in the translation rules that are extracted from the parallel corpus. The goal of adding them to the translation rules is to constrain the space of possible translations to more grammatical and more semantically coherent output. We explored whether including such semantic elements could improve translation output in the face of sparse training data and few source language annotations. Results were encouraging. Translation quality, as measured by the Bleu metric (Papineni et al. 2002), improved when the training process for the Joshua machine translation system (Li et al. 2009) used in the SCALE workshop included MN annotation. We were particularly interested in identifying modalities and negation because they can be used to characterize events in a variety of automated analytic processes. Modalities and negation can distinguish realized events from unrealized events, beliefs from certainties, and can distinguish positive and negative instances of entities and events. For example, the correct identiﬁcation and retention of negation in a particular language—such as a single"
J12-2006,C10-2117,0,0.0335943,"Missing"
J12-2006,prasad-etal-2008-penn,0,0.0139365,"al predicates. ¨ The Prague Dependency Treebank (Hajiˇc et al. 2001; Bohmov´ a, Cinkov´a, and Hajiˇcov´a 2005) (PDT) is a multi-level system of annotation for texts in Czech and other languages, with its roots in the Prague school of linguistics. Besides a morphological layer and an analytical layer, there is a Tectogrammatical layer. The Tectogrammatical layer includes functional relationships, dependency relations, and co-reference. The PDT also integrates propositional and extra-propositional meanings in a single annotation framework. The Penn Discourse Treebank (PDTB) (Webber et al. 2003; Prasad et al. 2008) annotates discourse connectives and their arguments over a portion of the Penn Treebank. Within this framework, senses are annotated for the discourse connectives in a hierarchical scheme. Relevant to the current work, one type of tag in the scheme is the Conditional tag, which includes hypothetical, general, unreal present, unreal past, factual present, and factual past arguments. The PDTB work is related to that of Wiebe, Wilson, and Cardie (2005) for establishing the importance of attributing a belief or assertion expressed in text to its agent (equivalent to the notion of holder in our sc"
J12-2006,P08-1001,0,0.123143,"he rule extraction algorithm with augmented parse trees containing syntactic labels that have semantic annotations grafted onto them so that they additionally express semantic information. Our strategy for producing semantically grafted parse trees involves three steps: 1. The English sentences in the parallel training data are parsed with a syntactic parser. In our work, we used the lexicalized probabilistic context free grammar parser provided by Basis Technology Corporation. 2. The English sentences are MN-tagged by the system described herein and named-entity-tagged by the Phoenix tagger (Richman and Schone 2008). 3. The modality/negation and entity markers are grafted onto the syntactic parse trees using a tree-grafting procedure. The grafting procedure was implemented as part of the SIMT effort. Details are further spelled out in Section 7.2. Figure 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, t"
J12-2006,N07-2036,0,0.0114641,"tion is asserted to be real or not real). A major annotation effort for temporal and event expressions is the TimeML speciﬁcation language, which has been developed in the context of reasoning for question answering (Saur´ı, Verhagen, and Pustejovsky 2006). TimeML, which includes modality annotation on events, is the basis for creating the TimeBank and FactBank corpora (Pustejovsky et al. 2006; Saur´ı and Pustejovsky 2009). In FactBank, event mentions are marked with their degree of factuality. Recent work incorporating modality annotation includes work on detecting certainty and uncertainty. Rubin (2007) describes a scheme for ﬁve levels of certainty, referred to as Epistemic modality, in news texts. Annotators identify explicit certainty markers and also take into account Perspective, Focus, and Time. Focus separates certainty into facts and opinions, to include attitudes. In our scheme, Focus would be covered by want and belief modality. Also, separating focus and uncertainty can allow the annotation of both on one trigger word. Prabhakaran, Rambow, and Diab (2010) describe a scheme for automatic committed belief tagging. Committed belief indicates the writer believes the proposition. The a"
J12-2006,C94-1018,0,0.350259,"l. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn, Condorovdi, and Karttunen 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modiﬁers. 415 Computational Linguistics Volume 38, Number 2 Earlier work describing the difﬁculty of correctly translating modality using ma´ chine translation includes Sigurd and Gawronska (1994) and Murata et al. (2005). Sigurd ´ and Gawronska (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to speciﬁcally handle modal constructions, whereas our modal annotation approach is a part of a full translation system. We now consider other literature, relating to tree"
J12-2006,P99-1039,0,0.0211303,"e 10 illustrates how modality tags are grafted onto a parse tree. Note that although we focus the discussion here on the modality and negation, our framework is general and we were able to incorporate other semantic elements (speciﬁcally, named entities) into the SIMT effort. Once the semantically grafted trees have been produced for the parallel corpus, the trees are presented, along with word alignments (produced by the Berkeley aligner), to the rule extraction software to extract synchronous grammar rules that are both 1 For non-constituent phrases, composite CCG-style categories are used (Steedman 1999). 428 Baker et al. Modality and Negation in SIMT syntactically and semantically informed. These grammar rules are used by the decoder to produce translations. In our experiments, we used the Joshua decoder (Li et al. 2009), the SAMT grammar extraction software (Venugopal and Zollmann 2009), and special purpose-built tree-grafting software. Figure 11 shows example semantic rules that are used by the decoder. The verb phrase rules are augmented with modality and negation, taken from the semantic categories listed in Table 2. Because these get marked on the Urdu source as well as the English tran"
J12-2006,W08-0606,0,0.0640498,"d corpus of committed belief, non-committed belief, and not applicable (Diab et al. 2009), and derive features for machine learning from parse trees. The authors desire to combine their work with FactBank annotation. The CoNLL-2010 shared task (Farkas et al. 2010) was about the detection of cues for uncertainty and their scope. The task was described as “hedge detection,” that is, ﬁnding statements which do not or cannot be backed up with facts. Auxiliary verbs such as may, might, can, and so forth, are one type of hedge cue. The training data for the shared task included the BioScope corpus (Szarvas et al. 2008), which is manually annotated with negation and speculation cues and their scope, and paragraphs from Wikipedia possibly containing hedge information. Our scheme also identiﬁes cues in the form of triggers, but our desired outcome is to cover the full range of modalities and not just certainty and uncertainty. To identify scope, we use syntactic parse trees, as was allowed in the CoNLL task. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based r"
J12-2006,N07-1063,0,0.0476172,"Missing"
J12-2006,J10-2004,0,0.0138227,"is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process"
J12-2006,J09-3003,0,0.010836,"Missing"
J12-2006,W06-3119,0,0.00897083,"xternal information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain (Klein and Manning 2003). Klein and Manning’s methods include lexicalizing determiners and percent markers, making more ﬁne-grained verb phrase (VP) categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source. Wang et al. (2010) use this type of node splitting in machine translation and report a small increase in BLEU score. We use the methods described in Zollmann and Venugopal (2006) and Venugopal, Zollmann, and Vogel (2007) to induce synchronous grammar rules, a process which requires phrase alignments and syntactic parse trees. Venugopal, Zollmann, and Vogel (2007) use generic non-terminal category symbols, as in Chiang (2005), as well as grammatical categories from the Stanford parser (Klein and Manning 2003). Their method for rule induction generalizes to any set of non-terminals. We further reﬁne this process by adding semantic notations onto the syntactic non-terminals produced by a Penn Treebank trained parser, thus making the categories more informative. In the pa"
J12-2006,W08-2225,0,\N,Missing
J12-2006,J03-4002,0,\N,Missing
J12-2006,C98-1013,0,\N,Missing
L16-1529,P13-2054,0,0.122817,"e most useful orthographic features in named-entity recognition – capitalization – is absent, as the language’s Perso-Arabic script does not make a distinction between uppercase and lowercase letters. We describe a system for deriving an inferred capitalization value from closely related languages by phonological similarity, and illustrate the system using several related Western Iranian languages. Keywords: Kurdish, named-entity recognition, phonology 1. Introduction In constructing a named-entity recognition system for Sorani Kurdish (Gautier, 1998; Thackston, 2006; Walther and Sagot, 2010; Esmaili and Salavati, 2013), a low-resource Western Iranian language written in a Perso-Arabic script, we were faced with a dilemma: one of the most useful orthographic features in named-entity recognition— capitalization—is absent in Perso-Arabic writing. However, within the Western Iranian family there are several languages, including Kurmanji Kurdish, Zazaki, and Tajik, that are written in Latin or Cyrillic scripts and therefore do feature capitalization. This article details the process we developed and the challenges we faced in attempting to infer a “surrogate” capitalization feature for Sorani named entity recogn"
L16-1529,qian-etal-2010-python,0,0.24998,"egments—both simple (consisting of a single letter) and complex (consisting of a letter and one or more diacritics/modifiers—with their corresponding definitions in terms of articulatory features. A second (very simple) script validates Unicode IPA files (UTF-8 only) against this comprehensive table. 6 5 Word-initially, /ɛ/ is expressed with a preceding hamza, which eliminates the ambiguity between word-initial /ɛ/ and word-initial /h/ (Thackston, 2006); we also encountered this form when /ɛ/ follows another vowel. During the course of the project, we developed an improved version of Unitran (Qian et al., 2010) which can be used to generate a lookup table of this type rapidly. 7 This resource (PanPhon) will be made available through the ELRA Catalogue of LRs. 3320 Additionally, PanPhon includes a Python library with numerous utility functions for manipulating articulatory feature vectors and a Python class for interacting with the comprehensive IPA feature database. This class includes methods for querying the database in various ways, for querying IPA segment inventories, for fixed width pattern matching based on articulatory features, for calculating the sonority of an IPA segment, and for impleme"
L18-1611,W17-0101,0,0.0187796,"parser combinators Multi-output parsers It is straightforward to associate a parser with multiple different kinds of outputs (Hutton, 1992) letting us simultaneously write parsers that target different representations for different NLP tasks. Intuitive representation of morphological phenomena Some morphological phenomena that are awkward to express as finite-state transducers are more straightforwardly expressed in a recursive grammar, such as the kinds of templatic or circumfixal morphology that requires finite-state transducers to be extended with flag “memory” (Pretorius and Bosch, 2003; Bower et al., 2017). It is worth noting that there is no conceptual requirement that an atomic parser define a string truncation like “remove 'd' from the end of a string”, although because of the concatenative nature of most morphology this is the most common kind of atomic parser. The relationship between the input string and the remnant string can be any string-tostring transduction. In the Tigrinya system (§4.1.), we defined some parsers using regular expression substitutions, to handle some particularly difficult plurals that involve both reduplication and root-and-pattern morphology.4 Ease of extension We"
L18-1611,H89-2010,0,0.496414,"r rather than finite-state paradigm. This paradigm allows rapid development and ease of integration with other systems, although at the cost of non-optimal theoretical efficiency. These parsers produce multiple output representations simultaneously, including lemmatization, morphological segmentation, and an English word-for-word gloss, and we evaluate these representations as input for entity detection and linking and humanitarian need detection. Keywords: morphology, parsing, Tigrinya, Oromo 1. Introduction In this paper, we experiment with using parser combinators (Hutton and Meijer, 1988; Frost and Launchbury, 1989) for the rapid development of practical morphological parsers, as an alternative or supplement to the typical finitestate transducers (Karttunen and Beesley, 1992; Karttunen, 1993). This paradigm offered some practical advantages over a finite-state system, allowing parsers to be written very rapidly in a familiar programming language, although at a cost of runtime efficiency. We present morphological parsers for two Afroasiatic languages, the Tigrinya language of Eritrea and Ethiopia (§4.1.), and the Oromo language of Ethiopia and Kenya (§4.2.).1 These parsers were designed during the LoReHLT"
L18-1611,E09-2008,0,0.0426789,"stem, and roughly doubles Oromo performance with a ~.04 F1 point improvement (~.07 when weighted for occurrence). 6. Future research While this paper has presented parser combinators as if they were in opposition to finite-state methods, the two paradigms are compatible; the ability of parser combinators to incorporate arbitrary functions into their parsing paradigm means that there are no conceptual reasons why some parts of the grammar could not be parsed in a finitestate manner and others in a recursive-descent manner. We are therefore looking into the possibility of integrating Foma FSTs (Hulden, 2009) as parser functions, and/or compiling “safe” subgraphs of the grammar into finite-state systems, to take advantage of the linear time execution where it is possible. The other benefit of finite-state parsers is that they can be run “backwards” (that is, generating rather than parsing). Incorporating this ability into a parser-combinator framework would be valuable both for pure parser-combinator systems and for the hybrid systems proposed above. The small parser combinator library released with these parsers already supports this to a limited degree: as seen in the examples in §2., the syntax"
L18-1611,littell-etal-2014-morphological,1,0.846722,"ine of the grammar. specialized parsers for root-and-pattern morphology and reduplication, and combinators that allow parsing either from the left (for prefixes) and the right (for suffixes). Familiar programming syntax and environment The programming syntax and execution environment is familiar Python. Boilerplate and repetitive code (e.g., a class of morphemes all of which have a complex environmental restriction or cause a particular morphophonological change) can be automated within the code itself; it is unnecessary to have a separate transpilation or pre-processing step, as was done in (Littell et al., 2014), to enable a new command or syntactic sugar. Even complex functions entirely outside of the parsing paradigm (e.g., orthographic conversion and normalization, dictionary lookup, etc.) can be wrapped up as a parser object and integrated into the morphological grammar. 2.4. Disadvantages of parser combinators Multi-output parsers It is straightforward to associate a parser with multiple different kinds of outputs (Hutton, 1992) letting us simultaneously write parsers that target different representations for different NLP tasks. Intuitive representation of morphological phenomena Some morpholog"
L18-1611,P17-1178,0,0.0279448,"detection and linking (EDL) for LoReHLT17 was concerned with the recognition of named entities (a subset of proper nouns) in text, their categorization as one of four entity types (person, organization, location, geopolitical entity), and their linking to an external knowledge-base of entities (compiled from several existing databases). The primary metric for EDL in LoReHLT17 was typed_mention_ceaf_plus, an F1 measure of detecting the entity and getting both the category and the link correct. We used word-to-word translation with bilingual lexicons for linking entities to the knowledge-base (Pan et al., 2017). Adding lemmatization improved translation of the entities and resulted in F1 point gain for both Tigrinya and Oromo, as seen in Table 1. 5.2. Situation Frame detection Situation frames (SFs) are a structured representation of events intended to “enable information from many different data streams to be aggregated into a comprehensive, actionable understanding of the basic facts needed to mount a response to an emerging situation” (Strassel et al., 2017). Situation frame detection involves detecting eight humanitarian requirements (e.g. water, food, medicine, evacuation) and three background"
L18-1611,L16-1521,0,0.0229746,"ical morphological parsers, as an alternative or supplement to the typical finitestate transducers (Karttunen and Beesley, 1992; Karttunen, 1993). This paradigm offered some practical advantages over a finite-state system, allowing parsers to be written very rapidly in a familiar programming language, although at a cost of runtime efficiency. We present morphological parsers for two Afroasiatic languages, the Tigrinya language of Eritrea and Ethiopia (§4.1.), and the Oromo language of Ethiopia and Kenya (§4.2.).1 These parsers were designed during the LoReHLT17 “surprise-language” evaluation (Strassel and Tracey, 2016) (§3.) to support machine translation, entity detection and linking, and humanitarian need detection (Strassel et al., 2017). These parsers were operable within about 36 hours of learning the identity of the languages, although they underwent further development during the next two weeks of evaluation. 2. Parser combinators 2.1. Introduction The “parser combinator” paradigm (Burge, 1975; Wadler, 1985; Hutton and Meijer, 1988; Frost and Launchbury, 1989) is a kind of declarative programming that simultaneously defines the grammar being parsed and the executable code that parses it. This paradig"
lavie-etal-2002-nespole,W00-0203,1,\N,Missing
lavie-etal-2002-nespole,costantini-etal-2002-nespole,1,\N,Missing
lavie-etal-2002-nespole,H01-1007,1,\N,Missing
levin-etal-2000-lessons,P99-1073,0,\N,Missing
levin-etal-2000-lessons,W00-0203,1,\N,Missing
levin-etal-2000-lessons,P97-1035,0,\N,Missing
levin-etal-2014-resources,khokhlova-zakharov-2010-studying,0,\N,Missing
levin-etal-2014-resources,D10-1004,0,\N,Missing
levin-etal-2014-resources,ivanova-etal-2008-evaluating,0,\N,Missing
levin-etal-2014-resources,P14-1024,1,\N,Missing
levin-etal-2014-resources,macwhinney-fromm-2014-two,1,\N,Missing
levin-etal-2014-resources,W13-0906,1,\N,Missing
levin-etal-2014-resources,feely-etal-2014-cmu,1,\N,Missing
littell-etal-2014-morphological,cristea-etal-2008-evaluate,0,\N,Missing
monson-etal-2004-data,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2004-data,carbonell-etal-2002-automatic,1,\N,Missing
monson-etal-2008-linguistic,2001.mtsummit-road.7,1,\N,Missing
monson-etal-2008-linguistic,2004.tmi-1.1,1,\N,Missing
monson-etal-2008-linguistic,N06-2002,1,\N,Missing
monson-etal-2008-linguistic,J01-2001,0,\N,Missing
monson-etal-2008-linguistic,W07-1315,1,\N,Missing
N03-4015,W02-0717,1,\N,Missing
N03-4015,lavie-etal-2002-nespole,1,\N,Missing
N06-1018,N03-2019,0,0.0372668,"ly related work. 1 Introduction With increasing demand from ever more sophisticated NLP applications, interest in extracting and understanding temporal information from texts has seen much growth in recent years. Several works have addressed the problems of representing temporal information in natural language (Setzer, 2001; Hobbs and Pan, 2004; Saur´ı et al., 2006), extracting and/or anchoring (normalizing) temporal and event related expressions (Wiebe et al., 1998; Mani and Wilson, 2000; Schilder and Habel, 2001; Vazov, 2001; Filatova and Hovy, 2001), and discovering the ordering of events (Mani et al., 2003). Most of these works have focused on capturing temporal information contained in newswire texts, and whenever both recognition and normalization tasks of temporal expressions were attempted, the latter almost always fell far behind from the former in terms of performance. In this paper we will focus on a different combination of the problems: anchoring temporal expressions in scheduling-related emails. In our project work of building personal agents capable of scheduling meetings among different users1 , understanding temporal expressions is a crucial step. We have therefore developed and eva"
N06-1018,P00-1010,0,0.132734,"EA), and the result shows that it performs significantly better than the baseline, and compares favorably with some of the closely related work. 1 Introduction With increasing demand from ever more sophisticated NLP applications, interest in extracting and understanding temporal information from texts has seen much growth in recent years. Several works have addressed the problems of representing temporal information in natural language (Setzer, 2001; Hobbs and Pan, 2004; Saur´ı et al., 2006), extracting and/or anchoring (normalizing) temporal and event related expressions (Wiebe et al., 1998; Mani and Wilson, 2000; Schilder and Habel, 2001; Vazov, 2001; Filatova and Hovy, 2001), and discovering the ordering of events (Mani et al., 2003). Most of these works have focused on capturing temporal information contained in newswire texts, and whenever both recognition and normalization tasks of temporal expressions were attempted, the latter almost always fell far behind from the former in terms of performance. In this paper we will focus on a different combination of the problems: anchoring temporal expressions in scheduling-related emails. In our project work of building personal agents capable of schedulin"
N06-1018,W01-1309,0,0.31344,"ws that it performs significantly better than the baseline, and compares favorably with some of the closely related work. 1 Introduction With increasing demand from ever more sophisticated NLP applications, interest in extracting and understanding temporal information from texts has seen much growth in recent years. Several works have addressed the problems of representing temporal information in natural language (Setzer, 2001; Hobbs and Pan, 2004; Saur´ı et al., 2006), extracting and/or anchoring (normalizing) temporal and event related expressions (Wiebe et al., 1998; Mani and Wilson, 2000; Schilder and Habel, 2001; Vazov, 2001; Filatova and Hovy, 2001), and discovering the ordering of events (Mani et al., 2003). Most of these works have focused on capturing temporal information contained in newswire texts, and whenever both recognition and normalization tasks of temporal expressions were attempted, the latter almost always fell far behind from the former in terms of performance. In this paper we will focus on a different combination of the problems: anchoring temporal expressions in scheduling-related emails. In our project work of building personal agents capable of scheduling meetings among different"
N06-1018,W01-1313,0,0.109636,"r than the baseline, and compares favorably with some of the closely related work. 1 Introduction With increasing demand from ever more sophisticated NLP applications, interest in extracting and understanding temporal information from texts has seen much growth in recent years. Several works have addressed the problems of representing temporal information in natural language (Setzer, 2001; Hobbs and Pan, 2004; Saur´ı et al., 2006), extracting and/or anchoring (normalizing) temporal and event related expressions (Wiebe et al., 1998; Mani and Wilson, 2000; Schilder and Habel, 2001; Vazov, 2001; Filatova and Hovy, 2001), and discovering the ordering of events (Mani et al., 2003). Most of these works have focused on capturing temporal information contained in newswire texts, and whenever both recognition and normalization tasks of temporal expressions were attempted, the latter almost always fell far behind from the former in terms of performance. In this paper we will focus on a different combination of the problems: anchoring temporal expressions in scheduling-related emails. In our project work of building personal agents capable of scheduling meetings among different users1 , understanding temporal expres"
N06-1018,W01-1314,0,0.0275457,"icantly better than the baseline, and compares favorably with some of the closely related work. 1 Introduction With increasing demand from ever more sophisticated NLP applications, interest in extracting and understanding temporal information from texts has seen much growth in recent years. Several works have addressed the problems of representing temporal information in natural language (Setzer, 2001; Hobbs and Pan, 2004; Saur´ı et al., 2006), extracting and/or anchoring (normalizing) temporal and event related expressions (Wiebe et al., 1998; Mani and Wilson, 2000; Schilder and Habel, 2001; Vazov, 2001; Filatova and Hovy, 2001), and discovering the ordering of events (Mani et al., 2003). Most of these works have focused on capturing temporal information contained in newswire texts, and whenever both recognition and normalization tasks of temporal expressions were attempted, the latter almost always fell far behind from the former in terms of performance. In this paper we will focus on a different combination of the problems: anchoring temporal expressions in scheduling-related emails. In our project work of building personal agents capable of scheduling meetings among different users1 , und"
N06-2002,2005.mtsummit-posters.10,1,0.792873,"mitive subject matter that may be seen as insulting. Moreover, we did our best to avoid lexical gaps; for example, many languages do not have a single word that means winner. 7 chine learning we will use information detected from translated sentences in order to decide what parts of the feature space are redundant and what parts must be explored and translated next. A further description of this process can be read in Levin et al. (2006). Additionally, we will change from using humans to write sentences and context fields to having them generated by using a natural language generation system (Alvarez et al. 2005). We also ran small scale experiments to measure translator accuracy and consistency and encountered positive results. Hebrew and Japanese translators provided consistent, accurate translations. Large scale experiments will be conducted in the near future to see if the success of the smaller experiments will carry over to a larger scale. Translator accuracy was also an important objective and we took pains to construct natural sounding, unambiguous sentences. The context field is used to clarify the sentence meaning and spell out features that may not manifest themselves in English. 5 Tools In"
N15-1144,P14-2131,0,0.0355048,"ng (Mikolov et al., 2011; Collobert and Weston, 2008). For the POS induction task, we specifically need embeddings that capture syntactic similarities. Therefore we experiment with two types of embeddings 1311 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311–1316, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics that are known for such properties: • Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments. • Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context. We use the tool word2vec3 and Ling et al. (2015)’s modified version4 to generate both plain and structured skip-gram embeddings in nine languages. 3 Models for POS Induction In this section, we briefly review two classes of models used for POS induction (HMMs"
N15-1144,N10-1083,0,0.0184866,"ts the transition probability and p(wi |t i ) is the emission probability, the probability of a particular tag generating the word at position i.5 We consider two variants of the HMM as baselines: 3https://code.google.com/p/word2vec/ 4https://github.com/wlin12/wang2vec/ 5Terms for the starting and stopping transition probabilities are omitted for brevity. 1312 • p(wi |t i ) is parameterized as a “naïve multinomial” distribution with one distinct parameter for each word type. • p(wi |t i ) is parameterized as a multinomial logistic regression model with hand-engineered features as detailed in (Berg-Kirkpatrick et al., 2010). Gaussian Emissions. We now consider incorporating word embeddings in the HMM. Given a tag t ∈ T, instead of generating the observed word w ∈ V , we generate the (pre-trained) embedding vw ∈ Rd of that word. The conditional probability density assigned to vw |t follows a multivariate Gaussian distribution with mean µ t and covariance matrix Σ t :   exp − 12 (vw − µ t ) > Σ−1 t (v w − µ t ) p(vw ; µ t , Σ t ) = p (2π) d |Σ t | (2) This parameterization makes the assumption that embeddings of words which are often tagged as t are concentrated around some point µ t ∈ Rd , and the concentration"
N15-1144,J92-4003,0,0.205066,"model is a linear-chain CRF with feature vector λ and local feature functions f. 4.1 Choice of POS Induction Models Here, we compare the following models for POS induction: • Baseline: HMM with multinomial emissions (Kupiec, 1992), • Baseline: HMM with log-linear emissions (BergKirkpatrick et al., 2010), • Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7 • Proposed: HMM with Gaussian emissions, and • Proposed: CRF autoencoder with Gaussian reconstructions. In (Ammar et al., 2014), we explored two kinds of reconstructions w: ˆ surface forms and Brown clusters (Brown et al., 1992), and used “stupid multinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding u"
N15-1144,W06-2920,0,0.0145533,"1992), • Baseline: HMM with log-linear emissions (BergKirkpatrick et al., 2010), • Baseline: CRF autoencoder with multinomial reconstructions (Ammar et al., 2014),7 • Proposed: HMM with Gaussian emissions, and • Proposed: CRF autoencoder with Gaussian reconstructions. In (Ammar et al., 2014), we explored two kinds of reconstructions w: ˆ surface forms and Brown clusters (Brown et al., 1992), and used “stupid multinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012). This is the same set up we used in (Ammar et al., 2014). Gaussian Reconstruction. In this paper, we use ddimensional word embedding reconstructions wˆ i = vw i ∈ Rd , and replace the multi"
N15-1144,P10-4002,1,0.210161,"{1, . . . , d}.9 While the CRF autoencoder with multinomial reconstructions were carefully initialized as p( w,t ˆ |w) = p(t |w) × p( wˆ |t) ∝ p( wˆ |t) × exp λ · |w | X f(t i ,t i−1 , w) (5) i=1 4 Experiments In this section, we attempt to answer the following questions: • §4.1: Do syntactically-informed word embeddings improve POS induction? Which model performs best? • §4.2: What kind of word embeddings are suitable for POS induction? 1313 7We use the configuration with best performance which reconstructs Brown clusters. 8We used the corpus/tokenize-anything.sh script in the cdec decoder (Dyer et al., 2010) to tokenize the corpora from (Quasthoff et al., 2006). The other corpora were already tokenized. In Arabic and Italian, we found a lot of discrepancies between the tokenization used for inducing word embeddings and the tokenization used for evaluation. We expect our results to improve with consistent tokenization. 9Surprisingly, we found that estimating Σ t significantly degrades the performance. This may be due to overfitting (Shinozaki and Kawahara, 2007). Possible remedies include using a prior (Gauvain and Lee, 1994). Gaussian CRF Autoencoder V−measure Multinomial CRF Autoencoder Gaussian"
N15-1144,D07-1031,0,0.0152212,"der. One explanation is that our word embeddings were induced using larger unlabeled corpora than those used to train the POS induction models. The best results are obtained using both word embeddings and feature-rich models using the Gaussian CRF autoencoder model. This set of results suggest that word embeddings and hand-engineered features play complementary roles in POS induction. It is worth noting that the CRF autoencoder model with Gaussian reconstructions did not require careful initialization.11 10We found the V-measure results to be consistent with the many-to-one evaluation metric (Johnson, 2007). We only show one set of results for brevity. 11In (Ammar et al., 2014), we found that careful initialization for the CRF autoencoder model with multinomial reconstructions is necessary. 1314 4.2 Choice of Embeddings Standard skip-gram vs. structured skip-gram. On Gaussian HMMs, structured skip-gram embeddings score moderately higher than standard skipgrams. And as context window size gets larger, the gap widens (as shown in Fig. 2.) The reason may be that structured skip-gram embeddings give each position within the context window its own project matrix, so the smearing effect is not as pron"
N15-1144,N15-1142,1,0.1527,"gs 1311 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1311–1316, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics that are known for such properties: • Skip-gram embeddings (Mikolov et al., 2013) are based on a log bilinear model that predicts an unordered set of context words given a target word. Bansal et al. (2014) found that smaller context window sizes tend to result in embeddings with more syntactic information. We confirm this finding in our experiments. • Structured skip-gram embeddings (Ling et al., 2015) extend the standard skip-gram embeddings (Mikolov et al., 2013) by taking into account the relative positions of words in a given context. We use the tool word2vec3 and Ling et al. (2015)’s modified version4 to generate both plain and structured skip-gram embeddings in nine languages. 3 Models for POS Induction In this section, we briefly review two classes of models used for POS induction (HMMs and CRF autoencoders), and explain how to generate word embedding observations in each class. We will represent a sentence of length ` as w = hw1 , w2 , . . . , w` i ∈ V ` and a sequence of tags as t"
N15-1144,petrov-etal-2012-universal,0,0.0343714,"tinomials” as the underlying distributions for re-generating w. ˆ Data. To train the POS induction models, we used the plain text from the training sections of the CoNLL-X shared task (Buchholz and Marsi, 2006) (for Danish and Turkish), the CoNLL 2007 shared task (Nivre et al., 2007) (for Arabic, Basque, Greek, Hungarian and Italian), and the Ukwabelana corpus (Spiegler et al., 2010) (for Zulu). For evaluation, we obtain the corresponding gold-standard POS tags by deterministically mapping the language-specific POS tags in the aforementioned corpora to the corresponding universal POS tag set (Petrov et al., 2012). This is the same set up we used in (Ammar et al., 2014). Gaussian Reconstruction. In this paper, we use ddimensional word embedding reconstructions wˆ i = vw i ∈ Rd , and replace the multinomial distribution of the reconstruction model with the multivariate Gaussian distribution in Eq. 2. We again use the Baum– Welch algorithm to estimate µ t ∗ and Σ t ∗ similar to Eq. 3. The only difference is that posterior label probabilities are now conditional on both the input sequence w and the embeddings sequence v, i.e., replace p(t i = t ∗ |v) in Eq. 2 with p(t i = t ∗ |w, v). Setup. In this sectio"
N15-1144,D07-1043,0,0.15676,"ll word embeddings. Left: Models which use standard skip-gram word embeddings (i.e., Gaussian HMM and Gaussian CRF Autoencoder) outperform all baselines on average across languages. Right: comparison between standard and structured skip-grams on Gaussian HMM and CRF Autoencoder. discussed in (Ammar et al., 2014), CRF autoencoder with Gaussian reconstructions were initialized uniformly at random in [−1, 1]. All HMM models were also randomly initialized. We tuned all hyperparameters on the English PTB corpus, then fixed them for all languages. Evaluation. We use the V-measure evaluation metric (Rosenberg and Hirschberg, 2007) to evaluate the predicted syntactic classes at the token level.10 Results. The results in Fig. 1 clearly suggest that we can use word embeddings to improve POS induction. Surprisingly, the feature-less Gaussian HMM model outperforms the strong feature-rich baselines: Multinomial Featurized HMM and Multinomial CRF Autoencoder. One explanation is that our word embeddings were induced using larger unlabeled corpora than those used to train the POS induction models. The best results are obtained using both word embeddings and feature-rich models using the Gaussian CRF autoencoder model. This set"
N15-1144,P14-2044,0,0.01583,"ur findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types. However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts. This result is unsurprising, but it illustrates the lack of an evaluation metric that measures the syntactic (rather than semantic) information in word embeddings. Our results also confirm the conclusions of Sirts et al. (2014) who were likewise able to improve POS induction results, albeit using a custom clustering model based on the the distance-dependent Chinese restaurant process (Blei and Frazier, 2011). Our contributions are as follows: (i) reparameterization of token-level POS induction models to use word embeddings; and (ii) a systematic evaluation of word embeddings with respect to the syntactic information they contain. 2 Vector Space Word Embeddings Word embeddings represent words in a language’s vocabulary as points in a d-dimensional space such that nearby words (points) are similar in terms of their di"
N15-1144,C10-1115,0,0.0267207,"Missing"
N15-1144,C14-1217,0,0.0521622,"tion is the problem of assigning word tokens to syntactic categories given only a corpus of untagged text. In this paper we explore the effect of replacing words with their vector space embeddings1 in two POS induction models: the classic first-order HMM (Kupiec, 1992) and the newly introduced conditional random field autoencoder (Ammar et al., 2014). In each model, instead of using a conditional multinomial distribution2 to generate a word token wi ∈ V given a POS tag t i ∈ T, we use a conditional Gaussian distribution and generate a d-dimensional word embedding vw i ∈ Rd given t i . 1Unlike Yatbaz et al. (2014), we leverage easily obtainable and widely used embeddings of word types. 2Also known as a categorical distribution. Our findings suggest that, in both models, substantial improvements are possible when word embeddings are used rather than opaque word types. However, the independence assumptions made by the model used to induce embeddings strongly determines its effectiveness for POS induction: embedding models that model short-range context are more effective than those that model longer-range contexts. This result is unsurprising, but it illustrates the lack of an evaluation metric that meas"
N15-1144,quasthoff-etal-2006-corpus,0,\N,Missing
N16-1161,P14-2131,0,0.011684,"e tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360 words—constitute 10–"
N16-1161,P15-1033,1,0.113206,"Missing"
N16-1161,E14-1049,1,0.21174,") that polyglot phonetic feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages c"
N16-1161,D14-1012,0,0.0184753,"Missing"
N16-1161,D15-1127,0,0.0128799,"feature representations are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from"
N16-1161,D13-1196,0,0.0422633,"at (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for a long period of time (Thomason and Kaufman, 2001). Borrowed words—also called loan1360"
N16-1161,L16-1529,1,0.829863,"learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguistic matrix and 5 This matrix is described in Littell et al. (2016) and is available at https://github.com/dmort27/panphon/. 1364 manually examined aligned dimensions in the phone vectors from §5.3 (trained on six languages). In the maximally-correlated columns—corresponding to linguistic features long, consonant, nasalized—we examined phones with highest coefficients. These &gt; were: [5:, U:, i:, O:, E:] for long; [v, ñ, dZ, d, f, j, &gt; ts, N] for consonant; and [˜O, ˜E, A˜, œ] ˜ for nasalized. Clearly, the learned representation discover standard phonological features. Moreover, these top-ranked sounds are not grouped by a single language, e.g., &gt; /dZ/ is pres"
N16-1161,N15-1028,0,0.0136804,"ions are of higher quality than those learned monolingually. 1 Introduction Nearly all existing language model (LM) architectures are designed to model one language at a time. This is unsurprising considering the historical importance of count-based models in which every surface form of a word is a separately modeled entity (English cat and Spanish gato would not likely benefit from sharing counts). However, recent models that use distributed representations—in particular models that share representations across languages (Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Huang et al., 2015; Lu et al., 2015, inter alia)—suggest universal models applicable to multiple languages are a possibility. This paper takes a Exploration of polyglot language models at the sentence level—the traditional domain of language modeling—requires dealing with a massive event space (i.e., the union of words across many languages). To work in a more tractable domain, we evaluate our model on phone-based language modeling, the modeling sequences of sounds, rather than words. We choose this domain since a common assumption of many theories of phonology is that all spoken languages construct words from a finite inventor"
N16-1161,W11-2124,0,0.0106161,"gful related groupings across languages. 6 Related Work Multilingual language models. Interpolation of monolingual LMs is an alternative to obtain a multilingual model (Harbeck et al., 1997; Weng et al., 1997). However, interpolated models still require a trained model per language, and do not allow parameter sharing at training time. Bilingual language models trained on concatenated corpora were explored mainly in speech recognition (Ward et al., 1998; Wang et al., 2002; Fügen et al., 2003). Adaptations have been proposed to apply language models in bilingual settings in machine translation (Niehues et al., 2011) and code switching (Adel et al., 2013). These approaches, however, require adaptation to every pair of languages, and an adapted model cannot be applied to more than two languages. Independently, Ammar et al. (2016) used a different polyglot architecture for multilingual dependency parsing. This work has also confirmed the utility of polyglot architectures in leveraging multilinguality. Multimodal neural language models. Multimodal language modeling is integrating image/video modalities in text LMs. Our work is inspired by the neural multimodal LMs (Kiros and Salakhutdinov, 2013; Kiros et al."
N16-1161,qian-etal-2010-python,0,0.0593655,"Missing"
N16-1161,D13-1170,0,0.00194071,"Missing"
N16-1161,P15-2021,1,0.840243,"Missing"
N16-1161,D15-1243,1,0.588955,"with hand-crafted features. We found that using both feature sets added no value, suggesting that learned phone vectors are capturing information that is equivalent to the hand-engineered vectors. 5.5 Qualitative analysis of vectors Phone vectors learned by Polyglot LMs are mere sequences of real numbers. An interesting question is whether these vectors capture linguistic (phonological) qualities of phones they are encoding. To analyze to what extent our vectors capture linguistic properties of phones, we use the QVEC—a tool to quantify and interpret linguistic content of vector space models (Tsvetkov et al., 2015). The tool aligns dimensions in a matrix of learned distributed representations with dimensions of a hand-crafted linguistic matrix. Alignments are induced via correlating columns in the distributed and the linguistic matrices. To analyze the content of the distributed matrix, annotations from the linguistic matrix are projected via the maximally-correlated alignments. We constructed a phonological matrix in which 5,059 rows are IPA phones and 21 columns are boolean indicators of universal phonological properties, e.g. consonant, voiced, labial.5 We the projected annotations from the linguisti"
N16-1161,P10-1040,0,0.0233197,"nd /U/ in “bit” and “book.” Only through linguistic analysis does it become evident that (1) all languages have tense vowels—a feature based on the presence of tense vowels is uninformative and that (2) a significant minority of languages make a distinction between tense and lax vowels—a feature based on whether languages display a minimal difference of this kind would be more useful. 4 Applications of Phonetic Vectors Learned continuous word representations—word vectors—are an important by-product of neural LMs, and these are used as features in numerous NLP applications, including chunking (Turian et al., 2010), part-of-speech tagging (Ling et al., 2015), dependency parsing (Lazaridou et al., 2013; Bansal et al., 2014; Dyer et al., 2015; Watanabe and Sumita, 2015), named entity recognition (Guo et al., 2014), and sentiment analysis (Socher et al., 2013; Wang et al., 2015). We evaluate phone vectors learned by Polyglot LMs in two downstream applications that rely on phonology: modeling lexical borrowing (§4.1) and speech synthesis (§4.2). 4.1 Lexical borrowing Lexical borrowing is the adoption of words from another language, that inevitably happens when speakers of different languages communicate for"
N16-1161,P15-1130,0,0.00692841,"Missing"
N16-1161,P15-1113,0,0.0360795,"Missing"
N16-1161,P13-2037,0,\N,Missing
P13-2134,P80-1004,1,0.0803408,"gorithm becomes practical given refined resources. More broadly, this paper shows that resource quality matters tremendously, sometimes even more than algorithmic improvements. 1 Introduction A variety of NLP tasks have been addressed using selectional preferences or restrictions, including word sense disambiguation (see Navigli (2009)), semantic parsing (e.g., Shi and Mihalcea (2005)), and metaphor processing (see Shutova (2010)). These semantic problems are quite challenging; metaphor analysis, for instance, has long been recognized as requiring considerable semantic knowledge (Wilks, 1978; Carbonell, 1980). The advent of extensive lexical resources, annotated corpora, and a spectrum of NLP tools 2 The Preference Violation Detection Task DAVID builds on the insight of Wilks (1978) that the strongest indicator of metaphoricity is the violation of selectional preferences. For example, only plants can literally be pruned. If laws is the object of pruned, the verb is likely metaphorical. Flagging such semantic mismatches between verbs and arguments is the task of preference violation detection. 765 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 765–770"
P13-2134,D11-1022,0,0.0328976,"Missing"
P13-2134,J05-1004,0,0.0381744,"re also ignored. 3 pruned regulating created inspecting The politician laws The politician -- laws . . . bags plastic bags new fees dairy farms Table 1: SENNA’s SRL output for the example sentence above. Though this example demonstrates only two arguments, SENNA is capable of labeling up to six. Algorithm Design To identify violations, DAVID employs a simple algorithm based on several existing tools and resources: SENNA (Collobert et al., 2011), a semantic role labeling (SRL) system; VerbNet, a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 physical object.n.01 matter.n.01 substance.n.01 social group.n.01 district.n.01 concrete organization Table 2: DAVID’s mappings between some common VerbNet restriction types and WordNet synsets. 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). Each VerbNet restriction is interprete"
P13-2134,P10-1071,0,0.0312002,"Missing"
P13-2134,N03-1033,0,0.0042109,", a computational verb lexicon; SemLink (Loper et al., 2007), which includes mappings between PropBank (Palmer et al., 2005) and VerbNet; and WordNet. As one metaphor detection component of METAL’s several, DAVID is designed to favor precision over recall. The algorithm is as follows: Restriction WordNet Synsets animate animate being.n.01 people.n.01 person.n.01 physical object.n.01 matter.n.01 substance.n.01 social group.n.01 district.n.01 concrete organization Table 2: DAVID’s mappings between some common VerbNet restriction types and WordNet synsets. 1. Run the Stanford CoreNLP POS tagger (Toutanova et al., 2003) and the TurboParser dependency parser (Martins et al., 2011). Each VerbNet restriction is interpreted as mandating or forbidding a set of WordNet hypernyms, defined by a custom mapping (see Table 2). For example, VerbNet requires both the Patient of a verb in carve-21.2-2 and the Theme of a verb in wipe manner-10.4.1-1 to be concrete. By empirical inspection, concrete nouns are hyponyms of the WordNet synsets physical object.n.01, matter.n.03, or substance.n.04. Laws (the Patient of prune) is a hyponym of none of these, so prune would be flagged as a violation. 2. Run SENNA to identify the se"
P95-1005,1993.iwpt-1.12,0,0.0588063,"Missing"
P95-1005,C88-2120,0,0.0306128,"w is the next week? (13) If all else fails there is always video conferencing. (14) S 1: Monday, Tuesday, and Wednesday I am out of town. (15) But Thursday and Friday are both good. (16) How about Thursday at twelve? (17) $2: Sounds good. (18) See you then. Figure 1: E x a m p l e o f D e l i b e r a t i n g O v e r A M e e t i n g T i m e purpose of making our argument easy to follow. Notice that in both of these examples, the speakers negotiate over multiple alternatives in parallel. We challenge an assumption underlying the best known theories of discourse structure (Grosz and Sidner 1986; Scha and Polanyi 1988; Polanyi 1988; Mann and Thompson 1986), namely that discourse has a recursive, tree-like structure. Webber (1991) points out that Attentional State i is modeled equivalently as a stack, as in Grosz and Sidner&apos;s approach, or by constraining the current discourse segment to attach on the rightmost frontier of the discourse structure, as in Polanyi and Scha&apos;s approach. This is because attaching a leaf node corresponds to pushing a new element on the stack; adjoining a node Di to a node Dj corresponds to popping all the stack elements through the one corresponding to Dj and pushing Di on the stac"
P95-1005,J86-3001,0,\N,Missing
P98-2185,C96-1075,1,0.898713,"Missing"
P98-2185,W97-0303,1,0.892031,"Missing"
P98-2185,W94-0113,1,0.861543,"Missing"
P98-2185,P95-1005,1,0.873607,"Missing"
Q17-1009,S07-1018,0,0.0832889,"ang, 2005), which take a “constructions all the way down” approach. Some HPSG parsers and formalisms, particularly those based on the English Resource Grammar (Copestake and Flickinger, 2000; Flickinger, 2011) or Sign-Based Construction Grammar (Boas and Sag, 2012), also take constructions into account. Thus far, however, only a few attempts (e.g., Hwang and Palmer, 2015) have been made to 128 integrate constructions with robust, broad-coverage NLP tools/representations. Other aspects of our work are more closely related to previous NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational scheme"
Q17-1009,P98-1013,0,0.183874,"Missing"
Q17-1009,D14-1159,0,0.0199751,"n extremely diverse ways, demanding an operationalized concept of constructions. Recognizing causal relations also requires a combination of linguistic analysis and broader world knowledge. Additionally, causal relations are ubiquitous, both in our thinking and in our language (see, e.g., Conrath et al., 2014). Recognizing these relations is thus invaluable for many semantics-oriented applications, including textual entailment and question answering (especially for “why” questions). They are especially helpful for domain-specific applications such as finance, politics, and biology (see, e.g., Berant et al., 2014), where extracting cause and effect relationships can help drive decision-making. More general applications like machine translation and summarization, which ought to preserve stated causal relationships, can also benefit. In the remainder of this paper, we suggest two related approaches for tagging causal constructions and their arguments. We first review an annotation scheme for causal language and present a new corpus annotated using that scheme (§2). We then define the task of tagging causal language, casting it as a construction recognition problem (§3). Because it is so hard to identify"
Q17-1009,bethard-etal-2008-building,0,0.0352546,"lity. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey a"
Q17-1009,P08-2045,0,0.370297,"one of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier"
Q17-1009,bonial-etal-2014-propbank,0,0.0264304,"84.5 33.1 43.2 40.0 52.3 62.1 73.4 31.8 35.7 (b) With gold-standard parses Pipeline P Connectives R F1 SC Causes HC JC SE Effects HE JE Causeway-S w/o classifier Causeway-S w/ classifier + SC filter 10.2 62.7 70.6 51.6 17.7 56.0 79.4 80.2 98.1 96.4 45.7 45.6 52.8 59.0 90.2 92.7 41.3 43.4 Causeway-L w/o classifier Causeway-L w/ classifier + SC filter 9.1 56.4 84.1 37.9 16.4 44.3 57.8 77.0 68.2 85.3 33.3 41.8 53.0 67.2 68.0 83.4 34.4 40.4 Table 8: Results for Experiment 3. structions in English (Fillmore et al., 2012). Similar efforts are underway for VerbNet (Bonial et al., 2011) and PropBank (Bonial et al., 2014). On the NLPtools side, some work has been done on parsing text directly into constructions, particularly through the formalisms of Fluid Construction Grammar (Steels, 2012) and Embodied Construction Grammar (Bergen and Chang, 2005), which take a “constructions all the way down” approach. Some HPSG parsers and formalisms, particularly those based on the English Resource Grammar (Copestake and Flickinger, 2000; Flickinger, 2011) or Sign-Based Construction Grammar (Boas and Sag, 2012), also take constructions into account. Thus far, however, only a few attempts (e.g., Hwang and Palmer, 2015) hav"
Q17-1009,W11-0910,0,0.050187,"Missing"
Q17-1009,C14-1206,0,0.0189312,"mena that directly carry meaning. Rather than specifying by hand the constraints and properties that characterize each construction, we allow machine learning algorithms to learn these characteristics. Causal relations present an ideal testbed for this approach. As noted above, causal relations are realized in extremely diverse ways, demanding an operationalized concept of constructions. Recognizing causal relations also requires a combination of linguistic analysis and broader world knowledge. Additionally, causal relations are ubiquitous, both in our thinking and in our language (see, e.g., Conrath et al., 2014). Recognizing these relations is thus invaluable for many semantics-oriented applications, including textual entailment and question answering (especially for “why” questions). They are especially helpful for domain-specific applications such as finance, politics, and biology (see, e.g., Berant et al., 2014), where extracting cause and effect relationships can help drive decision-making. More general applications like machine translation and summarization, which ought to preserve stated causal relationships, can also benefit. In the remainder of this paper, we suggest two related approaches fo"
Q17-1009,copestake-flickinger-2000-open,0,0.0516385,"only evaluate on instances where both arguments are present, and our algorithms check for spans or tokens that at least could be arguments.) We leave addressing both limitations to future work. Nonetheless, this task is more difficult than it may appear. Two of the reasons for this are familiar issues in NLP. First, there is a surprisingly long tail of causal constructions (as we finished annotating, we 4 We use the non-collapsed enhanced dependency representation. We could have selected a parser that produces both syntactic and semantic structures, such as the English Resource Grammar (ERG; Copestake and Flickinger, 2000) or another HPSG variant. Though these parsers can produce impressively sophisticated analyses, we elected to use dependency parsers because they proved significantly more robust; there were many sentences in our corpus that we could not parse with ERG. However, incorporating semantic information from such a system when it is available would be an interesting extension for future work. Another possible input would have been semantic role labeling (SRL) tags. SRL tags could not form the basis of our system the way syntactic relations can, because they only apply to limited classes of words (pri"
Q17-1009,J14-1002,0,0.040443,"elated to previous NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causal"
Q17-1009,de-marneffe-etal-2006-generating,0,0.0713493,"Missing"
Q17-1009,W15-1622,1,0.847105,"ers and construction-independent classifiers to determine when causal constructions are truly present. We report on three sets of experiments (§5) assessing the two systems’ performance, the impacts of various design features, and the effects of parsing errors. The results indicate the viability of the approach, and point to further work needed to improve construction recognition (§6). 2 Causal Language Annotation Scheme and Corpus Causation is a slippery notion (see Schaffer, 2014), so the parameters of annotating causal language require careful definition. We follow the annotation scheme of Dunietz et al. (2015), which we now briefly review. 2.1 Causal Language Annotation Scheme The scheme of Dunietz et al. (2015) focuses specifically on causal language – language used to appeal to psychological notions of cause and effect. It is not concerned with what causal relationships hold in the real world; rather, it represents what causal relationships are asserted by the text. For example, cancer causes smoking states a false causation, but it would nonetheless be annotated. On the other hand, the bacon pizza is delicious would not be annotated, even though bacon may in fact cause deliciousness, because the"
Q17-1009,W03-1210,0,0.380695,"ndicators. This was a somewhat easier task than ours, given their much larger dataset and that they limited their causal triggers to contiguous phrases. Their dataset and methods for constructing it, however, could likely be adapted to improve our systems. Our pattern-matching techniques are based on earlier work on LEXICO - SYNTACTIC PATTERNS. These patterns, similarly represented as fragments of dependency parse trees with slots, have proven useful for hypernym discovery (Hearst, 1992; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see Grivaz, 2010). 8 Conclusion and Future Work With this work, we have demonstrated the viability of two approaches to taggi"
Q17-1009,grivaz-2010-human,0,0.447437,"; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see Grivaz, 2010). 8 Conclusion and Future Work With this work, we have demonstrated the viability of two approaches to tagging causal constructions. We hope that the constructional perspective will prove applicable to other domains, as well. Our code and corpus are available at https://github.com/ duncanka/causeway and https://github. com/duncanka/BECauSE, respectively. In the immediate future, we plan to explore more sophisticated, flexible algorithms for tagging causal constructions that rely less on fixed patterns. Two promising directions for flexible matching are tree kernels and parse forests (Tomita, 1"
Q17-1009,C92-2082,0,0.117832,"et with PDTBstyle AltLex annotations for causality. Using this corpus, they achieved high accuracy in finding causality indicators. This was a somewhat easier task than ours, given their much larger dataset and that they limited their causal triggers to contiguous phrases. Their dataset and methods for constructing it, however, could likely be adapted to improve our systems. Our pattern-matching techniques are based on earlier work on LEXICO - SYNTACTIC PATTERNS. These patterns, similarly represented as fragments of dependency parse trees with slots, have proven useful for hypernym discovery (Hearst, 1992; Snow et al., 129 2005). They have also been used both for the more limited task of detecting causal verbs (Girju, 2003) and for detecting causation relations that are not exclusively verbal (Ittoo and Bouma, 2011). Our work extends this earlier research in several ways. We propose several methods (CRF-based argument ID and statistical classifiers) for overcoming the ambiguity inherent in such patterns. We also take care to ground our notion of causality in a principled annotation scheme for causal language. This avoids the difficulties of agreeing on what counts as real-world causation (see"
Q17-1009,P16-1135,0,0.179996,"Missing"
Q17-1009,P03-1054,0,0.00848877,"spans of causal connectives are annotated. A connective span may be any set of tokens from the sentence. This can be thought of as recognizing instantiations of causal constructions. 2. Argument identification (or argument ID), in which cause and effect spans are identified for each causal connective. This can be thought of as identifying the causal construction’s slot-fillers. We assume as input a set of sentences, each with POS tags, lemmas, NER tags, and a syntactic parse in the Universal Dependencies (UD; Nivre et al., 2016) scheme, all obtained from version 3.5.2 of the Stanford parser (Klein and Manning, 2003).4 This task is defined in terms of text spans. Still, to achieve a high score on it, a tagger must respond to the meaning of the construction and arguments in context, just as annotators do. This may be achieved by analyzing indirect cues that correlate with meaning, such as lexical information, dependency labels, and tense/aspect/modality information. Compared to the annotation scheme, the task is limited in two important ways: first, we do not distinguish between types or degrees of causation; and second, we only tag instances where both the cause and the effect are present. (Even for conne"
Q17-1009,levin-etal-2014-resources,1,0.807054,"n Matching For pattern matching, we use TRegex (Levy and Andrew, 2006), a grep-inspired utility for matching patterns against syntax trees. During training, the sys122 • t2 has a child t3 via a dependency labeled mark • t3 has the lemma because and a POS tag of IN At test time, TRegex matches these extracted patterns against the test sentences. Continuing with the same example, we would recover t1 as the effect head, t2 as the cause head, and {t3 } as the connective. TRegex is designed for phrase-structure trees, so we transform each dependency tree into a PTB-like parenthetical notation (see Levin et al., 2014). Patterns involving verbs vary systematically: the verbs can become passives or verbal modifiers (e.g., the disaster averted last week), which changes the UD dependency relationships. To generalize across these, we crafted a set of scripts for TSurgeon (Levy and Andrew, 2006), a tree-transformation utility built on TRegex. The scripts normalize passive verbs and past participial modifiers into their active forms. Each sentence is transformed before pattern extraction or matching. TRegex Pattern Extraction The algorithm for extracting TRegex patterns first preprocesses all training sentences w"
Q17-1009,levy-andrew-2006-tregex,0,0.0131048,"x-based approach relies on a simple intuition: each causal construction corresponds, at least in part, to a fragment of a dependency tree, where several nodes’ lemmas and POS tags are fixed (see Figure 1). Accordingly, the first stage of Causeway-S induces lexico-syntactic patterns from the training data. At test time, it matches the patterns against new dependency trees to identify possible connectives and the putative heads of their cause and effect arguments. The second stage then expands these heads into complete argument spans. TRegex Pattern Matching For pattern matching, we use TRegex (Levy and Andrew, 2006), a grep-inspired utility for matching patterns against syntax trees. During training, the sys122 • t2 has a child t3 via a dependency labeled mark • t3 has the lemma because and a POS tag of IN At test time, TRegex matches these extracted patterns against the test sentences. Continuing with the same example, we would recover t1 as the effect head, t2 as the cause head, and {t3 } as the connective. TRegex is designed for phrase-structure trees, so we transform each dependency tree into a PTB-like parenthetical notation (see Levin et al., 2014). Patterns involving verbs vary systematically: the"
Q17-1009,H94-1020,0,0.507861,"Missing"
Q17-1009,C14-1198,0,0.064584,"nder their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey and McKeown (2016) automatically constructed a large dataset with PDTBstyle AltLex annotations for causality. Using this corpus, they achieved high accuracy in finding causality indicators. This was a somewhat easier task than ours, given their much larger dat"
Q17-1009,W16-1007,0,0.023541,"oposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. A broader-coverage linguistic approach was taken by Mirza and Tonelli (2014). They enriched TimeML to include causal links and their lexical triggers, and built an SVM-based system for predicting them. Their work differs from ours in that it requires arguments to be TimeML events; it requires connectives to be contiguous spans; and their classifier relies on gold-standard TimeML annotations. More recently, Hidey and McKeown (2016) automatically constructed a large datase"
Q17-1009,L16-1262,0,0.0202501,"Missing"
Q17-1009,J05-1004,0,0.0409024,"he restrictions of the representational schemes they are based on. Many semantic annotation schemes limit themselves to the argument structures of particular word classes. For example, the Penn Discourse Treebank (PDTB; 117 Transactions of the Association for Computational Linguistics, vol. 5, pp. 117–133, 2017. Action Editor: Christopher Potts. Submission batch: 9/2016; Revision batch: 11/2016; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Prasad et al., 2008) includes only conjunctions and adverbials as connectives,1 and PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) focus on verb arguments. FrameNet (Baker et al., 1998; Fillmore, 2012) is less restrictive, allowing many parts of speech as triggers. Most importantly, though, all these representations share the fundamental simplifying assumption that the basic linguistic carrier of meaning is the lexical unit. Some (e.g., PDTB and FrameNet) allow MWEs as lexical units, and much work has been done on detecting and interpreting MWEs (see Baldwin and Kim, 2010). But even these schemes overlook essential linguistic elements that encode meanings. In example 9, for instance, a lexical"
Q17-1009,prasad-etal-2008-penn,0,0.17783,"icon and grammar. This diversity presents a problem for most semantic parsers, which inherit the restrictions of the representational schemes they are based on. Many semantic annotation schemes limit themselves to the argument structures of particular word classes. For example, the Penn Discourse Treebank (PDTB; 117 Transactions of the Association for Computational Linguistics, vol. 5, pp. 117–133, 2017. Action Editor: Christopher Potts. Submission batch: 9/2016; Revision batch: 11/2016; Published 6/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Prasad et al., 2008) includes only conjunctions and adverbials as connectives,1 and PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) focus on verb arguments. FrameNet (Baker et al., 1998; Fillmore, 2012) is less restrictive, allowing many parts of speech as triggers. Most importantly, though, all these representations share the fundamental simplifying assumption that the basic linguistic carrier of meaning is the lexical unit. Some (e.g., PDTB and FrameNet) allow MWEs as lexical units, and much work has been done on detecting and interpreting MWEs (see Baldwin and Kim, 2010). But even these schemes over"
Q17-1009,Q15-1032,0,0.0257064,"NLP research. Our task is similar to frame-semantic parsing (Baker et al., 2007), the task of automatically producing FrameNet annotations. Lexical triggers of a frame correspond roughly to our causal connectives, and both tasks require identifying argument spans for each trigger. The tasks differ in that FrameNet covers a much wider range of semantics, with more frame-specific argument types, but its triggers are limited to lexical units, whereas we permit arbitrary constructions. Our multi-stage approach is also loosely inspired by SEMAFOR and subsequent FrameNet parsers (Das et al., 2014; Roth and Lapata, 2015; T¨ackstr¨om et al., 2015). Several representational schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more co"
Q17-1009,W16-1712,0,0.0368452,"Missing"
Q17-1009,W15-1612,0,0.0532723,"n, which marks the cause and effect spans. 3. A statistical filter to remove false matches. 4. A constraint-based filter to remove redundant connectives. Smaller connectives like to (which is causal in sentences like I left to get lunch)5 are usually spurious when a larger connective includes the same word, like cause X to Y . When a larger and a smaller connective both make it through Stage 3 together, we remove the smaller one. Because argument ID is done before filtering, the arguments output by Stage 2 do not quite represent the cause and effect arguments of a causal instance. 5 Following Schneider et al. (2015), B ECAUSE considers the “in order to” usage of the infinitive to to carry lexical meaning beyond just marking an infinitival clause. tem examines each causal language instance, generating a TRegex pattern that will match tree fragments with the same connective and argument structure. In the example from Figure 1, the generated pattern would match any tree meeting three conditions:6 worry/VBP nsubj advcl I/PRP care/VBP nsubj I/PRP mark • some token t1 has a child t2 via a dependency labeled advcl because/IN Figure 1: A UD parse for the sentence I worry because I care, with the tree fragment co"
Q17-1009,W14-2505,0,0.0304922,"Missing"
Q17-1009,Q15-1003,0,0.0797588,"Missing"
Q17-1009,L16-1603,0,0.120986,"ional schemes have incorporated elements of causal language. PDTB includes reason and result relations; FrameNet frames often include Purpose and Explanation roles; preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanation-related senses; and VerbNet and PropBank include verbs of causation. As described in §1, however, none of these covers the full range of linguistic realizations of causality. The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units (Vieu et al., 2016). Some constructions would still be too complex to represent, but under their framework, many of our insights could likely be merged into mainline English FrameNet. Other projects have attempted to address causality more specifically. For example, a small corpus of event pairs conjoined with and has been annotated as causal or not causal (Bethard and Martin, 2008), and a classifier was built for such pairs (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a l"
Q17-1009,C98-1013,0,\N,Missing
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
ries-etal-2000-shallow,J97-1003,0,\N,Missing
W00-0203,levin-etal-2000-lessons,1,0.873434,"Missing"
W02-0703,2000.iwpt-1.12,0,0.193328,"Missing"
W02-0703,H01-1007,1,0.857394,"Missing"
W02-0703,A00-1012,0,0.117687,"Missing"
W02-0703,woszczcyna-etal-1998-modular,1,0.876903,"Missing"
W02-0703,W00-0203,1,\N,Missing
W02-0708,J94-4004,0,0.0926924,"Missing"
W02-0708,W02-0717,1,0.890666,"Missing"
W02-0708,H01-1041,0,0.0396561,"Missing"
W02-0708,W00-0203,1,0.849921,"Missing"
W02-0708,lavie-etal-2002-nespole,1,\N,Missing
W03-2118,2000.iwpt-1.12,0,0.0208218,"avel, and medical in Figure 3) show only a small increase in the number of domain actions Langley et al. (2002; Langley and Lavie, 2003) describe the hybrid analysis approach that is used in the NESPOLE! system (Lavie et al., 2002). The hybrid analysis approach combines grammar-based phrasal parsing and machine learning techniques to transform utterances into our interlingua representation. Our analyzer operates in three stages to identify the domain action and arguments. First, an input utterance is parsed into a sequence of arguments using phrase-level semantic grammars and the SOUP parser (Gavaldà, 2000). Four grammars are defined for argument parsing: an argument grammar, a pseudo-argument grammar, a cross-domain grammar, and a shared grammar. The argument grammar contains phrase-level rules for parsing arguments defined in the interlingua. The pseudo-argument grammar contains rules for parsing common phrases that are not covered by interlingua arguments. For example, all booked up, full, and sold out might be grouped into a class of phrases that indicate unavailability. The cross-domain grammar contains rules for parsing complete DAs that are domain independent. For example, this grammar co"
W03-2118,W03-3014,1,0.890479,"is partially captured with a direct or indirect speech act. However, whereas speech acts are generally domain independent, task-oriented language abounds with fixed expressions that have domain specific functions. For example, the phrases We have… or There are… in the hotel reservation domain express availability of rooms in addition to their more literal meanings of possession and existence. In the past six years, we have been successful in using domain specific domain actions as the basis for translation of limited-domain taskoriented spoken language (Levin et al., 1998, Levin et al. 2002; Langley and Lavie, 2003) 4 Scalability and Portability of Domain Actions Domain actions, like speech acts, convey speaker intention. However, domain actions also represent components of meaning and are therefore more numerous than domain independent speech acts. 1168 unique domain actions are used in our NESPOLE database, in contrast to only 72 speech acts. We show in this section that domain actions yield good coverage of task-oriented domains, that domain actions can be coded effectively by humans, and that scaling up to larger domains or porting to new domains is feasible without an explosion of domain actions. Co"
W03-2118,W02-0708,1,0.814441,". Speaker intention is partially captured with a direct or indirect speech act. However, whereas speech acts are generally domain independent, task-oriented language abounds with fixed expressions that have domain specific functions. For example, the phrases We have… or There are… in the hotel reservation domain express availability of rooms in addition to their more literal meanings of possession and existence. In the past six years, we have been successful in using domain specific domain actions as the basis for translation of limited-domain taskoriented spoken language (Levin et al., 1998, Levin et al. 2002; Langley and Lavie, 2003) 4 Scalability and Portability of Domain Actions Domain actions, like speech acts, convey speaker intention. However, domain actions also represent components of meaning and are therefore more numerous than domain independent speech acts. 1168 unique domain actions are used in our NESPOLE database, in contrast to only 72 speech acts. We show in this section that domain actions yield good coverage of task-oriented domains, that domain actions can be coded effectively by humans, and that scaling up to larger domains or porting to new domains is feasible without an explo"
W03-2118,C96-1061,0,0.0232167,"eech acts and an inventory of concepts. The allowable combinations of speech acts and concepts are formalized in a human- and machine-readable specification document. The specification document is supported by a database of over 14,000 tagged sentences in English, German, and Italian. The discourse community has long recognized the potential for improving NLP systems by identifying speaker intention. It has been hypothesized that predicting speaker intention of the next utterance would improve speech recognition (Reithinger et al., Stolcke et al.), or reduce ambiguity for machine translation (Qu et al., 1996, Qu et al., 1997). Identifying speaker intention is also critical for sentence generation. We argue in this paper that the explicit representation of speaker intention using domain actions can serve as the basis for an effective language-independent representation of meaning for speech-to-speech translation and that the relevant units of speaker intention are the domain specific domain action as well as the domain independent speech act. After a brief description of our database, we present linguistic motivation for domain actions. We go on to show that although domain actions are domain spec"
W03-2118,J00-3003,0,0.203575,"Missing"
W03-2118,W02-0703,1,\N,Missing
W04-0107,carbonell-etal-2002-automatic,1,0.892932,"Missing"
W04-0107,W99-0904,0,0.186112,"ame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and b"
W04-0107,J01-2001,0,0.314787,"m a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description length local optimum. Finally, the work of Harris (1955; 1967), and later Hafer and Weiss (1974), has direct bearing on the approach taken in this paper. Couched in"
W04-0107,P04-2012,1,0.332363,"rently pursuing MT systems with Mapudungun, an indigenous language spoken by 900,000 people in southern Chile and Argentina, and Aymara, spoken by 3 million people in Bolivia, Peru, and northern Chile, as lowdensity languages and Spanish the resource rich language. A vital first step in a rule-based machine translation system is morphological analysis. This paper outlines a framework for automatic natural language morphology induction inspired by the traditional and linguistic concept of inflection classes. Additional details concerning the candidate inflection class framework can be found in Monson (2004). This paper then goes on to describe one implemented search strategy within this framework, presenting both a simple summary of results and an in depth error analysis. While the intent of this research direction is to define techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the re"
W04-0107,W00-0712,0,0.261239,"phic shape of related word forms. Next along the spectrum of orthographic similarVerb Paradigm Basic 3rd Person Singular Non-past Inflection Classes A B C blame roam solve show sow saw sing ring -/z/ -/z/ -/z/ blames roams solves shows sows saws sings rings →sang/eI/ V -/d/ -/d/ blamed roamed solved showed sowed sawed Perfective or Passive -/d/ -/n/ blamed roamed solved shown sown sawn -/i / -/i / -/i / Progressive blaming roaming solving showing sowing sawing singing ringing Past ŋ ŋ rang →sung/Λ/ V rung ŋ Table 1: A few inflection classes of the English verb paradigm ity bias is the work of Schone and Jurafsky (2000), who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work"
W04-0107,N01-1024,0,0.634042,"who first acquire a list of pairs of potential morphological variants (PPMV’s) using an orthographic similarity technique due to Gaussier (1999), in which pairs of words from a corpus vocabulary with the same initial string are identified. They then apply latent semantic analysis (LSA) to score each PPMV with a semantic distance. Pairs measuring a small distance, those whose potential variants tend to occur where a neighborhood of the nearest hundred words contains similar counts of individual high-frequency forms, are then proposed as true morphological variants of one anther. In later work, Schone and Jurafsky (2001) extend their technique to identify not only suffixes but also prefixes and circumfixes by building both forward and backward tries over a corpus. Goldsmith (2001), by searching over a space of morphology models limited to substitution of suffixes, ties morphology yet closer to orthography. Segmenting word forms in a corpus, Goldsmith creates an inventory of stems and suffixes. Suffixes which can interchangeably concatenate onto a set of stems form a signature. After defining the space of signatures, Goldsmith searches for that choice of word segmentations resulting in a minimum description le"
W04-0107,H01-1035,0,0.0603411,"efine techniques applicable to low-density languages, this paper employs English to illustrate the main conjectures and Spanish, a language with a reasonably complex morphological system, for quantitative analysis. All experiments detailed in this paper are over a Spanish newswire corpus of 40,011 tokens and 6,975 types. 2 Previous Work It is possible to organize much of the recent work on unsupervised morphology induction by considering the bias each approach has toward discovering morphologically related words that are also orthographically similar. At one end of the spectrum is the work of Yarowsky et al. (2001), who derive a morphological analyzer for a language, L, by projecting the morphological analysis of a resource-rich language onto L through a clever application of statistical machine translation style word alignment probabilities. The word alignments are trained over a sentence aligned parallel bilingual text for the language pair. While the probabilistic model they use to generalize their initial system contains a bias toward orthographic similarity, the unembellished algorithm contains no assumptions on the orthographic shape of related word forms. Next along the spectrum of orthographic s"
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W07-1315,J01-2001,0,0.538661,"scarding the large number of erroneous initially selected candidate inflection classes. Finally, with a strong grasp on the paradigm structure, ParaMor straightforwardly segments the words of a corpus into morphemes. 1.3 Related Work In this section we highlight previously proposed minimally supervised approaches to the induction of morphology that, like ParaMor, draw on the unique structure of natural language morphology. One facet of NL morphological structure commonly leveraged by morphology induction algorithms is that morphemes are recurrent building blocks of words. Brent et al. (1995), Goldsmith (2001), and Creutz (2006) emphasize the building block nature of morphemes when they each use recurring word segments to efficiently encode a corpus. These approaches then hypothesize that those recurring segments which most efficiently encode a corpus are likely morphemes. Another technique that exploits morphemes as repeating sub-word segments encodes the lexemes of a corpus as a character tree, i.e. trie, (Harris, 1955; Hafer and Weis, 1974), or as a finite state automaton (FSA) over characters (Johnson, H. and Martin, 119 2003; Altun and M. Johnson, 2001). A trie or FSA conflates multiple instan"
W07-1315,N01-1024,0,\N,Missing
W07-1315,N03-2015,0,\N,Missing
W08-0211,P97-1023,0,0.0520935,"Missing"
W08-0410,N06-2002,1,0.800422,"ture value was given to the bilingual person to translate). This is the case for the first four features and their values in Figure 1. The last two function features and their values tell us what possible roles participants and clauses can take in sentences. 5.2 Elicitation Corpus As outlined in Section 3, feature detection uses an Elicitation Corpus (see Figure 2), a corpus that has been carefully constructed to provide a large number of minimal pairs of sentences such as He sings and She sings so that only a single feature (e.g. gender) differs between the two sentences (Levin et al., 2006; Alvarez et al., 2006). If two features had varied at once (e.g. It sang) or lexical choice varied (e.g. She reads), then making assertions about which features the language does and does not express becomes much more difficult. Notice that each input sentence has been tagged with an identifier for a lexical cluster as a preprocessing step. Specifying lexical clusters ensures that we don’t compare sentences with different content just because their feature structures match. For example, we would not want to compare Dog bites man and Man bites dog nor The student snored and The professor snored. Note that bag-of-wor"
W08-0410,W07-0409,0,0.0338923,"Missing"
W08-0410,carbonell-etal-2002-automatic,1,0.888087,"showing subject-verb agreement being separated by 12 words. ment if it produced a target-side dependency tree as in Ding and Palmer (2005). However, we are not aware of any systems that attempt this. Therefore, the correct hypotheses, which have correct agreement, will likely be produces as hypotheses of traditional beam-search MT systems, but their features might not be able to discern the correct hypothesis, allowing it to fall below the 1-best or out of the beam entirely. By constructing a feature-rich grammar in a framework that allows unification-based feature constraints such as AVENUE (Carbonell et al., 2002), we can prune these bad hypotheses lacking agreement from the search space. Returning to the example of subject-verb agreement, consider the following Urdu sentences taken from the Urdu-English Elicitation Corpus in LDC’s LCTL language pack: Danish ne Amna Danish ERG Amna “Danish punished Amna.” Danish Amna ko Danish Amna DAT “Danish punishes Amna.” ko DAT sza punish sza punish di give.PERF dita give.HAB hai be.PRES These examples show the split-ergativity of Urdu in which the ergative marker ne is used only for the subject of transitive, perfect aspect verbs. In particular, since these sente"
W08-0410,P05-1033,0,0.0108628,"can augment structure-based MT models with these rich features, we believe the discriminative power of current models can be improved. We conclude by outlining how the resulting output can then be used in inducing a morphosyntactically feature-rich grammar for AVENUE, a modern syntax-based MT system. 1 2 Introduction Recent trends in Machine Translation have begun moving toward the incorporation of syntax and structure in translation models in hopes of gaining better translation quality. In fact, some structurebased systems have already shown that they can outperform phrase-based SMT systems (Chiang, 2005). Still, even the best-performing data-driven systems have not fully explored the depth of such linguistic features as morphosyntax. Certainly, many have brought linguistically motivated features into their models in the past. Huang and Knight (2006) explored relabeling of nonterminal symbols to embed more information diTask Overview Feature Detection is the process of determining from a corpus annotated with feature structures (Figure 2) which feature values (Figure 1) have a distinct representation in a target language in terms of morphemes (Figure 3). By leveraging knowledge from the field"
W08-0410,P05-1067,0,0.0270765,"roblem without using a huge number of non-terminals, each marked for all possible agreements. A syntax-based system might be able to check this sort of agreeek talb alm arshad jo mchhlyoN ke liye pani maiN aata phink raha tha . . . a.SG student named Irshad who fish for water in flour throw PROG.SG.M be.PAST.SG.M “A student named Irshad who was throwing flour in the water for the fish . . . ” Figure 5: A glossed example from parallel text in LDC’s Urdu-English LCTL language pack showing subject-verb agreement being separated by 12 words. ment if it produced a target-side dependency tree as in Ding and Palmer (2005). However, we are not aware of any systems that attempt this. Therefore, the correct hypotheses, which have correct agreement, will likely be produces as hypotheses of traditional beam-search MT systems, but their features might not be able to discern the correct hypothesis, allowing it to fall below the 1-best or out of the beam entirely. By constructing a feature-rich grammar in a framework that allows unification-based feature constraints such as AVENUE (Carbonell et al., 2002), we can prune these bad hypotheses lacking agreement from the search space. Returning to the example of subject-ve"
W08-0410,N06-1031,0,0.0188419,"ure-rich grammar for AVENUE, a modern syntax-based MT system. 1 2 Introduction Recent trends in Machine Translation have begun moving toward the incorporation of syntax and structure in translation models in hopes of gaining better translation quality. In fact, some structurebased systems have already shown that they can outperform phrase-based SMT systems (Chiang, 2005). Still, even the best-performing data-driven systems have not fully explored the depth of such linguistic features as morphosyntax. Certainly, many have brought linguistically motivated features into their models in the past. Huang and Knight (2006) explored relabeling of nonterminal symbols to embed more information diTask Overview Feature Detection is the process of determining from a corpus annotated with feature structures (Figure 2) which feature values (Figure 1) have a distinct representation in a target language in terms of morphemes (Figure 3). By leveraging knowledge from the field of language typology, we know what types of phenomena are possible across languages and, thus, which features to include in our feature specification. But not every language will display each of these phenomena. Our goal is to determine which feature"
W08-0410,D07-1091,0,0.0246453,"t aspect. If, during translation, a hypothesis is proposed that does not meet either of these conditions, unification will fail and the hypothesis will be pruned 1 . Certainly, unification-based grammars are not the 1 If the reader is not familiar with Unification Grammars, we recommend Kaplan (1995) 81 only way in which this rich source of linguistic information could be used to augment a structure-based translation system. One could also imagine a system in which the feature annotations are simply used to improve the discriminative power of a model. For example, factored translation models (Koehn and Hoang, 2007) retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features. Similarly, there exists a continuum of degrees to which this linguistic information can be used in current syntax-based MT systems. As modern systems move toward integrating many features (Liang et al., 2006), resources such as this will become increasingly important in improving translation quality. 5 System Description In the following sections, we will describe the process of inductive feature detection by way of a running example. 5.1 Feature Specification The first input to our system i"
W08-0410,P06-1096,0,0.0171477,"n which this rich source of linguistic information could be used to augment a structure-based translation system. One could also imagine a system in which the feature annotations are simply used to improve the discriminative power of a model. For example, factored translation models (Koehn and Hoang, 2007) retain the simplicity of phrase-based SMT while adding the ability to incorporate additional features. Similarly, there exists a continuum of degrees to which this linguistic information can be used in current syntax-based MT systems. As modern systems move toward integrating many features (Liang et al., 2006), resources such as this will become increasingly important in improving translation quality. 5 System Description In the following sections, we will describe the process of inductive feature detection by way of a running example. 5.1 Feature Specification The first input to our system is a feature specification (Figure 1). The feature specification used for this experiment was written by an expert in language typology and is stored in a human-readable XML format. It is intended to cover a large number of phenomena that are possible in the languages of the world. Note that features beginning w"
W08-0410,W07-1315,1,0.803627,"manually written rules. We define inductive feature detection as a recall-oriented task since its output is intended to be analyzed by a Morphosyntactic Lexicon Generator, which will address the issue of precision. This, in turn, allows us to inform a rule learner about which language features can be clustered and handled by a single set of rules and which must be given special attention. However, due to the complexity of this component, describing it is beyond the scope of this paper. We also note that future work will include the integration of a morphology analysis system such as ParaMor (Monson et al., 2007) to extract and annotate the valuable morphosyntactic information of inflected languages. An example of this processing pipeline is given in Figure 4. Feature np-gen np-gen np-gen np-def np-def np-num np-num c-ten c-ten Value m f n + sg dl-pl past-pres fut Candidate Morphemes el, ni˜no ella, ni˜na *unobserved* el, la, las una, unas el, ella, la, una, come, ni˜no, ni˜na las, unas, comen, ni˜nas – *unobserved* Figure 3: An example of the output of our system for the above corpus: a list of feature-morpheme pairings. Elicitation Corpus Decoder Inductive Feature Detection Morphosyntactic Lexicon G"
W08-0410,clark-etal-2008-toward,1,\N,Missing
W08-0708,J01-2001,0,0.0655504,"al. (1995) take this approach. A third technique leverages inflectional paradigms as the organizational structure of morphology. The ParaMor algorithm, which this paper extends, joins Snover (2002), Zeman (2007), and Goldsmith’s Linguistica in building morphology models around the paradigm. ParaMor tackles three challenges that face morphology induction systems which Goldsmith&apos;s Linguistica algorithm does not yet address. First, section 2.2 of this paper introduces an agglutinative segmentation model. This agglutinative model segments words into as many morphemes as the data justify. Although Goldsmith (2001) and Goldsmith and Hu (2004) discuss ideas for segmenting individual words into more than two morphemes, the implemented Linguistica algorithm, as presented in Goldsmith (2006), permits at most a single morpheme boundary in each word. Second, ParaMor decouples the task of paradigm identification from that of word segmentation (Monson et al., 2007b). In contrast, morphology models in Linguistica inherently encode both a belief about paradigm structure on individual words as well as a segmentation of those words. Without ParaMor’s decoupling of paradigm structure from specific segmentation model"
W08-0708,W07-1315,1,0.826288,"ently lack morphological analysis systems. Unsupervised induction could facilitate, for these lesser-resourced languages, the quick development of morphological systems from raw text corpora. Unsupervised morphology induction has been shown to help NLP tasks including speech recognition (Creutz, 2006) and information retrieval (Kurimo et al., 2007b). In this paper we work with languages like Spanish, German, and Turkish for which morphological analysis systems already exist. The baseline ParaMor algorithm which we extend here competed in the English and German tracks of Morpho Challenge 2007 (Monson et al., 2007b). The peer operated competitions of the Morpho Challenge series standardize the evaluation of unsupervised morphology induction algorithms (Kurimo et al., 2007a; 2007b). The ParaMor algorithm showed promise in the 2007 Challenge, placing first in the linguistic evaluation of German. Developed after the close of Morpho Challenge 2007, our improvements to the ParaMor algorithm could not officially compete in this Challenge. However, the Morpho Challenge 2007 Organizing Committee (Kurimo et al., 2008) graciously oversaw the quantitative evaluation of our agglutinative version of ParaMor. Abstra"
W08-0708,N01-1024,0,\N,Missing
W08-0708,P07-1013,0,\N,Missing
W08-0708,N03-2015,0,\N,Missing
W08-0708,H05-1085,0,\N,Missing
W09-3012,J04-3002,0,0.199612,"Missing"
W09-3012,krestel-etal-2008-minding,0,0.0374878,"Missing"
W09-3012,C08-1101,0,0.0165575,"is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil"
W09-3012,W05-0308,0,0.0144516,"d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief"
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W13-2234,W11-2103,0,0.0339276,"Missing"
W13-2234,P13-1110,0,0.02121,"Missing"
W13-2234,2012.eamt-1.60,0,0.0414271,"Missing"
W13-2234,P05-1045,0,0.0316055,"Missing"
W13-2234,2011.iwslt-evaluation.19,0,0.221029,"including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold."
W13-2234,E12-1068,0,0.0894067,"e need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing o"
W13-2234,N12-1047,0,0.012334,"matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data. In language pairs that ar"
W13-2234,P06-1121,0,0.169489,"Missing"
W13-2234,J07-2003,0,0.343534,"Missing"
W13-2234,P11-2031,1,0.83514,"say He is in the hospital while UK English speakers might prefer He is in hospital. 7 https://github.com/redpony/creg 8 Preliminary experiments indicated that the excess of N labels resulted in poor performance. 274 et al., 2007) to train a baseline phrase-based SMT system. Each configuration we compare has a different phrase table, with synthetic phrases generated with best-first or iterative strategies, from a phrase table with- or without-determiners, with variable number of translation features. To verify that system improvement is consistent, and is not a result of optimizer instability (Clark et al., 2011), we replicate each experimental setup three times, and then estimate the translation quality of the median MT system using the MultEval toolkit.9 The corpus is the same as in Section 4.3: the training part contains 112,527 sentences from Russian-English TED corpus, randomly sampled 3K sentences are used for tuning and a disjoint set of 2K sentences is used for test. We lowercase both sides, and use Stanford CoreNLP10 tools to tokenize the corpora. We employ SRILM toolkit (Stolcke, 2002) to linearly interpolate the target side of the training corpus with the WMT English corpus, optimizing towa"
W13-2234,N12-1023,0,0.011934,"t sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics standard phrase-based translation approaches, and why this technique might be better than some alternative proposals that been made for generalizing beyond translation examples directly observable in the training data."
W13-2234,P11-1004,0,0.0666999,"billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7"
W13-2234,C08-1022,0,0.0705118,"Missing"
W13-2234,D07-1091,0,0.036584,"of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrates the synthesis of translation options into the decoder. While related in spirit, their method only supports a limited set of generative processes for producing the candidate set (lacking, for instance, the simple and effective phrase post-editing process we have used), and Conclusions and future work The contribution of this work is twofold. First, we propose a new su"
W13-2234,P10-1147,0,0.0342768,"Missing"
W13-2234,W00-1308,0,0.176747,"Missing"
W13-2234,N03-1017,0,0.0089937,"usual. Doing so, we obtain significant improvements in quality relative to a standard phrase-based baseline and to a to post-editing complete translations with the classifier. 1 Introduction Phrase-based translation works as follows. A set of candidate translations for an input sentence is created by matching contiguous spans of the input against an inventory of phrasal translations, reordering them into a target-language appropriate order, and choosing the best one according to a discriminative model that combines features of the phrases used, reordering patterns, and target language model (Koehn et al., 2003). This relatively simple approach to translation can be remarkably effective, and, since its introduction, it has been the basis for further innovations, including developing better models for distinguishing the good translations from bad ones (Chiang, 2012; Gimpel and Smith, 2012; Cherry and Foster, 2012; 2 Why Synthetic Translation Options? Before turning to the problem of generating English articles, we give arguments for why synthetic translation options are a useful extension of 271 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 271–280, c Sofia, Bulgaria, Au"
W13-2234,P07-2045,1,0.013907,"Missing"
W13-2234,W00-0708,0,0.154151,"him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabr"
W13-2234,W07-0704,0,0.101819,"Missing"
W13-2234,P02-1040,0,0.0861695,"Missing"
W13-2234,N10-1018,0,0.0318527,"determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject makes when producing or interpreting a phrase. 7 Related Work 8 Automated determiner prediction has been found beneficial in a variety of applications, including postediting of MT output (Knight and Chander, 1994), text generation (Elhadad, 1993; Minnen et al., 2000), and more recently identification and correction of ESL errors (Han et al., 2006; De Felice and Pulman, 2008; Gamon et al., 2009; Rozovskaya and Roth, 2010). Our work on determiners extends previous studies in several dimensions. While all previous approaches were tested only on NP constructions, we evaluate our classifier on any sequence of tokens. To the best of our knowledge, the only studies that directly address generation of synthetic phrase table entries was conducted by Chen et al. (2011) and Koehn and Hoang (2007). The former find semantically similar source phrases and produce “fabricated” translations by combining these source phrases with a set of their target phrases; however, they do not observe improvements. The later work integrat"
W13-2234,P06-1132,0,0.0145425,"eed to feed the three billion urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiabi"
W13-2234,N07-1007,0,0.0515896,"n urban hundreds of them . we need to feed three billion people in the city . Table 5: Examples of translations with improved articles handling. their implementation has been plagued by computational challenges. Post-processing techniques have been extremely popular. These can be understood as using a translation model to generate a translation skeleton (or k-best skeletons) and then post-editing these in various ways. These have been applied to translation into morphologically rich languages, such as Japanese, German, Turkish, and Finnish (de Gispert et al., 2005; Suzuki and Toutanova, 2006; Suzuki and Toutanova, 2007; Fraser et al., 2012; Clifton and Sarkar, 2011; Oflazer and Durgar El-Kahlout, 2007). billion people refers to a nonidentifiable referent. The baseline inserts the definite article the. If a human subject reads this translation, it would mislead him/her to interpret the object three billion people as referring to a specific identifiable set. Our system, on the other hand, correctly selects the determiner class N and hence does not insert an article. Thus we see that our system does not just add fluency but it also captures a semantic distinction, namely identifiability, that a human subject m"
W13-2234,D08-1033,0,\N,Missing
W13-2234,I08-1059,0,\N,Missing
W13-3403,W08-0211,1,0.886205,"Missing"
W14-3909,C82-1023,0,0.643512,"Missing"
W14-3909,N13-1131,0,0.163962,"Missing"
W14-3909,D14-1098,0,0.0717799,"Missing"
W14-3909,W14-1303,0,0.0366131,"a: semi-supervised learning, word embeddings, and word lists. 1 Introduction Code switching (CS) occurs when a multilingual speaker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framewor"
W14-3909,Q14-1003,0,0.0518801,"Missing"
W14-3909,D13-1084,0,0.187942,"Missing"
W14-3909,W14-3907,0,0.0215797,"ker uses more than one language in the same conversation or discourse. Automatic idenefication of the points at which code switching occurs is important for two reasons: (1) to help sociolinguists analyze the frequency, circumstances and motivations related to code switching (Gumperz, 1982), and (2) to automatically determine which language-specific NLP models to use for analyzing segments of text or speech. CS is pervasive in social media due to its informal nature (Lui and Baldwin, 2014). The first workshop on computational approaches to code switching in EMNLP 2014 organized a shared task (Solorio et al., 2014) on identifying code switching, providing training data of multilingual tweets with token-level language-ID annotations. See §2 for a detailed description of the shared task. This short paper documents our submission in the shared task. We note that constructing a CS data set that is annotated at the token level requires remarkable manual effort. However, collecting raw tweets is easy and fast. We propose leveraging both labeled and unlabeled data in a unified framework; conditional random field autoencoders (Ammar et al., 2 Task Description The shared task training data consists of code– swit"
W14-3909,P10-1040,0,0.0720813,"ven by: p(y, x ˆ |x, φ) = |x| Y i=1 y:|y|=|x| ``semi (λ, θ) = cL2 ||λ||22 + RDirichlet (θ, α)+ X X cunlabeled × log p(y, x ˆ |x)+ hx,ˆ xi∈U clabeled × X y:|y|=|x| log p(y |x) hx,yi∈L We use block coordinate descent to optimize this objective. First, we use cem iterations of the expectation maximization algorithm to optimize the θ-block while the λ-block is fixed, then we optimize the λ-block with clbfgs iterations of L-BFGS (Liu et al., 1989) while the θ-block is fixed.4 4.2 Unsupervised Word Embeddings For many NLP tasks, using unsupervised word representations as features improves accuracy (Turian et al., 2010). We use word2vec (Mikolov et al., 2013) to train 100–dimensional word embeddings from a large Twitter corpus of about 20 million tweets extracted from the live stream, in multiple languages. We define an additional feature function θxˆi |yi × P|x| exp λ&gt; i=1 f (x, yi−1 , yi , i, φ) P P|x| 0 0 &gt; y0 exp λ i=1 f (x, yi−1 , yi , i, φ) where λ is a vector of CRF feature weights, f is a vector of local feature functions (we use the same features described in §3.2), and θxˆi |yi are categor4 An open source efficient c++ implementation of our method can be found at https://github.com/ldmt-muri/alignm"
W15-1622,P98-1013,0,0.219728,"nguistics limit the complexity of annotation by focusing not on the hairy metaphysics of causation, but on the assertions about causation that are explicit in the language. We ultimately plan to use this scheme in an automated causal information extraction system. Our second contribution is to compare two approaches to annotating causality, one using an annotation manual only and the other using a constructicon developed by an expert along with an annotation manual. The constructicon-based methodology is similar to the two-stage methodology used in PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) annotations: an initial phase of corpus lexicography produces a lexicon, followed by a second phase in which annotators identify instances of the lexical frames in a corpus. In our case, the “lexicon” is a list of English constructions that conventionally express causality. We also offer suggestions for when such an approach may be appropriate. Finally, we discuss the broader implications of our experience for difficult annotation tasks. In particular, we address the concern of arbitrariness in schemes which can only be successfully be applied by experts or highly trained annotators. 2 2.1 Re"
W15-1622,C14-1206,0,0.0629989,"oting or hindering another (the effect). The cause and effect must be deliberately related by an explicit causal connective. (As emphasized above, the words “presented as” are essential to this definition.) Causal relations can be expressed in English in many different ways. In this project, we exclude: • Causal relationships with no lexical trigger. We do not annotate implicit causal relationships (“zero” discourse connectives). We expect our work to be compatible with other work on such relationships, such as the implicit relations in the PDTB and systems for recovering those relationships (Conrath et al., 2014). • Connectives that lexicalize the means or the result of the causation. For example, kill can be interpreted as “cause to die,” but it encodes the result, so we exclude it. This decision was made to allow the scheme to focus specifically on language that expresses causation. If lexical causatives were included, nearly every transitive verb in the English language would have to be considered causal; it would be impossible to disentangle causation as a semantic phenomenon with its own linguistic realizations. It would also be impossible to annotate the cause and effect separately from the conn"
W15-1622,S07-1003,0,0.166541,"Missing"
W15-1622,grivaz-2010-human,0,0.164499,", rather than causal language. SemEval 2007 included a task (Girju et al., 2007) concerning classifying semantic relations between nominals, including causal relations. As part of this task, the organizers provided a dataset tagged with noun-noun relations. However, this task relied on a less precise, common-sense notion of real-world causation, and the annotations do not indicate the causal connectives, presumably because real-world causal relationships may not be indicated in the text. The SemEval data also limited the causes and effects to nouns (in our experience, they are often clauses). Grivaz (2010) finds that human annotators struggle to apply standard philosophical tests to make binary decisions about the presence of causation in a text segment. She suggests alternative criteria, which we take into account in our coding manual. 189 Many of her criteria, however, are concerned with how people identify real-world causal relationships, rather than how speakers or writers explicitly invoke the concept of causality. The Richer Events Description schema has also incorporated cause/effect relations (Ikuta et al., 2014). This effort, too, is concerned with bringing annotators to agreement on w"
W15-1622,W14-2903,0,0.14281,"Missing"
W15-1622,W14-0702,0,0.197127,"several relation types that are relevant to causation (primarily C AUSE and R EASON). Its representation of causal relations is limited in three important ways that we attempt to overcome. First, it does not capture the subtleties of different types of causal relationships. Second, it is limited to discourse relations, and so excludes other realizations of the relationship (e.g., verb arguments). Finally, its relation hierarchy fails to capture overlaps between the semantics of different discourse phenomena (e.g., hypotheticals may also be causal). Closer to our work is the scheme proposed by Mirza et al. (2014), who base their representation on Talmy’s “force dynamics” model of causation (Talmy, 1988). Their model is rich enough to capture linguistic triggers of causation, as well as causes and effects. It particularly follows Wolff’s (2005) taxonomy of expressions of causation. However, like the PDTB, it does not distinguish the different types of causal relationships. It also does not rigorously define what it counts as causal, and like Ikuta’s work, it is limited to event-event relations. The project most similar in spirit to ours is BioCause (Mih˘ail˘a et al., 2013), which provides an annotation"
W15-1622,J05-1004,0,0.097244,"15 Association for Computational Linguistics limit the complexity of annotation by focusing not on the hairy metaphysics of causation, but on the assertions about causation that are explicit in the language. We ultimately plan to use this scheme in an automated causal information extraction system. Our second contribution is to compare two approaches to annotating causality, one using an annotation manual only and the other using a constructicon developed by an expert along with an annotation manual. The constructicon-based methodology is similar to the two-stage methodology used in PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) annotations: an initial phase of corpus lexicography produces a lexicon, followed by a second phase in which annotators identify instances of the lexical frames in a corpus. In our case, the “lexicon” is a list of English constructions that conventionally express causality. We also offer suggestions for when such an approach may be appropriate. Finally, we discuss the broader implications of our experience for difficult annotation tasks. In particular, we address the concern of arbitrariness in schemes which can only be successfully be applied by experts or h"
W15-1622,prasad-etal-2008-penn,0,0.340978,"2014). This effort, too, is concerned with bringing annotators to agreement on what counts as realworld causation. It is also limited to event-event relations, even though causal language often describes states or objects as causes or effects. 2.2 Annotating Causal Language Other projects have, to a greater or lesser extent, focused on annotating stated causal relationships, much as we have. In general, our scheme attempts to be more precise in its definitions, more general in its scope, and more rich in its representational capacity than these prior works. The Penn Discourse TreeBank (PDTB; Prasad et al., 2008) includes several relation types that are relevant to causation (primarily C AUSE and R EASON). Its representation of causal relations is limited in three important ways that we attempt to overcome. First, it does not capture the subtleties of different types of causal relationships. Second, it is limited to discourse relations, and so excludes other realizations of the relationship (e.g., verb arguments). Finally, its relation hierarchy fails to capture overlaps between the semantics of different discourse phenomena (e.g., hypotheticals may also be causal). Closer to our work is the scheme pr"
W15-1622,J14-1009,0,0.0173849,"rhaps the annotation scheme fails to capture meaningful semantic categories – perhaps it is merely a fiction of the minds of its designers. It is to this concern that we turn next. 7 What Does Low Non-Expert Agreement Say About Validity? What imparts validity to an annotation scheme is a fundamental question that haunts every annotation project. Even a well-thought-out scheme can include arbitrary, empirically meaningless decisions, which would seem to undermine the scheme’s value as a description of a real linguistic phenomenon.3 This risk of arbitrariness is precisely what appears to bother Riezler (2014) in his discussion of circularity in computational linguistics: it is entirely possible that an annotation scheme has high interannotator agreement and can even be reproduced by software, and yet the scheme is empirically empty. The agreement can be achieved simply by developing a shared body of implicit, arbitrary theoretical assumptions among expert or intensively trained coders. Meanwhile, the fact that the annotations can be reproduced automatically shows only that the theory can be expressed both as an annotation scheme and as an annotation machine, not that it encapsulates something mean"
W15-1622,E12-2021,0,0.0564496,"Missing"
W15-1622,C98-1013,0,\N,Missing
W17-0812,bethard-etal-2008-building,0,0.0271808,"ons, which are not expressed as discourse connectives. BECauSE 2.0 can be thought of as an adaptation of PDTB’s multiple-annotation approach. Instead of focusing on a particular type of construction (discourse relations) and annotating all the meanings it can convey, we start from a particular meaning (causality), find all constructions that express it, and annotate each instance in the text with all the meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to b"
W17-0812,burchardt-etal-2006-salsa,0,0.0976747,"Missing"
W17-0812,W15-1622,1,0.948538,"ausality and Overlapping Relations Lori Levin and Jaime Carbonell Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {lsl,jgc}@cs.cmu.edu Jesse Dunietz Computer Science Department Carnegie Mellon University Pittsburgh, PA 15213, USA jdunietz@cs.cmu.edu Abstract press), the corpus contains annotations for causal language. It also includes annotations for seven commonly co-present meanings when they are expressed using constructions shared with causality. To deal with the wide variation in linguistic expressions of causation (see Neeleman and Van de Koot, 2012; Dunietz et al., 2015), BECauSE draws on the principles of Construction Grammar (CxG; Fillmore et al., 1988; Goldberg, 1995). CxG posits that the fundamental units of language are constructions – pairings of meanings with arbitrarily simple or complex linguistic forms, from morphemes to structured lexico-syntactic patterns. Accordingly, BECauSE admits arbitrary constructions as the bearers of causal relationships. As long as there is at least one fixed word, any conventionalized expression of causation can be annotated. By focusing on causal language – conventionalized expressions of causation – rather than real-wo"
W17-0812,H94-1020,0,0.13477,"example, so offensive that I left would be annotated as both causal (M OTIVATION) and E XTREMITY /S UFFICIENCY. When causality is not present in a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is n"
W17-0812,C14-1198,0,0.0630157,"meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. No"
W17-0812,W16-1007,0,0.034078,". BECauSE 2.0 can be thought of as an adaptation of PDTB’s multiple-annotation approach. Instead of focusing on a particular type of construction (discourse relations) and annotating all the meanings it can convey, we start from a particular meaning (causality), find all constructions that express it, and annotate each instance in the text with all the meanings it expresses. Other projects have attempted to address causality more narrowly. For example, a small corpus of event pairs conjoined with and has been tagged as causal or not causal (Bethard et al., 2008). The CaTeRS annotation scheme (Mostafazadeh et al., 2016), based on TimeML, also includes causal relations, but from a commonsense reasoning standpoint rather than a linguistic one. Similarly, Richer Event Description (O’Gorman et al., 2016) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less pre"
W17-0812,W16-5706,0,0.0992062,"Missing"
W17-0812,grivaz-2010-human,0,0.091269,"Times corpus (Sandhaus, 2008) All relation types present in the instance are marked. For example, so offensive that I left would be annotated as both causal (M OTIVATION) and E XTREMITY /S UFFICIENCY. When causality is not present in a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reaso"
W17-0812,J05-1004,0,0.104621,"6) integrates real-world temporal and causal relations between events into a unified framework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. None of these, however, unifies all linguistic realizations of causation into one framework; they are concerned with specific classes of words, rather than the semantics of causality. FrameNet (Ruppenhofer et al., 2016) is closer in spirit to BECauSE, in that it starts from meanings 96 of causing, preventing, and enabling. 2.1 cause of her illness was dehydration. But this is an unparsimonious account of the causal construction: the copula and pre"
W17-0812,P10-2013,0,0.0275138,"f these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is not causal. 5.2 Inter-Annotator Agreement Inter-annotator agreement was calculated between the two primary annotators on a sample of 260 • The temporal order test: Is the cause asserted to precede the effect? If not, it is not causal. 3 Publicly available, along with the constructicon, at https://github.com/duncanka/BECauSE. 4 We excluded WSJ d"
W17-0812,prasad-etal-2008-penn,0,0.283111,"Missing"
W17-0812,P03-1054,0,0.0281262,"Missing"
W17-0812,W16-1712,0,0.0525976,"Missing"
W17-0812,W15-1612,0,0.0258048,"ramework. A broadercoverage linguistic approach was taken by Mirza and Tonelli (2014), who enriched TimeML to include causal links and their lexical triggers. Their work differs from ours in that it requires arguments to be TimeML events; it requires causal connectives to be contiguous; and its guidelines define causality less precisely, relying on intuitive notions Related Work Several annotation schemes have addressed elements of causal language. Verb resources such as VerbNet (Schuler, 2005) and PropBank (Palmer et al., 2005) include verbs of causation. Likewise, preposition schemes (e.g., Schneider et al., 2015, 2016) include some purpose- and explanationrelated senses. None of these, however, unifies all linguistic realizations of causation into one framework; they are concerned with specific classes of words, rather than the semantics of causality. FrameNet (Ruppenhofer et al., 2016) is closer in spirit to BECauSE, in that it starts from meanings 96 of causing, preventing, and enabling. 2.1 cause of her illness was dehydration. But this is an unparsimonious account of the causal construction: the copula and preposition do not contribute to the causal meaning, and other language could be used to ti"
W17-0812,W14-2505,0,0.0203412,"a use of a sometimes-causal construction, the instance is annotated as N ON - CAUSAL, and the overlapping relations present are marked. It can be difficult to determine when language that expresses one of these relationships was also intended to convey a causal relationship. Annotators used a variety of questions to assess an ambiguous instance, largely based on Grivaz (2010): • 47 documents randomly selected4 from sections 2-23 of the Penn Treebank (Marcus et al., 1994) • 679 sentences5 transcribed from Congress’ Dodd-Frank hearings, taken from the NLP Unshared Task in PoliInformatics 2014 (Smith et al., 2014) • 10 newspaper documents (Wall Street Journal and New York Times articles, totalling 547 sentences) and 2 journal documents (82 sentences) from the Manually Annotated SubCorpus (MASC; Ide et al., 2010) The first three sets of documents are the same dataset that was annotated for BECauSE 1.0. • The “why” test: After reading the sentence, could a reader reasonably be expected to answer a “why” question about the potential effect argument? If not, it is not causal. 5.2 Inter-Annotator Agreement Inter-annotator agreement was calculated between the two primary annotators on a sample of 260 • The t"
W17-0812,E12-2021,0,0.184861,"Missing"
W17-0812,L16-1603,0,0.0457577,"r Computational Linguistics tative Hall visit next week. and catalogs/annotates a wide variety of lexical items that can express those meanings. Our work differs in several ways. First, FrameNet represents causal relationships through a variety of unrelated frames (e.g., C AUSATION and T HWARTING) and frame roles (e.g., P URPOSE and E XPLANATION). As with other schemes, this makes it difficult to treat causality in a uniform way. (The ASFALDA French FrameNet project recently proposed a reorganized frame hierarchy for causality, along with more complete coverage of French causal lexical units [Vieu et al., 2016]. Merging their framework into mainline FrameNet would mitigate this issue.) Second, FrameNet does not allow a lexical unit to evoke more than one frame at a time (although SALSA [Burchardt et al., 2006], the German FrameNet, does allow this). Each sentence conveys a causal relation, but piggybacks it on a related relation type. (1) uses a temporal relationship to suggest causality. (3) employs a correlative construction, and (2) contains elements of both time and correlation in addition to causation. (4), meanwhile, is framed as bringing something into existence, and (5) suggests both permis"
W17-2911,P13-1025,0,0.0790416,"Missing"
W17-2911,W14-3629,0,0.0202044,"choices have in other contexts? These questions motivate our study. Terms and definitions for code-switching and code-mixing across studies vary considerably (Gardner-Chloros, 2009). Since we are interested in all deviation from the likely norm of Arabic, we accept any instances of switching between languages in a conversation as code-switching. We also include “script-switching”, since we assume most editors can use Arabic characters and there may be social significance attached to writing Arabic in Latin script (something called Arabizi), especially since such language is usually dialectal (Darwish, 2014). Table 1 presents a few motivating examples of language variety in Arabic Wikipedia talk pages. Most CS we see is Arabic-English, but there are examples of French and Arabizi, the romanized Arabic seen in the third example in Table 1. We also note apologies for using English, including a longer exchange on the Israel talk page where an editor is confronted about language choice and replies in Arabizi: 4 Data and Task To capture the social effect of code-switching, we choose a task predicting social influence from CS features in discussion. In the context of the Arabic Wikipedia, we measure so"
W17-2911,W14-3911,0,0.0238548,"maxims, but instead are using them to understand meaning in interaction and to more fully explain natural language data. We assume a community norm of Arabic on the Arabic Wikipedia and expect CS to be marked and have some sort of social effect. However, MyersScotton (1998) allows the possibility of contexts where CS is itself unmarked; this would also be 3 Code-Switching on Arabic Wikipedia Talk Pages Though many language Wikipedias contain codeswitching on their talk pages, we select the Arabic Wikipedia for the variation we observe and previous Arabic CS work in NLP (Solorio et al., 2014; Elfardy et al., 2014). 74 Talk page GNU/Linux Oran, Algeria Said Aouita Lebanon Text English translation threaded fs. Salam, Les missions principales du centre sont: la recherche... hafid hassan ana fakhour dh TfO b3outa Sorry for talking english I notice you use the image... threaded fs. Greetings, the main missions of the center are: research... Target page [name] I am proud to... CAm F®F and it has a multi- ... a string used, and it has a multiSorry for talking english I notice you use the image... Table 1: Observations of non-Arabic text in Arabic Wikipedia talk pages Arabic is leveled as grounds for n"
W17-2911,P11-4017,0,0.0746933,"Missing"
W17-2911,P16-1021,1,0.833207,"ttings (Safi-Stagni, 1991). Codeswitching could demonstrate a level of expertise or world knowledge and have a positive effect on the acceptance of an editor’s contributions. 73 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 73–82, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics possible in our case. Recent computational analyses of style, metaphor, framing and politeness have investigated how language is used to achieve social goals in online communities (Danescu-Niculescu-Mizil et al., 2012, 2013; Jang et al., 2016; Tsur et al., 2015). We examine CS in a similar fashion. Interactional, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on"
W17-2911,L16-1260,0,0.0592635,", 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum et al. (2016) present an annotation scheme for the pragmatic functions of Hindi-English code-switched tweets, which includes reinforcement, sarcasm, reported speech, and changes from narration to evaluation. Rudra et al. (2016) study language preference for the expression of sentiment among Hindi-English multilinguals, finding that speakers more commonly use Hindi to express negative sentiment and English for positive sentiment on Twitter. To determine which of these hypotheses is a more likely explanation for CS in this context, we construct a publicly released dataset that pairs discussion between Wikipe"
W17-2911,P12-3005,0,0.0285116,"editor’s concatenated text in the entire thread (all their posts), along with the combination of all other editors’ text as separate features. 4.2 Editor Success Scores Language Identification Pn ||ci || s(u, t) = 1 − Pni=1 We find a diversity of language on Arabic Wikipedia talk pages not written in the Arabic script, including English, French, Hebrew, Turkish, Chinese and even a few words written in the Tifinagh and Syriac scripts. To initially survey the distribution of languages, we run all spans of tokens without Arabic characters (and that are not wholly punctuation) through langid.py (Lui and Baldwin, 2012), a language identification tool that can detect 97 languages. It is trained in a supervised fashion with Naive Bayes on byte n-grams, using cross-domain training data. langid.py finds 66 languages present within the dataset, but a qualitative analysis finds that named entities and noise in the dataset (special characters, usernames that passed through our prei=1 ||ei || Each editor’s score is the proportion of tokens they changed that remain changed, so s ∈ [0, 1]. In a qualitative evaluation, this editor score formulation was found to accurately reflect an editor’s impact on the revision of"
W17-2911,W14-3907,0,0.136327,"onal, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switchi"
W17-2911,W16-5805,0,0.0312643,"features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum e"
W17-2911,D08-1102,0,0.0515102,"fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS language identification, encouraged by shared tasks (Solorio et al., 2014; Molina et al., 2016). Others have worked to predict code-switch points from preceding text. Solorio and Liu (2008) predict codeswitch points with features including the previous n-grams’ identified language, POS tag, and location in constituent parses in both languages. Piergallini et al. (2016) tackle the same task in combination with language identification on a SwahiliEnglish online forum dataset. They note the possibility of using discourse structure and social variables for predicting code-switch points. Interest in computational models of the social and pragmatic nature of code-switching is growing. Begum et al. (2016) present an annotation scheme for the pragmatic functions of Hindi-English code-sw"
W17-2911,P15-1157,0,0.0149864,", 1991). Codeswitching could demonstrate a level of expertise or world knowledge and have a positive effect on the acceptance of an editor’s contributions. 73 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 73–82, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics possible in our case. Recent computational analyses of style, metaphor, framing and politeness have investigated how language is used to achieve social goals in online communities (Danescu-Niculescu-Mizil et al., 2012, 2013; Jang et al., 2016; Tsur et al., 2015). We examine CS in a similar fashion. Interactional, discourse-level features are contextspecific, and the relationship between social and linguistic features is fluid and often difficult to computationalize (Nguyen et al., 2016). Codeswitching may not carry clear social meaning at all in a given context (Auer, 2013), much less a predictable signal. Our work enters this conversation by exploring the effect of code-switching on social influence in an online community. The NLP community has largely studied codeswitching apart from its social context. Much work has focused on word-level CS langua"
W17-2911,D16-1121,1,0.879023,"Missing"
W91-0202,J89-3002,0,0.0259686,"Missing"
W91-0202,W91-0221,0,0.121105,"Missing"
W91-0202,P90-1032,0,0.0478417,"Missing"
W97-0410,C96-1075,1,0.852035,"ented. Introduction Spoken language understanding systems have been reasonably successful in limited semantic domains I. The limited domains naturally constrain vocabulary and perplexity, making speech recognition tractable. In addition, the relatively small range of meanings that could be conveyed make parsing and understanding tractable. Now, with the increasing success of large vocabulary continuous speech recognition (LVCSR), the challenge is to similarly scale up spoken language understanding. In this paper we describe our plans for extending the JANUS speech-to-speech translation system [1] [2] from the Appointment Scheduling domain to a broader domain, Travel Planning, which has a rich sub-domain structure, covering many topics. In the last three years, the JANUS project has been developing a speech-to-speech translation system for the Appointment Scheduling domain (two people setting up a time to meet with each other). Although the data we have been working with is spontaneous speech, the scheduling scenario naturally limits the vocabulary to about 3000 words in English and about 4000 words in Spanish and German, which have more inflection. Similarly, the types of dialogues ar"
W97-0410,C96-1061,0,0.0193217,"guage input string is first analyzed by a parser, which produces a languageindependent interlingua content representation. The interlingua is then passed to a generation component, which produces an output string in the target language. In an attempt to achieve both robustness and translation accuracy when faced with speech disfluencies and recognition errors, we use two different parsing strategies: a GLFt parser designed to be more accurate, and a Phoenix parser designed to be more robust. Detailed descriptions of the system components appear in our previous publications [1] [2] [3] [4] [5] [6]. 68 s c h e d u l e - m e e t i n g in addition to syntactic categories such as NP and VP. There were several reasons for chosing semantic grammars. First, the domain lends itself well to semantic g r a m m a r s because there are many fixed expressions and c o m m o n expressions that are almost formulaic. Breaking these down syntactically would be an unnecessary complication. Additionally, spontaneous spoken language is often syntactically ill formed, yet semantically coherent. Semantic g r a m m a r s allow our robust parsers to extract the key concepts being conveyed, even when the input"
W99-0306,J97-1002,0,0.203131,"Missing"
W99-0306,woszczcyna-etal-1998-modular,1,0.730827,"Missing"
woszczcyna-etal-1998-modular,H90-1027,0,\N,Missing
woszczcyna-etal-1998-modular,H94-1093,0,\N,Missing
woszczcyna-etal-1998-modular,P98-2229,0,\N,Missing
woszczcyna-etal-1998-modular,C98-2224,0,\N,Missing
woszczcyna-etal-1998-modular,W97-0410,1,\N,Missing
woszczcyna-etal-1998-modular,P80-1024,0,\N,Missing
