2021.wat-1.25,{IIIT} Hyderabad Submission To {WAT} 2021: Efficient Multilingual {NMT} systems for {I}ndian languages,2021,-1,-1,3,1,377,sourav kumar,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper describes the work and the systems submitted by the IIIT-Hyderbad team in the WAT 2021 MultiIndicMT shared task. The task covers 10 major languages of the Indian subcontinent. For the scope of this task, we have built multilingual systems for 20 translation directions namely English-Indic (one-to- many) and Indic-English (many-to-one). Individually, Indian languages are resource poor which hampers translation quality but by leveraging multilingualism and abundant monolingual corpora, the translation quality can be substantially boosted. But the multilingual systems are highly complex in terms of time as well as computational resources. Therefore, we are training our systems by efficiently se- lecting data that will actually contribute to most of the learning process. Furthermore, we are also exploiting the language related- ness found in between Indian languages. All the comparisons were made using BLEU score and we found that our final multilingual sys- tem significantly outperforms the baselines by an average of 11.3 and 19.6 BLEU points for English-Indic (en-xx) and Indic-English (xx- en) directions, respectively."
2021.sigmorphon-1.7,Sample-efficient Linguistic Generalizations through Program Synthesis: Experiments with Phonology Problems,2021,-1,-1,4,0,1302,saujas vaduguru,"Proceedings of the 18th SIGMORPHON Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Neural models excel at extracting statistical patterns from large amounts of data, but struggle to learn patterns or reason about language from only a few examples. In this paper, we ask: Can we learn explicit rules that generalize well from only a few examples? We explore this question using program synthesis. We develop a synthesis model to learn phonology rules as programs in a domain-specific language. We test the ability of our models to generalize from few training examples using our new dataset of problems from the Linguistics Olympiad, a challenging set of tasks that require strong linguistic reasoning ability. In addition to being highly sample-efficient, our approach generates human-readable programs, and allows control over the generalizability of the learnt programs."
2021.mtsummit-loresmt.16,{E}nglish-{M}arathi Neural Machine Translation for {L}o{R}es{MT} 2021,2021,-1,-1,2,1,5095,vandan mujadia,Proceedings of the 4th Workshop on Technologies for MT of Low Resource Languages (LoResMT2021),0,"In this paper, we (team - oneNLP-IIITH) describe our Neural Machine Translation approaches for English-Marathi (both direction) for LoResMT-20211 . We experimented with transformer based Neural Machine Translation and explored the use of different linguistic features like POS and Morph on subword unit for both English-Marathi and Marathi-English. In addition, we have also explored forward and backward translation using web-crawled monolingual data. We obtained 22.2 (overall 2 nd) and 31.3 (overall 1 st) BLEU scores for English-Marathi and Marathi-English on respectively"
2021.disrpt-1.2,A Transformer Based Approach towards Identification of Discourse Unit Segments and Connectives,2021,-1,-1,2,0,11201,sahil bakshi,Proceedings of the 2nd Shared Task on Discourse Relation Parsing and Treebanking (DISRPT 2021),0,"Discourse parsing, which involves understanding the structure, information flow, and modeling the coherence of a given text, is an important task in natural language processing. It forms the basis of several natural language processing tasks such as question-answering, text summarization, and sentiment analysis. Discourse unit segmentation is one of the fundamental tasks in discourse parsing and refers to identifying the elementary units of text that combine to form a coherent text. In this paper, we present a transformer based approach towards the automated identification of discourse unit segments and connectives. Early approaches towards segmentation relied on rule-based systems using POS tags and other syntactic information to identify discourse segments. Recently, transformer based neural systems have shown promising results in this domain. Our system, SegFormers, employs this transformer based approach to perform multilingual discourse segmentation and connective identification across 16 datasets encompassing 11 languages and 3 different annotation frameworks. We evaluate the system based on F1 scores for both tasks, with the best system reporting the highest F1 score of 97.02{\%} for the treebanked English RST-DT dataset."
2021.acl-srw.12,How do different factors Impact the Inter-language Similarity? A Case Study on {I}ndian languages,2021,-1,-1,3,1,377,sourav kumar,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"India is one of the most linguistically diverse nations of the world and is culturally very rich. Most of these languages are somewhat similar to each other on account of sharing a common ancestry or being in contact for a long period of time. Nowadays, researchers are constantly putting efforts in utilizing the language relatedness to improve the performance of various NLP systems such as cross lingual semantic search, machine translation, sentiment analysis systems, etc. So in this paper, we performed an extensive case study on similarity involving languages of the Indian subcontinent. Language similarity prediction is defined as the task of measuring how similar the two languages are on the basis of their lexical, morphological and syntactic features. In this study, we concentrate only on the approach to calculate lexical similarity between Indian languages by looking at various factors such as size and type of corpus, similarity algorithms, subword segmentation, etc. The main takeaways from our work are: (i) Relative order of the language similarities largely remain the same, regardless of the factors mentioned above, (ii) Similarity within the same language family is higher, (iii) Languages share more lexical features at the subword level."
2020.wmt-1.48,{NMT} based Similar Language Translation for {H}indi - {M}arathi,2020,-1,-1,2,1,5095,vandan mujadia,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the participation of team F1toF6 (LTRC, IIIT-Hyderabad) for the WMT 2020 task, similar language translation. We experimented with attention based recurrent neural network architecture (seq2seq) for this task. We explored the use of different linguistic features like POS and Morph along with back translation for Hindi-Marathi and Marathi-Hindi machine translation."
2020.winlp-1.41,Enhanced {U}rdu Word Segmentation using Conditional Random Fields and Morphological Context Features,2020,-1,-1,3,0,14032,aamir farhan,Proceedings of the The Fourth Widening Natural Language Processing Workshop,0,"Word segmentation is a fundamental task for most of the NLP applications. Urdu adopts Nastalique writing style which does not have a concept of space. Furthermore, the inherent non-joining attributes of certain characters in Urdu create spaces within a word while writing in digital format. Thus, Urdu not only has space omission but also space insertion issues which make the word segmentation task challenging. In this paper, we improve upon the results of Zia, Raza and Athar (2018) by using a manually annotated corpus of 19,651 sentences along with morphological context features. Using the Conditional Random Field sequence modeler, our model achieves F 1 score of 0.98 for word boundary identification and 0.92 for sub-word boundary identification tasks. The results demonstrated in this paper outperform the state-of-the-art methods."
2020.wildre-1.8,A Fully Expanded Dependency Treebank for {T}elugu,2020,-1,-1,3,0,14046,sneha nallani,Proceedings of the WILDRE5{--} 5th Workshop on Indian Language Data: Resources and Evaluation,0,"Treebanks are an essential resource for syntactic parsing. The available Paninian dependency treebank(s) for Telugu is annotated only with inter-chunk dependency relations and not all words of a sentence are part of the parse tree. In this paper, we automatically annotate the intra-chunk dependencies in the treebank using a Shift-Reduce parser based on Context Free Grammar rules for Telugu chunks. We also propose a few additional intra-chunk dependency relations for Telugu apart from the ones used in Hindi treebank. Annotating intra-chunk dependencies finally provides a complete parse tree for every sentence in the treebank. Having a fully expanded treebank is crucial for developing end to end parsers which produce complete trees. We present a fully expanded dependency treebank for Telugu consisting of 3220 sentences. In this paper, we also convert the treebank annotated with Anncorra part-of-speech tagset to the latest BIS tagset. The BIS tagset is a hierarchical tagset adopted as a unified part-of-speech standard across all Indian Languages. The final treebank is made publicly available."
2020.lrec-1.456,Linguistically Informed {H}indi-{E}nglish Neural Machine Translation,2020,-1,-1,3,1,13798,vikrant goyal,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Hindi-English Machine Translation is a challenging problem, owing to multiple factors including the morphological complexity and relatively free word order of Hindi, in addition to the lack of sufficient parallel training data. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. To overcome the data sparsity issue caused by the lack of large parallel corpora for Hindi-English, we propose a method to employ additional linguistic knowledge which is encoded by different phenomena depicted by Hindi. We generalize the embedding layer of the state-of-the-art Transformer model to incorporate linguistic features like POS tag, lemma and morph features to improve the translation performance. We compare the results obtained on incorporating this knowledge with the baseline systems and demonstrate significant performance improvements. Although, the Transformer NMT models have a strong efficacy to learn language constructs, we show that the usage of specific features further help in improving the translation performance."
2020.icon-termtraction.1,Graph Based Automatic Domain Term Extraction,2020,-1,-1,2,0,19073,hema ala,Proceedings of the 17th International Conference on Natural Language Processing (ICON): TermTraction 2020 Shared Task,0,"We present a Graph Based Approach to automatically extract domain specific terms from technical domains like Biochemistry, Communication, Computer Science and Law. Our approach is similar to TextRank with an extra post-processing step to reduce the noise. We performed our experiments on the mentioned domains provided by ICON TermTraction - 2020 shared task. Presented precision, recall and f1-score for all experiments. Further, it is observed that our method gives promising results without much noise in domain terms."
2020.icon-termtraction.3,N-Grams {T}ext{R}ank A Novel Domain Keyword Extraction Technique,2020,-1,-1,4,0,19074,saransh rajput,Proceedings of the 17th International Conference on Natural Language Processing (ICON): TermTraction 2020 Shared Task,0,"The rapid growth of the internet has given us a wealth of information and data spread across the web. However, as the data begins to grow we simultaneously face the grave problem of an \textit{Information Explosion}. An abundance of data can lead to large scale data management problems as well as the loss of the true meaning of the data. In this paper, we present an advanced domain specific keyword extraction algorithm in order to tackle this problem of paramount importance. Our algorithm is based on a modified version of TextRank algorithm - an algorithm based on PageRank to successfully determine the keywords from a domain specific document. Furthermore, this paper proposes a modification to the traditional TextRank algorithm that takes into account bigrams and trigrams and returns results with an extremely high precision. We observe how the precision and f1-score of this model outperforms other models in many domains and the recall can be easily increased by increasing the number of results without affecting the precision. We also discuss about the future work of extending the same algorithm to Indian languages."
2020.icon-techdofication.6,Automatic Technical Domain Identification,2020,-1,-1,2,0,19073,hema ala,Proceedings of the 17th International Conference on Natural Language Processing (ICON): TechDOfication 2020 Shared Task,0,"In this paper we present two Machine Learning algorithms namely Stochastic Gradient Descent and Multi Layer Perceptron to Identify the technical domain of given text as such text provides information about the specific domain. We performed our experiments on Coarse-grained technical domains like Computer Science, Physics, Law, etc for English, Bengali, Gujarati, Hindi, Malayalam, Marathi, Tamil, and Telugu languages, and on fine-grained sub domains for Computer Science like Operating System, Computer Network, Database etc for only English language. Using TFIDF as a feature extraction method we show how both the machine learning models perform on the mentioned languages."
2020.icon-techdofication.7,Fine-grained domain classification using Transformers,2020,-1,-1,3,0,19075,akshat gahoi,Proceedings of the 17th International Conference on Natural Language Processing (ICON): TechDOfication 2020 Shared Task,0,"The introduction of transformers in 2017 and successively BERT in 2018 brought about a revolution in the field of natural language processing. Such models are pretrained on vast amounts of data, and are easily extensible to be used for a wide variety of tasks through transfer learning. Continual work on transformer based architectures has led to a variety of new models with state of the art results. RoBERTa(CITATION) is one such model, which brings about a series of changes to the BERT architecture and is capable of producing better quality embeddings at an expense of functionality. In this paper, we attempt to solve the well known text classification task of fine-grained domain classification using BERT and RoBERTa and perform a comparative analysis of the same. We also attempt to evaluate the impact of data preprocessing specially in the context of fine-grained domain classification. The results obtained outperformed all the other models at the ICON TechDOfication 2020 (subtask-2a) Fine-grained domain classification task and ranked first. This proves the effectiveness of our approach."
2020.icon-main.54,Polarization and its Life on Social Media: A Case Study on Sabarimala and Demonetisation,2020,-1,-1,2,0,19185,ashutosh ranjan,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"This paper is an attempt to study polarisation on social media data. We focus on two hugely controversial and talked about events in the Indian diaspora, namely 1) the Sabarimala Temple (located in Kerala, India) incident which became a nationwide controversy when two women under the age of 50 secretly entered the temple breaking a long standing temple rule that disallowed women of menstruating age (10-50) to enter the temple and 2) the Indian government{'}s move to demonetise all existing 500 and 1000 denomination banknotes, comprising of 86{\%} of the currency in circulation, in November 2016. We gather tweets around these two events in various time periods, preprocess and annotate them with their sentiment polarity and emotional category, and analyse trends to help us understand changing polarity over time around controversial events. The tweets collected are in English, Hindi and code-mixed Hindi-English. Apart from the analysis on the annotated data, we also present the twitter data comprising a total of around 1.5 million tweets."
2020.icon-adapmt.2,{A}dap{NMT} : Neural Machine Translation with Technical Domain Adaptation for Indic Languages,2020,-1,-1,2,0,19073,hema ala,Proceedings of the 17th International Conference on Natural Language Processing (ICON): Adap-MT 2020 Shared Task,0,"Adapting new domain is highly challenging task for Neural Machine Translation (NMT). In this paper we show the capability of general domain machine translation when translating into Indic languages (English - Hindi , English - Telugu and Hindi - Telugu), and low resource domain adaptation of MT systems using existing general parallel data and small in domain parallel data for AI and Chemistry Domains. We carried out our experiments using Byte Pair Encoding(BPE) as it solves rare word problems. It has been observed that with addition of little amount of in-domain data to the general data improves the BLEU score significantly."
2020.acl-srw.19,A Simple and Effective Dependency Parser for {T}elugu,2020,-1,-1,3,0,14046,sneha nallani,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"We present a simple and effective dependency parser for Telugu, a morphologically rich, free word order language. We propose to replace the rich linguistic feature templates used in the past approaches with a minimal feature function using contextual vector representations. We train a BERT model on the Telugu Wikipedia data and use vector representations from this model to train the parser. Each sentence token is associated with a vector representing the token in the context of that sentence and the feature vectors are constructed by concatenating two token representations from the stack and one from the buffer. We put the feature representations through a feedforward network and train with a greedy transition based approach. The resulting parser has a very simple architecture with minimal feature engineering and achieves state-of-the-art results for Telugu."
2020.acl-srw.22,Efficient Neural Machine Translation for Low-Resource Languages via Exploiting Related Languages,2020,-1,-1,3,1,13798,vikrant goyal,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"A large percentage of the world{'}s population speaks a language of the Indian subcontinent, comprising languages from both Indo-Aryan (e.g. Hindi, Punjabi, Gujarati, etc.) and Dravidian (e.g. Tamil, Telugu, Malayalam, etc.) families. A universal characteristic of Indian languages is their complex morphology, which, when combined with the general lack of sufficient quantities of high-quality parallel data, can make developing machine translation (MT) systems for these languages difficult. Neural Machine Translation (NMT) is a rapidly advancing MT paradigm and has shown promising results for many language pairs, especially in large training data scenarios. Since the condition of large parallel corpora is not met for Indian-English language pairs, we present our efforts towards building efficient NMT systems between Indian languages (specifically Indo-Aryan languages) and English via efficiently exploiting parallel data from the related languages. We propose a technique called Unified Transliteration and Subword Segmentation to leverage language similarity while exploiting parallel data from related language pairs. We also propose a Multilingual Transfer Learning technique to leverage parallel data from multiple related languages to assist translation for low resource language pair of interest. Our experiments demonstrate an overall average improvement of 5 BLEU points over the standard Transformer-based NMT baselines."
2020.acl-srw.38,Checkpoint Reranking: An Approach to Select Better Hypothesis for Neural Machine Translation Systems,2020,-1,-1,2,0,22508,vinay pandramish,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"In this paper, we propose a method of re-ranking the outputs of Neural Machine Translation (NMT) systems. After the decoding process, we select a few last iteration outputs in the training process as the $N$-best list. After training a Neural Machine Translation (NMT) baseline system, it has been observed that these iteration outputs have an oracle score higher than baseline up to 1.01 BLEU points compared to the last iteration of the trained system.We come up with a ranking mechanism by solely focusing on the decoder{'}s ability to generate distinct tokens and without the usage of any language model or data. With this method, we achieved a translation improvement up to +0.16 BLEU points over baseline.We also evaluate our approach by applying the coverage penalty to the training process.In cases of moderate coverage penalty, the oracle scores are higher than the final iteration up to +0.99 BLEU points, and our algorithm gives an improvement up to +0.17 BLEU points.With excessive penalty, there is a decrease in translation quality compared to the baseline system. Still, an increase in oracle scores up to +1.30 is observed with the re-ranking algorithm giving an improvement up to +0.15 BLEU points is found in case of excessive penalty.The proposed re-ranking method is a generic one and can be extended to other language pairs as well."
W19-7810,Dependency Parser for {B}engali-{E}nglish Code-Mixed Data enhanced with a Synthetic Treebank,2019,0,0,2,0,23437,urmi ghosh,"Proceedings of the 18th International Workshop on Treebanks and Linguistic Theories (TLT, SyntaxFest 2019)",0,None
W19-7724,{P}{\\=a}á¹inian Syntactico-Semantic Relation Labels,2019,0,0,2,0,14360,amba kulkarni,"Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019)",0,None
W19-5316,The {IIIT}-{H} {G}ujarati-{E}nglish Machine Translation System for {WMT}19,2019,0,0,2,1,13798,vikrant goyal,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper describes the Neural Machine Translation system of IIIT-Hyderabad for the GujaratiâEnglish news translation shared task of WMT19. Our system is basedon encoder-decoder framework with attention mechanism. We experimented with Multilingual Neural MT models. Our experiments show that Multilingual Neural Machine Translation leveraging parallel data from related language pairs helps in significant BLEU improvements upto 11.5, for low resource language pairs like Gujarati-English"
W19-4020,A Dataset for Semantic Role Labelling of {H}indi-{E}nglish Code-Mixed Tweets,2019,0,0,2,0,24313,riya pal,Proceedings of the 13th Linguistic Annotation Workshop,0,"We present a data set of 1460 Hindi-English code-mixed tweets consisting of 20,949 tokens labelled with Proposition Bank labels marking their semantic roles. We created verb frames for complex predicates present in the corpus and formulated mappings from Paninian dependency labels to Proposition Bank labels. With the help of these mappings and the dependency tree, we propose a baseline rule based system for Semantic Role Labelling of Hindi-English code-mixed data. We obtain an accuracy of 96.74{\%} for Argument Identification and are able to further classify 73.93{\%} of the labels correctly. While there is relevant ongoing research on Semantic Role Labelling and on building tools for code-mixed social media data, this is the first attempt at labelling semantic roles in code-mixed data, to the best of our knowledge."
D19-5538,Towards Automated Semantic Role Labelling of {H}indi-{E}nglish Code-Mixed Tweets,2019,0,0,2,0,24313,riya pal,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"We present a system for automating Semantic Role Labelling of Hindi-English code-mixed tweets. We explore the issues posed by noisy, user generated code-mixed social media data. We also compare the individual effect of various linguistic features used in our system. Our proposed model is a 2-step system for automated labelling which gives an overall accuracy of 84{\%} for Argument Classification, marking a 10{\%} increase over the existing rule-based baseline model. This is the first attempt at building a statistical Semantic Role Labeller for Hindi-English code-mixed data, to the best of our knowledge."
D19-5216,{LTRC}-{MT} Simple {\\&} Effective {H}indi-{E}nglish Neural Machine Translation Systems at {WAT} 2019,2019,0,0,2,1,13798,vikrant goyal,Proceedings of the 6th Workshop on Asian Translation,0,This paper describes the Neural Machine Translation systems of IIIT-Hyderabad (LTRC-MT) for WAT 2019 Hindi-English shared task. We experimented with both Recurrent Neural Networks {\&} Transformer architectures. We also show the results of our experiments of training NMT models using additional data via backtranslation.
2019.icon-1.15,Dataset for Aspect Detection on Mobile reviews in {H}indi,2019,-1,-1,3,1,17340,pruthwik mishra,Proceedings of the 16th International Conference on Natural Language Processing,0,"In recent years Opinion Mining has become one of the very interesting fields of Language Processing. To extract the gist of a sentence in a shorter and efficient manner is what opinion mining provides. In this paper we focus on detecting aspects for a particular domain. While relevant research work has been done in aspect detection in resource rich languages like English, we are trying to do the same in a relatively resource poor Hindi language. Here we present a corpus of mobile reviews which are labelled with carefully curated aspects. The motivation behind Aspect detection is to get information on a finer level about the data. In this paper we identify all aspects related to the gadget which are present on the reviews given online on various websites. We also propose baseline models to detect aspects in Hindi text after conducting various experiments."
2019.icon-1.18,Towards Handling Verb Phrase Ellipsis in {E}nglish-{H}indi Machine Translation,2019,-1,-1,2,0,11542,niyati bafna,Proceedings of the 16th International Conference on Natural Language Processing,0,"English-Hindi machine translation systems have difficulty interpreting verb phrase ellipsis (VPE) in English, and commit errors in translating sentences with VPE. We present a solution and theoretical backing for the treatment of English VPE, with the specific scope of enabling English-Hindi MT, based on an understanding of the syntactical phenomenon of verb-stranding verb phrase ellipsis in Hindi (VVPE). We implement a rule-based system to perform the following sub-tasks: 1) Verb ellipsis identification in the English source sentence, 2) Elided verb phrase head identification 3) Identification of verb segment which needs to be induced at the site of ellipsis 4) Modify input sentence; i.e. resolving VPE and inducing the required verb segment. This system obtains 94.83 percent precision and 83.04 percent recall on subtask (1), tested on 3900 sentences from the BNC corpus. This is competitive with state-of-the-art results. We measure accuracy of subtasks (2) and (3) together, and obtain a 91 percent accuracy on 200 sentences taken from the WSJ corpus. Finally, in order to indicate the relevance of ellipsis handling to MT, we carried out a manual analysis of the English-Hindi MT outputs of 100 sentences after passing it through our system. We set up a basic metric (1-5) for this evaluation, where 5 indicates drastic improvement, and obtained an average of 3.55. As far as we know, this is the first attempt to target ellipsis resolution in the context of improving English-Hindi machine translation."
2019.icon-1.22,Kunji : A Resource Management System for Higher Productivity in Computer Aided Translation Tools,2019,-1,-1,3,0,27395,priyank gupta,Proceedings of the 16th International Conference on Natural Language Processing,0,"Complex NLP applications, such as machine translation systems, utilize various kinds of resources namely lexical, multiword, domain dictionaries, maps and rules etc. Similarly, translators working on Computer Aided Translation workbenches, also require help from various kinds of resources - glossaries, terminologies, concordances and translation memory in the workbenches in order to increase their productivity. Additionally, translators have to look away from the workbenches for linguistic resources like Named Entities, Multiwords, lexical and lexeme dictionaries in order to get help, as the available resources like concordances, terminologies and glossaries are often not enough. In this paper we present Kunji, a resource management system for translation workbenches and MT modules. This system can be easily integrated in translation workbenches and can also be used as a management tool for resources for MT systems. The described resource management system has been integrated in a translation workbench Transzaar. We also study the impact of providing this resource management system along with linguistic resources on the productivity of translators for English-Hindi language pair. When the linguistic resources like lexeme, NER and MWE dictionaries were made available to translators in addition to their regular translation memories, concordances and terminologies, their productivity increased by 15.61{\%}."
Y18-1002,Automated Error Correction and Validation for {POS} Tagging of {H}indi,2018,0,1,3,0,27466,sachi angle,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1053,{E}qu{G}ener: A Reasoning Network for Word Problem Solving by Generating Arithmetic Equations,2018,0,0,3,1,17340,pruthwik mishra,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
N18-1090,{U}niversal {D}ependency Parsing for {H}indi-{E}nglish Code-Switching,2018,0,4,4,1,18044,irshad bhat,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Code-switching is a phenomenon of mixing grammatical structures of two or more languages under varied social constraints. The code-switching data differ so radically from the benchmark corpora used in NLP community that the application of standard technologies to these data degrades their performance sharply. Unlike standard corpora, these data often need to go through additional processes such as language identification, normalization and/or back-transliteration for their efficient processing. In this paper, we investigate these indispensable processes and other problems associated with syntactic parsing of code-switching data and propose methods to mitigate their effects. In particular, we study dependency parsing of code-switching data of Hindi and English multilingual speakers from Twitter. We present a treebank of Hindi-English code-switching tweets under Universal Dependencies scheme and propose a neural stacking model for parsing that efficiently leverages the part-of-speech tag and syntactic tree annotations in the code-switching treebank and the preexisting Hindi and English treebanks. We also present normalization and back-transliteration models with a decoding process tailored for code-switching data. Results show that our neural stacking parser is 1.5{\%} LAS points better than the augmented parsing model and 3.8{\%} LAS points better than the one which uses first-best normalization and/or back-transliteration."
L18-1048,No more beating about the bush : A Step towards Idiom Handling for {I}ndian Language {NLP},2018,0,0,4,1,27736,ruchit agrawal,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
K18-3013,{IIT}({BHU}){--}{IIITH} at {C}o{NLL}{--}{SIGMORPHON} 2018 Shared Task on Universal Morphological Reinflection,2018,0,4,3,0,20583,abhishek sharma,Proceedings of the {C}o{NLL}{--}{SIGMORPHON} 2018 Shared Task: Universal Morphological Reinflection,0,None
W17-7503,Three-phase training to address data sparsity in Neural Machine Translation,2017,0,0,3,1,27736,ruchit agrawal,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-7505,A vis-{\\`a}-vis evaluation of {MT} paradigms for linguistically distant languages,2017,0,0,3,1,27736,ruchit agrawal,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-7507,{POS} Tagging For Resource Poor Languages Through Feature Projection,2017,0,1,3,1,17340,pruthwik mishra,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-7561,Semisupervied Data Driven Word Sense Disambiguation for Resource-poor Languages,2017,0,0,3,0,31251,pratibha rani,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-6529,Unity in Diversity: A Unified Parsing Strategy for Major {I}ndian Languages,2017,0,1,2,0,31422,juhi tandon,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-6309,Leveraging Newswire Treebanks for Parsing Conversational Data with Argument Scrambling,2017,0,0,3,1,29432,riyaz bhat,Proceedings of the 15th International Conference on Parsing Technologies,0,"We investigate the problem of parsing conversational data of morphologically-rich languages such as Hindi where argument scrambling occurs frequently. We evaluate a state-of-the-art non-linear transition-based parsing system on a new dataset containing 506 dependency trees for sentences from Bollywood (Hindi) movie scripts and Twitter posts of Hindi monolingual speakers. We show that a dependency parser trained on a newswire treebank is strongly biased towards the canonical structures and degrades when applied to conversational data. Inspired by Transformational Generative Grammar (Chomsky, 1965), we mitigate the sampling bias by generating all theoretically possible alternative word orders of a clause from the existing (kernel) structures in the treebank. Training our parser on canonical and transformed structures improves performance on conversational data by around 9{\%} LAS over the baseline newswire parser."
I17-3017,Deep Neural Network based system for solving Arithmetic Word problems,2017,9,4,5,0,32851,purvanshi mehta,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"This paper presents DILTON a system which solves simple arithmetic word problems. DILTON uses a Deep Neural based model to solve math word problems. DILTON divides the question into two parts - worldstate and query. The worldstate and the query are processed separately in two different networks and finally, the networks are merged to predict the final operation. We report the first deep learning approach for the prediction of operation between two numbers. DILTON learns to predict operations with 88.81{\%} accuracy in a corpus of primary school questions."
E17-2052,Joining Hands: Exploiting Monolingual Treebanks for Parsing of Code-mixing Data,2017,0,1,4,1,18044,irshad bhat,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"In this paper, we propose efficient and less resource-intensive strategies for parsing of code-mixed data. These strategies are not constrained by in-domain annotations, rather they leverage pre-existing monolingual annotated resources for training. We show that these methods can produce significantly better results as compared to an informed baseline. Due to lack of an evaluation set for code-mixed structures, we also present a data set of 450 Hindi and English code-mixed tweets of Hindi multilingual speakers for evaluation."
W16-5201,{K}athaa : {NLP} Systems as Edge-Labeled Directed Acyclic {M}ulti{G}raphs,2016,9,0,4,0,33506,sharada mohanty,Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies ({WLSI}/{OIAF}4{HLT}2016),0,"We present Kathaa, an Open Source web-based Visual Programming Framework for Natural Language Processing (NLP) Systems. Kathaa supports the design, execution and analysis of complex NLP systems by visually connecting NLP components from an easily extensible Module Library. It models NLP systems an edge-labeled Directed Acyclic MultiGraph, and lets the user use publicly co-created modules in their own NLP applications irrespective of their technical proficiency in Natural Language Processing. Kathaa exposes an intuitive web based Interface for the users to interact with and modify complex NLP Systems; and a precise Module definition API to allow easy integration of new state of the art NLP components. Kathaa enables researchers to publish their services in a standardized format to enable the masses to use their services out of the box. The vision of this work is to pave the way for a system like Kathaa, to be the Lego blocks of NLP Research and Applications. As a practical use case we use Kathaa to visually implement the Sampark Hindi-Panjabi Machine Translation Pipeline and the Sampark Hindi-Urdu Machine Translation Pipeline, to demonstrate the fact that Kathaa can handle really complex NLP systems while still being intuitive for the end user."
W16-1716,Conversion from Paninian Karakas to {U}niversal {D}ependencies for {H}indi Dependency Treebank,2016,18,1,4,0,31422,juhi tandon,Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016),0,"Universal Dependencies (UD) are gaining much attention of late for systematic evaluation of cross-lingual techniques for crosslingual dependency parsing. In this paper we present our work in line with UD. Our contribution to this is manifold. We extend UD to Indian languages through conversion of Pxc4x81nxe1xbbx8bnian Dependencies to UD for the Hindi Dependency Treebank (HDTB). We discuss the differences in annotation in both the schemes, present parsing experiments for both the formalisms and empirically evaluate their weaknesses and strengths for Hindi. We produce an automatically converted Hindi Treebank conforming to the international standard UD scheme, making it useful as a resource for multilingual language technology."
P16-3006,Significance of an Accurate Sandhi-Splitter in Shallow Parsing of {D}ravidian Languages,2016,9,2,2,1,33706,devadath,Proceedings of the {ACL} 2016 Student Research Workshop,0,"This paper evaluates the challenges involved in shallow parsing of Dravidian languages which are highly agglutinative and morphologically rich. Text processing tasks in these languages are not trivial because multiple words concatenate to form a single string with morpho-phonemic changes at the point of concatenation. This phenomenon known as Sandhi, in turn complicates the individual word identification. Shallow parsing is the task of identification of correlated group of words given a raw sentence. The current work is an attempt to study the effect of Sandhi in building shallow parsers for Dravidian languages by evaluating its effect on Malayalam, one of the main languages from Dravidian family. We provide an in-depth analysis of effect ofSandhi in developing a robust shallow parser pipeline with experimental results emphasizing on how sensitive the individual components of shallow parser are, towards the accuracy of a sandhi splitter. Our work can serve as a guiding light for building robust text processing systems in Dravidian languages."
N16-3019,{K}athaa: A Visual Programming Framework for {NLP} Applications,2016,6,2,4,0,33506,sharada mohanty,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,None
N16-2010,Explicit Argument Identification for Discourse Parsing In {H}indi: A Hybrid Pipeline,2016,1,0,2,0,34663,rohit jain,Proceedings of the {NAACL} Student Research Workshop,0,"Shallow discourse parsing enables us to study discourse as a coherent piece of information rather than a sequence of clauses, sentences and paragraphs. In this paper, we identify arguments of explicit discourse relations in Hindi. This is the first such work carried out for Hindi. Building upon previous work carried out on discourse connective identification in Hindi, we propose a hybrid pipeline which makes use of both sub-tree extraction and linear tagging approaches. We report state-ofthe-art performance for this task."
N16-2014,Non-decreasing Sub-modular Function for Comprehensible Summarization,2016,23,1,5,0.952381,4798,litton kurisinkel,Proceedings of the {NAACL} Student Research Workshop,0,"Extractive summarization techniques typically aim to maximize the information coverage of the summary with respect to the original corpus and report accuracies in ROUGE scores. Automated text summarization techniques should consider the dimensions of comprehensibility, coherence and readability. In the current work, we identify the discourse structure which provides the context for the creation of a sentence. We leverage the information from the structure to frame a monotone (non-decreasing) sub-modular scoring function for generating comprehensible summaries. Our approach improves the overall quality of comprehensibility of the summary in terms of human evaluation and gives sufficient content coverage with comparable ROUGE score. We also formulate a metric to measure summary comprehensibility in terms of Contextual Independence of a sentence. The metric is shown to be representative of human judgement of text comprehensibility."
N16-1159,Shallow Parsing Pipeline - {H}indi-{E}nglish Code-Mixed Social Media Text,2016,19,14,7,0,34729,arnav sharma,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this study, the problem of shallow parsing of Hindi-English code-mixed social media text (CSMT) has been addressed. We have annotated the data, developed a language identifier, a normalizer, a part-of-speech tagger and a shallow parser. To the best of our knowledge, we are the first to attempt shallow parsing on CSMT. The pipeline developed has been made available to the research community with the goal of enabling better text analysis of Hindi English CSMT. The pipeline is accessible at 1 ."
L16-1025,Coreference Annotation Scheme and Relation Types for {H}indi,2016,11,0,3,1,5095,vandan mujadia,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper describes a coreference annotation scheme, coreference annotation specific issues and their solutions through our proposed annotation scheme for Hindi. We introduce different co-reference relation types between continuous mentions of the same coreference chain such as {``}Part-of{''}, {``}Function-value pair{''} etc. We used Jaccard similarity based Krippendorff{`}s{'} alpha to demonstrate consistency in annotation scheme, annotation and corpora. To ease the coreference annotation process, we built a semi-automatic Coreference Annotation Tool (CAT). We also provide statistics of coreference annotation on Hindi Dependency Treebank (HDTB)."
L16-1276,Using lexical and Dependency Features to Disambiguate Discourse Connectives in {H}indi,2016,0,0,3,0,34663,rohit jain,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Discourse parsing is a challenging task in NLP and plays a crucial role in discourse analysis. To enable discourse analysis for Hindi, Hindi Discourse Relations Bank was created on a subset of Hindi TreeBank. The benefits of a discourse analyzer in automated discourse analysis, question summarization and question answering domains has motivated us to begin work on a discourse analyzer for Hindi. In this paper, we focus on discourse connective identification for Hindi. We explore various available syntactic features for this task. We also explore the use of dependency tree parses present in the Hindi TreeBank and study the impact of the same on the performance of the system. We report that the novel dependency features introduced have a higher impact on precision, in comparison to the syntactic features previously used for this task. In addition, we report a high accuracy of 96{\%} for this task."
L16-1377,A {P}roposition {B}ank of {U}rdu,2016,16,0,3,0,35107,maaz anwar,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper describes our efforts for the development of a Proposition Bank for Urdu, an Indo-Aryan language. Our primary goal is the labeling of syntactic nodes in the existing Urdu dependency Treebank with specific argument labels. In essence, it involves annotation of predicate argument structures of both simple and complex predicates in the Treebank corpus. We describe the overall process of building the PropBank of Urdu. We discuss various statistics pertaining to the Urdu PropBank and the issues which the annotators encountered while developing the PropBank. We also discuss how these challenges were addressed to successfully expand the PropBank corpus. While reporting the Inter-annotator agreement between the two annotators, we show that the annotators share similar understanding of the annotation guidelines and of the linguistic phenomena present in the language. The present size of this Propbank is around 180,000 tokens which is double-propbanked by the two annotators for simple predicates. Another 100,000 tokens have been annotated for complex predicates of Urdu."
L16-1409,A Finite-State Morphological Analyser for {S}indhi,2016,0,4,3,0,34659,raveesh motlani,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Morphological analysis is a fundamental task in natural-language processing, which is used in other NLP applications such as part-of-speech tagging, syntactic parsing, information retrieval, machine translation, etc. In this paper, we present our work on the development of free/open-source finite-state morphological analyser for Sindhi. We have used Apertium{'}s lttoolbox as our finite-state toolkit to implement the transducer. The system is developed using a paradigm-based approach, wherein a paradigm defines all the word forms and their morphological features for a given stem (lemma). We have evaluated our system on the Sindhi Wikipedia corpus and achieved a reasonable coverage of 81{\%} and a precision of over 97{\%}."
L16-1727,Towards Building Semantic Role Labeler for {I}ndian Languages,2016,15,0,2,0,35107,maaz anwar,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present a statistical system for identifying the semantic relationships or semantic roles for two major Indian Languages, Hindi and Urdu. Given an input sentence and a predicate/verb, the system first identifies the arguments pertaining to that verb and then classifies it into one of the semantic labels which can either be a DOER, THEME, LOCATIVE, CAUSE, PURPOSE etc. The system is based on 2 statistical classifiers trained on roughly 130,000 words for Urdu and 100,000 words for Hindi that were hand-annotated with semantic roles under the PropBank project for these two languages. Our system achieves an accuracy of 86{\%} in identifying the arguments of a verb for Hindi and 75{\%} for Urdu. At the subsequent task of classifying the constituents into their semantic roles, the Hindi system achieved 58{\%} precision and 42{\%} recall whereas Urdu system performed better and achieved 83{\%} precision and 80{\%} recall. Our study also allowed us to compare the usefulness of different linguistic features and feature combinations in the semantic role labeling task. We also examine the use of statistical syntactic parsing as feature in the role labeling task."
C16-1039,A House United: Bridging the Script and Lexical Barrier between {H}indi and {U}rdu,2016,23,0,4,1,29432,riyaz bhat,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In Computational Linguistics, Hindi and Urdu are not viewed as a monolithic entity and have received separate attention with respect to their text processing. From part-of-speech tagging to machine translation, models are separately trained for both Hindi and Urdu despite the fact that they represent the same language. The reasons mainly are their divergent literary vocabularies and separate orthographies, and probably also their political status and the social perception that they are two separate languages. In this article, we propose a simple but efficient approach to bridge the lexical and orthographic differences between Hindi and Urdu texts. With respect to text processing, addressing the differences between the Hindi and Urdu texts would be beneficial in the following ways: (a) instead of training separate models, their individual resources can be augmented to train single, unified models for better generalization, and (b) their individual text processing applications can be used interchangeably under varied resource conditions. To remove the script barrier, we learn accurate statistical transliteration models which use sentence-level decoding to resolve word ambiguity. Similarly, we learn cross-register word embeddings from the harmonized Hindi and Urdu corpora to nullify their lexical divergences. As a proof of the concept, we evaluate our approach on the Hindi and Urdu dependency parsing under two scenarios: (a) resource sharing, and (b) resource augmentation. We demonstrate that a neural network-based dependency parser trained on augmented, harmonized Hindi and Urdu resources performs significantly better than the parsing models trained separately on the individual resources. We also show that we can achieve near state-of-the-art results when the parsers are used interchangeably."
W15-5951,Applying {S}anskrit Concepts for Reordering in {MT},2015,14,0,5,0.825992,36415,akshar bharati,Proceedings of the 12th International Conference on Natural Language Processing,0,"This paper presents a rule-based reordering approach for English-Hindi machine translation. We have used the concept of pada, from Pxc4x81n. inian Grammar to frame the reordering rules. A pada is a word form which is ready to participate in a sentence. The rules are generic enough to apply on any English-Indian language pair. We tested the rules on English-Hindi language pair and obtained better comprehensibility score as compared to Google Translate on the same test set. In assessing the effectiveness of the rules on padas which are analogous to minimal phrases in English, we achieved upto 93% accuracy on the test data."
W15-4005,Exploring the effect of semantic similarity for Phrase-based Machine Translation,2015,22,0,2,1,36729,kunal sachdeva,Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality,0,The paper investigates the use of semantic similarity scores as feature in the phrase based machine translation system. We propose the use of partial least square regression to learn the bilingual word embedding using compositional distributional semantics. The model outperforms the baseline system which is shown by an increase in BLEU score. We also show the effect of varying the vector dimension and context window for two different approaches of learning word vectors.
W14-5603,Exploring the effects of Sentence Simplification on {H}indi to {E}nglish Machine Translation System,2014,15,4,4,0,38225,kshitij mishra,Proceedings of the Workshop on Automatic Text Simplification - Methods and Applications in the Multilingual Society ({ATS}-{MA} 2014),0,"Even though, a lot of research has already been done on Machine Translation, translating complex sentences has been a stumbling block in the process. To improve the performance of machine translation on complex sentences, simplifying the sentences becomes imperative. In this paper, we present a rule based approach to address this problem by simplifying complex sentences in Hindi into multiple simple sentences. The sentence is split using clause boundaries and dependency parsing which identifies different arguments of verbs, thus changing the grammatical structure in a way that the semantic information of the original sentence stay preserved."
W14-5208,{SSF}: A Common Representation Scheme for Language Analysis for Language Technology Infrastructure Development,2014,18,6,3,0.825992,36415,akshar bharati,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"We describe a representation scheme and an analysis engine using that scheme, both of which have been used to develop infrastructure for HLT. The Shakti Standard Format is a readable and robust representation scheme for analysis frameworks and other purposes. The representation is highly extensible. This representation scheme, based on the blackboard architectural model, allows a very wide variety of linguistic and non-linguistic information to be stored in one place and operated upon by any number of processing modules. We show how it has been successfully used for building machine translation systems for several language pairs using the same architecture. It has also been used for creation of language resources such as treebanks and for different kinds of annotation interfaces. There is even a query language designed for this representation. Easily wrappable into XML, it can be used equally well for distributed computing."
W14-5123,Identification of Karaka relations in an {E}nglish sentence,2014,0,0,4,0,38315,sai gorthi,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-5125,A Sandhi Splitter for {M}alayalam,2014,6,5,3,1,33706,devadath,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-5146,{H}indi Word Sketches,2014,0,1,3,0,38334,anil eragani,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-4206,Adapting Predicate Frames for {U}rdu {P}rop{B}anking,2014,20,1,6,1,29432,riyaz bhat,Proceedings of the {EMNLP}{'}2014 Workshop on Language Technology for Closely Related Languages and Language Variants,0,"Hindi and Urdu are two standardized registers of what has been called the Hindustani language, which belongs to the IndoAryan language family. Although, both the varieties share a common grammar, they differ significantly in their vocabulary to an extent where both become mutually incomprehensible (Masica, 1993). Hindi draws its vocabulary from Sanskrit while Urdu draws its vocabulary from Persian, Arabic and even Turkish. In this paper, we present our efforts to adopt frames of nominal and verbal predicates that Urdu shares with either Hindi or Arabic for Urdu PropBanking. We discuss the feasibility of porting such frames from either of the sources (Arabic or Hindi) and also present a simple and reasonably accurate method to automatically identify the origin of Urdu words which is a necessary step in the process of porting such frames."
W14-4211,Exploring System Combination approaches for {I}ndo-{A}ryan {MT} Systems,2014,15,2,6,0,22835,karan singla,Proceedings of the {EMNLP}{'}2014 Workshop on Language Technology for Closely Related Languages and Language Variants,0,Statistical Machine Translation (SMT) systems are heavily dependent on the quality of parallel corpora used to train translation models. Translation quality between certain Indian languages is often poor due to the lack of training data of good quality. We used triangulation as a technique to improve the quality of translations in cases where the direct translation model did not perform satisfactorily. Triangulation uses a third language as a pivot between the source and target languages to achieve an improved and more efficient translation model in most cases. We also combined multi-pivot models using linear mixture and obtained significant improvement in BLEU scores compared to the direct source-target models.
W14-4006,Reducing the Impact of Data Sparsity in Statistical Machine Translation,2014,24,3,4,0,22835,karan singla,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Morphologically rich languages generally require large amounts of parallel data to adequately estimate parameters in a statistical Machine Translation(SMT) system. However, it is time consuming and expensive to create large collections of parallel data. In this paper, we explore two strategies for circumventing sparsity caused by lack of large parallel corpora. First, we explore the use of distributed representations in an Recurrent Neural Network based language model with different morphological features and second, we explore the use of lexical resources such as WordNet to overcome sparsity of content words."
yeka-etal-2014-benchmarking,Benchmarking of {E}nglish-{H}indi parallel corpora,2014,13,2,3,0,39362,jayendra yeka,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present several parallel corpora for English{\^a}ÂÂHindi and talk about their natures and domains. We also discuss briefly a few previous attempts in MT for translation from English to Hindi. The lack of uniformly annotated data makes it difficult to compare these attempts and precisely analyze their strengths and shortcomings. With this in mind, we propose a standard pipeline to provide uniform linguistic annotations to these resources using state-of-art NLP technologies. We conclude the paper by presenting evaluation scores of different statistical MT systems on the corpora detailed in this paper for English{\^a}ÂÂHindi and present the proposed plans for future work. We hope that both these annotated parallel corpora resources and MT systems will serve as benchmarks for future approaches to MT in English{\^a}ÂÂHindi. This was and remains the main motivation for the attempts detailed in this paper."
bhat-etal-2014-towards,Towards building a {K}ashmiri Treebank: Setting up the Annotation Pipeline,2014,10,0,3,1,29432,riyaz bhat,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Kashmiri is a resource poor language with very less computational and language resources available for its text processing. As the main contribution of this paper, we present an initial version of the Kashmiri Dependency Treebank. The treebank consists of 1,000 sentences (17,462 tokens), annotated with part-of-speech (POS), chunk and dependency information. The treebank has been manually annotated using the Paninian Computational Grammar (PCG) formalism (Begum et al., 2008; Bharati et al., 2009). This version of Kashmiri treebank is an extension of its earlier verion of 500 sentences (Bhat, 2012), a pilot experiment aimed at defining the annotation guidelines on a small subset of Kashmiri corpora. In this paper, we have refined the guidelines with some significant changes and have carried out inter-annotator agreement studies to ascertain its quality. We also present a dependency parsing pipeline, consisting of a tokenizer, a stemmer, a POS tagger, a chunker and an inter-chunk dependency parser. It, therefore, constitutes the first freely available, open source dependency parser of Kashmiri, setting the initial baseline for Kashmiri dependency parsing."
sachdeva-etal-2014-hindi,{H}indi to {E}nglish Machine Translation: Using Effective Selection in Multi-Model {SMT},2014,23,5,4,1,36729,kunal sachdeva,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Recent studies in machine translation support the fact that multi-model systems perform better than the individual models. In this paper, we describe a Hindi to English statistical machine translation system and improve over the baseline using multiple translation models. We have considered phrase based as well as hierarchical models and enhanced over both these baselines using a regression model. The system is trained over textual as well as syntactic features extracted from source and target of the aforementioned translations. Our system shows significant improvement over the baseline systems for both automatic as well as human evaluations. The proposed methodology is quite generic and easily be extended to other language pairs as well."
W13-3705,Divergences in {E}nglish-{H}indi Parallel Dependency Treebanks,2013,8,3,3,0,33958,himani chaudhry,Proceedings of the Second International Conference on Dependency Linguistics ({D}ep{L}ing 2013),0,"We present, here, our analysis of systematic divergences in parallel EnglishHindi dependency treebanks based on the Computational Paninian Grammar (CPG) framework. Study of structural divergences in parallel treebanks not only helps in developing larger treebanks automatically, but can also be useful for many NLP applications such as data-driven machine translation (MT) systems. Given that the two treebanks are based on the same grammatical model, a study of divergences in them could be of advantage to such tasks, along with making it more interesting to study how and where they diverge. We consider two parallel trees divergent based on differences in constructions, relations marked, frequency of annotation labels and tree depth. Some interesting instances of structural divergences in the treebanks have been discussed in the course of this paper. We also present our task of alignment of the two treebanks, wherein we talk about our extraction of divergent structures in the trees, and discuss the results of this exercise."
W13-3725,Towards Building Parallel Dependency Treebanks: Intra-Chunk Expansion and Alignment for {E}nglish Dependency Treebank,2013,17,0,6,0,40831,debanka nandi,Proceedings of the Second International Conference on Dependency Linguistics ({D}ep{L}ing 2013),0,"The paper presents our work on the annotation of intra-chunk dependencies on an English treebank that was previously annotated with Inter-chunk dependencies, and for which there exists a fully expanded parallel Hindi dependency treebank. This provides fully parsed dependency trees for the English treebank. We also report an analysis of the inter-annotator agreement for this chunk expansion task. Further, these fully expanded parallel Hindi and English treebanks were word aligned and an analysis for the task has been given. Issues related to intra-chunk expansion and alignment for the language pair HindiEnglish are discussed and guidelines for these tasks have been prepared and released."
W13-2320,{A}nimacy Annotation in the {H}indi Treebank,2013,24,1,4,0,40938,itisree jena,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"In this paper, we discuss our efforts to annotate nominals in the Hindi Treebank with the semantic property of animacy. Although the treebank already encodes lexical information at a number of levels such as morph and part of speech, the addition of animacy information seems promising given its relevance to varied linguistic phenomena. The suggestion is based on the theoretical and computational analysis of the property of animacy in the context of anaphora resolution, syntactic parsing, verb classification and argument differentiation."
I13-1008,{A}nimacy Acquisition Using Morphological Case,2013,28,0,2,1,29432,riyaz bhat,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Animacy is an inherent property of entities that nominals refer to in the physical world. This semantic property of a nominal has received much attention in both linguistics and computational linguistics. In this paper, we present a robust unsupervised technique to infer the animacy of nominals in languages with rich morphological case. The intuition behind our method is that the control/agency of a noun depicted by case marking can approximate its animacy. A higher control over an action implies higher animacy. Our experiments on Hindi show promising results withF andPurity scores of 89 and 86 respectively."
I13-1022,Exploring Semantic Information in {H}indi {W}ord{N}et for {H}indi Dependency Parsing,2013,30,6,5,0.984848,38366,sambhav jain,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we present our efforts towards incorporating external knowledge from Hindi WordNet to aid dependency parsing. We conduct parsing experiments on Hindi, an Indo-Aryan language, utilizing the information from concept ontologies available in Hindi WordNet to complement the morpho-syntactic information already available. The work is driven by the insight that concept ontologies capture a specific real world aspect of lexical items, which is quite distinct and unlikely to be deduced from morpho-syntactic information such as morph, POS-tag and chunk. This complementing information is encoded as an additional feature for data driven parsing and experiments are conducted. We perform experiments over datasets of different sizes. We achieve an improvement of 1.1% (LAS) when training on 1,000 sentences and 0.2% (LAS) on 13,371 sentences over the baseline. The improvements are statistically significant at p<0.01. The higher improvements on 1,000 sentences suggest that the semantic information could address the data sparsity problem."
I13-1130,A Hybrid Approach for Anaphora Resolution in {H}indi,2013,11,17,3,1,23619,praveen dakwale,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper we present a hybrid approach to resolve Entity-pronoun references in Hindi. While most of the existing approaches, syntactic as well as data-driven, use phrase-structure syntax for anaphora resolution, we explore use of dependency structures as a source of syntactic information. In our approach, dependency structures are used by a rule-based module to resolve simple anaphoric references, while a decision tree classifier is used to resolve more ambiguous instances, using grammatical and semantic features. Our results show that, use of dependency structures provides syntactic knowledge which helps to resolve some specific types of references. Semantic information such as animacy and Named Entity categories further helps to improve the resolution accuracy."
I13-1151,Exploring Verb Frames for Sentence Simplification in {H}indi,2013,14,2,3,0,38226,ankush soni,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Systems processing on natural language text encounters fatal problems due to long and complex sentences. Their performance degrades as the complexity of the sentence increases. This paper addresses the task of simplifying complex sentences in Hindi into multiple simple sentences, using a rule based approach. Our approach utilizes two linguistic resources viz. verb demand frames and conjunctsxe2x80x99 list. We performed automatic as well as human evaluation of our system."
Y12-1042,Anaphora Annotation in {H}indi Dependency {T}ree{B}ank,2012,12,7,3,1,23619,praveen dakwale,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"In this paper, we propose a scheme for anaphora annotation in Hindi Dependency Treebank. The goal is to identify and handle the challenges that arise in the annotation of reference relations in Hindi. We identify some of the issues related to anaphora annotation specific to Hindi such as distribution of markable span, sequential annotation, representation format, annotation of multiple referents etc. The scheme hence incorporates some characteristics specific to these issues in order to achieve a consistent annotation. Most significant among these characteristics is the head-modifier separation in referent selection. The modifier-modified dependency relations inside a markable is utilized for this headmodifier distinction. A part of the Hindi Dependency Treebank, of around 2500 sentences has been annotated with anaphoric relations and an inter-annotator study was carried out which shows a significant agreement over selection of the head referent using the proposed scheme as compared to MUC annotation format. The current annotation is done for a limited set of pronominal categories."
W12-3607,Intra-Chunk Dependency Annotation : Expanding {H}indi Inter-Chunk Annotated Treebank,2012,6,6,4,0,42201,prudhvi kosaraju,Proceedings of the Sixth Linguistic Annotation Workshop,0,"We present two approaches (rule-based and statistical) for automatically annotating intra-chunk dependencies in Hindi. The intra-chunk dependencies are added to the dependency trees for Hindi which are already annotated with inter-chunk dependencies. Thus, the intra-chunk annotator finally provides a fully parsed dependency tree for a Hindi sentence. In this paper, we first describe the guidelines for marking intra-chunk dependency relations. Although the guidelines are for Hindi, they can easily be extended to other Indian languages. These guidelines are used for framing the rules in the rule-based approach. For the statistical approach, we use MaltParser, a data driven parser. A part of the ICON 2010 tools contest data for Hindi is used for training and testing the MaltParser. The same set is used for testing the rule-based approach."
W12-3623,Dependency Treebank of {U}rdu and its Evaluation,2012,27,10,2,1,29432,riyaz bhat,Proceedings of the Sixth Linguistic Annotation Workshop,0,"In this paper we describe a currently underway treebanking effort for Urdu-a South Asian language. The treebank is built from a newspaper corpus and uses a Karaka based grammatical framework inspired by Paninian grammatical theory. Thus far 3366 sentences (0.1M words) have been annotated with the linguistic information at morpho-syntactic (morphological, part-of-speech and chunk information) and syntactico-semantic (dependency) levels. This work also aims to evaluate the correctness or reliability of this manual annotated dependency treebank. Evaluation is done by measuring the inter-annotator agreement on a manually annotated data set of 196 sentences (5600 words) annotated by two annotators. We present the qualitative analysis of the agreement statistics and identify the possible reasons for the disagreement between the annotators. We also show the syntactic annotation of some constructions specific to Urdu like Ezafe and discuss the problem of word segmentation (tokenization)."
W12-2302,{H}indi Derivational Morphological Analyzer,2012,25,12,3,0,42345,nikhil kanuparthi,Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology,0,"Hindi is an Indian language which is relatively rich in morphology. A few morphological analyzers of this language have been developed. However, they give only inflectional analysis of the language. In this paper, we present our Hindi derivational morphological analyzer. Our algorithm upgrades an existing inflectional analyzer to a derivational analyzer and primarily achieves two goals. First, it successfully incorporates derivational analysis in the inflectional analyzer. Second, it also increases the coverage of the inflectional analysis of the existing inflectional analyzer."
kolachina-etal-2012-evaluation,Evaluation of Discourse Relation Annotation in the {H}indi Discourse Relation Bank,2012,15,10,3,0.634921,24267,sudheer kolachina,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe our experiments on evaluating recently proposed modifications to the discourse relation annotation scheme of the Penn Discourse Treebank (PDTB), in the context of annotating discourse relations in Hindi Discourse Relation Bank (HDRB). While the proposed modifications were driven by the desire to introduce greater conceptual clarity in the PDTB scheme and to facilitate better annotation quality, our findings indicate that overall, some of the changes render the annotation task much more difficult for the annotators, as also reflected in lower inter-annotator agreement for the relevant sub-tasks. Our study emphasizes the importance of best practices in annotation task design and guidelines, given that a major goal of an annotation effort should be to achieve maximally high agreement between annotators. Based on our study, we suggest modifications to the current version of the HDRB, to be incorporated in our future annotation work."
W11-3405,Error Detection for Treebank Validation,2011,17,6,5,0.833333,34684,bharat ambati,Proceedings of the 9th Workshop on {A}sian Language Resources,0,This paper describes an error detection mechanism which helps in validation of dependency treebank annotation. Consistency in treebank annotation is a must for making data as errorfree as possible and for assuring the usefulness of treebank. This work is aimed at ensuring this consistency and to make the task of validation cost effective by detecting major errors induced during completely manual annotation. We evaluated our system on the Hindi dependency treebank which is currently under development. We could detect 76.63% of errors at dependency level. Results show that our system performs well even when the training data is low.
W11-0414,Creating an Annotated {T}amil Corpus as a Discourse Resource,2011,6,7,2,0,44410,ravi rachakonda,Proceedings of the 5th Linguistic Annotation Workshop,0,"We describe our efforts to apply the Penn Discourse Treebank guidelines on a Tamil corpus to create an annotated corpus of discourse relations in Tamil. After conducting a preliminary exploratory study on Tamil discourse connectives, we show our observations and results of a pilot experiment that we conducted by annotating a small portion of our corpus. Our ultimate goal is to develop a Tamil Discourse Relation Bank that will be useful as a resource for further research in Tamil discourse. Furthermore, a study of the behavior of discourse connectives in Tamil will also help in furthering the cross-linguistic understanding of discourse connectives."
W10-3216,A Preliminary Work on {H}indi Causatives,2010,12,1,2,1,34994,rafiya begum,Proceedings of the Eighth Workshop on {A}sian Language Resouces,0,"This paper introduces a preliminary work on Hindi causative verbs: their classification, a linguistic model for their classification and their verb frames. The main objective of this work is to come up with a classification of the Hindi causative verbs. In the classification we show how different types of Hindi verbs have different types of causative forms. It will be a linguistic resource for Hindi causative verbs which can be used in various NLP applications. This resource enriches the already available linguistic resource on Hindi verb frames (Begum et al., 2008b). This resource will be helpful in getting proper insight into Hindi verbs. In this paper, we present the morphology, semantics and syntax of the causative verbs. The morphology is captured by the word generation process; semantics is captured by the linguistic model followed for classifying the verbs and the syntax has been captured by the verb frames using relations given by Panini."
W10-2103,On the Role of {NLP} in Linguistics,2010,13,2,1,1,379,dipti sharma,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"This paper summarizes some of the applications of NLP techniques in various linguistic sub-fields, and presents a few examples that call for a deeper engagement between the two fields."
W10-1403,Two Methods to Incorporate {'}Local Morphosyntactic{'} Features in {H}indi Dependency Parsing,2010,23,25,4,0.833333,34684,bharat ambati,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"In this paper we explore two strategies to incorporate local morphosyntactic features in Hindi dependency parsing. These features are obtained using a shallow parser. We first explore which information provided by the shallow parser is most beneficial and show that local morphosyntactic features in the form of chunk type, head/non-head information, chunk boundary information, distance to the end of the chunk and suffix concatenation are very crucial in Hindi dependency parsing. We then investigate the best way to incorporate this information during dependency parsing. Further, we compare the results of various experiments based on various criterions and do some error analysis. All the experiments were done with two data-driven parsers, MaltParser and MSTParser, on a part of multi-layered and multi-representational Hindi Treebank which is under development. This paper is also the first attempt at complete sentence level parsing for Hindi."
N10-1093,Improving Data Driven Dependency Parsing using Clausal Information,2010,14,16,4,0,40073,phani gadde,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"The paper describes a data driven dependency parsing approach which uses clausal information of a sentence to improve the parser performance. The clausal information is added automatically during the parsing process. We demonstrate the experiments on Hindi, a language with relatively rich case marking system and free-word-order. All the experiments are done using a modified version of MSTParser. We did all the experiments on the ICON 2009 parsing contest data. We achieved an improvement of 0.87% and 0.77% in unlabeled attachment and labeled attachment accuracies respectively over the baseline parsing accuracies."
bhatia-etal-2010-empty,Empty Categories in a {H}indi Treebank,2010,8,9,6,0,11651,archna bhatia,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We are in the process of creating a multi-representational and multi-layered treebank for Hindi/Urdu (Palmer et al., 2009), which has three main layers: dependency structure, predicate-argument structure (PropBank), and phrase structure. This paper discusses an important issue in treebank design which is often neglected: the use of empty categories (ECs). All three levels of representation make use of ECs. We make a high-level distinction between two types of ECs, trace and silent, on the basis of whether they are postulated to mark displacement or not. Each type is further refined into several subtypes based on the underlying linguistic phenomena which the ECs are introduced to handle. This paper discusses the stages at which we add ECs to the Hindi/Urdu treebank and why. We investigate methodically the different types of ECs and their role in our syntactic and semantic representations. We also examine our decisions whether or not to coindex each type of ECs with other elements in the representation."
ambati-etal-2010-high,A High Recall Error Identification Tool for {H}indi Treebank Validation,2010,12,11,4,0.833333,34684,bharat ambati,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes the development of a hybrid tool for a semi-automated process for validation of treebank annotation at various levels. The tool is developed for error detection at the part-of-speech, chunk and dependency levels of a Hindi treebank, currently under development. The tool aims to identify as many errors as possible at these levels to achieve consistency in the task of annotation. Consistency in treebank annotation is a must for making data as error-free as possible and for providing quality assurance. The tool is aimed at ensuring consistency and to make manual validation cost effective. We discuss a rule based and a hybrid approach (statistical methods combined with rule-based methods) by which a high-recall system can be developed and used to identify errors in the treebank. We report some results of using the tool on a sample of data extracted from the Hindi treebank. We also argue how the tool can prove useful in improving the annotation guidelines which would in turn, better the quality of annotation in subsequent iterations."
gupta-etal-2010-partial,Partial Parsing as a Method to Expedite Dependency Annotation of a {H}indi Treebank,2010,23,4,4,1,44097,mridul gupta,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The paper describes an approach to expedite the process of manual annotation of a Hindi dependency treebank which is currently under development. We propose a way by which consistency among a set of manual annotators could be improved. Furthermore, we show that our setup can also prove useful for evaluating when an inexperienced annotator is ready to start participating in the production of the treebank. We test our approach on sample sets of data obtained from an ongoing work on creation of this treebank. The results asserting our proposal are reported in this paper. We report results from a semi-automated approach of dependency annotation experiment. We find out the rate of agreement between annotators using CohenÂs Kappa. We also compare results with respect to the total time taken to annotate sample data-sets using a completely manual approach as opposed to a semi-automated approach. It is observed from the results that this semi-automated approach when carried out with experienced and trained human annotators improves the overall quality of treebank annotation and also speeds up the process."
Y09-2020,Constraint Based Hybrid Approach to Parsing {I}ndian Languages,2009,17,13,5,0.9375,36415,akshar bharati,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"The paper describes the overall design of a new two stage constraint based hybrid approach to dependency parsing. We define the two stages and show how different grammatical construct are parsed at appropriate stages. This division leads to selective identification and resolution of specific dependency relations at the two stages. Furthermore, we show how the use of hard constraints and soft constraints helps us build an efficient and robust hybrid parser. Finally, we evaluate the implemented parser on Hindi and compare the results with that of two data driven dependency parsers."
W09-3029,The {H}indi Discourse Relation Bank,2009,7,32,4,0,42439,umangi oza,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"We describe the Hindi Discourse Relation Bank project, aimed at developing a large corpus annotated with discourse relations. We adopt the lexically grounded approach of the Penn Discourse Treebank, and describe our classification of Hindi discourse connectives, our modifications to the sense classification of discourse relations, and some cross-linguistic comparisons based on some initial annotations carried out so far."
W09-3030,Simple Parser for {I}ndian Languages in a Dependency Framework,2009,4,18,5,0.9375,36415,akshar bharati,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,This paper is an attempt to show that an intermediary level of analysis is an effective way for carrying out various NLP tasks for linguistically similar languages. We describe a process for developing a simple parser for doing such tasks. This parser uses a grammar driven approach to annotate dependency relations (both inter and intra chunk) at an intermediary level. Ease in identifying a particular dependency relation dictates the degree of analysis reached by the parser. To establish efficiency of the simple parser we show the improvement in its results over previous grammar driven dependency parsing approaches for Indian languages like Hindi. We also propose the possibility of usefulness of the simple parser for Indian languages that are similar in nature.
W09-3036,A Multi-Representational and Multi-Layered Treebank for {H}indi/{U}rdu,2009,11,89,5,1,40828,rajesh bhatt,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"This paper describes the simultaneous development of dependency structure and phrase structure treebanks for Hindi and Urdu, as well as a PropBank. The dependency structure and the PropBank are manually annotated, and then the phrase structure treebank is produced automatically. To ensure successful conversion the development of the guidelines for all three representations are carefully coordinated."
begum-etal-2008-developing,Developing Verb Frames for {H}indi,2008,11,12,4,1,34994,rafiya begum,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,This paper introduces an ongoing work on developing verb frames for Hindi. Verb frames capture syntactic commonalities of semantically related verbs. The main objective of this work is to create a linguistic resource which will prove to be indispensable for various NLP applications. We also hope this resource to help us better understand Hindi verbs. We motivate the basic verb argument structure using relations as introduced by Panini. We show the methodology used in preparing these frames and the criteria followed for classifying Hindi verbs.
I08-7010,Towards an Annotated Corpus of Discourse Relations in {H}indi,2008,9,16,3,1,4772,rashmi prasad,Proceedings of the 6th Workshop on {A}sian Language Resources,0,"We describe our initial efforts towards developing a large-scale corpus of Hindi texts annotated with discourse relations. Adopting the lexically grounded approach of the Penn Discourse Treebank (PDTB), we present a preliminary analysis of discourse connectives in a small corpus. We describe how discourse connectives are represented in the sentence-level dependency annotation in Hindi, and discuss how the discourse annotation can enrich this level for research and applications. The ultimate goal of our work is to build a Hindi Discourse Relation Bank along the lines of the PDTB. Our work will also contribute to the cross-linguistic understanding of discourse connectives."
I08-5005,Aggregating Machine Learning and Rule Based Heuristics for Named Entity Recognition,2008,15,35,5,0,45220,karthik gali,Proceedings of the {IJCNLP}-08 Workshop on Named Entity Recognition for South and South East {A}sian Languages,0,"This paper, submitted as an entry for the NERSSEAL-2008 shared task, describes a system build for Named Entity Recognition for South and South East Asian Languages. Our paper combines machine learning techniques with language specific heuristics to model the problem of NER for Indian languages. The system has been tested on five languages: Telugu, Hindi, Bengali, Urdu and Oriya. It uses CRF (Conditional Random Fields) based machine learning, followed by post processing which involves using some heuristics or rules. The system is specifically tuned for Hindi and Telugu, we also report the results for the other four languages."
I08-2099,Dependency Annotation Scheme for {I}ndian Languages,2008,14,102,4,1,34994,rafiya begum,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"The paper introduces a dependency annotation effort which aims to fully annotate a million word Hindi corpus. It is the first attempt of its kind to develop a large scale tree-bank for an Indian language. In this paper we provide the motivation for following the Paninian framework as the annotation scheme and argue that the Paninian framework is better suited to model the various linguistic phenomena manifest in Indian languages. We present the basic annotation scheme. We also show how the scheme handles some phenomenon such as complex verbs, ellipses, etc. Empirical results of some experiments done on the currently annotated sentences are also reported."
W07-1608,Simple Preposition Correspondence: A Problem in {E}nglish to {I}ndian Language Machine Translation,2007,16,4,2,1,11543,samar husain,Proceedings of the Fourth {ACL}-{SIGSEM} Workshop on Prepositions,0,The paper describes an approach to automatically select from Indian Language the appropriate lexical correspondence of English simple preposition. The paper describes this task from a Machine Translation (MT) perspective. We use the properties of the head and complement of the preposition to select the appropriate sense in the target language. We later show that the results obtained from this approach are promising.
W02-1202,{A}nn{C}orra: Building Tree-banks in {I}ndian Languages,2002,2,2,5,0,36415,akshar bharati,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,"This paper describes a dependency based tagging scheme for creating tree banks for Indian languages. The scheme has been so designed that it is comprehensive, easy to use with linear notation and economical in typing effort. It is based on Paninian grammatical model."
