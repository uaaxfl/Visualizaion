2021.starsem-1.1,Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge,2021,-1,-1,5,1,926,paolo pedinotti,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge). Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and we conducted a detailed error analysis to investigate which factors affect their behavior. Our results show that TLMs can reach performances that are comparable to those achieved by SDM. However, additional analysis consistently suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, such as frequent words, collocations and syntactic patterns, thereby showing sub-optimal generalization abilities."
2021.cmcl-1.12,{PIHK}ers at {CMCL} 2021 Shared Task: Cosine Similarity and Surprisal to Predict Human Reading Patterns.,2021,-1,-1,2,0,11529,lavinia salicchi,Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics,0,"Eye-tracking psycholinguistic studies have revealed that context-word semantic coherence and predictability influence language processing. In this paper we show our approach to predict eye-tracking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a regression model, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features."
2021.blackboxnlp-1.13,A howling success or a working sea? Testing what {BERT} knows about metaphors,2021,-1,-1,4,1,926,paolo pedinotti,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Metaphor is a widespread linguistic and cognitive phenomenon that is ruled by mechanisms which have received attention in the literature. Transformer Language Models such as BERT have brought improvements in metaphor-related tasks. However, they have been used only in application contexts, while their knowledge of the phenomenon has not been analyzed. To test what BERT knows about metaphors, we challenge it on a new dataset that we designed to test various aspects of this phenomenon such as variations in linguistic structure, variations in conventionality, the boundaries of the plausibility of a metaphor and the interpretations that we attribute to metaphoric expressions. Results bring out some tendencies that suggest that the model can reproduce some human intuitions about metaphors."
2020.starsem-1.14,{PISA}: A measure of Preference In Selection of Arguments to model verb argument recoverability,2020,-1,-1,2,0,14534,giulia cappelli,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Our paper offers a computational model of the semantic recoverability of verb arguments, tested in particular on direct objects and Instruments. Our fully distributional model is intended to improve on older taxonomy-based models, which require a lexicon in addition to the training corpus. We computed the selectional preferences of 99 transitive verbs and 173 Instrument verbs as the mean value of the pairwise cosines between their arguments (a weighted mean between all the arguments, or an unweighted mean with the topmost k arguments). Results show that our model can predict the recoverability of objects and Instruments, providing a similar result to that of taxonomy-based models but at a much cheaper computational cost."
2020.lt4gov-1.2,{FRAQUE}: a {FRA}me-based {QUE}stion-answering system for the Public Administration domain,2020,-1,-1,3,0,16586,martina miliani,Proceedings of the 1st Workshop on Language Technologies for Government and Public Administration (LT4Gov),0,"In this paper, we propose FRAQUE, a question answering system for factoid questions in the Public administration domain. The system is based on semantic frames, here intended as collections of slots typed with their possible values. FRAQUE queries unstructured textual data and exploits the potential of different approaches: it extracts pattern elements from texts which are linguistically analyzed through statistical methods.FRAQUE allows Italian users to query vast document repositories related to the domain of Public Administration. Given the statistical nature of most of its components such as word embeddings, the system allows for a flexible domain and language adaptation process. FRAQUE{'}s goal is to associate questions with frames stored into a Knowledge Graph along with relevant document passages, which are returned as the answer."
2020.lrec-1.114,{``}Voices of the Great War{''}: A Richly Annotated Corpus of {I}talian Texts on the First World War,2020,-1,-1,11,0,16843,federico boschetti,Proceedings of the 12th Language Resources and Evaluation Conference,0,"{``}Voices of the Great War{''} is the first large corpus of Italian historical texts dating back to the period of First World War. This corpus differs from other existing resources in several respects. First, from the linguistic point of view it gives account of the wide range of varieties in which Italian was articulated in that period, namely from a diastratic (educated vs. uneducated writers), diaphasic (low/informal vs. high/formal registers) and diatopic (regional varieties, dialects) points of view. From the historical perspective, through a collection of texts belonging to different genres it represents different views on the war and the various styles of narrating war events and experiences. The final corpus is balanced along various dimensions, corresponding to the textual genre, the language variety used, the author type and the typology of conveyed contents. The corpus is fully annotated with lemmas, part-of-speech, terminology, and named entities. Significant corpus samples representative of the different {``}voices{''} have also been enriched with meta-linguistic and syntactic information. The layer of syntactic annotation forms the first nucleus of an Italian historical treebank complying with the Universal Dependencies standard. The paper illustrates the final resource, the methodology and tools used to build it, and the Web Interface for navigating it."
2020.lrec-1.700,Are Word Embeddings Really a Bad Fit for the Estimation of Thematic Fit?,2020,-1,-1,4,1,180,emmanuele chersoni,Proceedings of the 12th Language Resources and Evaluation Conference,0,"While neural embeddings represent a popular choice for word representation in a wide variety of NLP tasks, their usage for thematic fit modeling has been limited, as they have been reported to lag behind syntax-based count models. In this paper, we propose a complete evaluation of count models and word embeddings on thematic fit estimation, by taking into account a larger number of parameters and verb roles and introducing also dependency-based embeddings in the comparison. Our results show a complex scenario, where a determinant factor for the performance seems to be the availability to the model of reliable syntactic information for building the distributional representations of the roles."
2020.lrec-1.718,Representing Verbs with Visual Argument Vectors,2020,-1,-1,2,0,11013,irene sucameli,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Is it possible to use images to model verb semantic similarities? Starting from this core question, we developed two textual distributional semantic models and a visual one. We found particularly interesting and challenging to investigate this Part of Speech since verbs are not often analysed in researches focused on multimodal distributional semantics. After the creation of the visual and textual distributional space, the three models were evaluated in relation to SimLex-999, a gold standard resource. Through this evaluation, we demonstrate that, using visual distributional models, it is possible to extract meaningful information and to effectively capture the semantic similarity between verbs."
2020.coling-main.602,Don{'}t Invite {BERT} to Drink a Bottle: Modeling the Interpretation of Metonymies Using {BERT} and Distributional Representations,2020,-1,-1,2,1,926,paolo pedinotti,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this work, we carry out two experiments in order to assess the ability of BERT to capture the meaning shift associated with metonymic expressions. We test the model on a new dataset that is representative of the most common types of metonymy. We compare BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge. The results reveal that, while BERT ability to deal with metonymy is quite limited, SDM is good at predicting the meaning of metonymic expressions, providing support for an account of metonymy based on event knowledge."
2020.aacl-main.26,"Comparing Probabilistic, Distributional and Transformer-Based Models on Logical Metonymy Interpretation",2020,-1,-1,3,1,927,giulia rambelli,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In linguistics and cognitive science, Logical metonymies are defined as type clashes between an event-selecting verb and an entity-denoting noun (e.g. The editor finished the article), which are typically interpreted by inferring a hidden event (e.g. reading) on the basis of contextual cues. This paper tackles the problem of logical metonymy interpretation, that is, the retrieval of the covert event via computational methods. We compare different types of models, including the probabilistic and the distributional ones previously introduced in the literature on the topic. For the first time, we also tested on this task some of the recent Transformer-based models, such as BERT, RoBERTa, XLNet, and GPT-2. Our results show a complex scenario, in which the best Transformer-based models and some traditional distributional models perform very similarly. However, the low performance on some of the testing datasets suggests that logical metonymy is still a challenging phenomenon for computational modeling."
W19-3312,Distributional Semantics Meets Construction Grammar. towards a Unified Usage-Based Model of Grammar and Meaning,2019,0,0,5,1,927,giulia rambelli,Proceedings of the First International Workshop on Designing Meaning Representations,0,"In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Construction Grammar, where grammar and meaning are systematically modeled from language use, and finally, we discuss the kind of contributions that distributional models can provide to CxG representation from a linguistic and cognitive perspective."
W18-4603,Modeling Violations of Selectional Restrictions with Distributional Semantics,2018,0,2,4,1,180,emmanuele chersoni,Proceedings of the Workshop on Linguistic Complexity and Natural Language Processing,0,"Distributional Semantic Models have been successfully used for modeling selectional preferences in a variety of scenarios, since distributional similarity naturally provides an estimate of the degree to which an argument satisfies the requirement of a given predicate. However, we argue that the performance of such models on rare verb-argument combinations has received relatively little attention: it is not clear whether they are able to distinguish the combinations that are simply atypical, or implausible, from the semantically anomalous ones, and in particular, they have never been tested on the task of modeling their differences in processing complexity. In this paper, we compare two different models of thematic fit by testing their ability of identifying violations of selectional restrictions in two datasets from the experimental studies."
S18-1117,{S}em{E}val-2018 Task 10: Capturing Discriminative Attributes,2018,0,11,2,0,28851,alicia krebs,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the SemEval 2018 Task 10 on Capturing Discriminative Attributes. Participants were asked to identify whether an attribute could help discriminate between two concepts. For example, a successful system should determine that {`}urine{'} is a discriminating feature in the word pair {`}kidney{'}, {`}bone{'}. The aim of the task is to better evaluate the capabilities of state of the art semantic models, beyond pure semantic similarity. The task attracted submissions from 21 teams, and the best system achieved a 0.75 F1 score."
W17-6803,Is Structure Necessary for Modeling Argument Expectations in Distributional Semantics?,2017,32,0,4,1,180,emmanuele chersoni,{IWCS} 2017 - 12th International Conference on Computational Semantics - Long papers,0,"Despite the number of NLP studies dedicated to thematic fit estimation, little attention has been paid to the related task of composing and updating verb argument expectations. The few exceptions have mostly modeled this phenomenon with structured distributional models, implicitly assuming a similarly structured representation of events. Recent experimental evidence, however, suggests that human processing system could also exploit an unstructured bag-of-arguments type of event representation to predict upcoming input. In this paper, we re-implement a traditional structured model and adapt it to compare the different hypotheses concerning the degree of structure in our event knowledge, evaluating their relative performance in the task of the argument expectations update."
W17-6524,{UDL}ex: Towards Cross-language Subcategorization Lexicons,2017,27,0,2,1,927,giulia rambelli,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
S17-1021,Logical Metonymy in a Distributional Model of Sentence Comprehension,2017,15,2,2,1,180,emmanuele chersoni,Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*{SEM} 2017),0,"In theoretical linguistics, logical metonymy is defined as the combination of an event-subcategorizing verb with an entity-denoting direct object (e.g., The author began the book), so that the interpretation of the VP requires the retrieval of a covert event (e.g., writing). Psycholinguistic studies have revealed extra processing costs for logical metonymy, a phenomenon generally explained with the introduction of new semantic structure. In this paper, we present a general distributional model for sentence comprehension inspired by the Memory, Unification and Control model by Hagoort (2013,2016). We show that our distributional framework can account for the extra processing costs of logical metonymy and can identify the covert event in a classification task."
D17-1068,Measuring Thematic Fit with Distributional Feature Overlap,2017,33,1,3,1,181,enrico santus,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we introduce a new distributional method for modeling predicate-argument thematic fit judgments. We use a syntax-based DSM to build a prototypical representation of verb-specific roles: for every verb, we extract the most salient second order contexts for each of its roles (i.e. the most salient dimensions of typical role fillers), and then we compute thematic fit as a weighted overlap between the top features of candidate fillers and role prototypes. Our experiments show that our method consistently outperforms a baseline re-implementing a state-of-the-art system, and achieves better or comparable results to those reported in the literature for the other unsupervised systems. Moreover, it provides an explicit representation of the features characterizing verb-specific semantic roles."
Y16-2021,Testing {APS}yn against Vector Cosine on Similarity Estimation,2016,29,11,3,1,181,enrico santus,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,"In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation."
W16-5302,"{``}Beware the Jabberwock, dear reader!{''} Testing the distributional reality of construction semantics",2016,29,1,2,1,33488,gianluca lebani,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"Notwithstanding the success of the notion of construction, the computational tradition still lacks a way to represent the semantic content of these linguistic entities. Here we present a simple corpus-based model implementing the idea that the meaning of a syntactic construction is intimately related to the semantics of its typical verbs. It is a two-step process, that starts by identifying the typical verbs occurring with a given syntactic construction and building their distributional vectors. We then calculated the weighted centroid of these vectors in order to derive the distributional signature of a construction. In order to assess the goodness of our approach, we replicated the priming effect described by Johnson and Golberg (2013) as a function of the semantic distance between a construction and its prototypical verbs. Additional support for our view comes from a regression analysis showing that our distributional information can be used to model behavioral data collected with a crowdsourced elicitation experiment."
W16-5309,The {C}og{AL}ex-{V} Shared Task on the Corpus-Based Identification of Semantic Relations,2016,0,3,4,1,181,enrico santus,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"The shared task of the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V) aims at providing a common benchmark for testing current corpus-based methods for the identification of lexical semantic relations (synonymy, antonymy, hypernymy, part-whole meronymy) and at gaining a better understanding of their respective strengths and weaknesses. The shared task uses a challenging dataset extracted from EVALution 1.0, which contains word pairs holding the above-mentioned relations as well as semantically unrelated control items (random). The task is split into two subtasks: (i) identification of related word pairs vs. unrelated ones; (ii) classification of the word pairs according to their semantic relation. This paper describes the subtasks, the dataset, the evaluation metrics, the seven participating systems and their results. The best performing system in subtask 1 is GHHH (F1 = 0.790), while the best system in subtask 2 is LexNet (F1 = 0.445). The dataset and the task description are available at \url{https://sites.google.com/site/cogalex2016/home/shared-task}."
W16-5322,Antonymy and Canonicity: Experimental and Distributional Evidence,2016,13,0,2,0,33505,andreana pastena,Proceedings of the 5th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex - V),0,"The present paper investigates the phenomenon of antonym canonicity by providing new behavioural and distributional evidence on Italian adjectives. Previous studies have showed that some pairs of antonyms are perceived to be better examples of opposition than others, and are so considered representative of the whole category (e.g., Deese, 1964; Murphy, 2003; Paradis et al., 2009). Our goal is to further investigate why such canonical pairs (Murphy, 2003) exist and how they come to be associated. In the literature, two different approaches have dealt with this issue. The lexical-categorical approach (Charles and Miller, 1989; Justeson and Katz, 1991) finds the cause of canonicity in the high co-occurrence frequency of the two adjectives. The cognitive-prototype approach (Paradis et al., 2009; Jones et al., 2012) instead claims that two adjectives form a canonical pair because they are aligned along a simple and salient dimension. Our empirical evidence, while supporting the latter view, shows that the paradigmatic distributional properties of adjectives can also contribute to explain the phenomenon of canonicity, providing a corpus-based correlate of the cognitive notion of salience."
W16-4102,Towards a Distributional Model of Semantic Complexity,2016,20,2,3,1,180,emmanuele chersoni,Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC}),0,"In this paper, we introduce for the first time a Distributional Model for computing semantic complexity, inspired by the general principles of the Memory, Unification and Control framework(Hagoort, 2013; Hagoort, 2016). We argue that sentence comprehension is an incremental process driven by the goal of constructing a coherent representation of the event represented by the sentence. The composition cost of a sentence depends on the semantic coherence of the event being constructed and on the activation degree of the linguistic constructions. We also report the results of a first evaluation of the model on the Bicknell dataset (Bicknell et al., 2010)."
W16-1803,Lexical Variability and Compositionality: Investigating Idiomaticity with Distributional Semantic Models,2016,34,6,3,0,33945,marco senaldi,Proceedings of the 12th Workshop on Multiword Expressions,0,In this work we carried out an idiom type identification task on a set of 90 Italian V-NP and V-PP constructions comprising both idioms and non-idioms. Lexical variants were generated from these expressions by replacing their components with semantically related words extracted distributionally and from the Italian section of MultiWordNet. Idiomatic phrases turned out to be less similar to their lexical variants with respect to non-idiomatic ones in distributional semantic spaces. Different variant-based distributional measures of idiomaticity were tested. Our indices proved reliable in identifying also those idioms whose lexical variants are poorly or not at all attested in our corpus.
L16-1148,{L}ex{F}r: Adapting the {L}ex{I}t Framework to Build a Corpus-based {F}rench Subcategorization Lexicon,2016,27,1,4,1,927,giulia rambelli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper introduces LexFr, a corpus-based French lexical resource built by adapting the framework LexIt, originally developed to describe the combinatorial potential of Italian predicates. As in the original framework, the behavior of a group of target predicates is characterized by a series of syntactic (i.e., subcategorization frames) and semantic (i.e., selectional preferences) statistical information (a.k.a. distributional profiles) whose extraction process is mostly unsupervised. The first release of LexFr includes information for 2,493 verbs, 7,939 nouns and 2,628 adjectives. In these pages we describe the adaptation process and evaluated the final resource by comparing the information collected for 20 test verbs against the information available in a gold standard dictionary. In the best performing setting, we obtained 0.74 precision, 0.66 recall and 0.70 F-measure."
L16-1347,Evaluating Context Selection Strategies to Build Emotive Vector Space Models,2016,17,3,2,0,16587,lucia passaro,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we compare different context selection approaches to improve the creation of Emotive Vector Space Models (VSMs). The system is based on the results of an existing approach that showed the possibility to create and update VSMs by exploiting crowdsourcing and human annotation. Here, we introduce a method to manipulate the contexts of the VSMs under the assumption that the emotive connotation of a target word is a function of both its syntagmatic and paradigmatic association with the various emotions. To study the differences among the proposed spaces and to confirm the reliability of the system, we report on two experiments: in the first one we validated the best candidates extracted from each model, and in the second one we compared the models{'} performance on a random sample of target words. Both experiments have been implemented as crowdsourcing tasks."
L16-1419,{I}talian {V}erb{N}et: A Construction-based Approach to {I}talian Verb Classification,2016,0,1,2,0,787,lucia busso,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper proposes a new method for Italian verb classification -and a preliminary example of resulting classes- inspired by Levin (1993) and VerbNet (Kipper-Schuler, 2005), yet partially independent from these resources; we achieved such a result by integrating Levin and VerbNet{'}s models of classification with other theoretic frameworks and resources. The classification is rooted in the constructionist framework (Goldberg, 1995; 2006) and is distribution-based. It is also semantically characterized by a link to FrameNet{'}ssemanticframesto represent the event expressed by a class. However, the new Italian classes maintain the hierarchic {``}tree{''} structure and monotonic nature of VerbNet{'}s classes, and, where possible, the original names (e.g.: Verbs of Killing, Verbs of Putting, etc.). We therefore propose here a taxonomy compatible with VerbNet but at the same time adapted to Italian syntax and semantics. It also addresses a number of problems intrinsic to the original classifications, such as the role of argument alternations, here regarded simply as epiphenomena, consistently with the constructionist approach."
L16-1722,Nine Features in a Random Forest to Learn Taxonomical Semantic Relations,2016,3,12,2,1,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7{\%}, against a baseline of 57.2{\%} (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline. hypernyms-co-hyponyms 95.7{\%} vs. 69.8{\%}, hypernyms-random 91.8{\%} vs. 64.1{\%} and co-hyponyms-random 97.8{\%} vs. 79.4{\%}. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias."
L16-1723,What a Nerd! Beating Students and Vector Cosine in the {ESL} and {TOEFL} Datasets,2016,4,6,2,1,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we claim that Vector Cosine â which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models â can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that â independently of the adopted parameters â outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50{\%}) and several state-of-the-art approaches."
D16-1099,The Effects of Data Size and Frequency Range on Distributional Semantic Models,2016,12,10,2,0,2700,magnus sahlgren,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1205,Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity,2016,22,1,3,1,180,emmanuele chersoni,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus."
W15-4208,{EVAL}ution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models,2015,28,37,3,1,181,enrico santus,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1. An extension in RDF format, including also scripts for data processing, is under development."
Y14-1018,Taking Antonymy Mask off in Vector Space,2014,36,13,3,1,181,enrico santus,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Automatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms. In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis."
W14-0406,The {PAIS{\\`A}} Corpus of {I}talian Web Texts,2014,-1,-1,8,0,3005,verena lyding,Proceedings of the 9th Web as Corpus Workshop ({W}a{C}-9),0,None
sprugnoli-lenci-2014-crowdsourcing,Crowdsourcing for the identification of event nominals: an experiment,2014,34,1,2,0,16573,rachele sprugnoli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents the design and results of a crowdsourcing experiment on the recognition of Italian event nominals. The aim of the experiment was to assess the feasibility of crowdsourcing methods for a complex semantic task such as distinguishing the eventive interpretation of polysemous nominals taking into consideration various types of syntagmatic cues. Details on the theoretical background and on the experiment set up are provided together with the final results in terms of accuracy and inter-annotator agreement. These results are compared with the ones obtained by expert annotators on the same task. The low values in accuracy and FleissÂ kappa of the crowdsourcing experiment demonstrate that crowdsourcing is not always optimal for complex linguistic tasks. On the other hand, the use of non-expert contributors allows to understand what are the most ambiguous patterns of polysemy and the most useful syntagmatic cues to be used to identify the eventive reading of nominals."
lebani-etal-2014-bootstrapping,Bootstrapping an {I}talian {V}erb{N}et: data-driven analysis of verb alternations,2014,25,0,3,1,33488,gianluca lebani,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The goal of this paper is to propose a classification of the syntactic alternations admitted by the most frequent Italian verbs. The data-driven two-steps procedure exploited and the structure of the identified classes of alternations are presented in depth and discussed. Even if this classification has been developed with a practical application in mind, namely the semi-automatic building of a VerbNet-like lexicon for Italian verbs, partly following the methodology proposed in the context of the VerbNet project, its availability may have a positive impact on several related research topics and Natural Language Processing tasks"
romeo-etal-2014-choosing,Choosing which to use? A study of distributional models for nominal lexical semantic classification,2014,24,3,4,0,39664,lauren romeo,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper empirically evaluates the performances of different state-of-the-art distributional models in a nominal lexical semantic classification task. We consider models that exploit various types of distributional features, which thereby provide different representations of nominal behavior in context. The experiments presented in this work demonstrate the advantages and disadvantages of each model considered. This analysis also considers a combined strategy that we found to be capable of leveraging the bottlenecks of each model, especially when large robust data is not available."
E14-4008,Chasing Hypernyms in Vector Spaces with Entropy,2014,20,58,2,1,181,enrico santus,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"In this paper, we introduce SLQS , a new entropy-based measure for the unsupervised identification of hypernymy and its directionality in Distributional Semantic Models (DSMs). SLQS is assessed through two tasks: (i.) identifying the hypernym in hyponym-hypernym pairs, and (ii.) discriminating hypernymy among various semantic relations. In both tasks, SLQS outperforms other state-of-the-art measures."
W13-0604,The Curious Case of Metonymic Verbs: A Distributional Characterization,2013,21,5,2,0,31309,jason utt,Proceedings of the {IWCS} 2013 Workshop Towards a Formal Distributional Semantics,0,"Logical metonymy combines an event-selecting verb with an entity-denoting noun (e.g., The writer began the novel), triggering a covert event interpretation (e.g., reading, writing). Experimental investigations of logical metonymy must assume a binary distinction between metonymic (i.e. eventselecting) verbs and non-metonymic verbs to establish a control condition. However, this binary distinction (whether a verb is metonymic or not) is mostly made on intuitive grounds, which introduces a potential confounding factor. We describe a corpus-based approach which characterizes verbs in terms of their behavior at the syntax-semantics interface. The model assesses the extent to which transitive verbs prefer event-denoting objects over entity-denoting objects. We then test this xe2x80x9ceventhoodxe2x80x9d measure on psycholinguistic datasets, showing that it can distinguish not only metonymic from non-metonymic verbs, but that it can also capture more fine-grained distinctions among different classes of metonymic verbs, putting such distinctions into a new graded perspective."
W13-0216,"Fitting, Not Clashing! A Distributional Semantic Model of Logical Metonymy",2013,21,5,2,0.833333,2924,alessandra zarcone,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"Logical metonymy interpretation (e.g. begin the book xe2x86x92 writing) has received wide attention in linguistics. Experimental results have shown higher processing costs for metonymic conditions compared with non-metonymic ones (read the book). According to a widely held interpretation, it is the type clash between the event-selecting verb and the entity-denoting object (begin the book) that triggers coercion mechanisms and leads to additional processing effort. We propose an alternative explanation and argue that the extra processing effort is an effect of thematic fit. This is a more economical hypothesis that does not need to postulate a separate type clash mechanism: entitydenoting objects simply have a low fit as objects of event-selecting verbs. We test linguistic datasets from psycholinguistic experiments and find that a structured distributional model of thematic fit, which does not encode any explicit argument type information, is able to replicate all significant experimental findings. This result provides evidence for a graded account of coercion phenomena in which thematic fit accounts for both the trigger of the coercion and the retrieval of the covert event."
W12-0908,Unseen features. Collecting semantic data from congenital blind subjects,2012,0,0,1,1,928,alessandro lenci,Proceedings of the Workshop on Computational Models of Language Acquisition and Loss,0,"Congenital blind subjects are able to learn how to use color terms and other types of vision-related words in a way that is de facto undistinguishable from sighted people. It has actually been proposed that language provides a rich source of information that blind subjects can exploit to acquire aspects of word meaning that are related to visual experience, such as the color of fruits or animals. Despite this, whether and how sensory deprivation affects the structure of semantic representations is still an open question. In this talk, we present a new, freely available collection of feature norms produced by congenital blind subjects and normal sighted people. Subjects were asked to produce semantic features describing the meaning of concrete and abstract nouns and verbs. Data were collected from Italian subjects, translated into English, and categorized with respect to their semantic type (e.g. hypernym, meronym, physical property, etc.). First analyses of the feature norms highlight important differences between blind and sighted subjects, for instance for the role of color and other visual features in the produced semantic descriptions. This resource can provide new evidence on the role of perceptual experience in shaping concepts, as well as on its interplay with information extracted from linguistic data. The norms will also be used to carry out computational experiments with distributional semantic models to simulate blind and sighted semantic spaces."
S12-1012,Identifying hypernyms in distributional semantic spaces,2012,15,81,1,1,928,alessandro lenci,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,In this paper we apply existing directional similarity measures to identify hypernyms with a state-of-the-art distributional semantic model. We also propose a new directional measure that achieves the best performance in hypernym identification.
lenci-etal-2012-lexit,{L}ex{I}t: A Computational Resource on {I}talian Argument Structure,2012,18,13,1,1,928,alessandro lenci,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The aim of this paper is to introduce LexIt, a computational framework for the automatic acquisition and exploration of distributional information about Italian verbs, nouns and adjectives, freely available through a web interface at the address http://sesia.humnet.unipi.it/lexit. LexIt is the first large-scale resource for Italian in which subcategorization and semantic selection properties are characterized fully on distributional ground: in the paper we describe both the process of data extraction and the evaluation of the subcategorization frames extracted with LexIt."
lenci-etal-2012-enriching,Enriching the {ISST}-{TANL} Corpus with Semantic Frames,2012,16,4,1,1,928,alessandro lenci,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The paper describes the design and the results of a manual annotation methodology devoted to enrich the ISST--TANL Corpus, derived from the Italian Syntactic--Semantic Treebank (ISST), with Semantic Frames information. The main issues encountered in applying the English FrameNet annotation criteria to a corpus of Italian language are discussed together with the choice of anchoring the semantic annotation layer to the underlying dependency syntactic structure. The results of a case study aimed at extending and specialising this methodology for the annotation of a corpus of legislative texts are also discussed."
W11-2501,How we {BLESS}ed distributional semantic evaluation,2011,18,153,2,0.411266,12129,marco baroni,Proceedings of the {GEMS} 2011 Workshop on {GE}ometrical Models of Natural Language Semantics,0,"We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models."
W11-0607,Composing and Updating Verb Argument Expectations: A Distributional Semantic Model,2011,24,27,1,1,928,alessandro lenci,Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,0,"The aim of this paper is to present a computational model of the dynamic composition and update of verb argument expectations using Distributional Memory, a state-of-the-art framework for distributional semantics. The experimental results conducted on psycholinguistic data sets show that the model is able to successfully predict the changes on the patient argument thematic fit produced by different types of verb agents."
bosco-etal-2010-comparing,Comparing the Influence of Different Treebank Annotations on Dependency Parsing,2010,11,16,6,0,17906,cristina bosco,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"As the interest of the NLP community grows to develop several treebanks also for languages other than English, we observe efforts towards evaluating the impact of different annotation strategies used to represent particular languages or with reference to particular tasks. This paper contributes to the debate on the influence of resources used for the training and development on the performance of parsing systems. It presents a comparative analysis of the results achieved by three different dependency parsers developed and tested with respect to two treebanks for the Italian language, namely TUT and ISST--TANL, which differ significantly at the level of both corpus composition and adopted dependency representations."
attardi-etal-2010-resource,A Resource and Tool for Super-sense Tagging of {I}talian Texts,2010,16,10,4,0,5833,giuseppe attardi,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"A SuperSense Tagger is a tool for the automatic analysis of texts that associates to each noun, verb, adjective and adverb a semantic category within a general taxonomy. The developed tagger, based on a statistical model (Maximum Entropy), required the creation of an Italian annotated corpus, to be used as a training set, and the improvement of various existing tools. The obtained results significantly improved the current state-of-the art for this particular task."
lenci-etal-2010-building,Building an {I}talian {F}rame{N}et through Semi-automatic Corpus Analysis,2010,12,9,1,1,928,alessandro lenci,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"n this paper, we outline the methodology we adopted to develop a FrameNet for Italian. The main element of novelty with respect to the original FrameNet is represented by the fact that the creation and annotation of Lexical Units is strictly grounded in distributional information (statistical distribution of verbal subcategorization frames, lexical and semantic preferences of each frame) automatically acquired from a large, dependency-parsed corpus. We claim that this approach allows us to overcome some of the shortcomings of the classical lexicographic method used to create FrameNet, by complementing the accuracy of manual annotation with the robustness of data on the global distributional patterns of a verb. In the paper, we describe our method for extracting distributional data from the corpus and the way we used it for the encoding and annotation of LUs. The long-term goal of our project is to create an electronic lexicon for Italian similar to the original English FrameNet. For the moment, we have developed a database of syntactic valences that will be made freely accessible via a web interface. This represents an autonomous resource besides the FrameNet lexicon, of which we have a beginning nucleus consisting of 791 annotated sentences."
poesio-etal-2010-babyexp,{B}aby{E}xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do,2010,15,1,4,0,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions."
J10-4006,Distributional Memory: A General Framework for Corpus-Based Semantics,2010,115,402,2,0.471206,12129,marco baroni,Computational Linguistics,0,"Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this one task, one model approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
W09-0201,"One Distributional Memory, Many Semantic Spaces",2009,20,25,2,0.475097,12129,marco baroni,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks."
zarcone-lenci-2008-computational,Computational Models for Event Type Classification in Context,2008,14,15,2,0.833333,2924,alessandra zarcone,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Verb lexical semantic properties are only one of the factors that contribute to the determination of the event type expressed by a sentence, which is instead the result of a complex interplay between the verb meaning and its linguistic context. We report on two computational models for the automatic identification of event type in Italian. Both models use linguistically-motivated features extracted from Italian corpora. The main goal of our experiments is to evaluate the contribution of different types of linguistic indicators to identify the event type of a sentence, as well as to model various cases of context-driven event type shift. In the first model, event type identification has been modelled as a supervised classification task, performed with Maximum Entropy classifiers. In the second model, Self-Organizing Maps have been used to define and identify event types in an unsupervised way. The interaction of various contextual factors in determining the event type expressed by a sentence makes event type identification a highly challenging task. Computational models can help us to shed new light on the real structure of event type classes as well as to gain a better understanding of context-driven semantic shifts."
lenci-etal-2008-unsupervised,Unsupervised Acquisition of Verb Subcategorization Frames from Shallow-Parsed Corpora,2008,21,13,1,1,928,alessandro lenci,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we reported experiments of unsupervised automatic acquisition of Italian and English verb subcategorization frames (SCFs) from general and domain corpora. The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs. Although preliminary, reported results are in line with state-of-the-art lexical acquisition systems. The issue of whether verbs sharing similar SCFs distributions happen to share similar semantic properties as well was also explored by clustering verbs that share frames with the same distribution using the Minimum Description Length Principle (MDL). First experiments in this direction were carried out on Italian verbs with encouraging results."
W07-0607,{ISA} meets {L}ara: An incremental word space model for cognitively plausible simulations of semantic learning,2007,13,19,2,0.475097,12129,marco baroni,Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,0,"We introduce Incremental Semantic Analysis, a fully incremental word space model, and we test it on longitudinal child-directed speech data. On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique. In addition, the model has interesting properties that might also be characteristic of the semantic space of children."
W06-0604,Probing the Space of Grammatical Variation: Induction of Cross-Lingual Grammatical Constraints from Treebanks,2006,11,2,2,1,6236,felice dellorletta,Proceedings of the Workshop on Frontiers in Linguistically Annotated Corpora 2006,0,"The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work uses a Maximum Entropy model of stochastic resolution of conflicting grammatical constraints and is demonstrably capable of putting explanatory theoretical accounts to the test of usage-based empirical verification."
dellorletta-etal-2006-searching,Searching treebanks for functional constraints: cross-lingual experiments in grammatical relation assignment,2006,12,0,2,1,6236,felice dellorletta,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The paper reports on a detailed quantitative analysis of distributional language data of both Italian and Czech, highlighting the relative contribution of a number of distributed grammatical factors to sentence-based identification of subjects and direct objects. The work is based on a Maximum Entropy model of stochastic resolution of grammatical conflicting constraints, and is demonstrably capable of putting explanatory theoretical accounts to the challenging test of an extensive, usage-based empirical verification."
bartolini-etal-2006-creation,Creation and Use of Lexicons and Ontologies for {NL} Interfaces to Databases,2006,14,7,4,1,29604,roberto bartolini,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper we present an original approach to natural language query interpretation which has been implemented withinthe FuLL (Fuzzy Logic and Language) Italian project of BC S.r.l. In particular, we discuss here the creation of linguisticand ontological resources, together with the exploitation of existing ones, for natural language-driven database access andretrieval. Both the database and the queries we experiment with are Italian, but the methodology we broach naturally extends to other languages."
W05-0509,Climbing the Path to Grammar: A Maximum Entropy Model of Subject/Object Learning,2005,21,14,2,1,6236,felice dellorletta,Proceedings of the Workshop on Psychocomputational Models of Human Language Acquisition,0,"In this paper, we discuss an application of Maximum Entropy to modeling the acquisition of subject and object processing in Italian. The model is able to learn from corpus data a set of experimentally and theoretically well-motivated linguistic constraints, as well as their relative salience in Italian grammar development and processing. The model is also shown to acquire robust syntactic generalizations by relying on the evidence provided by a small number of high token frequency verbs only. These results are consistent with current research focusing on the role of high frequency verbs in allowing children to converge on the most salient constraints in the grammar."
declerck-etal-2004-towards,Towards a Language Infrastructure for the Semantic Web,2004,8,8,4,0,2109,thierry declerck,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In recent years, the Internet evolved from a global medium for information exchange (directed mainly towards human users) into a xe2x80x9dglobal, virtual work environmentxe2x80x9d (for both human users and machines). Building on the world-wide-web, developments such as grid technology, web services and the semantic web contributed to this transformation, the implications of which are now slowly but clearly being integrated into all areas of the new digital society (e-business, e-government, e-science, etc.) In this conctext the semantic web allows for increasingly intelligent and therefore autonomous processing. This development brings new challenges for Human Language Technology (HLT), which require not only some adaptation of processes within the state of the art processing chain of HLT, but also changes at the infrastructure level of HLT resources."
calzolari-etal-2004-enabler,"{ENABLER} Thematic Network of National Projects: Technical, Strategic and Political Issues of {LR}s",2004,2,6,7,0,18003,nicoletta calzolari,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present general strategies concerning Language Resources (LRs) xe2x80x93 Written, Spoken and, recently, Multimodal xe2x80x93 as developed within the ENABLER Thematic Network. LRs are a central component of the so-called xe2x80x9clinguistic infrastructurexe2x80x9d (the other key element being Evaluation), necessary for the development of any Human Language Technology (HLT) application. They play a critical role, as horizontal technology, in different emerging areas of FP6, and have been recognized as a priority within a number of national projects around Europe and world-wide. The availability of LRs is also a xe2x80x9csensitivexe2x80x9d issue, touching directly the sphere of linguistic and cultural identity, but also with economical, societal and political implications. This is going to be even more true in the new Europe with 25 languages on a par. Introduction After considering the strategic and infrastructural role of Language Resources (LRs) within any Human Language Technology (HLT) application (section 1), we focus on the main issues discussed within the ENABLER (European National Activities for Basic Language Resources) Thematic Network (section 2): the survey of LRs, interoperability and multilinguality, open access to LRs, validation methodologies for LRs, industrial and basic requirements. Finally, few recommendations are provided towards the design of general strategies and an overall coordination for the field of LRs (section 3). 1. The Strategic Role of LRs Language Resources (LRs) xe2x80x93 Written, Spoken and, recently, Multimodal xe2x80x93 are a central and strategic component of the so-called xe2x80x9clinguistic infrastructurexe2x80x9d (the other key element being Evaluation), necessary for the development of any Human Language Technology (HLT) application and product. The availability of adequate LRs for as many languages as possible is a pre-requisite for the development of a truly multilingual Information Society. They play a critical role, as horizontal technology, in different areas of the 6 Framework Programme, and have been recognized as a priority within a number of national projects around Europe. The availability of LRs is also a xe2x80x9csensitivexe2x80x9d issue, touching directly the sphere of linguistic and cultural identity, but also with economical, societal and political implications. This is going to be even more true in the new Europe with 25 languages on a par. The ENABLER Thematic Network of HLT National Projects in European countries xe2x80x93 an EC funded IST project, designed and started by Antonio Zampolli, with a clear strategic vision for the field of LRs xe2x80x93 is the first broad European initiative which has the mission of explicitly considering together the technical, organizational, strategic and political issues of LRs. In ENABLER these various aspects are put together in a coherent framework, to set up mediumand long-term set of priorities (both technical and strategic) and to promote these at the national and international levels. Moreover, ENABLER has recognized the importance to promote actions aiming at integrating the different resource types, until now developed independently, and xe2x80x93 as a consequence xe2x80x93 at promoting the cooperation between the communities of Speech, Text and Multimodality. In the following we briefly highlight the main issues tackled by ENABLER on the different layers. 2. The Main Issues 2.1. The Survey of LRs The ENABLER Consortium conducted the Survey of LRs to get a global picture of the situation on LRs, in order to be able to compare the various conditions that hold across different languages and xe2x80x93 on this basis xe2x80x93 to suggest more"
bartolini-etal-2004-semantic,Semantic Mark-up of {I}talian Legal Texts Through {NLP}-based Techniques,2004,4,6,2,1,29604,roberto bartolini,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we illustrate an approach to information extraction from legal texts using SALEM. SALEM is an NLP architecture for semantic annotation and indexing of Italian legislative texts, developed by ILC in close collaboration with ITTIG-CNR, Florence. Results of SALEM performance on a test sample of about 500 Italian law paragraphs are provided."
bartolini-etal-2004-hybrid,Hybrid Constraints for Robust Parsing: First Experiments and Evaluation,2004,5,12,2,1,29604,roberto bartolini,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we present IDEAL, a parsing architecture for Italian, which pursues the goal of pairing robustness with deep linguistic analysis by extending a shallow processing kernel with a pool of hybrid constraints for the incremental identification of grammatical relations. The parsing output takes the form of dependency structures representing the full range of instantiated functional relations (e.g. subject, object, modifier, complement, etc.). The paper focuses on nature and interaction of the battery of hybrid constraints and evaluates their joint impact against a gold standard of more than 700 manually annotated sentences."
bertagna-etal-2004-content,Content Interoperability of Lexical Resources: Open Issues and {``}{MILE}{''} Perspectives,2004,4,13,2,0.740741,48220,francesca bertagna,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The paper tackles the issue of content interoperability among lexical resources, by presenting an experiment of mapping differently conceived lexicons, FrameNet and NOMLEX, onto MILE (Multilingual ISLE Lexical Entry), a meta-entry for the encoding of multilingual lexical information, acting as a general schema of shared and common lexical objects. The aim is to (i) raise problems and (ii) test the expressive potentialities of MILE as a standard environment for Computational Lexicons."
W03-1905,{RDF} Instantiation of {ISLE}/{MILE} Lexical Entries,2003,1,15,2,0,16303,nancy ide,Proceedings of the {ACL} 2003 Workshop on Linguistic Annotation: Getting the Model Right,0,"In this paper we describe the overall model for MILE lexical entries and provide an instantiation of the model in RDF/OWL. This work has been done with an eye toward the goal of creating a web-based registry of lexical data categories and enabling the description of lexical information by establishing relations among them, and/or using predefined objects that may reside at various locations on the web. It is also assumed that using OWL specifications to enhance specifications of the ontology of lexical objects will eventually enable the exploitation of inferencing engines to retrieve and possibly create lexical information on the fly, as suited to particular contexts. As such, the model and RDF instantiation provided here are in line with the goals of ISO TC37 SC4, and should be fully mappable to the proposed pivot."
W02-1501,"Grammar and Lexicon in the Robust Parsing of {I}talian towards a Non-Na{\\\\\i}ve Interplay""",2002,15,12,2,1,29604,roberto bartolini,{COLING}-02: Grammar Engineering and Evaluation,0,In the paper we report a qualitative evaluation of the performance of a dependency analyser of Italian that runs in both a non-lexicalised and a lexicalised mode. Results shed light on the contribution of types of lexical information to parsing.
W02-1204,Broadening the Scope of the {EAGLES}/{ISLE} Lexical Standardization Initiative,2002,8,6,2,0,18003,nicoletta calzolari,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,"ISLE is a continuation of the long standing EAGLES initiative and it is supported by EC and NSF under the Human Language Technology (HLT) programme. Its objective is to develop widely agreed and urgently demanded standards and guidelines for infrastructural language resources, tools, and HLT products. EAGLES itself is a well-known trademark and point of reference for HLT projects and products and its previous results have already become de facto widely adopted standards. Multilingual computational lexicons, natural interaction and multimodality, and evaluation are the three areas targeted by ISLE. In the first section of the paper we describe the overall goals and methodology of EAGLES/ISLE, in the second section we focus on the work of the Computational Lexicon Working Group, introducing its work strategy and the preliminary guidelines of a standard framework for multilingual computational lexicons, based on a general schema for the Multilingual ISLE Lexical Entry (MILE)."
atkins-etal-2002-resources,From Resources to Applications. Designing the Multilingual {ISLE} Lexical Entry,2002,4,10,8,0,53506,sue atkins,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
calzolari-etal-2002-towards,Towards Best Practice for Multiword Expressions in Computational Lexicons,2002,6,99,5,0,18003,nicoletta calzolari,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The importance a nd role of multi-word expressions (MWE) in the description and p rocessing o f natural l anguage has been long recognized. However, multi-word information has often been relegated to the marginal role of idiosyncratic lexical information. The need for MWE lexicons grows even more acute for multi-lingual applications, for which (sometimes complex) correspondences must be identified, classified, and recorded. Within the XMELLT and ISLE projects we have started to investigate the potential to develop multi-lingual, multi-word expression lexicons incorporating both syntactic and semantic information. We aim at specifying means to acquire and represent multi-word lexical entries for multiple languages, and establishing uniform (or inter-translatable) standards for describing multi-word lexical entries. We explored theoretical approaches used in large lexicon-building projects, in p articular FrameNet and SIMPLE. They constitute interesting frameworks for the explicit syntactic and semantic representation of MWEs, due mainly to their ability to capture semantic multidimensionality, through frame e lements and qualia relations respectively. We a lso developed an abstract data model for lexical information together with a representation in XML for it. Our goal is to define a set of minimal lexicon xe2x80x9cobjectsxe2x80x9d, which can serve not only as a model for MWEs but also for lexical data in general."
lenci-etal-2002-multilingual,Multilingual Summarization by Integrating Linguistic Resources in the {MLIS}-{MUSI} Project,2002,9,19,1,1,928,alessandro lenci,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
bartolini-etal-2002-lexicon,The Lexicon-Grammar Balance in Robust Parsing of {I}talian,2002,5,7,2,1,29604,roberto bartolini,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"What is the role of lexical information in robust parsing of unrestricted texts? In this paper we provide experimental evidence showing that, in order to strike the balance between robustness and coverage needed for practical NLP applications, judicious use of positive lexical evidence given a text should be complemented with a battery of dynamic parsing strategies aimed at solving local constraint conflicts. Likewise, negative lexical evidence should not blindly override grammatical information. Unlike fully lexicalised approaches to parsing where cross-categorial constraints on lexicon usage apply freely, optimal results can be obtained by modulating the way subcategorisation information is brought to bear in identifying dependency relations in context."
W01-1507,International Standards for Multilingual Resource Sharing: The {ISLE} Computational Lexicon Working Group,2001,11,7,2,0,18003,nicoletta calzolari,Proceedings of the {ACL} 2001 Workshop on Sharing Tools and Resources,0,"The ISLE project is a continuation of the long standing EAGLES initiative, carried out under the Human Language Technology (HLT) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation, supported by NSF and EC. We concentrate in this paper on the current position of the ISLE Computational Lexicon Working Group. We provide a short description of the EU SIMPLE lexicons built on the basis of previous EAGLES recommendations. We then point at a few basic methodological principles applied in previous EAGLES phases, and describe a few principles to be followed in the definition of a Multilingual ISLE Lexical Entry (MILE)."
2001.mtsummit-papers.13,The {ISLE} in the ocean. Transatlantic standards for multilingual lexicons (with an eye to machine translation),2001,-1,-1,2,0,18003,nicoletta calzolari,Proceedings of Machine Translation Summit VIII,0,"The ISLE project is a continuation of the long standing EAGLES initiative, carried out under the Human Language Technology (HLT) programme in collaboration between American and European groups in the framework of the EU-US International Research Co-operation, supported by NSF and EC. In this paper we concentrate on the current position of the ISLE Computational Lexicon Working Group (CLWG), whose activities aim at defining a general schema for a multilingual lexical entry (MILE), as the basis for a standard framework for multilingual computational lexicons. The needs and features of existing Machine Translation systems provide the main reference points for the process of consensual definition of the MILE. The overall structure of the MILE will be illustrated with particular attention to some of the issues raised for multilingual lexicons by the need of expressing complex transfer conditions among translation equivalents"
bel-etal-2000-simple,{SIMPLE}: A General Framework for the Development of Multilingual Lexicons,2000,7,156,5,0.952381,17513,nuria bel,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The project LE-SIMPLE is an innovative attempt of building harmonized syntactic-semantic lexicons for twelve European languages, aimed at use in different Human Language Technology applications. SIMPLE provides a general design model for the encoding of a large amount of semantic information, spanning from ontological typing, to argument structure and terminology. SIMPLE thus provides a general framework for resource development, where state-of-the-art results in lexical semantics are coupled with the needs of Language Engineering applications accessing semantic information."
villegas-etal-2000-multilingual,Multilingual Linguistic Resources: From Monolingual Lexicons to Bilingual Interrelated Lexicons,2000,3,2,3,0,8512,marta villegas,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper describes a procedure to convert the PAROLE-SIMPLE monolingual lexicons into bilingual interrelated lexicons where each word sense of a given language is linked to the pertinent sense of the right words in one or more target lexicons. Nowadays, SIMPLE lexicons are monolingual although the ultimate goal of these harmonised monolingual lexicons is to build multilingual lexical resources. For achieving this goal it is necessary to automatise the linking among the different senses of the different monolingual lexicons, as the production of such multilingual relations by hand will be, as all tasks related with the development of linguistic resources, unaffordable in terms of human resources and time spent. The system we describe in this paper takes advantage of the SIMPLE model and the SIMPLE based lexicons so that, in the best case, it can find fully automatically the relevant sense-to-sense correspondences for determining the translational equivalence of two words in two different languages and, in the worst case, it will be able to narrow the set of admissible links between words and relevant senses. This paper also explores to what extent semantic encoding in already existing computational lexicons such as SIMPLE can help in overcoming the problems arisen when using monolingual meaning descriptions for bilingual links and aims to set the basis for defining a model for adding a bilingual layer to the SIMPLE model. This bilingual layer based on a bilingual relation model will be the basis indeed for defining the multilingual language resource we want PAROLE-SIMPLE lexicons to become."
lenci-etal-2000-opposites,Where Opposites Meet. A Syntactic Meta-scheme for Corpus Annotation and Parsing Evaluation,2000,15,12,1,1,928,alessandro lenci,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The paper describes the use of FAME, a functional annotation metaxe2x80x93scheme for comparison and evaluation of syntactic annotation schemes, i) as a flexible yardstick in multixe2x80x93lingual and multixe2x80x93modal parser evaluation campaigns and ii) for corpus annotation. We show that FAME complies with a variety of nonxe2x80x93trivial methodological requirements, and has the potential for being effectively used as an xe2x80x9cinterlinguaxe2x80x9d between different syntactic representation formats. 1. Motivation and background"
W99-0407,{FAME}: a Functional Annotation Meta-scheme for multi-modal and multi-lingual Parsing Evaluation,1999,14,9,1,1,928,alessandro lenci,Computer Mediated Language Assessment and Evaluation in Natural Language Processing,0,"The paper describes FAME, a functional annotation meta-scheme for comparison and evaluation of existing syntactic annotation schemes, intended to be used as a flexible yardstick in multi-lingual and multi-modal parser evaluation campaigns. We show that FAME complies with a variety of non-trivial methodological requirements, and has the potential for being effectively used as an interlingua between different syntactic representation formats."
