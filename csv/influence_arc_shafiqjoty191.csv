2015.mtsummit-papers.10,abdelali-etal-2014-amara,1,0.856126,"a standard task of translating German-to-English and Arabic-to-English IWSLT TED talks, we observed statistically significant improvements of up to +0.9 BLEU points. 1 Introduction Parallel data required to train Statistical Machine Translation (SMT) systems is often inadequate, and is typically collected opportunistically from wherever it is available. The conventional wisdom is that more data improves the translation quality. Additional data however, may not be best suited for tasks such as translating TED talks (Cettolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution i"
2015.mtsummit-papers.10,D11-1033,0,0.702839,"ni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improvements of +0.4 BLEU points for DE-EN and +0.5 for AR-EN. • Linear interpolation for NNJM models was slightly behind its log-linear variant. Data Selection: • OSM-based selection performed better for AR-EN task giving an average improvement of +0.7 • NNJM performed better at the DE-EN task giving an average improvement of +0.6 points. • Both OSM- and NNJM-based selection gave slightly better results than Modified-MooreLewis (MML) selection (Axelrod et al., 2011). The rest of the paper is organized as follows. Section 2 briefly describes the OSM and the NNJM models. Section 3 describes mixture model and data selection techniques that we apply using the OSM and the NNJM models to carry out adaptation. Section 4 presents the results. Section 5 discusses related work and Section 6 concludes the paper. 2 Joint Sequence Models In this section, we revisit Operation Sequence and Neural Network Joint models briefly. 2.1 Operation Sequence Model The Operation Sequence Model (OSM) is a bilingual model that couples translation and reordering by representing them"
2015.mtsummit-papers.10,2014.iwslt-evaluation.6,1,0.864662,"52K 24K 32K 28K Table 2: Statistics of the German-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry an"
2015.mtsummit-papers.10,2011.iwslt-evaluation.18,0,0.186872,"ata, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but n"
2015.mtsummit-papers.10,P13-1141,0,0.232794,"Missing"
2015.mtsummit-papers.10,N13-1114,0,0.591829,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,P13-1126,0,0.590988,"EM-based weighting, (ii) using log-linear model inside the SMT pipeline. Secondly, we use cross-entropy difference (Moore and Lewis, 2010) between in- and out-domain models to perform data selection for domain adaptation. The bilingual property of the OSM and NNJM models gives them an edge over traditional LM-based methods, which do not capture source and target domain relevance jointly. The embedded reordering information modeled in OSM helps it to preserve reordering characteristic of the in-domain data. Capturing reordering variation across domains have been shown to be beneficial also by Chen et al. (2013a). NNJM adds a different dimension to it by semantically generalizing the data using distributed representation of words (Bengio et al., 2003). We evaluated our systems on a standard task of translating IWSLT TED talks for Germanto-English (DE-EN) and Arabic-to-English (AR-EN) language pairs. Below is a summary of our main findings: Model Weighting: • Linearly interpolating OSM models through EM-based weighting gave average BLEU (Papineni et al., 2002) improvements of up to +0.6 for DE-EN and +0.9 for AR-EN. • Log-linear variant performed better in the case of NNJM giving an average improveme"
2015.mtsummit-papers.10,N12-1047,0,0.0324171,"l. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-domain UN data 4 Training NNJM with backpropagation could be proh"
2015.mtsummit-papers.10,J81-4005,0,0.515198,"Missing"
2015.mtsummit-papers.10,P14-1129,0,0.128935,". Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features insid"
2015.mtsummit-papers.10,P13-2119,0,0.300031,"based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards th"
2015.mtsummit-papers.10,P13-2071,1,0.880717,"g a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language mod"
2015.mtsummit-papers.10,C14-1041,1,0.833548,"epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) based on EM-weighting shows significant improvements with average BLEU gains of +0.6 in DE-EN and +0.9 in AR-EN over the baseline system Bcat (see Table 3).6 One reason for better gains in AR-EN is the fact that the out-d"
2015.mtsummit-papers.10,P11-1105,1,0.931772,"y distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as additional language model (LM) features inside the SMT decoder. Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 117 The diversity of the two models, i.e., OSM with embedded reordering information and NNJM with continuous space modeling, makes them interesting to be explored for domain adaptation."
2015.mtsummit-papers.10,N13-1073,0,0.0478691,"arget). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT sy"
2015.mtsummit-papers.10,P12-2023,0,0.0681817,"adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data select"
2015.mtsummit-papers.10,W08-0334,0,0.165313,"l cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network traini"
2015.mtsummit-papers.10,D10-1044,0,0.19196,"ion model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixtu"
2015.mtsummit-papers.10,W07-0717,0,0.155727,"enges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates r"
2015.mtsummit-papers.10,W09-0439,0,0.123872,"filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain ada"
2015.mtsummit-papers.10,D08-1089,0,0.0222621,"r training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also trained on the concatenated data. Linear interpolation (OSMln ) ba"
2015.mtsummit-papers.10,P14-1066,0,0.0237562,"his allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional ("
2015.mtsummit-papers.10,E14-1035,0,0.0822255,"Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences"
2015.mtsummit-papers.10,W11-2123,0,0.0290447,"Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in baseline MT systems were also tr"
2015.mtsummit-papers.10,2005.eamt-1.19,1,0.837721,"be an effective way to discard poor quality or irrelevant training instances, which when included in the MT systems, hurts its performance. The idea is to score the out-domain data using model trained from the in-domain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather th"
2015.mtsummit-papers.10,C14-1182,0,0.626827,"Missing"
2015.mtsummit-papers.10,D15-1147,1,0.427467,"e out-domain data that is unknown to the in-domain OSM, gets high probability7 and is ranked higher in the search space. On the contrary, the same gets down-weighted in a linearly interpolated global model. Both linear and log-linear interpolation of the NNJM models showed improvements over the baseline system Bcat (refer to Table 4). Log-linear interpolation (NNJMlg ) performed slightly better in both cases. Notice that NNJMlg does not face the same problem as OSMlg because all NNJM models are trained using the in-domain vocabulary with a low probability assigned to the out-domain UNKs.8 See Joty et al. (2015) for more details on our novel handling 7 Due to probability mass assigned to UNK sequences. order to reduce the training time and to learn better word representations, neural models are trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk. This results in a large number of n-gram sequences containing at least one unk word and thereby, makes unk a highly probable word for the model. As a result of this discrepancy, sentences with more number of unk words will be selected. To solve this problem we created a separate class for"
2015.mtsummit-papers.10,D13-1176,0,0.0465297,"tion or reordering) decisions. This allows the model to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has"
2015.mtsummit-papers.10,P07-2045,0,0.00623946,"erman-English and Arabic-English training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in Millions. ep = Europarl, cc = Common Crawl, un = United Nations and an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Mo"
2015.mtsummit-papers.10,N12-1005,0,0.0261074,"t-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (ii) the Neural Network Joint Model or NNJM (Devlin et al., 2014) — a continuous space model that learns neural network over augmented streams of source and target sequences. Both models are used as a"
2015.mtsummit-papers.10,P14-2093,0,0.222498,"to reduce computational cost when training is expensive and also when memory is constrained. Data selection was earlier done for language modeling using information retrieval techniques (Hildebrand et al., 2005) and using perplexity measure (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source side and target side language models. Duh et al. (2013) used recurrent neural network language model instead of an ngram-based language model to do the same. Translation model features were used recently by Liu et al. (2014); Hoang and Sima’an (2014) to do data selection. 5.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases ra"
2015.mtsummit-papers.10,N13-1074,0,0.30113,"ain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et a"
2015.mtsummit-papers.10,J06-4004,0,0.0704009,"Missing"
2015.mtsummit-papers.10,C14-1105,0,0.264728,"pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code will be contributed to Mo"
2015.mtsummit-papers.10,D09-1074,0,0.345709,"tolo et al., 2014) or patents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore"
2015.mtsummit-papers.10,P10-2041,0,0.680232,"tents (Fujii et al., 2010) or educational content (Abdelali et al., 2014), and often come with the challenges of dealing with word-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) th"
2015.mtsummit-papers.10,D09-1141,0,0.0657375,"time consuming process. Therefore rather than filtering less useful data, an alternative way is to down-weight it and boost the data closer to the in-domain. It is robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013b) used vector space model for adaptation at phrase level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NN"
2015.mtsummit-papers.10,P02-1040,0,0.102972,"Missing"
2015.mtsummit-papers.10,C12-2104,0,0.064795,"el to learn very rich translation and reordering patterns. Moreover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valu"
2015.mtsummit-papers.10,I08-2089,0,0.0130472,"nd an output embedding layer of 750. Only one hidden layer is used with NCE4 to allow faster training and decoding. Training was done using mini-batch size of 1000 and using 100 noise samples. We train the out-domain NNJM models using the same vocabulary as the in-domain vocabulary. All models were trained for 25 epochs. Machine Translation Settings: We followed Birch et al. (2014) to train a Moses system Koehn et al. (2007) with the following settings: maximum sentence length of 80, Fast-Align (Dyer et al., 2013) for word-alignments, an interpolated Kneser-Ney smoothed 5-gram language model (Schwenk and Koehn, 2008) with KenLM (Heafield, 2011) for querying, lexicalized reordering (Galley and Manning, 2008) and other default parameters. We used Moses implementations of OSM and NNJM as a part of their respective baseline systems. Arabic OOVs were translated using an unsupervised transliteration module (Durrani et al., 2014b) in Moses. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning.5 4.1 Results: Model Weighting We first discuss the results of applying mixture modeling approach. The MT systems are trained on a concatenation of all in- and out-domain data. The OSM and NNJM models used in base"
2015.mtsummit-papers.10,E12-1055,0,0.662401,"ord-sense ambiguities and stylistic variance of other domains. When additional data, later referred as out-domain, is much larger than in-domain, the resultant distribution can get biased towards out-domain, yielding a sub-optimal system. Domain adaptation aims to preserve the identity of the in-domain data while using the best of the out-domain data. This is done by selecting a subset from the out-domain data, which is closer to the in-domain (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Bilingual sequence models (Mari˜no et al., 2006) have shown to be effective in improving the quality of machine translation and have achieved state-of-the-art performance recently (Le et al., 2012; Durrani et al., 2013; Devlin et al., 2014). Their ability to capture non-local dependencies makes them superior to the traditional phrase-based models, which do not consider contextual information across phrasal boundaries. Two such models that we explore in this paper are (i) the Operation Sequence Model or OSM (Durrani et al., 2011) — a markov translation model that integrates reordering, and (i"
2015.mtsummit-papers.10,P13-1082,0,0.207199,"se level. Every phrase pair is represented as a vector where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Joty et al. (2015) performed model weighting by regularizing the loss function towards the in-domain model directly inside neural network training. They also used NNJM model as their basis. Other work on domain adaptation includes but not limited to studies that focus on topic modeling (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation where no in-domain data is available (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion We targeted an unexplored area of using bilingual language models for domain adaptation. We applied model weighting and data selection techniques using OSM and NNJM models. Both methods were shown to be effective in the target translation tasks. Interpolating multi-domain models gave an average improvement of up to +0.9 BLEU points using OSM and +0.5 using NNJM. We also used NNJM and OSM models for data selection using differences in cross entropy and showed improvements of up to +0.6 BLEU points. The code wil"
2015.mtsummit-papers.10,P13-1045,0,0.034229,"reover, the model is based on minimal translation units (MTUs) and considers source and target contextual information across phrasal boundaries, thus addressing phrasal independence assumption and spurious segmentation problems in traditional phrase-based MT. 2.2 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to MT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013; Socher et al., 2013; Hinton et al., 2012). A bilingual Neural Network Joint model for MT was recently proposed by Devlin et al. (2014). It learns a feedforward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. Each input word in the context has a D dimensional (continuous-valued) Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami"
2015.mtsummit-papers.10,D13-1140,0,0.293,"models are trained by randomly selecting corpora of same size as that of the in-domain data. 4 Experiments Data: We used TED talks (Cettolo et al., 2014) as our in-domain corpus. For German-toEnglish (DE-EN), we used the data made available for WMT’14.2 This contains News, Europarl and Common Crawl as out-domain data. For Arabic-English (AR-EN), we used the UN corpus as out-domain data. We concatenated dev- and test-2010 for tuning and used test2011-2013 for evaluation. Table 2 shows the size of the training and test data used. NNJM Settings: The NNJM models were trained using NPLM3 toolkit (Vaswani et al., 2013) with the following settings. We used a target context of 5 words and an aligned source window of 9 words, forming a joint stream of 14-grams for training. We restricted source and target side vocabularies to 20K and 40K most frequent words. We used an input embedding layer of 150 2 http://www.statmt.org/wmt14/ 3 http://nlg.isi.edu/software/nplm/ Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 122 German-English Arabic-English Corpus Sent. TokDE TokEN Corpus Sent. TokAR TokEN iwslt news ep cc 177K 200K 1.9M 2.3M 3.3M 5.1M 48.7M 53.9M 3.5M 5.0M 51.0M 57"
2020.acl-main.263,S13-1004,0,0.0679212,"Missing"
2020.acl-main.263,D18-1316,0,0.182867,"in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left"
2020.acl-main.263,N19-3002,0,0.0256124,"bustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by"
2020.acl-main.263,P16-2096,0,0.0396823,"eness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP. It is crucial that NLP systems do not amplify and entrench social biases (Hovy and Spruit, 2016). Recent research on fairness has primarily focused on racial and gender biases within distributed word representations (Bolukbasi et al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model"
2020.acl-main.263,N18-1170,0,0.0348915,"l. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive survey of this topic. Adversarial training. In order to ensure our NLP systems are not left vulnerable to powerful attacks, most existing work make use of adversarial training to improve the model’s robustness (Goodfellow et al., 2015). This involves augmenting the training data either by adding the adversaries to or replacing the clean examples in the training set. Summary. Existing work"
2020.acl-main.263,D17-1215,0,0.0316738,"t al., 2016), coreference resolution (Rudinger et al., 2018), sentence encoders (May et al., 2019), and language models (Bordia and Bowman, 2019). However, we posit that there exists a significant potential for linguistic bias that has yet to be investigated, which is the motivation for our work. Adversarial attacks in NLP. First discovered in computer vision by Szegedy et al. (2014), adversarial examples are data points crafted with the intent of causing a model to output a wrong prediction. In NLP, this could take place at the character, morphological, lexical, syntactic, or semantic level. Jia and Liang (2017) showed that question answering models could be misled into choosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to"
2020.acl-main.263,N19-1063,0,0.0496558,"g performance on clean data.1 1 Introduction In recent years, Natural Language Processing (NLP) systems have gotten increasingly better at learning complex patterns in language by pretraining large language models like BERT, GPT-2, and CTRL (Devlin et al., 2019; Radford et al., 2019; Keskar et al., 2019), and fine-tuning them on taskspecific data to achieve state of the art results has become a norm. However, deep learning models are only as good as the data they are trained on. Existing work on societal bias in NLP primarily focuses on attributes like race and gender (Bolukbasi et al., 2016; May et al., 2019). In contrast, we investigate a uniquely NLP attribute that has been largely ignored: linguistic background. Current NLP models seem to be trained with the implicit assumption that everyone speaks fluent (often U.S.) Standard English, even though twothirds (&gt;700 million) of the English speakers in the world speak it as a second language (L2) (Eberhard et al., 2019). Even among native speakers, a significant number speak a dialect like African American Vernacular English (AAVE) rather than Standard English (Crystal, 2003). In addition, these 1 Code and adversarially fine-tuned models available"
2020.acl-main.263,N19-1314,0,0.136714,"hoosing a distractor sentence in the passage that was created by replacing key entities in the correct answer sentence. Belinkov and Bisk (2018) followed by demonstrating the brittleness of neural machine translation systems against character-level perturbations like randomly swapping/replacing characters. However, these attacks are not optimized on the target models, unlike Ebrahimi et al. (2018), which makes use of the target model’s gradient to find the character change that maximizes the model’s error. Since these attacks tend to disrupt the sentence’s semantics, Ribeiro et al. (2018) and Michel et al. (2019) propose searching for adversaries that preserve semantic content. Alzantot et al. (2018) and Jin et al. (2019) explore the use of synonym substitution to create adversarial examples, using word embeddings to find the n nearest words. Eger et al. (2019) take a different approach, arguing that adding visual noise to characters leaves their semantic content undisturbed. Iyyer et al. (2018) propose to create paraphrase adversaries by conditioning their generation on a syntactic template, while Zhang et al. (2019b) swap key entities in the sentences. Zhang et al. (2019a) provide a comprehensive su"
2020.acl-main.263,N19-4009,0,0.0397215,"Missing"
2020.acl-main.263,W18-6301,0,0.0191745,"ed solution. To solve this problem, we propose M ORPHEUS (Algorithm 1), an approach that greedily searches for the inflectional form of each noun, verb, or adjective in x that maximally increases f ’s loss (Eq. 1). For each token in x, M ORPHEUS calls M AX I NFLECTED to find the inflected form that caused the greatest increase in f ’s loss.3 Table 1 presents some adversarial examples obtained by running M ORPHEUS on state-of-theart machine reading comprehension and translation models: namely, BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2019), and Transformer-big (Vaswani et al., 2017; Ott et al., 2018). 3 A task-specific evaluation metric may be used instead of the loss in situations where it is unavailable. However, as we discuss later, the choice of metric is important for optimal performance and should be chosen wisely. Require: Original instance x, Label y, Model f Ensure: Adversarial example x0 T ← TOKENIZE(x) for all i = 1, . . . , |T |do if POS(Ti ) ∈ {NOUN, VERB, ADJ} then I ← G ET I NFLECTIONS(Ti ) Ti ← M AX I NFLECTED(I, T, y, f ) end if end for x0 ← DETOKENIZE(T ) return x0 There are two possible approaches to implementing M AX I NFLECTED: one is to modify each token independentl"
2020.acl-main.263,P02-1040,0,0.108717,"Missing"
2020.acl-main.263,N18-1202,0,0.0583613,"Missing"
2020.acl-main.263,W15-3049,0,0.0418897,"Missing"
2020.acl-main.263,W18-6319,0,0.0326363,"Missing"
2020.acl-main.263,P18-2124,0,0.0425504,"Missing"
2020.acl-main.263,D16-1264,0,0.016695,"cts each eligible word in each original example. Measures. In addition to the raw scores, we also report the relative decrease for easier comparison across models since they perform differently on the clean dataset. Relative decrease (dr ) is calculated using the following formula: dr = 4.1 scoreoriginal − scoreadversarial scoreoriginal (2) Extractive Question Answering Given a question and a passage containing spans corresponding to the correct answer, the model is expected to predict the span corresponding to the answer. Performance for this task is computed using exact match or average F1 (Rajpurkar et al., 2016). We evaluate the effectiveness of our attack using average F1 , which is more forgiving (for the target model). From our experiments, the exact match score is usually between 3-9 points lower than the average F1 score. SQuAD 1.1 and 2.0. The Stanford Question Answering Dataset (SQuAD) comprises over 100,000 question–answer pairs written by crowdworkers 5 https://github.com/bjascob/LemmInflect 2923 https://github.com/alvations/sacremoses Dataset Model Clean Random M ORPHEUS SQuAD 2.0 Answerable Questions (F1 ) GloVe-BiDAF ELMo-BiDAF BERTSQuAD 1.1 SpanBERTSQuAD 1.1 BERTSQuAD 2 SpanBERTSQuAD 2 7"
2020.acl-main.263,P18-1079,0,0.219105,"chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialectal examples. 2920 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2920–2935 c July 5 - 10, 2020. 2020 Association for Computational Linguistics • Propose M ORPHEUS, a method for generating plausible and semantically similar adversaries by perturbing the inflections in the clean examples (Figure 1). In contrast to recent work on adversarial examples in NLP (Belinkov and Bisk, 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018), we exploit morphology to craft our adversaries. • Demonstrate its effectiveness on multiple machine comprehension and translation models, including BERT and Transformer (Tables 1 & 2). • Show that adversarially fine-tuning the model on an adversarial training set generated via weighted random sampling is sufficient for it to acquire significant robustness, while preserving performance on clean examples (Table 5). To the best of our knowledge, we are the first to investigate the robustness of NLP models to inflectional perturbations and its ethical implications. 2 Related Work Fairness in NLP"
2020.acl-main.263,N18-2002,0,0.0526665,"Missing"
2020.acl-main.263,P16-1162,0,0.110478,"Missing"
2020.acl-main.263,N19-1131,0,0.0227152,"Missing"
2020.acl-main.263,W17-1606,0,0.0225226,"izes the target model’s loss. To maximize semantic preservation, M ORPHEUS only considers inflections belonging to the same universal part of speech as the original word. World Englishes exhibit variation at multiple levels of linguistic analysis (Kachru et al., 2009). Therefore, putting these models directly into production without addressing this inherent bias puts them at risk of committing linguistic discrimination by performing poorly for many speech communities (e.g., AAVE and L2 speakers). This could take the form of either failing to understand these speakers (Rickford and King, 2016; Tatman, 2017), or misinterpreting them. For example, the recent mistranslation of a minority speaker’s social media post resulted in his wrongful arrest (Hern, 2017). Since L2 (and many L1 dialect) speakers often exhibit variability in their production of inflectional morphology2 (Lardiere, 1998; Pr´evost and White, 2000; Haznedar, 2002; White, 2003; Seymour, 2004), we argue that NLP models should be robust to inflectional perturbations in order to minimize their chances of propagating linguistic discrimination. Hence, in this paper, we: 2 Inflections convey tense, quantity, etc. See Appendix A for dialect"
2020.acl-main.263,P18-2006,0,\N,Missing
2020.acl-main.263,N19-1165,0,\N,Missing
2020.acl-main.263,N19-1423,0,\N,Missing
2020.acl-main.263,W19-4406,0,\N,Missing
2020.acl-main.301,P05-1022,0,0.439213,"n Hebrew Hungarian Korean Polish Swedish 7,577 14,759 40,472 5,000 8,146 23,010 6,578 5,000 948 1,235 5,000 500 1,051 2,066 821 494 946 2,541 5,000 716 1,009 2,287 822 666 Table 6: SPMRL Multilingual dataset split. method of Kitaev et al. (2019). 4 Related Work Prior to the neural tsunami in NLP, parsing methods typically model correlations in the output space through probabilistic context-free grammars (PCFGs) on top of sparse (and discrete) input representations either in a generative regime (Klein and Manning, 2003) or a discriminative regime (Finkel et al., 2008) or a combination of both (Charniak and Johnson, 2005). Beside the chart parser approach, there is also a long tradition of transition-based parsers (Sagae and Lavie, 2005) Recently, however, with the advent of powerful neural encoders such as LSTMs (Hochreiter and Schmidhuber, 1997), the focus has been switched more towards effective modeling of correlations in the input’s latent space, as the output structures are nothing but a function of the input (Gaddy et al., 2018). Various neural network models have been proposed to effectively encode the dense input representations and correlations, and have achieved state-of-the-art parsing results. To"
2020.acl-main.301,E17-2053,0,0.291754,"Missing"
2020.acl-main.301,D16-1001,0,0.0547304,"pproaches, various neural architectures have been proposed for constituency parsing as they are able to effectively encode the input tokens into dense vector representations while modeling the structural dependencies between tokens in a sentence. These include recurrent networks (Dyer et al., 2016; Stern et al., 2017b) and more recently selfattentive networks (Kitaev and Klein, 2018). The parsing methods can be broadly distinguished based on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scoring functions for subtrees and perform global search over all possible trees to find the most probable tree for a sentence (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). In this way, these methods can ensure consistency in predicting structured output. The l"
2020.acl-main.301,N19-1423,0,0.509609,"e use of structured loss that requires expensive O(n3 ) CKY inference (Gaddy et al., 2018; Kitaev and Klein, 2018). The training in our model can be fully parallelized without requiring structured inference as in (Shen et al., 2018; G´omez and Vilares, 2018). Our pointing mechanism also allows efficient top-down decoding with a best and worse case running time of O(n log n) and O(n2 ), respectively. In the experiments with English Penn Treebank parsing, our model without any pre-training achieves 92.78 F1, outperforming all existing methods with similar time complexity. With pre-trained BERT (Devlin et al., 2019), our model pushes the F1 score to 95.48, which is on par with the state-of-the-art (Kitaev et al., 2019), while supporting faster decoding. Our model also performs competitively on the multilingual parsing tasks in the SPMRL 2013/2014 shared tasks and establishes new state-of-the-art in Basque and Swedish. We will release our code at https://ntunlpsg.github.io/project/parser/ptrconstituency-parser 2 Model Similar to Stern et al. (2017a), we view constituency parsing as the problem of finding a set of labeled spans over the input sentence. Let S(T ) denote the set of labeled spans for a parse"
2020.acl-main.301,P15-1030,0,0.0765544,"on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scoring functions for subtrees and perform global search over all possible trees to find the most probable tree for a sentence (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). In this way, these methods can ensure consistency in predicting structured output. The limitation, however, is that they run slowly at O(n3 ) or higher time complexity. In this paper, we propose a novel parsing approach that casts constituency parsing into a series of pointing problems (Figure 1). Specifically, 3284 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3284–3294 c July 5 - 10, 2020. 2020 Association for Computational Linguistics our parsing model estimates the pointing"
2020.acl-main.301,N16-1024,0,0.21016,"span and pointing representations of the tree. Introduction Constituency or phrase structure parsing is a core task in natural language processing (NLP) with myriad downstream applications. Therefore, devising effective and efficient algorithms for parsing has been a key focus in NLP. With the advancements in neural approaches, various neural architectures have been proposed for constituency parsing as they are able to effectively encode the input tokens into dense vector representations while modeling the structural dependencies between tokens in a sentence. These include recurrent networks (Dyer et al., 2016; Stern et al., 2017b) and more recently selfattentive networks (Kitaev and Klein, 2018). The parsing methods can be broadly distinguished based on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the ot"
2020.acl-main.301,P15-1147,0,0.0436924,"Missing"
2020.acl-main.301,P08-1109,0,0.12527,"Missing"
2020.acl-main.301,N18-1091,0,0.139279,"Missing"
2020.acl-main.301,D18-1162,0,0.205871,"Missing"
2020.acl-main.301,P19-1340,0,0.374431,"ally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scoring functions for subtrees and perform global search over all possible trees to find the most probable tree for a sentence (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). In this way, these methods can ensure consistency in predicting structured output. The limitation, however, is that they run slowly at O(n3 ) or higher time complexity. In this paper, we propose a novel parsing approach that casts constituency parsing into a series of pointing problems (Figure 1). Specifically, 3284 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3284–3294 c July 5 - 10, 2020. 2020 Association for Computational Linguistics our parsing model estimates the pointing score from one word to another in the input sentence, which repres"
2020.acl-main.301,P18-1249,0,0.0534253,"structure parsing is a core task in natural language processing (NLP) with myriad downstream applications. Therefore, devising effective and efficient algorithms for parsing has been a key focus in NLP. With the advancements in neural approaches, various neural architectures have been proposed for constituency parsing as they are able to effectively encode the input tokens into dense vector representations while modeling the structural dependencies between tokens in a sentence. These include recurrent networks (Dyer et al., 2016; Stern et al., 2017b) and more recently selfattentive networks (Kitaev and Klein, 2018). The parsing methods can be broadly distinguished based on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scoring functions for subtrees and perform global search over all possib"
2020.acl-main.301,P18-1108,0,0.158798,"s the pointing score from one word to another in the input sentence, which represents the likelihood of the span covering those words being a legitimate phrase structure (i.e., a subtree in the constituency tree). During training, the likelihoods of legitimate spans are maximized using the cross entropy loss. This enables our model to enforce structural consistency, while avoiding the use of structured loss that requires expensive O(n3 ) CKY inference (Gaddy et al., 2018; Kitaev and Klein, 2018). The training in our model can be fully parallelized without requiring structured inference as in (Shen et al., 2018; G´omez and Vilares, 2018). Our pointing mechanism also allows efficient top-down decoding with a best and worse case running time of O(n log n) and O(n2 ), respectively. In the experiments with English Penn Treebank parsing, our model without any pre-training achieves 92.78 F1, outperforming all existing methods with similar time complexity. With pre-trained BERT (Devlin et al., 2019), our model pushes the F1 score to 95.48, which is on par with the state-of-the-art (Kitaev et al., 2019), while supporting faster decoding. Our model also performs competitively on the multilingual parsing task"
2020.acl-main.301,P03-1054,0,0.19718,"experiment pre-trained model test (with pretraining). Language Train Valid Test Basque French German Hebrew Hungarian Korean Polish Swedish 7,577 14,759 40,472 5,000 8,146 23,010 6,578 5,000 948 1,235 5,000 500 1,051 2,066 821 494 946 2,541 5,000 716 1,009 2,287 822 666 Table 6: SPMRL Multilingual dataset split. method of Kitaev et al. (2019). 4 Related Work Prior to the neural tsunami in NLP, parsing methods typically model correlations in the output space through probabilistic context-free grammars (PCFGs) on top of sparse (and discrete) input representations either in a generative regime (Klein and Manning, 2003) or a discriminative regime (Finkel et al., 2008) or a combination of both (Charniak and Johnson, 2005). Beside the chart parser approach, there is also a long tradition of transition-based parsers (Sagae and Lavie, 2005) Recently, however, with the advent of powerful neural encoders such as LSTMs (Hochreiter and Schmidhuber, 1997), the focus has been switched more towards effective modeling of correlations in the input’s latent space, as the output structures are nothing but a function of the input (Gaddy et al., 2018). Various neural network models have been proposed to effectively encode th"
2020.acl-main.301,Q17-1004,0,0.378214,"al architectures have been proposed for constituency parsing as they are able to effectively encode the input tokens into dense vector representations while modeling the structural dependencies between tokens in a sentence. These include recurrent networks (Dyer et al., 2016; Stern et al., 2017b) and more recently selfattentive networks (Kitaev and Klein, 2018). The parsing methods can be broadly distinguished based on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scoring functions for subtrees and perform global search over all possible trees to find the most probable tree for a sentence (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Kitaev et al., 2019). In this way, these methods can ensure consistency in predicting structured output. The limitation, however, is"
2020.acl-main.301,P18-1130,0,0.068444,"ce during training. This makes their training time considerably slower than that of our method, which is trained 3289 Model Our model BERTBASE-uncased Our model BERTLARGE-cased Model F1 95.34 95.48 Kitaev and Klein (2018) ELMO 95.13 Kitaev et al. (2019) BERTLARGE-cased 95.59 Table 2: Restuls on PTB WSJ test set with pretraining. directly with span-wise cross entropy loss. In addition, Zhou and Zhao (2019) uses external supervision (head information) from the dependency parsing task. Dependency parsing models, in fact, have a strong resemblance to the pointing mechanism that our model employs (Ma et al., 2018). As such, integrating dependency parsing information into our model may also be beneficial. We leave this for future work. Results with Pre-training Similar to Kitaev and Klein (2018) and Kitaev et al. (2019), we also evaluate our models with BERT (Devlin et al., 2019) embeddings . Following them in the inclusion of contextualized token representations, we adjust the number of self-attentive layers to 2 and the base learning rate to 0.00005. As shown in Table 2, our model achieves an F1 score of 95.48, which is on par with the state-ofthe-art models. However, the advantage of our method is th"
2020.acl-main.301,J93-2004,0,0.072737,"the general and unary spans respectively, wlgc and wluc are the class-specific trainable weight vectors. 2.4 Model We train our parsing model by minimizing the total loss Ltotal (θ) defined as: Other Approaches G´omez and Vilares (2018) 90.7 91.8 Liu and Zhang (2017) Stern et al. (2017b) 92.57 92.56 92.56 Zhou and Zhao (2019) 93.64 93.92 93.78 +Lgc (θe , θgc ) + Luc (θe , θuc ) (14) Experiments To show the effectiveness of our approach, we conduct experiments on English and Multilingual parsing tasks. For English, we use the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) (Marcus et al., 1993), whereas for multilingual, we experiment with seven (7) different languages from the SPMRL 2013-2014 shared task (Seddah et al., 2013): Basque, French, German, Hungarian, Korean, Polish and Swedish. For evaluation on PTB, we report the standard labeled precision (LP), labeled recall (LR), and labelled F1 computed by evalb1 . For the SPMRL datasets, we report labeled F1 and use the same setup in evalb as Kitaev and Klein (2018). 3.1 English (PTB) Experiments Setup. We follow the standard train/valid/test split, which uses sections 2-21 for training, section 22 for development and section 23 fo"
2020.acl-main.301,N07-1051,0,0.486539,"Missing"
2020.acl-main.301,W05-1513,0,0.106746,"821 494 946 2,541 5,000 716 1,009 2,287 822 666 Table 6: SPMRL Multilingual dataset split. method of Kitaev et al. (2019). 4 Related Work Prior to the neural tsunami in NLP, parsing methods typically model correlations in the output space through probabilistic context-free grammars (PCFGs) on top of sparse (and discrete) input representations either in a generative regime (Klein and Manning, 2003) or a discriminative regime (Finkel et al., 2008) or a combination of both (Charniak and Johnson, 2005). Beside the chart parser approach, there is also a long tradition of transition-based parsers (Sagae and Lavie, 2005) Recently, however, with the advent of powerful neural encoders such as LSTMs (Hochreiter and Schmidhuber, 1997), the focus has been switched more towards effective modeling of correlations in the input’s latent space, as the output structures are nothing but a function of the input (Gaddy et al., 2018). Various neural network models have been proposed to effectively encode the dense input representations and correlations, and have achieved state-of-the-art parsing results. To enforce the structural consistency, existing neural parsing methods either employ a transition-based algorithm (Dyer e"
2020.acl-main.301,D17-1178,0,0.104601,"epresentations of the tree. Introduction Constituency or phrase structure parsing is a core task in natural language processing (NLP) with myriad downstream applications. Therefore, devising effective and efficient algorithms for parsing has been a key focus in NLP. With the advancements in neural approaches, various neural architectures have been proposed for constituency parsing as they are able to effectively encode the input tokens into dense vector representations while modeling the structural dependencies between tokens in a sentence. These include recurrent networks (Dyer et al., 2016; Stern et al., 2017b) and more recently selfattentive networks (Kitaev and Klein, 2018). The parsing methods can be broadly distinguished based on whether they employ a greedy transition-based algorithm or a globally optimized chart parsing algorithm. The transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017) generate trees autoregressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps which would suffer from exposure bias. Chart parsing methods, on the other hand, learn scor"
2020.acl-main.301,N03-1033,0,0.0634634,"valuation on PTB, we report the standard labeled precision (LP), labeled recall (LR), and labelled F1 computed by evalb1 . For the SPMRL datasets, we report labeled F1 and use the same setup in evalb as Kitaev and Klein (2018). 3.1 English (PTB) Experiments Setup. We follow the standard train/valid/test split, which uses sections 2-21 for training, section 22 for development and section 23 for evaluation. This gives 45K sentences for training, 1,700 sentences for development, and 2,416 sentences for testing. Following previous studies, our model uses POS tags predicted by the Stanford tagger (Toutanova et al., 2003). For our model, we adopt the self-attention encoder with similar hyperparameter details proposed by Kitaev and Klein (2018). The character embeddings are of 64 dimensions. For general 1 http://nlp.cs.nyu.edu/evalb/ F1 CKY/Chart Inference Gaddy et al. (2018) 92.10 Kitaev and Klein (2018) 93.20 93.90 93.55 Ltotal (θ) = Lgp (θe , θgp ) + Lsp (θe , θsp ) 3 LP Top-Down Inference Stern et al. (2017a) 93.20 90.30 91.80 Shen et al. (2018) 92.00 91.70 91.80 Our Model 92.81 92.75 92.78 Training Objective where each individual loss is a cross entropy loss computed for the corresponding labeling or point"
2020.acl-main.301,P19-1230,0,0.483248,", . . . , hn ) uc and H uc = (huc 1 , . . . , hn ) into the respective softmax classification layers as follows. 3288 exp(hgc wgc ) gc(l|i) = P|L |i gcl gc g l=1 exp(hi wl ) exp(huc wuc ) uc(l|i) = P|L |i l u uc uc l=1 exp(hi wl ) (12) (13) where Lg and Lu are the set of possible labels for the general and unary spans respectively, wlgc and wluc are the class-specific trainable weight vectors. 2.4 Model We train our parsing model by minimizing the total loss Ltotal (θ) defined as: Other Approaches G´omez and Vilares (2018) 90.7 91.8 Liu and Zhang (2017) Stern et al. (2017b) 92.57 92.56 92.56 Zhou and Zhao (2019) 93.64 93.92 93.78 +Lgc (θe , θgc ) + Luc (θe , θuc ) (14) Experiments To show the effectiveness of our approach, we conduct experiments on English and Multilingual parsing tasks. For English, we use the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) (Marcus et al., 1993), whereas for multilingual, we experiment with seven (7) different languages from the SPMRL 2013-2014 shared task (Seddah et al., 2013): Basque, French, German, Hungarian, Korean, Polish and Swedish. For evaluation on PTB, we report the standard labeled precision (LP), labeled recall (LR), and labelled F1 c"
2020.acl-main.301,P13-1043,0,0.44837,"Missing"
2020.acl-main.589,P17-1080,0,0.0186737,"fragmented key sequence (past keys) is visible in the decoder. Second, the above argument is further reinforced as we found that cross-attention layers also prefer additive window attention, where the entire source sequence is available. Third, crossattention works better with segment-based masking, which provides smoothness and facilitates phrase (n-gram) based translations. Lower-layer Local Attentions. It has been shown that deep neural models learn simple word features and local syntax in the lower layers, while higher layers learn more complex contextdependent aspects of word semantics. Belinkov et al. (2017) show this on NMT models, while Peters et al. (2018) and Jawahar et al. (2019) show this on representation learning with ELMo and BERT respectively. In other words, local contextual information can still be derived in higher layers with the standard global attention. As such, we propose to apply our dynamic window attention methods only to the first 3 layers of the Transformer network, leaving the top 3 layers intact. Our diverse experiments in the following section support this setup as it offers substantial improvements, whereas using local attention in higher layers does not show gains, but"
2020.acl-main.589,N19-1423,0,0.0126174,"d attMW = (S(score) M )(V W V ) score = (12) (13) In this approach, the standard global attention weights are suppressed and partially overshadowed by the attention window imposed by M . Thus, it can be interpreted as a local attention method similar to Luong et al. (2015). However, instead of using a static Gaussian bias, we use a dynamic mask to modulate the attention weights. 4.2 Additive Window Attention Having a local attention window could be beneficial, but it does not rule out the necessity of global attention, which has been shown effective in many applications (Vaswani et al., 2017; Devlin et al., 2019). Thus, we also propose an additive window attention, which implements a combination of global attention and local attention. The attention output in this method is formally defined as Q K T sglb = (QWglb )(QWglb ) (14) Q K T sloc = (QWloc )(QWloc ) M (15) sglb + sloc √ (16) scoreAW = d attAW = S(scoreAW )(V W V ) (17) Q K , W Q , and W K ∈ IRd×d are where Wglb , Wglb loc loc the weight matrices for global and local attentions. Compared to the multiplicative window attention where the mask re-evaluates the global attention weights, additive window attention applies the mask vector to the local"
2020.acl-main.589,P19-1356,0,0.0111786,"ve argument is further reinforced as we found that cross-attention layers also prefer additive window attention, where the entire source sequence is available. Third, crossattention works better with segment-based masking, which provides smoothness and facilitates phrase (n-gram) based translations. Lower-layer Local Attentions. It has been shown that deep neural models learn simple word features and local syntax in the lower layers, while higher layers learn more complex contextdependent aspects of word semantics. Belinkov et al. (2017) show this on NMT models, while Peters et al. (2018) and Jawahar et al. (2019) show this on representation learning with ELMo and BERT respectively. In other words, local contextual information can still be derived in higher layers with the standard global attention. As such, we propose to apply our dynamic window attention methods only to the first 3 layers of the Transformer network, leaving the top 3 layers intact. Our diverse experiments in the following section support this setup as it offers substantial improvements, whereas using local attention in higher layers does not show gains, but rather increases model parameters. 5 Experiment In this section, we present t"
2020.acl-main.589,Q16-1037,0,0.0333815,"target token to a source token. We can see the relevant subwords are captured by the attentions quite well, which promotes ngram-level alignments. For instance, the mask ˆ q ) guides the model to evenly distribute atten(m tion scores on sub-words “Co@@” and “en” (Fig. 3b), while standard attention is biased towards “Co@@” (Fig. 3c). Similar phenomenon can be seen for “Bro@@” and “thers” (towards “fr`eres”). 5.3 Text Classification We evaluate our models on the Stanford Sentiment Treebank (SST) (Socher et al., 2013), IMDB sentiment analysis (Maas et al., 2011) and SubjectVerb Aggreement (SVA) (Linzen et al., 2016) tasks. We compare our attention methods (incorporated into the Transformer encoder) with the encoders of Vaswani et al. ( 2017), Shaw et al. (2018) and Yang et al. (2018). Model STT IMDB SVA Vaswani et al. (2017) Shaw et al. (2018) Yang et al. (2018) 79.36 79.73 79.24 83.65 84.61 84.13 94.48 95.27 95.00 Enc (MW) Enc (AW) 79.70 82.13 85.09 87.98 95.95 96.19 Table 5: Classification accuracy on on Stanford Sentiment Treebank (SST) and IMDB sentiment analysis and Subject-Verb Agreement(SVA) tasks. Training Setup. As the datasets are quite small compared to the MT datasets, we used tiny versions o"
2020.acl-main.589,D15-1166,0,0.780223,"f inputs can be regarded as one of the important advances in modern deep learning research. This paradigm, commonly known as attention (Bahdanau et al., 2015), has demonstrated immense success across a wide spectrum of applications. To this end, learning to compute contextual representations (Vaswani et al., 2017), to point to the relevant part in the input (Vinyals et al., 2015), or to select windows or spans (Wang and Jiang, 2017) from sequences forms the crux of many modern deep neural architectures. Despite aggressive advances in developing neural modules for computing relative relevance (Luong et al., 2015; Chiu and Raffel, 2018), there has been no general purpose solution for learning differentiable attention windows. While span selectionbased pointer network models typically predict a start boundary and an end boundary (Wang and Jiang, 2017; Seo et al., 2017), these soft predictions generally reside at the last layer of the net∗ *Equal contributions work and are softly optimized. To the best of our knowledge, there exists no general purpose component for learning differentiable windows within networks. Although the practical advantages of learning differentiable windows are plenty, this paper"
2020.acl-main.589,P11-1015,0,0.0325086,"darker the score, the higher the attention is from a target token to a source token. We can see the relevant subwords are captured by the attentions quite well, which promotes ngram-level alignments. For instance, the mask ˆ q ) guides the model to evenly distribute atten(m tion scores on sub-words “Co@@” and “en” (Fig. 3b), while standard attention is biased towards “Co@@” (Fig. 3c). Similar phenomenon can be seen for “Bro@@” and “thers” (towards “fr`eres”). 5.3 Text Classification We evaluate our models on the Stanford Sentiment Treebank (SST) (Socher et al., 2013), IMDB sentiment analysis (Maas et al., 2011) and SubjectVerb Aggreement (SVA) (Linzen et al., 2016) tasks. We compare our attention methods (incorporated into the Transformer encoder) with the encoders of Vaswani et al. ( 2017), Shaw et al. (2018) and Yang et al. (2018). Model STT IMDB SVA Vaswani et al. (2017) Shaw et al. (2018) Yang et al. (2018) 79.36 79.73 79.24 83.65 84.61 84.13 94.48 95.27 95.00 Enc (MW) Enc (AW) 79.70 82.13 85.09 87.98 95.95 96.19 Table 5: Classification accuracy on on Stanford Sentiment Treebank (SST) and IMDB sentiment analysis and Subject-Verb Agreement(SVA) tasks. Training Setup. As the datasets are quite sma"
2020.acl-main.589,W18-6301,0,0.0164609,"r encoding (Sennrich et al., 2016) with shared source-target vocabularies of 32,768 and 40,000 sub-words for En-De and En-Fr translation tasks, respectively. We compare our models with three strong baselines: (i) Transformer Base (Vaswani et al., 2017), (ii) Transformer Base with Relative Position (Shaw et al., 2018), and (ii) Transformer Base with Localness Modeling (Yang et al., 2018). To ensure a fair comparison, we trained our models and the baselines with the following training setup. Training Setup. We followed model specifications in (Vaswani et al., 2017) and optimization settings in (Ott et al., 2018), with some minor modifications. Specifically, we used word embeddings of dimension 512, feedforward layers with inner dimension 2048, and multi-headed attentions with 8 heads. We trained our models on a single physical GPU but replicated the 8-GPU setup following the gradient aggregation method proposed by Ott et al. (2018). We trained the models for 200,000 updates for En-De and 150,000 updates for En-Fr translation tasks. Finally, we averaged the last 5 checkpoints to obtain the final models for evaluation. The segment size b in the segment-based masking method was set to 5.3 Translation Re"
2020.acl-main.589,D18-1179,0,0.0194181,"decoder. Second, the above argument is further reinforced as we found that cross-attention layers also prefer additive window attention, where the entire source sequence is available. Third, crossattention works better with segment-based masking, which provides smoothness and facilitates phrase (n-gram) based translations. Lower-layer Local Attentions. It has been shown that deep neural models learn simple word features and local syntax in the lower layers, while higher layers learn more complex contextdependent aspects of word semantics. Belinkov et al. (2017) show this on NMT models, while Peters et al. (2018) and Jawahar et al. (2019) show this on representation learning with ELMo and BERT respectively. In other words, local contextual information can still be derived in higher layers with the standard global attention. As such, we propose to apply our dynamic window attention methods only to the first 3 layers of the Transformer network, leaving the top 3 layers intact. Our diverse experiments in the following section support this setup as it offers substantial improvements, whereas using local attention in higher layers does not show gains, but rather increases model parameters. 5 Experiment In"
2020.acl-main.589,P16-1162,0,0.0574054,"ison with the baselines on machine translation (MT), sentiment analysis, subject verb agreement and language modeling (LM) tasks. 5.1 Machine Translation We trained our models on the standard WMT’16 English-German (En-De) and WMT’14 EnglishFrench (En-Fr) datasets containing about 4.5 and 36 million sentence pairs, respectively. For validation (development) purposes, we used newstest2013 for En-De and a random split from the training set for En-Fr. All translation tasks were evaluated against their respective newstest2014 test sets, in case-sensitive tokenized BLEU. We used byte-pair encoding (Sennrich et al., 2016) with shared source-target vocabularies of 32,768 and 40,000 sub-words for En-De and En-Fr translation tasks, respectively. We compare our models with three strong baselines: (i) Transformer Base (Vaswani et al., 2017), (ii) Transformer Base with Relative Position (Shaw et al., 2018), and (ii) Transformer Base with Localness Modeling (Yang et al., 2018). To ensure a fair comparison, we trained our models and the baselines with the following training setup. Training Setup. We followed model specifications in (Vaswani et al., 2017) and optimization settings in (Ott et al., 2018), with some minor"
2020.acl-main.589,N18-2074,0,0.444209,"K and V contain the encoder states. The attention mechanism adopted in the Transformer is considered global since the attention context spans the entire sequence. 2.2 Windows in Attentions In theory, given enough training data, global attention should be able to model dependencies between the query and the key vectors well. However, in practice we have access to only a limited amount of training data. Several recent studies suggest that incorporating more focused attention over important local regions in the input sequence as an explicit inductive bias could be more beneficial. In particular, Shaw et al. (2018) show that adding relative positional biases to the attention scores (Eq. 1) increases BLEU scores in machine translation. Specifically, for each query qi ∈ Q at position i and key kj ∈ K at position j, a trainable vector ai,j = wmax(−τ,min(j−i,τ )) is added to the key vector before the query-key dot product is performed. The window size τ is chosen via tuning. Sperber et al. (2018) also consider local information by restricting self-attention to neighboring representations to improve long-sequence acoustic modeling. Although shown to be effective, their methods only apply to self-attention an"
2020.acl-main.589,D18-1475,0,0.131539,"ce. That said, Luong et al. (2015) are the first to propose a Gaussian-based local attention for crossattention. At each decoding step t, their model approximates the source-side pivot position pt as a function of the decoding state and the source sequence length. Then, local attention is achieved by multiplying the attention score with a confidence term derived from a N (pt , σ 2 ) distribution. The aligned pivot pt and the variance σ 2 (a hyperparameter) respectively represent the center and the size of the local window. 6590 1 Initially, Q, K, and V contain the token embeddings. Meanwhile, Yang et al. (2018) improve the method of Luong et al. (2015) by assigning a soft window weight (a Gaussian bias) to obtain a flexible window span. Despite effective, the aligned pivot position in the source is determined only by the decoder state, while the encoder states are disregarded - these should arguably give more relevant information regarding the attention spans over the source sequence. Besides, the confidence for local attention span may not strictly follow a normal distribution, but rather vary dynamically depending on the relationship between the query and the key. Furthermore, the approach of Luon"
2020.acl-main.589,D13-1170,0,0.00307915,"ed by our Enc(AW)-Cr(AW,Seg)Dec(MW) model. The darker the score, the higher the attention is from a target token to a source token. We can see the relevant subwords are captured by the attentions quite well, which promotes ngram-level alignments. For instance, the mask ˆ q ) guides the model to evenly distribute atten(m tion scores on sub-words “Co@@” and “en” (Fig. 3b), while standard attention is biased towards “Co@@” (Fig. 3c). Similar phenomenon can be seen for “Bro@@” and “thers” (towards “fr`eres”). 5.3 Text Classification We evaluate our models on the Stanford Sentiment Treebank (SST) (Socher et al., 2013), IMDB sentiment analysis (Maas et al., 2011) and SubjectVerb Aggreement (SVA) (Linzen et al., 2016) tasks. We compare our attention methods (incorporated into the Transformer encoder) with the encoders of Vaswani et al. ( 2017), Shaw et al. (2018) and Yang et al. (2018). Model STT IMDB SVA Vaswani et al. (2017) Shaw et al. (2018) Yang et al. (2018) 79.36 79.73 79.24 83.65 84.61 84.13 94.48 95.27 95.00 Enc (MW) Enc (AW) 79.70 82.13 85.09 87.98 95.95 96.19 Table 5: Classification accuracy on on Stanford Sentiment Treebank (SST) and IMDB sentiment analysis and Subject-Verb Agreement(SVA) tasks."
2020.acl-main.88,D18-1241,0,0.0381685,"t locate the span within the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC C"
2020.acl-main.88,N19-1423,0,0.47998,"licitly tracking rules with external memories boosts both the decision accuracy and the quality of generated follow-up questions. In particular, EMT outperforms the previous best model E3 by 1.3 in macro-averaged decision accuracy and 10.8 in BLEU4 for follow-up question generation. In addition to the performance improvement, EMT yields interpretability by explicitly tracking rules, which is visualized to show the entailment-oriented reasoning process of our model. 2 As illustrated in Figure 2, our proposed method consists of the following four main modules. (1) The Encoding module uses BERT (Devlin et al., 2019) to encode the concatenation of the rule text, initial question, scenario and dialog history into contextualized representations. However, there are two main drawbacks to the existing methods. First, with respect to the reasoning of the rule text, existing methods do not explicitly track whether a condition listed in the rule has already been satisfied as the conversation flows so that it can make a better decision. Second, with respect to the extraction of question-related rules, it is difficult in the current approach to extract the most relevant text span to generate the next question. For"
2020.acl-main.88,P18-1177,0,0.0218596,"Missing"
2020.acl-main.88,P19-1480,1,0.83023,"inds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend"
2020.acl-main.88,D19-1001,0,0.306002,"onal question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and edit the span with the highest score into a follow-up question. However, they do not use these entailment scores for decision making. Sharma et al. (2019) study patterns of the dataset and include addition"
2020.acl-main.88,P18-1136,1,0.84912,"tion) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coarseto-fine follow-up question generation in a unified manner. EMT achieved a new state-of-the-art result on the ShARC CMR challenge. EMT also gives interpretability by showing the entailment-oriented reasoning process as the conversation"
2020.acl-main.88,P02-1040,0,0.10833,"Missing"
2020.acl-main.88,Q19-1016,0,0.061325,"ithin the ground truth rule sentence, while in 9 cases, it finds the correct rule sentence but extracts a different span. Another challenge comes from the one-to-many problem in sequence generation. When there are multiple underspecified rule sentences, the model asks about one of these underspecified rule sentences which is different from the ground truth one. This suggests that new evaluation metrics could be proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi"
2020.acl-main.88,D18-1233,0,0.180418,"Missing"
2020.acl-main.88,D16-1027,0,0.0143228,"as two key differences: (1) EMT makes decision via explicitly entailmentoriented reasoning, which, to our knowledge, is the first such approach; (2) Instead of treating decision making and follow-up question generation (or span extraction) separately, EMT is a unified approach that exploits its memory states for both decision making and question generation. Memory-Augmented Neural Networks. Our work is also related to memory-augmented neural networks (Graves et al., 2014, 2016), which have been applied in some NLP tasks such as question answering (Henaff et al., 2017) and machine translation (Wang et al., 2016). For dialog applications, Zhang et al. (2019) propose a dialogue management model that employs a memory controller and a slot-value memory, Bordes et al. (2016) learn a restaurant bot by end-to-end memory networks, Madotto et al. (2018) incorporate external memory modules into dialog generation. 5 Conclusions In this paper, we have proposed a new framework for conversational machine reading (CMR) that comprises a novel explicit memory tracker (EMT) to track entailment states of the rule sentences explicitly within its memory module. The updated states are utilized for decision making and coar"
2020.acl-main.88,E17-1042,0,0.073334,"Missing"
2020.acl-main.88,P19-1078,1,0.832483,"this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an entailment score, and"
2020.acl-main.88,P18-1135,1,0.842643,"proposed by taking this into consideration. 4 Related Work ShARC Conversational Machine Reading (Saeidi et al., 2018) differs from conversational question answering (Choi et al., 2018; Reddy et al., 2019) and conversational question generation (Gao et al., 2019) in that 1) machines are required to formulate follow-up questions to fill the information gap, and 2) machines have to interpret a set of complex decision rules and make a question-related conclusion, instead of extracting the answer from the text. CMR can be viewed as a special type of task-oriented dialog systems (Wen et al., 2017; Zhong et al., 2018; Wu et al., 2019) to help users achieve their goals. However, it does not rely on predefined slot and ontology information but natural language rules. On the ShARC CMR challenge (Saeidi et al., 2018), Lawrence et al. (2019) propose an end-toend bidirectional sequence generation approach with mixed decision making and question generation stages. Saeidi et al. (2018) split it into sub-tasks and combines hand-designed sub-models for decision classification, entailment and question generation. Zhong and Zettlemoyer (2019) propose to extract all possible rule text spans, assign each of them an ent"
2020.acl-main.88,P19-1223,0,0.218973,"Missing"
2020.cl-2.2,D18-1214,0,0.668077,"ng et al. (2017a, 2017b) first reported encouraging results for unsupervised models with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for many language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al. 2018a, 2018b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose non-adversarial methods more recently (Xu et al. 2018; Hoshen and Wolf 2018; Alvarez-Melis and Jaakkola 2018; Artetxe, Labaka, and Agirre 2018b). In particular, Artetxe, Labaka, and Agirre (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a, 2017b) fail for many difficult language pairs. In this article, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings. Our main idea is to learn the crosslingual mapping in a projected latent space (code space) and add more constraints to guide the unsupervised mapping in this space. We accomplish this by proposing a novel adversarial autoencoder frame"
2020.cl-2.2,D16-1250,0,0.113598,"Missing"
2020.cl-2.2,P17-1042,0,0.107822,"Missing"
2020.cl-2.2,P18-1073,0,0.119214,"Missing"
2020.cl-2.2,W16-1614,0,0.0399622,"and dimensionality reduction is applied to both, but re-weighting and de-whitening are not performed. Similarly, orthogonal methods such as the methods of Artetxe, Labaka, and Agirre (2016) and Smith et al. (2017) correspond to the case where only orthogonal mapping is applied. 2.2 Unsupervised Models As mentioned, a more recent line of research attempts to eliminate the seed dictionary and learn the bilingual mapping in a completely unsupervised way. Initial approaches used adversarial methods, but some non-adversarial methods have also been proposed more recently. 2.2.1 Adversarial Methods. Barone (2016) is the first to propose a model for solving bilingual word translation in an unsupervised way. He initially used an adversarial network similar to Conneau et al. (2018), and found that the mapper (which is also the encoder) translates everything to a single embedding, commonly known as the mode collapse issue (Goodfellow 2017). To preserve diversity in mapping, he then used a decoder to reconstruct the source embedding from the mapped embedding, extending the framework to an adversarial autoencoder. His analysis (qualitative) shows promising but not competitive with methods that use bilingual"
2020.cl-2.2,Q17-1010,0,0.0766218,"WFT )WF ( f ) Sample a batch from Y as source and X as target and update accordingly (symmetric to (b) -(e) steps). end Use validation criterion to save the best model. end // Fine-tuning 3. Load the best model. // Iterative Procrustes solution for n iterations do (a) Build a synthetic dictionary (b) Apply the Procrustes solution on the dictionary. end // Symmetric re-weighting for n iterations do (a) Build a synthetic dictionary (b) Apply the symmetric re-weighting for the refinement. end // Test 4. Test the model on gold bilingual dictionary. monolingual embeddings of (d =) 300 dimensions (Bojanowski et al. 2017) trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs.2 To show the generality of different methods, we consider seven (7) different language pairs with fourteen (7 × 2 = 14) different translation tasks from high- and low-resource languages 2 https://github.com/facebookresearch/MUSE. 274 Mohiuddin and Joty Unsupervised Word Translation from different language families. In particular, we evaluate on English (En) from/to Spanish (Es), German (De), Italian (It), Finnish (Fi), Arabic (Ar), Malay (Ms), and Hebrew (He). Malay and Hebrew are generally considered as low"
2020.cl-2.2,E14-1049,0,0.315482,"learn the embedding and the objective to learn the linear mapping. They solve this by enforcing the word vectors to be of unit length during the learning of the embeddings. Instead of using Euclidean distance in the objective function for learning the mapping, they propose to use cosine similarity: WCOS = max W n X (Wxi )T yi (2) i=1 To preserve unit length after mapping, they enforce the orthogonality constraint on W, i.e., WW T = I. As a result, the inner product in Equation (2) is equivalent to cosine similarity. Instead of learning a mapping from the source to the target embedding space, Faruqui and Dyer (2014) use a technique based on Canonical Correlation Analysis to project both source and target embeddings to a common low-dimensional space, where the correlation of the word pairs in the seed dictionary is maximized. Artetxe, Labaka, and Agirre (2016) show that the above methods are variants of the same core optimization objective and propose a general framework that explains the relation between the methods of Mikolov, Le, and Sutskever (2013), Xing et al. (2015), and Faruqui and Dyer (2014). The orthogonality constraint on W and the unit-length normalization of word embeddings ensure that Equat"
2020.cl-2.2,D18-1043,0,0.0657872,"Missing"
2020.cl-2.2,D18-1549,0,0.0491549,"Missing"
2020.cl-2.2,W15-1521,0,0.0534178,"Missing"
2020.cl-2.2,N19-1386,1,0.128587,"tagging, and parsing. To the best of our knowledge, our work in this article is the first to give a comprehensive overview of existing methods for unsupervised word translation, which is one of the most emerging topics in crosslingual representation learning. We therefore hope that the codebase5 released with this article will serve as a benchmark that will facilitate other researchers in pushing the state-of-the-art and in applying bilingual word embeddings to their downstream NLP tasks. Bibliographic Note Portions of this work have been published in the NAACL-HLT 2019 conference proceeding (Mohiuddin and Joty 2019). However, this article substantially extends the published work in several ways, most notably: (i) we extend our approach by incorporating two types of refinement (fine-tuning) procedures (Section 3.2); (ii) alongside evaluating the performance with Conneau et al. (2018) refinement and Artetxe, Labaka, and Agirre (2018b) refinement, we also present the performance of our adversarial model with our proposed refinement procedure (Section 5.2); and (iii) we analyze the components of different refinement methods and assess the impact of regularization and orthogonality constraint on refinement me"
2020.cl-2.2,P18-1072,0,0.0518607,"Missing"
2020.cl-2.2,D18-1034,0,0.0660989,"Missing"
2020.cl-2.2,N15-1104,0,0.195425,"rpora, and then learn a linear mapping from one language to another with the underlying assumption that two embedding spaces exhibit similar geometric structures, also known as the isomorphic assumption. This allows the model to learn effective crosslingual representations without expensive supervision. Given monolingual word embeddings of two languages, Mikolov, Le, and Sutskever (2013) show that a linear mapping can be learned from a seed dictionary of 5,000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent studies (Xing et al. 2015; Artetxe, Labaka, and Agirre 2016, 2017; Smith et al. 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. These methods assume some supervision in the form of a seed dictionary, although recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a, 2017b) first reported encouraging results for unsupervised models with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for many language pairs"
2020.cl-2.2,D18-1268,0,0.641039,"ds have shown competitive results. Zhang et al. (2017a, 2017b) first reported encouraging results for unsupervised models with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for many language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al. 2018a, 2018b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose non-adversarial methods more recently (Xu et al. 2018; Hoshen and Wolf 2018; Alvarez-Melis and Jaakkola 2018; Artetxe, Labaka, and Agirre 2018b). In particular, Artetxe, Labaka, and Agirre (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a, 2017b) fail for many difficult language pairs. In this article, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings. Our main idea is to learn the crosslingual mapping in a projected latent space (code space) and add more constraints to guide the unsupervised mapping in this space. We accomplish"
2020.cl-2.2,P17-1179,0,0.450856,"tskever (2013) show that a linear mapping can be learned from a seed dictionary of 5,000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent studies (Xing et al. 2015; Artetxe, Labaka, and Agirre 2016, 2017; Smith et al. 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. These methods assume some supervision in the form of a seed dictionary, although recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a, 2017b) first reported encouraging results for unsupervised models with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for many language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al. 2018a, 2018b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose non-adversarial methods more recently (Xu et al. 2018; Hoshen and Wolf 2018; Alvarez-Melis a"
2020.cl-2.2,D17-1207,0,0.469685,"tskever (2013) show that a linear mapping can be learned from a seed dictionary of 5,000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent studies (Xing et al. 2015; Artetxe, Labaka, and Agirre 2016, 2017; Smith et al. 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. These methods assume some supervision in the form of a seed dictionary, although recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a, 2017b) first reported encouraging results for unsupervised models with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for many language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al. 2018a, 2018b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose non-adversarial methods more recently (Xu et al. 2018; Hoshen and Wolf 2018; Alvarez-Melis a"
2020.emnlp-main.177,D18-1040,0,0.116904,"o-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works proposing strategies to further improve it (Hoa; Yang et al., 2019; Caswell et al., 2019). However, recent studies have suggested that there is a limit beyond which the addition of synthetic data hurts the performance of the model (Fadaee and Monz, 2018; Poncelas et al., 2018). Also, recent work (Edunov et al., 2020; Nguyen et al., 2020) point out that back-translation suffers from the translationese effect, where back-translation only improves the performance when the source sentences are translationese but does not offer any improvement when the sentences are natural text. Automatic post-editing (APE) is another common strategy that is used to improve translations. APE models are commonly monolingual, and typically take the output from some MT model as input, which they then modify. In the absence of adequate human post-edited data to trai"
2020.emnlp-main.177,W19-5206,0,0.0188852,"ion (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works proposing strategies to further improve it (Hoa; Yang et al., 2019; Caswell et al., 2019). However, recent studies have suggested that there is a limit beyond which the addition of synthetic data hurts the performance of the model (Fadaee and Monz, 2018; Poncelas et al., 2018). Also, recent work (Edunov et al., 2020; Nguyen et al., 2020) point out that back-translation suffers from the translationese effect, where back-translation only improves the performance when the source sentences are translationese but does not offer any improvement when the sentences are natural text. Automatic post-editing (APE) is another common strategy that is used to improve translations. APE models ar"
2020.emnlp-main.177,E12-3001,0,0.0193663,"rve the translation adequacy and fluency. In our experiments, we simply set λ = 0.5. 3 Fine-tuning Data & MT Baselines 3.1 Pronoun-Targeted Fine-tuning Data We create a subset of the training corpus in order to find training data that has not been fully learnt from; particularly, we focus our fine-tuning experiments on pronoun translation. Pronouns are an important discourse phenomenon that provide references to entities that have previously occurred in a text. Mistranslations can lead to loss of grammaticality or inference of the wrong antecedent, resulting in a misunderstanding of the text (Guillou, 2012). Consider a parallel corpus D = (S, R), where S is the source and R is the target/reference text. Assuming that the baseline NMT models (§3.2) are trained until convergence using this data, for our targeted fine-tuning of pronoun translations, we derive a subset of the training corpus D as follows: 1. Translate D using a baseline model M to obtain source to target translations TM . (7) 2. Align TM with reference R using efmaral ¨ (Ostling and Tiedemann, 2016). where µ is the margin; we use µ = 0.3. Note that the additional losses can be applied to all the tokens in the sequence, or restricted"
2020.emnlp-main.177,2012.eamt-1.60,0,0.0248697,"aseline models are trained for 100,000 steps. Other parameter details are in the Appendix. 4 Experiments We conduct our fine-tuning experiments on the German-English (De-En) translation task. We describe our baseline training and fine-tuning corpus (§4.1), our experiments and results on fine-tuning using only the targeted subset data (§4.4), and finetuning using both the targeted subset data and the hybrid training losses (§4.5). 4.1 MT Training Data Baseline training corpus. We use a De-En training dataset consisting of about 2.5 million sentence pairs, taken from the News Commentary, IWSLT (Cettolo et al., 2012) and Europarl (Tiedemann, 2012) corpora.4 Sentences are encoded through 4 We exclude the UN corpus as our analysis showed that it does not have a high incidence of pronouns. WMT14 Model S EN 2S EN C ONCAT Pronoun Testset Train BLEU BLEU P R F1 D D 31.64 31.81 35.56 36.16 77.92 80.39 66.01 68.49 69.55 72.03 Table 1: Baseline BLEU results on the WMT14 DeEn testset and the BLEU (for translation), Precision, Recall and F1 scores (for pronoun translations) on the pronoun testset from Jwalapuram et al. (2019). Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 40,000 operations, which results in"
2020.emnlp-main.177,P17-2061,0,0.0416477,"Missing"
2020.emnlp-main.177,2010.iwslt-papers.10,0,0.102132,"Missing"
2020.emnlp-main.177,W02-1001,0,0.038122,"57.72 57.77 61.03 60.88 62.53* 62.54 All tokens Only Pronouns 36.05* 35.86* 35.97 36.09 Fine-tuning 26.13* 26.08* 27.56 27.50 Only Pronouns (a) Fine-tuning results for French-English 26.10* 26.01* 27.48 27.59 (b) Fine-tuning results for Czech-English Table 7: Results for experiments on generalizability to other source languages, Fr-En and Cs-En. * indicates results are statistically significant. 6 Related Work Our idea of conditional generative-discriminative training is related to the idea of discriminative training of generative models. Previously, this idea was proposed for Markov models. Collins (2002) trained a Hidden Markov Model (HMM) discriminatively for sequence tagging with structured perceptron algorithm. Yakhnenko et al. (2005) used a similar idea for sequence classification. In deep learning, the well-known generative adversarial networks (GANs) (Goodfellow et al., 2014) are an example where a generator is trained with the help of a discriminator. To the best of our knowledge, ours is the first work to explore this idea with conditional language models for guiding the model on what to generate and what not to generate. A few fine-tuning methods are related to our work. Abdulmumin e"
2020.emnlp-main.177,D18-1045,0,0.0276168,"models far behind. However, the availability of large corpora has been no small part of that success, with recent NMT models using millions of sentences for training. A lack of availability of such large parallel corpora across languages has given 1 Code available at &lt;https://github.com/ ntunlp/pronoun-finetuning&gt;. rise to methods utilizing large amounts of monolingual data, such as for backtranslation (Sennrich et al., 2016a), language modeling (C¸aglar G¨ulc¸ehre et al., 2017; Zheng et al., 2020), or for large-scale pre-training (Lewis et al., 2020). Backtranslation (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works proposing strategies to further improve it (Hoa; Yang et al., 2019; Caswell et al., 2019). However, recent studies ha"
2020.emnlp-main.177,2020.acl-main.253,0,0.0294285,"which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works proposing strategies to further improve it (Hoa; Yang et al., 2019; Caswell et al., 2019). However, recent studies have suggested that there is a limit beyond which the addition of synthetic data hurts the performance of the model (Fadaee and Monz, 2018; Poncelas et al., 2018). Also, recent work (Edunov et al., 2020; Nguyen et al., 2020) point out that back-translation suffers from the translationese effect, where back-translation only improves the performance when the source sentences are translationese but does not offer any improvement when the sentences are natural text. Automatic post-editing (APE) is another common strategy that is used to improve translations. APE models are commonly monolingual, and typically take the output from some MT model as input, which they then modify. In the absence of adequate human post-edited data to train data-hungry neural models, Voita et al. (2019) and Freitag et"
2020.emnlp-main.177,D19-1294,1,0.907816,"ntil convergence using this data, for our targeted fine-tuning of pronoun translations, we derive a subset of the training corpus D as follows: 1. Translate D using a baseline model M to obtain source to target translations TM . (7) 2. Align TM with reference R using efmaral ¨ (Ostling and Tiedemann, 2016). where µ is the margin; we use µ = 0.3. Note that the additional losses can be applied to all the tokens in the sequence, or restricted to some 3. Find pronoun translations in TM that do not match reference R. To exclude equivalent but non-identical translations, we use the list provided by Jwalapuram et al. (2019)3 . n Lmm 1X = max{0, µ − yˆt+ + yˆt− } n t=1 2 3 For the sake of simplicity, we omit τ in Eq. 3 - 4 2269 https://github.com/ntunlp/eval-anaphora 4. For each sentence with a mistranslated pronoun, extract the source sentences from S. 5. The corresponding source and reference sentences form the pronoun-targeted fine-tuning 0 0 subset, referred to as Dprn = (S , T ). 3.2 Baseline MT Models Typically, MT models are trained at the sentence level, taking one sentence as input and producing one sentence as output. Most MT systems at the sentence-level do not have access to adequate context that may"
2020.emnlp-main.177,W17-3204,0,0.0207918,"onal Linguistics model. This round-trip translated text is considered an approximation of poor quality MT output, which can be used in combination with the original target reference text to train the post-editing model. Voita et al. (2019) train a model to make corrections in context, using groups of 4 sentences as input, and show improvements in BLEU as well as translations of discourse phenomena. NMT models typically fail on rare words that may not be adequately seen during training, such as named entities, or on words whose interpretation depends on the context such as discourse phenomena (Koehn and Knowles, 2017; Sennrich, 2018). For the latter, NMT models tend to prefer a more typical alternative to a relatively rare but correct one (e.g., French “Il” is often wrongly translated to the more common “it” than “he” ). However, these seemingly trivial errors can erode translation to the extent that they can be easily distinguishable from human-translated texts (L¨aubli et al., 2018). There could be several reasons for why NMT models make such mistakes; our hypothesis is that since almost all NMT models are trained with a conditional language model objective, it is clear that this objective alone is prov"
2020.emnlp-main.177,D18-1512,0,0.0562913,"Missing"
2020.emnlp-main.177,2020.acl-main.703,0,0.0426314,"eft the previously successful statistical machine translation models far behind. However, the availability of large corpora has been no small part of that success, with recent NMT models using millions of sentences for training. A lack of availability of such large parallel corpora across languages has given 1 Code available at &lt;https://github.com/ ntunlp/pronoun-finetuning&gt;. rise to methods utilizing large amounts of monolingual data, such as for backtranslation (Sennrich et al., 2016a), language modeling (C¸aglar G¨ulc¸ehre et al., 2017; Zheng et al., 2020), or for large-scale pre-training (Lewis et al., 2020). Backtranslation (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works proposing strategies to further improve it (Hoa; Yang"
2020.emnlp-main.177,2015.iwslt-evaluation.11,0,0.0299922,"tune it on authentic parallel data, and show that it can improve 0.7 BLEU over backtranslation on English-Vietnamese. Fadaee and Monz (2018) use various sampling strategies to improve the results of backtranslation by targeting difficult-to-predict words based on prediction loss. Our strategy is similar in that we also try to target words that the model has trouble with, but we do not use additional data. A number of methods have been proposed for adapting a trained MT model to another domain by fine-tuning. A common strategy is to simply perform additional training on the new domain dataset (Luong and Manning, 2015) or use a mix of in-domain and out-domain data for fine-tuning without loss of generalization (Chu et al., 2017) or upweight out-of-domain data (Wang et al., 2017). There has been some work on targeted improvement of translations, specifically for named-entities. Ugawa et al. (2018) adapt MT network architecture to encode named entity features and tags while Li et al. (2018) perform domain adaptation in addition to feature encoding. With respect to discourse phenomena, Stojanovski and Fraser (2019) propose a curriculum learning based approach, where a context-aware model is trained on randomly"
2020.emnlp-main.177,D18-1325,0,0.0183903,"l model as our baselines: S EN 2S EN: A standard 6-layer base Transformer model (Vaswani et al., 2017) trained to translate each sentence independently. C ONCAT: A standard 6-layer base Transformer trained to translate a sentence given one previous sentence as context (Tiedemann and Scherrer, 2017). The input to the model is the previous sentence and the current sentence combined with a special separator character. Jwalapuram et al. (2020) show that this simple context model performs comparably or better than other elaborate contextual models like Voita et al. (2018), Zhang et al. (2018), and Miculicich et al. (2018). Both the baseline models are trained for 100,000 steps. Other parameter details are in the Appendix. 4 Experiments We conduct our fine-tuning experiments on the German-English (De-En) translation task. We describe our baseline training and fine-tuning corpus (§4.1), our experiments and results on fine-tuning using only the targeted subset data (§4.4), and finetuning using both the targeted subset data and the hybrid training losses (§4.5). 4.1 MT Training Data Baseline training corpus. We use a De-En training dataset consisting of about 2.5 million sentence pairs, taken from the News Comment"
2020.emnlp-main.177,W19-6614,0,0.0155851,"by fine-tuning. A common strategy is to simply perform additional training on the new domain dataset (Luong and Manning, 2015) or use a mix of in-domain and out-domain data for fine-tuning without loss of generalization (Chu et al., 2017) or upweight out-of-domain data (Wang et al., 2017). There has been some work on targeted improvement of translations, specifically for named-entities. Ugawa et al. (2018) adapt MT network architecture to encode named entity features and tags while Li et al. (2018) perform domain adaptation in addition to feature encoding. With respect to discourse phenomena, Stojanovski and Fraser (2019) propose a curriculum learning based approach, where a context-aware model is trained on randomly sampled oracle data containing gold-standard pronouns. In our work, we focus on the baseline model’s failings and try to increase its learning capacity by proposing additional losses. Most recent work on improving pronoun translations has involved building more complex architectures that incorporate contextual information (Voita et al., 2018; Wong et al., 2020). In contrast, we present a more generalized approach. 7 Conclusions and Future Work We have proposed a class of conditional generativedisc"
2020.emnlp-main.177,tiedemann-2012-parallel,0,0.0582986,"00 steps. Other parameter details are in the Appendix. 4 Experiments We conduct our fine-tuning experiments on the German-English (De-En) translation task. We describe our baseline training and fine-tuning corpus (§4.1), our experiments and results on fine-tuning using only the targeted subset data (§4.4), and finetuning using both the targeted subset data and the hybrid training losses (§4.5). 4.1 MT Training Data Baseline training corpus. We use a De-En training dataset consisting of about 2.5 million sentence pairs, taken from the News Commentary, IWSLT (Cettolo et al., 2012) and Europarl (Tiedemann, 2012) corpora.4 Sentences are encoded through 4 We exclude the UN corpus as our analysis showed that it does not have a high incidence of pronouns. WMT14 Model S EN 2S EN C ONCAT Pronoun Testset Train BLEU BLEU P R F1 D D 31.64 31.81 35.56 36.16 77.92 80.39 66.01 68.49 69.55 72.03 Table 1: Baseline BLEU results on the WMT14 DeEn testset and the BLEU (for translation), Precision, Recall and F1 scores (for pronoun translations) on the pronoun testset from Jwalapuram et al. (2019). Byte-Pair Encoding (BPE) (Sennrich et al., 2016b) with 40,000 operations, which results in a shared vocabulary of 40,224"
2020.emnlp-main.177,W17-4811,0,0.0194249,"ence as input and producing one sentence as output. Most MT systems at the sentence-level do not have access to adequate context that may be required for the translation of pronouns (Sennrich, 2018). Since it is our aim to improve pronoun translation, we train both a sentence-level model and a simple concatenationbased contextual model as our baselines: S EN 2S EN: A standard 6-layer base Transformer model (Vaswani et al., 2017) trained to translate each sentence independently. C ONCAT: A standard 6-layer base Transformer trained to translate a sentence given one previous sentence as context (Tiedemann and Scherrer, 2017). The input to the model is the previous sentence and the current sentence combined with a special separator character. Jwalapuram et al. (2020) show that this simple context model performs comparably or better than other elaborate contextual models like Voita et al. (2018), Zhang et al. (2018), and Miculicich et al. (2018). Both the baseline models are trained for 100,000 steps. Other parameter details are in the Appendix. 4 Experiments We conduct our fine-tuning experiments on the German-English (De-En) translation task. We describe our baseline training and fine-tuning corpus (§4.1), our ex"
2020.emnlp-main.177,C18-1274,0,0.0230992,"ategy is similar in that we also try to target words that the model has trouble with, but we do not use additional data. A number of methods have been proposed for adapting a trained MT model to another domain by fine-tuning. A common strategy is to simply perform additional training on the new domain dataset (Luong and Manning, 2015) or use a mix of in-domain and out-domain data for fine-tuning without loss of generalization (Chu et al., 2017) or upweight out-of-domain data (Wang et al., 2017). There has been some work on targeted improvement of translations, specifically for named-entities. Ugawa et al. (2018) adapt MT network architecture to encode named entity features and tags while Li et al. (2018) perform domain adaptation in addition to feature encoding. With respect to discourse phenomena, Stojanovski and Fraser (2019) propose a curriculum learning based approach, where a context-aware model is trained on randomly sampled oracle data containing gold-standard pronouns. In our work, we focus on the baseline model’s failings and try to increase its learning capacity by proposing additional losses. Most recent work on improving pronoun translations has involved building more complex architecture"
2020.emnlp-main.177,D19-1081,0,0.0277948,"Missing"
2020.emnlp-main.177,P18-1117,0,0.0624915,"odel and a simple concatenationbased contextual model as our baselines: S EN 2S EN: A standard 6-layer base Transformer model (Vaswani et al., 2017) trained to translate each sentence independently. C ONCAT: A standard 6-layer base Transformer trained to translate a sentence given one previous sentence as context (Tiedemann and Scherrer, 2017). The input to the model is the previous sentence and the current sentence combined with a special separator character. Jwalapuram et al. (2020) show that this simple context model performs comparably or better than other elaborate contextual models like Voita et al. (2018), Zhang et al. (2018), and Miculicich et al. (2018). Both the baseline models are trained for 100,000 steps. Other parameter details are in the Appendix. 4 Experiments We conduct our fine-tuning experiments on the German-English (De-En) translation task. We describe our baseline training and fine-tuning corpus (§4.1), our experiments and results on fine-tuning using only the targeted subset data (§4.4), and finetuning using both the targeted subset data and the hybrid training losses (§4.5). 4.1 MT Training Data Baseline training corpus. We use a De-En training dataset consisting of about 2.5"
2020.emnlp-main.177,P16-1009,0,0.170342,"ent of neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017) brought about significant improvements that left the previously successful statistical machine translation models far behind. However, the availability of large corpora has been no small part of that success, with recent NMT models using millions of sentences for training. A lack of availability of such large parallel corpora across languages has given 1 Code available at &lt;https://github.com/ ntunlp/pronoun-finetuning&gt;. rise to methods utilizing large amounts of monolingual data, such as for backtranslation (Sennrich et al., 2016a), language modeling (C¸aglar G¨ulc¸ehre et al., 2017; Zheng et al., 2020), or for large-scale pre-training (Lewis et al., 2020). Backtranslation (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and ha"
2020.emnlp-main.177,D17-1155,0,0.0229554,"ies to improve the results of backtranslation by targeting difficult-to-predict words based on prediction loss. Our strategy is similar in that we also try to target words that the model has trouble with, but we do not use additional data. A number of methods have been proposed for adapting a trained MT model to another domain by fine-tuning. A common strategy is to simply perform additional training on the new domain dataset (Luong and Manning, 2015) or use a mix of in-domain and out-domain data for fine-tuning without loss of generalization (Chu et al., 2017) or upweight out-of-domain data (Wang et al., 2017). There has been some work on targeted improvement of translations, specifically for named-entities. Ugawa et al. (2018) adapt MT network architecture to encode named entity features and tags while Li et al. (2018) perform domain adaptation in addition to feature encoding. With respect to discourse phenomena, Stojanovski and Fraser (2019) propose a curriculum learning based approach, where a context-aware model is trained on randomly sampled oracle data containing gold-standard pronouns. In our work, we focus on the baseline model’s failings and try to increase its learning capacity by proposi"
2020.emnlp-main.177,P16-1162,0,0.353869,"ent of neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017) brought about significant improvements that left the previously successful statistical machine translation models far behind. However, the availability of large corpora has been no small part of that success, with recent NMT models using millions of sentences for training. A lack of availability of such large parallel corpora across languages has given 1 Code available at &lt;https://github.com/ ntunlp/pronoun-finetuning&gt;. rise to methods utilizing large amounts of monolingual data, such as for backtranslation (Sennrich et al., 2016a), language modeling (C¸aglar G¨ulc¸ehre et al., 2017; Zheng et al., 2020), or for large-scale pre-training (Lewis et al., 2020). Backtranslation (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and ha"
2020.emnlp-main.177,2020.acl-main.530,0,0.0409199,"and tags while Li et al. (2018) perform domain adaptation in addition to feature encoding. With respect to discourse phenomena, Stojanovski and Fraser (2019) propose a curriculum learning based approach, where a context-aware model is trained on randomly sampled oracle data containing gold-standard pronouns. In our work, we focus on the baseline model’s failings and try to increase its learning capacity by proposing additional losses. Most recent work on improving pronoun translations has involved building more complex architectures that incorporate contextual information (Voita et al., 2018; Wong et al., 2020). In contrast, we present a more generalized approach. 7 Conclusions and Future Work We have proposed a class of conditional generativediscriminative losses to increase the learning potential of NMT models, showing that it is possible to leverage “unlearned” training data to further improve an MT model, by strategically filtering the data and applying additional targeted losses. We demonstrated the effectiveness of our methods on different languages and testsets, also reporting improved pronoun translations. Although we focus on pronoun translations, our fine-tuning method is generic and can b"
2020.emnlp-main.177,D18-1049,0,0.0363087,"Missing"
2020.emnlp-main.177,2020.wmt-1.63,0,0.0184456,"., 2017) brought about significant improvements that left the previously successful statistical machine translation models far behind. However, the availability of large corpora has been no small part of that success, with recent NMT models using millions of sentences for training. A lack of availability of such large parallel corpora across languages has given 1 Code available at &lt;https://github.com/ ntunlp/pronoun-finetuning&gt;. rise to methods utilizing large amounts of monolingual data, such as for backtranslation (Sennrich et al., 2016a), language modeling (C¸aglar G¨ulc¸ehre et al., 2017; Zheng et al., 2020), or for large-scale pre-training (Lewis et al., 2020). Backtranslation (Sennrich et al., 2016a; Edunov et al., 2018) is a commonly used strategy to improve MT models in the absence of adequate parallel data for training. A target-to-source model is first trained using the available parallel data, which is then used to translate a large target-monolingual corpus into the source to create pseudo-parallel data for training a source-to-target MT model. This has been shown to result in improvements in the BLEU score, and has become a popular method for improving NMT models, with many recent works"
2020.emnlp-main.191,N18-1016,0,0.050044,"Missing"
2020.emnlp-main.191,N18-2097,0,0.0233528,"n: Is this loan suitable for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly buil"
2020.emnlp-main.191,N19-1423,0,0.0191646,", 2019) or coarse-to-fine reasoning (Gao et al., 2020). However, we find that such sophisticated modelings may not be necessary, and we propose a simple but effective approach here. We split the rule text into sentences and concatenate the rule sentences and user-provided information into a sequence. Then we use RoBERTa to encode them into vectors grounded to tokens, as here we want to predict the position of a span within the rule text. Let [t1,1 , ..., t1,s1 ; t2,1 , ..., t2,s2 ; ...; tN,1 , ..., tN,sN ] be the encoded vectors for tokens from N rule sentences, we follow the BERTQA approach (Devlin et al., 2019) to learn a start vector ws ∈ Rd and an end vector we ∈ Rd to locate the start and end positions, under the restriction that the start and end positions must belong to the same rule sentence: Span = arg max(ws&gt; tk,i + we&gt; tk,j ) (10) i,j,k where i, j denote the start and end positions of the selected span, and k is the sentence which the span belongs to. The training objective is the sum of the log-likelihoods of the correct start and end positions. To supervise the span extraction process, the noisy supervision of spans are generated by selecting the span which has the minimum edit distance w"
2020.emnlp-main.191,2020.acl-main.88,1,0.759919,"(Saeidi et al., 2018). In this case, the machine needs to read the rule text, interpret the user scenario, clarify the unknown user’s background by asking questions, and derive the final answer. Taking Figure 1 as an example, to answer the user whether he is suitable for the loan program, the machine needs to interpret the rule text to know what are the requirements, understand he meets “American small business” from the user scenario, ask follow-up clarification questions about “for-profit business” and “not get financing Existing approaches (Zhong and Zettlemoyer, 2019; Sharma et al., 2019; Gao et al., 2020) decompose this problem into two sub-tasks. Given the rule text, user question, user scenario, and dialog history (if any), the first sub-task is to make a decision among “Yes”, “No”, “Inquire” and “Irrelevant”. The “Yes/No” directly answers the user question and “Irrelevant” means the user question is unanswerable by the rule text. If the user-provided information (user scenario, previous dialogs) are not enough to determine his fulfillment or eligibility, an “Inquire” decision is made and the second sub-task is activated. The second sub-task is to capture the underspecified condition from th"
2020.emnlp-main.191,P19-4003,1,0.816408,"was flooded Question: Is this loan suitable for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D IS"
2020.emnlp-main.191,J15-3002,1,0.901989,"Missing"
2020.emnlp-main.191,D19-1001,0,0.365184,"ard deviations. It takes two hours for training on a 4-core server with an Nvidia GeForce GTX Titan X GPU. 3.2 Results Decision Making Sub-task. The decision making results in macro- and micro- accuracy on the blind, held out test set of ShARC are shown in Table 1. D ISCERN outperforms the previous best 2443 End-to-End Task (Leaderboard Performance) Micro Acc. Macro Acc. BLEU1 BLEU4 Seq2Seq (Saeidi et al., 2018) 44.8 42.8 34.0 7.8 Pipeline (Saeidi et al., 2018) 61.9 68.9 54.4 34.4 BERTQA (Zhong and Zettlemoyer, 2019) 63.6 70.8 46.2 36.3 UrcaNet (Sharma et al., 2019) 65.1 71.2 60.5 46.1 BiSon (Lawrence et al., 2019) 66.9 71.6 58.8 44.3 E3 (Zhong and Zettlemoyer, 2019) 67.6 73.3 54.1 38.7 EMT (Gao et al., 2020) 69.4 74.8 60.9 46.0 EMT+entailment (Gao et al., 2020) 69.1 74.6 63.9 49.5 D ISCERN (our single model) 73.2 78.3 64.0 49.1 Models Table 1: Performance on the blind, held-out test set of ShARC end-to-end task. Models BERTQA E3 UrcaNet EMT D ISCERN Yes 61.2 65.9 63.3 70.5 71.9 No 61.0 70.6 68.4 73.2 75.8 Inq. 62.6 60.5 58.9 70.8 73.3 Models E3 E3 +UniLM EMT D ISCERN (BERT) D ISCERN Irr. 96.4 96.4 95.7 98.6 99.3 Table 2: Class-wise decision prediction accuracy among “Yes”, “No”, “Inquire” and “Irreleva"
2020.emnlp-main.191,D19-5808,0,0.0270142,"Missing"
2020.emnlp-main.191,2021.ccl-1.108,0,0.109338,"Missing"
2020.emnlp-main.191,D19-1257,0,0.0448277,"GIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly builds the connection between entailment states of conditions and the final decisions. Results on the ShARC benchmark shows that D ISCERN o"
2020.emnlp-main.191,P02-1040,0,0.107488,"text, user question, user scenario, and dialog history (if any). The output is the answer among Yes, No, Irrelevant, or a follow-up question. The train, development, and test dataset sizes are 21890, 2270, and 8276, respectively. Evaluation Metrics. The decision making subtask uses macro- and micro- accuracy of four classes “Yes”, “No”, “Irrelevant”, “Inquire” as metrics. For the question generation sub-task, we evaluate models under both the official end-to-end setting (Saeidi et al., 2018) and the recently proposed oracle setting (Gao et al., 2020). In the official setting, the BLEU score (Papineni et al., 2002) is calculated only when both the ground truth decision and the predicted decision are “Inquire”, which makes the score dependent on the model’s “Inquire” predictions. For the oracle question generation setting, models are asked to generate a question when the ground truth decision is “Inquire”. Implementation Details. For the decision making sub-task, we finetune RoBERTa-base model (Wolf et al., 2019) with Adam (Kingma and Ba, 2015) optimizer for 5 epochs with a learning rate of 5e-5, a warm-up rate of 0.1, a batch size of 16, and a dropout rate of 0.35. The number of inter-sentence transform"
2020.emnlp-main.191,D18-1233,0,0.181667,"Missing"
2020.emnlp-main.191,2020.acl-main.451,0,0.107211,"able for me? Entailment State Gold: Entailment; Predict: Unknown 27 Relevant Rule: The Montgomery GI Bill (MGIB) is an educational assistance program enacted by Congress to attract high quality men and women into the Armed Forces. Scenario: I applied and found out I can get a loan. My dad wants me to join the army, but I don’t. I’d rather go to school. Question: Does this program meet my needs? Entailment State Gold: Contradiction; Predict: Unknown Figure 3: Types of scenario interpretation errors in the development data based on 100 samples. marization (Joty et al., 2019; Cohan et al., 2018; Xu et al., 2020). Recently, discourse information has also been introduced in neural reading comprehension. Mihaylov and Frank (2019) design a discourse-aware semantic self-attention mechanism to supervise different heads of the transformer by discourse relations and coreferring mentions. Different from their use of discourse information, we use it as a parser to segment surface-level in-line conditions for entailment reasoning. 5 Conclusion In this paper, we present D ISCERN, a system that does discourse-aware entailment reasoning for conversational machine reading. D ISCERN explicitly builds the connection"
2020.emnlp-main.191,P19-1223,0,0.148231,"Missing"
2020.emnlp-main.215,D18-1214,0,0.117394,"linear mapping superior to non-linear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and does not fail for any language pair. Other concurrent work (Ormazabal et al., 2019; Doval et al., 2019) also advocates for"
2020.emnlp-main.215,D16-1250,0,0.0327911,"ction to transform the source embedding space to the target language by minimizing the squared Euclidean distance between the translation pairs of a seed dictionary. They assume that the similarity of geometric arrangements in the embedding spaces is the key reason for their method to succeed as they found linear mapping superior to non-linear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest t"
2020.emnlp-main.215,P17-1042,0,0.358056,"produce meaningful results on this dataset. English, Italian, and German embeddings were trained on WacKy crawling corpora using CBOW (Mikolov et al., 2013b), while Spanish and Finnish embeddings were trained on WMT News Crawl and Common Crawl, respectively. 4.2 Baseline Methods We compare our proposed LNM AP with several existing methods comprising supervised, semisupervised, and unsupervised models. For each baseline model, we conduct experiments with the publicly available code. In the following, we give a brief description of the baseline models. Supervised & Semi-supervised Methods. (a) Artetxe et al. (2017) propose a self-learning framework that performs two steps iteratively until 3 https://github.com/facebookresearch/MUSE 2716 https://github.com/artetxem/vecmap/ convergence. In the first step, they use the dictionary (starting with the seed dictionary) to learn a linear mapping, which is then used in the second step to induce a new dictionary. They use linear autoencoders in their model, and the mappers are also linear. (b) Artetxe et al. (2018a) propose a multi-step framework that generalizes previous studies. Their framework consists of several steps: whitening, orthogonal mapping, re-weight"
2020.emnlp-main.215,P18-1073,0,0.0707327,"Missing"
2020.emnlp-main.215,J82-2005,0,0.710577,"Missing"
2020.emnlp-main.215,W16-1614,0,0.0731981,"Missing"
2020.emnlp-main.215,Q17-1010,0,0.396311,"en layers and tanh in the final layer of the decoder (Eq. 6). We use linear activations in the output layer of the encoder (Eq. 3). We train autoenc`x with l2 reconstruction loss as: Lautoenc`x (ΘE`x , ΘD`x ) = LNM AP Semi-supervised Framework D h1 `x = φ(θ3 `x zxi ) nx 1 X kxi − x ˆi k2 (7) nx i=1 Let V`x ={vx1 , ..., vxnx } and V`y ={vy1 , ..., vyny } be two sets of vocabulary consisting of nx and ny words for a source (`x ) and a target (`y ) language, respectively. Each word vxi (resp. vyj ) has an embedding xi ∈ Rd (resp. yj ∈ Rd ), trained with any word embedding models, e.g., FastText (Bojanowski et al., 2017). Let E`x ∈ Rnx ×d and E`y ∈ Rny ×d be the word embedding matrices for the source and target languages, respectively. We are also given with a seed dictionary D ={(x1 , y1 ), ..., (xk , yk )} with k word pairs. Our objective is to learn a transformation function M such that for any vxi ∈ V`x , M(xi ) corresponds to its translation yj , where vyj ∈ V`y . Our approach LNM AP (Figure 1) follows two sequential steps: (i) Unsupervised latent space induction using monolingual autoencoders (§3.1), and (ii) Supervised non-linear transformation learning with back-translation and source embedding recons"
2020.emnlp-main.215,2020.lrec-1.495,0,0.0486011,"Missing"
2020.emnlp-main.215,E17-1102,0,0.0678771,"Missing"
2020.emnlp-main.215,D18-1043,0,0.0384371,"succeed as they found linear mapping superior to non-linear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and does not fail for any language pair. Other concurrent work (Ormazabal et al., 2019; Dova"
2020.emnlp-main.215,Q19-1007,0,0.604798,"onsider K = 15000 most frequent words from the source and target languages. For dictionary update, we set C = 2000. (c) Conneau et al. (2018) compare their unsupervised model with a supervised baseline that learns an orthogonal mapping between the embedding spaces by iterative Procrustes refinement. They also propose CSLS for nearest neighbour search. (d) Joulin et al. (2018) show that minimizing a convex relaxation of the CSLS loss significantly improves the quality of bilingual word vector alignment. Their method achieves state-of-the-art results for many languages (Patra et al., 2019). (e) Jawanpuria et al. (2019) propose a geometric approach where they decouple CLWE learning into two steps: (i) learning rotations for languagespecific embeddings to align them to a common space, and (ii) learning a similarity metric in the common space to model similarities between the embeddings of the two languages. (f) Patra et al. (2019) propose a semi-supervised technique that relaxes the isomorphic assumption while leveraging both seed dictionary pairs and a larger set of unaligned word embeddings. Unsupervised Methods. (a) Conneau et al. (2018) are the first to show impressive results for unsupervised word transl"
2020.emnlp-main.215,D18-1330,0,0.406919,"Missing"
2020.emnlp-main.215,N19-1386,1,0.363308,"ear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and does not fail for any language pair. Other concurrent work (Ormazabal et al., 2019; Doval et al., 2019) also advocates for weak supervision in CLWE"
2020.emnlp-main.215,2020.cl-2.2,1,0.857286,"Missing"
2020.emnlp-main.215,P18-2036,0,0.271987,"ap/. 2 Background Limitations of Isomorphic Assumption. Almost all CLWE methods inherently assume that embedding spaces of different languages are approximately isomorphic (i.e., similar in geometric structure). However, recently researchers have questioned this simplified assumption and attributed the performance degradation of existing CLWE methods to the strong mismatches in embedding spaces caused by the linguistic and domain divergences (Søgaard et al., 2019; Ormazabal et al., 2019). Søgaard et al. (2018) empirically show that even closely related languages are far from being isomorphic. Nakashole and Flauger (2018) argue that mapping between embedding spaces of different languages can be approximately linear only at small local regions, but must be non-linear globally. Patra et al. (2019) also recently show that etymologically distant language pairs cannot be aligned properly using orthogonal transformations. Towards Semi-supervised Methods. A number of recent studies have questioned the robustness of existing unsupervised CLWE methods (Ruder et al., 2019). Vuli´c et al. (2019) show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs; it gives"
2020.emnlp-main.215,P19-1492,0,0.262982,"2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and does not fail for any language pair. Other concurrent work (Ormazabal et al., 2019; Doval et al., 2019) also advocates for weak supervision in CLWE methods. Almost all mapping-based CLWE methods, supervised and unsupervised alike, solve the Procrustes problem in the final step or during selflearning (Ruder et al., 2019). This restricts the transformation to be orthogonal linear mappings. 2712 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2712–2723, c November 16–20, 2020. 2020 Association for Computational Linguistics However, learning an orthogonal linear mapping inherently assumes that the embedding spaces of different langu"
2020.emnlp-main.215,P19-1018,0,0.0588248,"ings. 2712 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2712–2723, c November 16–20, 2020. 2020 Association for Computational Linguistics However, learning an orthogonal linear mapping inherently assumes that the embedding spaces of different languages exhibit similar geometric structures (i.e., approximately isomorphic). Several recent studies have questioned this strong assumption and empirically showed that the isomorphic assumption does not hold in general even for two closely related languages like English and German (Søgaard et al., 2018; Patra et al., 2019). In this work, we propose LNM AP (Latent space Non-linear Mapping), a novel semi-supervised approach that uses non-linear mapping in the latent space to learn CLWE. It uses minimal supervision from a seed dictionary, while leveraging semantic information from the monolingual word embeddings. As shown in Figure 1, LNM AP comprises two autoencoders, one for each language. The auto-encoders are first trained independently in a self-supervised way to induce the latent code space of the respective languages. Then, we use a small seed dictionary to learn the non-linear mappings between the two code"
2020.emnlp-main.215,D18-1268,0,0.0576599,"r their method to succeed as they found linear mapping superior to non-linear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and does not fail for any language pair. Other concurrent work (Ormaz"
2020.emnlp-main.215,P17-1179,0,0.0553754,"ometric arrangements in the embedding spaces is the key reason for their method to succeed as they found linear mapping superior to non-linear mappings with multi-layer neural networks. Subsequent studies propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the linear mapper, modifying the objective function, and reducing the seed dictionary size (Artetxe et al., 2016, 2017, 2018a; Smith et al., 2017). A more recent line of research attempts to eliminate the seed dictionary totally and learn the mapping in a purely unsupervised way (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Artetxe et al., 2018b; Xu et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Mohiuddin and Joty, 2019, 2020). While not requiring any cross-lingual supervision makes these methods attractive, Vuli´c et al. (2019) recently show that even the most robust unsupervised method (Artetxe et al., 2018b) fails for a large number of language pairs. They suggest to rethink the main motivations behind fully unsupervised methods showing that with a small seed dictionary (500-1K pairs) their semi-supervised method always outperforms the unsupervised method and doe"
2020.emnlp-main.215,D19-1449,0,0.0968985,"Missing"
2020.emnlp-main.215,P18-1072,0,0.210303,"Missing"
2020.emnlp-main.215,P19-4007,0,\N,Missing
2020.emnlp-main.269,D19-1209,0,0.175531,"estions through multiple rounds of interactions together with visual content understanding. The primary research direction in VisDial has been mostly focusing on developing various attention mechanisms (Bahdanau et al., 2015) for a bet* This work was mainly done when Yue Wang was an intern at Salesforce Research Asia, Singapore. ter fusion of vision and dialog contents. Compared to VQA that predicts an answer based only on the question about the image (Figure 1(a)), VisDial needs to additionally consider the dialog history. Typically, most of previous work (Niu et al., 2019; Gan et al., 2019; Kang et al., 2019) uses the question as a query to attend to relevant image regions and dialog history, where their interactions are usually exploited to obtain better visual-historical cues for predicting the answer. In other words, the attention flow in these methods is unidirectional – from question to the other components (Figure 1(b)). By contrast, in this work, we allow for bidirectional attention flow between all the entities using a unified Transformer (Vaswani et al., 2017) encoder, as shown in Figure 1(c). In this way, all the entities simultaneously play the role of an “information seeker” (query) an"
2020.emnlp-main.269,Q14-1006,0,\N,Missing
2020.emnlp-main.269,D14-1086,0,\N,Missing
2020.emnlp-main.269,P18-1238,0,\N,Missing
2020.emnlp-main.269,N19-1423,0,\N,Missing
2020.emnlp-main.269,P19-1644,0,\N,Missing
2020.emnlp-main.455,P18-2006,0,0.0422403,"ction by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to d"
2020.emnlp-main.455,D18-1366,0,0.0307226,"as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fi"
2020.emnlp-main.455,P19-1425,0,0.0331759,"pose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to detect adversaries and recover clean examples. Jia et al. (2019) and Huang et al. (2019)"
2020.emnlp-main.455,W02-1001,0,0.0335196,"arn each inflection’s grammatical role more quickly. This is because the model does not need to first learn that the same grammatical category can manifest in orthographically different forms. Crucially, the original sentence can usually be reconstructed from the base forms and grammatical information preserved by the inflection symbols, except in cases of overabundance (Thornton, 2019). Implementation details. We use the BertPreTokenizer from the tokenizers6 library for whitespace and punctuation splitting. We use the NLTK (Bird et al., 2009) implementation of the averaged perceptron tagger (Collins, 2002) with greedy decoding to generate POS tags, which serve to improve lemmatization accuracy and as inflection symbols. For lemmatization and reinflection, we use lemminflect7 , which uses a dictionary look-up together with rules for lemmatizing and inflecting words. A benefit of this approach is that the neural network can now generate orthographically appropriate inflected forms by generating the base form and the corresponding inflection symbol. 3.2 Compatibility with Data-Driven Methods Although BITE has the numerous advantages outlined above, it suffers from the same weakness as regular word"
2020.emnlp-main.455,W02-0603,0,0.0426279,"en subword tokenizer such as byte pair encoding (BPE; Sennrich et al. (2016)). However, a purely data-driven approach may fail to find the optimal encoding, both in terms of vocabulary efficiency and cross-dialectal generalization. This could make the neural model more vulnerable to inflectional perturbations. Hence, we: • Propose Base-InflecTion Encoding (BITE), which uses morphological information to help the data-driven tokenizer use its vocabulary efficiently and generate robust symbol3 sequences. In contrast to morphological segmentors such as Linguistica (Goldsmith, 2000) and Morfessor (Creutz and Lagus, 2002), we reduce inflected forms to their base forms before reinjecting the inflection information into the encoded sequence as special symbols. This approach gracefully handles the canonicalization of words with nonconcatenative morphology while generally allowing the original sentence to be reconstructed. • Demonstrate BITE’s effectiveness at making neural NLP systems robust to non-standard inflection use while preserving performance on Standard English examples. Crucially, simply fine-tuning the pretrained model for the downstream task after adding BITE is sufficient. Unlike adversarial training"
2020.emnlp-main.455,D19-1423,0,0.0676907,"Missing"
2020.emnlp-main.455,P16-2090,0,0.0626025,"Missing"
2020.emnlp-main.455,D07-1091,0,0.120448,"19) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples,"
2020.emnlp-main.455,W17-3204,0,0.0141092,"tion of subwords in the vocabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To"
2020.emnlp-main.455,W18-6301,0,0.025453,"with nonconcatenative morphology while generally allowing the original sentence to be reconstructed. • Demonstrate BITE’s effectiveness at making neural NLP systems robust to non-standard inflection use while preserving performance on Standard English examples. Crucially, simply fine-tuning the pretrained model for the downstream task after adding BITE is sufficient. Unlike adversarial training, BITE does not enlarge the dataset and is more computationally efficient. • Show that BITE helps BERT (Devlin et al., 2019) generalize to dialects unseen during training and also helps Transformer-big (Ott et al., 2018) converge faster for the WMT’14 En-De task. • Propose metrics like symbol complexity to operationalize and evaluate the vocabulary efficiency of an encoding scheme. Our metrics are generic and can be used to evaluate any tokenizer. 2 Related Work Subword tokenization. Before neural models can learn, raw text must first be encoded into symbols with the help of a fixed-size vocabulary. Early 3 Following Sennrich et al. (2016), we use symbol instead of token to avoid confusion with the unencoded word token. models represented each word as a single symbol in the vocabulary (Bengio et al., 2001; Co"
2020.emnlp-main.455,P02-1040,0,0.107697,"Missing"
2020.emnlp-main.455,N19-1326,0,0.0299045,"versarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 2018; Cheng et al., 2019). However, this generally involves retraining the model with the adversarial data, which is computationally expensive and time-consuming. Tan et al. (2020) showed that simply fine-tuning a trained model for a single epoch on appropriately generated adversarial training data is sufficient to harden the model against inflectional adversaries. Instead of adversarial training, Piktus et al. (2019) train word embeddings to be robust to misspellings, while Zhou et al. (2019b) propose using a BERT-based model to detect adversaries and recover clean examples. Jia et al. (2019) and Huang et al. (2019) use Interval Bound Propagation to train provably robust pre-Transformer models, while Shi et al. (2020) propose an efficient algorithm for training certifiably robust Transformer architectures. Summary. Popular subword tokenizers operate on surface forms in a purely data-driven manner. Existing adversarial robustness methods for largescale Transformers are computationally expensive, while prov"
2020.emnlp-main.455,P18-2124,0,0.0283581,"+ WP (+1 epoch) 79.07 75.46 72.21 72.56 74.45 73.69 68.23 70.66 83.86 82.21 83.87 81.05 83.86 83.36 75.77 81.04 Table 1: BERTbase results on the clean and adversarial MultiNLI and SQuAD 2.0 examples. We compare BITE+WordPiece to both WordPiece alone and with one epoch of adversarial fine-tuning. For fair comparison with adversarial fine-tuning, we trained the BITE+WordPiece model for an extra epoch (bottom) on clean data. 4.1 Adversarial Robustness (Classification) We evaluate BITE’s ability to improve model robustness for question answering and natural language understanding using SQuAD 2.0 (Rajpurkar et al., 2018) and MultiNLI (Williams et al., 2018), respectively. We use M ORPHEUS (Tan et al., 2020), an adversarial attack targeting inflectional morphology, to test the overall system’s robustness to non-standard inflections. They previously demonstrated M ORPHEUS’s ability to generate plausible and semantically equivalent adversarial examples resembling L2 English sentences. We attack each BERTbase model separately and report F1 scores on the answerable questions and the full SQuAD 2.0 dataset, following Tan et al. (2020). In addition, for MNLI, we report scores for both the in-domain (MNLI) and out-of"
2020.emnlp-main.455,P18-1079,0,0.19352,"odels, such as BERTScore (Zhang et al., 2020). In particular, Tan et al. (2020) show that current question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing work on adversarial robustness for NLP primarily focuses on adversarial training methods (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Tan et al., 2020) or classifying and correcting adversarial examples (Zhou et al., 2019a). However, this effectively increases the size of the training dataset by including adversarial examples or training a new model to identify and correct perturbations, thereby significantly increasing the overall computational cost of creating robust models. These approaches also only operate on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word t"
2020.emnlp-main.455,P06-1001,0,0.0716823,"cabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems agains"
2020.emnlp-main.455,2020.acl-main.240,0,0.044966,"Missing"
2020.emnlp-main.455,P16-1162,0,0.287998,"te on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word tokens that separates base from inflection. This improves both model robustness and vocabulary efficiency by explicitly inducing linguistic structure in the input to the NLP system (Erdmann et al., 2019; Henderson, 2020). Many extant NLP systems use a combination of a whitespace and punctuation tokenizer followed by a data-driven subword tokenizer such as byte pair encoding (BPE; Sennrich et al. (2016)). However, a purely data-driven approach may fail to find the optimal encoding, both in terms of vocabulary efficiency and cross-dialectal generalization. This could make the neural model more vulnerable to inflectional perturbations. Hence, we: • Propose Base-InflecTion Encoding (BITE), which uses morphological information to help the data-driven tokenizer use its vocabulary efficiently and generate robust symbol3 sequences. In contrast to morphological segmentors such as Linguistica (Goldsmith, 2000) and Morfessor (Creutz and Lagus, 2002), we reduce inflected forms to their base forms befor"
2020.emnlp-main.455,2020.acl-main.263,1,0.815286,"er to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE. Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers. Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.1 1 Introduction Large-scale neural models have proven successful at a wide range of natural language processing (NLP) tasks but are susceptible to amplifying discrimination against minority linguistic communities (Hovy and Spruit, 2016; Tan et al., 2020) due to selection bias in the training data and model overamplification (Shah et al., 2019). Most datasets implicitly assume a distribution of error-free Standard English speakers, but this does not accurately reflect the majority of the global English speaking population who are either second language (L2) or non-standard dialect speakers (Crystal, 2003; Eberhard et al., 2019). These World Englishes differ at lexical, morphological, and syntactic levels (Kachru et al., 2009); sensitivity to 1 Code will be available at github.com/salesforce/bite. Figure 1: Base-Inflection Encoding reduces infl"
2020.emnlp-main.455,W17-1606,0,0.0164059,"ither second language (L2) or non-standard dialect speakers (Crystal, 2003; Eberhard et al., 2019). These World Englishes differ at lexical, morphological, and syntactic levels (Kachru et al., 2009); sensitivity to 1 Code will be available at github.com/salesforce/bite. Figure 1: Base-Inflection Encoding reduces inflected words to their base forms, then reinjects the grammatical information into the sentence as inflection symbols. these variations predisposes English NLP systems to discriminate against speakers of World Englishes by either misunderstanding or misinterpreting them (Hern, 2017; Tatman, 2017). Left unchecked, these biases could inadvertently propagate to future models via metrics built around pretrained models, such as BERTScore (Zhang et al., 2020). In particular, Tan et al. (2020) show that current question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing"
2020.emnlp-main.455,W19-2304,0,0.0234626,", where this is often the case. In contrast, BITE performs equally well on both in- and out-of-domain data, demonstrating its applicability to practical scenarios where the training and testing domains may not match. This is the result of preserving the base forms, which we investigate further in §5.2. 4.3 Dialectal Variation Apart from second languages, dialects are another common source of non-standard inflections. However, there is a dearth of task-specific datasets in English dialects like AAVE and CSE. Therefore, in this section’s experiments, we use the model’s pseudo perplexity (pPPL) (Wang and Cho, 2019) on monodialectal corpora as a proxy for its performance on downstream tasks in the corresponding dialect. The pPPL measures how certain the pretrained model is about its prediction and reflects its generalization ability on the dialectal datasets. To ensure fair comparisons across different subword segmentations, we normalize the pseudo log-likelihoods by the number of word tokens fed into the WordPiece component of each tokenization pipeline (Mielke, 2019). This avoids unfairly penalizing BITE for inevitably generating longer sequences. Finally, we scale the pseudo loglikelihoods by the mask"
2020.emnlp-main.455,D19-5214,0,0.0238724,"ry (Bengio et al., 2001; Collobert et al., 2011) and uncommon words were represented by an unknown symbol. However, such a representation is unable to adequately deal with words absent in the training vocabulary. Therefore, subword representations like WordPiece (Schuster and Nakajima, 2012) and BPE (Sennrich et al., 2016) were proposed to encode out-of-vocabulary (OOV) words by segmenting them into subwords and encoding each subword as a separate symbol. This way, less information is lost in the encoding process since OOV words are approximated as a combination of subwords in the vocabulary. Wang et al. (2019) reduce vocabulary sizes by operating on bytes instead of characters (as in standard BPE). To make subword regularization more tractable, Kudo (2018) proposed an alternative method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koe"
2020.emnlp-main.455,N18-1101,0,0.0187221,"6 74.45 73.69 68.23 70.66 83.86 82.21 83.87 81.05 83.86 83.36 75.77 81.04 Table 1: BERTbase results on the clean and adversarial MultiNLI and SQuAD 2.0 examples. We compare BITE+WordPiece to both WordPiece alone and with one epoch of adversarial fine-tuning. For fair comparison with adversarial fine-tuning, we trained the BITE+WordPiece model for an extra epoch (bottom) on clean data. 4.1 Adversarial Robustness (Classification) We evaluate BITE’s ability to improve model robustness for question answering and natural language understanding using SQuAD 2.0 (Rajpurkar et al., 2018) and MultiNLI (Williams et al., 2018), respectively. We use M ORPHEUS (Tan et al., 2020), an adversarial attack targeting inflectional morphology, to test the overall system’s robustness to non-standard inflections. They previously demonstrated M ORPHEUS’s ability to generate plausible and semantically equivalent adversarial examples resembling L2 English sentences. We attack each BERTbase model separately and report F1 scores on the answerable questions and the full SQuAD 2.0 dataset, following Tan et al. (2020). In addition, for MNLI, we report scores for both the in-domain (MNLI) and out-of-domain dev. set (MNLI-MM). BITE+Word"
2020.emnlp-main.455,P15-2111,0,0.0316112,"e method of building a subword vocabulary by reducing an initially oversized vocabulary down to the required size with the aid of a unigram language model, as opposed to incrementally building a vocabulary as in WordPiece and BPE variants. However, machine translation systems operating on subwords still have trouble translating rare words from highlyinflected categories (Koehn and Knowles, 2017). Sadat and Habash (2006), Koehn and Hoang (2007), and Kann and Sch¨utze (2016) propose to improve machine translation and morphological reinflection by encoding morphological features separately while Sylak-Glassman et al. (2015) propose a schema for inflectional features. Avraham and Goldberg (2017) explore the effect of learning word embeddings from base forms and morphological tags for Hebrew, while Chaudhary et al. (2018) show that representing words as base forms, phonemes, and morphological tags improve crosslingual transfer for low-resource languages. Adversarial robustness in NLP. To harden NLP systems against adversarial examples, existing work largely uses adversarial training (Goodfellow et al., 2015; Jia and Liang, 2017; Ebrahimi et al., 2018; Belinkov and Bisk, 2018; Ribeiro et al., 2018; Iyyer et al., 20"
2020.emnlp-main.455,D19-1496,0,0.163056,"rrent question answering and machine translation systems are overly sensitive to non-standard inflections—a common feature of dialects such as Colloquial Singapore English (CSE) and African American Vernacular English (AAVE).2 Since people naturally correct for or ignore non-standard inflection use (Foster and Wigglesworth, 2016), we should expect NLP systems to be equally robust. Existing work on adversarial robustness for NLP primarily focuses on adversarial training methods (Belinkov and Bisk, 2018; Ribeiro et al., 2018; Tan et al., 2020) or classifying and correcting adversarial examples (Zhou et al., 2019a). However, this effectively increases the size of the training dataset by including adversarial examples or training a new model to identify and correct perturbations, thereby significantly increasing the overall computational cost of creating robust models. These approaches also only operate on either raw text or the model, ignoring tokenization—an operation that transforms raw text into a form that the neural network can learn from. We introduce a 2 Examples in Appendix A. new representation for word tokens that separates base from inflection. This improves both model robustness and vocabu"
2020.emnlp-main.455,P16-2096,0,\N,Missing
2020.emnlp-main.455,N18-1170,0,\N,Missing
2020.emnlp-main.455,N19-1423,0,\N,Missing
2020.emnlp-main.455,D19-1419,0,\N,Missing
2020.emnlp-main.488,N19-4010,0,0.02402,"Missing"
2020.emnlp-main.488,C18-1139,0,0.0275194,"Missing"
2020.emnlp-main.488,P18-1246,0,0.0265305,"Missing"
2020.emnlp-main.488,Q17-1010,0,0.0180151,"4) with initial learning rate 1e-3 and batch size 32. Learning rate is decayed by 0.5 if the performance on dev set does not improve in 3 consecutive epochs. We stop training when the learning rate drops below 1e-5 or number of epochs reaches 100. We use the pre3 Condition tag c is also in w<t , since it is a special token added to the beginning of each sentence. We write it explicitly to emphasize the conditional effect. 4 The baseline model provided in the original paper is used for evaluating end to end target based sentiment analysis task. trained 300-dimensional fastText word embeddings (Bojanowski et al., 2017) for all languages. We employ relatively simple basic models because: 1) They help to avoid the possible overfitting problems due to the small data size under the low resource setting; 2) They allow more faithful understanding on the effects of the proposed data augmentation method. 4.2 Supervised Experiments To verify the effectiveness of our data augmentation method in the supervised settings, we evaluate it on three different tagging tasks, including NER, POS and E2E-TBSA. Most of the prior works rely on additional information, so we use random deletion (rd) (Wei and Zou, 2019) as our basel"
2020.emnlp-main.488,D17-1047,1,0.833037,"Missing"
2020.emnlp-main.488,D17-1091,0,0.0158196,"t is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Emp"
2020.emnlp-main.488,P17-2090,0,0.0976786,"lexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 202"
2020.emnlp-main.488,P19-1553,1,0.679347,"wt |w<t ) in Eq. 2 becomes pθ (wt |w<t , c).3 A similar approach is used in CTRL (Keskar et al., 2019) to control style, task-specific behavior, etc., during text generation. 4 Experiments In this section, we present our experiments in both supervised and semi-supervised settings. In the supervised settings, only gold data are used for augmentation. In the semi-supervised settings, we also leverage unlabeled data and knowledge bases. 4.1 Basic Models Language Model We use the language model described in Section 3.2 for synthetic data generation. We modified the decoder of the LSTM-LM model in Kruengkrai (2019) to implement this language model. We set LSTM hidden state size to 512 and embedding size to 300. We use dropout rate 0.5 for the two dropout layers. All language model are trained using Stochastic gradient descent (SGD) with initial learning rate 1 and batch size 32. Learning rate will be decayed by 0.5 in the next epoch if the perplexity on dev set does not improve. We set the maximum number of epochs to 30 and stop training early if the perplexity on dev set does not improve in 3 consecutive epochs. During synthetic data generation, we use the average length of gold sentences in the traini"
2020.emnlp-main.488,2020.lifelongnlp-1.3,0,0.13528,"2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et al., 2019). Weakly labeled data will inevitably introduce more nois"
2020.emnlp-main.488,N16-1030,0,0.308802,"ings, our method demonstrates strong ability to exploit useful information from unlabeled data and knowledge base. 2 Background Named Entity Recognition (NER) Named entities refer to phrases that are names of persons, organizations and locations, etc. in text. For example, “[ORG U.N.] official [PER Ekeus] heads for [LOC Baghdad] ”. Named entity recognition is an important task of information extraction and it aims to locate and classify named entities in text into the predefined types (Mikheev et al., 1999; Sang and De Meulder, 2003; Li et al., 2020). It is a challenging task for two reasons (Lample et al., 2016): 1) in most languages and domains, the amount of manually labeled training data for NER is limited; 2) it is difficult to generalize from this small sample of training data due to the constraints on the kinds of words that can be names. Part-of-Speech (POS) Tagging Part-of-speech tagging consists of assigning a tag that represents a grammatical class to each word in a given sentence. It is a critical component of most NLP systems and is fundamental to facilitate downstream tasks such as syntactic parsing (Sch¨utze, 1993) and opinion analysis (Liu et al., 2015). The current state-ofthe-art POS"
2020.emnlp-main.488,D19-5505,1,0.898289,"sunaga et al., 2018). Target Based Sentiment Analysis The target based sentiment analysis is a fundamental task of sentiment analysis and it aims to detect the opinion targets in sentences and predict the sentiment polarities over the targets (Liu et al., 2015; Chen et al., 2017; Li et al., 2018, 2019a). For example, “USB3 Peripherals are noticeably less expensive than the ThunderBolt ones”. In this sentence, two opinion targets were mentioned, namely “USB3 Peripherals” and “ThunderBolt ones” and the user expresses a positive sentiment over the first, and a negative sentiment over the second. Li et al. (2019a,b) propose an end-to-end solution (E2ETBSA) of TBSA, which converts TBSA to a tagging task, and aims to solve the two subtasks (i.e. target detection and sentiment classification) in a unified manning by predicting unified tags. For example, the tag “B-POS” indicates the beginning of a target with positive sentiment. So after annotation, the above example becomes “[B-POS USB3] [E-POS Peripherals] are noticeably less expensive than the [B-NEG ThunderBolt] [E-NEG ones]”. 3 Proposed Method We propose a novel data augmentation method for sequence tagging tasks. We first linearize labeled sentenc"
2020.emnlp-main.488,D15-1168,1,0.889428,"Missing"
2020.emnlp-main.488,P14-5010,0,0.00283339,"gold data). Our method. Generate synthetic data with LM, where LM is trained on gold data and unlabeled data. Baseline method. Annotate unlabeled data with knowledge base. Our method. Generate synthetic data with LM, where LM is trained on gold data and knowledge base annotated data. Table 5: Data sources for the semi-supervised setting. 4.3.1 Only Using Unlabeled Data Dataset We use CoNLL2003 English NER data (Tjong Kim Sang and De Meulder, 2003) for evaluation. In addition to the gold NER training data, we utilize unlabeled data for semi-supervised training. The Stanford CoreNLP tokenizer (Manning et al., 2014) is used to tokenize Wikipedia sentences. Experimental Settings Similar to the above experiments, we use 1k, 2k, 4k, 6k and 8k sentences randomly sampled from NER gold data as well as the full dataset to evaluate our method. For fair comparison, we only use the same set of 10k sentences randomly sampled from Wikipedia dump in both of our and baseline methods. Let Dgold and Dunlabeled be the sampled gold NER data and the Wikipedia data, respectively. In our method, Dgold and Dunlabeled are concatenated to train language models, following the steps 6051 Method 1k 2k 4k 6k 8k all gold 58.06 67.85"
2020.emnlp-main.488,E99-1001,0,0.544131,"Missing"
2020.emnlp-main.488,P16-2067,0,0.0505893,"Missing"
2020.emnlp-main.488,S15-2082,0,0.0602815,"Missing"
2020.emnlp-main.488,S14-2004,0,0.149978,"Missing"
2020.emnlp-main.488,W03-0419,0,0.698327,"Missing"
2020.emnlp-main.488,P93-1034,0,0.295575,"Missing"
2020.emnlp-main.488,P16-1009,0,0.0477906,"owever, due to the complexity of language, it is more challenging to apply data augmentation techniques to natural language processing (NLP). Unlike computer vision and speech, where handcrafted rules (such as rotation, cropping, masking, etc.) can be easily applied to transform original data, it is difficult to generalize such rules for languages. Although simple distortion usually does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 P"
2020.emnlp-main.488,P16-1056,0,0.021012,"a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compa"
2020.emnlp-main.488,W02-2024,0,0.554171,"ted. See Table 1 for the notations of the methods used in our supervised experiments. Method Description gold gen rd rd* Only use the gold data. Our method. Generate synthetic data with the language models, and oversample gold data. Baseline method. Generate synthetic data by random deletion, and oversample gold data with the same ratio as gen. Baseline method. Similar to rd, except that gold and synthetic data are equally sampled. Table 1: Data sources for the supervised setting. 4.2.1 Named Entity Recognition Dataset We evaluate our proposed methods on the CoNLL2002/2003 NER data (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), with four languages: English, German, Dutch and Spanish. Besides, we evaluate our methods on Thai and Vietnamese NER data, which are product titles obtained from major e-commerce websites in Southeast Asian countries and annotated with 11 product attribute NER tags, including PRODUCT, BRAND, CONSUMER GROUP, MATERIAL, PATTERN, COLOR, FABRIC, OCCASION, ORIGIN, SEASON and STYLE. See Appendix for the statistics of the Thai and Vietnamese NER data used in our experiments. Experimental Settings In addition to evaluating our method on the full training data, we"
2020.emnlp-main.488,Q16-1035,0,0.0522738,"does not change the semantics of visual information, deleting or replacing a single word could completely change the meaning of the sentence. One successful method for data augmentation in NLP is back translation (Sennrich et al., 2016; Fadaee et al., 2017; Dong et al., 2017; Yu et al., 2018), where a translation model is used to translate monolingual sentences from target language to source language to generate synthetic parallel sentences. Other successful methods include: systematically reordering the dependents of some nodes in gold data to generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar"
2020.emnlp-main.488,D19-1670,0,0.046969,"o generate synthetic data for dependency parsing (Wang and Eisner, 2016), leveraging knowledge base for question generation (Serban et al., 2016) and using simulation-based approach to generate a set of prerequisite toy tasks for QA (Weston et al., 2015). Besides, synonym replace6045 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6045–6057, c November 16–20, 2020. 2020 Association for Computational Linguistics ment, random deletion/swap/insertion, generation with VAE or pre-trained language models are also used in some NLP tasks (Kobayashi, 2018; Wei and Zou, 2019; Anaby-Tavor et al., 2020; Raille et al., 2020; Kumar et al., 2020), but mainly for translation and classification tasks. Compared with the above-mentioned downstream tasks like translation and classification, sequence tagging is more fragile when it is confronted with data augmentation noises due to the finer granularity of the (token-level) task. Annotating unlabeled data with a weak tagger, leveraging aligned bilingual corpora to induce annotation and synonym replacement are three attempted data augmentation methods for sequence tagging (Shang et al., 2018; Yarowsky et al., 2001; Mathew et"
2020.emnlp-main.512,P08-1095,0,0.615351,"domains. 2 Background In this section, we give a brief overview of previous work on conversation disentanglement and the generic pointer network model. 2.1 Conversation disentanglement Most existing approaches treat disentanglement as a two-stage problem. The first stage involves link prediction that models “reply-to” relation between two utterances. The second stage is a clustering step, which utilizes the results from link prediction to construct the individual conversation threads. For link prediction, earlier methods used discourse cues and content features within statistical classifiers. Elsner and Charniak (2008, 2010) combine conversation cues like speaker, mention, and time with content features like the number of shared words to train a linear classifier. Recent methods use neural models to represent utterances with compositional features. Mehri and Carenini (2017) pre-train an LSTM network to predict reply probability of an utterance, which is then used in a link prediction classifier along with other handcrafted features. Jiang et al. (2018) model high and low-level linguistic information using a siamese hierarchical convolutional network that models similarity between pairs of utterances in the"
2020.emnlp-main.512,C18-1022,0,0.264294,"2020 Association for Computational Linguistics in the widely used Ubuntu IRC dialog corpus were extracted correctly by the heuristics employed by Lowe et al. (2015, 2017). Previous studies have therefore investigated traditional machine learning methods with statistical and linguistic features for conversation disentanglement, e.g., (Shen et al., 2006), (Wang and Oard, 2009; Wang et al., 2011b,a), (Elsner and Charniak, 2010, 2011), to name a few. The task is generally solved by first finding links between utterances, and then grouping them into a set of distinct conversations. Recent work by Jiang et al. (2018) and Kummerfeld et al. (2019) adopt deep learning approaches to learn abstract linguistic features and compute message pair similarity. However, these methods heavily rely on hand-engineered features that are often too specific to the particular datasets (or domains) on which the model is trained and evaluated. For example, many of the features used in (Kummerfeld et al., 2019) are only applicable to the Ubuntu IRC dataset. This hinders the model’s generalization and adaptability to other domains. In this work, we propose a more general framework for conversation disentanglement while avoiding"
2020.emnlp-main.512,P19-4003,1,0.891549,"Missing"
2020.emnlp-main.512,J10-3004,0,0.176116,"tion. Mentions of names are highlighted. Introduction With the fast growth of Internet and mobile devices, people now commonly communicate in the virtual world to discuss events, issues, tasks, and personal experiences. Among the various methods of communication, text-based conversational media, such as Internet Relay Chat (IRC), Facebook Messenger, Whatsapp, and Slack, has been and remains one of the most popular choices. Multiple ongoing conversations seem to occur naturally in such social and organizational interactions, especially when the conversation involves more than two participants (Elsner and Charniak, 2010). For example, consider the excerpt of a multi-party conversation in Figure 1 taken from the Ubuntu IRC corpus (Lowe et al., 2015). Even in this small excerpt, there are 4 concurrent conversations (distinguished by different colors) among 4 participants. Identifying or disentangling individual conversations is often considered as a prerequisite for downstream dialog tasks such as utterance ranking and generation (Lowe et al., 2017; Kim et al., 2019). It can also help building other applications such as search, summarization, and question answering over conversations, and support users by provi"
2020.emnlp-main.512,P19-1410,1,0.842423,"to infer the output: ˆyt = arg max(at ). Similar to the standard pointer network, each pointing mechanism in our approach is modeled as a multinomial distribution over the indices of the input sequence. However, unlike the original pointer network where a decoder state points to an encoder state, in our approach, the current encoder state points to the previous states. Pointer networks have recently yielded stateof-the-art results in constituency parsing (Nguyen et al., 2020), dependency parsing (Ma et al., 2018), anaphora resolution (Lee et al., 2017), and discourse segmentation and parsing (Lin et al., 2019). To the best of our knowledge, this is the very first work that utilizes a pointer network for conversation disentanglement. It is also a natural fit for online conversation disentanglement. 3 Our Disentanglement Model Given a sequence of streaming utterances U = {U1 , U2 , . . . , Ui , . . .}, our task in link prediction (§3.1) is to find the parent utterances Upi ⊂ U≤i that the current utterance Ui replies to. Here, U≤i refers to all the previous utterances until i, that is, U≤i = (U0 , U1 . . . , Ui ). An utterance can reply to itself (i.e., i = pi ), for example, the initial message in a"
2020.emnlp-main.512,P11-1118,0,0.781059,"Missing"
2020.emnlp-main.512,P18-1130,0,0.0190132,"for attention, which can be a neural network or simply a dot product operation. The model uses at to infer the output: ˆyt = arg max(at ). Similar to the standard pointer network, each pointing mechanism in our approach is modeled as a multinomial distribution over the indices of the input sequence. However, unlike the original pointer network where a decoder state points to an encoder state, in our approach, the current encoder state points to the previous states. Pointer networks have recently yielded stateof-the-art results in constituency parsing (Nguyen et al., 2020), dependency parsing (Ma et al., 2018), anaphora resolution (Lee et al., 2017), and discourse segmentation and parsing (Lin et al., 2019). To the best of our knowledge, this is the very first work that utilizes a pointer network for conversation disentanglement. It is also a natural fit for online conversation disentanglement. 3 Our Disentanglement Model Given a sequence of streaming utterances U = {U1 , U2 , . . . , Ui , . . .}, our task in link prediction (§3.1) is to find the parent utterances Upi ⊂ U≤i that the current utterance Ui replies to. Here, U≤i refers to all the previous utterances until i, that is, U≤i = (U0 , U1 . ."
2020.emnlp-main.512,D19-1682,0,0.183409,"y of an utterance, which is then used in a link prediction classifier along with other handcrafted features. Jiang et al. (2018) model high and low-level linguistic information using a siamese hierarchical convolutional network that models similarity between pairs of utterances in the same conversation. The interactions between two utterances is captured by taking element-wise absolute difference of the encoded sentence features along with other handcrafted features. Kummerfeld et al. (2019) uses feed-forward networks with averaged pre-trained word embedding and many hand-engineered features. Tan et al. (2019) used an utterance-level LSTM network, while Zhu et al. (2019) used a masked transformer to get a contextaware utterance representation considering utterances in the same conversation. Finding a globally optimal clustering solution for conversation disentanglement has been shown to be NP-hard (McCallum and Wellner, 2005). Previous methods focus mostly on approximating the global 1 https://github.com/vode/onlinePtrNet_ disentanglement 6322 optimal by either using greedy decoding (Wang and Oard, 2009; Elsner and Charniak, 2008, 2010, 2011; Jiang et al., 2018; Aumayr et al., 2011) or training mul"
2020.emnlp-main.512,I17-1062,0,0.428012,"irst stage involves link prediction that models “reply-to” relation between two utterances. The second stage is a clustering step, which utilizes the results from link prediction to construct the individual conversation threads. For link prediction, earlier methods used discourse cues and content features within statistical classifiers. Elsner and Charniak (2008, 2010) combine conversation cues like speaker, mention, and time with content features like the number of shared words to train a linear classifier. Recent methods use neural models to represent utterances with compositional features. Mehri and Carenini (2017) pre-train an LSTM network to predict reply probability of an utterance, which is then used in a link prediction classifier along with other handcrafted features. Jiang et al. (2018) model high and low-level linguistic information using a siamese hierarchical convolutional network that models similarity between pairs of utterances in the same conversation. The interactions between two utterances is captured by taking element-wise absolute difference of the encoded sentence features along with other handcrafted features. Kummerfeld et al. (2019) uses feed-forward networks with averaged pre-trai"
2020.emnlp-main.512,D11-1002,0,0.0731056,"Missing"
2020.emnlp-main.512,2020.acl-main.301,1,0.728312,") (1) where σ(., .) is a scoring function for attention, which can be a neural network or simply a dot product operation. The model uses at to infer the output: ˆyt = arg max(at ). Similar to the standard pointer network, each pointing mechanism in our approach is modeled as a multinomial distribution over the indices of the input sequence. However, unlike the original pointer network where a decoder state points to an encoder state, in our approach, the current encoder state points to the previous states. Pointer networks have recently yielded stateof-the-art results in constituency parsing (Nguyen et al., 2020), dependency parsing (Ma et al., 2018), anaphora resolution (Lee et al., 2017), and discourse segmentation and parsing (Lin et al., 2019). To the best of our knowledge, this is the very first work that utilizes a pointer network for conversation disentanglement. It is also a natural fit for online conversation disentanglement. 3 Our Disentanglement Model Given a sequence of streaming utterances U = {U1 , U2 , . . . , Ui , . . .}, our task in link prediction (§3.1) is to find the parent utterances Upi ⊂ U≤i that the current utterance Ui replies to. Here, U≤i refers to all the previous utterance"
2020.emnlp-main.512,D14-1162,0,0.0844074,"nformation about the conversation clusters. 3.2 The final training loss of our model is: fi,j = ti,j ⊕ mentioni,j ⊕ mentionj,i ⊕Mi,j ⊕ Mj,i ⊕ hi,j L(θ) = Llink (θ) + λLpair (θ) (11) where w is a shared linear layer parameter. We use cross entropy (CE) loss for the pointer module. Llink (θ) = − i X yi,j log p(Ui , Uj ) (15) (10) T score(Ui , Uj ) = tanh(w fi,j ) exp(score(Ui , Uj )) p(Ui , Uj ) = Pi s=0 exp(score(Ui , Us )) for j ∈ (0, ..., i) Training (12) where λ is the hyper-parameter for tuning the importance of the pairwise classification loss. We use Glove 128-dimensional word embedding (Pennington et al., 2014), pre-trained by Lowe et al. (2015) on the #Ubuntu corpus. The hidden layers in the Bi-LSTM are of 256 dimensions. We optimize our model using Adam (Kingma and Ba, 2014) optimizer with a learning rate of 1 × 10−5 . For regularization, we set the dropout at 0.2 and L2 penalize weight with 1 × 10−7 . 3.3 j=0 where yi,j = 1 if Ui replies to Uj , otherwise 0, and θ are the model parameters. Decoding Our framework naturally allows us to disentangle the threads in an online fashion. As a new utterance 6325 Ui arrives, the utterance encoder encodes it into a vector. The pointer module then computes a"
2020.emnlp-main.512,N09-1023,0,0.304097,"feed-forward networks with averaged pre-trained word embedding and many hand-engineered features. Tan et al. (2019) used an utterance-level LSTM network, while Zhu et al. (2019) used a masked transformer to get a contextaware utterance representation considering utterances in the same conversation. Finding a globally optimal clustering solution for conversation disentanglement has been shown to be NP-hard (McCallum and Wellner, 2005). Previous methods focus mostly on approximating the global 1 https://github.com/vode/onlinePtrNet_ disentanglement 6322 optimal by either using greedy decoding (Wang and Oard, 2009; Elsner and Charniak, 2008, 2010, 2011; Jiang et al., 2018; Aumayr et al., 2011) or training multiple link classifiers to do voting (Kummerfeld et al., 2019). Mehri and Carenini (2017) trained additional classifiers to decide whether an utterance belongs to a conversation or not. Wang et al. (2020) use a multi-task topic tracking framework for conversation disentanglement, topic prediction and next utterance ranking. Our work is fundamentally different from previous studies in that we treat link prediction as a pointing problem modeled by a multinomial distribution over the previous utterance"
2020.emnlp-main.512,2020.emnlp-main.533,1,0.465409,"Missing"
2020.emnlp-main.533,P17-1152,0,0.212702,"Recent approaches such as Sequential Matching Netowrk (SMN) (Wu et al., 2019) leverage the contextual information by matching each contextual utterance with response and the multi channel Convolutional Neural Network (CNN) was proposed to generate multiple levels of granularity of matched segment. These hierarchy-based methods use LSTM to encode the text, which is not cost effective to capture multi-grained segment representation (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2019). A particular work on sequence-based method stand out in DSTC-7; Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) achieves the state-of-the-art performance in DSTC-7 by taking advantage of inter-sentence matching (Chen et al., 6582 2016; Chen and Wang, 2019). It converts multiturn dialogue setting to natural language inference setting. In addition, transformer-based approach Deep Attention Matching (DAM) solve response selection problem by attention mechanism (Zhou et al., 2018). It utilizes utterance self-attention and context-to-response cross attention to leverage the hidden representation at multi-grained level. Similar to DAM, Multi-hop Selector Network (MSN) was proposed to fuse and select relevant"
2020.emnlp-main.533,D19-1193,0,0.176366,"ted modules for language understanding, dialog management, and generation, thus simplifying the system design. Due to these reasons, retrieval-based systems have been widely adopted in commercial dialogue systems (Gao et al., 2019; Gunasekara et al., 2019). Initially, researchers considered response selection in single-turn conversations, where only the last input utterance is considered as the context query (Yan et al., 2016). More recent work deals with multi-turn context, which shows improvements over the single-turn context (Lowe et al., 2015, 2017; Zhou et al., 2016; Chen and Wang, 2019; Gu et al., 2019; Zhou et al., 2018). These methods typically aim to encode the context and the candidate responses in a joint semantic space by capturing short and long range dependencies, and then retrieve the most relevant response by matching the query representation against each candidate’s representation through attentions. However, most of these works are limited to only 6581 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6581–6591, c November 16–20, 2020. 2020 Association for Computational Linguistics two-party conversations. As dialogue research progress"
2020.emnlp-main.533,W19-4107,0,0.0156429,"Compared to the generation-based systems that generate novel utterances (Serban et al., 2016), retrieval-based systems produce fluent, grammatical and informative responses (Weston et al., 2018; Henderson et al., 1 Code will be available at https://github.com/ salesforce/TopicBERT. 2019). Also compared to the traditional modular approach, it does not rely on dedicated modules for language understanding, dialog management, and generation, thus simplifying the system design. Due to these reasons, retrieval-based systems have been widely adopted in commercial dialogue systems (Gao et al., 2019; Gunasekara et al., 2019). Initially, researchers considered response selection in single-turn conversations, where only the last input utterance is considered as the context query (Yan et al., 2016). More recent work deals with multi-turn context, which shows improvements over the single-turn context (Lowe et al., 2015, 2017; Zhou et al., 2016; Chen and Wang, 2019; Gu et al., 2019; Zhou et al., 2018). These methods typically aim to encode the context and the candidate responses in a joint semantic space by capturing short and long range dependencies, and then retrieve the most relevant response by matching the query"
2020.emnlp-main.533,D19-1199,0,0.126632,"Missing"
2020.emnlp-main.533,W15-4640,0,0.298762,"d to the traditional modular approach, it does not rely on dedicated modules for language understanding, dialog management, and generation, thus simplifying the system design. Due to these reasons, retrieval-based systems have been widely adopted in commercial dialogue systems (Gao et al., 2019; Gunasekara et al., 2019). Initially, researchers considered response selection in single-turn conversations, where only the last input utterance is considered as the context query (Yan et al., 2016). More recent work deals with multi-turn context, which shows improvements over the single-turn context (Lowe et al., 2015, 2017; Zhou et al., 2016; Chen and Wang, 2019; Gu et al., 2019; Zhou et al., 2018). These methods typically aim to encode the context and the candidate responses in a joint semantic space by capturing short and long range dependencies, and then retrieve the most relevant response by matching the query representation against each candidate’s representation through attentions. However, most of these works are limited to only 6581 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6581–6591, c November 16–20, 2020. 2020 Association for Computational Lin"
2020.emnlp-main.533,W12-1607,0,0.0815165,"Missing"
2020.emnlp-main.533,W18-6319,0,0.0125318,"0.710, when we jointly train response selection and topic prediction (2nd last row), validating an effective utilization of topic information in selecting response. Then we replace topic prediction with disentanglement, which further improves from 0.710 to 0.720, showing response selection can utilize topic tracing by sharing the connection of utterances. Finally, our TopicBERT with the multi-task learning achieves the best result (0.726) and significantly outperform the prior state-of-the-art Adapt-BERT in DSTC-8 response selection task (Kim et al., 2019). We further compute BLEU4 SacreBLEU (Post, 2018) for the incorrectly selected responses by Topic-BERT and ToD-BERT. From Table 3, we 2 https://github.com/huggingface/ transformers Model Recall@1 Recall@5 Recall@10 MRR BERTbase BERT+post-train ToD-BERT 0.287 0.532 0.588 0.503 0.797 0.823 0.572 0.840 0.885 0.351 0.677 0.691 Adapt-BERT 0.706 0.916 0.957 0.799 Topic-BERT −TP −D −TP −D 0.726 0.720 0.710 0.696 0.930 0.927 0.924 0.910 0.970 0.964 0.960 0.950 0.807 0.803 0.800 0.790 Table 2: Response selection results on DSTC-8 Ubuntu. “-TP” means our model excluding topic prediction loss and “-D” means excluding topic disentanglement loss. Adapt-B"
2020.emnlp-main.533,P19-1001,0,0.0394056,"hing (Chen et al., 6582 2016; Chen and Wang, 2019). It converts multiturn dialogue setting to natural language inference setting. In addition, transformer-based approach Deep Attention Matching (DAM) solve response selection problem by attention mechanism (Zhou et al., 2018). It utilizes utterance self-attention and context-to-response cross attention to leverage the hidden representation at multi-grained level. Similar to DAM, Multi-hop Selector Network (MSN) was proposed to fuse and select relevant context utterances and match it with the response utterance (Yuan et al., 2019). In addition, Tao et al. (2019) studied the relationship between context utterance and response which indicates that the depth of interaction affect the effectiveness of the model. Compared to LSTM-based approaches, methods based on transformers (Vaswani et al., 2017) present a promising performance in both accuracy and efficiency (Yang et al., 2020). Devlin et al. (2018) proposed BERT, a transformer-based largescale pretrained language model, which achieves state-of-the-art performance in different NLP tasks. BERT is also a good match to response selection problem as shown by Vig and Ramea (2019). Our Topic-BERT is initial"
2020.emnlp-main.533,W18-5713,0,0.0273363,"n natural language processing (NLP), there has been a lot of interests in building effective task-oriented dialogue systems that can assist people in real-world business such as booking tickets, ordering food and solving technical issues (Bui, 2006). Retrieval-based response generation that selects a suitable response from a pool of candidates (pre-existing human responses) has become a popular approach to framing dialog. Compared to the generation-based systems that generate novel utterances (Serban et al., 2016), retrieval-based systems produce fluent, grammatical and informative responses (Weston et al., 2018; Henderson et al., 1 Code will be available at https://github.com/ salesforce/TopicBERT. 2019). Also compared to the traditional modular approach, it does not rely on dedicated modules for language understanding, dialog management, and generation, thus simplifying the system design. Due to these reasons, retrieval-based systems have been widely adopted in commercial dialogue systems (Gao et al., 2019; Gunasekara et al., 2019). Initially, researchers considered response selection in single-turn conversations, where only the last input utterance is considered as the context query (Yan et al., 2"
2020.emnlp-main.533,J19-1005,0,0.0210762,"election A dual encoder framework was proposed to match the context and response (Lowe et al., 2015), and the long short-term memory (LSTM) was utilized to learn the long and short term dependencies among tokens. Beyond tokens, the sentence view matching was introduced by applying a hierarchical recurrent neural network to model sentence level relationships (Zhou et al., 2016). However, context utterances and response are encoded separately without interaction; thus the semantics extracted from context are not based on the response. Recent approaches such as Sequential Matching Netowrk (SMN) (Wu et al., 2019) leverage the contextual information by matching each contextual utterance with response and the multi channel Convolutional Neural Network (CNN) was proposed to generate multiple levels of granularity of matched segment. These hierarchy-based methods use LSTM to encode the text, which is not cost effective to capture multi-grained segment representation (Lowe et al., 2015; Zhou et al., 2016; Wu et al., 2019). A particular work on sequence-based method stand out in DSTC-7; Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) achieves the state-of-the-art performance in DSTC-7 by taki"
2020.emnlp-main.533,D19-1011,0,0.138382,"Missing"
2020.emnlp-main.533,D16-1036,0,0.109191,"Missing"
2020.emnlp-main.533,P18-1103,0,0.275115,"anguage understanding, dialog management, and generation, thus simplifying the system design. Due to these reasons, retrieval-based systems have been widely adopted in commercial dialogue systems (Gao et al., 2019; Gunasekara et al., 2019). Initially, researchers considered response selection in single-turn conversations, where only the last input utterance is considered as the context query (Yan et al., 2016). More recent work deals with multi-turn context, which shows improvements over the single-turn context (Lowe et al., 2015, 2017; Zhou et al., 2016; Chen and Wang, 2019; Gu et al., 2019; Zhou et al., 2018). These methods typically aim to encode the context and the candidate responses in a joint semantic space by capturing short and long range dependencies, and then retrieve the most relevant response by matching the query representation against each candidate’s representation through attentions. However, most of these works are limited to only 6581 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6581–6591, c November 16–20, 2020. 2020 Association for Computational Linguistics two-party conversations. As dialogue research progresses, it is necessary"
2021.acl-long.154,2020.repl4nlp-1.1,0,0.175582,"ng α, we can control how many samples a dataset can provide in the mix. 3 Experiments We consider three tasks in the zero-resource crosslingual transfer setting. We assume labeled training data only in English, and transfer the trained model to a target language. For all experiments, we report the mean score of the three models that use different seeds. 3.1 Tasks & Settings XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl). We also evaluate on Finnish (fi) and Arabic (ar) datasets collected from Bari et al. (2020). Note that Arabic is structurally different from English, and Finnish is from a different language family. To show how the models perform on extremely lowresource languages, we experiment with three structurally different languages from WikiANN (Pan et al., 2017) of different (unlabeled) training data sizes: Urdu (ur-20k training samples), Bengali (bn10K samples), and Burmese (my-100 samples). XNLI We use the standard dataset (Conneau et al., 2018). For a given pair of sentences, the task is to predict the entailment relationship between the two sentences, i.e., whether the second sentence (h"
2021.acl-long.154,P17-1178,0,0.0447362,"language. For all experiments, we report the mean score of the three models that use different seeds. 3.1 Tasks & Settings XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl). We also evaluate on Finnish (fi) and Arabic (ar) datasets collected from Bari et al. (2020). Note that Arabic is structurally different from English, and Finnish is from a different language family. To show how the models perform on extremely lowresource languages, we experiment with three structurally different languages from WikiANN (Pan et al., 2017) of different (unlabeled) training data sizes: Urdu (ur-20k training samples), Bengali (bn10K samples), and Burmese (my-100 samples). XNLI We use the standard dataset (Conneau et al., 2018). For a given pair of sentences, the task is to predict the entailment relationship between the two sentences, i.e., whether the second sentence (hypothesis) is an Entailment, Contradiction, or 1982 Model en es nl de ar fi 85.16 92.53 92.9 78.14 85.81 86.2 75.49 – 86.8 84.21 – 92.4 79.58 80.87 81.46 70.99 73.40 75.40 45.48 49.04 52.30 65.95 75.57 76.85 79.26 81.07 85.21 85.32 72.31 73.76 80.33 80.99 47.03 49"
2021.acl-long.154,W02-2024,0,0.269906,"l distribution to sample from: fα pi = PN i α j=1 fj , where fi ni = PN j=1 nj (2) where α is the sampling factor and ni is the total number of samples in the ith dataset. By tweaking α, we can control how many samples a dataset can provide in the mix. 3 Experiments We consider three tasks in the zero-resource crosslingual transfer setting. We assume labeled training data only in English, and transfer the trained model to a target language. For all experiments, we report the mean score of the three models that use different seeds. 3.1 Tasks & Settings XNER: We use the standard CoNLL datasets (Sang, 2002; Sang and Meulder, 2003) for English (en), German (de), Spanish (es) and Dutch (nl). We also evaluate on Finnish (fi) and Arabic (ar) datasets collected from Bari et al. (2020). Note that Arabic is structurally different from English, and Finnish is from a different language family. To show how the models perform on extremely lowresource languages, we experiment with three structurally different languages from WikiANN (Pan et al., 2017) of different (unlabeled) training data sizes: Urdu (ur-20k training samples), Bengali (bn10K samples), and Burmese (my-100 samples). XNLI We use the standard"
2021.acl-long.154,W03-0419,0,0.642562,"Missing"
2021.acl-long.154,P16-1009,0,0.122809,"Missing"
2021.acl-long.154,N12-1052,0,0.0309422,". Wu and Dredze (2019), Keung et al. (2019), and Pires et al. (2019) evaluate zero-shot cross-lingual transferability of mBERT on several tasks and attribute its generalization capability to shared subword units. Pires et al. (2019) also found structural similarity (e.g., word order) to be another important factor for successful crosslingual transfer. K et al. (2020), however, show that the shared subword has a minimal contribution; instead, the structural similarity between languages is more crucial for effective transfer. Older data augmentation approaches relied on distributional clusters (Täckström et al., 2012). A number of recent methods have been proposed using contextualized LMs (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2019; Ding et al., 2020; Liu et al., 2021). These methods rely on labels to perform label-constrained augmentation, thus not directly comparable with ours. Also, there are fundamental differences in the way we use the pretrained LM. Unlike them our LM augmentation is purely unsupervised and we do not perform any fine-tuning of the pretrained vicinity model. This disjoint characteristic gives our framework the flexibility to replace θlm even with a better monolingual LM for a"
2021.acl-long.154,D19-1077,0,0.0252847,"thers in row 5). This indicates that augmentation of U XLA does not overfit on a target language. More baselines, analysis and visualizations are added in Appendix. 5 Related Work Recent years have witnessed significant progress in learning multilingual pretrained models. Notably, mBERT (Devlin et al., 2019) extends (English) BERT by jointly training on 102 languages. XLM (Lample and Conneau, 2019) extends mBERT with a conditional LM and a translation LM (using parallel data) objectives. Conneau et al. (2020) train the largest multilingual language model XLM-R with RoBERTa (Liu et al., 2019). Wu and Dredze (2019), Keung et al. (2019), and Pires et al. (2019) evaluate zero-shot cross-lingual transferability of mBERT on several tasks and attribute its generalization capability to shared subword units. Pires et al. (2019) also found structural similarity (e.g., word order) to be another important factor for successful crosslingual transfer. K et al. (2020), however, show that the shared subword has a minimal contribution; instead, the structural similarity between languages is more crucial for effective transfer. Older data augmentation approaches relied on distributional clusters (Täckström et al., 2012"
2021.acl-long.154,D19-1382,0,0.0614617,"Missing"
2021.acl-long.321,D18-1316,0,0.0133931,"in a controlled manner via reliability tests can be a complementary method of evaluating the system’s out-of-distribution generalization ability. 4 Adversarial Attacks as Reliability Tests We first give a brief introduction to adversarial attacks in NLP before showing how they can be used for reliability testing. We refer the reader to Zhang et al. (2020b) for a comprehensive survey. Existing work on NLP adversarial attacks perturbs the input at various levels of linguistic analysis: phonology (Eger and Benz, 2020), orthography (Ebrahimi et al., 2018), morphology (Tan et al., 2020), lexicon (Alzantot et al., 2018; Jin et al., 2020), and syntax (Iyyer et al., 2018). Early work did not place any constraints on the attacks and merely used the degradation to a tar3 Dua et al. (2019) reports a cost of 60k USD for 96k question–answer pairs. Algorithm 1 General Reliability Test Require: Data distribution Dd = {X , Y} modeling the dimension of interest d, NLP system M, Source dataset X ∼ X , Desired labels Y 0 ∼ Y, Scoring function S. Ensure: Average- or worst-case examples X 0 , Result r. 1: X 0 ← {∅}, r ← 0 2: for x, y 0 in X, Y 0 do 3: C ← S AMPLE C ANDIDATES(X ) 4: switch TestType do 5: case AverageCaseTe"
2021.acl-long.321,2020.acl-main.485,0,0.0293702,"s. Expert. An actor who has specialized knowledge, such as ethicists, linguists, domain experts, social scientists, or NLP practitioners. 3 The Case for Reliability Testing in NLP The accelerating interest in building NLP-based products that impact many lives has led to urgent questions of fairness, safety, and accountability (Hovy and Spruit, 2016; Bender et al., 2021), 2 The “degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions” (IEEE, 2017). 4154 prompting research into algorithmic bias (Bolukbasi et al., 2016; Blodgett et al., 2020), explainability (Ribeiro et al., 2016; Danilevsky et al., 2020), robustness (Jia and Liang, 2017), etc. Research is also emerging on best practices for productizing ML: from detailed dataset documentation (Bender and Friedman, 2018; Gebru et al., 2018), model documentation for highlighting important but often unreported details such as its training data, intended use, and caveats (Mitchell et al., 2019), and documentation best practices (Partnership on AI, 2019), to institutional mechanisms such as auditing (Raji et al., 2020) to enforce accountability and red-teaming (Brundage et al., 2020)"
2021.acl-long.321,2020.aacl-main.46,0,0.0290211,"icists, linguists, domain experts, social scientists, or NLP practitioners. 3 The Case for Reliability Testing in NLP The accelerating interest in building NLP-based products that impact many lives has led to urgent questions of fairness, safety, and accountability (Hovy and Spruit, 2016; Bender et al., 2021), 2 The “degree to which a system or component can function correctly in the presence of invalid inputs or stressful environmental conditions” (IEEE, 2017). 4154 prompting research into algorithmic bias (Bolukbasi et al., 2016; Blodgett et al., 2020), explainability (Ribeiro et al., 2016; Danilevsky et al., 2020), robustness (Jia and Liang, 2017), etc. Research is also emerging on best practices for productizing ML: from detailed dataset documentation (Bender and Friedman, 2018; Gebru et al., 2018), model documentation for highlighting important but often unreported details such as its training data, intended use, and caveats (Mitchell et al., 2019), and documentation best practices (Partnership on AI, 2019), to institutional mechanisms such as auditing (Raji et al., 2020) to enforce accountability and red-teaming (Brundage et al., 2020) to address developer blind spots, not to mention studies on the"
2021.acl-long.321,W19-3504,0,0.0608711,"Missing"
2021.acl-long.321,N19-1423,0,0.0535642,"ting distributions, may inadvertently result in systems that discriminate against minorities, who are often underrepresented in the training data. This can take ∗ Correspondence to: samson.tan@salesforce.com the form of misrepresentation of or poorer performance for people with disabilities, specific gender, ethnic, age, or linguistic groups (Hovy and Spruit, 2016; Crawford, 2017; Hutchinson et al., 2020). Amongst claims of NLP systems achieving human parity in challenging tasks such as question answering (Yu et al., 2018), machine translation (Hassan et al., 2018), and commonsense inference (Devlin et al., 2019), research has demonstrated these systems’ fragility to natural and adversarial noise (Goodfellow et al., 2015; Belinkov and Bisk, 2018) and out-of-distribution data (Fisch et al., 2019). It is also still common practice to equate “testing” with “measuring held-out accuracy”, even as datasets are revealed to be harmfully biased (Wagner et al., 2015; Geva et al., 2019; Sap et al., 2019). Many potential harms can be mitigated by detecting them early and preventing the offending model from being put into production. Hence, in addition to being mindful of the biases in the NLP pipeline (Bender and"
2021.acl-long.321,2020.coling-main.76,0,0.0429937,"rectness, we would expect the ATS system to ignore linguistic variation and sensitive attributes as long as they do not affect the answer’s validity. Hence, we would expect variation in these dimensions to have no effect on scores: answer length, language/vocabulary simplicity, alternative spellings/misspellings of non-keywords, grammatical variation, syntactic variation (especially those resembling transfer from a first language), and proxies for sensitive attributes. On the other hand, the system should be able to differentiate proper answers from those aimed at gaming the test (Chin, 2020; Ding et al., 2020). When grading students on language skills, however, we would expect ATS systems to be only sensitive to the relevant skill. For example, when assessing grammar use, we would expect the system to be sensitive to grammatical errors (from the perspective of the language variety the student is expected to use), but not to the other dimensions mentioned above (e.g., misspellings). Actors. Relevant experts include teachers of the subjects where the ATS systems will be deployed, linguists, and computer scientists. The stakeholders (students) may be represented by student unions (at the university le"
2021.acl-long.321,2020.emnlp-main.500,0,0.0981568,"d for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lower bound performance for that dimension. Said another way, adversarial attacks can be reframed as interpretable reliability tests if we cons"
2021.acl-long.321,D17-1153,0,0.0219433,"aluation metrics for natural language generation (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019): due to the tests’ synthetic nature they may not fully capture the nuances of reality. For example, if a test’s objective were to test an NLP system’s reliability when interacting with African American English (AAE) speakers, would it be possible to guarantee (in practice) that all generated examples fall within the distribution of AAE texts? Potential research directions would be to design adversary generation techniques that can offer such guarantees or incorporate human feedback (Nguyen et al., 2017; Kreutzer et al., 2018; Stiennon et al., 2020). 9 Conclusion Once language technologies leave the lab and start impacting real lives, concerns around safety, fairness, and accountability cease to be thought experiments. While it is clear that NLP can have a positive impact on our lives, from typing autocompletion to revitalizing endangered languages (Zhang et al., 2020a), it also has the potential to perpetuate harmful stereotypes (Bolukbasi et al., 2016; Sap et al., 2019), perform disproportionately poorly for underrepresented groups (Hern, 2017; Bridgeman et al., 2012), and even erase alrea"
2021.acl-long.321,2020.acl-main.441,0,0.24244,"et al., 2019; 4153 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus"
2021.acl-long.321,D17-1238,0,0.1766,"of operationalizing potential dimensions: “What is the system’s performance when exposed to variation along dimension d?”. For example, rather than simply “gender”, a better-defined dimension would be “gender pronouns”. With this understanding, experts and policymakers can then create a set of reliability requirements, comprising the testing dimensions, performance metric(s), and passing thresholds. Next, we recommend using the same metrics for held-out, average-case, and worst-case performance for easy comparison. These often vary from task to task and are still a subject of active research (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019), hence the question of the right metric to use is beyond the scope of this paper. Finally, ethicists, in consultation with the other aforementioned experts and stakeholders, will determine acceptable thresholds for worst-case performance. The system under test must perform above said thresholds when exposed to variation along those dimensions in order to pass. For worst-case performance, we recommend reporting thresholds as relative differences (δ) between the average-case and worst-case performance. These questions may help in applying this step and de"
2021.acl-long.321,J18-3002,0,0.0876608,"ential dimensions: “What is the system’s performance when exposed to variation along dimension d?”. For example, rather than simply “gender”, a better-defined dimension would be “gender pronouns”. With this understanding, experts and policymakers can then create a set of reliability requirements, comprising the testing dimensions, performance metric(s), and passing thresholds. Next, we recommend using the same metrics for held-out, average-case, and worst-case performance for easy comparison. These often vary from task to task and are still a subject of active research (Novikova et al., 2017; Reiter, 2018; Kryscinski et al., 2019), hence the question of the right metric to use is beyond the scope of this paper. Finally, ethicists, in consultation with the other aforementioned experts and stakeholders, will determine acceptable thresholds for worst-case performance. The system under test must perform above said thresholds when exposed to variation along those dimensions in order to pass. For worst-case performance, we recommend reporting thresholds as relative differences (δ) between the average-case and worst-case performance. These questions may help in applying this step and deciding if spec"
2021.acl-long.321,P19-1103,0,0.0196223,"8: case WorstCaseTest 9: x0 , s ← arg minxc ∈C S(y 0 , M(xc )) 10: X 0 ← X 0 ∪ {x0 } 11: r ←r+s 12: end for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lower bound performance for that dimensio"
2021.acl-long.321,P18-1079,0,0.0243937,"6: s ← M EAN(S(y 0 , M(C))) 7: X0 ← X0 ∪ C 8: case WorstCaseTest 9: x0 , s ← arg minxc ∈C S(y 0 , M(xc )) 10: X 0 ← X 0 ∪ {x0 } 11: r ←r+s 12: end for r 13: r ← |X| 14: return X 0 , r get model’s accuracy as the measure of success. However, this often resulted in the semantics and expected prediction changing, leading to an overestimation of the attack’s success. Recent attacks aim to preserve the original input’s semantics. A popular approach has been to substitute words with their synonyms using word embeddings or a language model as a measure of semantic similarity (Alzantot et al., 2018; Ribeiro et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019a; Li et al., 2019; Jin et al., 2020; Garg and Ramakrishnan, 2020; Li et al., 2020a). Focusing on maximally degrading model accuracy overlooks the key feature of adversarial attacks: the ability to find the worst-case example for a model from an arbitrary distribution. Many recent attacks perturb the input across multiple dimensions at once, which may make the result unnatural. By constraining our sample perturbations to a distribution modeling a specific dimension of interest, the performance on the generated adversaries is a valid lo"
2021.acl-long.321,2020.acl-main.442,0,0.269267,"Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often simply rely on word embeddings or language models for perturbation proposal (see §4). Wh"
2021.acl-long.321,P19-1163,0,0.094836,"son et al., 2020). Amongst claims of NLP systems achieving human parity in challenging tasks such as question answering (Yu et al., 2018), machine translation (Hassan et al., 2018), and commonsense inference (Devlin et al., 2019), research has demonstrated these systems’ fragility to natural and adversarial noise (Goodfellow et al., 2015; Belinkov and Bisk, 2018) and out-of-distribution data (Fisch et al., 2019). It is also still common practice to equate “testing” with “measuring held-out accuracy”, even as datasets are revealed to be harmfully biased (Wagner et al., 2015; Geva et al., 2019; Sap et al., 2019). Many potential harms can be mitigated by detecting them early and preventing the offending model from being put into production. Hence, in addition to being mindful of the biases in the NLP pipeline (Bender and Friedman, 2018; Mitchell et al., 2019; 4153 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et"
2021.acl-long.321,2020.acl-main.468,0,0.031264,"ial examples and out-of-distribution generalization has found ML systems to be particularly vulnerable to slight perturbations in the input (Goodfellow et al., 2015) and natural distribution shifts (Fisch et al., 2019). While these perturbations are often chosen to maximize model failure, they highlight serious reliability issues for putting ML models into production since they show that these models could fail catastrophically in naturally noisy, diverse, real-world environments (Saria and Subbaswamy, 2019). Additionally, bias can seep into the system at multiple stages of the NLP lifecycle (Shah et al., 2020), resulting in discrimination against minority groups (O’Neil, 2016). The good news, however, is that rigorous testing can help to highlight potential issues before the systems are deployed. The need for rigorous testing in NLP is reflected in ACL 2020 giving the Best Paper Award to CheckList (Ribeiro et al., 2020), which applied the idea of behavior testing from software engineering to testing NLP systems. While invaluable as a first step towards the development of comprehensive testing methodology, the current implementation of CheckList may still overestimate the reliability of NLP systems"
2021.acl-long.321,2021.eacl-main.156,0,0.0394858,"nt Conference on Natural Language Processing, pages 4153–4169 August 1–6, 2021. ©2021 Association for Computational Linguistics Waseem et al., 2021) and holding creators accountable via audits (Raji et al., 2020; Brundage et al., 2020), we argue for the need to evaluate an NLP system’s reliability in diverse operating conditions. Initial research on evaluating out-of-distribution generalization involved manually-designed challenge sets (Jia and Liang, 2017; Nie et al., 2020; Gardner et al., 2020), counterfactuals (Kaushik et al., 2019; Khashabi et al., 2020; Wu et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often sim"
2021.acl-long.321,2021.naacl-main.282,1,0.78736,"Missing"
2021.acl-long.321,2020.acl-main.263,1,0.838983,"et al., 2021), biased sampling (Søgaard et al., 2021) or toolkits for testing if a system has specific capabilities (Ribeiro et al., 2020) or robustness to distribution shifts (Goel et al., 2021). However, most of these approaches inevitably overestimate a given system’s worst-case performance since they do not mimic the NLP system’s adversarial distribution1 . A promising technique for evaluating worst-case performance is the adversarial attack. However, although some adversarial attacks explicitly focus on specific linguistic levels of analysis (Belinkov and Bisk, 2018; Iyyer et al., 2018; Tan et al., 2020; Eger and Benz, 2020), many often simply rely on word embeddings or language models for perturbation proposal (see §4). While the latter may be useful to evaluate a system’s robustness to malicious actors, they are less useful for dimension-specific testing (e.g., reliability when encountering grammatical variation). This is because they often perturb the input across multiple dimensions at once, which may make the resulting adversaries unnatural. Hence, in this paper targeted at NLP researchers, practitioners, and policymakers, we make the case for reliability testing and reformulate adversa"
2021.acl-long.321,D19-1221,0,0.0261856,"Missing"
2021.acl-long.321,2020.acl-main.540,0,0.036226,"Missing"
2021.acl-long.321,P19-1559,0,0.0758545,"§5). Finally, we note that reliabil4155 ity testing and standards are established practices in engineering industries (e.g., aerospace (Nelson, 2003; Wilkinson et al., 2016)) and advocate for NL engineering to be at parity with these fields. 3.2 Evaluating worst-case performance in a label-scarce world A proposed approach for testing robustness to natural and adverse distribution shifts is to construct test sets using data from different domains or writing styles (Miller et al., 2020; Hendrycks et al., 2020), or to use a human vs. model method of constructing challenge sets (Nie et al., 2020; Zhang et al., 2019b). While they are the gold standard, such datasets are expensive to construct,3 making it infeasible to manually create worst-case test examples for each NLP system being evaluated. Consequently, these challenge sets necessarily overestimate each system’s worst-case performance when the inference distribution differs from the training one. Additionally, due to their crowdsourced nature, these challenge sets inevitably introduce distribution shifts across multiple dimensions at once, and even their own biases (Geva et al., 2019), unless explicitly controlled for. Building individual challenge"
2021.acl-long.321,2020.emnlp-main.43,0,0.0589549,"ension would be prohibitively expensive due to combinatorial explosion, even before having to account for concept drift (Widmer and Kubat, 1996). This coupling complicates efforts to design a nuanced and comprehensive testing regime. Hence, simulating variation in a controlled manner via reliability tests can be a complementary method of evaluating the system’s out-of-distribution generalization ability. 4 Adversarial Attacks as Reliability Tests We first give a brief introduction to adversarial attacks in NLP before showing how they can be used for reliability testing. We refer the reader to Zhang et al. (2020b) for a comprehensive survey. Existing work on NLP adversarial attacks perturbs the input at various levels of linguistic analysis: phonology (Eger and Benz, 2020), orthography (Ebrahimi et al., 2018), morphology (Tan et al., 2020), lexicon (Alzantot et al., 2018; Jin et al., 2020), and syntax (Iyyer et al., 2018). Early work did not place any constraints on the attacks and merely used the degradation to a tar3 Dua et al. (2019) reports a cost of 60k USD for 96k question–answer pairs. Algorithm 1 General Reliability Test Require: Data distribution Dd = {X , Y} modeling the dimension of intere"
2021.acl-long.450,P13-5002,0,0.0338061,"ite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin. 1 Introduction A number of formalisms have been introduced to analyze natural language at different linguistic levels. This includes syntactic structures in the form of phrasal and dependency trees, semantic structures in the form of meaning representations (Banarescu et al., 2013; Artzi et al., 2013), and discourse structures with Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or DiscourseLTAG (Webber, 2004). Many of these formalisms have a constituency structure, where textual units (e.g., phrases, sentences) are organized into nested constituents. For example, Figure 1 shows examples of a phrase structure tree and a sentence-level discourse tree (RST) that respectively represent how the phrases and clauses are hierarchically organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP. In this work, we cons"
2021.acl-long.450,W13-2322,0,0.0143633,"entation as a pre-requisite. Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours. In discourse parsing, our method outperforms SoTA by a good margin. 1 Introduction A number of formalisms have been introduced to analyze natural language at different linguistic levels. This includes syntactic structures in the form of phrasal and dependency trees, semantic structures in the form of meaning representations (Banarescu et al., 2013; Artzi et al., 2013), and discourse structures with Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) or DiscourseLTAG (Webber, 2004). Many of these formalisms have a constituency structure, where textual units (e.g., phrases, sentences) are organized into nested constituents. For example, Figure 1 shows examples of a phrase structure tree and a sentence-level discourse tree (RST) that respectively represent how the phrases and clauses are hierarchically organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP."
2021.acl-long.450,W14-6110,0,0.0215489,"Missing"
2021.acl-long.450,E17-2053,0,0.0462906,"Missing"
2021.acl-long.450,D16-1001,0,0.0802854,"ow the phrases and clauses are hierarchically organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP. In this work, we consider both phrasal (syntactic) and discourse parsing. In recent years, neural end-to-end parsing methods have outperformed traditional methods that use grammar, lexicon and hand-crafted features. These methods can be broadly categorized based on whether they employ a greedy transition-based, a globally optimized chart parsing or a greedy topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dyn"
2021.acl-long.450,N19-1423,0,0.0189031,"at the rate 0.75 exponentially at every 5k steps. Model selection for final evaluation is performed based on the labeled F1 score on the development set. Results without Pre-training From the results shown in Table 1, we see that our model achieves an F1 of 93.77, the highest among models that use 2 Extending the discourse parser to the document level may require handling of intra- and multi-sentential constituents differently, which we leave for future work. 3 http://nlp.cs.nyu.edu/evalb/ 4 https://github.com/ntunlpsg/ UnifiedParser_RST 5800 Model LR LP uate our parser with BERT embeddings (Devlin et al., 2019). They fine-tuned Bert-large-cased on the task, while in our work keeping it frozen was already good enough (gives training efficiency). As shown in Table 2, our model achieves an F1 of 95.7, which is on par with SoTA models. However, our parser runs faster than other methods. Specifically, our model runs at O(n) time complexity, while CKY needs O(n3 ). Comprehensive comparisons on parsing speed are presented later. F1 Top-Down Inference Stern et al. (2017a) 93.20 90.30 92.00 91.70 Shen et al. (2018) Nguyen et al. (2020) 92.91 92.75 Our Model 93.90 93.63 91.80 91.80 92.78 93.77 CKY/Chart Infer"
2021.acl-long.450,P15-1030,0,0.0153479,"a globally optimized chart parsing or a greedy topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one"
2021.acl-long.450,N16-1024,0,0.134987,"ctively represent how the phrases and clauses are hierarchically organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP. In this work, we consider both phrasal (syntactic) and discourse parsing. In recent years, neural end-to-end parsing methods have outperformed traditional methods that use grammar, lexicon and hand-crafted features. These methods can be broadly categorized based on whether they employ a greedy transition-based, a globally optimized chart parsing or a greedy topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012,"
2021.acl-long.450,N19-1076,0,0.0199561,"ns. Our approach differs from previous methods in that it represents the constituency structure as a series of splitting representations, and uses a Seq2Seq framework to model the splitting decision at each step. By enabling beam search, our model can find the best trees without the need to perform an expensive global search. We also unify discourse segmentation and parsing into one system by generalizing our model, which has been done for the first time to the best of our knowledge. Our splitting mechanism shares some similarities with Pointer Network (Vinyals et al., 2015a; Ma et al., 2018; Fernández-González and GómezRodríguez, 2019, 2020b) or head-selection approaches (Zhang et al., 2017; Kurita and Søgaard, 2019), but is distinct from them that in each decoding step, our method identifies the splitting point of a span and generates a new input for future steps instead of pointing to generate the next decoder input. 6 Conclusion We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework. Our method supports an efficient top-down decoding algorithm that uses a pointing function for scoring possible splitting points. The pointing mechanism captures global structural properties"
2021.acl-long.450,2020.acl-main.376,0,0.0356091,"Missing"
2021.acl-long.450,2020.acl-main.629,0,0.0344985,"Missing"
2021.acl-long.450,N18-1091,0,0.0231869,"Missing"
2021.acl-long.450,D18-1162,0,0.0334782,"Missing"
2021.acl-long.450,P13-1048,1,0.832276,"Missing"
2021.acl-long.450,J15-3002,1,0.851666,"Missing"
2021.acl-long.450,I17-2002,0,0.0123523,"parsing (Gaddy et al., 2018; Kitaev and Klein, 2018), or a greedy top-down algorithm (Stern et al., 2017a; Shen et al., 2018). Meanwhile, researchers also tried to cast the parsing problem into tasks that can be solved differently. For example, Gómez and Vilares (2018); Shen et al. (2018) proposed to map the syntactic tree of a sentence containing n tokens into a sequence of n − 1 labels or scalars. However, parsers of this type suffer from the exposure bias during inference. Beside these methods, Seq2Seq models have been used to generate a linearized form of the tree (Vinyals et al., 2015b; Kamigaito et al., 2017; Suzuki et al., 2018; Fernández-González and Gómez-Rodríguez, 2020a). However, these methods may generate invalid trees when the open and end brackets do not match. In discourse parsing, existing parsers receive the EDUs from a segmenter to build the discourse tree, which makes them susceptible to errors when the segmenter produces incorrect EDUs (Joty et al., 2012, 2015; Lin et al., 2019; Zhang et al., 2020a; Liu et al., 2020). There are also attempts which model constituency and discourse parsing jointly (Zhao and Huang, 2017) and do not need to perform EDU preprocessing. It is based on the"
2021.acl-long.450,P19-1340,0,0.0293222,"Missing"
2021.acl-long.450,P18-1249,0,0.059203,"y topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one incorrect splitting step may affect subsequ"
2021.acl-long.450,P19-1232,0,0.0174532,"eries of splitting representations, and uses a Seq2Seq framework to model the splitting decision at each step. By enabling beam search, our model can find the best trees without the need to perform an expensive global search. We also unify discourse segmentation and parsing into one system by generalizing our model, which has been done for the first time to the best of our knowledge. Our splitting mechanism shares some similarities with Pointer Network (Vinyals et al., 2015a; Ma et al., 2018; Fernández-González and GómezRodríguez, 2019, 2020b) or head-selection approaches (Zhang et al., 2017; Kurita and Søgaard, 2019), but is distinct from them that in each decoding step, our method identifies the splitting point of a span and generates a new input for future steps instead of pointing to generate the next decoder input. 6 Conclusion We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework. Our method supports an efficient top-down decoding algorithm that uses a pointing function for scoring possible splitting points. The pointing mechanism captures global structural properties of a tree and allows efficient training with a cross entropy loss. Our formulation,"
2021.acl-long.450,P19-1410,1,0.757829,"nd, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one incorrect splitting step may affect subsequent steps. Discourse parsing in RST requires an additional step – discourse segmentation which involves breaking the text into contiguous clause-like units called Elementary Discourse Units or EDUs (Figure 1). Traditionally, segmentation has been 5795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5795–5807 August 1–6, 2021. ©2021 Association for Comp"
2021.acl-long.450,Q17-1004,0,0.0447472,"ses are hierarchically organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP. In this work, we consider both phrasal (syntactic) and discourse parsing. In recent years, neural end-to-end parsing methods have outperformed traditional methods that use grammar, lexicon and hand-crafted features. These methods can be broadly categorized based on whether they employ a greedy transition-based, a globally optimized chart parsing or a greedy topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, the"
2021.acl-long.450,2020.coling-main.591,0,0.0194996,"fer from the exposure bias during inference. Beside these methods, Seq2Seq models have been used to generate a linearized form of the tree (Vinyals et al., 2015b; Kamigaito et al., 2017; Suzuki et al., 2018; Fernández-González and Gómez-Rodríguez, 2020a). However, these methods may generate invalid trees when the open and end brackets do not match. In discourse parsing, existing parsers receive the EDUs from a segmenter to build the discourse tree, which makes them susceptible to errors when the segmenter produces incorrect EDUs (Joty et al., 2012, 2015; Lin et al., 2019; Zhang et al., 2020a; Liu et al., 2020). There are also attempts which model constituency and discourse parsing jointly (Zhao and Huang, 2017) and do not need to perform EDU preprocessing. It is based on the finding that each EDU generally corresponds to a constituent in constituency tree, i.e., discourse structure usually aligns with constituency structure. However, it has the drawback that it needs to build joint syntactodiscourse data set for training which is not easily adaptable to new languages and domains. Our approach differs from previous methods in that it represents the constituency structure as a series of splitting rep"
2021.acl-long.450,P18-1130,0,0.0172573,"nguages and domains. Our approach differs from previous methods in that it represents the constituency structure as a series of splitting representations, and uses a Seq2Seq framework to model the splitting decision at each step. By enabling beam search, our model can find the best trees without the need to perform an expensive global search. We also unify discourse segmentation and parsing into one system by generalizing our model, which has been done for the first time to the best of our knowledge. Our splitting mechanism shares some similarities with Pointer Network (Vinyals et al., 2015a; Ma et al., 2018; Fernández-González and GómezRodríguez, 2019, 2020b) or head-selection approaches (Zhang et al., 2017; Kurita and Søgaard, 2019), but is distinct from them that in each decoding step, our method identifies the splitting point of a span and generates a new input for future steps instead of pointing to generate the next decoder input. 6 Conclusion We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework. Our method supports an efficient top-down decoding algorithm that uses a pointing function for scoring possible splitting points. The pointing mec"
2021.acl-long.450,J93-2004,0,0.0740475,"25: T = labeled-spans ∪ labeled-singletons By enabling beam search, our method can find the best tree by comparing high scoring trees within a reasonable search space, making our model competitive with existing structured (globally) inference methods that use more expensive algorithms like CKY and/or larger models (Kitaev and Klein, 2018; Zhang et al., 2020b). 4 Experiment Datasets and Metrics To show the effectiveness of our approach, we conduct experiments on both syntactic and sentence-level RST parsing tasks.2 We use the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) (Marcus et al., 1993) for syntactic parsing and RST Discourse Treebank (RST-DT) (Lynn et al., 2002) for discourse parsing. For syntactic parsing, we also experiment with the multilingual parsing tasks on seven different languages from the SPMRL 2013-2014 shared task (Seddah et al., 2013): Basque, French, German, Hungarian, Korean, Polish and Swedish. For evaluation on syntactic parsing, we report the standard labeled precision (LP), labeled recall (LR), and labelled F1 computed by evalb3 . For evaluation on RST-DT, we report the standard span, nuclearity label, relation label F1 scores, computed using the implemen"
2021.acl-long.450,2020.acl-main.301,1,0.680929,"coring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one incorrect splitting step may affect subsequent steps. Discourse parsing in RST requires an additional step – discourse segmentation which involves breaking the text into contiguous clause-like units called Elementary Discourse Units or EDUs (Figure 1). Traditionally, segmentation has been 5795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5795–5807 August 1–6, 2021. ©2021 Association for Computational Linguistics"
2021.acl-long.450,N18-1202,0,0.0409508,"Missing"
2021.acl-long.450,N07-1051,0,0.343736,"Missing"
2021.acl-long.450,P18-1108,0,0.0614542,"ds, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one incorrect splitting step may affect subsequent steps. Discourse parsing in RST requires an additional step – discourse segmentation which involves breaking the text into contiguous clause-like units called Elementary Discourse Units or EDUs (Figure 1). Traditionally, segmentation has been 5795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5795–5807 August 1–6, 2021. ©2021 As"
2021.acl-long.450,N03-1030,0,0.674929,"RST requires an additional step – discourse segmentation which involves breaking the text into contiguous clause-like units called Elementary Discourse Units or EDUs (Figure 1). Traditionally, segmentation has been 5795 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5795–5807 August 1–6, 2021. ©2021 Association for Computational Linguistics considered separately and as a prerequisite step for the parsing task which links the EDUs (and larger spans) into a discourse tree (Soricut and Marcu, 2003; Joty et al., 2012; Wang et al., 2017). In this way, the errors in discourse segmentation can propagate to discourse parsing (Lin et al., 2019). In this paper, we propose a generic top-down neural framework for constituency parsing that we validate on both syntactic and sentence-level discourse parsing. Our main contributions are: • We cast the constituency parsing task into a series of conditional splitting decisions and use a seq2seq architecture to model the splitting decision at each decoding step. Our parsing model, which is an instance of a Pointer Network (Vinyals et al., 2015a), estim"
2021.acl-long.450,P17-1076,0,0.0477206,"Missing"
2021.acl-long.450,D17-1178,0,0.04056,"Missing"
2021.acl-long.450,P18-2097,0,0.0129181,"2018; Kitaev and Klein, 2018), or a greedy top-down algorithm (Stern et al., 2017a; Shen et al., 2018). Meanwhile, researchers also tried to cast the parsing problem into tasks that can be solved differently. For example, Gómez and Vilares (2018); Shen et al. (2018) proposed to map the syntactic tree of a sentence containing n tokens into a sequence of n − 1 labels or scalars. However, parsers of this type suffer from the exposure bias during inference. Beside these methods, Seq2Seq models have been used to generate a linearized form of the tree (Vinyals et al., 2015b; Kamigaito et al., 2017; Suzuki et al., 2018; Fernández-González and Gómez-Rodríguez, 2020a). However, these methods may generate invalid trees when the open and end brackets do not match. In discourse parsing, existing parsers receive the EDUs from a segmenter to build the discourse tree, which makes them susceptible to errors when the segmenter produces incorrect EDUs (Joty et al., 2012, 2015; Lin et al., 2019; Zhang et al., 2020a; Liu et al., 2020). There are also attempts which model constituency and discourse parsing jointly (Zhao and Huang, 2017) and do not need to perform EDU preprocessing. It is based on the finding that each ED"
2021.acl-long.450,P17-2029,0,0.127602,"y organized into a constituency structure. Developing efficient and effective parsing solutions has always been a key focus in NLP. In this work, we consider both phrasal (syntactic) and discourse parsing. In recent years, neural end-to-end parsing methods have outperformed traditional methods that use grammar, lexicon and hand-crafted features. These methods can be broadly categorized based on whether they employ a greedy transition-based, a globally optimized chart parsing or a greedy topdown algorithm. Transition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perfo"
2021.acl-long.450,2020.acl-main.299,0,0.152154,"it frozen was already good enough (gives training efficiency). As shown in Table 2, our model achieves an F1 of 95.7, which is on par with SoTA models. However, our parser runs faster than other methods. Specifically, our model runs at O(n) time complexity, while CKY needs O(n3 ). Comprehensive comparisons on parsing speed are presented later. F1 Top-Down Inference Stern et al. (2017a) 93.20 90.30 92.00 91.70 Shen et al. (2018) Nguyen et al. (2020) 92.91 92.75 Our Model 93.90 93.63 91.80 91.80 92.78 93.77 CKY/Chart Inference Gaddy et al. (2018) 91.76 92.41 Kitaev and Klein (2018) 93.20 93.90 Wei et al. (2020) 93.3 94.1 Zhang et al. (2020b) 93.84 93.58 92.08 93.55 93.7 93.71 4.2 Other Approaches Gómez and Vilares (2018) 90.7 Liu and Zhang (2017) 91.8 92.57 92.56 92.56 Stern et al. (2017b) Zhou and Zhao (2019) 93.64 93.92 93.78 We use the identical hyper-parameters and optimizer setups as in English PTB. We follow the standard train/valid/test split provided in the SPMRL datasets; details are reported in the Table 3. Table 1: Results for single models (no pre-training) on the PTB WSJ test set, Section 23. Model F1 Nguyen et al. (2020) Our model 95.5 95.7 Kitaev et al. (2019) Zhang et al. (2020b) Wei"
2021.acl-long.450,2020.acl-main.569,0,0.0740901,"nsition-based parsers (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Wang et al., 2017) generate trees auto-regressively as a form of shift-reduce decisions. Though computationally attractive, the local decisions made at each step may propagate errors to subsequent steps due to exposure bias (Bengio et al., 2015). Moreover, there may be mismatches in shift and reduce steps, resulting in invalid trees. Chart based methods, on the other hand, train neural scoring functions to model the tree structure globally (Durrett and Klein, 2015; Gaddy et al., 2018; Kitaev and Klein, 2018; Zhang et al., 2020b; Joty et al., 2012, 2013). By utilizing dynamic programming, these methods can perform exact inference to combine these constituent scores into finding the highest probable tree. However, they are generally slow with at least O(n3 ) time complexity. Greedy top-down parsers find the split points recursively and have received much attention lately due to their efficiency, which is usually O(n2 ) (Stern et al., 2017a; Shen et al., 2018; Lin et al., 2019; Nguyen et al., 2020). However, they still suffer from exposure bias, where one incorrect splitting step may affect subsequent steps. Discourse"
2021.acl-long.450,E17-1063,0,0.0237039,"ncy structure as a series of splitting representations, and uses a Seq2Seq framework to model the splitting decision at each step. By enabling beam search, our model can find the best trees without the need to perform an expensive global search. We also unify discourse segmentation and parsing into one system by generalizing our model, which has been done for the first time to the best of our knowledge. Our splitting mechanism shares some similarities with Pointer Network (Vinyals et al., 2015a; Ma et al., 2018; Fernández-González and GómezRodríguez, 2019, 2020b) or head-selection approaches (Zhang et al., 2017; Kurita and Søgaard, 2019), but is distinct from them that in each decoding step, our method identifies the splitting point of a span and generates a new input for future steps instead of pointing to generate the next decoder input. 6 Conclusion We have presented a novel, generic parsing method for constituency parsing based on a Seq2Seq framework. Our method supports an efficient top-down decoding algorithm that uses a pointing function for scoring possible splitting points. The pointing mechanism captures global structural properties of a tree and allows efficient training with a cross entr"
2021.acl-long.450,D17-1225,0,0.0186956,"generate a linearized form of the tree (Vinyals et al., 2015b; Kamigaito et al., 2017; Suzuki et al., 2018; Fernández-González and Gómez-Rodríguez, 2020a). However, these methods may generate invalid trees when the open and end brackets do not match. In discourse parsing, existing parsers receive the EDUs from a segmenter to build the discourse tree, which makes them susceptible to errors when the segmenter produces incorrect EDUs (Joty et al., 2012, 2015; Lin et al., 2019; Zhang et al., 2020a; Liu et al., 2020). There are also attempts which model constituency and discourse parsing jointly (Zhao and Huang, 2017) and do not need to perform EDU preprocessing. It is based on the finding that each EDU generally corresponds to a constituent in constituency tree, i.e., discourse structure usually aligns with constituency structure. However, it has the drawback that it needs to build joint syntactodiscourse data set for training which is not easily adaptable to new languages and domains. Our approach differs from previous methods in that it represents the constituency structure as a series of splitting representations, and uses a Seq2Seq framework to model the splitting decision at each step. By enabling be"
2021.acl-long.450,P19-1230,0,0.15722,"ods. Specifically, our model runs at O(n) time complexity, while CKY needs O(n3 ). Comprehensive comparisons on parsing speed are presented later. F1 Top-Down Inference Stern et al. (2017a) 93.20 90.30 92.00 91.70 Shen et al. (2018) Nguyen et al. (2020) 92.91 92.75 Our Model 93.90 93.63 91.80 91.80 92.78 93.77 CKY/Chart Inference Gaddy et al. (2018) 91.76 92.41 Kitaev and Klein (2018) 93.20 93.90 Wei et al. (2020) 93.3 94.1 Zhang et al. (2020b) 93.84 93.58 92.08 93.55 93.7 93.71 4.2 Other Approaches Gómez and Vilares (2018) 90.7 Liu and Zhang (2017) 91.8 92.57 92.56 92.56 Stern et al. (2017b) Zhou and Zhao (2019) 93.64 93.92 93.78 We use the identical hyper-parameters and optimizer setups as in English PTB. We follow the standard train/valid/test split provided in the SPMRL datasets; details are reported in the Table 3. Table 1: Results for single models (no pre-training) on the PTB WSJ test set, Section 23. Model F1 Nguyen et al. (2020) Our model 95.5 95.7 Kitaev et al. (2019) Zhang et al. (2020b) Wei et al. (2020) Zhou and Zhao (2019) 95.6 95.7 95.8 95.8 SPMRL Multilingual Syntactic Parsing Language Train Valid Test Basque French German Hungarian Korean Polish Swedish 7,577 14,759 40,472 8,146 23,01"
2021.acl-long.450,P13-1043,0,0.0813162,"Missing"
2021.acl-long.453,2020.repl4nlp-1.1,0,0.131258,"Missing"
2021.acl-long.453,2021.acl-long.154,1,0.628573,"Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER pe"
2021.acl-long.453,R19-2004,0,0.0242052,"resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection"
2021.acl-long.453,D18-1366,0,0.0226682,"projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual da"
2021.acl-long.453,2020.acl-main.747,0,0.173144,"Missing"
2021.acl-long.453,N19-1423,0,0.0260841,"ine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang e"
2021.acl-long.453,2020.emnlp-main.488,1,0.922928,"e than 100 languages. Alternatively, there are also many pretrained MT models conveniently accessible, e.g., more than 1,000 MarianMT (Junczys-Dowmunt et al., 2018; Kim et al., 2019) models have been released on the Hugging Face model hub.3 Note that the instance-based transfer methods add limited semantic variety to the training set, since they only translate entities and the corresponding contexts to a different language. In contrast, data augmentation has been proven to be a successful method for tackling the data scarcity problem. Inspired by a recent monolingual data augmentation method (Ding et al., 2020), we propose a generation-based multilingual data augmentation method to increase the diversity, where LMs are trained on multilingual labeled data and then used to generate more synthetic training data. We conduct extensive experiments and analysis to verify the effectiveness of our methods. Our main contributions can be summarized as follows: augmentation method for NER, which leverages the multilingual language models to add more diversity to the training data. • Through empirical experiments, we observe that when fine-tuning pretrained multilingual LMs for low-resource cross-lingual NER, t"
2021.acl-long.453,2020.acl-main.413,0,0.0189525,"tic data and the language-independent features from multilingual synthetic data. An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.1 1 Introduction Named entity recognition (NER) aims to identify and classify entities in a text into predefined types, which is an essential tool for information extraction. It has also been proven to be useful in various downstream natural language processing (NLP) tasks, including information retrieval (Banerjee et al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-l"
2021.acl-long.453,D19-1100,0,0.150896,"ip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et al., 2020; Bari et al., 2021). Recently, Wu et al. (2020) unify instance-based and model-based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot"
2021.acl-long.453,P18-4020,0,0.024355,"Missing"
2021.acl-long.453,D19-1138,0,0.0154432,"ntities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representations. Recent methods have achieved significant performance improvement by fine-tuning large scale pretrained multilingual LMs (Devlin et al., 2019; Keung et al., 2019; Conneau et al., 2020). Besides, there are also some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech"
2021.acl-long.453,N18-2072,0,0.0234329,"n Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effect"
2021.acl-long.453,P19-1553,0,0.0173464,"English NER performance. Particularly, En + Multi-Tran achieves the best performance. Therefore, we can also use multilingual translated data to improve low-resource monolingual NER performance. 3.3 Generation-based Multilingual Data Augmentation In this section, we run experiments to verify whether applying generation-based data augmentation methods to the multilingual translated data can further improve cross-lingual performance in the low resource scenarios. Experimental settings We follow the steps described in §2.2 to implement the proposed data augmentation framework on top of LSTM-LM (Kruengkrai, 2019) and mBART (Liu et al., 2020) sep5839 500 Method 1k 2k de es nl avg de es nl avg de es nl avg En + Multi-Tran MulDA-LSTM MulDA-mBART 70.40 70.04 72.37 65.70 67.38 68.19 72.20 72.81 74.59 69.43 70.08 71.72 73.42 74.80 75.04 72.71 74.27 74.56 76.74 77.21 77.78 74.29 75.42 75.79 75.91 76.05 77.54 76.04 76.05 76.32 77.85 78.46 78.21 76.60 76.85 77.36 En + Tgt-Tran BiDA-LSTM 69.16 72.51 64.57 68.77 71.40 72.65 68.38 71.31 73.63 74.97 69.81 73.69 75.83 77.51 73.09 75.39 74.45 76.59 75.88 76.47 78.40 78.97 76.24 77.34 Table 4: Cross-lingual NER results of models trained on multilingual augmented data"
2021.acl-long.453,2020.acl-main.523,1,0.729419,"t al., 2019), question answering (Fabbri et al., 2020) and text summarization (Nallapati et al., 2016). However, except for some resource-rich languages ∗ Equal contribution, order decided by coin flip. Linlin Liu and Bosheng Ding are under the Joint PhD Program between Alibaba and Nanyang Technological University. 1 Our code is available at https://ntunlpsg. github.io/project/mulda/. (e.g., English, German), training sets for most of the other languages are still very limited. Moreover, it is usually expensive and time-consuming to annotate such data, particularly for low-resource languages (Kruengkrai et al., 2020). Therefore, zero-shot cross-lingual NER has attracted growing interest recently, especially with the influx of deep learning methods (Mayhew et al., 2017; Joty et al., 2017; Jain et al., 2019; Bari et al., 2021). Existing approaches to cross-lingual NER can be roughly grouped into two main categories: instance-based transfer via machine translation (MT) and label projection (Mayhew et al., 2017; Jain et al., 2019), and model-based transfer with aligned cross-lingual word representations or pretrained multilingual language models (Joty et al., 2017; Baumann, 2019; Wang et al., 2020; Conneau et"
2021.acl-long.453,2020.lifelongnlp-1.3,0,0.0280795,"ining data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging place"
2021.acl-long.453,2020.tacl-1.47,0,0.298836,"based transfer via knowledge distillation. These recent methods have demonstrated promising zero-shot cross-lingual NER performance. However, most of them assume the availability of a considerable amount of training data in the source language. When we reduce the size of the training data, we observe significant performance decrease. For instance-based transfer, decreasing training set size also amplifies the negative impact of the noise introduced by MT and label projection. For model-based transfer, although the large-scale pretrained multilingual language models (LM) (Conneau et al., 2020; Liu et al., 2020) have achieved state-of-the-art performance on many cross-lingual transfer tasks, simply fine-tuning them on a small training set is prone to over-fitting (Wu et al., 2018; Si et al., 2020; Kou et al., 2020). To address the above problems under the setting of low-resource cross-lingual NER, we propose a multilingual data augmentation (MulDA) framework to make better use of the cross-lingual 5834 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5834–5846 August 1–6, 2021. ©20"
2021.acl-long.453,D17-1269,0,0.046708,"Missing"
2021.acl-long.453,2021.findings-acl.267,1,0.434557,"n. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al., 2021; Mohiuddin et al., 2021), but most of the exiting methods overlook the better utilization of multilingual training data when such resources are available. 5 Conclusions We have proposed a multilingual data augmentation framework for low resource cross-lingual NER. Our labeled sequence translation method effectively avoids many label projection related problems by leveraging placeholders during MT. Our generation-based multilingual data augmentation method generates high quality synthetic training data to add more diversity. The proposed framework has demonstrated encouraging performance improvement in various low-res"
2021.acl-long.453,K16-1028,0,0.0200591,"Missing"
2021.acl-long.453,P17-1135,0,0.0321719,"Missing"
2021.acl-long.453,P17-1178,0,0.0256521,"g, while BiDA-LSTM trains one model for each target language in each setting. Therefore, we compare BiDA-LSTM with 8 https://github.com/pytorch/fairseq/blob/master/ examples/mbart/README.md En + Tgt-Tran only. As we can see, the proposed multilingual data augmentation methods further improve cross-lingual NER performance consistently. For the 1k and 2k setting, MulDA-LSTM achieves comparable average performance as BiDA-LSTM. 3.4 Evaluation on More Distant Languages We evaluate the proposed method on a wider range of target languages in this section. Experimental settings The Wikiann NER data (Pan et al., 2017) processed by Hu et al. (2020) is used in these experiments. 1k English sentences S ) are sampled from the gold train data to sim(D1k ulate the low resource scenarios. We also assume MT models are not available for all of the target languages, so we only translate the sampled English sentences to 6 target languages: ar, fr, it, ja, T tr and zh. Dtrans is used to denote the translated target-language sentences by following steps described in §2.1. The low quality translated sentences are filtered out in the same way as §3.2. To evaluate our method in the semi-supervised setting, we also sample"
2021.acl-long.453,2020.coling-main.305,0,0.0361386,"t combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al., 2020; Bari et al"
2021.acl-long.453,N12-1052,0,0.0914152,"Missing"
2021.acl-long.453,2021.naacl-main.282,1,0.721511,".1 Example 2 Gold EN: . . . (LOC U.S. Midwest) . . . Jain et al. (2019): . . . (LOC Mittlerer Westen) der (LOC USA) ... Li et al. (2020): . . . Mittlerer (LOC Westen) der (LOC USA) ... Ours: . . . (LOC Mittlerer Westen der USA) . . . Figure 6: Two examples that the previous methods fail to find the correct entity boundaries. Figure 7: Examples of multilingual sentences. the NER tags can be viewed as a shared vocabulary between different languages. As a result, we find that some generated sentences contain tokens from multiple languages, which are useful to help improve cross-lingual transfer (Tan and Joty, 2021). Two examples are shown in Figure 7. 4 Case Study Effectiveness in Label Projection The label projection step of the previous methods needs to locate the entities and determine their boundaries, which is vulnerable to many problems, such as word order change, long entities, etc. Our method effectively avoids these problems with placeholders. In the two examples shown in Figure 6, Jain et al. (2019) either labeled only part of the whole entity or incorrectly split the entity into two, Li et al. (2020) incorrectly split the entities into two in both examples, while our method can correctly map"
2021.acl-long.453,W14-1614,0,0.0595736,"Missing"
2021.acl-long.453,W02-2024,0,0.263791,"forward layer to the Transformer final layer for label classification. Specifically, to demonstrate that our framework can help achieve additional performance gain even on the top of the state-of-the-art multilingual LMs, the checkpoint of the pretrained XLM-R large (Conneau et al., 2020) model is used to initialize our NER models. 3.1 Labeled Sequence Translation We finetune the NER model on the translated targetlanguage data to compare our labeled sequence translation method (§2.1) with the existing instancebased transfer methods. Experimental settings The CoNLL02/03 NER dataset (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) is used for evaluation, which contains data in four different languages: English, German, Dutch and Spanish. All of the data are annotated with the same set of NER tags. We follow the steps described in §2.1 to translate En7 Similar to the token classification https://github.com/huggingface/transformers. model in glish train data to the other three languages. Following Jain et al. (2019) and Li et al. (2020), Google translation system is used in the experiments. Since our NER model is more powerful than those used by Jain et al. (2019) and Li et al. (2020"
2021.acl-long.453,K16-1022,0,0.0163074,"t language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word order change, entity span determination, noise-sensitive similarity metrics and so on. Model-based transfer directly applies the model trained on the source language to the targetlanguage test data (T¨ackstr¨om et al., 2012; Ni et al., 2017; Joty et al., 2017; Chaudhary et al., 2018), which heavily relies on the quality of cross-lingual representation"
2021.acl-long.453,Q16-1035,0,0.0256703,"lso some approaches that combine instance-based and model-based transfer (Xu et al., 2020; Wu et al., 2020). Compared with these methods, our approach leverages MT models and LMs to add more diversity to the training data, and prevents over-fitting on language-specific features by fine-tuning NER models on multilingual data. Data augmentation Data augmentation (Simard et al., 1998) adds more diversity to training data to help improve model generalization, which has been widely used in many fields, such as computer vision (Zhang et al., 2018), speech (Cui et al., 2015; Park et al., 2019), NLP (Wang and Eisner, 2016; Sun et al., 2020) and so on. For NLP, back translation (Sennrich et al., 2016) is one of the most successful data augmentation approaches, which translates target-language monolingual data to the source language to generate more parallel data for MT model training. Other popular approaches include synonym replacement (Kobayashi, 2018), random deletion/swap/insertion (Sun et al., 2020; Kumar et al., 2020), generation (Ding et al., 2020), etc. Data augmentation has also been proven to be useful in the cross-lingual settings (Zhang et al., 2019; Singh et al., 2020; Riabi et al., 2020; Qin et al"
2021.acl-long.453,D18-1034,0,0.473471,"anslation method to translate the source ping with alignment models or algorithms. Howtraining data to a desired language. Compared with exiting methods, our labeled sentence trans- ever, these methods suffer from a few label projeclation approach leverages placeholders for la- tion problems, such as word order change, wordspan determination (Li et al., 2020), and so on. An bel projection, which effectively avoids many alternative to avoid the label projection problems issues faced during word alignment, such as word order change, entity span determination, noise- is word-by-word translation (Xie et al., 2018), but often at the sacrifice of the translation quality. sensitive similarity metrics and so on. We address the problems identified above by • We propose a generation-based multilingual data first replacing named entities with contextual place2 holders before sentence translation, and then after https://cloud.google.com/translate 3 https://huggingface.co/transformers/model doc/marian.html translation, we replace placeholders in translated 5835 B-PER E-PER O O O S-LOC O Jamie Valentine was born in London . Labeled sentence in the source language: [PER Jamie Valentine] was born in [LOC London]."
2021.acl-long.453,D19-1092,0,0.0452794,"Missing"
2021.acl-long.453,2020.emnlp-main.410,0,0.0245173,"Missing"
2021.acl-long.453,H01-1035,0,0.13327,"k into the data generated by our multilingual data augmentation method. During LM training, Related Work Cross-lingual NER There has been growing interest in cross-lingual NER. Prior approaches can be grouped into two main categories, instancebased transfer and model-based transfer. Instancebased transfer translates source-language training data to target language, and then apply label projection to annotate the translated data (Tiedemann et al., 2014; Jain et al., 2019). Instead of MT, some earlier approaches also use parallel corpora to construct pseudo training data in the target language (Yarowsky et al., 2001; Fu et al., 2014). To minimize resource requirement, Mayhew et al. (2017) and Xie et al. (2018) design frameworks that only rely on word-to-word/phrase-to-phrase translation with bilingual dictionaries. Besides, there are also many studies on improving label projection quality 5841 with additional feature or better mapping methods (Tsai et al., 2016; Li et al., 2020). Different from these methods, our labeled sentence translation approach leverages placeholders to determine the position of entities after translation, which effectively avoids many issues during label projection, such as word o"
2021.eacl-main.308,N04-1015,0,0.227713,"ory (Grosz et al., 1995), the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of co"
2021.eacl-main.308,N18-1150,0,0.015332,"global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an additional feature in downstream tasks like readability assessment and essay scoring (Barzilay and Lapata, 2008; Mesgar and Strube, 2018). But since the concept of coherence goes beyond these constrained tasks and domains, so should the models. Given the recent advances in neural NLP methods, with claims of reaching human parity in machine translation (Hassan et al., 2018), fluency in summarization (Liu et al., 2017; Celikyilmaz et al., 2018), or context-consistent response generation (Zhang et al., 2020; Hosseini-Asl et al., 2020), coherence modeling of machine-generated texts, par3528 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3528–3539 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ticularly at a document-level, is now more crucial than ever (L¨aubli et al., 2018; Sharma et al., 2019). Traditional task-specific evaluation methods (e.g., BLEU, ROUGE) may not be an accurate reflection of their real-world performance in terms of readabili"
2021.eacl-main.308,N07-1055,0,0.0480123,", the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an ad"
2021.eacl-main.308,P11-1118,0,0.255434,"n, 1976). Computational models that can assess coherence have applications in text generation and ranking, such as summarization, machine translation, essay scoring and dialog systems. Researchers have proposed a number of formal theories of discourse coherence, which have in∗ *Equal contribution Code and data used for evaluation available at https://ntunlpsg.github.io/project/ coherence/coh-eval/ 1 spired the development of many coherence models – both traditional and neural ones. Inspired by the Centering Theory (Grosz et al., 1995), the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 20"
2021.eacl-main.308,P11-2022,0,0.223592,"n, 1976). Computational models that can assess coherence have applications in text generation and ranking, such as summarization, machine translation, essay scoring and dialog systems. Researchers have proposed a number of formal theories of discourse coherence, which have in∗ *Equal contribution Code and data used for evaluation available at https://ntunlpsg.github.io/project/ coherence/coh-eval/ 1 spired the development of many coherence models – both traditional and neural ones. Inspired by the Centering Theory (Grosz et al., 1995), the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 20"
2021.eacl-main.308,P05-1018,0,0.110438,"ntages of generative models and uses a smaller negative sampling space that can learn against incorrect orderings. Moon et al. (2019) propose a unified model that incorporates sentence syntax, inter-sentence coherence relations, and global topic structures in a single Siamese framework. We benchmark the performance of five representative coherence models on the tasks discussed above. Our selected models comprise of both traditional and neural models. Moreover, two models are currently the state-of-the-art at the time of submission (Transferable and Unified Neural Model). Entity Grid (EG RID). Barzilay and Lapata (2005, 2008) introduced the popular entity-based model for representing and assessing text coherence motivated by the Centering Theory (Grosz et al., 1995). This model represents a text with a twodimensional array called an entity grid, that captures transitions of discourse entities across sentences. These local entity transitions are used as deciding patterns for text coherence; a local entity transition of length k is a sequence of {S,O,X,–}k representing grammatical roles (Subject, Object, Other, and Absent, respectively) played by an entity in k consecutive sentences. The salience of the entit"
2021.eacl-main.308,J95-2003,0,0.694774,"Missing"
2021.eacl-main.308,D19-1051,0,0.035168,"Missing"
2021.eacl-main.308,D18-1512,0,0.0653413,"Missing"
2021.eacl-main.308,D14-1218,0,0.0194531,"human judgments. This leads us to conclude that there is a possible mismatch in the task setting that is used to train coherence models. Models trained on traditional synthetic tasks do not seem to be learning features that are useful for downstream applications. We hope that our results will motivate the broadening of the standard of coherence model evaluations to include more downstream tasks, and also motivate the redesigning of the training paradigm for coherence models. 2 Coherence Models Advancements in deep learning have inspired researchers to neuralize many of the traditional models. Li and Hovy (2014) model syntax and intersentence relations using a recurrent sentence encoder followed by a fully-connected layer. In a follow-up work, Li and Jurafsky (2017) use generative models to incorporate global topic information with an encoder-decoder architecture. Mohiuddin et al. (2018) propose a neural entity grid model using convolutions over distributed representations of entity transitions. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Xu et al. (2019) propose a local discriminative model that retains the advantages of generative models and use"
2021.eacl-main.308,D17-1019,0,0.0177113,"n traditional synthetic tasks do not seem to be learning features that are useful for downstream applications. We hope that our results will motivate the broadening of the standard of coherence model evaluations to include more downstream tasks, and also motivate the redesigning of the training paradigm for coherence models. 2 Coherence Models Advancements in deep learning have inspired researchers to neuralize many of the traditional models. Li and Hovy (2014) model syntax and intersentence relations using a recurrent sentence encoder followed by a fully-connected layer. In a follow-up work, Li and Jurafsky (2017) use generative models to incorporate global topic information with an encoder-decoder architecture. Mohiuddin et al. (2018) propose a neural entity grid model using convolutions over distributed representations of entity transitions. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Xu et al. (2019) propose a local discriminative model that retains the advantages of generative models and uses a smaller negative sampling space that can learn against incorrect orderings. Moon et al. (2019) propose a unified model that incorporates sentence syntax,"
2021.eacl-main.308,W04-1013,0,0.0468047,"Missing"
2021.eacl-main.308,P11-1100,0,0.0743008,"Missing"
2021.eacl-main.308,D16-1230,0,0.0903183,"Missing"
2021.eacl-main.308,D19-1387,0,0.015475,"0.68 0.71 0.55 0.68 0.52 0.70 0.57 0.38 0.35 Table 6: Abstractive Agreement and Extractive Agreement shows the AC1 agreements for the pairwise ranking of the generated abstractive summaries and extractive summaries between two annotators and the models, respectively. Setup. We use the CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) for this task. We collect the reference summaries from the CNN/DM testset as well as the summaries generated by the following four representative abstractive summarization systems: (a) Pointer-Generator (PG) (See et al., 2017), (b) B ERT S UM E XTA BS (BSEA) (Liu and Lapata, 2019), (c) UniLM (Dong et al., 2019), and (d) SENECA (Sharma et al., 2019). As discussed, we directly use the coherence models trained on the WSJ dataset for the global discrimination task. The coherence models predict the scores for each system-generated summary in the testset. The scores produced by the models are then used to rank the system-generated summaries of the same original article. We conducted a user study to validate the effectiveness of the rankings produced by the coherence models. We randomly sampled 10 sets of summaries from the dataset with each set containing four generated sum"
2021.eacl-main.308,D12-1106,0,0.0252565,"er and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an additional feature in downstream tasks like readability assessment and"
2021.eacl-main.308,D18-1464,0,0.0664957,"veloping novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an additional feature in downstream tasks like readability assessment and essay scoring (Barzilay and Lapata, 2008; Mesgar and Strube, 2018). But since the concept of coherence goes beyond these constrained tasks and domains, so should the models. Given the recent advances in neural NLP methods, with claims of reaching human parity in machine translation (Hassan et al., 2018), fluency in summarization (Liu et al., 2017; Celikyilmaz et al., 2018), or context-consistent response generation (Zhang et al., 2020; Hosseini-Asl et al., 2020), coherence modeling of machine-generated texts, par3528 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3528–3539 April 19 - 23, 202"
2021.eacl-main.308,P18-1052,1,0.949082,"our results will motivate the broadening of the standard of coherence model evaluations to include more downstream tasks, and also motivate the redesigning of the training paradigm for coherence models. 2 Coherence Models Advancements in deep learning have inspired researchers to neuralize many of the traditional models. Li and Hovy (2014) model syntax and intersentence relations using a recurrent sentence encoder followed by a fully-connected layer. In a follow-up work, Li and Jurafsky (2017) use generative models to incorporate global topic information with an encoder-decoder architecture. Mohiuddin et al. (2018) propose a neural entity grid model using convolutions over distributed representations of entity transitions. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Xu et al. (2019) propose a local discriminative model that retains the advantages of generative models and uses a smaller negative sampling space that can learn against incorrect orderings. Moon et al. (2019) propose a unified model that incorporates sentence syntax, inter-sentence coherence relations, and global topic structures in a single Siamese framework. We benchmark the performance"
2021.eacl-main.308,D19-1231,1,0.855357,"local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an additional feature in downstream tasks like readability assessment and essay scoring (Barzilay and Lapata, 2008; Mesgar and Strube, 2018). But since the concept of coherence goes beyond these constrained tasks and domains, so should the models. Given the recent advances in neural NLP methods, with claims of reaching human parity in machine translation (Hassan et al., 2018), fluency in summarization (Liu et al., 2017; Celikyilmaz et al., 2018), or context-consistent response generation (Zhang et al., 2020"
2021.eacl-main.308,K16-1028,0,0.07976,"Missing"
2021.eacl-main.308,P17-1121,1,0.9079,", and Absent, respectively) played by an entity in k consecutive sentences. The salience of the entities, quantified by the occurrence frequency, is also incorporated to identify transitions of important entities. Elsner and Charniak (2011b) improve the basic entity grid by including non-head nouns as entities (with the grammatical role X). Instead of using a coreference resolver, they match the nouns 3529 to detect coreferent entities. In our work, we consider this version of the entity grid model. Neural Entity Grid (N EURAL EG RID). A neural version of the entity grid model was proposed by Nguyen and Joty (2017). The grammatical roles in the grid are converted into their distributed representations, and the entity transitions are modeled in the distributed space by performing convolutions over it. The final coherence scores are computed from convolved features that have gone through a spatial max-pooling operation. A global, documentlevel pairwise loss is used to train the model. Lexicalized Neural Entity Grid. Mohiuddin et al. (2018) propose an improvement of the neural entity grid (L EX N EU EG RID) by lexicalizing the entity transitions using off-the-shelf word embeddings to achieve better general"
2021.eacl-main.308,D14-1162,0,0.0854638,"Missing"
2021.eacl-main.308,N18-1202,0,0.0132294,"mmarization, and machine translation). We then present the results of the coherence models re-trained on the next utterance ranking task. For each of the coherence models, we conducted experiments with publicly available codes from the respective authors. The three recent methods Train Test Sections # Doc. # Pairs 00-13 14-24 1,378 1,053 26,422 20,411 Table 1: Statistics of the WSJ news dataset used for the Global discrimination task. use word embeddings: L EX N EU EG RID, T RANS M ODEL and U NIFIED M ODEL use Word2vec (Mikolov et al., 2013), average GloVe (Pennington et al., 2014), and ELMo (Peters et al., 2018) embeddings respectively. We use the default settings and hyperparameters suggested by the authors. 3.1 Synthetic Tasks Traditionally coherence models have been evaluated mostly on synthetic tasks. For comparison with previous work, we use two representative synthetic tasks to compare the coherence models. 3.1.1 Global Discrimination. Introduced by Barzilay and Lapata (2008), in this task coherence models are asked to distinguish an original (coherent) document from its incoherent renderings generated by random permutations of its sentences. We follow the same experimental setting of the Wall"
2021.eacl-main.308,D08-1020,0,0.0849023,"oring and dialog systems. Researchers have proposed a number of formal theories of discourse coherence, which have in∗ *Equal contribution Code and data used for evaluation available at https://ntunlpsg.github.io/project/ coherence/coh-eval/ 1 spired the development of many coherence models – both traditional and neural ones. Inspired by the Centering Theory (Grosz et al., 1995), the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have"
2021.eacl-main.308,J18-3002,0,0.0251923,"esponse generation (Zhang et al., 2020; Hosseini-Asl et al., 2020), coherence modeling of machine-generated texts, par3528 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3528–3539 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ticularly at a document-level, is now more crucial than ever (L¨aubli et al., 2018; Sharma et al., 2019). Traditional task-specific evaluation methods (e.g., BLEU, ROUGE) may not be an accurate reflection of their real-world performance in terms of readability (Paulus et al., 2017; Reiter, 2018). However, it is unclear if existing coherence models are capable of this task, since their performance on downstream applications is rarely studied, even though that is one of the main motivations for their development. Our main goal in this work is to assess the performance of the existing coherence models not only on standard, challenging synthetic tasks like global and local discrimination, but more importantly on real downstream text generation problems. Specifically, we investigate the performance of coherence models in three different settings: • Traditional synthetic tasks involving di"
2021.eacl-main.308,P19-1004,0,0.02187,"elation with human judgements. They also suggest using metrics 5 See Appendix for details. Rankings are available at http://homepages.inf.ed.ac.uk/mlap/ coherence/ 3533 Train Dev R@1 Test # of conv. # of coh.-incoh. pairs/conv. # of total example pairs 50,535 20 10,10,700 500 99 49,500 269 99 26,631 500 99 49,500 1078 99 1,06,722 Ubuntu dataset # of conv. # of coh.-incoh. pairs/conv. # of total example pairs 49,387 20 9,87,740 Table 7: Statistics of the refined Advising and Ubuntu datasets for the utterance ranking task. that take dialog context into account. This is particularly important as Sankar et al. (2019) empirically show that current neural dialog systems rarely use conversational history. We therefore propose to evaluate the usefulness of coherence models in dialog systems. We evaluate the models on the Noetic End-toEnd Response Selection Challenge II (NOESIS II), a track in the Dialog System Technology Challenges 8 (DSTC 8) (Kim et al., 2019). In this problem, each example consists of a conversational context U = (u1 , . . . , u|U |) and a set of potential utterances (candidates) C = {c1 , . . . , c|C |} that may occur next in the dialog; the task is to select the correct next-utterance r ∈"
2021.eacl-main.308,P17-1099,0,0.0197477,"NS M ODEL U NIFIED M ODEL Abs. Agr. Ext. Agr. 0.71 0.68 0.71 0.55 0.68 0.52 0.70 0.57 0.38 0.35 Table 6: Abstractive Agreement and Extractive Agreement shows the AC1 agreements for the pairwise ranking of the generated abstractive summaries and extractive summaries between two annotators and the models, respectively. Setup. We use the CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) for this task. We collect the reference summaries from the CNN/DM testset as well as the summaries generated by the following four representative abstractive summarization systems: (a) Pointer-Generator (PG) (See et al., 2017), (b) B ERT S UM E XTA BS (BSEA) (Liu and Lapata, 2019), (c) UniLM (Dong et al., 2019), and (d) SENECA (Sharma et al., 2019). As discussed, we directly use the coherence models trained on the WSJ dataset for the global discrimination task. The coherence models predict the scores for each system-generated summary in the testset. The scores produced by the models are then used to rank the system-generated summaries of the same original article. We conducted a user study to validate the effectiveness of the rankings produced by the coherence models. We randomly sampled 10 sets of summaries from t"
2021.eacl-main.308,N19-1170,0,0.0252084,"be affected by the difference between the testing and training setup. To control for this, we re-train and test the coherence models on a task-specific setup for next utterance ranking. This task has the advantage of being non-synthetic while providing task specific training data, but also being similar to the synthetic task of insertion, helping us evaluate the generalizability of the coherence model performance. 3.3.1 Next Utterance Ranking The quality of a dialog depends on various conversational aspects such as engagement, coherence, coverage, conversational depth, and topical diversity (See et al., 2019). Liu et al. (2016) show that commonly used metrics such as BLEU and ROUGE show very weak or no correlation with human judgements. They also suggest using metrics 5 See Appendix for details. Rankings are available at http://homepages.inf.ed.ac.uk/mlap/ coherence/ 3533 Train Dev R@1 Test # of conv. # of coh.-incoh. pairs/conv. # of total example pairs 50,535 20 10,10,700 500 99 49,500 269 99 26,631 500 99 49,500 1078 99 1,06,722 Ubuntu dataset # of conv. # of coh.-incoh. pairs/conv. # of total example pairs 49,387 20 9,87,740 Table 7: Statistics of the refined Advising and Ubuntu datasets for t"
2021.eacl-main.308,D19-1323,0,0.0632242,"n neural NLP methods, with claims of reaching human parity in machine translation (Hassan et al., 2018), fluency in summarization (Liu et al., 2017; Celikyilmaz et al., 2018), or context-consistent response generation (Zhang et al., 2020; Hosseini-Asl et al., 2020), coherence modeling of machine-generated texts, par3528 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3528–3539 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ticularly at a document-level, is now more crucial than ever (L¨aubli et al., 2018; Sharma et al., 2019). Traditional task-specific evaluation methods (e.g., BLEU, ROUGE) may not be an accurate reflection of their real-world performance in terms of readability (Paulus et al., 2017; Reiter, 2018). However, it is unclear if existing coherence models are capable of this task, since their performance on downstream applications is rarely studied, even though that is one of the main motivations for their development. Our main goal in this work is to assess the performance of the existing coherence models not only on standard, challenging synthetic tasks like global and local discrimination, but more i"
2021.eacl-main.308,W15-2507,0,0.0576926,"Missing"
2021.eacl-main.308,W16-3407,0,0.0167596,"on two downstream tasks: machine translation (MT) and summarization coherence evaluation. Note that both the MT and summarization data are from the same domain (news) as the original WSJ training data. 3.2.1 Machine Translation Evaluation The outputs of neural machine translation (NMT) systems have been shown to be more fluent than their phrase-based predecessors (Castilho et al., 2017). However, recent studies have shown that there is a statistically strong preference for human translations in terms of both adequacy and fluency at a document level (L¨aubli et al., 2018; Popel et al., 2020). Smith et al. (2016) evaluated traditional (nonneural) coherence models to see if they can distinguish a reference from a system translated document, and reported very low accuracy. However, the situation has changed with the advancements of neural models; today’s coherence models are claimed to be much more accurate. Our goal therefore is to evaluate the coherence models on how well they can judge the coherence of MT outputs at the document level. To do this, we use the system translations released by the annual Workshop (now Conference) on Machine Translation (WMT) through the years 2017 and 2018. At a document"
2021.eacl-main.308,P06-2103,0,0.0823901,"tional and neural ones. Inspired by the Centering Theory (Grosz et al., 1995), the entity based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) formulate coherence in terms of syntactic roles (e.g., subject, object) of entities in nearby sentences. Another branch of models (Pitler and Nenkova, 2008; Lin et al., 2011; Feng et al., 2014) use coherence relations between adjacent sentences to model local coherence, inspired by the discourse structure theories of Mann and Thompson (1988) and Webber (2004). Other traditional methods include word cooccurrence based local models (Soricut and Marcu, 2006), topic based global models (Barzilay and Lee, 2004; Elsner et al., 2007), and syntax based local and global models (Louis and Nenkova, 2012). Despite continuous research efforts in developing novel coherence models, their usefulness in downstream applications has largely been ignored. They have been evaluated in mainly two ways. The most common approach has been to evaluate them on synthetic discrimination tasks that involve identifying the right order of the sentences at the local and global levels (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Moon et al., 2019). The other (rather"
2021.eacl-main.308,P19-1067,0,0.057301,"nts in deep learning have inspired researchers to neuralize many of the traditional models. Li and Hovy (2014) model syntax and intersentence relations using a recurrent sentence encoder followed by a fully-connected layer. In a follow-up work, Li and Jurafsky (2017) use generative models to incorporate global topic information with an encoder-decoder architecture. Mohiuddin et al. (2018) propose a neural entity grid model using convolutions over distributed representations of entity transitions. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Xu et al. (2019) propose a local discriminative model that retains the advantages of generative models and uses a smaller negative sampling space that can learn against incorrect orderings. Moon et al. (2019) propose a unified model that incorporates sentence syntax, inter-sentence coherence relations, and global topic structures in a single Siamese framework. We benchmark the performance of five representative coherence models on the tasks discussed above. Our selected models comprise of both traditional and neural models. Moreover, two models are currently the state-of-the-art at the time of submission (Tra"
2021.eacl-main.308,2020.acl-demos.30,0,0.0344009,"Moon et al., 2019). The other (rather infrequent) way has been to assess the impact of coherence score as an additional feature in downstream tasks like readability assessment and essay scoring (Barzilay and Lapata, 2008; Mesgar and Strube, 2018). But since the concept of coherence goes beyond these constrained tasks and domains, so should the models. Given the recent advances in neural NLP methods, with claims of reaching human parity in machine translation (Hassan et al., 2018), fluency in summarization (Liu et al., 2017; Celikyilmaz et al., 2018), or context-consistent response generation (Zhang et al., 2020; Hosseini-Asl et al., 2020), coherence modeling of machine-generated texts, par3528 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3528–3539 April 19 - 23, 2021. ©2021 Association for Computational Linguistics ticularly at a document-level, is now more crucial than ever (L¨aubli et al., 2018; Sharma et al., 2019). Traditional task-specific evaluation methods (e.g., BLEU, ROUGE) may not be an accurate reflection of their real-world performance in terms of readability (Paulus et al., 2017; Reiter, 2018). However, it is unclear"
2021.emnlp-main.668,Q19-1038,0,0.0740687,"We retheir zero-shot transfer performance on the tasks port significant gains compared to directly fineas Dufter and Schütze (2020) show that injecttuning multilingual pre-trained models and ing cross-lingual signals by replacing masked toother semi-supervised alternatives.1 kens with semantically similar words from other 1 Introduction languages improves mBERT’s multilinguality and Self-supervised pre-trained models (Peters et al., zero-shot cross-lingual inference (XNLI) results. Concurrently, the multilingual embedding mod2018; Devlin et al., 2019; Liu et al., 2019; Lan els such as LASER (Artetxe and Schwenk, 2019) et al., 2019) have revolutionized natural language and LaBSE (Feng et al., 2020) use parallel data processing (NLP). Such pre-training with language to learn language invariant sentence representation modeling objectives provides a useful initial point by encoding texts from different languages into a for model parameters that adapt well to new tasks with supervised fine-tuning. Building on the suc- shared embedding space. These models can capcess of monolingual pre-trained language mod- ture semantic similarity well as often shown by their high Tatoeba scores and their success in tasks els ("
2021.emnlp-main.668,2021.acl-long.154,1,0.747859,"n 104 languages using masked language modeling (MLM) and next sentence prediction (NSP) tasks with no explicit cross-lingual objective. XLM-R (Conneau et al., 2020) improves over mBERT by training longer with more data from CommonCrawl, and without the NSP objective. Meanwhile, several studies examine what makes these pre-trained language models multilingual, In another line of work, researchers use data augand why it works well for cross-lingual transfer. mentation to solve language adaptation problem in Pires et al. (2019) hypothesize that the cross-lingual cross-lingual tasks. For example, Bari et al. (2021) 8493 use XLM-R’s mask language model to augment the data with vicinal samples. Liu et al. (2021) proposed a labeled sequence translation method to translate source-language NER training data to target languages and train a generation-based multilingual data augmentation method. These methods are orthogonal to our methods. Summary Current work on multilingual pretraining either does not consider sentence-level cross-lingual alignment in the pre-training (making them sacrifice transfer capability to structurally dissimilar language), or they only consider alignment signals, which makes them exp"
2021.emnlp-main.668,N19-1423,0,0.1149,"uld also potentially harm tion tasks across various languages. We retheir zero-shot transfer performance on the tasks port significant gains compared to directly fineas Dufter and Schütze (2020) show that injecttuning multilingual pre-trained models and ing cross-lingual signals by replacing masked toother semi-supervised alternatives.1 kens with semantically similar words from other 1 Introduction languages improves mBERT’s multilinguality and Self-supervised pre-trained models (Peters et al., zero-shot cross-lingual inference (XNLI) results. Concurrently, the multilingual embedding mod2018; Devlin et al., 2019; Liu et al., 2019; Lan els such as LASER (Artetxe and Schwenk, 2019) et al., 2019) have revolutionized natural language and LaBSE (Feng et al., 2020) use parallel data processing (NLP). Such pre-training with language to learn language invariant sentence representation modeling objectives provides a useful initial point by encoding texts from different languages into a for model parameters that adapt well to new tasks with supervised fine-tuning. Building on the suc- shared embedding space. These models can capcess of monolingual pre-trained language mod- ture semantic similarity well as ofte"
2021.emnlp-main.668,D19-1658,0,0.0417168,"Missing"
2021.emnlp-main.668,2020.emnlp-main.358,0,0.0186534,"bjective without considering parallel inbetter capture the semantic relationship in the parallel data, when a few translation pairs are formation or semantic equivalences, they cannot available. To show our method’s effectivecapture well semantic similarity across languages ness, we conduct extensive experiments on as reflected by their low Tatoeba score (Phang cross-lingual inference and review classificaet al., 2020). This could also potentially harm tion tasks across various languages. We retheir zero-shot transfer performance on the tasks port significant gains compared to directly fineas Dufter and Schütze (2020) show that injecttuning multilingual pre-trained models and ing cross-lingual signals by replacing masked toother semi-supervised alternatives.1 kens with semantically similar words from other 1 Introduction languages improves mBERT’s multilinguality and Self-supervised pre-trained models (Peters et al., zero-shot cross-lingual inference (XNLI) results. Concurrently, the multilingual embedding mod2018; Devlin et al., 2019; Liu et al., 2019; Lan els such as LASER (Artetxe and Schwenk, 2019) et al., 2019) have revolutionized natural language and LaBSE (Feng et al., 2020) use parallel data proces"
2021.emnlp-main.668,2021.naacl-main.280,0,0.091163,"Missing"
2021.emnlp-main.668,W19-4330,0,0.0168895,"the structural similarity (e.g., word order, word frequency, etc.) is more important for effective cross-lingual transfer. Another line of work on multilingual pretraining focuses on generating multilingual sentence embeddings such that semantically similar sentences across different languages will be closer in a shared vector space. LASER (Artetxe and Schwenk, 2019) uses an encoder-decoder architecture (Sutskever et al., 2014). It trains on large parallel data to learn multilingual fixed-length sentence embedding for 93 languages on a translation task. Multilingual Universal Encoder (mUSE) (Chidambaram et al., 2019) uses a dual-encoder architecture that is trained on one billion crawled question-answering pair with a translation ranking task: given a sentence from the source language and a group of candidate text from target languages, the model needs to recognize the corresponding translation of the source-language text from the candidates. LaBSE (Feng et al., 2020) is based on the BERT architecture using the same translation ranking task with mUSE but is trained on a much larger dataset of six billion translation pairs. Some researchers also tried to introduce crosslingual alignment from parallel data"
2021.emnlp-main.668,2020.acl-main.747,0,0.180773,"xts from different languages into a for model parameters that adapt well to new tasks with supervised fine-tuning. Building on the suc- shared embedding space. These models can capcess of monolingual pre-trained language mod- ture semantic similarity well as often shown by their high Tatoeba scores and their success in tasks els (LM) such as BERT (Devlin et al., 2019) and that involve cross-lingual similarity such as crossRoBERTa (Liu et al., 2019), multilingual models lingual retrieval and bitext mining. However, it has like mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020) been shown that these models generally lag behind the multilingual LMs on zero-shot cross-lingual have pushed the state-of-the-art on cross-lingual classification tasks like XNLI (Wang, 2019). We tasks by pre-training large Transformer (Vaswani hypothesize source information might be necessary et al., 2017) models jointly on many languages. The multilingual pre-trained LMs support zero- to achieve better zero-shot transfer as shown emshot transfer from a source language to target lan- pirically by Phang et al. (2020) with intermediate task fine-tuning in the source language (English). 1 Code"
2021.emnlp-main.668,D18-1269,0,0.0597635,"Missing"
2021.emnlp-main.668,2020.emnlp-main.369,0,0.0380369,"Missing"
2021.emnlp-main.668,2021.acl-long.453,1,0.743472,"no explicit cross-lingual objective. XLM-R (Conneau et al., 2020) improves over mBERT by training longer with more data from CommonCrawl, and without the NSP objective. Meanwhile, several studies examine what makes these pre-trained language models multilingual, In another line of work, researchers use data augand why it works well for cross-lingual transfer. mentation to solve language adaptation problem in Pires et al. (2019) hypothesize that the cross-lingual cross-lingual tasks. For example, Bari et al. (2021) 8493 use XLM-R’s mask language model to augment the data with vicinal samples. Liu et al. (2021) proposed a labeled sequence translation method to translate source-language NER training data to target languages and train a generation-based multilingual data augmentation method. These methods are orthogonal to our methods. Summary Current work on multilingual pretraining either does not consider sentence-level cross-lingual alignment in the pre-training (making them sacrifice transfer capability to structurally dissimilar language), or they only consider alignment signals, which makes them expensive to train. In contrast, in our work we focus on utilizing the parallel information in the f"
2021.emnlp-main.668,2021.ccl-1.108,0,0.0214982,"Missing"
2021.emnlp-main.668,N18-1202,0,0.0259406,"Missing"
2021.emnlp-main.668,N18-1101,0,0.0269233,"Missing"
2021.emnlp-main.668,D19-1077,0,0.0277362,"Missing"
2021.emnlp-main.668,2020.aacl-main.56,0,0.0266319,"s like mBERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020) been shown that these models generally lag behind the multilingual LMs on zero-shot cross-lingual have pushed the state-of-the-art on cross-lingual classification tasks like XNLI (Wang, 2019). We tasks by pre-training large Transformer (Vaswani hypothesize source information might be necessary et al., 2017) models jointly on many languages. The multilingual pre-trained LMs support zero- to achieve better zero-shot transfer as shown emshot transfer from a source language to target lan- pirically by Phang et al. (2020) with intermediate task fine-tuning in the source language (English). 1 Code and models are available at https://github.com/ tao-shiwu/co-training-xlu In this work, we argue that the multilingual em8492 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8492–8501 c November 7–11, 2021. 2021 Association for Computational Linguistics bedding models and multilingual LMs are complementary to each other – the task adaptability of the multilingual LMs can be complemented by the semantic awareness of the sentence embedding models and vice versa. For this, we"
2021.emnlp-main.668,P19-1493,0,0.0142469,"ive in learning cross-lingual information. mBERT was pre-trained on raw Wikipedia texts in 104 languages using masked language modeling (MLM) and next sentence prediction (NSP) tasks with no explicit cross-lingual objective. XLM-R (Conneau et al., 2020) improves over mBERT by training longer with more data from CommonCrawl, and without the NSP objective. Meanwhile, several studies examine what makes these pre-trained language models multilingual, In another line of work, researchers use data augand why it works well for cross-lingual transfer. mentation to solve language adaptation problem in Pires et al. (2019) hypothesize that the cross-lingual cross-lingual tasks. For example, Bari et al. (2021) 8493 use XLM-R’s mask language model to augment the data with vicinal samples. Liu et al. (2021) proposed a labeled sequence translation method to translate source-language NER training data to target languages and train a generation-based multilingual data augmentation method. These methods are orthogonal to our methods. Summary Current work on multilingual pretraining either does not consider sentence-level cross-lingual alignment in the pre-training (making them sacrifice transfer capability to structur"
2021.emnlp-main.685,2021.naacl-main.211,0,0.0626813,"We first alternately train span prediction, identifier prediction, and identifier tagging on both unimodal and bimodal data, and then leverage the bimodal data for dual generation training. encoder-decoder models based on T5 for programming language pre-training and support a more comprehensive set of tasks. Some emerging work (Clement et al., 2020; Mastropaolo et al., 2021; Elnaggar et al., 2021) in the recent literature also explore the T5 framework on code, but they only focus on a limited subset of generation tasks and do not support understanding tasks like us. Apart from these, PLBART (Ahmad et al., 2021) based on another encoder-decoder model BART can also support both understanding and generation tasks. However, all the above prior work simply processes code in the same way as natural language and largely ignores the code-specific characteristics. Instead, we propose to leverage the identifier information in code for pre-training. Recently, GraphCodeBERT (Guo et al., 2021) incorporates the data flow extracted from the code structure into CodeBERT, while Rozière et al. (2021) propose a deobfuscation objective to leverage the structural aspect of PL. These models only focus on training a bette"
2021.emnlp-main.685,2020.emnlp-main.728,0,0.602555,"et al., 2020) are the two pioneer models. CuBERT token type information (identifiers) from code. employs BERT’s powerful masked language modBesides, we propose to leverage the NL-PL eling objective to derive generic code-specific reppairs that are naturally available in source resentation, and CodeBERT further adds a replaced code to learn a better cross-modal alignment. token detection (Clark et al., 2020) task to learn • Extensive experiments show that CodeT5 NL-PL cross-modal representation. Besides the yields state-of-the-art results on the fourteen BERT-style models, Svyatkovskiy et al. (2020) and sub-tasks in CodeXGLUE. Further analysis Liu et al. (2020) respectively employ GPT and shows our CodeT5 can better capture the code UniLM (Dong et al., 2019) for the code completion semantics with the proposed identifier-aware task. Transcoder (Rozière et al., 2020) explores pre-training and bimodal dual generation pri- programming language translation in an unsupermarily benefits NL↔PL tasks. vised setting. Different from them, we explore 8697 Masked Input Masked Input # recursive MASK0 def binarySearch(arr, left, right, x): mid = (left + MASK1 if arr MASK2 == x: return mid # recursive b"
2021.emnlp-main.685,N19-1423,0,0.0316002,"to fine-tune CodeT5 on multiple tasks at a time using a task control code as the source prompt. In summary, we make the following contributions: • We present one of the first unified encoderdecoder models CodeT5 to support both coderelated understanding and generation tasks, and also allows for multi-task learning. 2 Related Work Pre-training on Natural Language. Pretrained models based on Transformer architectures (Vaswani et al., 2017) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and ELECTRA (Clark et al., 2020), decoder-only models like GPT (Radford et al., 2019), and encoder-decoder models such as MASS (Song et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming lang"
2021.emnlp-main.685,2020.findings-emnlp.139,0,0.0720251,"nyang Technological University, Singapore {wang.y,weishi.wang,sjoty,shoi}@salesforce.com Abstract which can be transferred to benefit multiple downstream tasks, especially those with limited data anPre-trained models for Natural Languages notation. Inspired by their success, there are many (NL) like BERT and GPT have been recently recent attempts to adapt these pre-training methshown to transfer well to Programming Languages (PL) and largely benefit a broad set of ods for programming language (PL) (Svyatkovskiy code-related tasks. Despite their success, most et al., 2020; Kanade et al., 2020; Feng et al., 2020), current methods either rely on an encoder-only showing promising results on code-related tasks. (or decoder-only) pre-training that is suboptiHowever, despite their success, most of these mal for generation (resp. understanding) tasks models rely on either an encoder-only model simior process the code snippet in the same way lar to BERT (Svyatkovskiy et al., 2020; Feng et al., as NL, neglecting the special characteristics of 2020) or a decoder-only model like GPT (Kanade PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transet al., 2020), which is suboptimal f"
2021.emnlp-main.685,D18-1192,0,0.016877,"SOTA) tions. The dataset consists of six PLs including pre-trained models that can be categorized into Ruby, JavaScript, Go, Python, Java, and PHP from three types: encoder-only, decoder-only, and CodeSearchNet (Husain et al., 2019). We employ encoder-decoder models. As encoder-only modthe smoothed BLEU-4 (Lin and Och, 2004) to eval- els, we consider RoBERTa (Liu et al., 2019b), uate this task. Code generation is the task to gen- RoBERTa (code) trained with masked language erate a code snippet based on NL descriptions. We modeling (MLM) on code, CodeBERT (Feng et al., employ the Concode data (Iyer et al., 2018) in Java 2020) trained with both MLM and replaced towhere the input contains both NL texts and class ken detection (Clark et al., 2020), GraphCodeenvironment contexts, and the output is a function. BERT (Guo et al., 2021) using data flow from code, We evaluate it with BLEU-4, exact match (EM) and DOBF (Rozière et al., 2021) trained with the accuracy, and CodeBLEU (Ren et al., 2020) that identifier deobfuscation objective. Note that alconsiders syntactic and semantic matches based on though DOBF employs a Seq2Seq model during the code structure in addition to the n-gram match. pre-training, it"
2021.emnlp-main.685,2020.acl-main.703,0,0.392108,"CodeT5 to support both coderelated understanding and generation tasks, and also allows for multi-task learning. 2 Related Work Pre-training on Natural Language. Pretrained models based on Transformer architectures (Vaswani et al., 2017) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and ELECTRA (Clark et al., 2020), decoder-only models like GPT (Radford et al., 2019), and encoder-decoder models such as MASS (Song et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming language and propose a novel identifier-aware denoising objective that enables the model to better comprehend the code. Pre-training on Programming Language. Pretraining on the programming language is a nasce"
2021.emnlp-main.685,C04-1072,0,0.0119293,"the provided public datasets and the same data splits following it for all these tasks. We first consider two cross-modal generation 4.4 Comparison Models tasks. Code summarization aims to summarize a function-level code snippet into English descrip- We compare CodeT5 with state-of-the-art (SOTA) tions. The dataset consists of six PLs including pre-trained models that can be categorized into Ruby, JavaScript, Go, Python, Java, and PHP from three types: encoder-only, decoder-only, and CodeSearchNet (Husain et al., 2019). We employ encoder-decoder models. As encoder-only modthe smoothed BLEU-4 (Lin and Och, 2004) to eval- els, we consider RoBERTa (Liu et al., 2019b), uate this task. Code generation is the task to gen- RoBERTa (code) trained with masked language erate a code snippet based on NL descriptions. We modeling (MLM) on code, CodeBERT (Feng et al., employ the Concode data (Iyer et al., 2018) in Java 2020) trained with both MLM and replaced towhere the input contains both NL texts and class ken detection (Clark et al., 2020), GraphCodeenvironment contexts, and the output is a function. BERT (Guo et al., 2021) using data flow from code, We evaluate it with BLEU-4, exact match (EM) and DOBF (Rozi"
2021.emnlp-main.685,P19-1441,0,0.382359,"e tasks at a time using a task control code as the source prompt. In summary, we make the following contributions: • We present one of the first unified encoderdecoder models CodeT5 to support both coderelated understanding and generation tasks, and also allows for multi-task learning. 2 Related Work Pre-training on Natural Language. Pretrained models based on Transformer architectures (Vaswani et al., 2017) have led to state-of-the-art performance on a broad set of NLP tasks. They can be generally categorized into three groups: encoder-only models such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b), and ELECTRA (Clark et al., 2020), decoder-only models like GPT (Radford et al., 2019), and encoder-decoder models such as MASS (Song et al., 2019), BART (Lewis et al., 2020), and T5 (Raffel et al., 2020). Compared to encoder-only and decoder-only models that respectively favor understanding and generation tasks, encoder-decoder models can well support both types of tasks. They often employ denoising sequence-to-sequence pre-training objectives that corrupt the source input and require the decoder to recover them. In this work, we extend T5 to the programming language and propose a novel id"
2021.emnlp-main.685,2021.ccl-1.108,0,0.0480293,"Missing"
2021.emnlp-main.685,P16-1162,0,0.0881846,"Missing"
2021.emnlp-main.737,L18-1500,0,0.0425258,"Missing"
2021.findings-acl.267,P18-1073,0,0.0184704,"ect/augvic/. 2 Related Work Two lines of studies are relevant to our work. Low-resource NMT Although the main focus of investigation and improvement in NMT has been in high-resource settings, there has been a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT sy"
2021.findings-acl.267,2021.acl-long.154,1,0.383878,"llowing, we describe how each of these steps are operationalized with NMT models. 3.2.1 Generation of Vicinal Samples We first generate vicinal samples for each eligible target sentence yi in the bitext D = {(xi , yi )}N i=1 . Let V(˜ yi |yi ) denote the vicinity distribution around yi , we create a corpus of vicinal samples as: y˜i ∼ V (˜ yi |yi ) (1) We generate vicinal samples for sentences having lengths between 3 and 100, and V can be modeled with existing syntactic and semantic alternation methods like language model (LM) augmentation (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2020; Bari et al., 2021), paraphrase generation (Li et al., 2018), constrained summarization (Laban et al., 2020), and similar sentence retrieval (Du et al., 2020). Most of these methods are supervised requiring extra annotations. Instead, in AUG V IC, we adopt an unsupervised LM augmentation, which makes the framework more robust and flexible to use. Specifically, we use a pretrained XLM-R masked LM (Conneau et al., 2020a) parameterized by θxlmr as our vicinal model. Thus, the vicinity distribution is defined as V (˜ yi |yi , θxlmr ). Note that we treat the vicinal model as an external entity, which is not trained/f"
2021.findings-acl.267,W19-5206,0,0.0138861,"irat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is back-translation (BT) (Sennrich et al., 2016; Hoang et al., 2018), which exploits targetside monolingual data. Edunov et al. (2018) investigated BT extensively and scaled the method to millions of target-side monolingual sentences. Caswell et al. (2019) explored the role of noise in noisedBT and proposed to use a tag for back-translated source sentences. Besides BT, self-training is another data augmentation strategy for NMT which leverages source-side monolingual data (He et al., 3035 Figure 1: Illustration of AUG V IC steps for Bengali-to-English translation system. Here (xi , yi ) is the original bitext pair, y˜i is a vicinal sample of yi , and (˜ xi , y˜i ) is a synthetic pair where x ˜i is generated by a reverse intermediate translation system Mt→s . Right side of the figure shows the successive steps of vicinal sample generation. 2020)"
2021.findings-acl.267,D19-5213,0,0.0800321,"d low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target-side monolingual data into the source language. However, when there are scarcity of in-domain data which indeed a common situation in many low-resource settings, the success of BT may be limited (Chen et al., 2019). Another understudied problem with BT is the issue with domain mismatch (Edunov et al., 2020). To elaborate, let us consider two scenarios: (i) the training and testing data come from the same or relevant domains (e.g., News), and (ii) the test domain (News) is different from the training domain (e.g., 1 Equal contribution See (Dabre et al., 2020) for a survey of the later. 3034 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3034–3045 August 1–6, 2021. ©2021 Association for Computational Linguistics Subtitles). In the former case, we can foresee two problems"
2021.findings-acl.267,2020.acl-main.747,0,0.124812,"Missing"
2021.findings-acl.267,W96-0200,0,0.877145,"Missing"
2021.findings-acl.267,2020.acl-main.253,0,0.0194772,"a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target-side monolingual data into the source language. However, when there are scarcity of in-domain data which indeed a common situation in many low-resource settings, the success of BT may be limited (Chen et al., 2019). Another understudied problem with BT is the issue with domain mismatch (Edunov et al., 2020). To elaborate, let us consider two scenarios: (i) the training and testing data come from the same or relevant domains (e.g., News), and (ii) the test domain (News) is different from the training domain (e.g., 1 Equal contribution See (Dabre et al., 2020) for a survey of the later. 3034 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3034–3045 August 1–6, 2021. ©2021 Association for Computational Linguistics Subtitles). In the former case, we can foresee two problems. First, if we use out-of-domain monolingual data which is abundant, it might misguide the mod"
2021.findings-acl.267,P17-2090,0,0.021619,"˜i is generated by a reverse intermediate translation system Mt→s . Right side of the figure shows the successive steps of vicinal sample generation. 2020). Large scale multilingual pre-training followed by bitext fine-tuning is a recent trend to utilize monolingual data for NMT, which is shown to be beneficial (Arivazhagan et al., 2019; Liu et al., 2020; Zhu et al., 2020; Lepikhin et al., 2021). Apart from using extra monolingual data, Xie et al. (2017) show that data noising is an effective regularization method for NMT, while Wu et al. (2019) use noised training. In low-resource settings, Fadaee et al. (2017) augment bitext by replacing a common word with a low-frequency word in the target sentence, and change its corresponding word in the source sentence to improve the translation quality of rare words. Wang et al. (2018) propose an unsupervised data augmentation method for NMT by replacing words in both source and target sentences based on hamming distance. Gao et al. (2019) propose a method that replaces words with a weighted combination of semantically similar words. Recently, Nguyen et al. (2020) propose an in-domain augmentation method by diversifying the available bitext data using multiple"
2021.findings-acl.267,N16-1101,0,0.195302,"ow-resourced despite being used by large portion of world population. Hence, improving low-resource MT quality has been of great interests to the MT researchers. There have been several attempts to extend the success of NMT in high-resource settings to lowresource language pairs that have a relatively small amount of available parallel data. Most of these methods mainly focus on leveraging extra monolingual data through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer through parallel data involving other assisting language pairs (Firat et al., 2016a,b; Johnson et al., 2017; Neubig and Hu, 2018).1 Large scale pre-training is another recent trend to utilize large monolingual data for NMT (Liu et al., 2020). However, very few work has considered low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target"
2021.findings-acl.267,D16-1026,0,0.0602107,"Missing"
2021.findings-acl.267,P19-1555,0,0.0126326,"epikhin et al., 2021). Apart from using extra monolingual data, Xie et al. (2017) show that data noising is an effective regularization method for NMT, while Wu et al. (2019) use noised training. In low-resource settings, Fadaee et al. (2017) augment bitext by replacing a common word with a low-frequency word in the target sentence, and change its corresponding word in the source sentence to improve the translation quality of rare words. Wang et al. (2018) propose an unsupervised data augmentation method for NMT by replacing words in both source and target sentences based on hamming distance. Gao et al. (2019) propose a method that replaces words with a weighted combination of semantically similar words. Recently, Nguyen et al. (2020) propose an in-domain augmentation method by diversifying the available bitext data using multiple forward and backward models. In their follow-up work (Nguyen et al., 2021), they extend the idea to unsupervised MT (UMT) using a cross-model distillation method, where one UMT model’s synthetic output is used as input for another UMT model. Summary Most of the previous work on improving BT involve either training iteratively or combining BT with self-training using monol"
2021.findings-acl.267,N18-1032,0,0.0202858,"018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is back-translation (BT) (Sennrich et al., 2016; Hoang et al., 2018), which exploits targetside monolingual data. Edunov et al. (2018) investigated BT extensively and scaled the method to millions of target-side monolingual sentences. Caswell et al. (2019) explored the role of noise in noisedBT"
2021.findings-acl.267,D19-1632,0,0.0263448,"Missing"
2021.findings-acl.267,D18-1045,0,0.0849285,"through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer through parallel data involving other assisting language pairs (Firat et al., 2016a,b; Johnson et al., 2017; Neubig and Hu, 2018).1 Large scale pre-training is another recent trend to utilize large monolingual data for NMT (Liu et al., 2020). However, very few work has considered low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target-side monolingual data into the source language. However, when there are scarcity of in-domain data which indeed a common situation in many low-resource settings, the success of BT may be limited (Chen et al., 2019). Another understudied problem with BT is the issue with domain mismatch (Edunov et al., 2020). To elaborate, let us consider two scenarios: (i) the training and testing data come from the same or relev"
2021.findings-acl.267,W18-2703,0,0.0204095,"ngual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is back-translation (BT) (Sennrich et al., 2016; Hoang et al., 2018), which exploits targetside monolingual data. Edunov et al. (2018) investigated BT extensively and scaled the method to millions of target-side monolingual sentences. Caswell et al. (2019) explored the role of noise in noisedBT and proposed to use a tag for back-translated source sentences. Besides BT, self-training is another data augmentation strategy for NMT which leverages source-side monolingual data (He et al., 3035 Figure 1: Illustration of AUG V IC steps for Bengali-to-English translation system. Here (xi , yi ) is the original bitext pair, y˜i is a vicinal sample of yi , and (˜ xi , y"
2021.findings-acl.267,D19-1080,0,0.0161274,"een a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is ba"
2021.findings-acl.267,N18-2072,0,0.0514934,"en relevant monolingual data is available. In the following, we describe how each of these steps are operationalized with NMT models. 3.2.1 Generation of Vicinal Samples We first generate vicinal samples for each eligible target sentence yi in the bitext D = {(xi , yi )}N i=1 . Let V(˜ yi |yi ) denote the vicinity distribution around yi , we create a corpus of vicinal samples as: y˜i ∼ V (˜ yi |yi ) (1) We generate vicinal samples for sentences having lengths between 3 and 100, and V can be modeled with existing syntactic and semantic alternation methods like language model (LM) augmentation (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2020; Bari et al., 2021), paraphrase generation (Li et al., 2018), constrained summarization (Laban et al., 2020), and similar sentence retrieval (Du et al., 2020). Most of these methods are supervised requiring extra annotations. Instead, in AUG V IC, we adopt an unsupervised LM augmentation, which makes the framework more robust and flexible to use. Specifically, we use a pretrained XLM-R masked LM (Conneau et al., 2020a) parameterized by θxlmr as our vicinal model. Thus, the vicinity distribution is defined as V (˜ yi |yi , θxlmr ). Note that we treat the vici"
2021.findings-acl.267,W18-6325,0,0.0180013,"et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is back-translation (BT) (Sennrich et al., 2016; Hoang et al., 2018), which exploits targetside monolingual data. Edunov et al. (2018) investigated BT extensively and scaled the method to millions of target-side monolingual sentences. Caswell et al. (2019) explored the role of"
2021.findings-acl.267,W17-3204,0,0.128139,"monolingual data in traditional back-translation. To understand the contributions of different components of AUG V IC, we perform an in-depth framework analysis. 1 Introduction Neural Machine Transaltion (NMT) has shown impressive performance in high-resource settings, even claiming to achieve parity with human professional translators (Hassan et al., 2018; Popel et al., 2020). Most successful NMT systems have billions of parameters (Lepikhin et al., 2021). They generally work well only when a good amount of parallel training data is available and perform poorly ∗ in low-resource conditions (Koehn and Knowles, 2017; Guzm´an et al., 2019). However, majority of the languages are low-resourced despite being used by large portion of world population. Hence, improving low-resource MT quality has been of great interests to the MT researchers. There have been several attempts to extend the success of NMT in high-resource settings to lowresource language pairs that have a relatively small amount of available parallel data. Most of these methods mainly focus on leveraging extra monolingual data through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer"
2021.findings-acl.267,2020.acl-main.460,0,0.0275804,"Generation of Vicinal Samples We first generate vicinal samples for each eligible target sentence yi in the bitext D = {(xi , yi )}N i=1 . Let V(˜ yi |yi ) denote the vicinity distribution around yi , we create a corpus of vicinal samples as: y˜i ∼ V (˜ yi |yi ) (1) We generate vicinal samples for sentences having lengths between 3 and 100, and V can be modeled with existing syntactic and semantic alternation methods like language model (LM) augmentation (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2020; Bari et al., 2021), paraphrase generation (Li et al., 2018), constrained summarization (Laban et al., 2020), and similar sentence retrieval (Du et al., 2020). Most of these methods are supervised requiring extra annotations. Instead, in AUG V IC, we adopt an unsupervised LM augmentation, which makes the framework more robust and flexible to use. Specifically, we use a pretrained XLM-R masked LM (Conneau et al., 2020a) parameterized by θxlmr as our vicinal model. Thus, the vicinity distribution is defined as V (˜ yi |yi , θxlmr ). Note that we treat the vicinal model as an external entity, which is not trained/fine-tuned. This disjoint characteristic gives our framework the flexibility to replace θx"
2021.findings-acl.267,D18-1421,0,0.0262749,"s are operationalized with NMT models. 3.2.1 Generation of Vicinal Samples We first generate vicinal samples for each eligible target sentence yi in the bitext D = {(xi , yi )}N i=1 . Let V(˜ yi |yi ) denote the vicinity distribution around yi , we create a corpus of vicinal samples as: y˜i ∼ V (˜ yi |yi ) (1) We generate vicinal samples for sentences having lengths between 3 and 100, and V can be modeled with existing syntactic and semantic alternation methods like language model (LM) augmentation (Kobayashi, 2018; Wu et al., 2018; Shi et al., 2020; Bari et al., 2021), paraphrase generation (Li et al., 2018), constrained summarization (Laban et al., 2020), and similar sentence retrieval (Du et al., 2020). Most of these methods are supervised requiring extra annotations. Instead, in AUG V IC, we adopt an unsupervised LM augmentation, which makes the framework more robust and flexible to use. Specifically, we use a pretrained XLM-R masked LM (Conneau et al., 2020a) parameterized by θxlmr as our vicinal model. Thus, the vicinity distribution is defined as V (˜ yi |yi , θxlmr ). Note that we treat the vicinal model as an external entity, which is not trained/fine-tuned. This disjoint characteristic g"
2021.findings-acl.267,2020.tacl-1.47,0,0.13987,". There have been several attempts to extend the success of NMT in high-resource settings to lowresource language pairs that have a relatively small amount of available parallel data. Most of these methods mainly focus on leveraging extra monolingual data through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer through parallel data involving other assisting language pairs (Firat et al., 2016a,b; Johnson et al., 2017; Neubig and Hu, 2018).1 Large scale pre-training is another recent trend to utilize large monolingual data for NMT (Liu et al., 2020). However, very few work has considered low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target-side monolingual data into the source language. However, when there are scarcity of in-domain data which indeed a common situation in many low-resource settin"
2021.findings-acl.267,2020.emnlp-main.215,1,0.732565,"elevant to our work. Low-resource NMT Although the main focus of investigation and improvement in NMT has been in high-resource settings, there has been a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning o"
2021.findings-acl.267,N19-1386,1,0.842574,"Work Two lines of studies are relevant to our work. Low-resource NMT Although the main focus of investigation and improvement in NMT has been in high-resource settings, there has been a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource la"
2021.findings-acl.267,2020.cl-2.2,1,0.762677,"Missing"
2021.findings-acl.267,D18-1103,0,0.102731,"ion of world population. Hence, improving low-resource MT quality has been of great interests to the MT researchers. There have been several attempts to extend the success of NMT in high-resource settings to lowresource language pairs that have a relatively small amount of available parallel data. Most of these methods mainly focus on leveraging extra monolingual data through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer through parallel data involving other assisting language pairs (Firat et al., 2016a,b; Johnson et al., 2017; Neubig and Hu, 2018).1 Large scale pre-training is another recent trend to utilize large monolingual data for NMT (Liu et al., 2020). However, very few work has considered low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse intermediate model is trained on the original parallel data, which is later used to generate synthetic parallel data by translating sentences from target-side monolingual data into the source language"
2021.findings-acl.267,N19-4009,0,0.0202636,"h the bitext given with the dataset. (ii) Upsample baseline Here we upsample the bitext to the same amount of AUG V IC’s data. (iii) Diversification baseline Nguyen et al. (2020) diversifies the original parallel data by using the predictions of multiple forward and backward NMT models. Then they merge the augmented data with the original bitext on which the final NMT model is trained. Their method is directly comparable to AUG V IC, as both methods diversify the original bitext, but in different ways. 4.3 Model Settings We use the Transformer (Vaswani et al., 2017) implementation in Fairseq (Ott et al., 2019). We follow the basic architectural settings from (Guzm´an et al., 2019), which establishes some standards for low-resource MT. For low-resource “Bitext baseline”, they use a smaller (5-layer) Transformer architecture as the dataset is small, while for larger datasets (e.g., with additional synthetic data) they use a bigger (6-layer) model.2 To keep the architecture the same in the respective rows (Table 3), we use a 6-layer model for “Upsample baseline” and 5-layer for “Bitext baseline”. More specifically, for datasets with less than a million bitext pairs, we use an architecture with 5 encod"
2021.findings-acl.267,P02-1040,0,0.109744,"Missing"
2021.findings-acl.267,W18-6319,0,0.0442341,"Missing"
2021.findings-acl.267,P16-1009,0,0.591235,"ng data is available and perform poorly ∗ in low-resource conditions (Koehn and Knowles, 2017; Guzm´an et al., 2019). However, majority of the languages are low-resourced despite being used by large portion of world population. Hence, improving low-resource MT quality has been of great interests to the MT researchers. There have been several attempts to extend the success of NMT in high-resource settings to lowresource language pairs that have a relatively small amount of available parallel data. Most of these methods mainly focus on leveraging extra monolingual data through back-translation (Sennrich et al., 2016) and self-training (He et al., 2020), or translation knowledge transfer through parallel data involving other assisting language pairs (Firat et al., 2016a,b; Johnson et al., 2017; Neubig and Hu, 2018).1 Large scale pre-training is another recent trend to utilize large monolingual data for NMT (Liu et al., 2020). However, very few work has considered low-resource NMT without using auxiliary data or other pivot languages. In the presence of a sufficient amount of indomain monolingual data, back-translation (BT) has proved to be quite successful (Edunov et al., 2018). In this approach, a reverse"
2021.findings-acl.267,tiedemann-2012-parallel,0,0.0523295,"osed framework. We open-source our framework at https://ntunlpsg.github.io/project/augvic/. 2 Related Work Two lines of studies are relevant to our work. Low-resource NMT Although the main focus of investigation and improvement in NMT has been in high-resource settings, there has been a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm"
2021.findings-acl.267,D18-1100,0,0.113692,"t far away from the actual test distribution. Second, even if the monolingual data is from a domain similar to that of the training/testing data, there might be differences in topics, modality, style, etc., which might induce noise. For the latter scenario, even if the monolingual data comes from the similar domain as the test data (News), the corresponding (reverse) translations will be noisy as the intermediate model would be trained on a different domain (Subtitles). Consequently, these noisy pseudo-parallel data will induce noise during training and might cause the model to perform worse (Wang et al., 2018). On the other hand, using in-domain (Subtitles) monolingual data in back-translation will not give enough diversity to cover the test domain (News). In this work, inspired by the Vicinal Risk Minimization principle (Chapelle et al., 2001), we propose AUG V IC, a novel method to augment vicinal samples around the bitext distribution. Instead of using extra monolingual data, AUG V IC aims to leverage the vicinal samples of the original bitext, thereby enlarging the support of the training bitext distribution to improve model generalization. The main advantage is that the resulting distribution"
2021.findings-acl.267,D19-1430,0,0.0183064,"nal sample of yi , and (˜ xi , y˜i ) is a synthetic pair where x ˜i is generated by a reverse intermediate translation system Mt→s . Right side of the figure shows the successive steps of vicinal sample generation. 2020). Large scale multilingual pre-training followed by bitext fine-tuning is a recent trend to utilize monolingual data for NMT, which is shown to be beneficial (Arivazhagan et al., 2019; Liu et al., 2020; Zhu et al., 2020; Lepikhin et al., 2021). Apart from using extra monolingual data, Xie et al. (2017) show that data noising is an effective regularization method for NMT, while Wu et al. (2019) use noised training. In low-resource settings, Fadaee et al. (2017) augment bitext by replacing a common word with a low-frequency word in the target sentence, and change its corresponding word in the source sentence to improve the translation quality of rare words. Wang et al. (2018) propose an unsupervised data augmentation method for NMT by replacing words in both source and target sentences based on hamming distance. Gao et al. (2019) propose a method that replaces words with a weighted combination of semantically similar words. Recently, Nguyen et al. (2020) propose an in-domain augmenta"
2021.findings-acl.267,D16-1163,0,0.0230646,"al lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NMT Till now, one of the most successful data augmentation strategies in NMT is back-translation (BT) (Sennrich et al., 2016; Hoang et al., 2018), which exploits targetside monolingual data. Edunov et al. (2018) investigated BT extensively and scaled the method to millions of target-side monolingual sentences. Caswell et al. (2019) explored the role of noise in noisedBT and proposed to use a tag for back-translated source sentences."
2021.findings-acl.267,P19-1021,0,0.0172633,"of investigation and improvement in NMT has been in high-resource settings, there has been a recent surge of interest in low-resource MT. However, achieving satisfactory performance in low-resource settings turns out to be challenging for NMT systems (Koehn and Knowles, 2017). Recent research has mainly focused on creating and cleaning parallel (Ramasamy et al., 2014; Islam, 2018) and comparable data (Tiedemann, 2012), utilizing bilingual lexicon induction (Conneau et al., 2017; Artetxe et al., 2018; Mohiuddin and Joty, 2019, 2020; Mohiuddin et al., 2020), fine-grained hyperparameter tuning (Sennrich and Zhang, 2019), and using other language pairs as pivot (Cheng et al., 2017; Kim et al., 2019). Another avenue of research follows multilingual translation, where translation knowledge from highresource language pairs are exploited by training a single NMT system on a mix of high-resource and low-resource language pairs (Firat et al., 2016a,b; Kocmi and Bojar, 2018; Gu et al., 2018; Neubig and Hu, 2018; Guzm´an et al., 2019). Zoph et al. (2016) proposed a variant where they pretrain NMT system on a high-resource language pair before finetuning on a target low-resource language pair. Data Augmentation for NM"
2021.findings-emnlp.424,N19-3002,0,0.0216482,"2020) gives an automatic evaluation of toxicity using generations from different language models using a set of webtext prompts. (Gehman et al., 2020) also tests methods for mitigating toxicity, and finds that applying PPLM was more effective than simpler decoding-based detoxification methods such as swear word filters. Xu et al. (2020) develop a human in the loop method for adversarially probing toxic responses in conversational agents, and train a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016). Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientatio"
2021.findings-emnlp.424,P19-1102,0,0.0611326,"Missing"
2021.findings-emnlp.424,W18-2706,0,0.0240402,"lace of x1 . This works well when GeDi is initialized as a pretrained language model, as the model will have learned embeddings for many topics during its pretraining that can be used as zero-shot control codes. 4 Related Work Methods for controlling text generation can be categorized broadly into two categories: training or finetuning a model directly for controllable gen&lt;false&gt; &lt;science&gt; T-rex achieved its massive eration (Chan et al., 2021; Madotto et al., 2020; size due to an enormous growth spurt during its Keskar et al., 2019; Ziegler et al., 2019; Rajani adolescent years. et al., 2019; Fan et al., 2018; Ficler and Goldberg, 4932 2017; Yu et al., 2017; Hu et al., 2017) or using a discriminator to guide decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018; Dathathri et al., 2020). Keskar et al. (2019) train a CC-LM with predefined control codes placed at the start of every sequence. GeDi also uses CC-LMs, but instead of generating from them directly, GeDi uses them as discriminators to guide decoding from another language model. This is much more computationally efficient than previous methods for discriminator guided decoding. Holtzman et al. (2018) apply discriminators to re-weight a"
2021.findings-emnlp.424,W17-4912,0,0.0489744,"Missing"
2021.findings-emnlp.424,N18-2070,0,0.0250397,"ases, since PPLM updates the previously stored keys and values. GeDi in comparison only adds constant overhead that is independent of the size of the base LM, and this constant will be minimal if the GeDi is significantly smaller than the base LM. GeDi also relates to the rational speech acts framework for computational pragmatics (Frank and Goodman, 2012; Goodman and Stuhlmüller, 2013) where a “listener” model and a “speaker” model interactively generate a sequence such that the listener can recover the input. GeDi most closely relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from diffe"
2021.findings-emnlp.424,2020.findings-emnlp.301,0,0.451059,"y relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from different language models using a set of webtext prompts. (Gehman et al., 2020) also tests methods for mitigating toxicity, and finds that applying PPLM was more effective than simpler decoding-based detoxification methods such as swear word filters. Xu et al. (2020) develop a human in the loop method for adversarially probing toxic responses in conversational agents, and train a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia"
2021.findings-emnlp.424,P18-1152,0,0.146388,"downstream applications. Existing approaches to controlling LMs have limitations. Class-conditional LMs (CC-LMs) such as CTRL (Keskar et al., 2019) attempt to control text generation by conditioning on a control code, which is an attribute variable representing a data source. However, using a specific control code can reduce sample diversity across prompts, as samples will generally resemble the data source of the control code. Another approach for controlling LMs is to use discriminators to guide decoding, but existing methods to do this are very computationally intensive. Weighted decoding (Holtzman et al., 2018) requires feeding candidate next tokens into a discriminator, and thus scales linearly in computation with the number of tokens to be re-weighted. Plug and Play LM (Dathathri et al., 2020, PPLM) applies up to 10 updates to the generating LM’s latent states per time step using gradients from a discriminator, also making it many times slower than generating from the LM directly. We present GeDi1,2 as a significantly more efficient algorithm for discriminator guided decoding. Our proposed method uses class-conditional LMs as generative discriminators (GeDis) to steer language generation towards d"
2021.findings-emnlp.424,2020.acl-main.487,0,0.0197636,"roblem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B parameter), and GPT-3 (Brown et al., 2020) in our detoxification experiments. All experiments were performed using adaptations of Huggingface Transformers (Wolf et al., 2020). We include experiments with greedy decoding with a repetition penalty ("
2021.findings-emnlp.424,2020.findings-emnlp.219,0,0.0308097,"he sequence. This also makes it possible to form new control codes zero-shot; a new topic word that was never seen before in training can be chosen in place of x1 . This works well when GeDi is initialized as a pretrained language model, as the model will have learned embeddings for many topics during its pretraining that can be used as zero-shot control codes. 4 Related Work Methods for controlling text generation can be categorized broadly into two categories: training or finetuning a model directly for controllable gen&lt;false&gt; &lt;science&gt; T-rex achieved its massive eration (Chan et al., 2021; Madotto et al., 2020; size due to an enormous growth spurt during its Keskar et al., 2019; Ziegler et al., 2019; Rajani adolescent years. et al., 2019; Fan et al., 2018; Ficler and Goldberg, 4932 2017; Yu et al., 2017; Hu et al., 2017) or using a discriminator to guide decoding (Ghazvininejad et al., 2017; Holtzman et al., 2018; Dathathri et al., 2020). Keskar et al. (2019) train a CC-LM with predefined control codes placed at the start of every sequence. GeDi also uses CC-LMs, but instead of generating from them directly, GeDi uses them as discriminators to guide decoding from another language model. This is muc"
2021.findings-emnlp.424,P19-1487,1,0.87302,"Missing"
2021.findings-emnlp.424,P19-1163,0,0.0186535,". Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B parameter), and GPT-3 (Brown et al., 2020) in our detoxification experiments. All experiments were performed using adaptations of Huggingface Transformers (Wolf et al., 2020). We include experiments with greedy decoding w"
2021.findings-emnlp.424,N19-1410,0,0.0232002,"he previously stored keys and values. GeDi in comparison only adds constant overhead that is independent of the size of the base LM, and this constant will be minimal if the GeDi is significantly smaller than the base LM. GeDi also relates to the rational speech acts framework for computational pragmatics (Frank and Goodman, 2012; Goodman and Stuhlmüller, 2013) where a “listener” model and a “speaker” model interactively generate a sequence such that the listener can recover the input. GeDi most closely relates to distractor based pragmatics (Andreas and Klein, 2016; Cohn-Gordon et al., 2018; Shen et al., 2019), where a single model processes a true input and a distractor input, and uses Bayes rule to produce text that fits the true input but not the distractor input. GeDi differs from previous pragmatics based approaches in that it trains a separate class-conditional language model (which acts as the listener) on a single attribute, allowing that attribute to be isolated, and uses it to guide generation from a separate language model (which acts as the speaker). ation. RealToxictyPrompts (Gehman et al., 2020) gives an automatic evaluation of toxicity using generations from different language models"
2021.findings-emnlp.424,D13-1170,1,0.01707,"Missing"
2021.findings-emnlp.424,W16-5618,0,0.0288337,"in a model to give preset responses when encountering potentially unsafe probes. Other work has focused on removing gender bias from language models (Bordia and Bowman, 2019; Dinan et al., 2020; Bolukbasi et al., 2016). Related to the problem of addressing toxicity in generation is toxicity detection, which can be performed using the Perspective API or using a classifier trained on a labelled toxicity dataset such as the Jigsaw Toxic Comment Classification Dataset (Borkan et al., 2019). Toxicity detection is difficult as toxicity labelling is subjective and often has poor annotator agreement (Waseem, 2016; Ross et al., 2017). Additionally, existing toxicity classifiers are often biased in that they overestimate the toxicity of text that mentions sexual orientations or racial minorities (Dixon et al., 2018; Sap et al., 2019; Hutchinson et al., 2020). 5 Experiments We experiment with GeDi-guided decoding for sentiment, detoxification, and topic control. We finetune GPT2-medium (345M parameter) (Radford et al., 2019) using the loss in Equation (3) with control codes specific to each task to form a classconditional language model. We use these CC-LMs as GeDis to guide generation from GPT2-XL (1.5B"
2021.naacl-industry.11,D18-1316,0,0.019054,"Following (Cheng et al., 2018), we generate semantically related perturbations in the decoder to increase the diversity of the translations. Together with NAL, our model shows its ability to resist noises in the input and produce more robust translations. Results on ZH-EN and FR-EN translation significantly improve over the baseline by +1.24 (MT03) and +1.4 (N15) BLEU on news domain, and +1.63 (Social), +1.3 (mtnt18) on social media domain respectively. Further fine-tuning experiments on FR-EN social media text even witness an average improvement of +1.25 BLEU over the best approach. 2 tions (Alzantot et al., 2018), paraphrasing (Iyyer et al., 2018; Ribeiro et al., 2018), character-level noise (Ebrahimi et al., 2018b; Tan et al., 2020a,b), or perturbations at embedding space (Miyato et al., 2016; Liang et al., 2020). Inspired by Lei et al. (2017) that nicely captures the semantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robu"
2021.naacl-industry.11,P06-2005,0,0.117768,"ying games over the night. Play the game all night and take points thief fast. Play games all night to score points quickly. 我已剪短了我的发,剪断了惩罚,剪一地伤透我 的尴尬。。。。 I have cut my hair, i cut off the punishment, i away the awkwardness that hurt me. I got my punishment, got rid of my embarrassment. I cut short my hair , cut off punishment , and cut off my embarrassment that hurts me. Table 1: Examples of NMT’s vulnerability in translating text containing noisy words (“zei&quot; → “thief&quot;, “chengfa&quot; → “punishment&quot;). CER mitigates the effect of noisy words. Noisy words have long been discussed in previous work. Aw et al. (2006) proposed the normalization approach to reduce the noise before translation. Tan et al. (2020a,b) addressed the character-level noise directly in the NMT model. Though these approaches addressed the effect of noisy words to some extent, they are limited to spelling errors, inflectional variations, and other noises definable during training. In addition, strong external supervision like a parallel corpus of noisy text translation or dictionary containing the translation of those noisy words are hard and expensive to obtain; they are also not practical in handling real noises as noisy words can"
2021.naacl-industry.11,W19-5361,0,0.011984,"s in three languages. Vaibhav et al. (2019) leverage effective synthetic noise to make NMT resilient to noisy text. We implement their approach on Transformer backbone. For a fair comparison, we limit the data to train back-translation models only with mtnttrain. Zhou et al. (2019) adopt a multitask transformer architecture with two decoders, where the first decoder learns to denoise and the second decoder learns to translate from the denoised text. They adopt the approach proposed by Vaibhav et al. (2019) to synthesize the noisy text for their first decoder. We do not compare our model with (Berard et al., 2019; Helcl et al., 2019) as they use much more out-domain data, a great number of monolingual data and a bigger Transformer model, and hence not comparable with our experimental settings. text translations on all test sets for both ZH-EN and FR-EN and outperforms the Transformer baseline in terms of average BLEU by +1.01 and +1.2 on ZH-EN and FR-EN respectively, illustrating the superiority of our approach. The performance on social media test sets shows significant improvement with up to +1.63 BLEU over Transformer and +0.84 BLEU over the best approach (Wang et al., 2018) on ZH-EN. For FR-EN, ou"
2021.naacl-industry.11,P19-1425,0,0.0927595,"alness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embedding to overcome homophone noise. Meanwhile, Michel and Neubig (2018) released a dataset for evaluating NMT on social media text. This dataset was used as a benchmark for WMT 19 Robustness shared task (Li et al., 2019) to improve the"
2021.naacl-industry.11,2020.acl-main.529,0,0.0511499,"Missing"
2021.naacl-industry.11,P18-1163,0,0.356552,"then use a Noise Adaptation Layer (NAL) to enable a more stable contextual representation by minimizing the reconstruction loss. In the decoder, we add perturbations with a semantic constraint and apply the same reconstruction loss. Unlike adversarial examples which are crafted to cause the target model to fail, our perturbation process does not have such constraint and does not rely on a target model. Our input perturbations are randomly generated, representing any types of noises that can be observed in real-world usage. This makes the perturbation process generic, easy and fast. Following (Cheng et al., 2018), we generate semantically related perturbations in the decoder to increase the diversity of the translations. Together with NAL, our model shows its ability to resist noises in the input and produce more robust translations. Results on ZH-EN and FR-EN translation significantly improve over the baseline by +1.24 (MT03) and +1.4 (N15) BLEU on news domain, and +1.63 (Social), +1.3 (mtnt18) on social media domain respectively. Further fine-tuning experiments on FR-EN social media text even witness an average improvement of +1.25 BLEU over the best approach. 2 tions (Alzantot et al., 2018), paraph"
2021.naacl-industry.11,P16-1185,0,0.0233949,"regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embedding to overcome homoph"
2021.naacl-industry.11,C18-1055,0,0.0395703,"e the diversity of the translations. Together with NAL, our model shows its ability to resist noises in the input and produce more robust translations. Results on ZH-EN and FR-EN translation significantly improve over the baseline by +1.24 (MT03) and +1.4 (N15) BLEU on news domain, and +1.63 (Social), +1.3 (mtnt18) on social media domain respectively. Further fine-tuning experiments on FR-EN social media text even witness an average improvement of +1.25 BLEU over the best approach. 2 tions (Alzantot et al., 2018), paraphrasing (Iyyer et al., 2018; Ribeiro et al., 2018), character-level noise (Ebrahimi et al., 2018b; Tan et al., 2020a,b), or perturbations at embedding space (Miyato et al., 2016; Liang et al., 2020). Inspired by Lei et al. (2017) that nicely captures the semantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adeq"
2021.naacl-industry.11,J09-4006,0,0.0318413,"evaluate on two social media test sets: mtnt18 (Michel and Neubig, 2018) and mtnt19 (Li et al., 2019). FR-EN Fine-Tuning: We use the noisy training set (mtnttrain) provided by Michel and Neubig (2018) to fine-tune the FR-EN model. We use fairseq’s implementation of Transformer (Ott et al., 2019). In evaluation, we report case-insensitive tokenized BLEU for ZH-EN (Papineni et al., 2002) and sacre-BLEU (Post, 2018) for FR-EN. Following Michel and Neubig (2018), we do not use development set but only report best results on three social media test sets. We segment the Chinese words using THULAC (Li and Sun, 2009) and tokenize both French and English words using tokenize.perl2 . We apply BPE (Sennrich et al., 2016b) to get sub-word vocabularies for the encoder and decoder, both with 20K merge operations. The hyper-parameters setting is the same as transformer-base in (Vaswani et al., 2017) except that we set dropout rate as 0.4 in all our experiments. Our proposed models are trained on top of Transformer baseline for efficiency purpose, where additional parameters from the embeddings of Dx− and ReL are uniformly initialized. The madeup dictionary size M is set to 10,000. The size of dynamic set m is se"
2021.naacl-industry.11,P18-2006,0,0.0223022,"e the diversity of the translations. Together with NAL, our model shows its ability to resist noises in the input and produce more robust translations. Results on ZH-EN and FR-EN translation significantly improve over the baseline by +1.24 (MT03) and +1.4 (N15) BLEU on news domain, and +1.63 (Social), +1.3 (mtnt18) on social media domain respectively. Further fine-tuning experiments on FR-EN social media text even witness an average improvement of +1.25 BLEU over the best approach. 2 tions (Alzantot et al., 2018), paraphrasing (Iyyer et al., 2018; Ribeiro et al., 2018), character-level noise (Ebrahimi et al., 2018b; Tan et al., 2020a,b), or perturbations at embedding space (Miyato et al., 2016; Liang et al., 2020). Inspired by Lei et al. (2017) that nicely captures the semantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adeq"
2021.naacl-industry.11,D18-1045,0,0.021039,"iscourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embeddi"
2021.naacl-industry.11,P19-1291,0,0.0874009,"g the translation of those noisy words are hard and expensive to obtain; they are also not practical in handling real noises as noisy words can exhibit in random forms and cannot be fully anticipated during training. Belinkov and Bisk (2018) pointed out NMT models are sensitive to small input perturbations and if this issue is not addressed, it will continue to bottleneck the translation quality. In such cases, not only the word embeddings of perturbations may cause irregularities with the local context, the contextual representation of other words may also get affected by such perturbations (Liu et al., 2019). This phenomenon applies to valid words in unfamiliar context as well, which will also cause the translation to fail as illustrated in Table 1 (case 2). Introduction Recent techniques (Bahdanau et al., 2014; Wu et al., 2016; Vaswani et al., 2017) in NMT have gained remarkable improvement in translation quality. However, robust NMT that is immune to real input noise remains a big challenge for NMT researchers. Real input noises can exhibit in many forms such as spelling and grammatical errors, homophones replacement, Internet slang, new words or even a valid word used in an unfamiliar or a new"
2021.naacl-industry.11,W19-5364,0,0.0178853,"Vaibhav et al. (2019) leverage effective synthetic noise to make NMT resilient to noisy text. We implement their approach on Transformer backbone. For a fair comparison, we limit the data to train back-translation models only with mtnttrain. Zhou et al. (2019) adopt a multitask transformer architecture with two decoders, where the first decoder learns to denoise and the second decoder learns to translate from the denoised text. They adopt the approach proposed by Vaibhav et al. (2019) to synthesize the noisy text for their first decoder. We do not compare our model with (Berard et al., 2019; Helcl et al., 2019) as they use much more out-domain data, a great number of monolingual data and a bigger Transformer model, and hence not comparable with our experimental settings. text translations on all test sets for both ZH-EN and FR-EN and outperforms the Transformer baseline in terms of average BLEU by +1.01 and +1.2 on ZH-EN and FR-EN respectively, illustrating the superiority of our approach. The performance on social media test sets shows significant improvement with up to +1.63 BLEU over Transformer and +0.84 BLEU over the best approach (Wang et al., 2018) on ZH-EN. For FR-EN, our model outperforms W"
2021.naacl-industry.11,D18-1050,0,0.113599,"approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embedding to overcome homophone noise. Meanwhile, Michel and Neubig (2018) released a dataset for evaluating NMT on social media text. This dataset was used as a benchmark for WMT 19 Robustness shared task (Li et al., 2019) to improve the robustness of NMT models on noisy text. We show our approach also benefits the fine-tuning process using additional social media data. 3 Approaches We propose a Context-Enhanced Reconstruction (CER) approach to learn robust contextual representation in the presence of noisy words through a perturbation step and a reconstruction step in both encoder and decoder during model training. Figure 1 shows the architecture. The perturbation"
2021.naacl-industry.11,N18-1170,0,0.0237391,"erate semantically related perturbations in the decoder to increase the diversity of the translations. Together with NAL, our model shows its ability to resist noises in the input and produce more robust translations. Results on ZH-EN and FR-EN translation significantly improve over the baseline by +1.24 (MT03) and +1.4 (N15) BLEU on news domain, and +1.63 (Social), +1.3 (mtnt18) on social media domain respectively. Further fine-tuning experiments on FR-EN social media text even witness an average improvement of +1.25 BLEU over the best approach. 2 tions (Alzantot et al., 2018), paraphrasing (Iyyer et al., 2018; Ribeiro et al., 2018), character-level noise (Ebrahimi et al., 2018b; Tan et al., 2020a,b), or perturbations at embedding space (Miyato et al., 2016; Liang et al., 2020). Inspired by Lei et al. (2017) that nicely captures the semantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Met"
2021.naacl-industry.11,N19-4009,0,0.0176165,"1 . FR-EN: We use the same datasets as Michel and Neubig (2018). The training set consists of 2.16M sentence pairs extracted from europarl-v7 and news-commentary-v10. We use the newsdiscussdev2015 as development set and evaluate the model on two news test sets, newstest2014 (N14) and newsdiscusstest2015 (N15). We also evaluate on two social media test sets: mtnt18 (Michel and Neubig, 2018) and mtnt19 (Li et al., 2019). FR-EN Fine-Tuning: We use the noisy training set (mtnttrain) provided by Michel and Neubig (2018) to fine-tune the FR-EN model. We use fairseq’s implementation of Transformer (Ott et al., 2019). In evaluation, we report case-insensitive tokenized BLEU for ZH-EN (Papineni et al., 2002) and sacre-BLEU (Post, 2018) for FR-EN. Following Michel and Neubig (2018), we do not use development set but only report best results on three social media test sets. We segment the Chinese words using THULAC (Li and Sun, 2009) and tokenize both French and English words using tokenize.perl2 . We apply BPE (Sennrich et al., 2016b) to get sub-word vocabularies for the encoder and decoder, both with 20K merge operations. The hyper-parameters setting is the same as transformer-base in (Vaswani et al., 2017"
2021.naacl-industry.11,D19-5506,0,0.0157914,"to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embedding to overcome homophone noise. Meanwhile, Michel and Neubig (2018) released a dataset for evaluating NMT on social media text. This dataset was used as a benchmark for WMT 19 Robustness shared task (Li et al.,"
2021.naacl-industry.11,P02-1040,0,0.110253,"ists of 2.16M sentence pairs extracted from europarl-v7 and news-commentary-v10. We use the newsdiscussdev2015 as development set and evaluate the model on two news test sets, newstest2014 (N14) and newsdiscusstest2015 (N15). We also evaluate on two social media test sets: mtnt18 (Michel and Neubig, 2018) and mtnt19 (Li et al., 2019). FR-EN Fine-Tuning: We use the noisy training set (mtnttrain) provided by Michel and Neubig (2018) to fine-tune the FR-EN model. We use fairseq’s implementation of Transformer (Ott et al., 2019). In evaluation, we report case-insensitive tokenized BLEU for ZH-EN (Papineni et al., 2002) and sacre-BLEU (Post, 2018) for FR-EN. Following Michel and Neubig (2018), we do not use development set but only report best results on three social media test sets. We segment the Chinese words using THULAC (Li and Sun, 2009) and tokenize both French and English words using tokenize.perl2 . We apply BPE (Sennrich et al., 2016b) to get sub-word vocabularies for the encoder and decoder, both with 20K merge operations. The hyper-parameters setting is the same as transformer-base in (Vaswani et al., 2017) except that we set dropout rate as 0.4 in all our experiments. Our proposed models are tra"
2021.naacl-industry.11,W18-6319,0,0.0124473,"from europarl-v7 and news-commentary-v10. We use the newsdiscussdev2015 as development set and evaluate the model on two news test sets, newstest2014 (N14) and newsdiscusstest2015 (N15). We also evaluate on two social media test sets: mtnt18 (Michel and Neubig, 2018) and mtnt19 (Li et al., 2019). FR-EN Fine-Tuning: We use the noisy training set (mtnttrain) provided by Michel and Neubig (2018) to fine-tune the FR-EN model. We use fairseq’s implementation of Transformer (Ott et al., 2019). In evaluation, we report case-insensitive tokenized BLEU for ZH-EN (Papineni et al., 2002) and sacre-BLEU (Post, 2018) for FR-EN. Following Michel and Neubig (2018), we do not use development set but only report best results on three social media test sets. We segment the Chinese words using THULAC (Li and Sun, 2009) and tokenize both French and English words using tokenize.perl2 . We apply BPE (Sennrich et al., 2016b) to get sub-word vocabularies for the encoder and decoder, both with 20K merge operations. The hyper-parameters setting is the same as transformer-base in (Vaswani et al., 2017) except that we set dropout rate as 0.4 in all our experiments. Our proposed models are trained on top of Transformer b"
2021.naacl-industry.11,D19-1087,1,0.845395,", 2020a,b), or perturbations at embedding space (Miyato et al., 2016; Liang et al., 2020). Inspired by Lei et al. (2017) that nicely captures the semantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple"
2021.naacl-industry.11,N18-2084,0,0.0210416,"not comparable with our experimental settings. text translations on all test sets for both ZH-EN and FR-EN and outperforms the Transformer baseline in terms of average BLEU by +1.01 and +1.2 on ZH-EN and FR-EN respectively, illustrating the superiority of our approach. The performance on social media test sets shows significant improvement with up to +1.63 BLEU over Transformer and +0.84 BLEU over the best approach (Wang et al., 2018) on ZH-EN. For FR-EN, our model outperforms Wang et al. (2018) by +1.5 and +1.0 BLEU on mtnt18 and mtnt19 respectively. Zhou et al. (2019) use mtnttrain and TED (Qi et al., 2018) to synthesize noisy sentences for their first decoder, hence effectively they are exploiting indomain data during training and thus not quite a fair comparison in the evaluation. Nevertheless, CER still significantly outperforms Zhou et al. (2019) by +2.0 BLEU on mtnt18. 5 5.2 5.1 Results and Analysis Effect of Noise We investigate the effect of different noise-insertion methods by dynamically inserting noise into the source side of the original training set using different strategies with a same probability σx . Madeup: Our approach to add made-up words. Semantics: We test our semantic const"
2021.naacl-industry.11,W19-5303,0,0.0525771,"t al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an additional phonetic embedding to overcome homophone noise. Meanwhile, Michel and Neubig (2018) released a dataset for evaluating NMT on social media text. This dataset was used as a benchmark for WMT 19 Robustness shared task (Li et al., 2019) to improve the robustness of NMT models on noisy text. We show our approach also benefits the fine-tuning process using additional social media data. 3 Approaches We propose a Context-Enhanced Reconstruction (CER) approach to learn robust contextual representation in the presence of noisy words through a perturbation step and a reconstruction step in both encoder and decoder during model training. Figure 1 shows the architecture. The perturbation step automatically inserts made-up words in the input sequence x to generate a noisy example x0 . The noisy example mimics input where text naturaln"
2021.naacl-industry.11,P16-1009,0,0.263978,"mantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an addit"
2021.naacl-industry.11,P16-1162,0,0.377902,"mantic interactions in discourse relation, we regard noise as a disruptor to break semantic interactions and propose our CER approach to mitigate this phenomenon. We make up “noisy” words randomly to act as random noise in the input to break the text naturalness. Our experiment demonstrates its superiority in multiple dimensions. Robust Neural Machine Translation: Methods have been proposed to make NMT models resilient not only to adequacy errors (Lei et al., 2019) but also to both natural and synthetic noise. Incorporating monolingual data into NMT has the capacity to improve the robustness (Sennrich et al., 2016a; Edunov et al., 2018; Cheng et al., 2016). Some non data-driven approaches that specifically designed to address the robustness problem of NMT (Sperber et al., 2017; Ebrahimi et al., 2018a; Wang et al., 2018; Karpukhin et al., 2019; Cheng et al., 2019, 2020) explored effective ways to synthesize adversarial examples into the training data. Belinkov and Bisk (2018) showed a structure-invariant word representation capable of addressing multiple typo noise. Cheng et al. (2018) used adversarial stability training strategy to make NMT resilient to arbitrary noise. Liu et al. (2019) added an addit"
2021.naacl-industry.11,W19-5368,0,0.0190413,"and a bigger Transformer model, and hence not comparable with our experimental settings. text translations on all test sets for both ZH-EN and FR-EN and outperforms the Transformer baseline in terms of average BLEU by +1.01 and +1.2 on ZH-EN and FR-EN respectively, illustrating the superiority of our approach. The performance on social media test sets shows significant improvement with up to +1.63 BLEU over Transformer and +0.84 BLEU over the best approach (Wang et al., 2018) on ZH-EN. For FR-EN, our model outperforms Wang et al. (2018) by +1.5 and +1.0 BLEU on mtnt18 and mtnt19 respectively. Zhou et al. (2019) use mtnttrain and TED (Qi et al., 2018) to synthesize noisy sentences for their first decoder, hence effectively they are exploiting indomain data during training and thus not quite a fair comparison in the evaluation. Nevertheless, CER still significantly outperforms Zhou et al. (2019) by +2.0 BLEU on mtnt18. 5 5.2 5.1 Results and Analysis Effect of Noise We investigate the effect of different noise-insertion methods by dynamically inserting noise into the source side of the original training set using different strategies with a same probability σx . Madeup: Our approach to add made-up word"
2021.naacl-industry.11,2020.acl-main.263,1,0.903927,"night to score points quickly. 我已剪短了我的发,剪断了惩罚,剪一地伤透我 的尴尬。。。。 I have cut my hair, i cut off the punishment, i away the awkwardness that hurt me. I got my punishment, got rid of my embarrassment. I cut short my hair , cut off punishment , and cut off my embarrassment that hurts me. Table 1: Examples of NMT’s vulnerability in translating text containing noisy words (“zei&quot; → “thief&quot;, “chengfa&quot; → “punishment&quot;). CER mitigates the effect of noisy words. Noisy words have long been discussed in previous work. Aw et al. (2006) proposed the normalization approach to reduce the noise before translation. Tan et al. (2020a,b) addressed the character-level noise directly in the NMT model. Though these approaches addressed the effect of noisy words to some extent, they are limited to spelling errors, inflectional variations, and other noises definable during training. In addition, strong external supervision like a parallel corpus of noisy text translation or dictionary containing the translation of those noisy words are hard and expensive to obtain; they are also not practical in handling real noises as noisy words can exhibit in random forms and cannot be fully anticipated during training. Belinkov and Bisk (2"
2021.naacl-industry.11,2020.emnlp-main.455,1,0.907918,"night to score points quickly. 我已剪短了我的发,剪断了惩罚,剪一地伤透我 的尴尬。。。。 I have cut my hair, i cut off the punishment, i away the awkwardness that hurt me. I got my punishment, got rid of my embarrassment. I cut short my hair , cut off punishment , and cut off my embarrassment that hurts me. Table 1: Examples of NMT’s vulnerability in translating text containing noisy words (“zei&quot; → “thief&quot;, “chengfa&quot; → “punishment&quot;). CER mitigates the effect of noisy words. Noisy words have long been discussed in previous work. Aw et al. (2006) proposed the normalization approach to reduce the noise before translation. Tan et al. (2020a,b) addressed the character-level noise directly in the NMT model. Though these approaches addressed the effect of noisy words to some extent, they are limited to spelling errors, inflectional variations, and other noises definable during training. In addition, strong external supervision like a parallel corpus of noisy text translation or dictionary containing the translation of those noisy words are hard and expensive to obtain; they are also not practical in handling real noises as noisy words can exhibit in random forms and cannot be fully anticipated during training. Belinkov and Bisk (2"
2021.naacl-industry.11,N19-1190,0,0.0374744,"Missing"
2021.naacl-main.128,2020.emnlp-main.191,1,0.836071,"Missing"
2021.naacl-main.128,D15-1263,0,0.0692259,"connected together to form a complexity of O(n3 ) for n EDUs. The top-down coherent discourse. The goal of discourse parsing is to uncover this underlying coherence structure, parsers are relatively new in discourse (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020). which has been shown to benefit numerous NLP These methods focus on finding splitting points in applications including text classification (Ji and each iteration to build a DT. However, the local Smith, 2017), summarization (Gerani et al., 2014), decisions could still affect the performance as most sentiment analysis (Bhatia et al., 2015), machine translation evaluation (Joty et al., 2017) and con- of the methods are still greedy. versational machine reading (Gao et al., 2020). Like most other fields in NLP, language parsing Rhetorical Structure Theory or RST (Mann and has also undergone a major paradigm shift from traThompson, 1988), one of the most influential the- ditional feature-based statistical parsing to end-toories of discourse, postulates a hierarchical dis- end neural parsing. Being able to parse a document course structure called discourse tree (DT). The end-to-end from scratch is appealing for several leaves of a"
2021.naacl-main.128,E17-1028,0,0.122553,"nkcisions at token boundaries and use a seq2seq network to model the splitting decisions. Our ing the EDUs into a DT (discourse parsing). framework facilitates discourse parsing from Discourse parsers can be singled out by whether scratch without requiring discourse segmenthey apply a bottom-up or top-down procedure. tation as a prerequisite; rather, it yields segBottom-up parsers include transition-based modmentation as part of the parsing process. Our els (Feng and Hirst, 2014; Ji and Eisenstein, 2014; unified parsing model adopts a beam search to decode the best tree structure by searching Braud et al., 2017; Wang et al., 2017) or globally through a space of high scoring trees. With optimized chart parsing models (Soricut and Marcu, extensive experiments on the standard English 2003; Joty et al., 2013, 2015). The former conRST discourse treebank, we demonstrate that structs a DT by a sequence of shift and reduce deour parser outperforms existing methods by a cisions, and can parse a text in asymptotic running good margin in both end-to-end parsing and time that is linear in number of EDUs. However, parsing with gold segmentation. More importhe transition-based parsers make greedy local decitantly"
2021.naacl-main.128,C16-1179,0,0.0184743,"t are provided with the EDUs. Nonetheless, we believe there is still room for speed improvement by choosing a better network, like the Longformer (Beltagy et al., 2020) which has an O(1) parallel time complexity in encoding a text, compared to the O(n) complexity of the recurrent encoder. ployed statistical models with handcrafted features (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2015). Even within the neural paradigm, most previous studies still rely on external features to achieve their best performances (Ji and Eisenstein, 2014; Wang et al., 2017; Braud et al., 2016, 2017; Yu et al., 2018). These parsers adopt a bottom-up approach, either transition-based or chart-based parsing. Recently, top-down parsing has attracted more attention due to its ability to maintain an overall view of the input text. Inspired by the Stack-Pointer network (Ma et al., 2018) for dependency parsing, Lin et al. (2019) first propose a seq2seq model for sentence-level parsing. Zhang et al. (2020) extend this to the document level. Kobayashi et al. (2020) adopt a greedy splitting mechanism for discourse parsing inspired by Stern et al. (2017)’s work in constituency parsing. By usi"
2021.naacl-main.128,D16-1001,0,0.0266366,"bi contextual representations. yt ∈S = |S| Y   Pθ (it , jt ) ) kt |((i, j) ) k)<t , x t=1 This end-to-end conditional splitting formulation is the main novelty of our method and is in contrast to previous approaches which rely on offlineinferred EDUs from a separate discourse segmenter. Our formalism streamlines the overall parsing process, unifies the neural components seamlessly and smoothens the training process. 2.2 Model Architecture Token-boundary Span Representations. To represent each token-boundary position k between token positions k and k + 1, we use the fencepost representation (Cross and Huang, 2016): hk = [fk ; bk+1 ] (3) where fk and bk+1 are the forward and backward LSTM hidden vectors of positions k and k + 1 respectively, and [·; ·] is the concatenation operation. Then, to represent the token-boundary span (i, j), we use the linear combination of the two endpoints i and j as: In the following, we describe the components of our parsing model: the document encoder, the 1616 hi,j = W1 hi + W2 hj (4) hlik = MLPl ([hi ; hk ]); hrkj = MLPr ([hk ; hj ])  Pθ (l|(i, k), (k, j)) = softmax (hlik )T Wlr hrkj  +(hlik )T Wl + (hrkj )T Wr + b ∗ l(i,k),(k,j) = arg max Pθ (l|(i, k), (k, j)) (8) (9)"
2021.naacl-main.128,P14-1048,0,0.408554,"e structure generally requires breaking the discourse parsing as a sequence of splitting detext into EDUs (discourse segmentation) and linkcisions at token boundaries and use a seq2seq network to model the splitting decisions. Our ing the EDUs into a DT (discourse parsing). framework facilitates discourse parsing from Discourse parsers can be singled out by whether scratch without requiring discourse segmenthey apply a bottom-up or top-down procedure. tation as a prerequisite; rather, it yields segBottom-up parsers include transition-based modmentation as part of the parsing process. Our els (Feng and Hirst, 2014; Ji and Eisenstein, 2014; unified parsing model adopts a beam search to decode the best tree structure by searching Braud et al., 2017; Wang et al., 2017) or globally through a space of high scoring trees. With optimized chart parsing models (Soricut and Marcu, extensive experiments on the standard English 2003; Joty et al., 2013, 2015). The former conRST discourse treebank, we demonstrate that structs a DT by a sequence of shift and reduce deour parser outperforms existing methods by a cisions, and can parse a text in asymptotic running good margin in both end-to-end parsing and time that is"
2021.naacl-main.128,P07-1062,0,0.165193,"Missing"
2021.naacl-main.128,D14-1168,0,0.073071,"Missing"
2021.naacl-main.128,P14-1002,0,0.378293,"requires breaking the discourse parsing as a sequence of splitting detext into EDUs (discourse segmentation) and linkcisions at token boundaries and use a seq2seq network to model the splitting decisions. Our ing the EDUs into a DT (discourse parsing). framework facilitates discourse parsing from Discourse parsers can be singled out by whether scratch without requiring discourse segmenthey apply a bottom-up or top-down procedure. tation as a prerequisite; rather, it yields segBottom-up parsers include transition-based modmentation as part of the parsing process. Our els (Feng and Hirst, 2014; Ji and Eisenstein, 2014; unified parsing model adopts a beam search to decode the best tree structure by searching Braud et al., 2017; Wang et al., 2017) or globally through a space of high scoring trees. With optimized chart parsing models (Soricut and Marcu, extensive experiments on the standard English 2003; Joty et al., 2013, 2015). The former conRST discourse treebank, we demonstrate that structs a DT by a sequence of shift and reduce deour parser outperforms existing methods by a cisions, and can parse a text in asymptotic running good margin in both end-to-end parsing and time that is linear in number of EDUs"
2021.naacl-main.128,P17-1092,0,0.0373963,"Missing"
2021.naacl-main.128,D12-1083,1,0.859872,"Missing"
2021.naacl-main.128,J15-3002,1,0.819998,"etrained models, our parser with gold segmentation is about 2.4 times faster than (Yu et al., 2018). Our end-to-end parser that also performs segmentation is faster than the baselines that are provided with the EDUs. Nonetheless, we believe there is still room for speed improvement by choosing a better network, like the Longformer (Beltagy et al., 2020) which has an O(1) parallel time complexity in encoding a text, compared to the O(n) complexity of the recurrent encoder. ployed statistical models with handcrafted features (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2015). Even within the neural paradigm, most previous studies still rely on external features to achieve their best performances (Ji and Eisenstein, 2014; Wang et al., 2017; Braud et al., 2016, 2017; Yu et al., 2018). These parsers adopt a bottom-up approach, either transition-based or chart-based parsing. Recently, top-down parsing has attracted more attention due to its ability to maintain an overall view of the input text. Inspired by the Stack-Pointer network (Ma et al., 2018) for dependency parsing, Lin et al. (2019) first propose a seq2seq model for sentence-level parsing. Zhang et al. (2020)"
2021.naacl-main.128,P13-1048,1,0.908209,"rs can be singled out by whether scratch without requiring discourse segmenthey apply a bottom-up or top-down procedure. tation as a prerequisite; rather, it yields segBottom-up parsers include transition-based modmentation as part of the parsing process. Our els (Feng and Hirst, 2014; Ji and Eisenstein, 2014; unified parsing model adopts a beam search to decode the best tree structure by searching Braud et al., 2017; Wang et al., 2017) or globally through a space of high scoring trees. With optimized chart parsing models (Soricut and Marcu, extensive experiments on the standard English 2003; Joty et al., 2013, 2015). The former conRST discourse treebank, we demonstrate that structs a DT by a sequence of shift and reduce deour parser outperforms existing methods by a cisions, and can parse a text in asymptotic running good margin in both end-to-end parsing and time that is linear in number of EDUs. However, parsing with gold segmentation. More importhe transition-based parsers make greedy local decitantly, it does so without using any handcrafted sions at each decoding step, which could propagate features, making it faster and easily adaptable to new languages and domains. errors into future steps."
2021.naacl-main.128,P81-1022,0,0.664519,"Missing"
2021.naacl-main.128,D16-1035,0,0.228338,"Missing"
2021.naacl-main.128,P19-1410,1,0.895603,"d easily adaptable to new languages and domains. errors into future steps. In contrast, chart parsers learn scoring functions for sub-trees and adopt a 1 Introduction CKY-like algorithm to search for the highest scorIn a document, the clauses, sentences and para- ing tree. These methods normally have higher accuracy but suffer from a slow parsing speed with a graphs are logically connected together to form a complexity of O(n3 ) for n EDUs. The top-down coherent discourse. The goal of discourse parsing is to uncover this underlying coherence structure, parsers are relatively new in discourse (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020). which has been shown to benefit numerous NLP These methods focus on finding splitting points in applications including text classification (Ji and each iteration to build a DT. However, the local Smith, 2017), summarization (Gerani et al., 2014), decisions could still affect the performance as most sentiment analysis (Bhatia et al., 2015), machine translation evaluation (Joty et al., 2017) and con- of the methods are still greedy. versational machine reading (Gao et al., 2020). Like most other fields in NLP, language parsing Rhetorical Structure T"
2021.naacl-main.128,P18-1130,0,0.0255765,"d statistical models with handcrafted features (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2014; Joty et al., 2015). Even within the neural paradigm, most previous studies still rely on external features to achieve their best performances (Ji and Eisenstein, 2014; Wang et al., 2017; Braud et al., 2016, 2017; Yu et al., 2018). These parsers adopt a bottom-up approach, either transition-based or chart-based parsing. Recently, top-down parsing has attracted more attention due to its ability to maintain an overall view of the input text. Inspired by the Stack-Pointer network (Ma et al., 2018) for dependency parsing, Lin et al. (2019) first propose a seq2seq model for sentence-level parsing. Zhang et al. (2020) extend this to the document level. Kobayashi et al. (2020) adopt a greedy splitting mechanism for discourse parsing inspired by Stern et al. (2017)’s work in constituency parsing. By using pretrained models/embeddings and extra features (e.g., syntactic, text organizational features), these models achieve competitive results. However, their decoder infers a tree greedily. Our approach differs from previous work in that it can perform end-to-end discourse parsing in a single"
2021.naacl-main.128,J00-3005,0,0.769337,"Missing"
2021.naacl-main.128,D17-1136,0,0.231973,"Missing"
2021.naacl-main.128,J18-2001,0,0.0125472,"57.1 Human Agreement Ji and Eisenstein (2014)+ 64.1 54.2 46.8 Feng and Hirst (2014)+ 68.6 55.9 45.8 Joty et al. (2015)+ 65.1 55.5 45.1 Li et al. (2016)+ 64.5 54.0 38.1 Braud et al. (2016) 59.5 47.2 34.7 Braud et al. (2017)∗ 62.7 54.5 45.5 Yu et al. (2018)+§ 71.4 60.3 49.2 Zhang et al. (2020)+ 67.2 55.5 45.3 Our with GloVe 71.1 59.6 47.7 Our with XLNet§ 74.3 64.3 51.6 Parsing with Gold Segmentation Settings. Discourse parsing with gold EDUs has been the standard practice in many previous studies. We compare our model with ten different baselines as shown in Table 1. We report most results from Morey et al. (2018); Zhang et al. (2020); Kobayashi et al. (2020), while we reproduce Yu et al. (2018) using their provided source code. For our model setup, we use the encoder-decoder framework with a 3-layer Bi-LSTM encoder and 3-layer unidirectional LSTM decoder. The LSTM hidden size is 400, the word embedding size is 100 for random initialization, while the character embedding size is 50. The hidden dimension in MLP modules and biaffine function for structure prediction is 500. The beam width B is 20. Our model is trained by Adam optimizer (Kingma and Ba, 2015) with a batch size of 10000 tokens. Our learning"
2021.naacl-main.128,D14-1162,0,0.0840678,"Missing"
2021.naacl-main.128,N03-1030,0,0.621587,"tional feature-rich methods. However, successful document level neural parsers still rely heavily on handcrafted features (Ji and Eisenstein, 2014; Yu et al., 2018; Zhang et al., 2020; Kobayashi et al., 2020). Therefore, even though these methods adopt a neural framework, they are not “end-to-end” and do not enjoy the above mentioned benefits of an end-to-end neural parser. Moreover, in existing methods (both traditional and neural), discourse segmentation is detached from parsing and treated as a prerequisite step. Therefore, the errors in segmentation affect the overall parsing performance (Soricut and Marcu, 2003; Joty et al., 2012). In view of the limitations of existing approaches, in this work we propose an end-to-end top-down document level parsing model that: • Can generate a discourse tree from scratch without requiring discourse segmentation as a prerequisite step; rather, it generates the EDUs as a by-product of parsing. Crucially, this novel formulation facilitates solving the two tasks in a single neural model. Our formulation is generic and works in the same way when it is provided with the EDU segmentation. • Treats discourse parsing as a sequence of splitting decisions at token boundaries"
2021.naacl-main.128,P17-1076,0,0.0208204,"nd Eisenstein, 2014; Wang et al., 2017; Braud et al., 2016, 2017; Yu et al., 2018). These parsers adopt a bottom-up approach, either transition-based or chart-based parsing. Recently, top-down parsing has attracted more attention due to its ability to maintain an overall view of the input text. Inspired by the Stack-Pointer network (Ma et al., 2018) for dependency parsing, Lin et al. (2019) first propose a seq2seq model for sentence-level parsing. Zhang et al. (2020) extend this to the document level. Kobayashi et al. (2020) adopt a greedy splitting mechanism for discourse parsing inspired by Stern et al. (2017)’s work in constituency parsing. By using pretrained models/embeddings and extra features (e.g., syntactic, text organizational features), these models achieve competitive results. However, their decoder infers a tree greedily. Our approach differs from previous work in that it can perform end-to-end discourse parsing in a single neural framework without needing segmentation as a prerequisite. Our model can parse a document from scratch without relying on any external features. Moreover, it can apply efficient beam search decoding to search for the best tree. 5 Conclusion We have presented a n"
2021.naacl-main.128,P17-2029,0,0.111834,"oundaries and use a seq2seq network to model the splitting decisions. Our ing the EDUs into a DT (discourse parsing). framework facilitates discourse parsing from Discourse parsers can be singled out by whether scratch without requiring discourse segmenthey apply a bottom-up or top-down procedure. tation as a prerequisite; rather, it yields segBottom-up parsers include transition-based modmentation as part of the parsing process. Our els (Feng and Hirst, 2014; Ji and Eisenstein, 2014; unified parsing model adopts a beam search to decode the best tree structure by searching Braud et al., 2017; Wang et al., 2017) or globally through a space of high scoring trees. With optimized chart parsing models (Soricut and Marcu, extensive experiments on the standard English 2003; Joty et al., 2013, 2015). The former conRST discourse treebank, we demonstrate that structs a DT by a sequence of shift and reduce deour parser outperforms existing methods by a cisions, and can parse a text in asymptotic running good margin in both end-to-end parsing and time that is linear in number of EDUs. However, parsing with gold segmentation. More importhe transition-based parsers make greedy local decitantly, it does so without"
2021.naacl-main.128,C18-1047,0,0.212726,"2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1613–1625 June 6–11, 2021. ©2021 Association for Computational Linguistics and domain/language expertise. Second, the lack of an explicit feature extraction phase makes the training and testing (decoding) faster. Because of the task complexity, it is only recently that neural approaches have started to outperform traditional feature-rich methods. However, successful document level neural parsers still rely heavily on handcrafted features (Ji and Eisenstein, 2014; Yu et al., 2018; Zhang et al., 2020; Kobayashi et al., 2020). Therefore, even though these methods adopt a neural framework, they are not “end-to-end” and do not enjoy the above mentioned benefits of an end-to-end neural parser. Moreover, in existing methods (both traditional and neural), discourse segmentation is detached from parsing and treated as a prerequisite step. Therefore, the errors in segmentation affect the overall parsing performance (Soricut and Marcu, 2003; Joty et al., 2012). In view of the limitations of existing approaches, in this work we propose an end-to-end top-down document level parsi"
2021.naacl-main.128,W19-2713,0,0.0176618,"d truth sentence segmentation is not available. We also notice that pretraining (GloVe) leads to improved performance. Error Analysis. We show our best parser’s (with gold EDUs) confusion matrix for the 10 most frequent relation labels in Fig. 5. The complete matrix with the 18 relations is shown in Appendix (Fig. 8). The imbalanced relation distribution in RST-DT affects our model’s performance to some extent. Also semantic similar relations tend to be confused 2 with each other. Fig. 6 shows an example where our We could not compare our segmentation results with the DISRPT 2019 Shared Task (Zeldes et al., 2019) participants. model mistakenly labels Summary as Elaboration. We found few inconsistencies in the settings. First, in their However, one could argue that the relation Elabo“gold sentence” dataset, instead of using the gold sentence, ration is also valid here because the parenthesized they pre-process the text with an automatic tokenizer and sentence segmenter. Second, in the evaluation, under the same text brings additional information (the equivalent settings, they do not exclude the trivial BeginSegment label at amount of money). We show more error examples the beginning of each sentence wh"
2021.naacl-main.128,2020.acl-main.569,0,0.502495,"to new languages and domains. errors into future steps. In contrast, chart parsers learn scoring functions for sub-trees and adopt a 1 Introduction CKY-like algorithm to search for the highest scorIn a document, the clauses, sentences and para- ing tree. These methods normally have higher accuracy but suffer from a slow parsing speed with a graphs are logically connected together to form a complexity of O(n3 ) for n EDUs. The top-down coherent discourse. The goal of discourse parsing is to uncover this underlying coherence structure, parsers are relatively new in discourse (Lin et al., 2019; Zhang et al., 2020; Kobayashi et al., 2020). which has been shown to benefit numerous NLP These methods focus on finding splitting points in applications including text classification (Ji and each iteration to build a DT. However, the local Smith, 2017), summarization (Gerani et al., 2014), decisions could still affect the performance as most sentiment analysis (Bhatia et al., 2015), machine translation evaluation (Joty et al., 2017) and con- of the methods are still greedy. versational machine reading (Gao et al., 2020). Like most other fields in NLP, language parsing Rhetorical Structure Theory or RST (Mann a"
2021.naacl-main.282,P18-2006,0,0.0173134,"lation with “code-switching pretraining”, replacing words with their translations in a similar manner to masked language modeling (Devlin et al., 2019). These word pairs are constructed from monolingual corpora using cosine similarity. Sitaram et al. (2019) provide a comprehensive survey of code-mixed language processing. Word-level adversaries. Modified inputs aimed at disrupting a model’s predictions are known as adversarial examples (Szegedy et al., 2014). In NLP, perturbations can be applied at the character, subword, word, phrase, or sentence levels. Early word-level adversarial attacks (Ebrahimi et al., 2018; Blohm et al., 2018) made use of the target model’s gradients to flip individual words to trick the model into making the wrong prediction. However, while the perturbations were adversarial for the target model, perturbed word’s original semantics was often not preserved. This could result in the expected prediction changing and making the model appear more brittle than it actually is. Later research addressed this by searching for adversarial rules (Ribeiro et al., 2018) or by constraining the candidate perturbations to the k nearest neighbors in the embedding space (Alzantot et al., 2018; M"
2021.naacl-main.282,2020.emnlp-main.498,0,0.0503446,"Missing"
2021.naacl-main.282,2020.emnlp-main.182,0,0.0359563,"до mahali there’s two or three aircraft arrive in a week and I didn’t know where they’re flying to. H: 从来没有 aircraft arriving. Before: Contradiction After: Entailment Table 1: B UMBLEBEE adversaries found for XLM-R on XNLI (P: Premise; H: Hypothesis). et al. (2020) take another approach by making use of a annotated sememes to disambiguate polysemous words, while Tan et al. (2020a) perturb only the words’ morphology and encourage semantic preservation via a part-of-speech constraint. Other approaches make use of language models to generate candidate perturbations (Garg and Ramakrishnan, 2020; Han et al., 2020). Wallace et al. (2019) find phrases that act as universally adversarial perturbations when prepended to clean inputs. Zhang et al. (2020) provide a comprehensive survey. rules, from different languages in a single sentence. This is distinguished from code-switching, which occurs at the inter-sentential level (Kachru, 1978). Extreme code-mixing. Inspired by the proliferation of real-life code-mixing and polyglots, we propose P OLY G LOSS and B UMBLEBEE, two multilingual adversarial attacks that adopt the persona of an adversarial code-mixer. We focus on the lexical component of code-mixing, wh"
2021.naacl-main.282,D19-1252,0,0.0516571,"Missing"
2021.naacl-main.282,C82-1023,0,0.34385,"cess of Transformer models (Vaswani et al., 2017), recent multilingual models like mBERT (Devlin et al., 2019), Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020a) take the pretraining→fine-tuning paradigm into the multilingual realm by pretraining Transformer encoders on unlabeled monolingual corpora with various language modeling objectives before fine-tuning them on task data from a high-resource language such as English. This is known as cross-lingual transfer. Code-mixed text processing. Previous research on code-mixed text processing focused on constructing formal grammars (Joshi, 1982) and tokenlevel language identification (Bali et al., 2014; Solorio et al., 2014; Barman et al., 2014), before progressing to named entity recognition and part-of-speech tagging (Ball and Garrette, 2018; AlGhamdi and Diab, 2019; Aguilar and Solorio, 2020). Recent work explores code-mixing in higher-level tasks such as question answering and task-oriented dialogue (Chandu et al., 2019; Ahn et al., 2020). Muller et al. (2020) demonstrate mBERT’s ability to transfer to an unseen dialect by exploiting its speakers’ tendency to code-mix. A key challenge of developing models that are robust to code-"
2021.naacl-main.282,K17-1024,1,0.889251,"Missing"
2021.naacl-main.282,2020.acl-main.329,0,0.0238509,"aximizes the target model’s loss in a black-box manner (Alg. 2 in Appendix B.1). In our implementation, we also keep track of successful adversaries and return the ones with the highest and lowest losses. The former is a stronger adversary, while the latter often has fewer perturbations. More details are in Appendix B.1. Orthographic preservation. When the embedded language uses a different script from the matrix language, code-mixers tend to transliterate borrowed words into the same script (Abuhakema, 2013; Bali et al., 2014). This still poses a significant challenge to multilingual models (Khanuja et al., 2020). We generally preserve the embedded language’s script where possible to avoid unfairly penalizing the target model since there is often no standard way of transliterating words. Scalable sense disambiguation. Due to the polysemous nature of many words, translating the right sense is crucial to preserving the word’s (and sentence’s) semantics. Common word sense disambiguation methods (Agirre and Edmonds, 2007) use a sense tagger trained on an annotated sense inventory such as WordNet (Miller, 1995). However, this approach requires individual taggers and sense inventories for each matrix and em"
2021.naacl-main.282,C12-1089,0,0.0493525,"attacks. • Propose an efficient adversarial training scheme that takes the same number of steps as standard supervised training and show that it creates more language-invariant representations, improving accuracy in the absence of lexical overlap. 2 Related Work Multilingual classifiers. Low resource languages often lack support due to the high cost of annotating data for supervised learning. An approach to tackle this challenge is to build cross-lingual representations that only need to be trained on task data from a high resource language to perform well on another under-resourced language (Klementiev et al., 2012). Artetxe and Schwenk (2019) presented the first general purpose multilingual representation using a BiLSTM encoder. Following the success of Transformer models (Vaswani et al., 2017), recent multilingual models like mBERT (Devlin et al., 2019), Unicoder (Huang et al., 2019), and XLM-R (Conneau et al., 2020a) take the pretraining→fine-tuning paradigm into the multilingual realm by pretraining Transformer encoders on unlabeled monolingual corpora with various language modeling objectives before fine-tuning them on task data from a high-resource language such as English. This is known as cross-l"
2021.naacl-main.282,2020.emnlp-main.484,0,0.0525439,"Missing"
2021.naacl-main.282,N19-1314,0,0.0207748,"8; Blohm et al., 2018) made use of the target model’s gradients to flip individual words to trick the model into making the wrong prediction. However, while the perturbations were adversarial for the target model, perturbed word’s original semantics was often not preserved. This could result in the expected prediction changing and making the model appear more brittle than it actually is. Later research addressed this by searching for adversarial rules (Ribeiro et al., 2018) or by constraining the candidate perturbations to the k nearest neighbors in the embedding space (Alzantot et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019; Li et al., 2019; Jin et al., 2020). Zang Original Adversary Prediction P: The girl that can help me is all the way across town. H: There is no one who can help me. P: olan girl that can help me is all the way across town. H: dw§ ¯ one who can help me. Before: Contradiction After: Entailment Original Adversary Prediction P: We didn’t know where they were going. H: We didn’t know where the people were traveling to. P: We didn’t know where they were going. H: We didn’t know where les gens allaient. Before: Entailment After: Neutral Original Adversary Predi"
2021.naacl-main.282,2020.emnlp-main.363,0,0.0802466,"Missing"
2021.naacl-main.282,P19-1612,0,0.0213134,"T when the premise and hypothesis were in different languages (Fake English vs. {Hindi, Russian, Spanish}), theorizing this to be an effect of disrupting the model’s reliance on lexical overlap. Our experiments in §4 and §5 lend support to this hypothesis. In Table 1, we see multiple examples where the prediction was flipped from “contradiction” to “entailment” simply by perturbing a few words. If the models did not rely on lexical overlap but performed comparisons at the semantic level, such perturbations should not have severely impacted their performance. Our results on QA also corroborate Lee et al. (2019)’s finding that models trained on SQuAD-style datasets exploit lexical overlap between the question and context. 5 Code-Mixed Adversarial Training Finally, we propose code-mixed adversarial training (CAT), an extension of the standard adversarial training paradigm (Goodfellow et al., 2015), to improve the robustness of multilingual models to adversarial polyglots. In standard adversarial training, adversarial attacks are run on the training set to generate adversaries for training. However, this makes adversarial training computationally expensive. Hence, we take inspiration from Tan et al. (2"
2021.naacl-main.282,P19-1493,0,0.0343982,"y.1 1 (a) Aligned words across sentences (b) Extracted candidate perturbations (c) Final multilingual adversary Figure 1: B UMBLEBEE’s three key stages of adversary generation: (a) Align words in the matrix (English) and embedded sentences (top: Indonesian, bottom: Chinese); (b) Extract candidate perturbations from embedded sentences; (c) Construct final adversary by maximizing the target model’s loss. Introduction The past year has seen incredible breakthroughs in cross-lingual generalization with the advent of massive multilingual models that aim to learn universal language representations (Pires et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020b). These models have demonstrated impressive cross-lingual transfer abilities: simply fine-tuning them on task data from a high resource language such as English after pretraining on monolingual corpora was sufficient to manifest such abilities. This was observed even for languages with different scripts and no vocabulary overlap (K et al., 2020). However, transferring from one language to another is insufficient for NLP systems to understand multilingual speakers in an increasingly multilingual world (Aronin and Singleton, 2008). In many 1 Code: git"
2021.naacl-main.282,P18-1079,0,0.0437398,"rbations can be applied at the character, subword, word, phrase, or sentence levels. Early word-level adversarial attacks (Ebrahimi et al., 2018; Blohm et al., 2018) made use of the target model’s gradients to flip individual words to trick the model into making the wrong prediction. However, while the perturbations were adversarial for the target model, perturbed word’s original semantics was often not preserved. This could result in the expected prediction changing and making the model appear more brittle than it actually is. Later research addressed this by searching for adversarial rules (Ribeiro et al., 2018) or by constraining the candidate perturbations to the k nearest neighbors in the embedding space (Alzantot et al., 2018; Michel et al., 2019; Ren et al., 2019; Zhang et al., 2019; Li et al., 2019; Jin et al., 2020). Zang Original Adversary Prediction P: The girl that can help me is all the way across town. H: There is no one who can help me. P: olan girl that can help me is all the way across town. H: dw§ ¯ one who can help me. Before: Contradiction After: Entailment Original Adversary Prediction P: We didn’t know where they were going. H: We didn’t know where the people were traveling to. P"
2021.naacl-main.282,2020.lrec-1.294,0,0.0590846,"Missing"
2021.naacl-main.282,2020.acl-main.263,1,0.93562,"olyglots. Although gold standard data (Bali et al., 2014; Patwa et al., 2020) is important for definitively evaluating code-mixed text processing ability, such datasets are expensive to collect and annotate. The 2 Examples of real code-mixing in Appendix A. dizzying range of potential language combinations further compounds the immensity of such an effort. We posit that performance on appropriately crafted adversaries could act as a lower bound of a model’s ability to generalize to the distribution simulated by said adversaries, an idea akin to worstcase analysis (Divekar, 1984). For example, Tan et al. (2020b) showed that an NLP system that was robust to morphological adversaries was less perplexed by dialectal text exhibiting morphological variation. Likewise, if a system is robust to codemixed adversaries constructed from some set of languages, it is reasonable to expect it to also perform better on real code-mixed text in those languages. While they may not fully model the intricacies of real code-mixing (Sridhar and Sridhar, 1980), we believe that they can be useful in the absence of appropriate evaluation data. Hence, we: • Propose two strong black-box adversarial attacks targeting the cross"
2021.naacl-main.282,2020.emnlp-main.455,1,0.916823,"olyglots. Although gold standard data (Bali et al., 2014; Patwa et al., 2020) is important for definitively evaluating code-mixed text processing ability, such datasets are expensive to collect and annotate. The 2 Examples of real code-mixing in Appendix A. dizzying range of potential language combinations further compounds the immensity of such an effort. We posit that performance on appropriately crafted adversaries could act as a lower bound of a model’s ability to generalize to the distribution simulated by said adversaries, an idea akin to worstcase analysis (Divekar, 1984). For example, Tan et al. (2020b) showed that an NLP system that was robust to morphological adversaries was less perplexed by dialectal text exhibiting morphological variation. Likewise, if a system is robust to codemixed adversaries constructed from some set of languages, it is reasonable to expect it to also perform better on real code-mixed text in those languages. While they may not fully model the intricacies of real code-mixing (Sridhar and Sridhar, 1980), we believe that they can be useful in the absence of appropriate evaluation data. Hence, we: • Propose two strong black-box adversarial attacks targeting the cross"
2021.naacl-main.282,2020.eamt-1.61,0,0.0357577,"Missing"
2021.naacl-main.282,D19-1221,0,0.0293555,"two or three aircraft arrive in a week and I didn’t know where they’re flying to. H: 从来没有 aircraft arriving. Before: Contradiction After: Entailment Table 1: B UMBLEBEE adversaries found for XLM-R on XNLI (P: Premise; H: Hypothesis). et al. (2020) take another approach by making use of a annotated sememes to disambiguate polysemous words, while Tan et al. (2020a) perturb only the words’ morphology and encourage semantic preservation via a part-of-speech constraint. Other approaches make use of language models to generate candidate perturbations (Garg and Ramakrishnan, 2020; Han et al., 2020). Wallace et al. (2019) find phrases that act as universally adversarial perturbations when prepended to clean inputs. Zhang et al. (2020) provide a comprehensive survey. rules, from different languages in a single sentence. This is distinguished from code-switching, which occurs at the inter-sentential level (Kachru, 1978). Extreme code-mixing. Inspired by the proliferation of real-life code-mixing and polyglots, we propose P OLY G LOSS and B UMBLEBEE, two multilingual adversarial attacks that adopt the persona of an adversarial code-mixer. We focus on the lexical component of code-mixing, where some words in a sen"
2021.naacl-main.282,N18-1101,0,0.0496303,": {contradiction, neutral, entailment}. We construct two more datasets from XNLI: XNLI-13 and XNLI-32. XNLI-13 comprises all XNLI languages except Swahili and Urdu due to the lack of suitable dictionaries for P OLY G LOSS. We then translate the English test set into eighteen other languages with MT systems to form XNLI31, increasing the number of embedded languages P OLY G LOSS can draw from. XQuAD is a multilingual dataset for extractive question answering (QA) with parallel translations in eleven languages. In the cross-lingual transfer setting, the models are trained on English data, MNLI (Williams et al., 2018) and SQuAD 1.1 (Rajpurkar et al., 2016), and tested on mulitlingual data, XNLI and XQuAD, respectively. We perturb the premise and hypothesis for NLI and only the question for QA. More experimental details can be found in Appendix D. Matrix language. Although our attacks work with any language as the matrix language, we use English as the matrix language in our experiments Model XLM-Rlarge XLM-Rbase mBERTbase Clean B UMBLEBEE 75.64 / 61.39 68.90 / 53.50 64.66 / 49.47 35.32 / 22.52 17.95 / 10.33 20.66 / 11.68 Table 4: B UMBLEBEE results on XQuAD (F1 /EM). due to the availability of English→T tr"
2021.naacl-main.282,K19-1026,0,0.153573,"4; Solorio et al., 2014; Barman et al., 2014), before progressing to named entity recognition and part-of-speech tagging (Ball and Garrette, 2018; AlGhamdi and Diab, 2019; Aguilar and Solorio, 2020). Recent work explores code-mixing in higher-level tasks such as question answering and task-oriented dialogue (Chandu et al., 2019; Ahn et al., 2020). Muller et al. (2020) demonstrate mBERT’s ability to transfer to an unseen dialect by exploiting its speakers’ tendency to code-mix. A key challenge of developing models that are robust to code-mixing is the availability of codemixed datasets. Hence, Winata et al. (2019) use a pointer-generator network to generate synthetically code-mixed sentences while Pratapa et al. (2018) explore the use of parse trees for the same purpose. Yang et al. (2020) propose to improve machine translation with “code-switching pretraining”, replacing words with their translations in a similar manner to masked language modeling (Devlin et al., 2019). These word pairs are constructed from monolingual corpora using cosine similarity. Sitaram et al. (2019) provide a comprehensive survey of code-mixed language processing. Word-level adversaries. Modified inputs aimed at disrupting a mo"
2021.naacl-main.282,D19-1077,0,0.032644,"rds across sentences (b) Extracted candidate perturbations (c) Final multilingual adversary Figure 1: B UMBLEBEE’s three key stages of adversary generation: (a) Align words in the matrix (English) and embedded sentences (top: Indonesian, bottom: Chinese); (b) Extract candidate perturbations from embedded sentences; (c) Construct final adversary by maximizing the target model’s loss. Introduction The past year has seen incredible breakthroughs in cross-lingual generalization with the advent of massive multilingual models that aim to learn universal language representations (Pires et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020b). These models have demonstrated impressive cross-lingual transfer abilities: simply fine-tuning them on task data from a high resource language such as English after pretraining on monolingual corpora was sufficient to manifest such abilities. This was observed even for languages with different scripts and no vocabulary overlap (K et al., 2020). However, transferring from one language to another is insufficient for NLP systems to understand multilingual speakers in an increasingly multilingual world (Aronin and Singleton, 2008). In many 1 Code: github.com/salesforce/ad"
2021.naacl-main.57,N19-1071,0,0.0212821,"etworks, small finetune-able layers that aim to reproduce characteristics of the target dataset as seen in a small set of labeled examples. In contrast, we aim to encode the characteristics of our target dataset, such as level of extraction and compression, a priori in the intermediate training phase. In other work, Lebanoff et al. (2018) adapt a single-document summarization model to multi-document settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was"
2021.naacl-main.57,D18-1045,0,0.0306921,"f that given target dataset. Unless otherwise stated, all results reported are ROUGE1/2/L. We run all few-shot transfer experiments on five subsets of supervised data, and the reported numbers, unless zero-shot, are the average of the top three results of the five runs following previous work (Gunel et al., 2020). The 10 data point sets are subsets of the 100 data point sets. Data Augmentation Parameters: For data augmentation via round-trip translation, we use a beam size of 10 and k of 10 on German and Russian translation models; fairseq provides bidirectional pretrained translation models (Edunov et al., 2018) from WMT19 (Ng et al., 2019) for these language pairs. For both 10 and 100 data points, this resulted in 2010 and 20100 total data points. For consistency loss, we use the same augmented data. Model Hyperparameters: We use the fairseq codebase (Ott et al., 2019) for our experiments. Our base abstractive text summarization model is BART-large (Lewis et al., 2020), a pretrained denoising autoencoder with 336M parameters that builds off of the sequence-to-sequence transformer 4 Experimental Settings of Vaswani et al. (2017). We fine-tune BART usDatasets: We experiment with four datasets, CN- ing"
2021.naacl-main.57,2020.acl-main.703,1,0.88821,"ine-tuning an already-pretrained model specifically for summarization on a downstream dataset 2 Related Work by leveraging a generic text corpus (Wikipedia) While advances have been made in neural tech- to create auxiliary fine-tuning data that transfers niques for summarization due in part to large across domains, allowing for more fine-grained datasets, less work has focused on domain adap- control over the transfer process. We show the tation of such methods in the zero and few-shot generalizability of such fine-tuning across domains. settings. Wang et al. (2019) examine domain adap- BART (Lewis et al., 2020) is a pretrained denoising tation, but in extractive summarization. Hua and autoencoder and achieved state-of-the-art perforWang (2017) examine domain adaptation between mance when fine-tuned on summarization tasks at opinion and news summarization, observing that the time. In this work, we use BART as our base models trained on one domain and applied to an- pretrained model but in future work will experiother domain can capture relevant content but differ ment with other pretrained models. 705 3 Methods Data Augmentation via Round-Trip Translation: In addition to fine-tuning on WikiTransfer d"
2021.naacl-main.57,W04-1013,0,0.0207062,"ning pretrained models using unsupervised Wikipedia data. We create dataset-specific unsupervised data for this intermediate fine-tuning, by making use of characteristics of the target dataset such as the average length of input documents, the average summary length, and the general bin of whether the summaries desired are very abstractive or very extractive, as discussed above. Assume that we want a summary of M sentences from source documents of N sentences on average, and that we know approximately how extractive the summaries are in the target dataset, as defined as the upper bound ROUGE (Lin, 2004) performance of an extractive model, the extractive oracle, on that dataset. We bin the level of extraction of the target summaries into extremely abstractive (ROUGE oracle 10-30), more abstractive (ROUGE oracle 20-30), more extractive (ROUGE oracle 30-50), and extremely extractive (ROUGE oracle 40-60). We then iterate the following procedure on all Wikipedia articles available in a Wikipedia dump: We remove the first M sentences from the Wikipedia article for use as a summary and the following N sentences for use as a source document. Then, we want to check whether this pseudo data point matc"
2021.naacl-main.57,W04-3252,0,0.010523,"t settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was shown for abstractive summarization in Zhang et al. (2019) and extractive summarization in Desai et al. (2020). Our work focuses on the zero-shot abstractive summarization setting and the transferability of models fine-tuned on taskspecific data from a generic corpus, rather than just the transferability of a single pretrained model. The closest work to ours for zero-shot transfer is Yang et al. (2020)"
2021.naacl-main.57,K16-1028,0,0.0325103,"of subfunctions of the input, called subaspects, which 1 Introduction determine the output form. Jung et al. (2019) deAutomatic text summarization aims to distill the fine three subaspects for summarization: position, most salient content of a given text in a compact importance, and diversity, and study how these form. Recent advances in summarization have been subaspects manifest themselves in summarization driven by the availability of large-scale datasets corpora and model outputs. For example, a comsuch as the CNN-DailyMail (CNNDM) corpus mon subaspect for the CNNDM dataset is position; (Nallapati et al., 2016) and the New York Times earlier sentences tend to constitute a good sum704 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 704–717 June 6–11, 2021. ©2021 Association for Computational Linguistics mary. Inspired by this view of summarization as subaspects, we aim to encode subaspects of a target dataset into unlabeled data to allow a model finetuned on this data to learn characteristics of the target dataset to improve zero-shot and few-shot transfer of the model. In our work, we focus on the s"
2021.naacl-main.57,P19-1212,0,0.0365573,"Missing"
C14-1020,P02-1034,0,0.423722,"itional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using"
C14-1020,P06-2034,0,0.0263295,"derable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a"
C14-1020,J02-3001,0,0.163552,"cognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of conc"
C14-1020,D12-1083,1,0.768831,"low trees. A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example, the first clause “along my route tell me the next steak house” is elaborated by the second clause “that is within a mile”. The relations by which clauses in a text are linked are called coherence relations (e.g., Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide additional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson, 1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is. 3.3 New features In order to compare to the structured representation, we also devoted significant effort towards engineering a set of features to be used in a flat feature-vector representation; they can be used in isolation or in combination with the kernel-based a"
C14-1020,P06-1115,0,0.033166,"eranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a local model, where the hyp"
C14-1020,H05-1064,0,0.0355336,"ated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly bas"
C14-1020,P05-1024,0,0.0308755,"o be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures f"
C14-1020,J08-2001,1,0.787599,"Missing"
C14-1020,H94-1053,0,0.881717,"{price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be fo"
C14-1020,W06-2909,1,0.869136,"of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-gr"
C14-1020,H91-1020,0,0.794759,"Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 193 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 193–202, Dublin, Ireland, August 23-29 2014. Finally, a database query is formed from the list of labels and values, and is then executed against the database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed. {$and [{cuisine:&quot;lebanese&quot;}, {city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied"
C14-1020,H89-1026,0,0.078377,"city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of"
C16-1299,N16-3003,1,0.814437,"IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM models were trained using the NPLM4 toolkit (Vaswani et al., 2013) with the following settings: a target context of 5 words and an aligned source window of 9 words. We restricted source and target side vocabularies to the 20K and 40K most frequent words in the in-domain data.5 The word vector size D and the hidden layer size were set to 150 and 750, respectively. Training was done using SGD with NCE using 100 noise samples and a mini-batch size o"
C16-1299,D11-1033,0,0.316495,"ata. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswan"
C16-1299,2014.iwslt-evaluation.6,1,0.849612,"ne test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concat"
C16-1299,2011.iwslt-evaluation.18,0,0.32717,". (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform these. Our approach is complementary and the gains obtained were found to be additive on top of phrase-table adaptation and MML-based data-selection (Axelrod et al., 2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine t"
C16-1299,P13-1141,0,0.0183407,"r combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models were combined with existing methods. Although t"
C16-1299,P13-1126,0,0.0143517,"complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The mod"
C16-1299,N12-1047,0,0.0163528,"ions training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried th"
C16-1299,P14-1129,0,0.551242,"n to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer to the word embedding layer of each model. The intuition behind learning the models separately, is to learn in-domain model parameters without contaminating them with the out-domain data. In a variant of our model, we restrict backpropagation to only the outermost hidden la"
C16-1299,P13-2119,0,0.55228,"set from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readju"
C16-1299,P13-2071,1,0.844807,"ented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by inte"
C16-1299,E14-4029,1,0.842113,"tences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained independently and adjusted towards in-domain data or by interpolating them linearly (EM-weighting) or log-linearly. We also tried the approach of Luong and Manning (2015) by Fine Tuning baseline model towards in-domain data (i.e., by trainin"
C16-1299,2015.mtsummit-papers.10,1,0.787326,"n data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters thr"
C16-1299,N13-1073,0,0.0253079,"1 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we eit"
C16-1299,P12-2023,0,0.0239499,". Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We"
C16-1299,eisele-chen-2010-multiun,0,0.0113535,"fusion and linear interpolation have the same number of parameters, which is the sum of the size of the base models. In fusion, we readjust all the parameters of the base models (or just the output layer weights in fusion-II), where in linear interpolation, we only learn their mixing weight. 4 Experiments Data: We experimented with the data made available for the translation task of the International Workshop on Spoken Language Translation IWSLT (Cettolo et al., 2014). We used TED talks as our in-domain (≈ 177K sentences) corpus. For Arabic-to-English, we used the multiUN (≈ 3.7M sentences) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used data made available (≈ 4.4M sentences) for the 9th Workshop on Machine Translation3 as our out-domain data. Language models were trained on all the available monolingual data (English: ≈ 287.3M and German: ≈ 59.5M sentences). Machine translation systems were tuned on concatenation of the dev- and test2010 and evaluated on test2011-2013 datasets. We used Farasa (Abdelali et al., 2016) to tokenize Arabic and the default Moses tokenizer for English-and German. All data was truecased. See Table 1 for data sizes. NN Training: The NNJM model"
C16-1299,W08-0334,0,0.0309179,"uence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al."
C16-1299,W07-0717,0,0.2591,"´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this"
C16-1299,W09-0439,0,0.0335122,"less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion"
C16-1299,D10-1044,0,0.0854332,"ame. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation"
C16-1299,D08-1089,0,0.0282,"lish training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or with our fusion models (NFM*), where models are trained in"
C16-1299,2013.iwslt-papers.2,1,0.90169,"Missing"
C16-1299,E14-1035,0,0.0175463,"e carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improveme"
C16-1299,W11-2123,0,0.00997717,"Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the data. We included NNJM model trained on a plain concatenation of the data as a feature in our baseline system. In the adapted systems, we either replaced it with the NDAM models trained on weighted concatenation, or w"
C16-1299,2005.eamt-1.19,0,0.0559647,".3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. Data selection has shown to be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015) performed data selection using operation sequence model and NNJM models. An alternative to completely filtering out less useful data is to m"
C16-1299,C14-1182,0,0.336565,"Missing"
C16-1299,D15-1147,1,0.338038,"ukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propose a deep fusion approach to domain adaptation for SMT. We use the Neural Network Joint Model (NNJM) proposed by Devlin et al. (2014) as our base model. However, rather than training the model on a plain concatenation of in- and out-domain data or a weighted concatenation (Joty et al., 2015), we first train in- and out-domain NNJM models, and then learn a composite model by readjusting their parameters through backpropagating errors from the output layer"
C16-1299,P07-2045,0,0.0037253,"est Set Sent. TokEN TokDE Corpus Sent. TokAR TokEN tune test-11 test-12 test-13 2437 1433 1700 993 51K 4K 28K 18K 48K 23K 26K 17K tune test-11 test-12 test-13 2456 1199 1702 1169 48K 21K 30K 26K 52K 24K 32K 28K Table 1: Statistics of the English-German and Arabic-English training corpora in terms of Sentences and Tokens (represented in millions). ep = Europarl, cc = Common Crawl, un = United Nations training to a subset of 1 million sentences containing all the in-domain data and a random selection of sentences from the out-domain data. Machine Translation Settings: We trained a Moses system (Koehn et al., 2007), with the settings described in (Birch et al., 2014): a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2013) and other defaults. We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. Arabic OOVs were transliterated using unsupervised transliteration module (Durrani et al., 2014) in Moses. Baselines: Baseline MT systems were trained by simply concatenating all the d"
C16-1299,W04-3250,0,0.0940932,"tion: In Table 4 we experiment with MML-based filtering and probe whether our model can also improve on top of data selection. Firstly, selecting no out-domain data degrades the English-to-German system. On the contrary, the Arabic-to-English system substantially improves. This shows that general domain data is useful for English-to-German and much of the outdomain data (UN corpus) used in these experiments is harmful in the case of Arabic-to-English. In comparison, data selection was found to be less useful in the case of English-to-German. But we found 9 p < 0.05 using bootstrap resampling (Koehn, 2004), with 1000 samples. 3183 English-to-German Arabic-to-English System tst11 tst12 tst13 Avg tst11 tst12 tst13 Avg Baselinecat BaselineID 27.3 26.7 22.9 22.5 24.5 23.6 24.9 24.3 26.1 27.2 29.4 30.0 30.5 30.2 28.7 29.1 MML +NFM-I 26.9 27.6 22.9 23.1 24.4 25.0 24.7 25.2 27.4 27.6 30.8 31.2 30.9 31.1 29.7 30.0 Table 4: Comparing with MML (Axelrod et al 2011) that using our fusion model instead of baseline NNJM in either system still gave improvements ( +0.5 and +0.3 in English-German and Arabic-English, respectively). 5 Related Work Previous work on domain adaptation in MT can be broken down broadl"
C16-1299,P14-2093,0,0.0440254,"Missing"
C16-1299,2015.iwslt-evaluation.11,0,0.400393,"ails: http://creativecommons.org/licenses/by/4.0/ 3177 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3177–3187, Osaka, Japan, December 11-17 2016. We evaluated our model on a standard task of translating TED talks for English-to-German and Arabicto-English language pairs. Compared to baseline NNJM models trained on a concatenation of in- and out-domain data, our model achieves an average improvement of up to 0.9 BLEU points. The most relevant to our work are the NDAM model of Joty et al. (2015) and the fine-tuning method of Luong and Manning (2015). The NDAM model uses data dependent regularization in the NNJM model to weight training instances, while training the model on the concatenated data. The fine-tuning method first trains the NNJM on the concatenated data, then runs a few additional epochs on the in-domain data to tune the model towards in-domain. We found our method to outperform both the NDAM and finetuning methods. We also carried experiments against phrase-table weighting methods such as instance weighting (Sennrich, 2012), and phrase-table fill-up combination (Bisazza et al., 2011) and found our approach to outperform thes"
C16-1299,D15-1166,0,0.0656008,"ibes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T an"
C16-1299,N13-1074,0,0.0203016,"takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et"
C16-1299,C14-1105,0,0.0141523,"Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were obtained when our models"
C16-1299,D09-1074,0,0.478744,"anslation tasks such as translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015"
C16-1299,P10-2041,0,0.291581,"translating TED talks (Cettolo et al., 2014), patents (Fujii et al., 2010) and educational content (Guzm´an et al., 2013). This is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based a"
C16-1299,D09-1141,0,0.0170088,"els. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Our work falls in this line of research. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense dis"
C16-1299,2013.iwslt-evaluation.8,1,0.909079,"Missing"
C16-1299,P13-1082,0,0.0755351,"nterpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Chen et al. (2013) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). 6 Conclusion and Future Work We presented a deep fusion model based on the neural network joint model (NNJM) of Devlin et al. (2014). The model is learned by fusing in- and out-domain NNJM models into a composite model by adjusting their parameters in favor of the in-domain data. When used as a feature during decoding, our model obtains statistically significant improvements on top of a competition grade phrase-based baseline system. We also showed improvements compared to previous adaptation methods. Further gains were ob"
C16-1299,P16-1009,0,0.0306581,"describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word source window for a target word ti based on the one-to-one alignment between T and S. This is essentially an (m+n)-gram bilingua"
C16-1299,E12-1055,0,0.546508,"is because of the difference in stylistic variations, vocabulary choices and word sense ambiguities across genres. Domain adaptation aims at finding the optimal point, that maximizes on the useful information available in the out-domain data, in favor of the in-domain data, while preventing it from degrading the performance of the system. This is either done by selecting a subset from the out-domain data, which is closer to the in-domain data (Matsoukas et al., 2009; Moore and Lewis, 2010), or by re-weighting the probability distribution in favor of the in-domain data (Foster and Kuhn, 2007; Sennrich, 2012). Recently, there has been a growing interest in deep neural networks (DNNs) and word embeddings with application to numerous NLP problems. The ability to generalize and to better capture non-local dependencies gives edge to the neural models over their traditional counter-part. Several researchers have also attempted to employ DNNs for domain adaptation in SMT. Duh et al. (2013) and Durrani et al. (2015) used DNNs for data selection. Joty et al. (2015) proposed a DNN-based adaptation model for SMT that regularizes the loss function with respect to the in-domain model. In this paper, we propos"
C16-1299,D13-1140,0,0.208513,"2011). The remainder of this paper is organized as follows. Section 2 briefly describes neural network joint model. Section 3 describes our fusion model for domain adaptation. Section 4 presents results and analysis. Section 5 gives an account on the related work and Section 6 concludes the paper. 2 Neural Network Joint Model Neural models are quickly becoming the state-of-the-art in machine translation. The ability to generalize and better capture non-local dependencies gives them edge over traditional models. The two most prevalent approaches are to use NNs as a feature inside SMT decoder (Vaswani et al., 2013; Devlin et al., 2014), or as an end-to-end translation system (Luong et al., 2015; Bahdanau et al., 2015; Sennrich et al., 2016) designed as fully trainable model of which every component is tuned based on training corpora to maximize its translation performance. Our work falls in the former category and extends NNJM. The NNJM model learns a feed-forward neural network from augmented streams of source and target sequences. For a bilingual sentence pair (S, T ), NNJM defines a conditional probability distribution: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−n+1 , si ) (1) i=1 where, si is an m-word"
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
D08-1032,C04-1064,0,0.071909,"Missing"
D08-1032,kingsbury-palmer-2002-treebank,0,0.0494762,".5.3 Shallow-semantic Feature Though introducing BE and syntactic information gives an improvement on BOW by the use of dependency/syntactic parses, but these, too are not adequate when dealing with complex questions whose answers are expressed by long and articulated sentences or even 5 BE website:http://www.isi.edu/ cyl/BE 308 Figure 1: Example of semantic trees paragraphs. Shallow semantic representations, bearing a more compact information, could prevent the sparseness of deep structural approaches and the weakness of BOW models (Moschitti et al., 2007). Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003). For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like"
D08-1032,W04-1013,0,0.0834185,"LCS, WLCS and Skip-Bigram A sequence W = [w1 , w2 , ..., wn ] is a subsequence of another sequence X = [x1 , x2 , ..., xm ], if there exists a strict increasing sequence [i1 , i2 , ..., ik ] of indices of X such that for all j = 1, 2, ..., k we have xij = wj . Given two sequences, S1 and S2 , the Longest Common Subsequence (LCS) of S1 and S2 is a common subsequence with maximum length. The longer the LCS of two sentences is, the more similar the two sentences are. The basic LCS has a problem that it does not differentiate LCSes of different spatial relations within their embedding sequences (Lin, 2004). To improve the basic LCS method, we can remember the length of consecutive matches encountered so far to a regular two dimensional dynamic program table computing LCS. We call this weighted LCS (WLCS) and use k to indicate the length of the current consecutive matches ending at words xi and yj . Given two sentences X and Y, the WLCS score of X and Y can be computed using the similar dynamic programming procedure as stated in (Lin, 2004). We computed the LCS and WLCS-based F-measure following (Lin, 2004) using both the query pool and the sentence pool as in the previous section. Skip-bigram i"
D08-1032,N06-1006,0,0.0612483,"Missing"
D08-1032,H05-1115,0,0.202799,"n 3 describes how the features are extracted, Section 4 discusses the scoring approaches, Section 5 discusses how we remove the redundant sentences before adding them to the summary, Section 6 describes our experimental study. We conclude and give future directions in Section 7. 2 Related Work Researchers all over the world working on querybased summarization are trying different directions to see which methods provide the best results. The LexRank method addressed in (Erkan and Radev, 2004) was very successful in generic multi-document summarization. A topic-sensitive LexRank is proposed in (Otterbacher et al., 2005). As in LexRank, the set of sentences in a document cluster is represented as a graph, where nodes are sentences and links between the nodes are induced by a similarity relation between the sentences. Then the system ranked the sentences according to a random walk model defined in terms of both the intersentence similarities and the similarities of the sentences to the topic description or question. The summarization methods based on lexical chain first extract the nouns, compound nouns and named entities as candidate words (Li et al., 2007). Then using WordNet, the systems find the semantic s"
D08-1032,P07-1098,0,\N,Missing
D08-1032,N04-1030,0,\N,Missing
D10-1038,N09-1042,0,0.0127496,"hey achieve similar results as (Galley et al., 2003), with the supervised approach outperforming LCSeg. However, for the subtopic level, LCSeg performs significantly better than the supervised one. In our work, we show how LCSeg performs when applied to the temporal ordering of the emails in a thread. We also propose its extension to leverage the finer conversation structure of emails. The probabilistic generative topic models, such as LDA and its variants (e.g., (Blei et al., 2003), (Steyvers and Griffiths, 2007)), have proven to be successful for topic segmentation in both monologue (e.g., (Chen et al., 2009)) and dialog (e.g., (Georgescul et al., 2008)). (Purver et al., 2006) uses a variant of LDA for the tasks of segmenting meeting transcripts and extracting the associated topic labels. However, their approach for segmentation does not perform better than LCSeg. In our work, we show how the general LDA performs when applied to email conversations and describe how it can be extended to exploit the conversation structure of emails. Several approaches have been proposed to capture an email conversation . Email programs (e.g., Gmail, Yahoomail) group emails into threads using headers. However, our a"
D10-1038,W01-0514,0,0.0348819,"2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when they are applied to e"
D10-1038,A00-2004,0,0.129285,"Missing"
D10-1038,P08-1095,0,0.36039,"renini and Gabriel Murray and Raymond T. Ng {rjoty, carenini, gabrielm, rng}@cs.ubc.ca Department of Computer Science University of British Columbia Vancouver, BC, V6T 1Z4, Canada Abstract express their opinions. For example, in the email thread shown in Figure 1, according to the majority of our annotators, participants discuss three topics (e.g., ‘telecon cancellation’, ‘TAG document’, and ‘responding to I18N’). Multiple topics seem to occur naturally in social interactions, whether synchronous (e.g., chats, meetings) or asynchronous (e.g., emails, blogs) conversations. In multi-party chat (Elsner and Charniak, 2008) report an average of 2.75 discussions active at a time. In our email corpus, we found an average of 2.5 topics per thread. This work concerns automatic topic segmentation of email conversations. We present a corpus of email threads manually annotated with topics, and evaluate annotator reliability. To our knowledge, this is the first such email corpus. We show how the existing topic segmentation models (i.e., Lexical Chain Segmenter (LCSeg) and Latent Dirichlet Allocation (LDA)) which are solely based on lexical information, can be applied to emails. By pointing out where these methods fail a"
D10-1038,P03-1071,0,0.663951,"LDA for segmenting an email thread into topical clusters and incorporating conversation structure into these models improves the performance significantly. 1 Introduction With the ever increasing popularity of emails and web technologies, it is very common for people to discuss issues, events, agendas or tasks by email. Effective processing of the email contents can be of great strategic value. In this paper, we study the problem of topic segmentation for emails, i.e., grouping the sentences of an email thread into a set of coherent topical clusters. Adapting the standard definition of topic (Galley et al., 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi e"
D10-1038,I08-2133,0,0.094015,"et al., 2003), with the supervised approach outperforming LCSeg. However, for the subtopic level, LCSeg performs significantly better than the supervised one. In our work, we show how LCSeg performs when applied to the temporal ordering of the emails in a thread. We also propose its extension to leverage the finer conversation structure of emails. The probabilistic generative topic models, such as LDA and its variants (e.g., (Blei et al., 2003), (Steyvers and Griffiths, 2007)), have proven to be successful for topic segmentation in both monologue (e.g., (Chen et al., 2009)) and dialog (e.g., (Georgescul et al., 2008)). (Purver et al., 2006) uses a variant of LDA for the tasks of segmenting meeting transcripts and extracting the associated topic labels. However, their approach for segmentation does not perform better than LCSeg. In our work, we show how the general LDA performs when applied to email conversations and describe how it can be extended to exploit the conversation structure of emails. Several approaches have been proposed to capture an email conversation . Email programs (e.g., Gmail, Yahoomail) group emails into threads using headers. However, our annotations show that topics change at a finer"
D10-1038,J97-1003,0,0.315662,"ing to two measures: ‘number of words in the chain’ and ‘compactness of the chain’. The more compact (in terms of number of sentences) and the more populated chains get higher scores. The algorithm then works with two adjacent analysis windows, each of a fixed size k which is empirically determined. For each sentence boundary, LCSeg computes the cosine similarity (or lexical cohesion function) at the transition between the two windows. Low similarity indicates low lexical cohesion, and a sharp change signals a high probability of an actual topic boundary. This method is similar to TextTiling (Hearst, 1997) except that the similarity is computed based on the scores of the ‘lexical chains’ instead of ‘term counts’. In order to apply LCSeg on email threads we arrange the emails based on their temporal relation (i.e., arrival time) and apply the LCSeg algorithm to get the topic boundaries. 9 One can also consider other lexical semantic relations (e.g., synonym, hypernym, hyponym) in lexical chaining. However, Galley et al., (Galley et al., 2003) uses only repetition relation as previous research results (e.g., (Choi, 2000)) account only for repetition. From: Brian To: rdf core Subject: 20030220 tel"
D10-1038,E06-1035,0,0.309155,"h the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when they are applied to email conversations. Our contributions in this paper aim to remedy 388 Procee"
D10-1038,P06-1004,0,0.41829,"inition of topic (Galley et al., 2003) to conversations/emails, we consider a topic is something about which the participant(s) discuss or argue or Topic segmentation is often considered a prerequisite for other higher-level conversation analysis and applications of the extracted structure are broad, encompassing: summarization (Harabagiu and Lacatusu, 2005), information extraction and ordering (Allan, 2002), information retrieval (Dias et al., 2007), and intelligent user interfaces (Dredze et al., 2008). While extensive research has been conducted in topic segmentation for monologues (e.g., (Malioutov and Barzilay, 2006), (Choi et al., 2001)) and synchronous dialogs (e.g., (Galley et al., 2003), (Hsueh et al., 2006)), none has studied the problem of segmenting asynchronous multi-party conversations (e.g., email). Therefore, there is no reliable annotation scheme, no standard corpus, and no agreedupon metrics available. Also, it is our key hypothesis that, because of its asynchronous nature, and the use of quotation (Crystal, 2001), topics in an email thread often do not change in a sequential way. As a result, we do not expect models which have proved successful in monologue or dialog to be as effective when"
D10-1038,P06-1003,0,0.0691643,"Missing"
D12-1083,P05-1022,0,0.0184229,"sible nuclearity statuses. (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we s"
D12-1083,A00-2018,0,0.0951801,"du/page/software 910 Not all relations take all the possible nuclearity statuses. (Subba and Eugenio, 2009) and also treat the reversals of non-commutative relations as separate relations. That is, PREPARATION-ACT and ACTPREPARATION are two different relations. Attaching the nuclearity statuses to these relations gives 70 distinct relations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies"
D12-1083,J03-4003,0,0.0271471,"structional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labeled (i.e., nuclear"
D12-1083,P07-1033,0,0.0448921,"sting to observe how much our full system is affected by an automatic segmenter on both RST-DT and the Instructional corpus (see Table 2 and Table 5). Nevertheless, taking into account the segmentation results in Table 4, this is not surprising because previous studies (Soricut and Marcu, 2003) have already shown that automatic segmentation is the primary impediment to high accuracy discourse parsing. This demonstrates the need for a more accurate segmentation model in the Instructional genre. A promising future direction would be to apply effective domain adaptation methods (e.g., easyadapt (Daume, 2007)) to improve the segmentation performance in the Instructional domain by leveraging the rich data in RST-DT. 5.6 Error Analysis and Discussion The results in Table 2 suggest that given a manually segmented discourse, our sentence-level discourse parser finds the unlabeled (i.e., span) discourse tree and assigns the nuclearity statuses to the spans at a performance level close to human annotators. We, therefore, look more closely into the performance of our parser on the hardest task of relation labeling. Figure 6 shows the confusion matrix for the relation labeling task using manual segmentati"
D12-1083,P09-1075,0,0.722715,"s are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing model, expressed as a Dynamic Conditional Random Field (DCR"
D12-1083,P08-1109,0,0.0649235,"Missing"
D12-1083,P07-1062,0,0.905371,"an EDU boundary after a token wk , we find the lowest constituent in the lexicalized syntactic tree that spans over tokens wi . . . wj such that i≤k&lt;j. The production that expands this constituent in the tree and its different variations, form the feature set. For example in Figure 5, the production NP(efforts)→PRP$(its)NNS(efforts)↑S(to) and its different variations depending on whether they include the lexical heads and how many nonterminals (up to two) to consider before and after the potential EDU boundary (↑), are used to determine the existence of a boundary after the word efforts (see (Fisher and Roark, 2007) for details). SPADE uses these features in a generative way, meaning that, it inserts an EDU boundary if the relative frequency (i.e., Maximum Likelihood Estimate (MLE)) of a potential boundary given the production in the training corpus is greater than 0.5. If the production has not been observed frequently enough, it uses its other variations to perform further smoothing. In contrast, we compute the MLE estimates for a production and its other variations, and use those as features with/without binarizing the values. Shallow syntactic parse (or Chunk) and POS tags have been shown to possess"
D12-1083,I11-1120,0,0.0322917,"Missing"
D12-1083,P95-1037,0,0.216158,"ations in the Instructional corpus. We use SPADE as our baseline model and apply the same modifications to its default setting as described in (Fisher and Roark, 2007), which delivers improved performance. Specifically, in testing, we replace the Charniak parser (Charniak, 2000) with a more accurate reranking parser (Charniak and Johnson, 2005). We use the reranking parser in all our models to generate the syntactic trees. This parser was trained on the sections of the Penn Treebank not included in the test set. For a fair comparison, we apply the same canonical lexical head projection rules (Magerman, 1995; Collins, 2003) to lexicalize the syntactic trees as done in SPADE and HILDA. Note that, all the previous works described in Section 2, report their models’ performance on a particular test set of a specific corpus. To compare our results with the previous studies, we test our models on those specific test sets. In addition, we show more general performance based on 10-fold cross validation. 5.3 Parsing based on Manual Segmentation First, we present the results of our discourse parser based on manual segmentation. The parsing performance is assessed using the unlabeled (i.e., span) and labele"
D12-1083,J00-3005,0,0.853402,"evel discourse analysis. Our framework comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Figure 1: Discourse structure of a sentence in RST-DT. Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursi"
D12-1083,prasad-etal-2008-penn,0,0.0610095,"Missing"
D12-1083,N03-1028,0,0.115076,"Missing"
D12-1083,N03-1030,0,0.514526,"ts (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span linked by a rhetorical relation can be either a NUCLEUS or a SATELLITE depending on how central the message is to the author. Discourse analysis in RST involves two subtasks: (i) breaking the Previous studies on discourse analysis have been quite successful in identifying what machine learning approaches and what features are more useful for automatic discourse segmentation and parsing (Soricut and Marcu, 2003; Subba and Eugenio, 2009; duVerle and Prendinger, 2009). However, all the proposed solutions suffer from at least one of the following two key limitations: first, they make strong independence assumptions on the structure and the labels of the resulting DT, and typically model the construction of the DT and the labeling of the relations separately; second, they apply a greedy, suboptimal algorithm to build the structure of the DT. In this paper, we propose a new sentence-level discourse parser that addresses both limitations. The crucial component is a probabilistic discriminative parsing mod"
D12-1083,H05-1033,0,0.358704,"k comprises a discourse segmenter, based on a binary classifier, and a discourse parser, which applies an optimal CKY-like parsing algorithm to probabilities inferred from a Dynamic Conditional Random Field. We show on two corpora that our approach outperforms the state-of-the-art, often by a wide margin. 1 Figure 1: Discourse structure of a sentence in RST-DT. Introduction Automatic discourse analysis has been shown to be critical in several fundamental Natural Language Processing (NLP) tasks including text generation (Prasad et al., 2005), summarization (Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, posits a tree representation of a discourse, known as a Discourse Tree (DT), as exemplified by the sample DT shown in Figure 1. The leaves of a DT correspond to contiguous atomic text spans, also called Elementary Discourse Units (EDUs) (three in the example). The adjacent EDUs are connected by a rhetorical relation (e.g., ELABORATION), and the resulting larger text spans are recursively also subject to this relation linking. A span li"
D12-1083,N09-1064,0,0.621581,"Missing"
D12-1083,J05-2005,0,0.230786,"Missing"
D12-1083,J93-2004,0,\N,Missing
D14-1027,W14-3352,1,0.726645,"Missing"
D14-1027,W07-0718,0,0.384691,"Missing"
D14-1027,W11-2103,0,0.0593209,"ourse parser can be downloaded from http://alt.qcri.org/tools/ 216 In particular, let r and r0 be the references for the pairs ht1 , t2 i and ht01 , t02 i, we can redefine all the members of Eq. 1, e.g., K(t1 , t01 ) becomes K(ht1 , ri, ht01 , r0 i) = PTK(φM (t1 , r), φM (t01 , r0 )) + PTK(φM (r, t1 ), φM (r0 , t01 )), In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (τ ), which was official at WMT12. Table 1 presents the τ scores for all metric variants introduced in this paper: for the individual language pairs"
D14-1027,W05-0904,0,0.114217,"ndation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav"
D14-1027,W12-3102,0,0.167189,"Missing"
D14-1027,W12-3129,0,0.108627,".org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio"
D14-1027,W10-1750,1,0.877508,"Missing"
D14-1027,P07-1098,1,0.765065,"ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are"
D14-1027,W08-0331,0,0.225781,"y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the fe"
D14-1027,W07-0738,1,0.935253,"Missing"
D14-1027,P14-1065,1,0.878614,"Missing"
D14-1027,P02-1040,0,0.0928706,"ranslations Francisco Guzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges"
D14-1027,D12-1083,1,0.785191,"relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feat"
D14-1027,W07-0707,0,0.511133,"Missing"
D14-1027,P13-1048,1,0.81161,"L "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feature space. Consideri"
D14-1027,C14-1020,1,0.886934,"Missing"
D14-1027,D14-1050,1,0.886979,"Missing"
D14-1027,W13-3509,1,0.930152,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,P13-2125,1,0.924292,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,2006.amta-papers.25,0,0.625255,"uzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some ev"
D14-1027,W11-2113,0,0.0478224,"re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically."
D14-1027,N03-1033,0,0.00879476,"o-REL .-REL &apos;&apos;-REL to think . "" to think . "" relation propagation direction DIS:ELABORATION Bag-of-words relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0"
D14-1027,D12-1097,0,0.0991997,"elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy"
D14-1027,P06-1051,1,\N,Missing
D14-1050,N09-1003,0,0.0327503,"Missing"
D14-1050,J92-4003,0,0.375881,"reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic similarity, while also combining them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the rerankin"
D14-1050,S13-2060,0,0.012357,"a baseline, we picked the best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100."
D14-1050,W10-2802,0,0.024579,"best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100. In our experiments, we will"
D14-1050,H94-1053,0,0.260291,"e list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with respect to the gold standard"
D14-1050,D11-1096,1,0.95073,"SKS) (b) SKS with Brown Clusters Figure 1: CSL structures: standard and with Brown Clusters. Another relevant line of research are the semantic kernels, i.e., kernels that use lexical similarity between features. One of the first that applyed LSA was (Cristianini et al., 2002), whereas (Bloehdorn et al., 2006; Basili et al., 2006) used WordNet. Semantic structural kernels of the type we use in this paper were first introduced in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b). The most advanced model based on tree kernels, which we also use in this paper, is the Smoothed PTK (Croce et al., 2011). We further apply a semantic kernel (SK), namely the Smoothed Partial Tree Kernel (Croce et al., 2011), which uses the lexical similarity between the tree nodes, while computing the substructure space. This is the first time that SKs are applied to reranking hypotheses. This (i) makes the global sentence structure along with concepts available to the learning algorithm, and (ii) enables computing the similarity between lexicals in syntactic patterns that are enriched by concepts. We tested our models on the Restaurant domain. Our results show that: (i) The basic CRF parser, which uses semi-Ma"
D14-1050,W06-2909,1,0.683466,"d Brown clusters, and another one using semantic similarity among the words composing the structure. The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers. 1 {$and [{cuisine:&quot;lebanese&quot;},{city:&quot;doha&quot;}, {price:&quot;low&quot;},{amenity:&quot;carry out&quot;}]} The state-of-the-art of CSL is represented by conditional models for sequence labeling such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) trained with simple morphological and lexical features. The basic CRF model was improved by means of reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic"
D14-1050,C10-5001,1,0.846723,"ng them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word se"
D14-1050,P12-4002,1,0.857346,"vative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word sequences, often re"
D14-1050,H89-1026,0,0.0214012,"hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with resp"
D14-1050,P10-1023,0,0.0382136,"Missing"
D14-1050,J07-2002,0,0.0577582,"Missing"
D14-1050,N03-1033,0,0.00778762,"the hypothesis. 3.3 Semantic structures Tree kernels allow us to compute structural similarities between two trees; thus, we engineered a special structure for the CSL task. In order to capture the structural dependencies between the semantic tags,1 we use a basic tree (see for example Figure 1a), where the words of a sentence are tagged with their semantic tags. 4 Experiments The experiments aim at investigating the role of feature vectors, PTK, SK and BCs in reranking. We first describe the experimental setting and then we move into the analysis of the results. 2 We use the Stanford tagger (Toutanova et al., 2003). For instance, if the output sequence is Other-RatingOther-Amenity the 3-gram patterns would be: S-OtherRating, Other-Rating-Other, Rating-Other-Amenity, and Other-Amenity-E. 3 1 They are associated with the following IDs: 0-Other, 1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6Hours, 7-Location, and 8-Price. 438 semi-CRF Reranker Train 6,922 7,000 Devel. 739 3,695 Test 1,521 7,605 Total 9,182 39,782 N F1 2 87.76 5 92.63 10 95.23 100 98.72 Table 2: Oracle F1 score for N -best lists. Table 1: Number of instances and pairs used to train the semi-CRF and rerankers, respectively. 4.1 1 83"
D14-1050,N04-3012,0,0.201946,"Missing"
D14-1050,H91-1020,0,0.482894,"probability to be globally correct as estimated using local classifiers or global classifiers that only use local features. Then, a reranker, typically a meta-classifier, tries to select the best hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whethe"
D14-1050,C14-1020,1,0.793271,"Missing"
D14-1050,C10-5000,0,\N,Missing
D14-1219,P13-1048,1,0.800791,"ected by coherence relations (e.g., E LABORA TION , C AUSE ). Discourse units connected by a relation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the"
D14-1219,P05-1022,0,0.016711,"tuent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and multi-sentential discourse parsing. However, their model does not conSummary Same-Unit Elaboration On the Big Board, Crawford & Co., Atlanta, (CFD) begins trading today. Figure 9: An error made by our reranker. sider long range dependencies between DT constituents, which are encoded by our kernels. Regarding the latter, our work is surely inspired by (Collins and Duffy, 2002), which uses TK for syntactic parsing reranking or in general discriminative reranking, e.g., (Collins and Koo, 2005; Charniak and Johnson, 2005; Dinarelli et al., 2011). However, such excellent studies do not regard discourse parsing, and in absolute they achieved lower improvements than our methods. 8 Conclusions and Future Work In this paper, we have presented a discriminative approach for reranking discourse trees generated by an existing discourse parser. Our reranker uses tree kernels in SVM preference ranking framework to effectively capture the long range structural dependencies between the constituents of a discourse tree. We have shown the reranking performance for sentence-level discourse parsing using the standard tree ker"
D14-1219,P02-1034,0,0.73226,"Reranking models can make the global structural information available to the system as follows: first, a base parser produces several DT hypotheses; and then a classifier exploits the entire information in each hypothesis, e.g., the complete DT with its dependencies, for selecting the best DT. Designing features capturing such global properties is however not trivial as it requires the selection of important DT fragments. This means selecting subtree patterns from an exponential feature space. An alternative approach is to implicitly generate the whole feature space using tree kernels (TKs) (Collins and Duffy, 2002; Moschitti, 2006). In this paper, we present reranking models for discourse parsing based on Support Vector Machines (SVMs) and TKs. The latter allows us to represent structured data using the substructure space thus capturing structural dependencies between DT constituents, which is essential for effective discourse parsing. Specifically, we made the following contributions. First, we extend the 2049 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2049–2060, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistic"
D14-1219,J05-1003,0,0.0325215,"he label of a DT constituent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and multi-sentential discourse parsing. However, their model does not conSummary Same-Unit Elaboration On the Big Board, Crawford & Co., Atlanta, (CFD) begins trading today. Figure 9: An error made by our reranker. sider long range dependencies between DT constituents, which are encoded by our kernels. Regarding the latter, our work is surely inspired by (Collins and Duffy, 2002), which uses TK for syntactic parsing reranking or in general discriminative reranking, e.g., (Collins and Koo, 2005; Charniak and Johnson, 2005; Dinarelli et al., 2011). However, such excellent studies do not regard discourse parsing, and in absolute they achieved lower improvements than our methods. 8 Conclusions and Future Work In this paper, we have presented a discriminative approach for reranking discourse trees generated by an existing discourse parser. Our reranker uses tree kernels in SVM preference ranking framework to effectively capture the long range structural dependencies between the constituents of a discourse tree. We have shown the reranking performance for sentence-level discourse parsing"
D14-1219,J00-3005,0,0.27392,", similar short parenthesized texts are also used to elaborate as in Senate Majority Leader George Mitchell (D., Maine), where the text (D., Maine) (i.e., Democrat from state Maine) elaborates its preceding text. This confuses our reranker. We also found error examples where the reranker failed to distinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorpo"
D14-1219,W06-2909,1,0.803843,"ncy of the algorithm did not limit us to produce k-best parses for larger k, it was not a priority in this work. 5 Kernels for Reranking Discourse Trees In Section 3, we described D ISC TK, which essentially can be used for any classification task involving discourse trees. For example, given a DT, we can use D ISC TK to classify it as correct vs. incorrect. However, such classification is not completely aligned to our purpose, since our goal is to select the best (i.e., the most correct) DT from k candidate DTs; i.e., a ranking task. We adopt a preference reranking technique as described in (Moschitti et al., 2006; Dinarelli et al., 2011). 5.1 Preference Reranker Preference reranking (PR) uses a classifier C of pairs of hypotheses hhi , hj i, which decides if hi (i.e., a candidate DT in our case) is better than hj . We generate positive and negative examples to train the classifier using the following approach. The pairs hh1 , hi i constitute positive examples, where h1 has the highest f -score accuracy on the Relation metric (to be described in Section 6) with respect to the gold standard among the candidate hypotheses, and vice versa, hhi , h1 i are considered as negative examples. At test time, C cl"
D14-1219,C10-5001,1,0.934708,"ole fragment space. A TK function T1 and T2 is defined as: P overP T K(T1 , T2 ) = n1 ∈NT n2 ∈NT2 ∆(n1 , n2 ), 1 where NT1 and NT2 are the sets of the nodes of T1 and T2 , respectively, and ∆(n1 , n2 ) is equal to the number of common fragments rooted in the n1 and n2 nodes.2 The computation of ∆ function depends on the shape of fragments, conversely, a different ∆ determines the richness of the kernel space and thus different tree kernels. In the following, we briefly describe two existing and well-known tree kernels. Please see several tutorials on kernels (Moschitti, 2013; Moschitti, 2012; Moschitti, 2010) for more details.3 Syntactic Tree Kernels (S TK) produce fragments such that each of their nodes includes all or none of its children. Figure 2 shows a tree T and its three fragments (do not consider the single nodes) in the S TK space on the left and right of the ar2 To get a similarity score between 0 and 1, it is common to apply a normalization in the kernel space, T K(T1 ,T2 ) i.e. √ . 3 T K(T1 ,T1 )×T K(T2 ,T2 ) c g b e b g c e a g b c a e a b b c e e Figure 3: A tree with its P TK fragments. node. The maximum out-degree of a tree is the highest index of all the nodes of the scendants (s"
D14-1219,P12-1007,0,0.204028,"lation are further distinguished depending on their relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the text spans from e3 to e6 (linked by C AUSE and E LAB ORATION ) compose the statement to be attribu"
D14-1219,P12-4002,1,0.901157,"onsidering the whole fragment space. A TK function T1 and T2 is defined as: P overP T K(T1 , T2 ) = n1 ∈NT n2 ∈NT2 ∆(n1 , n2 ), 1 where NT1 and NT2 are the sets of the nodes of T1 and T2 , respectively, and ∆(n1 , n2 ) is equal to the number of common fragments rooted in the n1 and n2 nodes.2 The computation of ∆ function depends on the shape of fragments, conversely, a different ∆ determines the richness of the kernel space and thus different tree kernels. In the following, we briefly describe two existing and well-known tree kernels. Please see several tutorials on kernels (Moschitti, 2013; Moschitti, 2012; Moschitti, 2010) for more details.3 Syntactic Tree Kernels (S TK) produce fragments such that each of their nodes includes all or none of its children. Figure 2 shows a tree T and its three fragments (do not consider the single nodes) in the S TK space on the left and right of the ar2 To get a similarity score between 0 and 1, it is common to apply a normalization in the kernel space, T K(T1 ,T2 ) i.e. √ . 3 T K(T1 ,T1 )×T K(T2 ,T2 ) c g b e b g c e a g b c a e a b b c e e Figure 3: A tree with its P TK fragments. node. The maximum out-degree of a tree is the highest index of all the nodes o"
D14-1219,W05-1506,0,0.0347966,"eously. More specifically, each cell in the dynamic programming tables (i.e., A, B and C) should now contain k entries (sorted by their probabilities), and for each such entry there should be a back-pointer that keeps track of the decoding path. The algorithm works in polynomial time. For n discourse units and M number of relations, the 1-best parsing algorithm has a time complexity of O(n3 M ) and a space complexity of O(n2 ), where the k-best version has a time and space complexities of O(n3 M k 2 log k) and O(n2 k), respectively. There are cleverer ways to reduce the complexity (e.g., see (Huang and Chiang, 2005) for three such ways). However, since the efficiency of the algorithm did not limit us to produce k-best parses for larger k, it was not a priority in this work. 5 Kernels for Reranking Discourse Trees In Section 3, we described D ISC TK, which essentially can be used for any classification task involving discourse trees. For example, given a DT, we can use D ISC TK to classify it as correct vs. incorrect. However, such classification is not completely aligned to our purpose, since our goal is to select the best (i.e., the most correct) DT from k candidate DTs; i.e., a ranking task. We adopt a"
D14-1219,D09-1012,1,0.758801,"des connecting two EDUs (i.e., SPANs in Figure 4), number of nodes connecting two relational nodes, number of nodes connecting a relational node and an EDU, number of nodes that connects a relational node as left child and an EDU as right child, and vice versa. Relation Features. We encode the relations in the DT as bag-of-relations (i.e., frequency count). This will allow us to assess the impact of a flat representation of the DT. Note that more important relational features would be the subtree patterns extracted from the DT. However, they are already generated by TKs in a simpler way. See (Pighin and Moschitti, 2009; Pighin and Moschitti, 2010) for a way to extract the most relevant features from a model learned in the kernel space. 6 Experiments Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art. To achieve this, we (i) compute the oracle accuracy of the kbest parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking performance using the best kernel for different number of hypotheses, and (iv) show the relative importance of features"
D14-1219,W10-2926,1,0.890972,"., SPANs in Figure 4), number of nodes connecting two relational nodes, number of nodes connecting a relational node and an EDU, number of nodes that connects a relational node as left child and an EDU as right child, and vice versa. Relation Features. We encode the relations in the DT as bag-of-relations (i.e., frequency count). This will allow us to assess the impact of a flat representation of the DT. Note that more important relational features would be the subtree patterns extracted from the DT. However, they are already generated by TKs in a simpler way. See (Pighin and Moschitti, 2009; Pighin and Moschitti, 2010) for a way to extract the most relevant features from a model learned in the kernel space. 6 Experiments Our experiments aim to show that reranking of discourse parses is a promising research direction, which can improve the state-of-the-art. To achieve this, we (i) compute the oracle accuracy of the kbest parser, (ii) test different kernels for reranking discourse parses by applying standard kernels to our new structures, (iii) show the reranking performance using the best kernel for different number of hypotheses, and (iv) show the relative importance of features coming from different source"
D14-1219,D12-1083,1,0.776641,"relative importance: nuclei are the core parts of the relation while satellites are the supportive ones. Conventionally, discourse analysis in RST involves two subtasks: (i) discourse segmentation: breaking the text into a sequence of EDUs, and (ii) discourse parsing: linking the discourse units to form a labeled tree. Despite the fact that discourse analysis is central to many NLP applications, the state-of-the-art document-level discourse parser (Joty et al., 2013) has an f -score Although recent work has proposed rich linguistic features (Feng and Hirst, 2012) and powerful parsing models (Joty et al., 2012), discourse parsing remains a hard task, partly because these approaches do not consider global features and long range structural dependencies between DT constituents. For example, consider the humanannotated DT (Figure 1a) and the DT generated by the discourse parser of Joty et al. (2013) (Figure 1b) for the same text. The parser makes a mistake in finding the right structure: it considers only e3 as the text to be attributed to e2 , where all the text spans from e3 to e6 (linked by C AUSE and E LAB ORATION ) compose the statement to be attributed. Such errors occur because existing systems"
D14-1219,N03-1030,0,0.0831879,"mocrat from state Maine) elaborates its preceding text. This confuses our reranker. We also found error examples where the reranker failed to distinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorporating rich linguistic features, which include lexical semantics and discourse production rules. Joty et al. (2013) achieved the best prior results by (i) jointly modelin"
D14-1219,H05-1033,0,0.00843381,"tinguish between Background and Elaboration, and between Cause and Elaboration. This suggests that we need rich semantic representation of the text to improve our reranker further. 7 Related Work Early work on discourse parsing applied handcoded rules based on discourse cues and surface patterns (Marcu, 2000a). Supervised learning was first attempted by Marcu (2000b) to build a shiftreduce discourse parser. This work was then considerably improved by Soricut and Marcu (2003). They presented probabilistic generative models for sentence-level discourse parsing based on lexicosyntactic patterns. Sporleder and Lapata (2005) investigated the necessity of syntax in discourse analysis. More recently, Hernault et al. (2010) presented the HILDA discourse parser that iteratively employs two SVM classifiers in pipeline to build a DT in a greedy way. Feng and Hirst (2012) improved the HILDA parser by incorporating rich linguistic features, which include lexical semantics and discourse production rules. Joty et al. (2013) achieved the best prior results by (i) jointly modeling the structure and the label of a DT constituent, (ii) performing optimal rather than greedy decoding, and (iii) discriminating between intra- and"
D14-1219,C10-5000,0,\N,Missing
D15-1068,S15-2047,1,0.877035,"Missing"
D15-1068,S12-1059,0,0.0177823,"Missing"
D15-1068,S15-2036,1,0.541482,"Missing"
D15-1068,P15-2113,1,0.19975,"Missing"
D15-1068,P04-1035,0,0.00602109,"og sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ ∈ [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1 , v2 , · · · , vn , s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi , s) with capacity siG , and to the sink node t by adding an edge w(vi , t) with capacity siB . Finally,"
D15-1068,I11-1164,0,0.0214203,"ssification in Community Question Answering ˜ Giovanni Da San Martino, Simone Filice, Shafiq Joty, Alberto Barr´on-Cedeno, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov, Qatar Computing Research Institute, HBKU {sjoty,albarron,gmartino,sfilice, lmarquez,amoschitti,pnakov}@qf.org.qa Abstract As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat"
D15-1068,W04-2401,0,0.398636,"he CQA-QL dataset: after merging Bad and Potential into Bad. The Task 1 Train 2,600 16,541 8,069 8,472 3 http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee http://www.qatarliving.com/forum http://alt.qcri.org/semeval2015/task3/ 574 3.1 ci and cj have the same label; assigning 0 to xijS means that ci and cj do not have the same label. The same interpretation holds for the other possible classes (in this case only Different).4 Let ciG be the cost of classifying ci as Good, cijS be the cost of assigning the same labels to ci and cj , etc. Following (Roth and Yih, 2004), these costs are obtained from local classifiers by taking log probabilities, i.e., ciG = − log siG , cijS = − log sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The"
D15-1068,S15-2035,0,0.084801,"ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in"
D15-1068,W03-0402,0,0.128215,"Missing"
D15-1068,P15-2117,0,0.0999102,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,W01-0515,0,0.0287716,"he comment-pair variables are consistent: xijD = xiG ⊕ xjG , ∀i, j 1 ≤ i < j ≤ n. λ ∈ [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); Integer Linear Programming Approach Here we follow the inference with clas"
D15-1068,S15-2037,0,0.0777175,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,D07-1002,0,\N,Missing
D15-1068,N10-1145,0,\N,Missing
D15-1068,P07-1098,1,\N,Missing
D15-1068,C10-1131,0,\N,Missing
D15-1068,N13-1106,0,\N,Missing
D15-1068,S15-2038,0,\N,Missing
D15-1068,P08-1082,0,\N,Missing
D15-1068,W13-3509,1,\N,Missing
D15-1068,D13-1044,1,\N,Missing
D15-1147,abdelali-etal-2014-amara,1,0.184811,"Missing"
D15-1147,D13-1106,0,0.0123399,"rge to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . ."
D15-1147,D11-1033,0,0.0505111,"nces, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An al"
D15-1147,2014.iwslt-evaluation.6,1,0.906521,"Missing"
D15-1147,2011.iwslt-evaluation.18,0,0.0228131,"etely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasl"
D15-1147,N13-1114,0,0.33049,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,P13-1126,0,0.513472,"domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural networ"
D15-1147,N12-1047,0,0.0255873,"sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJMb ) show that NEWS is the 1265 Doma"
D15-1147,P14-1129,0,0.286949,"the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our aim in this paper is to advance the state-ofthe-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out1259 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1259–1270, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguist"
D15-1147,P13-2119,0,0.178533,"a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advanta"
D15-1147,E14-4029,1,0.0587384,"del (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the relatedness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair. The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the s"
D15-1147,P13-1141,0,0.0984619,"ture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In r"
D15-1147,2015.mtsummit-papers.10,1,0.422186,"ensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting"
D15-1147,N13-1073,0,0.0372083,"9/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) f"
D15-1147,P12-2023,0,0.0241725,"2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss"
D15-1147,eisele-chen-2010-multiun,0,0.0290387,"Missing"
D15-1147,W08-0334,0,0.257478,"that finding an optimal cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptati"
D15-1147,W07-0717,0,0.0175294,"taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature. Our a"
D15-1147,W09-0439,0,0.181637,"An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidel"
D15-1147,D10-1044,0,0.0298816,"Missing"
D15-1147,D08-1089,0,0.134778,"training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used"
D15-1147,P14-1066,0,0.0175969,"s. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transfo"
D15-1147,P12-1016,0,0.0157674,"al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliteration module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English language pair for the development experiments and train domain-wise models to measure the"
D15-1147,2013.iwslt-papers.2,1,0.883493,"Missing"
D15-1147,E14-1035,0,0.111537,"2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptati"
D15-1147,W11-2123,0,0.0263896,"stics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer provided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transl"
D15-1147,2005.eamt-1.19,1,0.289763,"o be an effective way to discard poor quality or irrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that"
D15-1147,C14-1182,0,0.475615,"Missing"
D15-1147,D13-1176,0,0.0524999,"-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) d"
D15-1147,P07-2045,0,0.00828985,"are/nplm/ Corpus AR-EN Sent. IWSLT QED NEWS UN 150k 150k 203k 3.7M Tok. Corpus EN-DE Sent. 2.8/3.0 1.4/1.5 5.6/6.3 129/125 IWSLT CC NEWS EP 177K 2.3M 200K 1.8M Tok. 3.5/3.3 57/53 2.8/3.4 51/48 Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sentences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, except for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default parameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM models. We used ATB segmentation using the Stanford ATB segmenter (Green and"
D15-1147,P14-2093,0,0.196766,"Missing"
D15-1147,C12-2104,0,0.0153129,"lized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn"
D15-1147,N13-1074,0,0.197071,"It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Ma"
D15-1147,P13-1082,0,0.126125,"n (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data witho"
D15-1147,C14-1105,0,0.147949,"3). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dynamic adaptation without in-domain data (Sennrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013). In this paper, we do model adaptation using a neural network framework. In contrast to previous work, we perform it at the (bilingual) ngram level, where n is sufficiently large to capture long-range cross-lingual dependencies. The 1260 generalized vector representation of the neural network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability"
D15-1147,E12-1055,0,0.100976,"ss useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014),"
D15-1147,D09-1074,0,0.161736,"ice overload”. The sense of the Arabic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adaptation models that make use of all the data while preserving the in-domain preferences. A significant amount of research has been carried out recently in domain adaptation. The complexity of the SMT pipeline, starting from corpus preparation to word-alignment, and then training a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Matsoukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further research in model adaptation using the neural network framework. In recent years, there has been a growing interest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n-grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as"
D15-1147,P13-1045,0,0.0308553,"s the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weights from the last hidd"
D15-1147,N13-1090,0,0.072173,"al network model reduces the data sparsity issue of traditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. 3 Neural Network Joint Model In recent years, there has been a great deal of effort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hinton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly. Given a source sentence S and its corresponding target sentence T , the NNJM model computes the conditional probability P (T |S) as follows: P (T |S) ≈ |T | Y P (ti |ti−1 . . . ti−p+1 , si ) (1) where φ(xn ) defines the transformations of xn through the hidden layers, and wk are the weig"
D15-1147,D13-1140,0,0.283823,"(yn = k) is an indicator variable (i.e., ynk =1 when yn =k, otherwise 0). Optimization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfortunately, training NNLMs are impractically slow because for each training instance (xn , yn ), the softmax output layer (see Equation 2) needs to compute a summation over all words in the output vocabulary.2 Noise contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010) provides an efficient and stable way to avoid this repetitive computation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: P (yn = k|xn , θ) = σ(yn = k|xn , θ) Z(φ(xn ), W) (4) where σ(.) is the un-normalized score and Z(.) is the normalization factor. In NCE, we consider 2 This would take few weeks for a modern CPU machine to train a single NNJM model on the whole data. 1261 Look-up layer Hidden layer Output layer U Source token 1 W Source token 2 C π yn Source token 3 Target token 1 ψ ynm Target token 2 xn M φ(xn ) Figure 1: A simplified neural network joint model with noise contrastive loss, where we use 3-gram target words (i.e., 2-words history) an"
D15-1147,P10-2041,0,0.0346668,"rrelevant training instances, which when included in an MT system, hurts its performance. The idea is to score the outdomain data using a model trained from the indomain data and apply a cut-off based on the resulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when training is expensive and also when memory is constrained. Data selection was done earlier for language modeling using information retrieval techniques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source- and target-side language models. Duh et al. (2013) used a recurrent neural language model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima’an, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time c"
D15-1147,D09-1141,0,0.149564,"cut-off threshold is a time consuming process. An alternative to completely filtering out less useful data is to minimize its effect by downweighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain. Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model. Other work on domain adaptation includes but not l"
D15-1147,P02-1040,0,0.103895,"he cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative approach by additionally penalizing data instances similar to the out-domain data. We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve better perplexities (Chen and Goodman, 1999) than the models trained on in- and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these models within the SMT pipeline. We obtain gains of up to 0.5 and 0.6 on Arabic-English and EnglishGerman pairs over a competitive baseline system. The remainder of this paper is organized as follows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 discusses our models. Section 5 presents the experimental setup and the results. Section 6 concludes. 2 Related Work Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation. 2.1 Data Selection Data selection has"
D15-1147,P13-2071,1,\N,Missing
D15-1147,P11-1105,1,\N,Missing
D15-1147,J15-2001,1,\N,Missing
D15-1147,W13-2212,1,\N,Missing
D15-1147,W14-3302,0,\N,Missing
D15-1168,P10-2050,0,0.00837811,"ned word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type"
D15-1168,H05-1045,0,0.00883991,". In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam a"
D15-1168,S15-1002,0,0.00849171,"He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semiCRF and shallow RNN. They used word embeddings from Google without fine-tuning them. Although inspired, our work differs from the work of Irsoy and Cardie (2014) in many ways. (i) We experiment with not only Elman-type, but also with a Jordan"
D15-1168,D14-1080,0,0.716206,"uite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston"
D15-1168,S14-2004,0,0.779477,"om row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic composit"
D15-1168,P13-1161,0,0.00970649,"common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose se"
D15-1168,D13-1170,0,0.0225568,"opinion targets can be formulated as a token-level sequence tagging problem, where the task is to label each word in a sentence using the conventional BIO tagging scheme. For example, Table 1 shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have"
D15-1168,S14-2038,0,0.184408,"Missing"
D15-1168,P10-1040,0,0.0135739,"works for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort. We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations."
D15-1168,H05-1044,0,0.049066,"e second best on the Restaurant dataset in SemEval-2014. We make our source code available.1 In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3. In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 201"
D15-1168,D12-1122,0,0.0452663,"shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show t"
D15-1168,E14-1051,0,\N,Missing
D18-1452,S16-1130,1,0.8612,"Missing"
D18-1452,P15-2113,1,0.615975,"al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a"
D18-1452,E17-2115,0,0.0635121,"in answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model. Moreover, they focus on one task only, while we use multitask learning. Bonadiman et al. (2017) proposed a multitask neural architecture where the three tasks are trained together with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic"
D18-1452,I17-2075,0,0.03107,"een subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate ta"
D18-1452,S16-1172,0,0.0498933,"erent cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model."
D18-1452,P15-1078,1,0.900381,"Missing"
D18-1452,P16-2075,1,0.897508,"Missing"
D18-1452,S16-1137,1,0.897878,"Missing"
D18-1452,D15-1068,1,0.814627,"Missing"
D18-1452,P16-1165,1,0.636808,"al inference over arbitrary graph structures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first aut"
D18-1452,N16-1084,1,0.905394,"Missing"
D18-1452,K17-1024,1,0.89338,"Missing"
D18-1452,C18-1181,0,0.0548546,"tion and Motivation Question answering web forums such as StackOverflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question–comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content. Within community Question Answering (cQA) forums, two subtasks are of special relevance when a user poses a new question to the website (Hoogeveen et al., 2018; Lai et al., 2018): (i) finding similar questions (question-question relatedness), and (ii) finding relevant answers to the new question, if they already exist (answer selection). ∗ Work conducted while this author was at QCRI, HBKU. Both subtasks have been the focus of recent research as they result in end-user applications. The former is interesting for a user who wants to explore the space of similar questions in the forum and to decide whether to post a new question. It can also be relevant for the forum owners as it can help detect redundancy, eliminate question duplicates, and improve the overall forum st"
D18-1452,P16-1101,0,0.0474628,"er with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. Huang et al. (2015) proposed an LSTM-CRF framework for such tasks. Ma and Hovy (2016) included a CNN in the framework to compute word representations from character-level embeddings. While these studies consider tasks related to constituents in a sentence, e.g., words and phrases, we focus on methods to represent comments and to model dependencies between comment-level tags. We also experiment with arbitrary graph structures in our CRF model to model dependencies at different levels. 3 Learning Approach cim Let q be a newly-posed question, and denote the m-th comment (m ∈ {1, 2, . . . , M }) in the answer thread for the i-th potentially related question qi (i ∈ {1, 2, . . . ,"
D18-1452,K15-1032,1,0.833942,"uctures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first author would like to thank the funding su"
D18-1452,S16-1136,1,0.878923,"Missing"
D18-1452,N13-1090,0,0.119305,"Missing"
D18-1452,D16-1165,1,0.935991,"he relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these models, we use DNNs to induce taskspecific embeddings, and, more importantly, we perform multitask learning of three different cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features."
D18-1452,S16-1083,1,0.914124,"Missing"
D18-1452,P02-1040,0,0.100744,"Missing"
D18-1452,P15-2114,0,0.0698502,"cially answer-goodness and question-question-relatedness influence answerselection significantly; (iii) the CRFs exploit the dependencies between subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN model"
D18-1452,D18-1131,1,0.84905,"Missing"
D18-1452,2006.amta-papers.25,0,0.0322229,"Missing"
D18-1452,P18-1162,0,0.0171294,"sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddi"
D18-1452,P15-2117,0,0.0300461,"taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these mo"
D18-1452,P13-1045,0,0.0436668,"Missing"
D18-1452,P15-2116,0,0.0365402,"Missing"
D19-1093,P16-1231,0,0.178772,"ical University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse p"
D19-1093,D15-1041,0,0.0283709,"der the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained st"
D19-1093,D14-1179,0,0.033968,"Missing"
D19-1093,P15-1033,0,0.0242619,"∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networks capable of remembering long range relationships such as Stacked LSTMs (Dyer et al., 2015; Ballesteros et al., 2015) or using globally normalized models (Andor et al., 2016). The globally optimized methods, on the other hand, learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et"
D19-1093,C96-1058,0,0.150356,"xamples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subseq"
D19-1093,P14-1048,0,0.170523,"rence analysis in RST consists of two subtasks: (a) identifying the EDUs in a text, referred to as Discourse Segmentation, and (b) building a discourse tree by linking the EDUs hierarchically, referred to as Discourse Parsing. This work focuses on the more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures"
D19-1093,P14-1002,0,0.284759,". This work focuses on the more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures also hold here.1 First, while dependency relations can only be between words, discourse relations can be between elementary units, between larger units or both. Second, in dependency parsing, any two words can be linked, whereas"
D19-1093,D12-1083,1,0.889814,"Missing"
D19-1093,J15-3002,1,0.948703,"espectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue using neural networ"
D19-1093,Q16-1023,0,0.0631775,"Missing"
D19-1093,P14-1003,0,0.305546,"e more challenging task of discourse parsing assuming that EDUs have already been identified. In fact, state-of-the-art segmenter (Lin et al., 2019) has already achieved 95.6 F1 on RST discourse treebank, where the human agreement is 98.3 F1 . Earlier methods have mostly utilized handcrafted lexical and syntactic features (Soricut and Marcu, 2003; Feng and Hirst, 2014; Joty et al., 2015; Wang et al., 2017). Recent approaches have 1008 shown competitive results with neural models that are able to automatically learn the feature representations in an end-to-end fashion (Ji and Eisenstein, 2014; Li et al., 2014). Very recently, Lin et al. (2019) propose a parser based on pointer networks and achieve state-of-the-art performance. 3 Remark. Although related, the dependency and RST tree structures (hence the parsing tasks) are different. RST structure is similar to constituency structure. Therefore, the differences between constituency and dependency structures also hold here.1 First, while dependency relations can only be between words, discourse relations can be between elementary units, between larger units or both. Second, in dependency parsing, any two words can be linked, whereas RST allows connec"
D19-1093,P19-1410,1,0.721475,"ctions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse parsing tasks with the same computational efficiency (Ma et al., 2018; Lin et al., 2019); thanks to the encoder-decoder architecture that makes it possible to capture information from the whole text and the previously derived subtrees, 1007 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1007–1017, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics while limiting the number of parsing steps to linear. However, the decoder of these parsers has a sequential structure, which may not yield the most appropriate inductive bias for d"
D19-1093,N15-1142,0,0.0337556,"±0.02 ro 91.15±0.12 85.54±0.13 91.34±0.18 85.73±0.22 91.09±0.09 85.36±0.10 Table 1: Dependency parsing results on 7 UD Treebanks. StackPtr (code) denotes the experiments we rerun on our machine. H-PtrNet-PST (Gate) and H-PtrNet-PST (SGate) are H-PtrNet models with gating mechanism. we follow the standard split for training, validation and testing. It should be noted that Ma et al. (2018) used UD Treebanks 2.1, which is not the most up-to-date version. Therefore, during experiments, we rerun their codes with UD Treebanks 2.3 to match our experiments. To be specific, we use structured-skipgram (Ling et al., 2015) for English and German, while Polyglot embedding (AlRfou et al., 2013) for the other languages. Adam optimizer (Kingma and Ba, 2015) is used as the optimization algorithm. We apply 0.33 dropout rate between layers of encoder and to word embeddings as well as Eq. 8. We use beam size of 10 for English Penn Treebank, and beam size of 1 for UD Treebanks. The gold-standard POS tags is used for English Penn Treebank. We also use the universal POS tags (Petrov et al., 2011) provided in the dataset for UD Treebanks. See Appendix for a complete list of hyperparameters. Results on UD Treebanks. We eval"
D19-1093,P18-1130,0,0.0660044,"learn scoring functions for subtrees and perform search over all possible trees to find the most probable tree for a text. Recent graph-based methods use neural models as scoring functions (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017). Despite being more accurate than greedy parsers, these methods are generally slow having a polynomial time complexity (O(n3 ) or higher). Recently, transition-based top-down parsing with Pointer Networks (Vinyals et al., 2015) has attained state-of-the-art results in both dependency and discourse parsing tasks with the same computational efficiency (Ma et al., 2018; Lin et al., 2019); thanks to the encoder-decoder architecture that makes it possible to capture information from the whole text and the previously derived subtrees, 1007 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1007–1017, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics while limiting the number of parsing steps to linear. However, the decoder of these parsers has a sequential structure, which may not yield the most appropriate i"
D19-1093,J00-3005,0,0.459713,"Missing"
D19-1093,J18-2001,0,0.0661953,"d hierarchical pointer networks, we first revisit how pointer networks have been used for parsing tasks. 3.1 3.2 where σ(., .) is a scoring function for attention, which can be a neural network or an explicit formula like dot product. The model uses at to infer the output: ˆ yt = arg max(at ) = arg max p(yt |y&lt;t , X, θ) where θ is the set of parameters. To condition on yt−1 , the corresponding input xyt−1 is copied as the input to the decoder. 1 There are also studies that use dependency structure to directly represent the relations between the EDUs; see (Muller et al., 2012; Li et al., 2014; Morey et al., 2018). Hierarchical Pointer Networks Pointer Networks for Parsing. Limitations of Existing Methods One crucial limitation of the existing models is that the decoder has a linear structure, although the task is to construct a hierarchical structure. This can be noticed in the Figures 2 and 3, where the current decoder state dt is conditioned on the previous state dt−1 (see horizontal blue lines), but not on its parent’s decoder state or siblings’ decoder state, when it was pointed from its head. This can induce irrelevant information if the previous decoding state corresponds to an element that is n"
D19-1093,C12-1115,0,0.0211337,"st ) (1) Before presenting our proposed hierarchical pointer networks, we first revisit how pointer networks have been used for parsing tasks. 3.1 3.2 where σ(., .) is a scoring function for attention, which can be a neural network or an explicit formula like dot product. The model uses at to infer the output: ˆ yt = arg max(at ) = arg max p(yt |y&lt;t , X, θ) where θ is the set of parameters. To condition on yt−1 , the corresponding input xyt−1 is copied as the input to the decoder. 1 There are also studies that use dependency structure to directly represent the relations between the EDUs; see (Muller et al., 2012; Li et al., 2014; Morey et al., 2018). Hierarchical Pointer Networks Pointer Networks for Parsing. Limitations of Existing Methods One crucial limitation of the existing models is that the decoder has a linear structure, although the task is to construct a hierarchical structure. This can be noticed in the Figures 2 and 3, where the current decoder state dt is conditioned on the previous state dt−1 (see horizontal blue lines), but not on its parent’s decoder state or siblings’ decoder state, when it was pointed from its head. This can induce irrelevant information if the previous decoding sta"
D19-1093,K18-2008,0,0.017857,"o two main categories: greedy transitionbased parsing and graph-based parsing. In both paradigms, neural models have proven to be more effective than feature-based models where selecting the composition of features is a major challenge. Kiperwasser and Goldberg (2016) proposed graph-based and transition-based dependency parsers with a Bi-LSTM feature representation. Since then much work has been done to improve these two parsers. Dozat and Manning (2017) proposed a bi-affine classifier for the prediction of arcs and labels based on graph-based model, and achieved state-of-the-art performance. Nguyen and Verspoor (2018) adopted a joint modeling approach by adding a Bi-LSTM POS tagger to generate POS tags for the graph-based dependency parser. Though transition-based methods are superior in terms of time complexity, they fail in capturing the global dependency information when making decisions. To address this issue, Andor et al. (2016) proposed a globally optimized transition-based model. Recently, by incorporating a stack within a pointer network, Ma et al. (2018) proposed a transition-based model and achieved state-of-the-art performance across many languages . 2.2 Discourse Parsing Rhetorical Structure Th"
D19-1093,N18-1202,0,0.083523,"Missing"
D19-1093,L16-1376,0,0.0303939,"model with fusion function dt = f (dp(t) , ds(t) , hp(t) ), where the decoder receives the hidden states from both the parent and sibling in each decoding step. • H-PtrNet-PST: This is the full model with fusion function dt = f (dp(t) , ds(t) , dt−1 , hp(t) ) (Eq. 2). In this model, the decoder receives the hidden states from its parent, sibling and previous step in each decoding step. 4.1 Dependency Parsing Dataset. We evaluate our model on the English Penn Treebank (PTB v3.0) (Marcus et al., 1994), which is converted to Stanford Dependencies format with Stanford Dependency Converter 3.3.0 (Schuster and Manning, 2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on seven (7) languages from the Universal Dependency (UD) Treebanks5 (version 2.3). Metrics. We evaluate the performance of our models with unlabeled attachment score (UAS) and labeled attachment score (LAS). We ignore punctuations in the evaluation for English. Experimental Settings. We use the same setup as Ma et al. (2018) in the experiments for English Penn Treebank and UD Treebanks. For a fair comparison, we rerun their model with the hyperparameters provided by the authors on the same machine as"
D19-1093,N03-1030,0,0.435235,"vel discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disallowing the model to capture long distance dependencies and also causing error propagation to subsequent steps. Recent methods attempt to address this issue"
D19-1093,P17-2029,0,0.255334,"bes the relationships between the tree constituents (e.g., words, phrases). For example, Figure 1 shows examples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on local information, disa"
D19-1093,P11-2033,0,0.0382745,"ucture that best describes the relationships between the tree constituents (e.g., words, phrases). For example, Figure 1 shows examples of a dependency tree and a sentence-level discourse tree that respectively represent how the words and clauses are related in a sentence. Such parse trees are directly useful in numerous NLP applications, and also serve as intermediate representations for further language processing tasks such as semantic and discourse processing. Existing approaches to parsing can be distinguished based on whether they employ a greedy transition-based algorithm (Marcu, 1999; Zhang and Nivre, 2011; Wang et al., 2017) or a globally optimized algorithm such as graph-based methods for dependency parsing (Eisner, 1996) or chart parsing for discourse (Soricut and Marcu, 2003; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series ∗ Linlin Liu is under the Joint PhD Program between Alibaba and Nanyang Technological University. † Equal contribution. of shift-reduce decisions. The advantage of this method is that the parsing time is linear with respect to the sequence length. The limitation, however, is that the decisions made at each step are based on loc"
D19-1231,P05-1018,0,0.209385,"s used to capture the attention and topic structures. We evaluate our models on both local and global discrimination tasks on the benchmark dataset. Our results show that our approach outperforms existing methods by a wide margin in both tasks. We have released our code at https://ntunlpsg.github.io/project/coherence/ncoh-emnlp19/ for research purposes. 2 Related Works Inspired by various linguistic theories of discourse, many coherence models have been proposed. In this section, we give a brief overview of the existing coherence models. Motivated by the Centering Theory (Grosz et al., 1995), Barzilay and Lapata (2005, 2008) proposed the entity-based local model for representing and assessing text coherence, which showed significant improvements in two out of three evaluation tasks. Their model represents a 1 http://workshop.colips.org/wochat/data/index.html text by a two-dimensional array called entity grid that captures local transitions of discourse entities across sentences as the deciding patterns for assessing coherence. They consider the salience of the entities to distinguish between transitions of important entities from unimportant ones, by measuring the occurrence frequency of the entities. Subs"
D19-1231,J08-1001,0,0.947424,"discrimination tasks, we demonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art. 1 Introduction Coherence modeling involves building text analysis models that can distinguish a coherent text from incoherent ones. It has been a key problem in discourse analysis with applications in text generation, summarization, and coherence scoring. Various linguistic theories have been proposed to formulate coherence, some of which have inspired development of many of the existing coherence models. These include the entity-based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) that consider syntactic realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006),"
D19-1231,N04-1015,0,0.727666,"realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which"
D19-1231,N07-1055,0,0.654928,"in utterance ranking (Lowe et al., 2015) or bot detection1 in dialogue, and for sentence ordering in summarization. According to Grosz and Sidner (1986), three factors collectively contribute to discourse coherence: (a) the organization of discourse segments, (b) intention or purpose of the discourse, and (c) attention or focused items. The entitybased approaches capture attentional structure, the syntax-based approaches consider intention, and the organizational structure is largely captured by models that consider discourse relations and content (topic) distribution. Although methods like (Elsner et al., 2007; Li and Jurafsky, 2017) attempt to combine these aspects of coherence, to our knowledge, no methods consider all the three aspects together in a single framework. In this paper, we propose a unified neural model that incorporates sentence grammar (intentional structure), discourse relations, attention and topic structures in a single framework. We use an LSTM sentence encoder with explicit language model loss to capture the syntax. Intersentence discourse relations are modeled with a bilinear layer, and a lightweight convolution-pooling is used to capture the attention and topic structures. W"
D19-1231,P11-1118,0,0.504682,"emonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art. 1 Introduction Coherence modeling involves building text analysis models that can distinguish a coherent text from incoherent ones. It has been a key problem in discourse analysis with applications in text generation, summarization, and coherence scoring. Various linguistic theories have been proposed to formulate coherence, some of which have inspired development of many of the existing coherence models. These include the entity-based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) that consider syntactic realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribu"
D19-1231,P11-2022,0,0.734446,"emonstrate that our proposed model outperforms existing models by a good margin, and establish a new state-of-the-art. 1 Introduction Coherence modeling involves building text analysis models that can distinguish a coherent text from incoherent ones. It has been a key problem in discourse analysis with applications in text generation, summarization, and coherence scoring. Various linguistic theories have been proposed to formulate coherence, some of which have inspired development of many of the existing coherence models. These include the entity-based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) that consider syntactic realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribu"
D19-1231,E12-1032,0,0.0192997,"nguish between transitions of important entities from unimportant ones, by measuring the occurrence frequency of the entities. Subsequent studies extended the basic entity grid model. By including non-head nouns as entities in the grid, Elsner and Charniak (2011b) gained significant improvements. They incorporate entity-specific features like named entity, noun class, and modifiers to distinguish between entities of different types, which led to further improvements. Instead of using the transitions of grammatical roles, Lin et al. (2011) model the transitions of discourse roles for entities. Feng and Hirst (2012) used the basic entity grid, but improved its learning to rank scheme. Their model learns not only from the original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) proposed a graph-based unsupervised method for modeling text coherence. Assuming the sentences in a coherent discourse should share the same structural syntactic patterns, Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text. Their proposed method comprises of local and global coherence model, where the former one ca"
D19-1231,C14-1089,0,0.244905,"Missing"
D19-1231,J86-3001,0,0.455494,"al Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2262–2272, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics uddin et al., 2018), it is far from being solved. As we show later, state-of-the-art models often fail on harder tasks like local discrimination and insertion that ask the model to evaluate a local context (e.g., a 3-sentence window). This task has direct applications in utterance ranking (Lowe et al., 2015) or bot detection1 in dialogue, and for sentence ordering in summarization. According to Grosz and Sidner (1986), three factors collectively contribute to discourse coherence: (a) the organization of discourse segments, (b) intention or purpose of the discourse, and (c) attention or focused items. The entitybased approaches capture attentional structure, the syntax-based approaches consider intention, and the organizational structure is largely captured by models that consider discourse relations and content (topic) distribution. Although methods like (Elsner et al., 2007; Li and Jurafsky, 2017) attempt to combine these aspects of coherence, to our knowledge, no methods consider all the three aspects to"
D19-1231,J95-2003,0,0.956534,"Missing"
D19-1231,P13-1010,0,0.359579,"011b) gained significant improvements. They incorporate entity-specific features like named entity, noun class, and modifiers to distinguish between entities of different types, which led to further improvements. Instead of using the transitions of grammatical roles, Lin et al. (2011) model the transitions of discourse roles for entities. Feng and Hirst (2012) used the basic entity grid, but improved its learning to rank scheme. Their model learns not only from the original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) proposed a graph-based unsupervised method for modeling text coherence. Assuming the sentences in a coherent discourse should share the same structural syntactic patterns, Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text. Their proposed method comprises of local and global coherence model, where the former one captures the co-occurrence of structural features in adjacent sentences and the latter one captures the global structure based on clusters of sentences with similar syntax. Our model also considers syntactic patterns through a biLSTM sentence enc"
D19-1231,D14-1218,0,0.562807,"l., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Nguyen and Joty (2017); Mohiuddin et al. (2018) propose neural entity grid models using"
D19-1231,D17-1019,0,0.660892,"(Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Nguyen and Joty (2017); Mohiuddin et al. (2018) propose neural entity grid models using convolutions over distributed representations of entity transitions, and report state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Traditionally coherence models have been evaluated on two kinds of tasks. The first kind includes synthetic tasks su"
D19-1231,P11-1100,0,0.446333,"Missing"
D19-1231,D12-1106,0,0.775192,"ering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) mo"
D19-1231,W15-4640,0,0.0343988,"asks (Elsner and Charniak, 2011a; Mohi2262 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2262–2272, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics uddin et al., 2018), it is far from being solved. As we show later, state-of-the-art models often fail on harder tasks like local discrimination and insertion that ask the model to evaluate a local context (e.g., a 3-sentence window). This task has direct applications in utterance ranking (Lowe et al., 2015) or bot detection1 in dialogue, and for sentence ordering in summarization. According to Grosz and Sidner (1986), three factors collectively contribute to discourse coherence: (a) the organization of discourse segments, (b) intention or purpose of the discourse, and (c) attention or focused items. The entitybased approaches capture attentional structure, the syntax-based approaches consider intention, and the organizational structure is largely captured by models that consider discourse relations and content (topic) distribution. Although methods like (Elsner et al., 2007; Li and Jurafsky, 201"
D19-1231,D18-1464,0,0.0958523,"s (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Nguyen and Joty (2017); Mohiuddin et al. (2018) propose neural entity grid models using convolutions over distributed representations of entity transitions, and report state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Traditionally coherence models have been evaluated on two kinds of tasks. The first kind includes synthetic tasks such as discrimination and insertion that evaluate the models directly based on their ability to identify the right order of the sentences in a text"
D19-1231,P18-1052,1,0.834042,"with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Nguyen and Joty (2017); Mohiuddin et al. (2018) propose neural entity grid models using convolutions over distributed representations of entity transitions, and report state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Traditionally coherence models have been evaluated on two kinds of tasks. The first kind includes synthetic tasks such as discrimination and insertion that evaluate the models directly based on their ability to identify the right order of the sentences in a text with different levels of difficulty (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b). The latter kind of tasks eva"
D19-1231,P17-1121,1,0.761778,"ot their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) incorporate global topic information with an encoder-decoder architecture, which is also capable of generating discourse. Mesgar and Strube (2018) model change patterns of salient semantic information between sentences. Nguyen and Joty (2017); Mohiuddin et al. (2018) propose neural entity grid models using convolutions over distributed representations of entity transitions, and report state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Traditionally coherence models have been evaluated on two kinds of tasks. The first kind includes synthetic tasks such as discrimination and insertion that evaluate the models directly based on their ability to identify the right order of the sentences in a text with different levels of difficulty (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b). The"
D19-1231,N18-1202,0,0.0153861,"the current time step word vector representation et and the output of the previous time step ht−1 , where p is the number of features in the LSTM hidden state. The output of the last time step hm is considered as the representation of the sentence. A bi-LSTM processes a given sentence si in two directions: from left-to-right and right-to-left, yield−→ ←− ing a representation hi = [hm ; hm ] ∈ R2p , where ‘;’ denotes concatenation. We train our sentence encoder with an explicit language model loss. A bidirectional language model combines a forward and a backward language model (LM). Similar to Peters et al. (2018), 2264 Input Document Word Embedding Sentence Encoder Bilinear Layer Linear Layer Global Coherence Model s1 s1 s2 s3 s4 Embed s2 Embed e1 e2 e1 e2 em0 em1 Bi-LSTM h1 h2 Bilinear Bilinear h3 sn sn e1 e2 Embed emn u&apos; u Bi-LSTM Bi-LSTM hn v1 v2 Global Document Information Pos Neg Linear Global average pooling y0 LConv u Bilinear vn LConv Linear LConv yn LConv u Positive LConv s1 s1 Locally s3 negative s2 s4 sn Embed s3 Embed e1 e2 em0 e1 e2 em2 Bi-LSTM h1 Bi-LSTM h3 LConv Bilinear v&apos;1 Bilinear v&apos; 2 h2 sn Embed e1 e2 emn Bi-LSTM hn Negative Linear y&apos;0 u&apos; Bilinear vn Linear u&apos; pos neg h1 h1 h2 h3 h"
D19-1231,D08-1020,0,0.185429,"It has been a key problem in discourse analysis with applications in text generation, summarization, and coherence scoring. Various linguistic theories have been proposed to formulate coherence, some of which have inspired development of many of the existing coherence models. These include the entity-based local models (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) that consider syntactic realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For"
D19-1231,P06-2103,0,0.35306,"Barzilay and Lapata, 2008; Elsner and Charniak, 2011b) that consider syntactic realization of entities in adjacent sentences, inspired by the Centering Theory (Grosz et al., 1995). Another line of research uses discourse relations between sentences to predict local coherence (Pitler and Nenkova, 2008; Lin et al., 2011). These methods are inspired by the discourse structure theories like Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) that formalizes coherence in ∗ *Equal contribution terms of discourse relations. Other notable methods include word co-occurrence based local models (Soricut and Marcu, 2006), content (or topic distribution) based global models (Barzilay and Lee, 2004), and syntax based local and global models (Louis and Nenkova, 2012). With the neural invasion, some of the above traditional models have got their neural versions with much improved performance. For example, Li and Hovy (2014) implicitly model syntax and intersentence relations using a neural framework that uses a recurrent (or recursive) layer to encode each sentence and a fully-connected layer with sigmoid activations to estimate coherence probability for every window of three sentences. Li and Jurafsky (2017) inc"
D19-1294,W11-2103,0,0.0224755,"only two candidate translations of the same text as input for comparison: this could be a reference vs. a system translation, or a comparison between two candidate translations (see Section 5.5). 3 Dataset Generation We automatically generated our dataset, which we used to build a pronoun test suite and to train a pronoun evaluation model. In order to avoid generating synthetic data that may not necessarily represent a difficult context (for an MT system to correctly translate the pronouns), we used data from actual system outputs submitted for the WMT translation tasks in 2011–2015 and 2017 (Callison-Burch et al., 2011, 2012; Bojar et al., 2013, 2014, 2015, 2017). Using such data means that what is essentially a conditional language model solution, such as the one used by Bawden et al. (2018), has already failed on these examples. Original French input: Il e´ tait cr´eatif, g´en´ereux, drˆole, affectueux et talentueux, et il va beaucoup me manquer. Reference translation: He was creative, generous, funny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, gen"
D19-1294,N18-1118,0,0.432704,"sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 201"
D19-1294,D07-1007,0,0.046499,"ntiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et a"
D19-1294,P07-1005,0,0.044586,"onoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is a"
D19-1294,W13-2201,0,0.0312423,"the same text as input for comparison: this could be a reference vs. a system translation, or a comparison between two candidate translations (see Section 5.5). 3 Dataset Generation We automatically generated our dataset, which we used to build a pronoun test suite and to train a pronoun evaluation model. In order to avoid generating synthetic data that may not necessarily represent a difficult context (for an MT system to correctly translate the pronouns), we used data from actual system outputs submitted for the WMT translation tasks in 2011–2015 and 2017 (Callison-Burch et al., 2011, 2012; Bojar et al., 2013, 2014, 2015, 2017). Using such data means that what is essentially a conditional language model solution, such as the one used by Bawden et al. (2018), has already failed on these examples. Original French input: Il e´ tait cr´eatif, g´en´ereux, drˆole, affectueux et talentueux, et il va beaucoup me manquer. Reference translation: He was creative, generous, funny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, generous, funny, loving and t"
D19-1294,W15-3001,0,0.0618627,"Missing"
D19-1294,J93-2003,0,0.109724,"sive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model m"
D19-1294,W12-3102,0,0.106102,"Missing"
D19-1294,P05-1033,0,0.127023,"ges and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researc"
D19-1294,N19-1423,0,0.0276988,"Missing"
D19-1294,W08-0331,0,0.0463595,"ge Processing, pages 2964–2975, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Here we propose a targeted dataset for machine translation evaluation with a focus on anaphora. We further present a specialized evaluation measure trained on this dataset. The measure performs pairwise evaluations: it learns to distinguish good vs. bad translations of pronouns, without being given specific signals of the errors. It has been argued that pairwise evaluation is useful and sufficient for machine translation evaluation (Guzm´an et al., 2015, 2017). In particular, Duh (2008) has shown that ranking-based evaluation measures can achieve higher correlations with human judgments, as ranking judgments are easier to obtain from human judges and are also easy to use in training, while also directly achieving the purpose of comparing two systems. Note that while it may be possible to rank translations using strong pre-trained conditional language models such as GPT (Radford et al., 2018), all kinds of errors would influence the score, and it would not be targeted towards a specific source of error, such as anaphora here. Our model provides a way to do this, and we demons"
D19-1294,N13-1073,0,0.0119131,"ny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, generous, funny, loving and talented, and I will miss him dearly. Generated noisy example 2: He was creative, generous, funny, loving and talented, and we will miss him dearly. Figure 1: Noisy examples generated by substituting an MT-generated pronoun in the reference translation. In particular, we aligned the system outputs with the reference translation using an automatic alignment tool (Dyer et al., 2013), and we found examples in which the pronouns did not match the reference translation. This process yielded potentially noisy data, as the alignments are automatic and thus not always perfect. 3.1 User Study In order to ensure that the mismatched pronouns are not equally good translations in the given context, we conducted a user study on a subset of the data. To focus the study on pronouns and to remove the influence of other MT errors, we generated a noisy candidate by replacing the correct pronoun in the reference with the aligned (potentially) incorrect pronoun from the system output. We d"
D19-1294,N04-1035,0,0.120142,"slation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sent"
D19-1294,L16-1100,0,0.0186828,"slation, i.e., this was not a realistic translation task. The 2015 edition of the task also featured a pronoun-focused translation task, which was like a normal MT task except that the evaluation focused on the pronouns only, and was performed manually. In contrast, we have a real MT evaluation setup, and we develop and use a fully automatic evaluation measure. More recently, there has been a move towards using specialized test suites specifically designed to assess system quality for some fine-grained problematic categories, including pronoun translation. For example, the PROTEST test suite (Guillou and Hardmeier, 2016) comprises 250 pronoun tokens, used in a semi-automatic evaluation: the pronouns in the MT output and in the reference are compared automatically, but in case of no matches, manual evaluation was required. Moreover, no final aggregate score over all pronouns was produced. In contrast, we have a much larger test suite with a fully automatic measure. Another semi-automatic system is described in (Guillou et al., 2018). It focused on just two pronouns, it and they, and was applied to a single language pair. In contrast, we have a fully automated evaluation measure, we handle many English pronouns"
D19-1294,D18-1513,0,0.0734255,"to tell apart a human translation from a machine output when going beyond the sentence level (L¨aubli et al., 2018). Overall, it is clear that there is a need for machine translation evaluation measures that look beyond the sentence level, and thus can better appreciate the improvements that a discourseaware MT system could potentially bring. Alternatively, one could use diagnostic test sets that are designed to evaluate how an MT system handles specific discourse phenomena (Bawden et al., 2018; Rios et al., 2018). There have also been proposals to use semi-automatic measures and test suites (Guillou and Hardmeier, 2018). 2964 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2964–2975, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Here we propose a targeted dataset for machine translation evaluation with a focus on anaphora. We further present a specialized evaluation measure trained on this dataset. The measure performs pairwise evaluations: it learns to distinguish good vs. bad translations of pronouns, without being given specific signals of the err"
D19-1294,W18-6435,0,0.0395801,"evaluation: the pronouns in the MT output and in the reference are compared automatically, but in case of no matches, manual evaluation was required. Moreover, no final aggregate score over all pronouns was produced. In contrast, we have a much larger test suite with a fully automatic measure. Another semi-automatic system is described in (Guillou et al., 2018). It focused on just two pronouns, it and they, and was applied to a single language pair. In contrast, we have a fully automated evaluation measure, we handle many English pronouns, and we cover multiple source languages. Bawden et al. (2018) presented hand-crafted discourse test sets to test a model’s ability to exploit previous source and target sentences, based on 200 contrastive pairs, one with a correct and one with a wrong pronoun translation. This alleviates the need for an automatic evaluation measure as one can just count how many times the MT system has generated the correct pronoun. In contrast, we use texts from pre-existing MT evaluation datasets, we do not require them to be in contrastive pairs, and we have a fully automated evaluation measure; we also use larger datasets. 2965 M¨uller et al. (2018) also used contra"
D19-1294,P15-1078,1,0.896853,"Missing"
D19-1294,2010.iwslt-papers.10,0,0.458807,"Missing"
D19-1294,W15-2501,1,0.850389,"et and the model are based on actual system outputs. • Our evaluation measure achieves high agreement with human judgments. We make both the dataset and the evaluation measure publicly available at https://ntunlpsg.github.io/project/discomt/evalanaphora/. 2 Related Work Previous work on discourse-aware machine translation and MT evaluation has targeted a number of phenomena such as anaphora, gender agreement, lexical consistency, and coherence. In this work, we focus on pronoun translation. Pronoun translation has been the target of a shared task at the DiscoMT and WMT workshops in 2015-2017 (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). However, the focus was on cross-lingual pronoun prediction, which required choosing the correct pronouns in the context of an existing translation, i.e., this was not a realistic translation task. The 2015 edition of the task also featured a pronoun-focused translation task, which was like a normal MT task except that the evaluation focused on the pronouns only, and was performed manually. In contrast, we have a real MT evaluation setup, and we develop and use a fully automatic evaluation measure. More recently, there has been a move towards usi"
D19-1294,D12-1108,0,0.0206439,"y and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address int"
D19-1294,P82-1020,0,0.802189,"Missing"
D19-1294,N03-1017,0,0.0432782,"as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially tra"
D19-1294,D18-1512,0,0.0951009,"Missing"
D19-1294,W17-4801,1,0.88967,"Missing"
D19-1294,W17-4802,0,0.0953603,"existing MT evaluation datasets, we do not require them to be in contrastive pairs, and we have a fully automated evaluation measure; we also use larger datasets. 2965 M¨uller et al. (2018) also used contrastive translation pairs, mined from a parallel corpus using automatic coreference-based mining of context, thus minimizing the risk of producing wrong contrastive examples that are both valid translations. Yet, they did not propose an evaluation measure. Finally, there have been pronoun-focused automatic machine translation evaluation measures. Two important examples include APT (Miculicich Werlen and Popescu-Belis, 2017) and AutoPRF (Hardmeier and Federico, 2010). Both measures require alignments between the source, the reference and the system output texts for evaluating the pronoun translations. However, automatic alignments are noisy; Guillou and Hardmeier (2018) have shown that improvements using heuristics are not statistically significant. They also found low agreement between these measures and human judgments, primarily due to the possibility of many translation choices per pronoun. APT also uses a predetermined list of ‘equivalent pronouns’, obtained for specific pronouns based on a French grammar bo"
D19-1294,W18-6307,0,0.0656857,"Missing"
D19-1294,P02-1040,0,0.118067,"researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations of BLEU are well-known and have been discussed in detail in a recent study (Reiter, 2018). It has long been argued that as the quality of machine translation improves, there will be a singularity moment when existing evaluation measures would be unable to tell whether a given output was produced by a human or by a machine. Indeed, there have been recent claims that human parity has already been achieved (Hassan et al., 2018), but it has also been shown that it"
D19-1294,D14-1162,0,0.081598,"spect to the correct use of pronouns.3 In Section 3, we described how such datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be"
D19-1294,N18-1202,0,0.488144,"uch datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be a reference or a system translation. Figure 4: Our proposed frame"
D19-1294,D18-1179,0,0.201246,"uch datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be a reference or a system translation. Figure 4: Our proposed frame"
D19-1294,J18-3002,0,0.0185676,"ntial phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations of BLEU are well-known and have been discussed in detail in a recent study (Reiter, 2018). It has long been argued that as the quality of machine translation improves, there will be a singularity moment when existing evaluation measures would be unable to tell whether a given output was produced by a human or by a machine. Indeed, there have been recent claims that human parity has already been achieved (Hassan et al., 2018), but it has also been shown that it is easy to tell apart a human translation from a machine output when going beyond the sentence level (L¨aubli et al., 2018). Overall, it is clear that there is a need for machine translation evaluation measures that look bey"
D19-1294,W18-6437,0,0.051851,"Missing"
D19-1294,P18-1117,0,0.0597454,"d Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations o"
D19-1678,W16-5101,0,0.0189325,"ta and enormous progress in machine learning have opened up new possibilities for health care research. Monitoring patients in ICU is a challenging and high-cost task. Hence, 2 Related Work We provide a review of machine learning approaches for clinical prediction tasks. Biomedical natural language processing The recent developments in deep learning-based techniques for NLP have been adapted for clinical notes. Convolutional neural networks have been used to predict ICD codes from clinical texts (Mullenbach et al., 2018; Li et al., 2018; Gangavarapu et al., 2019). Rios and Kavuluru (2015) and Baker et al. (2016) used convolutional neural networks to classify various biomedical articles. Pre-trained word and sentence embeddings have also shown good results for sentence similarity tasks (Chen 6432 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6432–6437, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Doctor notes compliments measured physiological signals for better ICU management. 1. In-hospital Mortality: This is a binary classifica"
D19-1678,P16-5001,0,0.0390091,"orks have been models of choice for these recent works, with additional gains from using attention or multi-task learning (Song et al. (2018)). Xu et al. (2018) accommodated supplemental information like diagnosis, medications, and lab events to improve model performance. We use RNNs for modeling time-series in this work, utilizing the setup identical to Harutyunyan et al. (2019). Multi-modal learning has shown success in speech, natural language, and computer vision (Ngiam et al. (2011), Mao et al. (2014)). Recently, a lot of work has been done using images/videos with natural language text (Elliott et al. (2016)). We use a similar intuition for utilizing clinical notes with time-series data for ICU management tasks. In the next section, we define the three benchmark tasks we evaluate in this work. 3 Prediction Tasks We use the definitions of the benchmark tasks defined by Harutyunyan et al. (2019) as the following three problems: 2. Decompensation: Focus is to detect patients who are physiologically declining. Decompensation is defined as a sequential prediction task where the model has to predict at each hour after ICU admission. Target at each hour is to predict the mortality of the patient within"
D19-1678,N18-1100,0,0.0200143,"ngs, procedures, lab events, and clinical notes are recorded for reference. Availability of ICU data and enormous progress in machine learning have opened up new possibilities for health care research. Monitoring patients in ICU is a challenging and high-cost task. Hence, 2 Related Work We provide a review of machine learning approaches for clinical prediction tasks. Biomedical natural language processing The recent developments in deep learning-based techniques for NLP have been adapted for clinical notes. Convolutional neural networks have been used to predict ICD codes from clinical texts (Mullenbach et al., 2018; Li et al., 2018; Gangavarapu et al., 2019). Rios and Kavuluru (2015) and Baker et al. (2016) used convolutional neural networks to classify various biomedical articles. Pre-trained word and sentence embeddings have also shown good results for sentence similarity tasks (Chen 6432 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6432–6437, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Figure 1: Doctor notes compliments measured physiolo"
D19-1678,D14-1181,0,0.00256292,"tay. We trained the models using cross entropy (CE) loss defined as below. Lihm = CE(m, m) ˆ 1X CE(dt , dˆt ) Ldecom = T t 1X Llos = CE(lt , lˆt ) T t (2) LSTM h48 Xt N48 1D Convolution Based Feature Extractor FC z Embeddings Figure 2: Block diagram from the in-hospital mortality multi-modal network. 4.2 Multi-Modal Neural Network In our multimodal model, our goal is to improve the predictions by taking both the time series data xt and the doctor notes ni as input to the network. Convolutional Feature Extractor for Doctor Notes. As shown in Fig. 2, we adopt a convolutional approach similar to Kim (2014) to extract the textual features from the doctor’s notes. For a piece of clinical note N , our CNN takes the word embeddings e = (e1 , e2 , . . . , en ) as input and applies 1D convolution operations, followed by maxpooling over time to generate a p dimensional feature vector zˆ, which is fed to the fully connected layer along side the LSTM output from time series signal (described in the next paragraph) for further processing. From now onwards, we denote the 1D convolution over note N as zˆ = Conv1D(N ). Model for In-Hospital Mortality. This model takes the time series signals [xt ]Tt=1 and a"
D19-1678,D16-1076,0,0.0271703,"Missing"
J15-3002,W97-0703,0,0.675541,"Missing"
J15-3002,N07-1054,0,0.06076,"Missing"
J15-3002,A00-2018,0,0.26586,"Missing"
J15-3002,P05-1022,0,0.0217243,"DT. To cope with this limitation, we use the inferred (posterior) probabilities from our CRF parsing models in a probabilistic CKY-like bottom–up parsing algorithm (Jurafsky and Martin 2008), which is non-greedy and optimal. Furthermore, a simple modification of this parsing algorithm allows us to generate k-best (i.e., the k highest probability) parse hypotheses for each input text that could then be used in a reranker to improve over the initial ranking using additional (global) features of the discourse tree as evidence, a strategy that has been successfully explored in syntactic parsing (Charniak and Johnson 2005; Collins and Koo 2005). Third, most of the existing discourse parsers do not discriminate between intrasentential parsing (i.e., building the DTs for the individual sentences) and multisentential parsing (i.e., building the DT for the whole document). However, we argue that distinguishing between these two parsing conditions can result in more effective parsing. Two separate parsing models could exploit the fact that rhetorical relations 387 Computational Linguistics Volume 41, Number 3 are distributed differently intra-sententially versus multi-sententially. Also, they could independently ch"
J15-3002,N13-1136,0,0.0236685,"Missing"
J15-3002,P14-1085,0,0.0101241,"Missing"
J15-3002,J03-4003,0,0.122136,"Missing"
J15-3002,J05-1003,0,0.0266255,"Missing"
J15-3002,P98-1044,0,0.549926,"Missing"
J15-3002,P07-1033,0,0.0425522,"Missing"
J15-3002,P02-1057,0,0.0224856,"Missing"
J15-3002,P09-1075,0,0.0185577,"Missing"
J15-3002,C96-1058,0,0.204552,"also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and"
J15-3002,P12-1007,0,0.0128696,"like ours assigns a probability to every possible DT. The parsing algorithm then picks the most probable DTs. The existing discourse parsers (Marcu 1999; Soricut and Marcu 2003; Subba and Di-Eugenio 2009; Hernault et al. 2010) described in Section 2 use parsing models that disregard the structural interdependencies between the DT constituents. However, we hypothesize that, like syntactic parsing, discourse parsing is also a structured prediction problem, which involves predicting multiple variables (i.e., the structure and the relation labels) that depend on each other (Smith 2011). Recently, Feng and Hirst (2012) also found these interdependencies to be critical for parsing performance. To capture the structural dependencies between the DT constituents, CODRA uses undirected conditional graphical models (i.e., CRFs) as its parsing models. To find the most probable DT, unlike most previous studies (Marcu 1999; Subba and Di-Eugenio 2009; Hernault et al. 2010), which adopt a greedy solution, CODRA applies an optimal CKY parsing algorithm to the inferred posterior probabilities (obtained from the CRFs) of all possible DT constituents. Furthermore, the parsing algorithm allows CODRA to generate a list of k"
J15-3002,P14-1048,0,0.443102,"rendingerlab.net/hilda/. 391 Computational Linguistics Volume 41, Number 3 parsing, and both scenarios use a single uniform parsing model. Second, they take a greedy (i.e., sub-optimal) approach to construct a DT. Third, they disregard sequential dependencies between DT constituents. Furthermore, HILDA considers the structure and the labels of a DT separately. Our discourse parser CODRA, as described in the next section, addresses all these limitations. More recent work than ours also attempts to address some of the above-mentioned limitations of the existing discourse parsers. Similar to us, Feng and Hirst (2014) generate a document-level DT in two stages, where a multi-sentential parsing follows an intra-sentential one. At each stage, they iteratively use two separate linear-chain CRFs (Lafferty, McCallum, and Pereira 2001) in a cascade: one for predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsin"
J15-3002,P08-1109,0,0.0641963,"Missing"
J15-3002,P07-1062,0,0.561158,"Missing"
J15-3002,P03-1071,0,0.19933,"Missing"
J15-3002,I11-1120,0,0.0273248,"Missing"
J15-3002,D14-1027,1,0.887188,"Missing"
J15-3002,P14-1065,1,0.88264,"Missing"
J15-3002,D12-1108,0,0.0168877,"Missing"
J15-3002,W05-1506,0,0.0257676,"Missing"
J15-3002,P14-1002,0,0.474952,"se tree to get the vector representation for the EDU. Adjacent discourse units are then merged hierarchically to get the vector representations for the higher order discourse units. In every step, the merging is done using one binary (structure) and one multi-class (relation) classifier, each having a three-layer neural network architecture. The cost function for training the model is given by these two cascaded classifiers applied at different levels of the DT. Similar to our method, they use the classifier probabilities in a CKY-like parsing algorithm to find the global optimal DT. Finally, Ji and Eisenstein (2014) present a feature representation learning method in a shift–reduce discourse parser (Marcu 1999). Unlike DNNs, which learn non-linear feature transformations in a maximum likelihood model, they learn linear transformations of features in a max margin classification model. 3. Overview of Our Rhetorical Analysis Framework CODRA takes as input a raw text and produces a discourse tree that describes the text in terms of coherence relations that hold between adjacent discourse units (i.e., clauses, sentences) in the text. An example DT generated by an online demo of CODRA 392 Joty, Carenini, and N"
J15-3002,D12-1083,1,0.878223,"Missing"
J15-3002,P13-1048,1,0.84536,"Missing"
J15-3002,W14-3352,1,0.659695,"Missing"
J15-3002,P08-1026,0,0.0681691,"Missing"
J15-3002,P13-1160,0,0.045068,"Missing"
J15-3002,D14-1220,0,0.0643375,"predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl"
J15-3002,P14-1003,0,0.081651,"predicting the presence of rhetorical relations between adjacent discourse units in a sequence, and the other to predict the relation label between the two most probable adjacent units to be merged as selected by the previous CRF. While they use CRFs to take into account the sequential dependencies between DT constituents, they use them greedily during parsing to achieve efficiency. They also propose a greedy post-editing step based on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful appl"
J15-3002,W10-4327,0,0.24445,"Missing"
J15-3002,W14-3336,0,0.0224322,"Missing"
J15-3002,P95-1037,0,0.637605,"Missing"
J15-3002,P99-1047,0,0.527878,"tances stripped of their original discourse cues do not generalize well to implicit cases because they are linguistically quite different. Note that this approach to identifying discourse relations in the absence of manually labeled data does not fully solve the parsing problem (i.e., building DTs); rather, it only attempts to identify a small subset of coarser relations between two (flat) text segments (i.e., a tagging problem). Arguably, to perform a complete rhetorical analysis, one needs to use supervised machine learning techniques based on human-annotated data. 2.2 Supervised Approaches Marcu (1999) applies supervised machine learning techniques to build a discourse segmenter and a shift–reduce discourse parser. Both the segmenter and the parser rely on C4.5 decision tree classifiers (Poole and Mackworth 2010) to learn the rules automatically from the data. The discourse segmenter mainly uses discourse cues, shallowsyntactic (i.e., POS tags) and contextual features (i.e., neighboring words and their POS tags). To learn the shift–reduce actions, the discourse parser encodes five types of features: lexical (e.g., discourse cues), shallow-syntactic, textual similarity, operational (previous"
J15-3002,J00-3005,0,0.288845,"Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories"
J15-3002,P02-1047,0,0.224664,"Missing"
J15-3002,P07-1075,0,0.018781,"ther to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Representation Theory, which is dr"
J15-3002,P05-1012,0,0.0239027,"ased on an additional feature (i.e., depth of a discourse unit) to modify the initial DT, which gives them a significant gain in performance. In a different approach, Li et al. (2014) propose a discourse-level dependency structure to capture direct relationships between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse"
J15-3002,H05-1066,0,0.0645416,"Missing"
J15-3002,J91-1002,0,0.755208,"Missing"
J15-3002,P04-1035,0,0.00658881,"Missing"
J15-3002,P09-2004,0,0.0811038,"Missing"
J15-3002,prasad-etal-2008-penn,0,0.224284,"Missing"
J15-3002,P08-2062,0,0.0639658,"Missing"
J15-3002,N03-1028,0,0.198151,"Missing"
J15-3002,J02-4004,0,0.155312,"Missing"
J15-3002,P13-1045,0,0.0207601,"between EDUs rather than deep hierarchical relationships. They first create a discourse dependency treebank by converting the deep annotations in RST–DT to shallow head-dependent annotations between EDUs. To find the dependency parse (i.e., an optimal spanning tree) for a given text, they apply Eisner (1996) and Maximum Spanning Tree (McDonald et al. 2005) dependency parsing algorithms with the Margin Infused Relaxed Algorithm online learning framework (McDonald, Crammer, and Pereira 2005). With the successful application of deep learning to numerous NLP problems including syntactic parsing (Socher et al. 2013a), sentiment analysis (Socher et al. 2013b), and various tagging tasks (Collobert et al. 2011), a couple of recent studies in discourse parsing also use deep neural networks (DNNs) and related feature representation methods. Inspired by the work of Socher et al. (2013a, 2013b), Li, Li, and Hovy (2014) propose a recursive DNN for discourse parsing. However, as in Socher et al. (2013a, 2013b), word vectors (i.e., embeddings) are not learned explicitly for the task, rather they are taken from Collobert et al. (2011). Given the vectors of the words in an EDU, their model first composes them hiera"
J15-3002,D13-1170,0,0.00463377,"Missing"
J15-3002,N03-1030,0,0.577106,"Missing"
J15-3002,W04-3210,0,0.0680903,"Missing"
J15-3002,H05-1033,0,0.493898,"of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. Law. Computational Linguistics Volume 41, Number 3 text has a coherence structure (Halliday and Hasan 1976; Hobbs 1979), which logically binds its clauses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different vie"
J15-3002,W04-0213,0,0.0304719,"Missing"
J15-3002,N09-1064,0,0.0322482,"Missing"
J15-3002,J11-2001,0,0.00514583,"Missing"
J15-3002,J02-4002,0,0.140023,"uses and sentences together to express a meaning as a whole. Rhetorical analysis seeks to uncover this coherence structure underneath the text; this has been shown to be beneficial for many Natural Language Processing (NLP) applications, including text summarization and compression (Marcu 2000b; Daum´e and Marcu 2002; Sporleder and Lapata 2005; Louis, Joshi, and Nenkova 2010), text generation (Prasad et al. 2005), machine translation evaluation (Guzm´an et al. 2014a, 2014b; Joty et al. 2014), sentiment analysis (Somasundaran 2010; Lazaridou, Titov, and Sporleder 2013), information extraction (Teufel and Moens 2002; Maslennikov and Chua 2007), and question answering (Verberne et al. 2007). Furthermore, rhetorical structures can be useful for other discourse analysis tasks, including co-reference resolution using Veins theory (Cristea, Ide, and Romary 1998). Different formal theories of discourse have been proposed from different viewpoints to describe the coherence structure of a text. For example, Martin (1992) and Knott and Dale (1994) propose discourse relations based on the usage of discourse connectives (e.g., because, but) in the text. Asher and Lascarides (2003) propose Segmented Discourse Repres"
J15-3002,J05-2005,0,0.100595,"Missing"
J15-3002,P14-1019,0,0.0600359,"Missing"
J15-3002,J93-2004,0,\N,Missing
J15-3002,S07-1106,1,\N,Missing
J15-3002,C98-1044,0,\N,Missing
J17-4001,D14-1188,0,0.0242496,"et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, a"
J17-4001,P13-2068,0,0.126108,"ot until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gr"
J17-4001,W16-2302,0,0.0340023,"Missing"
J17-4001,J93-2003,0,0.0452676,"Missing"
J17-4001,W07-0718,0,0.047654,"d cohesion. In Section 4, we have suggested some simple ways to create such metrics, and we have also shown that they yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metri"
J17-4001,W08-0309,0,0.0358878,"y yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez 2007; Popovic"
J17-4001,W10-1703,0,0.0860047,"Missing"
J17-4001,W12-3102,0,0.454393,"tion on the training data set.5 Note that our approach to learn the interpolation weights is similar to the one used by PRO for tuning the relative weights of the components of a log-linear SMT model (Hopkins and May 2011). Unlike PRO, (i) we used human judgments, not automatic scores, and (ii) we trained on all pairs, not on a subsample. 3.3 Correlation Measures In our experiments, we only considered translation into English (as we had a discourse parser for English only), and we used the data described in Table 1. For evaluation, we followed the standard set-up of the Metrics task of WMT12 (Callison-Burch et al. 2012). For segment-level evaluation, we used Kendall’s τ (Kendall 1938), which can be 5 When fitting the model, we did not include a bias term, as this was harmful. 692 Joty et al. Discourse Structure in Machine Translation Evaluation calculated directly from the human pairwise judgments. For system-level evaluation, we used Spearman’s rank correlation (Spearman 1904) and, in some cases, also Pearson correlation (Pearson 1895), which are appropriate correlation measures as here we have vectors of scores. We measured the correlation of the evaluation metrics with the human judgments provided by the"
J17-4001,W09-0401,0,0.0707378,"Missing"
J17-4001,W11-2103,0,0.0547186,"Missing"
J17-4001,E06-1032,0,0.0857549,"Missing"
J17-4001,W09-2404,0,0.0325022,"Discourse in Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2"
J17-4001,W12-3156,0,0.0221975,"Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational L"
J17-4001,D07-1007,0,0.126996,"Missing"
J17-4001,W11-1211,0,0.0252185,"010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despit"
J17-4001,P07-1005,0,0.124229,"Missing"
J17-4001,P05-1033,0,0.0907183,"systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-senten"
J17-4001,D08-1024,0,0.100033,"Missing"
J17-4001,2003.mtsummit-papers.9,0,0.0284114,"mmunity. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm"
J17-4001,2003.mtsummit-papers.10,0,0.151421,"iciently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b;"
J17-4001,W11-2107,0,0.0349766,"ally significant improvements are marked with ∗∗ for p-value < 0.01. System D ISCO TKparty Best at WMT 14 FR - EN DE - EN HI - EN CS - EN RU - EN Overall 0.433∗∗ 0.417 +0.016 0.380∗∗ 0.345 +0.035 0.434 0.438 −0.004 0.328∗∗ 0.284 +0.044 0.355∗∗ 0.336 +0.019 0.386∗∗ 0.364 +0.024 the best performing metric both at the system level and at the segment level at the WMT08 and WMT09 metrics tasks. From the original ULC, we replaced M ETEOR by the four newer variants M ETEOR-ex (exact match), M ETEOR-st (+stemming), M ETEOR-sy (+synonymy lookup), and M ETEOR-pa (+paraphrasing) in A SIYA’s terminology (Denkowski and Lavie 2011). We also added to the mix TERp-A (a variant of TER with paraphrasing), BLEU, NIST, and R OUGE-W, for a total of 18 individual metrics. The metrics in this set use diverse linguistic information, including lexical-, syntactic-, and semantic-oriented individual metrics. Regarding the discourse metrics, we used five variants, including DR and DR-LEX described in Section 2, and three more constrained variants oriented to match words between trees only if they occur under the same substructure types (e.g., the same nuclearity type). These variants are designed by introducing structural modificatio"
J17-4001,2012.amta-papers.6,0,0.0150667,"a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovi"
J17-4001,N04-1035,0,0.0647715,"nformation (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual informa"
J17-4001,W07-0738,1,0.707228,"Missing"
J17-4001,W09-0440,1,0.876974,"Missing"
J17-4001,H93-1040,0,0.672723,"Missing"
J17-4001,W10-1750,1,0.808317,"Missing"
J17-4001,D11-1084,0,0.0630017,"Missing"
J17-4001,P12-3024,1,0.890983,"Missing"
J17-4001,E12-3001,0,0.0251663,"istency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. th"
J17-4001,W13-3302,0,0.0210552,"; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling"
J17-4001,W16-2345,1,0.883757,"Missing"
J17-4001,D14-1027,1,0.899067,"Missing"
J17-4001,P14-1065,1,0.865351,"Missing"
J17-4001,P15-1078,1,0.877827,"Missing"
J17-4001,W13-2252,0,0.0241376,"ch is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 2013). Thus, specialized evaluation measures have been proposed, for example, for the translation of discourse connectives (Hajlaoui and Popescu-Belis 2012; Meyer et al. 2012; Hajlaoui 2013) and for pronominal anaphora (Hardmeier and Federico 2010), among others. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few previous attempts to incorporate discourse information. One example includes the semantics-aware metrics of Gim´enez and M`arquez (2009) and Gim´enez et al. (2010), which used the Discourse Representation Theory (Kamp and Reyle 1993) and tree-based discourse representation structures (DRS) produced by a semantic parser. They calculated the similarity between the MT output and the references based on DRS subtree matching as defi"
J17-4001,2012.amta-caas14.1,0,0.0723287,"gt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT"
J17-4001,2010.iwslt-papers.10,0,0.146919,"esearch problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Har"
J17-4001,W15-2501,1,0.905816,"Missing"
J17-4001,D12-1108,0,0.0554427,"Missing"
J17-4001,D11-1125,0,0.347504,"es 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently tha"
J17-4001,D12-1083,1,0.888624,"Missing"
J17-4001,J15-3002,1,0.897197,"Missing"
J17-4001,W14-3352,1,0.889983,"Missing"
J17-4001,N03-1017,0,0.0235221,"Missing"
J17-4001,W07-0734,0,0.0592291,"the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of li"
J17-4001,W10-1737,0,0.0859654,"Missing"
J17-4001,D14-1220,0,0.0691207,"Missing"
J17-4001,P14-2047,0,0.0404142,"Missing"
J17-4001,W04-1013,0,0.011816,"ation groups: Group I contains our discourse-based evaluation metrics, DR, and DR-LEX. Group II includes the publicly available MT evaluation metrics that participated in the WMT12 metrics task, excluding those that did not have results for all language pairs (Callison-Burch et al. 2012). More precisely, they are SPEDE 07 P P, AMBER, M ETEOR, T ERROR C AT, SIMPBLEU, XE N E RR C ATS, W ORD B LOCK EC, B LOCK E RR C ATS, and POS F. Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (Papineni et al. 2002), NIST (Doddington 2002), R OUGE (Lin 2004), and TER (Snover et al. 2006). We calculated the metrics in this group using Asiya. In particular, we used the following Asiya versions of TER and R OUGE: TER P -A and ROUGE- W.8 For each metric in groups II and III, we present the system-level and segment-level results for the original metric as well as for the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results y"
J17-4001,W05-0904,0,0.34905,"post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sen"
J17-4001,W12-3129,0,0.0531251,"Missing"
J17-4001,P11-1023,0,0.0237776,"between constituency trees (Liu and Gildea 2005). In the semantic case, there are metrics that 16 A notable exception is the work of Tu, Zhou, and Zong (2013), who report up to 2.3 BLEU points of improvement for Chinese-to-English translation using an RST-based MT framework. 17 http://www.isi.edu/natural-language/mteval/. 18 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/. 712 Joty et al. Discourse Structure in Machine Translation Evaluation exploit the similarity over named entities, predicate–argument structures (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), or semantic frames (Lo and Wu 2011). Finally, there are metrics that combine several lexico-semantic aspects (Gim´enez and M`arquez 2010b). As we mentioned earlier, one problem with discourse-related MT research is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 20"
J17-4001,E14-1017,0,0.0578767,"that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the"
J17-4001,W13-2202,0,0.0551203,"Missing"
J17-4001,W14-3336,0,0.0496545,"Missing"
J17-4001,A00-2002,0,0.288346,"Missing"
J17-4001,P11-3009,0,0.0273654,"ang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the resear"
J17-4001,W13-3306,0,0.032096,"Missing"
J17-4001,W12-0117,0,0.023287,"Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate"
J17-4001,2012.amta-papers.20,0,0.112933,"u 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far"
J17-4001,W13-3303,0,0.0173794,"Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that c"
J17-4001,W13-2221,0,0.0481696,"Missing"
J17-4001,W13-3307,0,0.04636,"Missing"
J17-4001,P03-1021,0,0.159662,"Missing"
J17-4001,P02-1040,0,0.0997549,"been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has"
J17-4001,P09-2004,0,0.0245932,"al cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Mac"
J17-4001,popescu-belis-etal-2012-discourse,0,0.0540549,"Missing"
J17-4001,W07-0707,0,0.0583959,"2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sentence and between sentences in a text have not been"
J17-4001,prasad-etal-2008-penn,0,0.0680612,"man assessments; (ii) Different levels of discourse structure and relations provide different information, which shows smooth accumulative contribution to the final correlation score; (iii) Both discourse relations and nuclearity labels have sizeable impact on the evaluation metric, the latter being more important than the former. The last point emphasizes the appropriateness of the RST theory as a formalism for the discourse structure of texts. Contrary to other discourse theories (e.g., the Discourse Lexicalized Tree Adjoining Grammar [Webber 2004] used to build the Penn Discourse Treebank [Prasad et al. 2008]), RST accounts for the nuclearity as an important element of the discourse structure. 5.3 Qualitative Analysis of Good and Bad Translations In the previous two sections we provided a quantitative analysis of which discourse information has the biggest impact on the performance of our discourse-based measure (DR-LEX) and also which parts of the discourse trees help in distinguishing good from bad translations. In this section, we present some qualitative analysis by inspecting a 707 Computational Linguistics Volume 43, Number 4 WMT_2011 WMT_2012 WMT_2013 F1 measure 0.825 0.800 good bad 0.775"
J17-4001,P05-1034,0,0.192756,"Missing"
J17-4001,W03-0402,0,0.0123992,"urces of information in a more direct way. In that paper, we proposed a pairwise setting for learning MT evaluation metrics with preference tree kernels. The setting can incorporate syntactic and discourse information encapsulated in tree-based structures and the objective is to learn to differentiate better from worse translations by using all subtree structures as implicit features. The discourse parser we used is the same used in this article. The syntactic tree is mainly constructed using the Illinois chunker (Punyakanok and Roth 2001). The kernel used for learning is a preference kernel (Shen and Joshi 2003; Moschitti 2008), which decomposes into Partial Tree Kernel (Moschitti 2006) applications between pairs of enriched tree structures. Word unigram matching is also included in the kernel computation, thus being quite similar to DR-LEX. Table 8 shows the results obtained on the same WMT12 data set by using only discourse structures, only syntactic structures or both structures together. As we can see, the τ scores of the syntactic and the discourse variants are not very different (with a general advantage for syntax), but when put together there is a sizeable improvement in correlation for all"
J17-4001,2006.amta-papers.25,0,0.422386,"it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are u"
J17-4001,W15-3031,0,0.0560843,"Missing"
J17-4001,N15-2015,0,0.019606,"d Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that current automatic evaluation metrics such as B"
J17-4001,W12-4213,0,0.0584238,"Missing"
J17-4001,W10-2602,0,0.0143272,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,W10-1728,0,0.0292312,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,P13-2066,0,0.0534853,"Missing"
J17-4001,P14-1080,0,0.0440631,"Missing"
J17-4001,N12-1046,0,0.0614062,"Missing"
J17-4001,H05-1097,0,0.0561496,"creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and"
J17-4001,W12-2503,0,0.0257687,"ch had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu"
J17-4001,D07-1080,0,0.245176,"Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan"
J17-4001,D12-1097,0,0.0277845,"Missing"
J17-4001,N09-2004,0,0.0308979,"ive log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion an"
J17-4001,D13-1163,0,0.01503,"which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´a"
J17-4001,C00-2137,0,0.0572188,"r the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results yield statistically significant improvement over the original metric. Note that testing statistical significance is not trivial in our case because we have a complex correlation score for which the assumptions that standard tests make are not met. We thus resorted to a non-parametric randomization framework (Yeh 2000), which is commonly used in NLP research.9 4.1 System-Level Results Table 2 shows the system-level experimental results for WMT12. We can see that DR is already competitive by itself: On average, it has a correlation of 0.807, which is very close to the BLEU and the TER scores from group II (0.810 and 0.812, respectively). Moreover, DR yields improvements when combined with 13 of the 15 metrics, with a resulting correlation higher than those of the two individual metrics being combined. This fact suggests that DR contains information that is complementary to that used by most of the other metr"
J17-4001,W14-3302,0,\N,Missing
J18-4012,P06-1026,0,0.112174,"Missing"
J18-4012,D14-1226,0,0.325632,"Missing"
J18-4012,P08-1041,0,0.0460177,"Missing"
J18-4012,W04-3240,0,0.578691,"Missing"
J18-4012,P15-1033,0,0.0210032,"ection 6. 2. Related Work Three lines of research are related to our work: (i) compositionality with LSTM-RNNs, (ii) conditional structured models, and (iii) speech act recognition in asynchronous conversations. 2.1 LSTM-RNNs for Composition RNNs are arguably the most popular deep learning models in natural language processing, where they have been used for both encoding and decoding a text—for example, language modeling (Mikolov 2012a; Tran, Zukerman, and Haffari 2016), machine translation (Bahdanau, Cho, and Bengio 2015), summarization (Rush, Chopra, and Weston 2015), and syntactic parsing (Dyer et al. 2015). RNNs have also been used as a sequence tagger, as in opinion mining (Irsoy and Cardie 2014; Liu, Joty, and Meng 2015), named entity recognition (Lample et al. 2016), and part-of-speech tagging (Plank, Søgaard, and Goldberg 2016). Relevant to our implementation, Kalchbrenner and Blunsom (2013) use a simple RNN to model sequential dependencies between act types for speech act recognition in phone conversations. They use a convolutional neural network (CNN) to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but they also experiment with RNNs t"
J18-4012,N06-1027,0,0.594946,"Missing"
J18-4012,E12-1079,0,0.0584892,"Missing"
J18-4012,P08-1109,0,0.0917028,"Missing"
J18-4012,P11-2008,0,0.0711089,"Missing"
J18-4012,D14-1080,0,0.0325607,"onality with LSTM-RNNs, (ii) conditional structured models, and (iii) speech act recognition in asynchronous conversations. 2.1 LSTM-RNNs for Composition RNNs are arguably the most popular deep learning models in natural language processing, where they have been used for both encoding and decoding a text—for example, language modeling (Mikolov 2012a; Tran, Zukerman, and Haffari 2016), machine translation (Bahdanau, Cho, and Bengio 2015), summarization (Rush, Chopra, and Weston 2015), and syntactic parsing (Dyer et al. 2015). RNNs have also been used as a sequence tagger, as in opinion mining (Irsoy and Cardie 2014; Liu, Joty, and Meng 2015), named entity recognition (Lample et al. 2016), and part-of-speech tagging (Plank, Søgaard, and Goldberg 2016). Relevant to our implementation, Kalchbrenner and Blunsom (2013) use a simple RNN to model sequential dependencies between act types for speech act recognition in phone conversations. They use a convolutional neural network (CNN) to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but they also experiment with RNNs to compose sentence representations. Similarly, Khanpour, Guntakandla, and Nielsen (2016) use"
J18-4012,D09-1130,0,0.608544,"Missing"
J18-4012,N16-1037,0,0.0459399,"Missing"
J18-4012,P16-1165,1,0.690992,"tility of speech acts in downstream NLP tasks involving asynchronous conversations like next utterance ranking (Lowe et al. 2015), conversation generation (Ritter, Cherry, and Dolan 2010), and summarization (Murray, Carenini, and Ng 2010). Finally, we hope that the new corpus, the conversational word embeddings, and the source code that we have made publicly available in this work will facilitate other researchers in extending our work and in applying speech act models to their NLP tasks. Bibliographic Note Portions of this work were previously published in the ACL-2016 conference proceeding (Joty and Hoque 2016). This article significantly extends the published work in several ways, most notably: (i) we train new word2vec and Glove word embeddings based on a large conversational corpus, and show their effectiveness by comparing with off-the-shelf word embeddings (Section 4.2 and the Results section), (ii) we extend the LSTM-RNN for domain adaptation using adversarial training of neural networks (Section 3.2), (iii) we evaluate the domain adapted LSTM-RNN model on meeting and forum data sets (Section 5.2), and (iv) we train and evaluate CRFs based on sentence embeddings extracted from the adapted LSTM"
J18-4012,P18-1052,1,0.84343,"Missing"
J18-4012,W13-3214,0,0.0242234,"learning models in natural language processing, where they have been used for both encoding and decoding a text—for example, language modeling (Mikolov 2012a; Tran, Zukerman, and Haffari 2016), machine translation (Bahdanau, Cho, and Bengio 2015), summarization (Rush, Chopra, and Weston 2015), and syntactic parsing (Dyer et al. 2015). RNNs have also been used as a sequence tagger, as in opinion mining (Irsoy and Cardie 2014; Liu, Joty, and Meng 2015), named entity recognition (Lample et al. 2016), and part-of-speech tagging (Plank, Søgaard, and Goldberg 2016). Relevant to our implementation, Kalchbrenner and Blunsom (2013) use a simple RNN to model sequential dependencies between act types for speech act recognition in phone conversations. They use a convolutional neural network (CNN) to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but they also experiment with RNNs to compose sentence representations. Similarly, Khanpour, Guntakandla, and Nielsen (2016) use an LSTM-based RNN to compose sentence representations. Ji, Haffari, and Eisenstein (2016) propose a latent variable RNN that can jointly model sequences of words (i.e., language modeling) and discourse"
J18-4012,C16-1189,0,0.0390904,"Missing"
J18-4012,N16-1030,0,0.275984,"act recognition in asynchronous conversations. 2.1 LSTM-RNNs for Composition RNNs are arguably the most popular deep learning models in natural language processing, where they have been used for both encoding and decoding a text—for example, language modeling (Mikolov 2012a; Tran, Zukerman, and Haffari 2016), machine translation (Bahdanau, Cho, and Bengio 2015), summarization (Rush, Chopra, and Weston 2015), and syntactic parsing (Dyer et al. 2015). RNNs have also been used as a sequence tagger, as in opinion mining (Irsoy and Cardie 2014; Liu, Joty, and Meng 2015), named entity recognition (Lample et al. 2016), and part-of-speech tagging (Plank, Søgaard, and Goldberg 2016). Relevant to our implementation, Kalchbrenner and Blunsom (2013) use a simple RNN to model sequential dependencies between act types for speech act recognition in phone conversations. They use a convolutional neural network (CNN) to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but they also experiment with RNNs to compose sentence representations. Similarly, Khanpour, Guntakandla, and Nielsen (2016) use an LSTM-based RNN to compose sentence representations. Ji, Haffari, and E"
J18-4012,N16-1062,0,0.0318447,"Missing"
J18-4012,D15-1278,0,0.0225357,"ce technique similar to the forward pass of the traditional forward-backward inference algorithm to improve upon the greedy decoding methods typically used in the RNN-based sequence labeling models. Vinyals and Le (2015) and Serban et al. (2016) use RNN-based encoderdecoder framework for conversation modeling. Vinyals and Le (2015) use a single RNN to encode all the previous utterances (i.e., by concatenating the tokens of previous utterances), whereas Serban et al. (2016) use a hierarchical encoder—one to encode the words in each utterance, and another to connect the encoded context vectors. Li et al. (2015) compare recurrent neural models with recursive (syntax-based) models for several NLP tasks and conclude that recurrent models perform on par with the recursive for most tasks (or even better). For example, recurrent models outperform 863 Computational Linguistics Volume 44, Number 4 recursive on sentence level sentiment classification. This finding motivated us to use recurrent models rather than recursive ones. 2.2 Conditional Structured Models There has been an explosion of interest in CRFs for solving structured output problems in NLP; see Smith (2011) for an overview. The most common type"
J18-4012,D15-1168,1,0.8643,"Missing"
J18-4012,D15-1178,0,0.204843,"Missing"
J18-4012,W15-4640,0,0.025758,"k, we are developing coherence models for asynchronous conversations (Nguyen and Joty 2017; Joty, Mohiuddin, and Tien Nguyen 2018). Such coherence models could be useful for a number of downstream tasks including next utterance (or comment) ranking, conversation generation, and thread reconstruction (Nguyen et al. 2017). We are now looking into whether speech act information can help us in building better coherence models for asynchronous conversations. We also plan to evaluate the utility of speech acts in downstream NLP tasks involving asynchronous conversations like next utterance ranking (Lowe et al. 2015), conversation generation (Ritter, Cherry, and Dolan 2010), and summarization (Murray, Carenini, and Ng 2010). Finally, we hope that the new corpus, the conversational word embeddings, and the source code that we have made publicly available in this work will facilitate other researchers in extending our work and in applying speech act models to their NLP tasks. Bibliographic Note Portions of this work were previously published in the ACL-2016 conference proceeding (Joty and Hoque 2016). This article significantly extends the published work in several ways, most notably: (i) we train new word2"
J18-4012,P16-1101,0,0.0405683,"Missing"
J18-4012,N13-1090,0,0.0362971,"use a linear kernel with the default C value of 1.0. (e) MEskip-thought : We encode each sentence with the skip-thought encoder of Kiros et al. (2015). The skip-thought model uses an encoder-decoder framework to learn the sentence representation in a task-agnostic (unsupervised) way. It encodes each sentence with a GRU-RNN (Cho et al. 2014), and uses the encoded vector to decode the words of the neighboring sentences using another GRU-based RNN as a language model. The model is originally trained on the BookCorpus16 with a vocabulary size of 20K words. It then uses the CBOW word2vec vectors (Mikolov et al. 2013a) to expand the vocabulary size to 930,911 words. Following the recommendation from 14 For simplicity, we excluded the bias vectors from our computation. 15 SVM training with linear kernel did not scale to the concatenated vector. 16 http://yknzhu.wixsite.com/mbweb. 883 Computational Linguistics Volume 44, Number 4 the authors, we use the combine-skip model that concatenates the vectors encoded by a uni-directional encoder (uni-skip) and a bi-directional encoder (bi-skip). The resulting vectors are of 4,800 dimensions—the first 2,400 dimensions is the uniskip vector, and the last 2,400 dimens"
J18-4012,N06-1047,0,0.0353129,"asks a question in the third sentence. Other participants respond to the query by suggesting something or asking for clarification. In this process, the participants get into a conversation by taking turns, each of which consists of one or more speech acts. The two-part structures across posts like question-answer and request-grant are called adjacency pairs (Schegloff 1968). Identification of speech acts is an important step toward deep conversational analysis (Bangalore, Di Fabbrizio, and Stent 2006), and has been shown to be useful in many downstream applications, including summarization (Murray et al. 2006; McKeown, Shrestha, and Rambow 2007), question answering (Hong and Davison 2009), collaborative task learning agents (Allen et al. 2007), artificial companions for people to use the Internet (Wilks 2006), and flirtation detection in speed-dates (Ranganath, Jurafsky, and McFarland 2009). Availability of large annotated corpora like the Meeting Recorder Dialog Act (MRDA) (Dhillon et al. 2004) or the Switchboard-DAMSL (SWBD) (Jurafsky, Shriberg, and Biasca 1997) corpus has fostered research in data-driven automatic speech act recognition in synchronous domains like meeting and phone conversation"
J18-4012,P17-1121,1,0.739213,"ond, we would also like to apply our models to conversations, where the graph structure is extractable using metadata or other clues, for example, the fragment quotation graphs for e-mail threads (Carenini, Ng, and Zhou 2008). One interesting future work would be to jointly model the conversational structure (e.g., reply-to structure) and the speech acts so that the two tasks can inform each other. In another direction, we would like to evaluate our speech act recognition model on extrinsic tasks. In a separate thread of work, we are developing coherence models for asynchronous conversations (Nguyen and Joty 2017; Joty, Mohiuddin, and Tien Nguyen 2018). Such coherence models could be useful for a number of downstream tasks including next utterance (or comment) ranking, conversation generation, and thread reconstruction (Nguyen et al. 2017). We are now looking into whether speech act information can help us in building better coherence models for asynchronous conversations. We also plan to evaluate the utility of speech acts in downstream NLP tasks involving asynchronous conversations like next utterance ranking (Lowe et al. 2015), conversation generation (Ritter, Cherry, and Dolan 2010), and summariza"
J18-4012,W14-4318,0,0.55426,"corpora are not available in the asynchronous domains, and many of the existing (small-sized) corpora use task-specific speech act tagsets (Cohen, Carvalho, and Mitchelle 2004; Ravi and Kim 2007; Bhatia, Biyani, and Mitra 2014) as opposed to a standard one. The unavailability of large annotated data sets with standard tagsets is one of the reasons for speech act recognition not getting much attention in asynchronous domains. Previous attempts in automatic (sentence-level) speech act recognition in asynchronous conversations (Jeong, Lin, and Lee 2009; Qadir and Riloff 2011; Tavafi et al. 2013; Oya and Carenini 2014) suffer from at least one of the following two technical limitations. First, they use a bag-of-words (BOW) representation (e.g., unigram, bigram) to encode lexical information of a sentence. However, consider the Suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act for an utterance. Furthermore, BOW representation could be quite sparse, and may not generalize well when used in classification models. Recent research suggests that a condensed distributed representation learne"
J18-4012,D12-1009,0,0.215901,"model to define the emission distribution, their simple HMM model tends to find some topical clusters in addition to the clusters that are based on speech acts. The HMM+Topic model tries to separate the act indicators from the topic words. By visualizing the type of conversations found by the two models, they show that the output of the HMM+Topic model is more interpretable than that of the HMM one; however, their classification accuracy is not empirically evaluated. Therefore, it is not clear whether these models are actually useful, and which of the two models is a better speech act tagger. Paul (2012) proposes using a mixed membership Markov model to cluster sentences based on their speech acts, and show that this model outperforms a simple HMM. Joty, Carenini, and Lin (2011) propose unsupervised models for speech 865 Computational Linguistics Volume 44, Number 4 act recognition in e-mail and forum conversations. They propose a HMM+Mix model to separate out the topic indicators. By training their model based on a conversational structure, they demonstrate that conversational structure is crucial to learning a better speech act recognition model. In our work, we also demonstrate that conver"
J18-4012,D14-1162,0,0.08025,"Missing"
J18-4012,P16-2067,0,0.025542,"Missing"
J18-4012,D11-1069,0,0.0667658,"Missing"
J18-4012,D09-1035,0,0.0484945,"Missing"
J18-4012,N10-1020,0,0.268999,"Missing"
J18-4012,D15-1044,0,0.126759,"Missing"
J18-4012,D15-1036,0,0.0430926,"Missing"
J18-4012,C04-1128,0,0.0812952,"parent and children in the thread structure of the e-mail conversation. Our approach is similar in spirit to their approach with three crucial differences: (i) our CRFs are globally normalized to surmount the label bias problem, while their classifiers are normalized locally; (ii) the graph structure of the conversation is given in their case, which is not the case with ours; and (iii) their approach works at the comment level, whereas we work at the sentence level. Identification of adjacency pairs like question-answer pairs in e-mail discussions using supervised methods was investigated in Shrestha and McKeown (2004) and Ravi and Kim (2007). Ferschke, Gurevych, and Chebotar (2012) use speech acts to analyze the collaborative process of editing Wiki pages, and apply supervised models to identify the speech acts in Wikipedia Talk pages. Other sentence-level approaches use supervised classifiers and sequence taggers (Qadir and Riloff 2011; Tavafi et al. 2013; Oya and Carenini 2014). Vosoughi and Roy (2016) trained off-the-shelf classifiers (e.g., SVM, naive Bayes, Logistic Regression) with syntactic (e.g., punctuations, dependency relations, abbreviations) and semantic feature sets (e.g., opinion words, vulg"
J18-4012,J00-3003,0,0.61193,"Missing"
J18-4012,D17-1283,0,0.0432339,"Missing"
J18-4012,W13-4017,1,0.916025,"However, such large corpora are not available in the asynchronous domains, and many of the existing (small-sized) corpora use task-specific speech act tagsets (Cohen, Carvalho, and Mitchelle 2004; Ravi and Kim 2007; Bhatia, Biyani, and Mitra 2014) as opposed to a standard one. The unavailability of large annotated data sets with standard tagsets is one of the reasons for speech act recognition not getting much attention in asynchronous domains. Previous attempts in automatic (sentence-level) speech act recognition in asynchronous conversations (Jeong, Lin, and Lee 2009; Qadir and Riloff 2011; Tavafi et al. 2013; Oya and Carenini 2014) suffer from at least one of the following two technical limitations. First, they use a bag-of-words (BOW) representation (e.g., unigram, bigram) to encode lexical information of a sentence. However, consider the Suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act for an utterance. Furthermore, BOW representation could be quite sparse, and may not generalize well when used in classification models. Recent research suggests that a condensed distribut"
J18-4012,N16-1090,0,0.063353,"Missing"
J18-4012,D17-1229,0,0.0458604,"Missing"
J18-4012,P10-1040,0,0.119676,"Missing"
J18-4012,W12-0603,0,0.490296,"Missing"
J18-4012,D14-1179,0,\N,Missing
K17-1024,abdelali-etal-2014-amara,0,0.035599,"Missing"
K17-1024,S16-1081,0,0.0323627,"els. More relevant to our work is the work of Ganin et al. (2016), who proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target. Here, we use adversarial training to learn task-specific representations in a cross-language setting, which is novel for this task, to the best of our knowledge. Question-question similarity was part of Task 3 on cQA at SemEval-2016/2017 (Nakov et al., 2016b, 2017); there was also a similar subtask as part of SemEval-2016 Task 1 on Semantic Textual Similarity (Agirre et al., 2016). Question-question similarity is an important problem with application to question recommendation, question duplicate detection, community question answering, and question answering in general. Typically, it has been addressed using a variety of textual similarity measures. Some work has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another importan"
K17-1024,P15-2114,0,0.0838445,"rk has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another important aspect is syntactic structure, e.g., Wang et al. (2009) proposed a retrieval model for finding similar questions based on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) us"
K17-1024,P03-1003,0,0.017696,"ity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English)"
K17-1024,P15-1078,1,0.559126,"Missing"
K17-1024,P16-2075,1,0.644004,"Missing"
K17-1024,D15-1068,1,0.173955,"Missing"
K17-1024,N16-1084,1,0.911664,"Missing"
K17-1024,N16-1153,1,0.473014,"Missing"
K17-1024,W15-1521,0,0.0266254,"of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2: Architecture of CLANN for the question to question similarity problem in cQA. The network then models the interactions between the input embeddings by passing them th"
K17-1024,S16-1136,1,0.400568,"dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) numbe"
K17-1024,N13-1090,0,0.139962,"ify any test example {qn , qn,k n in Arabic. This scenario is of practical importance, e.g., when an Arabic speaker wants to query the system in Arabic, and the database of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2:"
K17-1024,S17-2003,1,0.046657,"Missing"
K17-1024,D16-1165,1,0.895205,"Missing"
K17-1024,S16-1083,1,0.894404,"Missing"
K17-1024,S15-2036,1,0.775796,"Missing"
K17-1024,P02-1040,0,0.117239,"to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English p"
K17-1024,W16-1403,0,0.021532,"etup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA), when the input question can be either in English or in Arabic, and the questions it is compared to are always in English. We start with a simple language-independent representation based on cross-language word embedd"
K17-1024,P07-1059,0,0.0110691,"that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved qu"
K17-1024,P13-1045,0,0.0193858,"nslating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) number of single/double/ triple exclamation/interrogation symbols; (8) number of interr"
K17-1024,J11-2003,0,0.0117402,"r question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved questions {qk0 }K k=1 written in a source language (e.g., English) a"
K17-1024,tiedemann-2012-parallel,0,0.00637833,"ith the goal to rerank the ten related English questions. As an example, this is the Arabic translation of the original English question from Figure 1:    K Éë ; à AË@ @ YîE. àñÊª®K @ XAÓ ? HAJ Ó@Q»B@ àñ¢ª . Jj . K AÓ èñ¯ ñë AÓ , ÑªK éK . Ag. B@ I KA¿ @ X@ ? éKñJ . ú¯ QK , AKXCK . éJÖ Ï AK. 20 úÍ@ 15 áÓ We further collected 221 additional original questions and 1,863 related questions as unlabeled data, and we got the 221 English questions translated to Arabic.2 4.2 Cross-language Embeddings We used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters descri"
K17-1024,D16-1055,0,0.100723,"neural network (CLANN) model over a strong nonadversarial system. 1 Introduction Developing natural language processing (NLP) systems that can work indistinctly with different input languages is a challenging task; yet, such a setup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA"
K17-1024,P16-1157,0,0.018411,"used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters described in (Upadhyay et al., 2016), with a context window of size 5 and iterating for 5 epochs. We then compute the representation for a question by averaging the embedding vectors of the words it contains. Using these cross-language embeddings allows us to compare directly representations of an Arabic or an English input question q to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation betwee"
K17-1024,2006.amta-papers.25,0,0.0426642,"atures In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained o"
K17-1024,P11-1066,0,0.013245,"sed on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Bril"
N13-1018,P12-3014,0,0.101141,"e chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa"
N13-1018,N06-1046,0,0.0173089,"imal”, or “giggle” and “chuckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (c"
N13-1018,P11-1062,0,0.568264,"acted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in ph2 , and vice-versa (the “unknown” cases i"
N13-1018,P07-1069,0,0.0201103,"extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Moreover, to generate a lab"
N13-1018,W00-1425,0,0.0099463,"ckle” can be replaced by “laugh”. The motivation behind the generalization step is to enrich the common terms between the phrases in favor of increasing the chance that they could merge to a single phrase. This also helps to move beyond the limitation of original lexical choices. 2.3.2 Phrase merging The goal is to merge the phrases that are connected, and to generate a human readable phrase that contains more information than a single extracted phrase. Several approaches have been proposed to aggregate and merge sentences in Natural Language Generation (NLG) (e.g. (Barzilay and Lapata, 2006; Cheng and Mellish, 2000)), however most of them use syntactic structure of the sentences. To merge phrases at the lexical level, we set few common linguistically motivated aggregation patterns such as: simple conjunction, and conjunction via shared participants (Reiter and Dale, 2000). Table 2 demonstrates the merging patterns, where wij is the jth word (or segment) in phrase i, cw is the common word (or segment) in both phrases and CP OS is the common part-of-speech tag of the corresponding word. To illustrate, pattern 1 looks for the first segment of each phrase (wi1 ). If they are same (cwi1 ) and share the same P"
N13-1018,W04-3205,0,0.0376809,"for each pair of phrases. In order to adapt the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind u"
N13-1018,W10-1751,0,0.0329095,"t the similarity scores to the entailment score, we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is tha"
N13-1018,D10-1038,1,0.842685,"tional datasets. Our interest in dealing with conversational texts derives from two reasons. First, the huge amount of textual data generated everyday in these conversations validates the need of text analysis frameworks to process such conversational texts effectively. Second, conversational texts pose challenges to the traditional techniques, including redundancies, disfluencies, higher language variabilities and ill-formed sentence structure (Liu et al., 2011). Our conversational datasets are from two different asynchronous media: email and blog. For email, we use the dataset presented in (Joty et al., 2010), where three individuals annotated the publicly available BC3 email corpus (Ulrich et al., 2008) with topics. The corpus contains 40 email threads (or conversations) at an average of 5 emails per thread. On average it has 26.3 sentences and 2.5 topics per thread. A topic has an average length of 12.6 sentences. In total, the three annotators found 269 topics in a cor184 pus of 1,024 sentences. There are no publicly available blog corpora annotated with topics. For this study, we build our own blog corpus containing 20 blog conversations of various lengths from Slashdot, each annotated with to"
N13-1018,C10-1065,0,0.0588898,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,S10-1004,0,0.127914,"the following sections. 2.1 Phrase extraction We tokenize and preprocess each cluster in the collection of topic clusters with lemmas, stems, part-ofspeech tags, sense tags and chunks. We also extract n-grams up to length 5 which do not start or end with a stop word. In this phase, we do not include any frequency count feature in our candidate extraction pipeline. Once we have built the candidates pool, the next step is to identify a subset containing the most significant of those candidates. Since most top systems in key phrase extraction use supervised approaches, we follow the same method (Kim et al., 2010b; Medelyan et al., 2008; Frank et al., 1999). Initially, we consider a set of features used in the other systems to determine whether a phrase is likely to be a key phrase. However, since our dataset is conversational (more details in Section 3), and the text segments are not long, we aim for a classifier with high recall. Thus, we only use TFxIDF (Salton and McGill, 1986), position of the first occurrence (Frank et al., 1999) and phrase length as our features. We merge the training and test data released for SemEval-2010 Task #5 (Kim et al., 2010b), which consists of 244 scientific articles"
N13-1018,W07-2324,0,0.0140014,"ng. We motivate our approach by applying over conversational data, and show that our framework improves performance significantly over baseline algorithms. 1 Table 1: Topic labeling example. Introduction Given text segments about the same topic written in different ways (i.e., language variability), topic labeling deals with the problem of automatically generating semantically meaningful labels for those text segments. The potential of integrating topic labeling as a prerequisite for higher-level analysis has been reported in several areas, such as summarization (Harabagiu and Lacatusu, 2010; Kleinbauer et al., 2007; Dias et al., 2007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Me"
N13-1018,P11-1154,0,0.028743,"007), information extraction (Allan, 2002) and conversation visualization (Liu et al., 2012). Moreover, the huge amount of textual data generated everyday specifically in conversations (e.g., emails and blogs) calls for automated methods to analyze and re-organize them into meaningful coherent clusters. Table 1 shows an example of two human written topic labels for a topic cluster collected from a blog1 , 1 http://slashdot.org and possible phrases that can be extracted from the topic cluster using different approaches. This example demonstrates that although most approaches (Mei et al., 2007; Lau et al., 2011; Branavan et al., 2007) advocate extracting phrase-level topic labels from the text segments, topically related text segments do not always contain one keyword or key phrase that can capture the meaning of the topic. As shown in this example, such labels do not exist in the original text and cannot be extracted using the existing probabilistic models (e.g., (Mei et al., 2007)). The same problem can be observed with many other examples. This suggests the idea of aggregating and generating topic labels, instead of simply extracting them, as a challenging scenario for this field of research. Mor"
N13-1018,N10-1045,1,0.855593,"keywords. We add those chunks to our extracted phrases and eliminate the associated keywords. 2.2 Entailment graph So far, we have extracted a pool of key phrases from each topic cluster. Many such phrases include redundant information which are semantically equivalent but vary in lexical choices. By identifying the semantic relations between the phrases we can discover the information in one phrase that is semantically equivalent, novel, or more/less informative with respect to the content of the other phrase. We set this problem as a variant of the Textual Entailment (TE) recognition task (Mehdad et al., 2010b; Adler et al., 2012; Berant et al., 2011). We build an entailment graph for each topic cluster, where nodes are the extracted phrases and edges are the entailment relations between nodes. Given two phrases (ph1 and ph2 ), we aim at identifying and handling the following cases: i) ph1 and ph2 express the same meaning (bidirectional entailment). In such cases one of the phrases should be eliminated; ii) ph1 is more informative than ph2 (unidirectional entailment). In such cases, the entailing phrase should replace or complement the entailed one; iii) ph1 contains facts that are not present in"
N13-1018,P11-1134,1,0.874875,"we normalize the similarity scores by the length of ph2 (in terms of lexical items), when checking the entailment direction from ph1 to ph2 . In this way, we can check the portion of information/facts in ph2 which is covered by ph1 . The first 5 scores are computed based on the exact lexical overlap between the phrases: word overlap, edit distance, ngram-overlap, longest common subsequence and Lesk (Lesk, 1986). The other scores were computed using lexical resources: WordNet (Fellbaum, 1998), VerbOcean (Chklovski and Pantel, 2004), paraphrases (Denkowski and Lavie, 2010) and phrase matching (Mehdad et al., 2011). We used WordNet to compute the word similarity as the least common subsumer between two words considering the synonymy-antonymy, hypernymyhyponymy, and meronymy relations. Then, we calculated the sentence similarity as the sum of the sim182 ilarity scores of the word pairs in Text and Hypothesis, normalized by the number of words in Hypothesis. We also use phrase matching features described in (Mehdad et al., 2011) which consists of phrasal matching at the level on ngrams (1 to 5 grams). The rationale behind using different entailment features is that combining various scores will yield a be"
N13-1018,P12-2024,1,0.873874,"tiveness of our feature set for this lexical entailment model. The reason that we gained a very high accuracy is because our selected sentences are a subset of RTE6 and RTE7 with a shorter length (less number of words) which makes the entailment recognition task much easier than recognizing entailment between paragraphs or complex long sentences. 2.2.3 Graph edge labeling We set the edge labeling problem as a two-way classification task. Two-way classification casts multidirectional entailment as a unidirectional problem, where each pair is analyzed checking for entailment in both directions (Mehdad et al., 2012). In this condition, each original test example is correctly classified if both pairs originated from it are correctly judged (“YES-YES” for bidirectional,“YESNO” and “NO-YES” for unidirectional entailment and “NO-NO” for unknown cases). Two-way classification represents an intuitive solution to capture multidimensional entailment relations. Moreover, since our training examples are labeled with binary judgments, we are not able to train a three-way classifier. 2.2.4 Identification and selection Assigning all entailment relations between the extracted phrase pairs, we are aiming at identifying"
N13-1018,D11-1062,1,0.854152,"the following three steps. 2.2.1 Training set collection In the last few years, TE corpora have been created and distributed in the framework of several evaluation campaigns, including the Recognizing Textual Entailment (RTE) Challenge3 and Crosslingual textual entailment for content synchronization4 (Negri et al., 2012). However, such datasets cannot directly support our application. Specifically, our entailment graph is built over the extracted phrases (with max. length of 5 tokens per phrase), while the RTE datasets are composed of longer sentences and paragraphs (Bentivogli et al., 2009; Negri et al., 2011). In order to collect a dataset which is more similar to the goal of our entailment framework, we decide to select a subset of the sixth and seventh RTE challenge main task (i.e., RTE within a Corpus). Our 3 4 http://pascallin.ecs.soton.ac.uk/Challenges/RTE/ http://www.cs.york.ac.uk/semeval-2013/task8/ dataset choice is based on the following reasons: i) the length of sentence pairs in RTE6 and RTE7 is shorter than the others, and ii) RTE6 and RTE7 main task datasets are originally created for summarization purpose which is closer to our work. We sort the RTE6 and RTE7 dataset pairs based on t"
N13-1018,S12-1053,1,0.898214,"Missing"
N13-1018,C00-2137,0,0.0330731,"ggregation Extraction+Entailment+ Aggregation 12.2 15.1 15.6 18.5 30.8 35.5 33.3 37.6 17.9 20.4 38.7 41.6 ing R-f1 and Sem-R-f1 metrics suggests the need for more flexible automatic evaluation methods for this task. Moreover, although the same trend of improvement is observed in blog and email corpora, the differences between their performance suggest the investigation of specialized methods for various conversational modalities. Table 4: Results for candidate topic labels on blog and email corpora. 8 The statistical significance tests was calculated by approximate randomization described in (Yeh, 2000). 186 0.8 Sem-R-f1 icantly8 in both datasets. On the blog corpus, our key phrase extraction method (Extraction-BL) fails to beat the other baselines (Lead-BL and Freq-BL) in majority of cases (except R-f1 for Lead-BL). However, in the email dataset, it improves the performance over both baselines in both evaluation metrics. This might be due to the shorter topic clusters (in terms of number of sentences) in email corpus which causes a smaller number of phrases to be extracted. We also observe the effectiveness of the aggregation phase. In all cases, there is a significant improvement (p &lt; 0.05"
N13-1018,W04-3252,0,\N,Missing
N13-1018,N10-1146,1,\N,Missing
N13-1018,W07-1401,0,\N,Missing
N16-1084,P11-1151,0,0.145112,"ize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored"
N16-1084,W02-1001,0,0.414237,"inference is intractable for general graphs, i.e., graphs with loops. Despite this, it has been advocated by Pearl (1988) to use BP in loopy graphs as an approximation scheme; see also (Murphy, 2012), page 768. The algorithm is then called “loopy” BP, or LBP. Although LBP gives approximate solutions for general graphs, it often works well in practice (Murphy et al., 1999), outperforming other methods such as mean field (Weiss, 2001) and graph-cut (Burfoot et al., 2011). It is important to mention that the approach presented above (i.e., subsection 3.1) is similar in spirit to the approach of Collins (2002), Carreras and M`arquez (2003) and Punyakanok et al. (2005). The main difference is that they use a Perceptron-like online algorithm, where the updates are done based on the best label configuration (i.e., argmaxy p(y|x, θ)) rather than the marginals. One can use graph-cut (applicable only for binary output variables) or max-product LBP for the decoding task. However, this yields a discontinuous estimate (even with averaged perceptron) for the gradient (see Section 5). For the same reason, we use sum-product LBP rather than max-product LBP. 707 3.2 A Joint Model with Global Normalization Altho"
N16-1084,P15-2114,0,0.0916561,"hibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 )."
N16-1084,P08-1019,0,0.0307759,", in your talking baout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) fo"
N16-1084,S15-2035,0,0.0824082,"of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting threadlevel information. Hou et al. (2015) used features about the position of the comment in the thread. Barr´on-Cede˜no et al. (2015) developed more elaborated global features to model thread structure and the interaction among users. Other work exploited global inference algorithms at the thread-level. For instance, (Zhou et al., 2015c; Zhou et al., 2015b; Barr´on-Cede˜no et al., 2015) treated the task as sequential classification, using a variety of machine learning algorithms to label the sequence of timesorted comments: LSTMs, CRFs, SVMhmm , etc. Finally, Joty et al. (2015) showed that exploiting the pairwise relations between c"
N16-1084,D15-1068,1,0.636014,"Missing"
N16-1084,P11-1143,0,0.118538,"aout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments"
N16-1084,S15-2047,1,0.844443,"Missing"
N16-1084,S15-2036,1,0.602545,"Missing"
N16-1084,P04-1035,0,0.0140909,"CRF model improves results significantly over all rivaling models, yielding the best results on the task to date. In the remainder of this paper, after discussing related work in Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to"
N16-1084,W00-0721,0,0.0961383,"ify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classi"
N16-1084,C04-1197,0,0.0586523,"a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting thre"
N16-1084,W04-2401,0,0.0624102,"-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a percept"
N16-1084,W06-1639,0,0.0569981,"Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction prob"
N16-1084,P15-1025,0,0.200763,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,P15-2117,0,0.206056,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,S15-2037,0,0.113962,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N19-1134,C18-1111,0,0.0670509,"Missing"
N19-1134,W04-3240,0,0.898481,"Missing"
N19-1134,Q18-1018,0,0.0142989,"netune our word embeddings on the SAR task. We construct sequences from the chronological order of the sentences in a conversation. Since MRDA conversations are much longer compared to those in asynchronous domains (955 vs. 18-34 sentences in Table 1), we split the MRDA conversations into smaller parts containing a maximum of 100 sentences.4 The number of epochs and batch size were fixed to 30 and 5 (conversations), respectively. We ran each experiment 5 times, each time with a different random seed, and report the average of the (2-fold×5=10) runs along with the standard deviation. Recently, Crane (2018) show that the main source of variability in results for neural models come from the random seed, and the author has recommended to report the distribution of results from a range of seeds. Results. We present the results in Table 5. From the first block of results, we notice that both SVM and FFN baselines perform poorly compared to other models that tune the word embeddings and learn the sentence representation on the SAR task. The second block contains five LSTM variants: (i) B-LSTMrand , referring to bi-LSTM with random initialization; (ii) B-LSTMgl , referring to biLSTM initialized with o"
N19-1134,P11-2008,0,0.0611317,"Missing"
N19-1134,D09-1130,0,0.58587,"NAACL-HLT 2019, pages 1326–1336 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics are not available in the asynchronous domains, and many of the existing (small-sized) ones use task-specific tagsets as opposed to a standard one. The unavailability of large annotated datasets with standard tagsets is one of the main reasons for SAR not getting much attention in asynchronous domains, and it is often quite expensive to annotate such datasets for each domain of interest. SAR methods proposed before the neural ‘tsunami’, e.g., (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013), used mostly bag-of-ngram representation (e.g., unigram, bigram) of a sentence, and most of these methods disregard conversational dependencies (discourse structure) between sentences. Recently, Joty and Hoque (2016) proposed a neural-CRF framework for SAR in forum conversations. In their approach, a bi-LSTM (trained on the SAR task) first encodes the sentences separately into task-specific embeddings, which are then used in a separate CRF model to capture the conversational dependencies between sentences. They also use labeled data from the MRDA meeting corpus, without"
N19-1134,P16-1165,1,0.853968,"c tagsets as opposed to a standard one. The unavailability of large annotated datasets with standard tagsets is one of the main reasons for SAR not getting much attention in asynchronous domains, and it is often quite expensive to annotate such datasets for each domain of interest. SAR methods proposed before the neural ‘tsunami’, e.g., (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013), used mostly bag-of-ngram representation (e.g., unigram, bigram) of a sentence, and most of these methods disregard conversational dependencies (discourse structure) between sentences. Recently, Joty and Hoque (2016) proposed a neural-CRF framework for SAR in forum conversations. In their approach, a bi-LSTM (trained on the SAR task) first encodes the sentences separately into task-specific embeddings, which are then used in a separate CRF model to capture the conversational dependencies between sentences. They also use labeled data from the MRDA meeting corpus, without which their LSTMs perform worse than simple feed-forward networks. Although their method attempts to model sentence structure (using LSTM) and conversational dependencies (using CRF), the approach has several limitations. First, the LSTM-C"
N19-1134,W13-3214,0,0.0334488,"synchronous conversations while adapting our model to account for the domain shift. Joty and Hoque (2016) use a bi-LSTM to encode a sentence, then use a separate CRF to model conversational dependencies. To learn an effective bi-LSTM model, they use the MRDA meeting data; however, without modeling the domain differences. 1327 The unsupervised methods use variations of Hidden Markov Models (HMM) including HMMTopic (Ritter et al., 2010), HMM-Mix (Joty et al., 2011), and Mixed Membership (Paul, 2012). Several neural methods have been proposed in recent years for SAR in synchronous conversations. Kalchbrenner and Blunsom (2013) use a simple recurrent neural network (RNN) to model sequential dependencies between act types in phone conversations. They use a convolutional network to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but also experiment with RNNs to compose sentence representations. Khanpour et al. (2016) use a stacked LSTM to compose word vectors into a sentence vector. Kumar et al. (2018) also use a hierarchical LSTM-CRF. However, none of these methods were applied to asynchronous conversations, where not much labeled data is available. Also to the best"
N19-1134,C16-1189,0,0.106369,"se variations of Hidden Markov Models (HMM) including HMMTopic (Ritter et al., 2010), HMM-Mix (Joty et al., 2011), and Mixed Membership (Paul, 2012). Several neural methods have been proposed in recent years for SAR in synchronous conversations. Kalchbrenner and Blunsom (2013) use a simple recurrent neural network (RNN) to model sequential dependencies between act types in phone conversations. They use a convolutional network to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but also experiment with RNNs to compose sentence representations. Khanpour et al. (2016) use a stacked LSTM to compose word vectors into a sentence vector. Kumar et al. (2018) also use a hierarchical LSTM-CRF. However, none of these methods were applied to asynchronous conversations, where not much labeled data is available. Also to the best of our knowledge, no prior work attempted to do domain adaptation from the synchronous conversation, which is our main contribution in this paper. 3 The Base Model We use a bidirectional long short-term memory or bi-LSTM (Hochreiter and Schmidhuber, 1997) to encode each sentence into a vector representation. Given an input sentence xi = (w1 ,"
N19-1134,N16-1062,0,0.02309,", they use the MRDA meeting data; however, without modeling the domain differences. 1327 The unsupervised methods use variations of Hidden Markov Models (HMM) including HMMTopic (Ritter et al., 2010), HMM-Mix (Joty et al., 2011), and Mixed Membership (Paul, 2012). Several neural methods have been proposed in recent years for SAR in synchronous conversations. Kalchbrenner and Blunsom (2013) use a simple recurrent neural network (RNN) to model sequential dependencies between act types in phone conversations. They use a convolutional network to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but also experiment with RNNs to compose sentence representations. Khanpour et al. (2016) use a stacked LSTM to compose word vectors into a sentence vector. Kumar et al. (2018) also use a hierarchical LSTM-CRF. However, none of these methods were applied to asynchronous conversations, where not much labeled data is available. Also to the best of our knowledge, no prior work attempted to do domain adaptation from the synchronous conversation, which is our main contribution in this paper. 3 The Base Model We use a bidirectional long short-term memory or bi-LSTM (Hochreiter"
N19-1134,D15-1178,0,0.499422,"you want to hook up for possible transport sharing [Suggestion] cheers. [Polite] Figure 1: Example of speech acts in a forum thread. Introduction With the ever-increasing popularity of Internet and mobile technologies, communication media like emails and forums have become an integral part of people’s daily life where they discuss events, issues and experiences. Participants interact with each other asynchronously in these media by writing at different times, generating a type of conversational discourse that is different from synchronous conversations such as meeting and phone conversations (Louis and Cohen, 2015). In the course of the interactions, the participants perform certain communicative acts like asking questions, requesting information, or suggesting something, which are known as speech acts (Austin, ∗ All authors contibuted equally. 1962). For example, consider the forum conversation in Figure 1. The participant who posted the initial comment C1 , describes his situation and asks a couple of questions. Other participants respond to the initial post with more information and provide suggestions. In this process, the participants get into a conversation by taking turns, each of which consists"
N19-1134,N13-1090,0,0.0150345,"f pretrained embeddings to boost the performance of our models. In addition, we have also trained word embeddings from a large conversational corpus to get more relevant conversational word embeddings. We use Glove (Pennington et al., 2014) to train our word embeddings from a corpus that contains 24K email threads from W3C (w3c.org), 25K threads from TripAdvisor, 220K threads from QatarLiving, and all conversations from SWBD and MRDA (a total of 120M tokens). Table 4 shows some statistics of the datasets used for training the conversational word embeddings. We also trained skip-gram word2vec (Mikolov et al., 2013), but its performance was worse than Glove. 6 Experiments We followed similar preprocessing steps as Joty and Hoque (2016); specifically: normalize all characters to lower case, spell out digits and URLs, and tokenize the texts using TweetNLP (Gimpel et al., 2011). For performance compar1331 SVMc-gl FFNc-gl FFNskip-th QC3 16.96±0.00 48.29±0.25 50.80±1.21 TA 20.17±0.00 61.36±0.21 61.44±0.92 BC3 17.20±0.00 39.58±0.26 47.67±0.74 MRDA 31.47±0.00 71.12±0.13 71.73±0.48 B-LSTMrand B-LSTMgl B-GRUc-gl B-LSTMc-gl S-LSTMc-gl 50.25±0.57 53.21±0.77 60.50±0.36 61.01±0.60 56.70±0.58 62.11±0.64 63.23±0.80 67."
N19-1134,W14-4318,0,0.502114,"Missing"
N19-1134,D12-1009,0,0.390713,"es between the act types, something we successfully exploit in our work. Also, we leverage labeled data from synchronous conversations while adapting our model to account for the domain shift. Joty and Hoque (2016) use a bi-LSTM to encode a sentence, then use a separate CRF to model conversational dependencies. To learn an effective bi-LSTM model, they use the MRDA meeting data; however, without modeling the domain differences. 1327 The unsupervised methods use variations of Hidden Markov Models (HMM) including HMMTopic (Ritter et al., 2010), HMM-Mix (Joty et al., 2011), and Mixed Membership (Paul, 2012). Several neural methods have been proposed in recent years for SAR in synchronous conversations. Kalchbrenner and Blunsom (2013) use a simple recurrent neural network (RNN) to model sequential dependencies between act types in phone conversations. They use a convolutional network to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but also experiment with RNNs to compose sentence representations. Khanpour et al. (2016) use a stacked LSTM to compose word vectors into a sentence vector. Kumar et al. (2018) also use a hierarchical LSTM-CRF. Howe"
N19-1134,D14-1162,0,0.0791506,"une the hyperparameters of the models for experiments on each fold. For experiments on MRDA, we use the same train:test:dev split as in (Jeong et al., 2009; Joty and Hoque, 2016). 5.2 Conversational Word Embeddings One simple and effective approach to semisupervised learning is to use word embeddings pretrained from a large unlabeled corpus. In our work, we use generic off-the-shelf pretrained embeddings to boost the performance of our models. In addition, we have also trained word embeddings from a large conversational corpus to get more relevant conversational word embeddings. We use Glove (Pennington et al., 2014) to train our word embeddings from a corpus that contains 24K email threads from W3C (w3c.org), 25K threads from TripAdvisor, 220K threads from QatarLiving, and all conversations from SWBD and MRDA (a total of 120M tokens). Table 4 shows some statistics of the datasets used for training the conversational word embeddings. We also trained skip-gram word2vec (Mikolov et al., 2013), but its performance was worse than Glove. 6 Experiments We followed similar preprocessing steps as Joty and Hoque (2016); specifically: normalize all characters to lower case, spell out digits and URLs, and tokenize t"
N19-1134,D11-1069,0,0.16179,"pora 1326 Proceedings of NAACL-HLT 2019, pages 1326–1336 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics are not available in the asynchronous domains, and many of the existing (small-sized) ones use task-specific tagsets as opposed to a standard one. The unavailability of large annotated datasets with standard tagsets is one of the main reasons for SAR not getting much attention in asynchronous domains, and it is often quite expensive to annotate such datasets for each domain of interest. SAR methods proposed before the neural ‘tsunami’, e.g., (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013), used mostly bag-of-ngram representation (e.g., unigram, bigram) of a sentence, and most of these methods disregard conversational dependencies (discourse structure) between sentences. Recently, Joty and Hoque (2016) proposed a neural-CRF framework for SAR in forum conversations. In their approach, a bi-LSTM (trained on the SAR task) first encodes the sentences separately into task-specific embeddings, which are then used in a separate CRF model to capture the conversational dependencies between sentences. They also use labeled data from the MRDA meet"
N19-1134,N10-1020,0,0.0468413,"rns the sub-tree features. This approach does not consider the dependencies between the act types, something we successfully exploit in our work. Also, we leverage labeled data from synchronous conversations while adapting our model to account for the domain shift. Joty and Hoque (2016) use a bi-LSTM to encode a sentence, then use a separate CRF to model conversational dependencies. To learn an effective bi-LSTM model, they use the MRDA meeting data; however, without modeling the domain differences. 1327 The unsupervised methods use variations of Hidden Markov Models (HMM) including HMMTopic (Ritter et al., 2010), HMM-Mix (Joty et al., 2011), and Mixed Membership (Paul, 2012). Several neural methods have been proposed in recent years for SAR in synchronous conversations. Kalchbrenner and Blunsom (2013) use a simple recurrent neural network (RNN) to model sequential dependencies between act types in phone conversations. They use a convolutional network to compose sentence representations from word vectors. Lee and Dernoncourt (2016) use a similar model, but also experiment with RNNs to compose sentence representations. Khanpour et al. (2016) use a stacked LSTM to compose word vectors into a sentence ve"
N19-1134,W13-4017,1,0.892626,"es 1326–1336 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics are not available in the asynchronous domains, and many of the existing (small-sized) ones use task-specific tagsets as opposed to a standard one. The unavailability of large annotated datasets with standard tagsets is one of the main reasons for SAR not getting much attention in asynchronous domains, and it is often quite expensive to annotate such datasets for each domain of interest. SAR methods proposed before the neural ‘tsunami’, e.g., (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013), used mostly bag-of-ngram representation (e.g., unigram, bigram) of a sentence, and most of these methods disregard conversational dependencies (discourse structure) between sentences. Recently, Joty and Hoque (2016) proposed a neural-CRF framework for SAR in forum conversations. In their approach, a bi-LSTM (trained on the SAR task) first encodes the sentences separately into task-specific embeddings, which are then used in a separate CRF model to capture the conversational dependencies between sentences. They also use labeled data from the MRDA meeting corpus, without which their LSTMs perf"
N19-1134,K17-1040,0,0.0562781,"Missing"
N19-1386,D18-1214,0,0.199764,"Missing"
N19-1386,D16-1250,0,0.314677,"a linear mapping from one language to another with the underlying assumption that two embedding spaces exhibit similar geometric structures (i.e., approximately isomorphic). This allows the model to learn effective cross-lingual representations without expensive supervision (Artetxe et al., 2017). Given monolingual word embeddings of two languages, Mikolov et al. (2013a) show that a linear mapping can be learned from a seed dictionary of 5000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent works (Xing et al., 2015; Artetxe et al., 2016, 2017; Smith et al., 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. While these methods assume some supervision in the form of a seed dictionary, recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a,b) first reported encouraging results with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs. Their learned mapping was then successfully used to train a"
N19-1386,P17-1042,0,0.318477,"s solved the associated word alignment problem using large parallel corpora (Luong et al., 2015), broader applicability demands methods to relax this requirement since acquiring a large corpus of parallel data is not feasible in most scenarios. Recent methods instead use embeddings learned from monolingual data, and learn a linear mapping from one language to another with the underlying assumption that two embedding spaces exhibit similar geometric structures (i.e., approximately isomorphic). This allows the model to learn effective cross-lingual representations without expensive supervision (Artetxe et al., 2017). Given monolingual word embeddings of two languages, Mikolov et al. (2013a) show that a linear mapping can be learned from a seed dictionary of 5000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent works (Xing et al., 2015; Artetxe et al., 2016, 2017; Smith et al., 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. While these methods assume some supervision in the form of a seed dictionary, recently fully unsuper"
N19-1386,D18-1043,0,0.678186,"ervised methods have shown competitive results. Zhang et al. (2017a,b) first reported encouraging results with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b). In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings. Our main idea is to learn the cross-lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. We accomplish this by proposing a novel adversarial autoencoder framework (Makhzani et al"
N19-1386,J82-2005,0,0.762554,"Missing"
N19-1386,W15-1521,0,0.043246,"Extensive experimentations with European, non-European and lowresource languages show that our method is more robust and achieves better performance than recently proposed adversarial and nonadversarial approaches. 1 Introduction Learning cross-lingual word embeddings has been shown to be an effective way to transfer knowledge from one language to another for many key linguistic tasks including machine translation, named entity recognition, part-of-speech tagging, and parsing (Ruder et al., 2017). While earlier efforts solved the associated word alignment problem using large parallel corpora (Luong et al., 2015), broader applicability demands methods to relax this requirement since acquiring a large corpus of parallel data is not feasible in most scenarios. Recent methods instead use embeddings learned from monolingual data, and learn a linear mapping from one language to another with the underlying assumption that two embedding spaces exhibit similar geometric structures (i.e., approximately isomorphic). This allows the model to learn effective cross-lingual representations without expensive supervision (Artetxe et al., 2017). Given monolingual word embeddings of two languages, Mikolov et al. (2013a"
N19-1386,W16-1614,0,0.0427063,"ution. In their follow-up work, Artetxe et al. (2017) obtain competitive results using a seed dictionary of only 25 word pairs. They propose a self-learning framework that performs two steps iteratively until convergence. In the first step, they use the dictionary (starting with the seed) to learn a linear mapping, which is then used in the second step to induce a new dictionary. A more recent line of research attempts to eliminate the seed dictionary totally and learn the map3858 1 see (Ruder et al., 2017) for a nice survey ping in a purely unsupervised way. This was first proposed by Miceli Barone (2016), who initially used an adversarial network similar to Conneau et al. (2018), and found that the mapper (which is also the encoder) translates everything to a single embedding, known commonly as the mode collapse issue (Goodfellow, 2017). To preserve diversity in mapping, he used a decoder to reconstruct the source embedding from the mapped embedding, extending the framework to an adversarial autoencoder. His preliminary qualitative analysis shows encouraging results but not competitive with methods using bilingual seeds. He suspected issues with training and with the isomorphic assumption. In"
N19-1386,P18-1073,0,0.161812,"Missing"
N19-1386,Q17-1010,0,0.13878,"e for the rest we report results by running the publicly available codes on our machine. For training our model on European languages, the weight for cycle consistency (λ1 ) in Eq. 7 was always set to 5, and the weight for post-cycle reconstruction (λ2 ) was set to 1. For non-European languages, we use different values of λ1 and λ2 for different language pairs. 4 The dimension of the code vectors in our model was set to 350. 5 We evaluate our model on two different datasets. The first one is from Conneau et al. (2018), which consists of FastText monolingual embeddings of (d =) 300 dimensions (Bojanowski et al., 2017) trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs.3 To show the generality of different methods, we consider European, non-European and low-resource languages. In particular, we evaluate on English (En) from/to Spanish (Es), German (De), Italian (It), Arabic (Ar), Malay (Ms), and Hebrew (He). We also evaluate on the more challenging dataset of Dinu et al. (2015) and its subsequent extension by Artetxe et al. (2018a). We will refer to this dataset as Dinu-Artexe dataset. From this dataset, we choose to experiment on English 3 https://github.com/facebookresear"
N19-1386,E14-1049,0,0.0738114,"ng the learned mapping and find the nearest target word. In their approach, they found that simple linear mapping works better than non-linear mappings with multilayer neural networks. Xing et al. (2015) enforce the word vectors to be of unit length during the learning of the embeddings and modify the objective function for learning the mapping to maximize the cosine similarity instead of using Euclidean distance. To preserve length normalization after mapping, they enforce the orthogonality constraint on the mapper. Instead of learning a mapping from the source to the target embedding space, Faruqui and Dyer (2014) use a technique based on Canonical Correlation Analysis (CCA) to project both source and target embeddings to a common low-dimensional space, where the correlation of the word pairs in the seed dictionary is maximized. Artetxe et al. (2016) show that the above methods are variants of the same core optimization objective and propose a closed form solution for the mapper under orthogonality constraint. Smith et al. (2017) find that this solution is closely related to the orthogonal Procrustes solution. In their follow-up work, Artetxe et al. (2017) obtain competitive results using a seed dictio"
N19-1386,P18-1072,0,0.269559,"Missing"
N19-1386,N15-1104,0,0.472394,"Missing"
N19-1386,D18-1268,0,0.0930682,"cently fully unsupervised methods have shown competitive results. Zhang et al. (2017a,b) first reported encouraging results with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b). In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings. Our main idea is to learn the cross-lingual mapping in a projected latent space and add more constraints to guide the unsupervised mapping in this space. We accomplish this by proposing a novel adversarial autoencoder f"
N19-1386,P17-1179,0,0.245763,"guages, Mikolov et al. (2013a) show that a linear mapping can be learned from a seed dictionary of 5000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent works (Xing et al., 2015; Artetxe et al., 2016, 2017; Smith et al., 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. While these methods assume some supervision in the form of a seed dictionary, recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a,b) first reported encouraging results with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe e"
N19-1386,D17-1207,0,0.267011,"guages, Mikolov et al. (2013a) show that a linear mapping can be learned from a seed dictionary of 5000 word pairs by minimizing the sum of squared Euclidean distances between the mapped vectors and the target vectors. Subsequent works (Xing et al., 2015; Artetxe et al., 2016, 2017; Smith et al., 2017) propose to improve the model by normalizing the embeddings, imposing an orthogonality constraint on the mapper, and modifying the objective function. While these methods assume some supervision in the form of a seed dictionary, recently fully unsupervised methods have shown competitive results. Zhang et al. (2017a,b) first reported encouraging results with adversarial training. Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs. Their learned mapping was then successfully used to train a fully unsupervised neural machine translation system (Lample et al., 2018a,b). Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe e"
P08-2003,kingsbury-palmer-2002-treebank,0,0.0999723,"in the collection. In cases where the query is composed of two or more sentences, we compute the similarity between the document sentence (s) and each of the query-sentences (qi ) then we take the average of the scores. 3 Encoding Syntactic and Shallow Semantic Structures Encoding syntactic structure is easier and straight forward. Given a sentence (or query), we first parse it into a syntactic tree using a syntactic parser (i.e. Charniak parser) and then we calculate the similarity between the two trees using the general tree kernel function (Section 4.1). Initiatives such as PropBank (PB) (Kingsbury and Palmer, 2002) have made possible the design of accurate automatic Semantic Role Labeling (SRL) systems like ASSERT (Hacioglu et al., 2003). For example, consider the PB annotation: [ARG0 all][TARGET use][ARG1 the french franc][ARG2 as their currency] Such annotation can be used to design a shallow semantic representation that can be matched against other semantically similar sentences, e.g. [ARG0 the Vatican][TARGET use][ARG1 the Italian lira][ARG2 as their currency] In order to calculate the semantic similarity between the sentences, we first represent the annotated sentence using the tree structures like"
P08-2003,W04-1013,0,0.00257693,"m is extremely large. Because of this, (Collins and Duffy, 2001) defines the tree kernel algorithm whose computational complexity does not depend on m. We followed the similar approach to compute the tree kernel between two syntactic trees. 11 5.1 Evaluation Setup The Document Understanding Conference (DUC) series is run by the National Institute of Standards and Technology (NIST) to further progress in summarization and enable researchers to participate in large-scale experiments. We used the DUC 2007 datasets for evaluation. We carried out automatic evaluation of our summaries using ROUGE (Lin, 2004) toolkit, which has been widely adopted by DUC for automatic summarization evaluation. It measures summary quality by counting overlapping units such as the n-gram (ROUGE-N), word sequences (ROUGE-L and ROUGE-W) and word pairs (ROUGE-S and ROUGE-SU) between the candidate summary and the reference summary. ROUGE parameters were set as the same as DUC 2007 evaluation setup. All the ROUGE measures were calculated by running ROUGE-1.5.5 with stemming but no removal of stopwords. The ROUGE run-time parameters are: ROUGE-1.5.5.pl -2 -1 -u -r 1000 -t 0 -n 4 -w 1.2 -m -l 250 -a The purpose of our expe"
P08-2003,H05-1115,0,0.121767,"ions, locations, dates, etc. Unlike informationally-simple factoid questions, complex questions often seek multiple different types of information simultaneously and do not presupposed that one single answer could meet all of its information needs. For example, with complex questions like “What are the causes of AIDS?”, the wider focus of this question suggests that the submitter may not have a single or well-defined inforRecently, the graph-based method (LexRank) is applied successfully to generic, multi-document summarization (Erkan and Radev, 2004). A topicsensitive LexRank is proposed in (Otterbacher et al., 2005). In this method, a sentence is mapped to a vector in which each element represents the occurrence frequency (TF*IDF) of a word. However, the major limitation of the TF*IDF approach is that it only retains the frequency of the words and does not take into account the sequence, syntactic and semantic information thus cannot distinguish between “The hero killed the villain” and “The villain killed the hero”. The task like answering complex questions that requires the use of more complex syntactic and semantics, the approaches with only TF*IDF are often inadequate to perform fine-level textual an"
P08-2003,P07-1098,0,\N,Missing
P08-2003,N04-1030,0,\N,Missing
P09-2083,C04-1064,0,0.278612,"h may or may not be relevant to the abstract summary sentences. We filter those BEs by checking possible matches with an abstract sentence word or a related word. For each abstract sentence, we assign a score to every document sentence as the sum of its filtered BE scores divided by the number of BEs in the sentence. Thus, every abstract sentence contributes to the BE score of each document sentence and we select the top N sentences based on average BE scores to have the label +1 and the rest to have the label −1. Extended String Subsequence Kernel (ESSK) Formally, ESSK is defined as follows (Hirao et al., 2004): d X X X Kessk (T, U ) =  Km (ti , uj ) = if m = 1 0  0 Km (ti , uj ) = 0 0 00 λKm (ti , uj−1 ) + Km (ti , uj−1 ) if j = 1 Here λ is the decay parameter for the number of skipped words. We choose λ = 0.5 for this 00 research. Km (ti , uj ) is defined as: 00  Km (ti , uj ) = 0 00 λKm (ti−1 , uj ) + Km (ti−1 , uj ) if i = 1 Finally, the similarity measure is defined after normalization as below: T K(T1 , T2 ) = v(T1 ).v(T2 ) The TK (tree kernel) function gives the similarity score between the abstract sentence and the document sentence based on the syntactic structure. Each abstract sentence"
P09-2083,W04-1013,0,0.025123,"sed methods we use are Support Vector Machines (SVM), Conditional Random Fields (CRF), Hidden Markov Models (HMM), and Maximum Entropy (MaxEnt). Evaluation results are presented to show the impact. 1 2 Automatic Annotation Schemes Using ROUGE Similarity Measures ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is an automatic tool to determine the quality of a summary using a collection of measures ROUGE-N (N=1,2,3,4), ROUGE-L, ROUGE-W and ROUGE-S which count the number of overlapping units such as n-gram, word-sequences, and word-pairs between the extract and the abstract summaries (Lin, 2004). We assume each individual document sentence as the extract summary and calculate its ROUGE similarity scores with the corresponding abstract summaries. Thus an average ROUGE score is assigned to each sentence in the document. We choose the top N sentences based on ROUGE scores to have the label Introduction In this paper, we consider the complex question answering problem defined in the DUC-2007 main task1 . We focus on an extractive approach of summarization to answer complex questions where a subset of the sentences in the original documents are chosen. For supervised learning methods, hug"
P09-2083,A00-2018,0,\N,Missing
P09-2083,P07-1098,0,\N,Missing
P13-1048,J00-3005,0,0.763712,"and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast"
P13-1048,P12-1007,0,0.355874,"istinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequential dependencies between the DT constituents, which has been recently shown to be critical (Feng a"
P13-1048,J91-1002,0,0.302844,"the inference much faster (i.e., complexity of O(M 2 )). Breaking the chain structure also allows us to balance the data for training (equal number instances with S=1 and S=0), which dramatically reduces the learning time of the model. We apply our model to all possible adjacent units at all levels for the multi-sentential case, and 490 Lexico-syntactic features dominance sets (Soricut and Marcu, 2003) are very effective for intra-sentential parsing. We include syntactic labels and lexical heads of head and attachment nodes along with their dominance relationship as features. Lexical chains (Morris and Hirst, 1991) are sequences of semantically related words that can indicate topic shifts. Features extracted from lexical chains have been shown to be useful for finding paragraph-level discourse structure (Sporleder and Lascarides, 2004). We compute lexical chains for a document following the approach proposed in (Galley and McKeown, 2003), that extracts lexical chains after performing word sense disambiguation. Following (Joty et al., 2012), we also encode contextual and rhetorical sub-structure features in our models. The rhetorical sub-structure features incorporate hierarchical dependencies between DT"
P13-1048,P07-1062,0,0.741307,", which in turn are also subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not"
P13-1048,N03-1030,0,0.527984,"me-Unit Contrast Explanation Figure 2: Distributions of six most frequent relations in intra-sentential and multi-sentential parsing scenarios. frequent relations on a development set containing 20 randomly selected documents from RST-DT. Notice that relations Attribution and Same-Unit are more frequent than Joint in intra-sentential case, whereas Joint is more frequent than the other two in multi-sentential case. On the other hand, different kinds of features are applicable and informative for intra-sentential vs. multi-sentential parsing. For example, syntactic features like dominance sets (Soricut and Marcu, 2003) are extremely useful for sentence-level parsing, but are not even applicable in multi-sentential case. Likewise, lexical chain features (Sporleder and Lascarides, 2004), that are useful for multi-sentential parsing, are not applicable at the sentence level. Based on these observations, our discourse parsing framework comprises two separate modules: an intra-sentential parser and a multisentential parser (Figure 3). First, the intrasentential parser produces one or more discourse sub-trees for each sentence. Then, the multisentential parser generates a full DT for the document from these sub-t"
P13-1048,D12-1083,1,0.239984,"subject to this relation linking. Discourse units linked by a rhetorical relation are further distinguished based on their relative importance in the text: nucleus being the central part, whereas satellite being the peripheral one. Discourse analysis in RST involves two subtasks: discourse segmentation is the task of identifying the EDUs, and discourse parsing is the task of linking the discourse units into a labeled tree. While recent advances in automatic discourse segmentation and sentence-level discourse parsing have attained accuracies close to human performance (Fisher and Roark, 2007; Joty et al., 2012), discourse parsing at the document-level still poses significant challenges (Feng and Hirst, 2012) and the performance of the existing document-level parsers (Hernault et al., 2010; Subba and DiEugenio, 2009) is still considerably inferior compared to human gold-standard. This paper aims to reduce this performance gap and take discourse parsing one step further. To this end, we address three key limitations of existing parsers as follows. First, existing discourse parsers typically model the structure and the labels of a DT separately in a pipeline fashion, and also do not consider the sequen"
P13-1048,H05-1033,0,0.0311665,"rsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaboration, Contrast), forming larger discourse units (represented by int"
P13-1048,C04-1007,0,0.820285,"ndow covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential parser Figure 3: Discourse parsing framework. Attribution Same-Unit Contrast Explanation Figure 2: Distributions"
P13-1048,W10-4327,0,0.250002,"rasentential parsing and the other for multisentential parsing. We present two approaches to combine these two stages of discourse parsing effectively. A set of empirical evaluations over two different datasets demonstrates that our discourse parser significantly outperforms the stateof-the-art, often by a wide margin. 1 Introduction Discourse of any kind is not formed by independent and isolated textual units, but by related and structured units. Discourse analysis seeks to uncover such structures underneath the surface of the text, and has been shown to be beneficial for text summarization (Louis et al., 2010; Marcu, 2000b), sentence compression (Sporleder and Lapata, 2005), text generation (Prasad et al., 2005), sentiment analysis (Somasundaran, 2010) and question answering (Verberne et al., 2007). Rhetorical Structure Theory (RST) (Mann and Thompson, 1988), one of the most influential theories of discourse, represents texts by labeled hierarchical structures, called Discourse Trees (DTs), as exemplified by a sample DT in Figure 1. The leaves of a DT correspond to contiguous Elementary Discourse Units (EDUs) (six in the example). Adjacent EDUs are connected by rhetorical relations (e.g., Elaborat"
P13-1048,W04-0213,0,0.0487185,"Missing"
P13-1048,N09-1064,0,0.688998,"en the discourse unit containing EDUs i through m and the unit containing EDUs m+1 through j. For example, the DT for the second sentence in Figure 1 can be represented as ument, HILDA iteratively employs two Support Vector Machine (SVM) classifiers in pipeline to build the DT. In each iteration, a binary classifier first decides which of the adjacent units to merge, then a multi-class classifier connects the selected units with an appropriate relation label. They evaluate their approach on the RST-DT corpus (Carlson et al., 2002) of news articles. On a different genre of instructional texts, Subba and Di-Eugenio (2009) propose a shift-reduce parser that relies on a classifier for relation labeling. Their classifier uses Inductive Logic Programming (ILP) to learn first-order logic rules from a set of features including compositional semantics. In this work, we address the limitations of these models (described in Section 1) introducing our novel discourse parser. 3 Our Discourse Parsing Framework Given a document with sentences already segmented into EDUs, the discourse parsing problem is determining which discourse units (EDUs or larger units) to relate (i.e., the structure), and how to relate them (i.e., t"
P13-1048,P02-1047,0,0.0642381,"h these cases, builds sentence-level sub-trees by applying the intra-sentential parser on a sliding window covering two adjacent sentences and by then consolidating the results produced by over2 Related work The idea of staging document-level discourse parsing on top of sentence-level discourse parsing was investigated in (Marcu, 2000a; LeThanh et al., 2004). These approaches mainly rely on discourse markers (or cues), and use hand-coded rules to build DTs for sentences first, then for paragraphs, and so on. However, often rhetorical relations are not explicitly signaled by discourse markers (Marcu and Echihabi, 2002), and discourse structures do not always correspond to paragraph structures (Sporleder and Lascarides, 2004). Therefore, rather than relying on hand-coded rules based on discourse markers, recent approaches employ supervised machine learning techniques with a large set of informative features. Hernault et al., (2010) presents the publicly available HILDA parser. Given the EDUs in a doc487 30 25 20 15 10 5 0 Multi-sentential Intra-sentential Elaboration Joint Algorithm Sentences segmented into EDUs Algorithm Document-level discourse tree model model Intra-sentential parser Multi-sentential pars"
P13-1048,C04-1048,0,\N,Missing
P14-1065,W11-2103,0,0.0360688,"herence relations in the source language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanis"
P14-1065,N04-1035,0,0.0377299,"ve process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, an"
P14-1065,W12-3102,0,0.0260244,"Missing"
P14-1065,W07-0738,1,0.784963,"Missing"
P14-1065,W09-0440,1,0.927179,"Missing"
P14-1065,W11-1211,0,0.0329553,"sed discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a diffe"
P14-1065,D08-1024,0,0.0729502,"Missing"
P14-1065,P05-1033,0,0.102259,"over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this"
P14-1065,2010.iwslt-papers.10,0,0.0454846,"ial of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR"
P14-1065,D12-1108,0,0.0251193,"ramework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source la"
P14-1065,W10-1750,1,0.883167,"Missing"
P14-1065,W11-2107,0,0.0510915,"the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judges 8 20 15 18 498 924 570 708 171 303 207 249 20 31 18 32"
P14-1065,D11-1125,0,0.120375,"et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield be"
P14-1065,D12-1083,1,0.558373,"is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus sho"
P14-1065,P02-1040,0,0.0916457,"html SPAN NUC EDU Attribution SPAN Satellite NUC Nucleus SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear"
P14-1065,P13-1048,1,0.222411,"the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into ac"
P14-1065,W07-0707,0,0.0299954,"g proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse info"
P14-1065,P05-1034,0,0.0192094,"tence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield bett"
P14-1065,W04-1013,0,0.0173282,"NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judge"
P14-1065,2006.amta-papers.25,0,0.149639,"us SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 sy"
P14-1065,W05-0904,0,0.154446,"n campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware m"
P14-1065,W12-3129,0,0.298188,"e of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is"
P14-1065,D07-1080,0,0.0300512,"ks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judg"
P14-1065,2012.amta-papers.20,0,0.0666078,"ion for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For"
P14-1065,P11-3009,0,0.0426777,"tation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse"
P14-1065,moschitti-basili-2006-tree,0,0.00987264,"of the relation while satellites are supportive ones. Note that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis. We argue that existing metrics that only use lexical and syntactic information cannot distinguish well between (b) and (c). 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can be compared. 2 The discourse"
P14-1065,D12-1097,0,0.146948,"o develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 3.1 Gener"
P14-1065,P07-1098,0,0.0569476,"ive partial credit to subtrees that differ in labels but match in their skeletons. More specifically, it uses the tags SPAN and EDU to build the skeleton of the tree, and considers the nuclearity and/or the relation labels as properties, added as children, of these tags. For example, a SPAN has two properties (its nuclearity and its relation), and an EDU has one property (its nuclearity). The words of an EDU are placed under the predefined children NGRAM. In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in (Moschitti et al., 2007); not shown in Figure 2, for simplicity. Experimental Setup In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.3 This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CS EN ), French-English ( FR - EN), German-English (DE - EN), and Spanish-English (ES - EN); as well as a dataset with the English references. We measured the correlation of the metrics with the human judgments pro"
P14-1065,N09-2004,0,\N,Missing
P14-1065,W13-3300,0,\N,Missing
P15-1078,W10-1750,1,0.923317,"Missing"
P15-1078,P14-1023,0,0.016965,"q. (3), can be rewritten as follows: 4.1 Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX 25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as W IKI -GW25. Furthermore, we experiment with W IKI GW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. Network Training The negative log likelihood of the training data for the model parameters θ = (W12 , W1r , W2r , wv , b12 , b1r , b2r , bv ) can be written"
P15-1078,W11-2107,0,0.0115221,"100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will be discussed below, their"
P15-1078,P14-1129,0,0.0611925,"Missing"
P15-1078,W08-0331,0,0.313473,"that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce"
P15-1078,W07-0718,0,0.0607111,"(SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In"
P15-1078,W07-0738,1,0.52667,"Missing"
P15-1078,W11-2103,0,0.0331433,"r improvements: +1.5 and +2.0 points absolute when adding SYNTAX 25 and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at"
P15-1078,D14-1027,1,0.819258,"Missing"
P15-1078,W12-3102,0,0.0577153,"Missing"
P15-1078,P14-1065,1,0.735686,"Missing"
P15-1078,W14-3352,1,0.565452,"Missing"
P15-1078,P02-1040,0,0.100737,"ans that rivals the state of the art. 1 Introduction Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems. Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach. Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations. In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference. This is the case of BLEU (Papineni et al., 2002), which has been the standard for MT evaluation for years. Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments. 1 We do not argue that the pairwise approach is better than the direct estimation of human quality scores. Both approaches have pros and cons; we see them as complementary. 805 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 805–814, c Beijing, China, July 26-31"
P15-1078,2004.tmi-1.8,0,0.0676924,"ased approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in an SVM classifier to discriminate good from bad translations. However, their setting is not pairwise comparison, but a classification task to distinguish human- from machineproduced translations. Moreover, in their work, using syntactic features decreased the correlation with human judgments dramatically (although classification accuracy improved), while in our case the effect is positive. In our previous work (Guzm´an et al., 2014a), we introduced a learning framework for the pairwise sett"
P15-1078,D14-1162,0,0.0794828,"h application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3 Learning Task Given two translation h"
P15-1078,W07-0707,0,0.0221202,"Missing"
P15-1078,W05-0904,0,0.150588,"ecause the use of kernels requires that the SVM operate in the much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to repr"
P15-1078,2006.amta-papers.25,0,0.187957,"2 VEC 300, trained on 100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as wi"
P15-1078,W12-3129,0,0.0269611,"much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our se"
P15-1078,P13-1045,0,0.14241,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W13-2202,0,0.0122239,"and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and B"
P15-1078,D13-1170,0,0.00357626,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W14-3336,0,0.0290128,"r, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and Bojar, 2014) for a discussion. Here we use the strict version used at WMT11 and WMT12. 4.4 Experiments and Results Experimental Settings Datasets: We train our neural models on WMT11 and we evaluate them on WMT12. We further use a random subset of 5,000 examples from WMT13 as a validation set to implement early stopping. Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall’s τ on the development set after each epoch. We then select the model that achieves the highest τ on the validation set; in case of ties for the best τ , we select the latest epoch that achieved the highes"
P15-1078,W11-2113,0,0.253095,"quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce absolute quality scor"
P15-1078,D12-1097,0,0.0127587,"o make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it bot"
P15-1078,N13-1090,0,0.0630498,"nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3"
P15-1078,W11-0329,0,0.0126675,"Missing"
P15-1078,W14-3302,0,\N,Missing
P15-2113,P07-1098,1,0.451732,"al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s"
P15-2113,N10-1145,0,0.0528465,"answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high"
P15-2113,S15-2047,1,0.84994,"Missing"
P15-2113,S15-2035,0,0.0819635,"1.45 65.57±1.54 76.23±0.45 76.43±0.92 75.05±0.70 75.61±0.63 75.71±0.71 Table 3: Precision, Recall, F1 , Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1 , F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup As in the competition, the results are macroaveraged at class level. The results of the top 3 Our local classifiers are support vector machines systems are reported for comparison: JAIST (Tran (SVM) with C = 1 (Joachims, 1999), logistic et al., 2015), HITSZ (Hou et al., 2015) and regression with a Gaussian prior with variance 10, QCRI (Nicosia et al., 2015), where the latter refers and logistic ordinal regression (McCullagh, 1980). to our old system that we used for the competition. In order to capture long-range sequential depenThe two main observations are (i) using threaddencies, we use a second-order SVMhmm (Yu level features helps significantly; and (ii) the ordiand Joachims, 2008) (with C = 500 and nal regression model, which captures the idea that epsilon = 0.01) and a second-order linear-chain potential lies between good and bad, achieves at CRF, which con"
P15-2113,S15-2036,1,0.620958,"Missing"
P15-2113,D13-1044,1,0.903656,"Missing"
P15-2113,W01-0515,0,0.0237286,"can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy. In other words, labels should be used together with answers’ content to account for stronger and more effective dependencies. 2 Basic and Thread-Level Features 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for"
P15-2113,W13-3509,1,0.800933,"g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used"
P15-2113,D07-1002,0,0.0974152,"a Abstract This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and"
P15-2113,P08-1082,0,0.0417245,"n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b"
P15-2113,S15-2038,0,0.16032,"Missing"
P15-2113,C10-1131,0,0.223512,"Missing"
P15-2113,N13-1106,0,0.0861461,"Missing"
P16-1165,P06-1026,0,0.228242,"Missing"
P16-1165,D14-1226,0,0.360346,"larly, Figure 3d shows the graph structure for LC-FC1 configuration, where sentences inside comments have linear chain connections, and sentences of the first comment are fully connected with the sentences of the other comments. 3 Corpora There exist large corpora of utterances annotated with speech acts in synchronous spoken domains, e.g., Switchboard-DAMSL or SWBD (Jurafsky et al., 1997) and Meeting Recorder Dialog Act or MRDA (Dhillon et al., 2004). However, such large corpus does not exist in asynchronous domains. Some prior work (Cohen et al., 2004; Ravi and Kim, 2007; Feng et al., 2006; Bhatia et al., 2014) tackles the task at the comment level, and uses task-specific tagsets. In contrast, in this work we are interested in identifying speech acts at the sentence level, and also using a standard tagset like the ones defined in SWBD and MRDA. More recent studies attempt to solve the task at the sentence level. Jeong et al. (2009) first created a dataset of TripAdvisor (TA) forum conversations annotated with the standard 12 act types defined in MRDA. They also remapped the BC3 email corpus (Ulrich et al., 2008) according to this tagset. Table 10 in the Appendix presents the tags and their relative"
P16-1165,P08-1041,0,0.0632125,"Missing"
P16-1165,W04-3240,0,0.810536,"Missing"
P16-1165,N06-1027,0,0.130205,"ther comments. Similarly, Figure 3d shows the graph structure for LC-FC1 configuration, where sentences inside comments have linear chain connections, and sentences of the first comment are fully connected with the sentences of the other comments. 3 Corpora There exist large corpora of utterances annotated with speech acts in synchronous spoken domains, e.g., Switchboard-DAMSL or SWBD (Jurafsky et al., 1997) and Meeting Recorder Dialog Act or MRDA (Dhillon et al., 2004). However, such large corpus does not exist in asynchronous domains. Some prior work (Cohen et al., 2004; Ravi and Kim, 2007; Feng et al., 2006; Bhatia et al., 2014) tackles the task at the comment level, and uses task-specific tagsets. In contrast, in this work we are interested in identifying speech acts at the sentence level, and also using a standard tagset like the ones defined in SWBD and MRDA. More recent studies attempt to solve the task at the sentence level. Jeong et al. (2009) first created a dataset of TripAdvisor (TA) forum conversations annotated with the standard 12 act types defined in MRDA. They also remapped the BC3 email corpus (Ulrich et al., 2008) according to this tagset. Table 10 in the Appendix presents the ta"
P16-1165,P11-2008,0,0.0317131,"Missing"
P16-1165,D14-1080,0,0.0488955,"n asynchronous conversations. LSTM RNNs for composition Li et al. (2015) compare recurrent neural models with recursive (syntax-based) models for several NLP tasks and conclude that recurrent models perform on par with the recursive for most tasks (or even better). For example, recurrent models outperform recursive on sentence level sentiment classification. This finding motivated us to use recurrent models rather than recursive. The application of LSTM RNNs to speech act recognition is novel to the best of our knowledge. LSTM RNNs have also been applied to sequence tagging in opinion mining (Irsoy and Cardie, 2014; Liu et al., 2015). Conditional structured models There has been an explosion of interest in CRFs for solving structured output problems in NLP; see (Smith, 2011) for an overview. Linear chain (for sequence labeling) and tree structured CRFs (for parsing) are the common ones in NLP. However, speech act recognition in asynchronous conversation posits a different problem, where the challenge is to model arbitrary conversational structures. In this work we propose a general class of models based on pairwise CRFs that work on arbitrary graph structures. Speech act recognition in asynchronous conv"
P16-1165,D09-1130,0,0.306834,"cation of speech acts is an important step towards deep conversation analysis in these media (Bangalore et al., 2006), and has been shown to be useful in many downstream applications including summarization (McKeown et al., 2007) and question answering (Hong and Davison, 2009). Previous attempts to automatic (sentence-level) 1746 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1746–1756, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics speech act recognition in asynchronous conversation (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013; Oya and Carenini, 2014) suffer from at least one of the two major flaws. Firstly, they use bag-of-word (BOW) representation (e.g., unigram, bigram) to encode lexical information in a sentence. However, consider the suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act. Furthermore, BOW representation could be quite sparse and may not generalize well when used in classification models. Secondly, existing approaches mostly disregard conversational depend"
P16-1165,D15-1278,0,0.0409317,"and inference makes a difference, let us consider the example in Figure 1 again. We notice that the two sentences in comment C4 were mistakenly identified as Statement and Response, respectively, by the B-LSTMp local model. However, by considering these two sentences together with others in the conversation, the global C RF ( FC - FC ) model could correct them. 5 Related Work Three lines of research are related to our work: (i) semantic compositionality with LSTM RNNs, (ii) conditional structured models, and (iii) speech act recognition in asynchronous conversations. LSTM RNNs for composition Li et al. (2015) compare recurrent neural models with recursive (syntax-based) models for several NLP tasks and conclude that recurrent models perform on par with the recursive for most tasks (or even better). For example, recurrent models outperform recursive on sentence level sentiment classification. This finding motivated us to use recurrent models rather than recursive. The application of LSTM RNNs to speech act recognition is novel to the best of our knowledge. LSTM RNNs have also been applied to sequence tagging in opinion mining (Irsoy and Cardie, 2014; Liu et al., 2015). Conditional structured models"
P16-1165,D15-1168,1,0.734425,"ions. LSTM RNNs for composition Li et al. (2015) compare recurrent neural models with recursive (syntax-based) models for several NLP tasks and conclude that recurrent models perform on par with the recursive for most tasks (or even better). For example, recurrent models outperform recursive on sentence level sentiment classification. This finding motivated us to use recurrent models rather than recursive. The application of LSTM RNNs to speech act recognition is novel to the best of our knowledge. LSTM RNNs have also been applied to sequence tagging in opinion mining (Irsoy and Cardie, 2014; Liu et al., 2015). Conditional structured models There has been an explosion of interest in CRFs for solving structured output problems in NLP; see (Smith, 2011) for an overview. Linear chain (for sequence labeling) and tree structured CRFs (for parsing) are the common ones in NLP. However, speech act recognition in asynchronous conversation posits a different problem, where the challenge is to model arbitrary conversational structures. In this work we propose a general class of models based on pairwise CRFs that work on arbitrary graph structures. Speech act recognition in asynchronous conversation Jeong et a"
P16-1165,W14-4318,0,0.72459,"p towards deep conversation analysis in these media (Bangalore et al., 2006), and has been shown to be useful in many downstream applications including summarization (McKeown et al., 2007) and question answering (Hong and Davison, 2009). Previous attempts to automatic (sentence-level) 1746 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1746–1756, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics speech act recognition in asynchronous conversation (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013; Oya and Carenini, 2014) suffer from at least one of the two major flaws. Firstly, they use bag-of-word (BOW) representation (e.g., unigram, bigram) to encode lexical information in a sentence. However, consider the suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act. Furthermore, BOW representation could be quite sparse and may not generalize well when used in classification models. Secondly, existing approaches mostly disregard conversational dependencies between sentences. For instance, consid"
P16-1165,D11-1069,0,0.516797,", respectively. Identification of speech acts is an important step towards deep conversation analysis in these media (Bangalore et al., 2006), and has been shown to be useful in many downstream applications including summarization (McKeown et al., 2007) and question answering (Hong and Davison, 2009). Previous attempts to automatic (sentence-level) 1746 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1746–1756, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics speech act recognition in asynchronous conversation (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013; Oya and Carenini, 2014) suffer from at least one of the two major flaws. Firstly, they use bag-of-word (BOW) representation (e.g., unigram, bigram) to encode lexical information in a sentence. However, consider the suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act. Furthermore, BOW representation could be quite sparse and may not generalize well when used in classification models. Secondly, existing approaches mostly disregard c"
P16-1165,W13-4017,1,0.905654,"s is an important step towards deep conversation analysis in these media (Bangalore et al., 2006), and has been shown to be useful in many downstream applications including summarization (McKeown et al., 2007) and question answering (Hong and Davison, 2009). Previous attempts to automatic (sentence-level) 1746 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1746–1756, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics speech act recognition in asynchronous conversation (Qadir and Riloff, 2011; Jeong et al., 2009; Tavafi et al., 2013; Oya and Carenini, 2014) suffer from at least one of the two major flaws. Firstly, they use bag-of-word (BOW) representation (e.g., unigram, bigram) to encode lexical information in a sentence. However, consider the suggestion sentences in the example. Arguably, a model needs to consider the structure (e.g., word order) and the compositionality of phrases to identify the right speech act. Furthermore, BOW representation could be quite sparse and may not generalize well when used in classification models. Secondly, existing approaches mostly disregard conversational dependencies between senten"
P17-1121,J08-1001,0,0.360822,"distinguishes a coherent text from a random sequence of sentences is that it binds the sentences together to express a meaning as a whole — the interpretation of a sentence usually depends on the meaning of its neighbors. Coherence models that can distinguish a coherent from incoherent texts have a wide range of applications in text generation, summarization, and coherence scoring. Several formal theories of coherence have been proposed (Mann and Thompson, 1988a; Grosz et al., 1995; Asher and Lascarides, 2003), and their principles have inspired development of many existing coherence models (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Hovy, 2014). Among these models, the entity grid (Barzilay and Lapata, 2008), which is based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models"
P17-1121,N10-1099,0,0.166112,"Missing"
P17-1121,P08-2011,0,0.258569,"removed in turn, and an insertion place is located for which the model gives the highest coherence score to the document. The insertion score is then computed as the average fraction of sentences per document reinserted in their actual position. Discrimination can be easier for longer documents, since a random permutation is likely to be different than the original one. Insertion is a much more difficult task since the candidate documents differ only by the position of one sentence. Dataset: For sentence ordering tasks, we use the Wall Street Journal (WSJ) portion of Penn Treebank, as used by Elsner and Charniak (2008, 2011); Lin et al. (2011); Feng et al. (2014). Table 1 gives basic statistics about the dataset. Following previous works, we use 20 random permutations of each article, and we exclude permutations that match the original document.5 The fourth column (# Pairs) in Table 1 shows the resulting number of (original, permuted) pairs used for training our model and for testing in the discrimination task. Some previous studies (Barzilay and Lapata, 2008; Li and Hovy, 2014) used the AIRPLANES and the EARTHQUAKES corpora, which contain reports on airplane crashes and earthquakes, respectively. Each of"
P17-1121,P11-2022,0,0.300747,"based on Centering Theory (Grosz et al., 1995), is arguably the most popular, and has seen a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). While the entity grid and its extensions have been successful in many applications, they are limited in several ways. First, they use discrete representation for grammatical roles and features, which prevents the model from considering sufficiently long transitions (Bengio et al., 2003). Second, feature vector computation in existing models is decoupled from the target task, which limits the model’s capacity to learn task-specific features. In this paper, we propose a neural architecture for coherence assessm"
P17-1121,E12-1032,0,0.507614,"ata (2005, 2008) introduced the entity grid representation of discourse to model local coherence that captures the distribution of discourse entities across sentences in a text. They also introduced three tasks to evaluate the performance of coherence models: discrimination, summary coherence rating, and readability. 11 Since we do not have access to the output of their systems, we could not do a significance test for this task. A number of extensions of the basic entity grid model has been proposed. Elsner and Charniak (2011) included entity-specific features to distinguish between entities. Feng and Hirst (2012) used the basic grid representation, but improved its learning to rank scheme. Their model learns not only from original document and its permutations but also from ranking preferences among the permutations themselves. Guinaudeau and Strube (2013) convert a standard entity grid into a bipartite graph representing entity occurrences in sentences. To model local entity transition, the method constructs a directed projection graph representing the connection between adjacent sentences. Two sentences have a connected edge if they share at least one entity in common. The coherence score of the doc"
P17-1121,C14-1089,0,0.691858,"a number of improvements over the years. As shown in Figure 1, the entity grid model represents a text by a grid that captures how ∗ Both authors contributed equally to this work. grammatical roles of different entities change from sentence to sentence. The grid is then converted into a feature vector containing probabilities of local entity transitions, which enables machine learning models to learn the degree of text coherence. Extensions of this basic grid model incorporate entity-specific features (Elsner and Charniak, 2011), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). While the entity grid and its extensions have been successful in many applications, they are limited in several ways. First, they use discrete representation for grammatical roles and features, which prevents the model from considering sufficiently long transitions (Bengio et al., 2003). Second, feature vector computation in existing models is decoupled from the target task, which limits the model’s capacity to learn task-specific features. In this paper, we propose a neural architecture for coherence assessment that can capture long range entity transitions along with arbitrary entityspecif"
P17-1121,W07-2321,0,0.109213,"Missing"
P17-1121,J95-2003,0,0.904306,"Missing"
P17-1121,P13-1010,0,0.68095,"ment with pairwise coherence rankings given by humans judges; see (Barzilay and Lapata, 2008) for details on the annotation method. There are 144 pairs of summaries for training and 80 pairs for testing. 5 Experiments In this section, we present our experiments — the models we compare, their settings, and the results. 5.1 Models Compared We compare our coherence model against a random baseline and several existing models. Random: The Random baseline makes a random decision for the evaluation tasks. Graph-based Model: This is the graph-based unsupervised model proposed by Guinaudeau and Strube (2013). We use the implementation from the cohere7 toolkit (Smith et al., 2016), and run it on the test set with syntactic projection (command line option ‘projection=3’) for graph construction. This setting yielded best scores for this model. Distributed Sentence Model: Li and Hovy (2014) proposed this neural model for measuring 6 7 https://bitbucket.org/melsner/browncoherence https://github.com/karins/CoherenceFramework text coherence. The model first encodes each sentence in a document into a fixed-length vector using a recurrent or a recursive neural network. Then it computes the coherence score"
P17-1121,P14-1062,0,0.00454977,"ion of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias 1322 Figure 2: Neural architecture for modeling local coherence and the pairwise training method. term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model. We apply this filter to each possible k-length transitions of different entities in the grid to generate a feature map, hi = [h1 , · · · , hm.n+k−1 ]. We repeat this process N times with N different filters to get N different feature maps (Figure 2). Notice that we use a wide convolution (Kalchbrenner et al., 2014), as opposed to narrow, to ensure that the filters reach entire columns of a grid, including the boundary entities. This is done by performing zero-padding, where out-of-range (i.e., for t &lt; 0 or t > {m, n}) vectors are assumed to be zero. Convolutional filters learn to compose local transition features of a grid into higher-level representations automatically. Since it operates over the distributed representation of grid entries, compared to traditional grid models, the transition length k can be sufficiently large (e.g., 5 − 8 in our experiments) to capture long-range transitional dependenci"
P17-1121,D14-1181,0,0.0121286,"dels, we assume there is no spatio-temporal relation between the entities in a document. In other words, columns in a grid are treated independently. • We are interested in modeling entity transitions of arbitrary lengths in a location-invariant way. This means, we aim to compose local patches of entity transitions into higher-level representations, while treating the patches independently of their position in the entity grid. Under these assumptions, the natural choice to tackle this problem is to use a convolutional approach, used previously to solve other NLP tasks (Collobert et al., 2011; Kim, 2014). Convolution layer: A convolution operation involves applying a filter w ∈ Rk.d (i.e., a vector of weight parameters) to each entity transition of length k to produce a new abstract feature ht = f (wT Lt:t+k−1,j + bt ) (2) where Lt:t+k−1,j denotes the concatenation of k vectors in the lookup layer representing a transition of length k for entity ej in the grid, bt is a bias 1322 Figure 2: Neural architecture for modeling local coherence and the pairwise training method. term, and f is a nonlinear activation function, e.g., ReLU (Nair and Hinton, 2010) in our model. We apply this filter to eac"
P17-1121,D14-1218,0,0.571069,"Missing"
P17-1121,D14-1220,0,0.0517117,"Missing"
P17-1121,P11-1100,0,0.719802,"Missing"
P17-1121,D12-1106,0,0.559092,"tect each NP as a label Lnp ∈ {new, old}. The coherenceQscore of the document is then estimated by np:N P s P (Lnp |np). In this work, they also estimate text coherence through pronoun coreference modeling. Lin et al. (2011) assume that a coherent text has certain discourse relation patterns. Instead of modeling entity transitions, they model discourse role transitions between sentences. In a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse structures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns. In recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a 1327 neural framework to compute"
P17-1121,P10-1158,0,0.0129557,"1) proposed a number of improvements. They initially show significant improvement by including non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.2 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles"
P17-1121,P10-1056,0,0.0199359,"w significant improvement by including non-head nouns (i.e., nouns that do not head NPs) as entities in the grid.2 Then, they extend the grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles, the basic entity grid model needs to compute Rk"
P17-1121,L16-1649,0,0.0300613,"Missing"
P17-1121,P13-1045,0,0.00971264,"n a follow up work, Feng et al. (2014) trained the same model but using features derived from deep discourse structures annotated with Rhetorical Structure Theory or RST (Mann and Thompson, 1988b) relations. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns in text by assuming that sentences in a coherent discourse should share the same structural syntactic patterns. In recent years, there has been a growing interest in neuralizing traditional NLP approaches – language modeling (Bengio et al., 2003), sequence tagging (Collobert et al., 2011), syntactic parsing (Socher et al., 2013), and discourse parsing (Li et al., 2014), etc. Following this tradition, in this paper we propose to neuralize the popular entity grid models. Li and Hovy (2014) also proposed a 1327 neural framework to compute the coherence score of a document by estimating coherence probability for every window of L sentences (in their experiments, L = 3). First, they use a recurrent or a recursive neural network to compute the representation for each sentence in L from its words and their pre-trained embeddings. Then the concatenated vector is passed through a non-linear hidden layer, and finally the outpu"
P17-1121,P06-2103,0,0.0490742,"e grid to distinguish between entities of different types by incorporating entity-specific features like named entity, noun class, modifiers, etc. These extensions led to the best results reported so far. The Entity grid and its extensions have been successfully applied to many downstream tasks including coherence rating (Barzilay and Lapata, 2008), essay scoring (Burstein et al., 2010), story generation (McIntyre and Lapata, 2010), and readability assessment (Pitler et al., 2010; Barzilay and Lapata, 2008). They have also been critical components in state-of-the-art sentence ordering models (Soricut and Marcu, 2006; Elsner and Charniak, 2011; Lin et al., 2011). 2.1 Limitations of Entity Grid Models Despite its success, existing entity grid models are limited in several ways. • Existing models use discrete representation for grammatical roles and features, which leads to the so-called curse of dimensionality problem (Bengio et al., 2003). In particular, to model transitions of length k with R different grammatical roles, the basic entity grid model needs to compute Rk tran2 1321 They match the nouns to detect coreferent entities. sition probabilities from a grid. One can imagine that the estimated distri"
P17-1121,P05-1018,0,\N,Missing
P18-1052,P15-2113,1,0.864539,"Missing"
P18-1052,P13-1010,0,0.148961,"erformance. We present our coherence model for asynchronous conversation in the next section. 3.1 Neural Entity Grid Figure 1 depicts the neural grid model of Nguyen and Joty (2017). Given an entity grid E, they first transform each entry Ei,j (a grammatical role) into a distributed representation of d dimensions by looking up a shared embedding matrix M ∈ R|G|×d , where G is the vocabulary of possible grammatical roles, i.e., G = {S, O, X, −}. Formally, the look-up operation can be expressed as: h i L = M (E1,1 ) · · · M (Ei,j ) · · · M (EI,J ) (1) Other Existing Models Guinaudeau and Strube (2013) proposed a graphbased unsupervised method. They convert an entity grid into a bipartite graph consisting of two sets of nodes, representing sentences and entities, respectively. The edges are assigned weights based on the grammatical role of the entities in the respective sentences. They perform one-mode projections to transform the bipartite graph to a directed graph containing only sentence nodes. The coherence score of the document is then computed where M (Ei,j ) refers to the row in M that corresponds to grammatical role Ei,j , and I and J are 560 where entity grid Ei exhibits a higher d"
P18-1052,J08-1001,0,0.770588,"long entity transitions, it is still limited in that it does not consider any lexical information regarding the entities, thereby, fails to distinguish Introduction Sentences in a text or a conversation do not occur independently, rather they are connected to form a coherent discourse that is easy to comprehend. Coherence models are computational models that can distinguish a coherent discourse from incoherent ones. It has ranges of applications in text generation, summarization, and coherence scoring. Inspired by formal theories of discourse, a number of coherence models have been proposed (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Jurafsky, 2017). The entity grid model (Barzilay and Lapata, 2008) is one of the most popular coherence models that has received much attention over the years. As exemplified in Table 1, the model represents a text by a grid that captures how grammatical roles of different discourse entities (e.g., nouns) change from one sentence to ∗ All authors contributed equally. 558 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 558–568 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Lin"
P18-1052,P08-1041,0,0.0952387,"Missing"
P18-1052,P11-1118,0,0.711899,"ables, rather than directly by LDI Corp. s3 : LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid"
P18-1052,D14-1218,0,0.377803,"at entity ej plays in sentence si , which can be one of: subject (S), object (O), other (X), or absent (–). In cases where an 559 as the average out-degree of sentence nodes. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns by assuming that sentences in a coherent text exhibit certain syntactic regularities. They propose a local coherence model that captures the co-occurrence of structural features in adjacent sentences, and a global model based on a hidden Markov model, which learns the global syntactic patterns from clusters of sentences with similar syntax. Li and Hovy (2014) proposed a neural framework to compute the coherence score of a document by estimating coherence probability for every window of three sentences. They encode each sentence in the window using either a recurrent or a recursive neural network. To get a documentlevel coherence score, they sum up the windowlevel log probabilities. Li and Jurafsky (2017) proposed two encoder-decoder models augmented with latent variables for both coherence evaluation and discourse generation. Their first model incorporates global discourse information (topics) by feeding the output of a sentence-level HMMLDA model"
P18-1052,P11-2022,0,0.72728,"ables, rather than directly by LDI Corp. s3 : LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid"
P18-1052,D17-1019,0,0.631416,"in that it does not consider any lexical information regarding the entities, thereby, fails to distinguish Introduction Sentences in a text or a conversation do not occur independently, rather they are connected to form a coherent discourse that is easy to comprehend. Coherence models are computational models that can distinguish a coherent discourse from incoherent ones. It has ranges of applications in text generation, summarization, and coherence scoring. Inspired by formal theories of discourse, a number of coherence models have been proposed (Barzilay and Lapata, 2008; Lin et al., 2011; Li and Jurafsky, 2017). The entity grid model (Barzilay and Lapata, 2008) is one of the most popular coherence models that has received much attention over the years. As exemplified in Table 1, the model represents a text by a grid that captures how grammatical roles of different discourse entities (e.g., nouns) change from one sentence to ∗ All authors contributed equally. 558 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 558–568 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tion for asynchronous conversatio"
P18-1052,E12-1032,0,0.34681,"LDI leases and sells data-processing, telecommunications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity tr"
P18-1052,P11-1100,0,0.682096,"Missing"
P18-1052,C14-1089,0,0.742325,"nications and other high-tech equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity transitions, it is still limited in that it doe"
P18-1052,D15-1178,0,0.0876731,"Missing"
P18-1052,D12-1106,0,0.454999,"structure in the grid representation and subsequently in feature computation. For this, we propose a novel grid representa2.1 Traditional Entity Grid Models Introduced by Barzilay and Lapata (2008), the entity grid model represents a text by a twodimensional matrix. As shown in Table 1, the rows correspond to sentences, and the columns correspond to entities (noun phrases). Each entry Ei,j represents the syntactic role that entity ej plays in sentence si , which can be one of: subject (S), object (O), other (X), or absent (–). In cases where an 559 as the average out-degree of sentence nodes. Louis and Nenkova (2012) introduced a coherence model based on syntactic patterns by assuming that sentences in a coherent text exhibit certain syntactic regularities. They propose a local coherence model that captures the co-occurrence of structural features in adjacent sentences, and a global model based on a hidden Markov model, which learns the global syntactic patterns from clusters of sentences with similar syntax. Li and Hovy (2014) proposed a neural framework to compute the coherence score of a document by estimating coherence probability for every window of three sentences. They encode each sentence in the w"
P18-1052,J95-2003,0,0.957445,"Missing"
P18-1052,P17-1121,1,0.401043,"equipment. Abstract 1 Dat Tien Nguyen∗ University of Amsterdam t.d.nguyen@uva.nl X − X − − X − − − − − X X − S S − − X − Table 1: Entity grid representation (bottom) for a document (top) from the WSJ corpus. another in the text. The grid is then converted into a feature vector containing probabilities of local entity transitions, enabling machine learning models to measure the degree of coherence. Earlier extensions of this basic model incorporate entityspecific features (Elsner and Charniak, 2011b), multiple ranks (Feng and Hirst, 2012), and coherence relations (Feng et al., 2014). Recently, Nguyen and Joty (2017) proposed a neural version of the grid models. Their model first transforms the grammatical roles in a grid into their distributed representations, and employs a convolution operation over it to model entity transitions in the distributed space. The spatially maxpooled features from the convoluted features are used for coherence scoring. This model achieves state-of-the-art results in standard evaluation tasks on the Wall Street Journal (WSJ) corpus. Although the neural grid model effectively captures long entity transitions, it is still limited in that it does not consider any lexical informa"
P18-1052,D14-1162,0,0.081676,"Missing"
P18-1052,prasad-etal-2008-penn,0,0.0276965,"ner and Charniak (2011b) show improvements to the grid model by including non-head nouns as entities. Instead of employing a coreference resolver, they match the nouns to detect coreferent entities. They demonstrate further improvements by extending the grid to distinguish between entities of different types. They do so by incorporating entity-specific features like named entity, noun class and modifiers. Lin et al. (2011) model transitions of discourse roles for entities as opposed to their grammatical roles. They instantiate discourse roles by discourse relations in Penn Discourse Treebank (Prasad et al., 2008). In a follow up work, Feng et al. (2014) trained the same model but using relations derived from deep discourse structures annotated with Rhetorical Structure Theory (Mann and Thompson, 1988). 2.2 3 Extending Neural Entity Grid In this section we first briefly describe the neural entity grid model proposed by Nguyen and Joty (2017). Then, we propose our extension to this model that leads to improved performance. We present our coherence model for asynchronous conversation in the next section. 3.1 Neural Entity Grid Figure 1 depicts the neural grid model of Nguyen and Joty (2017). Given an ent"
P18-1052,D15-1036,0,0.0345256,"Missing"
P18-1099,W06-1615,0,0.146773,"dings. Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed representation of the words in a tweet. Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph. Similar similarity-based graph has shown impressive results in learning sentence representations (Saha et al., 2017). In the literature, the proposed approaches for domain adaptation include supervised, semisupervised and unsupervised. It also varies from linear kernelized approach (Blitzer et al., 2006) to non-linear deep neural network techniques (Glorot et al., 2011; Ganin et al., 2016). One direction of research is to focus on feature space distribution matching by reweighting the samples from the source domain (Gong et al., 2013) to map source into target. The overall idea is to learn a good feature representation that is invariant across domains. In the deep learning paradigm, Glorot et al. (Glorot et al., 2011) used Stacked Denoising Auto-Encoders (SDAs) for domain adaptation. SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise. Adversa"
P18-1099,P14-1062,0,0.0202596,"nvolution and pooling layers to learn higher-level feature representations. A convolution operation applies a filter u ∈ Rk.d to a window of k vectors to produce a new feature ht as ht = f (u.Xt:t+k−1 ) (1) where Xt:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU. We apply this filter to each possible k-length windows in X with stride size of 1 to generate a feature map hj as: hj = [h1 , . . . , hn+k−1 ] (2) We repeat this process N times with N different filters to get N different feature maps. We use a wide convolution (Kalchbrenner et al., 2014), which ensures that the filters reach the entire tweet, including the boundary words. This is done by performing zero-padding, where out-ofrange (i.e., t<1 or t>n) vectors are assumed to be zero. With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features. After the convolution, we apply a max-pooling operation to each of the feature maps, m = [µp (h1 ), · · · , µp (hN )] 1078 (3) Shared Components Input tweet w1 w2 ! ! ! ! ! ! wn-1 ! ! wn ! ! Pre-trained Word Embeddings ! Feature map Dense (zc) Dense (z) Convolution ! ! ! ! ! !"
P18-1099,D14-1162,0,0.0919825,"rain the wordembedding model, we first pre-processed tweets collected using the AIDR system (Imran et al., 2014) during different events occurred between 2014 and 2016. In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol. After preprocessing, the resulting dataset contains about 364 million tweets and about 3 billion words. There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec (Mikolov et al., 2013), and Glove (Pennington et al., 2014). For our work, we trained the CBOW model from word2vec. While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples. The resulting embedding model contains about 2 million words with vector dimensions of 300. 3 Dataset tweet, we asked crowdsourcing workers to assign the “relevant” label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise"
P18-1099,U13-1011,0,0.0235473,"r work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (Imran et al., 2015). In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks. Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic damage (Varga et al., 2013; Vieweg et al., 2014; Power et al., 2013). In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time. Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining. However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake). On the other hand, in most cases, we can have access to a good am"
P19-1244,W18-5513,0,0.0348731,"al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditi"
P19-1244,D15-1075,0,0.592546,"rning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et"
P19-1244,W14-4012,0,0.0551999,"Missing"
P19-1244,D17-1070,0,0.0225496,"s: yˆ = softmax(Vo · hcS + bo ) Entailment-based Evidence Attention We further enhance the sentence representation by capturing the entailment relations between the sentences and the claim based on the NLI method (Bowman et al., 2015) for strengthening the semantic inference capacity of our model. Given c and si , we represent each such pair by integrating three matching functions between hc ˜ s : 1) concatenation [hc , h ˜ s ]; 2) elementand h i i ˜ wise product hc hsi ; and 3) absolute element˜ s |. The similar matching wise difference |hc − h i scheme was commonly used to train NLI models (Conneau et al., 2017; Mou et al., 2016; Liu et al., 2016; Chen et al., 2016). We then perform a transformation to obtain the joint representation hcsi as follow:  h i ˜ s , hc h ˜ s , |hc − h ˜s | hcsi = tanh We · hc , h i i i (6) where We are trainable weights for transforming the long concatenation into an l-dimensional vector. We omit the bias to avoid notational clutter. To capture entailment-based evidence, we again apply attention over the original sentences guided by the joint representation hcsi which is obtained on top of the coherence attention. This yields: bi = tanh(Ve · hcsi + be ) exp(bi ) βi = P"
P19-1244,P81-1022,0,0.360561,"Missing"
P19-1244,C18-1284,0,0.147537,"Missing"
P19-1244,N16-1138,0,0.0220374,"Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely rela"
P19-1244,W18-5516,0,0.122718,"a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditional coherence studies focu"
P19-1244,D16-1032,0,0.0158176,"o-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling. Different from traditional coherence studies focusing on discourse coherence among sentences that are widely applied in text generation (Park and Kim, 2015; Kiddon et al., 2016) and summarization (Logeswaran et al., 2018), we try to capture evidential sentences topically coherent not only among themselves but also with respect to the target claim. trieved from text collections containing variable number of sentences, and we disregard the order of sentences and which documents they are from. Our task is to classify an instance into a class defined by the specific dataset, such as veracity class labels, e.g., True/False, or NLI-style class labels, e.g., Supported/Refuted/NEI. Our approach exploits and integrates two core semantic relations: 1) coherence of the sentence"
P19-1244,P18-1184,1,0.846231,"019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral"
P19-1244,P16-2022,0,0.0365285,"cS + bo ) Entailment-based Evidence Attention We further enhance the sentence representation by capturing the entailment relations between the sentences and the claim based on the NLI method (Bowman et al., 2015) for strengthening the semantic inference capacity of our model. Given c and si , we represent each such pair by integrating three matching functions between hc ˜ s : 1) concatenation [hc , h ˜ s ]; 2) elementand h i i ˜ wise product hc hsi ; and 3) absolute element˜ s |. The similar matching wise difference |hc − h i scheme was commonly used to train NLI models (Conneau et al., 2017; Mou et al., 2016; Liu et al., 2016; Chen et al., 2016). We then perform a transformation to obtain the joint representation hcsi as follow:  h i ˜ s , hc h ˜ s , |hc − h ˜s | hcsi = tanh We · hc , h i i i (6) where We are trainable weights for transforming the long concatenation into an l-dimensional vector. We omit the bias to avoid notational clutter. To capture entailment-based evidence, we again apply attention over the original sentences guided by the joint representation hcsi which is obtained on top of the coherence attention. This yields: bi = tanh(Ve · hcsi + be ) exp(bi ) βi = P i exp(bi ) X βi ·"
P19-1244,W18-5527,0,0.0208916,"owing NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise and a hypothesis, as Entails, Contradicts or Neutral, Thorne et al. (2018a) formulated claim verification as a task that aims to classify claims into Supported, Refuted or Not Enough Info (NEI). They released a large dataset containing mutated claims based on relevant Wikipedia articles and developed a basic pipeline with document retrieval, sentence selection, and NLI modules. Similar pipelines were developed by most of the participating teams (Nie et al., 2019; Padia et al., 2018; Alhindi et al., 2018; Hanselowski et al., 2018) in FEVER shared task (Thorne et al., 2018b). Apart from the document retrieval function, our model is end-to-end and aims to learn sentence-level evidence with a hierarchical attention framework. Attention is in general used to attend on the most important part of texts, and has been successfully applied in machine translation (Luong et al.), question answering (Xiong et al., 2016) and parsing (Dozat and Manning, 2016), and is adopted in our model for attending on important sentences as evidence. Our work is also related to coherence modeling."
P19-1244,D14-1162,0,0.091726,"he distributions of the predicted and the ground-truth entailment classes. Overall Training After pre-training, all the model parameters are trained end-to-end by minimizing the squared error between the class probability distribution of the prediction and that of the ground truth over the claims. Parameters are updated through backpropagation (Collobert et al., 2011) with AdaGrad (Duchi et al., 2011) for speeding up convergence. The training process ends when the model converges or the maximum epoch number is met. We represent input words using pre-trained GloVe Wikipedia 6B word embeddings (Pennington et al., 2014). We set d to 300 for word vectors and l to 100 for hidden units, and no parameter depends on n which varies with different claims. 5 5.1 Experiments and Results Datasets and Evaluation Metrics We use three public fact-checking datasets for evaluation: 1) Snopes and 2) PolitiFact, released by Popat et al. (2018), containing 4,341 and 3,568 news claims, respectively, along with relevant articles collected from various web sources; 3) FEVER, released by Thorne et al. (2018a), which consists of 185,445 claims accompanied by human-annotated relevant Wikipedia articles and evidence-bearing sentence"
P19-1244,D18-1003,0,0.207437,"iable sources, e.g., encyclopedia articles, verified news, etc., as an important distinguishing factor (Thorne and Vlachos, 2018). Ferreira and Vlachos (2016) use news headlines as evidence to predict whether it is for, against or observing a claim. In the Fake News Challenge2 , the body text of an article is used as evidence to detect the stances relative to the claim made in the headline. Thorne et al. (2018a) formulate the Fact Extraction and VERification (FEVER) task which requires extracting evidence from Wikipedia and synthesizing information from multiple documents to verify the claim. Popat et al. (2018) propose DeClarE, an evidence-aware neural attention model to aggregate salient words from source news articles as the 2 http://www.fakenewschallenge.org/ 2561 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2561–2571 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics c: s1 : s2 : s3 : s4 : s5 : s6 : s7 : The test of a 5G cellular network is the cause of unexplained bird deaths occurring in a park in The Hague, Netherlands. Verdict: False [Contradict]: Lots of tests going on with it in the Netherlands, but"
P19-1244,D11-1147,0,0.0497903,"ollected from snopes.com, politifact.com and Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention m"
P19-1244,D17-1317,0,0.44431,"winternet.org/2018/03/ 01/social-media-use-in-2018/ claims being produced on daily basis. Therefore, it is an urgent need to automate the process and ease the human burden in assessing the veracity of claims (Thorne and Vlachos, 2018). Not surprisingly, various methods for automatic claim verification have been proposed using machine learning. Typically, given the claims, models are learned from auxiliary relevant sources such as news articles or social media responses for capturing words and linguistic units that might indicate viewpoint or language style towards the claim (Jin et al., 2016; Rashkin et al., 2017; Popat et al., 2017; Volkova et al., 2017; Dungs et al., 2018). However, the factuality of a claim is independent of people’s belief and subjective language use, and human perception is unconsciously prone to misinformation due to the common cognitive biases such as naive realism (Reed et al., 2013) and confirmation bias (Nickerson, 1998). A recent trend is that researchers are trying to establish more objective tasks and evidence-based verification solutions, which focus on the use of evidence obtained from more reliable sources, e.g., encyclopedia articles, verified news, etc., as an import"
P19-1244,W16-0802,0,0.0765855,", politifact.com and Wikipedia. 2 Related Work The literature on fact-checking and credibility assessment has been reviewed by several comprehensive surveys (Shu et al., 2017; Zubiaga et al., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on"
P19-1244,C18-1283,0,0.025314,"pes micF1 macF1 0.750 0.674 0.776 0.727 0.788 0.741 0.779 0.728 0.771 0.738 0.807 0.759 PolitiFact micF1 macF1 0.470 0.431 0.495 0.455 0.516 0.473 0.508 0.463 0.520 0.471 0.523 0.487 Results of Comparison Table 3: Results of ablation test across different attentions on Snopes (left) and PoliFact (right) datasets. 5.2 Experiments on Veracity-based Datasets We compare our model and several state-of-the-art baseline methods described below. 1) SVM: A linear SVM model for fake news detection using a set of linguistic features (e.g., bag-of-words, ngrams, etc.) handcrafted from relevant sentences (Thorne and Vlachos, 2018); 2) CNN and LSTM: The CNN-based detection model (Wang, 2017) and LSTM-based RNN model for representation learning from word sequences (Rashkin et al., 2017), respectively, both using only claim content without considering external resources; 3) DeClarE: The word-level neural attention model for Debunking Claims with Interpretable Evidence (Popat et al., 2018) capturing world-level evidence from relevant articles; 4) HAN: Our full model based on Hierarchical Attention Networks, where coherence component uses Eq. 3; 5) HAN-ba: A variant of HAN with biaffine attention in Eq. 2; 6) HANna: Our red"
P19-1244,N18-1074,0,0.146413,"Missing"
P19-1244,W18-5501,0,0.0882274,"Missing"
P19-1244,P17-2102,0,0.116006,"Missing"
P19-1244,P17-2067,0,0.218944,"l., 2018; Kumar and Shah, 2018; Sharma et al., 2019). We only briefly review prior works closely related to ours. Many studies on claim verification extracted veracity-indicative features that can reflect stances and writing styles from relevant texts such as news articles, microblog posts, etc. and used the traditional supervised models to learn the parameters (Castillo et al., 2011; Qazvinian et al., 2011; Rubin et al., 2016; Ferreira and Vlachos, 2016; Rashkin et al., 2017). Deep learning models such as recurrent neural networks (RNN) (Ma et al., 2016), convolutional neural networks (CNN) (Wang, 2017) and recursive neural 2562 networks (Ma et al., 2018) were also exploited to learn the feature representations. More recently, semantic matching methods were proposed to retrieve evidence from relatively trustworthy sources such as checked news and Wikipedia articles. Popat et al. (2018) attempted to debunk false claims by learning claim representations from relevant articles using an attention mechanism to focus on words that are closely related to the claim. Following NLI (Bowman et al., 2015), which is a task of classifying the relationship between a pair of sentences, composed by a premise"
P19-1244,D18-1010,0,0.100673,"t the essential objective of our model is not for evidence retrieval and ranking. Instead of ranking sentences into the top-k positions, we pay more attention on claim verification accuracy by embedding and aggregating the useful sentences as evidence like we have explained above. However, such discrepancy inspires us to investigate in the future an end-to-end approach to jointly model evidence retrieval and claim verification in a unified framework based on our sentence-level attention mechanism. Finally, thanks to one of our reviewers, we learn about another two-stage model named TwoWingOS (Yin and Roth, 2018), which achieves a comparable FEVER score but a little bit higher accuracy than ours on FEVER task. The TwoWingOS applies a two-wing optimization approach to jointly optimizing sentence selection and veracity classification. The reasons regarding their higher performance might lie in that: 1) their input word embeddings are fine-tuned based on the context of the evidence and claim while ours are fixed during training; and 2) the document retrieval module of the TwoWingOS has demonstrated higher effectiveness than that of the NSMN (see rate (recall) and acc ceiling (OFEVER) in Tables 2 in (Yin"
P19-1410,E17-1028,0,0.622795,"r Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of this method is that the parsing time is linear with respect to the number of EDUs (Sagae, 2009). The limitation, however, is that the decisions made at each step are based on local information, causing error propagation to subsequent steps. Also, when humans are asked to perform discourse analysis (segmentation and parsing), t"
P19-1410,D14-1179,0,0.0128124,"Missing"
P19-1410,P81-1022,0,0.701195,"Missing"
P19-1410,P12-1007,0,0.0298398,", Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of this method is that the parsing time is linear with respect to the number of EDUs (Sagae, 2009). The limitation, however, is that the decisions made at each step are based on local information, causing error propagation to subsequent steps. Also, when humans are asked to perf"
P19-1410,P14-1048,0,0.531385,"w the tree. Such sentence-level discourse annotations have been shown to be beneficial for a number of applications including machine translation (Guzm´an et al., 2014) and sentence compression (Sporleder and Lapata, 2005). Furthermore, sentence-level analysis is considered to be a crucial step towards full text-level analysis. For example, automatic discourse segmentation has been shown to be the main source of inaccuracies in discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012), and sentence-level parsing is considered as an essential first step in many existing discourse parsers (Feng and Hirst, 2014b; Joty et al., 2015) including the state-of-the-art one (Wang et al., 2017). While earlier methods have mostly relied on hand-crafted lexical and syntactic features, recently researchers have shown competitive or even better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion"
P19-1410,P07-1062,0,0.616925,"at https: //ntunlpsg.github.io/project/ parser/pointer-net-parser 2 Background In this section, we give a brief overview of coherence analysis with RST and pointer networks. 2.1 Coherence Analysis with RST Coherence analysis has been a long standing problem. We give a brief overview of the studies that are directly related to our method. Soricut and Marcu (2003) proposed SPADE, a system that uses generative models with syntactic features for discourse segmentation and sentencelevel parsing. Subsequent research focuses on the impact of syntax in discourse analysis (Sporleder and Lapata, 2005; Fisher and Roark, 2007; Hernault et al., 2010). Joty et al. (2015) propose CODRA, a system that comprises a discourse segmenter and a two-stage discourse parser – one 4191 for sentence-level parsing and the other for multisentential parsing. Feng and Hirst (2014a) also propose two-stage parsing based on CRFs that use many hand-crafted features. Li et al. (2014) propose a recursive network for discourse parsing. Ji and Eisenstein (2014) present a representation learning method in a shift-reduce discourse parser. Wang et al. (2017) propose a two-stage parser, where they use shift-reduce parsing to first construct a t"
P19-1410,P14-1065,1,0.891309,"Missing"
P19-1410,P82-1020,0,0.819884,"Missing"
P19-1410,P14-1002,0,0.77803,"th neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of this method is that the par"
P19-1410,D12-1083,1,0.912677,"Missing"
P19-1410,J15-3002,1,0.919862,"ce-level discourse annotations have been shown to be beneficial for a number of applications including machine translation (Guzm´an et al., 2014) and sentence compression (Sporleder and Lapata, 2005). Furthermore, sentence-level analysis is considered to be a crucial step towards full text-level analysis. For example, automatic discourse segmentation has been shown to be the main source of inaccuracies in discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012), and sentence-level parsing is considered as an essential first step in many existing discourse parsers (Feng and Hirst, 2014b; Joty et al., 2015) including the state-of-the-art one (Wang et al., 2017). While earlier methods have mostly relied on hand-crafted lexical and syntactic features, recently researchers have shown competitive or even better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is"
P19-1410,D14-1220,0,0.738419,"better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of th"
P19-1410,D16-1035,0,0.508595,"the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of this method is that the parsing time is linea"
P19-1410,P18-1130,0,0.232436,"epresentation learning method in a shift-reduce discourse parser. Wang et al. (2017) propose a two-stage parser, where they use shift-reduce parsing to first construct a tree structure with only nuclearity labels, and then in the second stage they identify the relations. They use SVMs with a large number of features. Wang et al. (2018) propose a discourse segmenter based on LSTM-CRF and achieve state-ofthe-art results with ELMo. Li et al. (2018) also propose a segmenter based on pointer networks. Pointer networks have also been used for summarization (See et al., 2017) and dependency parsing (Ma et al., 2018). In our work, we use pointer networks not only for segmentation but also for parsing, and we also show how the segmenter and parser can be trained jointly. 2.2 Pointer Networks Sequence-to-sequence paradigms (Sutskever et al., 2014) provide the flexibility that the output sequence can be of a different length than the input sequence. However, they still require the output vocabulary size to be fixed a priori, which limits their applicability to problems where one needs to select (or point to) an element in the input sequence; that is, the size of the output vocabulary depends on the length of"
P19-1410,J00-3005,0,0.54639,"on. We evaluate their parser based on these 881 sentences. This is also what the authors suggested when contacted. In our second setting for full system evaluation, we compare with the two existing end-to-end systems, SPADE and DCRF. The hyperparameters (learning rate, batch size, layer size) of our models in these two settings remain almost the same as the segmentation model (see Appendix for details). Metric and Relation Labels. We evaluate the performance by using the standard unlabeled (Span) and labeled (Nuclearity, Relation) microaveraged precision, recall and F1 -score as described in (Marcu, 2000). For brevity, we report only the F1 -scores here. Following previous work, we use the same 18 relations as used by previous studies, and we also attach the nuclearity statuses (NS, SN, NN) to these relations, giving a total of 39 distinctive relation labels. Results with Gold Segmentation. We present the results in Table 2. Our base model (with ELMo-medium) outperforms all the existing methods to date in all three tasks. We achieve an absolute 1.43 F1 improvement on the most difficult task of relation labeling, compared to the 2-stage parser (SOTA). Notably, the F1 score of 96.37 for Span of"
P19-1410,P17-2029,0,0.343568,"neficial for a number of applications including machine translation (Guzm´an et al., 2014) and sentence compression (Sporleder and Lapata, 2005). Furthermore, sentence-level analysis is considered to be a crucial step towards full text-level analysis. For example, automatic discourse segmentation has been shown to be the main source of inaccuracies in discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012), and sentence-level parsing is considered as an essential first step in many existing discourse parsers (Feng and Hirst, 2014b; Joty et al., 2015) including the state-of-the-art one (Wang et al., 2017). While earlier methods have mostly relied on hand-crafted lexical and syntactic features, recently researchers have shown competitive or even better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pret"
P19-1410,D14-1162,0,0.0824136,"lied on hand-crafted lexical and syntactic features, recently researchers have shown competitive or even better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4190–4200 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics units in an end-to-end fashion. This capability is particularly enhanced through the use of effective pretrained word embeddings such as Glove (Pennington et al., 2014) that provide better generalization. Despite this, successful discourse parsers (Li et al., 2014; Ji and Eisenstein, 2014; Li et al., 2016) still needed to use hand-engineered features to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers buil"
P19-1410,N18-1202,0,0.0596754,"ns and nuclearity statuses to use in connecting them (i.e., finding the correct labels). In the interests of presentational simplicity, we first describe the discourse parser in this section assuming that the EDUs have already been identified. Model Overview. As shown in Figure 2, our parser uses a pointer network as its backbone parsing model. Given an input sentence containing n words (w1 , . . . , wn ), we first embed the words into their respective distributed representation by initializing them either randomly or with pretrained embeddings such as Glove (Pennington et al., 2014) or ELMo (Peters et al., 2018). The result of this is a sequence of word vectors X = (x1 , . . . , xn ), which is fed to the network. The encoder of the pointer network first composes the entire sentence sequentially into a sequence of hidden states H = (h1 , . . . , hn ). The last hidden state of each EDU (e.g., h2 , h4 , h6 , h8 , h9 and h10 in Figure 2) are selected to represent the corresponding EDU, thus, forming a sequence of EDU representations E = (e1 , . . . , em ). From this, the greedy decoder then constructs the discourse tree in a top-down depthfirst manner. The decoder maintains a stack S to keep track of the"
P19-1410,D18-1116,0,0.232447,"ce-level parsing and the other for multisentential parsing. Feng and Hirst (2014a) also propose two-stage parsing based on CRFs that use many hand-crafted features. Li et al. (2014) propose a recursive network for discourse parsing. Ji and Eisenstein (2014) present a representation learning method in a shift-reduce discourse parser. Wang et al. (2017) propose a two-stage parser, where they use shift-reduce parsing to first construct a tree structure with only nuclearity labels, and then in the second stage they identify the relations. They use SVMs with a large number of features. Wang et al. (2018) propose a discourse segmenter based on LSTM-CRF and achieve state-ofthe-art results with ELMo. Li et al. (2018) also propose a segmenter based on pointer networks. Pointer networks have also been used for summarization (See et al., 2017) and dependency parsing (Ma et al., 2018). In our work, we use pointer networks not only for segmentation but also for parsing, and we also show how the segmenter and parser can be trained jointly. 2.2 Pointer Networks Sequence-to-sequence paradigms (Sutskever et al., 2014) provide the flexibility that the output sequence can be of a different length than the"
P19-1410,W09-3813,0,0.218172,"eatures to outperform the non-neural models. Another important distinction between existing methods is whether they employ a greedy transition-based algorithm (Marcu, 1999; Feng and Hirst, 2012, 2014b; Ji and Eisenstein, 2014; Braud et al., 2017; Li et al., 2016; Wang et al., 2017) or a globally optimized chart parsing algorithm (Soricut and Marcu, 2003; Li et al., 2014; Joty et al., 2015). Transition-based parsers build the tree incrementally by making a series of shiftreduce action decisions. The advantage of this method is that the parsing time is linear with respect to the number of EDUs (Sagae, 2009). The limitation, however, is that the decisions made at each step are based on local information, causing error propagation to subsequent steps. Also, when humans are asked to perform discourse analysis (segmentation and parsing), they tend to understand the full text first, before executing the tasks. Methods based on chart parsing, on the other hand, learn scoring functions for discourse subtrees and perform dynamic programming search over all possible trees to find the most probable tree for a text. While these methods are more accurate than greedy parsers, they are generally slow, having"
P19-1410,P17-1099,0,0.0115029,"sing. Ji and Eisenstein (2014) present a representation learning method in a shift-reduce discourse parser. Wang et al. (2017) propose a two-stage parser, where they use shift-reduce parsing to first construct a tree structure with only nuclearity labels, and then in the second stage they identify the relations. They use SVMs with a large number of features. Wang et al. (2018) propose a discourse segmenter based on LSTM-CRF and achieve state-ofthe-art results with ELMo. Li et al. (2018) also propose a segmenter based on pointer networks. Pointer networks have also been used for summarization (See et al., 2017) and dependency parsing (Ma et al., 2018). In our work, we use pointer networks not only for segmentation but also for parsing, and we also show how the segmenter and parser can be trained jointly. 2.2 Pointer Networks Sequence-to-sequence paradigms (Sutskever et al., 2014) provide the flexibility that the output sequence can be of a different length than the input sequence. However, they still require the output vocabulary size to be fixed a priori, which limits their applicability to problems where one needs to select (or point to) an element in the input sequence; that is, the size of the o"
P19-1410,N03-1030,0,0.136858,"enders will be considered timely if postmarked no later than Sunday, Oct.29, and received no later than tomorrow.”, which has four EDUs as shown below the tree. Such sentence-level discourse annotations have been shown to be beneficial for a number of applications including machine translation (Guzm´an et al., 2014) and sentence compression (Sporleder and Lapata, 2005). Furthermore, sentence-level analysis is considered to be a crucial step towards full text-level analysis. For example, automatic discourse segmentation has been shown to be the main source of inaccuracies in discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012), and sentence-level parsing is considered as an essential first step in many existing discourse parsers (Feng and Hirst, 2014b; Joty et al., 2015) including the state-of-the-art one (Wang et al., 2017). While earlier methods have mostly relied on hand-crafted lexical and syntactic features, recently researchers have shown competitive or even better results with neural models. One of the crucial advantages of neural models is that they can learn the feature representation of the discourse 4190 Proceedings of the 57th Annual Meeting of the Association for Computational Lingu"
P19-1410,H05-1033,0,0.477747,"L ATTRIBUTION Introduction ∗ S In this paper we consider sentence-level coherence analysis, which involves discourse segmentation and sentence-level parsing. For example, consider the DT in figure 1 for the sentence “The Treasury also said noncompetitive tenders will be considered timely if postmarked no later than Sunday, Oct.29, and received no later than tomorrow.”, which has four EDUs as shown below the tree. Such sentence-level discourse annotations have been shown to be beneficial for a number of applications including machine translation (Guzm´an et al., 2014) and sentence compression (Sporleder and Lapata, 2005). Furthermore, sentence-level analysis is considered to be a crucial step towards full text-level analysis. For example, automatic discourse segmentation has been shown to be the main source of inaccuracies in discourse parsing (Soricut and Marcu, 2003; Joty et al., 2012), and sentence-level parsing is considered as an essential first step in many existing discourse parsers (Feng and Hirst, 2014b; Joty et al., 2015) including the state-of-the-art one (Wang et al., 2017). While earlier methods have mostly relied on hand-crafted lexical and syntactic features, recently researchers have shown com"
P19-1410,J93-2004,0,\N,Missing
P19-4003,W04-3240,0,0.0668594,"Missing"
P19-4003,P09-1075,0,0.038371,"rization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structur"
P19-4003,D14-1220,0,0.0247646,"re emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical"
P19-4003,N16-1037,0,0.0230558,"zation). Again, evaluation metrics and approaches will be discussed and compared. We conclude with an interactive discussion of future challenges for discourse analMotivation Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics"
P19-4003,D16-1035,0,0.061192,"Missing"
P19-4003,P17-1092,0,0.0197289,"nalMotivation Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009)"
P19-4003,P19-1410,1,0.833143,"e computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eise"
P19-4003,P11-1100,0,0.0508656,"Missing"
P19-4003,J15-3002,1,0.895098,"timent analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse"
P19-4003,D18-1464,0,0.0138067,"-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB (b) Identifying discourse connectives (c) Implicit and explicit relations (a) Sentence ordering (Dis"
P19-4003,P16-1165,1,0.769595,"9 (QA track) and EMNLP-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentiment Analysis, Opinion M"
P19-4003,P18-1052,1,0.842032,"versational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB (b) Identifying discourse connectives (c) Implicit"
P19-4003,D17-1134,0,0.0175241,"n & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eisenstein, 2015; Lan et al., 2017) (d) Evaluation & Discussion (a) Coherence structure ⇒ Discourse segmentation & parsing (b) Coherence models ⇒ Coherence evaluation (c) Topic structure ⇒ Topic segmentation & labeling [not covered in this tutorial] (d) Coreference structure ⇒ Coreference resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Struct"
P19-4003,N19-1134,1,0.796203,"-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentiment Analysis, Opinion Mining, and Text Classific"
P19-4003,N03-1030,0,0.333163,"; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse"
P19-4003,D17-1136,0,0.0218375,"ls and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova"
P19-4003,J00-3003,0,0.132393,"Missing"
P19-4003,W17-5535,1,0.832508,"undamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; L¨aubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model"
P19-4003,N09-1064,0,0.0816957,"Missing"
P19-4003,D12-1009,0,0.0198669,"for ACL-2019 (QA track) and EMNLP-2019 (Discourse track) and a senior program committee member for IJCAI 2019. Shafiq is a recipient of NSERC CGS-D scholarship and Microsoft Research Excellent Intern award. 1. Conversational structures (a) Speech (or dialog) acts in synchronous and asynchronous conversations (b) Reply-to (thread) structure in asynchronous conversations (Carenini et al., 2007) (c) Conversation disentanglement in synchronous conversations 2. Computational models (a) Speech act recognition models (Stolcke et al., 2000; Cohen et al., 2004; Ritter et al., 2010; Joty et al., 2011; Paul, 2012; Joty and Hoque, 2016; Mohiuddin et al., 2019) (b) Thread reconstruction models (Shen et al., 2006; Wang et al., 2008, 2011a,b) (c) Conversation disentanglement models (Elsner and Charniak, 2008, 2011a) 3. Evaluation & Summary of results F. Future Challenges [15 mins] Dr. Giuseppe Carenini2 is a Professor in Computer Science at UBC. Giuseppe has broad interdisciplinary interests. His work on NLP and information visualization to support decision making has been published in over 100 peer-reviewed papers (including best paper at UMAP-14 and ACMTiiS-14). He was the area chair for ACL’09 “Sentime"
P19-4003,D08-1020,0,0.0631394,"rence resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applications of Discourse [45 mins] 4. Applications of discourse analysis B. Coherence Structure, Corpora & Discourse Parsing [45 mins] 1. Overview of coherence models (a) Entity grid and its extensions (Barzilay and Lapata, 2008; Elsner and Charniak, 2011b; Guinaudeau and Strube, 2013) (b) Discourse relation based model (Lin et al., 2011; Pitler and Nenkova, 2008) (c) Neural coherence models (Mohiuddin et al., 2018; Li and Jurafsky, 2017; Mesgar and Strube, 2018) (d) Coherence models for conversations (Elsner and Charniak, 2011a; Mohiuddin et al., 2018) 1. Discourse theories & coherence relations (a) Rhetorical Structure Theory (RST) & RST Treebank (Carlson et al., 2002) & Instructional domain (Subba and Di Eugenio, 2009) (b) Discourse Lexicalized Tree Adjoining Grammar (D-LTAG) & Penn Discourse Treebank (PDTB) (Prasad et al., 2005) 2. Discourse connectives & unsupervised relation identification 2. Evaluation tasks (a) Role of connectives in RST & PDTB"
P19-4003,D11-1002,0,0.22444,"Missing"
P19-4003,P09-2004,0,0.0347257,"orey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural models (Ji and Eisenstein, 2015; Lan et al., 2017) (d) Evaluation & Discussion (a) Coherence structure ⇒ Discourse segmentation & parsing (b) Coherence models ⇒ Coherence evaluation (c) Topic structure ⇒ Topic segmentation & labeling [not covered in this tutorial] (d) Coreference structure ⇒ Coreference resolution [not covered in this tutorial] (e) Conversational structure ⇒ Disentanglement & reply-to structure, speech act recognition 5. Final remarks (a) Tree vs. graph structure (b) Discourse Graphbank C. Coffee Break [15 mins] D. Coherence Models & Applicati"
P19-4003,P17-2029,0,0.0256295,"f challenges for the computational models. 12 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12–17 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (a) The tasks: discourse segmentation and parsing (b) Role of syntax (c) Traditional models – SPADE (Soricut and Marcu, 2003), HILDA (duVerle and Prendinger, 2009), CODRA (Joty et al., 2015), CRF-based model (Feng and Hirst, 2014). (d) Neural models (Ji and Eisenstein, 2014; Li et al., 2014, 2016; Morey et al., 2017) (e) State-of-the-Art (Wang et al., 2017; Lin et al., 2019) (f) Evaluation & Discussion ysis and its applications. In the following, we give a detailed breakdown of the tutorial content. A. Introduction [25 mins] 1. Discourse & its different forms (a) Monologue (b) Synchronous & asynchronous conversations (c) Modalities: written & spoken 2. Two discourse phenomena (a) Coherence (b) Cohesion 4. Discourse parsing in PDTB 3. Linguistic structures in discourse & discourse analysis tasks (a) The tasks: relation sense identification and scope disambiguation (b) Statistical models (Pitler and Nenkova, 2009; Ziheng et al., 2014) (c) Neural"
S07-1106,W97-0703,0,0.172558,"nd successors) in a text. (Halliday and Hasan, 1976) defined cohesion as “the set of possibilities that exist in the language for making text hang together”. Cohesion occurs where the interpretation of some element in the discourse is dependent on that of another. For example, an understanding of the reference of a pronoun (i.e.: he, she, it, etc.) requires to look back to something that has been said before. Through this cohesion relation, two text clauses are linked together. Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997). Among the different cohesion-building devices, the most easily identifiable and the most frequent type is Previous Work Lexical Chaining is the process of connecting semantically related words, creating a set of chains that represent different threads of cohesion through the text (Galley and McKeown, 2003). This intermediate representation of text has been used in many natural language processing applications, including automatic summarization (Barzilay and Elhadad, 1997; Silber and McCoy, 2003), information retrieval (Al-Halimi and Kazman, 1998), and intelligent spell checking (Hirst and St"
S07-1106,J02-4004,0,0.838429,"Missing"
S07-1106,J91-1002,0,\N,Missing
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S16-1138,P15-2113,1,0.112654,"Missing"
S16-1138,S15-2048,0,0.119863,"the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and com"
S16-1138,P15-4004,0,0.00840536,"the pairs as: K((t1 , t2 ), (u1 , u2 )) = T K(t1 , u1 ) + T K(t2 , u2 ), (1) where t and u are parse trees extracted from the text pair, i.e., either question and comment for task A or question and question for tasks B and D. 4 Submissions and Results We describe our primary submissions for the four tasks in Section 4.1. The contrastive submissions are discussed in Section 4.2. Table 1 shows our official competition results for both primary and contrastive submissions. In all submissions we employed Support Vector Machines (SVM) (Joachims, 1999) using either SVM-Light (Joachims, 1999), KeLP9 (Filice et al., 2015), or SVM-light-TK10 (Moschitti, 2006) (only the last two can handle tree kernels). 4.1 Task A. The submission consists in an SVM operating on two kernels: (i) the tree kernel described in Section 3.3, applied to the structures described by Tymoshenko and Moschitti (2015) without question and focus classification; (ii) a polynomial kernel of degree 3 applied to the feature vector that is a concatenation of the feature vector described in Section 3.1, and question and answer embeddings learned on the training set by the Convolutional Neural Network (CNN) described in (Severyn and Moschitti, 2015"
S16-1138,D15-1068,1,0.19243,"Missing"
S16-1138,N16-1084,1,0.885885,"Missing"
S16-1138,P14-5010,0,0.0035797,"found in (Nakov et al., 2016). 3 Approach In order to re-rank the comments according to their relevance, either against the forum questions or against the new questions, we train a binary SVM classifier and use its score as a measure of relevance. The classifier uses partial tree kernels (Moschitti, 2006) defined over shallow syntactic trees, along with other numeric features. We used the DKPro Core toolkit (Eckart de Castilho and Gurevych, 2014)5 for pre-processing the texts in English. More precisely, we used OpenNLP’s tokenizer, POS-tagger and chunk annotator6 , and Stanford’s lemmatizer (Manning et al., 2014), all accessible through DKPro Core. We used the MADAMIRA toolkit (Pasha et al., 2014) for segmenting Arabic texts. In order to split the texts into sentences, we used the Stanford splitter.7 For parsing Arabic texts into syntactic trees, we 3 http://www.qatarliving.com/forum https://www.webteb.com/, http://www. altibbi.com/, and http://consult.islamweb. net. 5 https://dkpro.github.io/dkpro-core/ 6 https://opennlp.apache.org/ 7 http://stanfordnlp.github.io/CoreNLP 4 used the Berkeley parser (Petrov and Klein, 2007). Following, we briefly describe the numeric features used in different tasks. 3"
S16-1138,N13-1090,0,0.00760434,"situations including the identification of potential dialogues, which usually represent a bunch of bad comments, or the position of the comment in the thread. We also considered the categories of the questions in the forum (as some of them tend to include more open-ended questions and even invite for discussion on ambiguous topics), as well as the occurrence of specific strings or the length of a comment. In-depth descriptions of these features are available in (Nicosia et al., 2015). For Arabic texts, we utilize the embedding vectors as obtained by Belinkov et al. (2015): employing word2vec (Mikolov et al., 2013) on the Arabic Gigaword corpus (Parker et al., 2011). More specifically, we concatenate the vectors representing a new question and an existing question in the question– answer pair, which is then fed to the SVM classifier. 3.2 Rank Feature The meta-information in the English corpus includes the position of the forum threads in the rank generated by the Google search engine for a given new question. We exploit this information in tasks B and C. We employ the inverse of such position as a feature and refer to it as the rank feature. 3.3 structed a syntactic tree for each comment or question. Ea"
S16-1138,P07-1098,1,0.0409837,"proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to"
S16-1138,S15-2047,1,0.299017,"; (B) to re-rank the set of questions Q according to their relevance against the new question q 0 ; and finally (C) to predict the appropriateness of the answers c ∈ Cq against q 0 . 1 http://alt.qcri.org/semeval2016/task3 Task 3 included these three tasks for English, whereas an adaptation of Task C was proposed for Arabic (Task D). The reader can refer to (Nakov et al., 2016) for a more detailed description of the tasks. Task A was also proposed in the SemEval2015 edition (Nakov et al., 2015).2 We designed systems for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko e"
S16-1138,S15-2036,1,0.501211,"Missing"
S16-1138,P02-1040,0,0.114288,"ll training and development sets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model,"
S16-1138,pasha-etal-2014-madamira,0,0.0108564,"Missing"
S16-1138,D13-1044,1,0.0339644,"ms for all tasks. We used the feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightl"
S16-1138,W13-3509,1,0.135312,"feature vectors designed by Barr´on-Cede˜no et al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh"
S16-1138,2006.amta-papers.25,0,0.0422727,"ets. The second contrastive submission consists of a rule-based system which relies on the outputs from tasks A and B. A comment is labeled as good if it is considered good with respect to the related question (Task A) and the related question is considered relevant with respect to the new question (Task B). The comment is considered bad otherwise. Task D. The contrastive systems did not use tree kernels. Our first contrastive run used only feature vectors. Our second contrastive run also used additional features borrowed from machine translation evaluation: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), Meteor (Lavie and Denkowski, 2009), NIST (Doddington, 2002), Precision and Recall, and length ratio between the question and the comment. 5 Results and Discussion Table 1 shows the results obtained in the four tasks. We achieved the second position for tasks A, B, and D. In Task A, tree kernels give no major boost, but without them our model would be cont2 , which 900 achieved the third position on the test set. The joint model cont1 , run on top of our primary system, was able to improve it by more than one point. We were not sure about the outcome of this model, thus we preferred not to us"
S16-1138,E14-1070,1,0.868985,"t al. (2015) and Nicosia et al. (2015) for tasks A, B and C, whereas we just used a basic feature vector derived from the system of Belinkov et al. (2015) for Task D. Most importantly, for tasks A, B and D, we combined feature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called"
S16-1138,N16-1152,1,0.718101,"eature vectors with tree kernels (Moschitti, 2006) for relational learning from short text (Moschitti et al., 2007; Moschitti, 2008). In particular, we used the improved models that have been successful applied for several tasks and datasets in standard QA, see for example, (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013; Severyn et al., 2013b; Severyn et al., 2013a; Tymoshenko et al., 2014; Tymoshenko and Moschitti, 2015). Additionally, we used Convolutional Neural Networks (CNNs) (Severyn and Moschitti, 2015) and combined them with vectors and tree kernels for Task A as we did in (Tymoshenko et al., 2016). We acknowledge that the automatic feature engineering of tree kernels was very useful to tackle the new challenges of the SemEval-2016 Task 3. Indeed, all our three systems using relational models based on tree kernels achieved the second official 2 Note that in that paper the naming convention is slightly different. The fresh user question and the forum question are called “original” and “related”, respectively. 896 Proceedings of SemEval-2016, pages 896–903, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics position. In contrast, for Task C, we did n"
S16-1138,N07-1051,0,\N,Missing
S16-1138,W14-5201,0,\N,Missing
W13-4017,W12-1616,0,0.100159,"s and bigrams have been shown to be useful for the task of DA modeling in previous studies (Sun, 2012; Ferschke, 2012; Kim, 2010a; Ravi, 2007; Carvalho, 2005). In addition, unigrams have been shown to be the most effective among the two. So, as the lexical feature, we include the frequency of unigrams in our feature set. Moreover, length of the utterance is another beneficial feature for DA recognition (Ferschke, 2012; Shrestha, 2004; Joty, 2011), which we add to our feature set. The speaker of an utterance has shown its utility for recognizing speech acts (Sun, 2012; Kim, 2010a; Joty, 2011). Sun and Morency (2012) specifically employ a speakeradaptation technique to demonstrate the effectiveness of this feature for DA modeling. We also include the relative position of a sentence in a post for DA modeling since most of previous studies (Ferschke, 2012; Kim, 2010a; Joty, 2011) prove the efficiency of this feature. Dialogue Act Recognition Conversational structure Adjacent utterances in a conversation have a strong correlation in terms of their dialogue acts. As an example, if speaker 1 asks a question to speaker 2, it is a high probability that the next utterance of the conversation would be an answer fr"
W13-4017,W10-4211,1,0.366658,"Missing"
W13-4017,C04-1128,0,0.0163628,"kes it possible to automatically 2 Related Work There have been several studies on supervised dialogue act (DA) modeling. To the best of our knowledge, none of them compare the performance of DA recognition on different synchronous (e.g., meeting and phone) and asynchronous (e.g., email and forum) conversations. Most of the works analyze DA modeling in a specific domain. Carvalho and Cohen (2005) propose classifying emails into their dialogue acts according to two ontologies for nouns and verbs. The ontologies are used for determining the speech acts of each single email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia disc"
W13-4017,D09-1130,0,0.0886966,"Missing"
W13-4017,E12-1079,0,0.094304,"le email with verb-noun pairs. Shrestha and McKeown (2004) also study the 117 Proceedings of the SIGDIAL 2013 Conference, pages 117–121, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics we build the Fragment Quotation Graph. To this end, we follow the procedure proposed by Joty et al. (2011) to extract the graph structure of a thread. problem of DA modeling in email conversations considering the two dialogue acts of question and answer. Likewise, Ravi and Kin (2007) present a DA recognition method for detecting questions and answers in educational discussions. Ferschke et al. (2012) apply DA modeling to Wikipedia discussions to analyze the collaborative process of editing Wikipedia pages. Kim et al. (2010a) study the task of supervised classification of dialogue acts in one-to-one online chats in the shopping domain. All these previous studies focus on DA recognition in one or two domains, and do not systematically analyze the performance of different dialog act modeling approaches on a comprehensive set of conversation domains. As far as we know, the present work is the first that proposes domain-independent supervised DA modeling techniques, and analyzes their effectiv"
W13-4017,D09-1035,0,0.0178444,"Missing"
W13-4017,W04-2319,0,\N,Missing
W13-4017,D10-1084,0,\N,Missing
W13-4017,W10-2923,0,\N,Missing
W14-3107,P03-1071,0,0.0513609,"g, we group the sentences of a blog conversation into a number of topical clusters and label each cluster by assigning a short informative topic descriptor (i.e., a keyphrase). To find the topical clusters and their associated labels, we apply the topic segmentation and labeling models recently proposed by (Joty et al., 2013b) for asynchronous conversations, and successfully evaluated on email and blog datasets. More specifically, for topic segmentation, we use their best unsupervised topic segmentation model LCSeg+FQG, which extends the generic lexical cohesion based topic segmenter (LCSeg) (Galley et al., 2003) 4.1 Tasks To understand the blog reading tasks, we reviewed the literature focusing on why and how people read blogs. From the analysis, we found that the primary goals of reading blogs include information seeking, fact checking, guidance/opinion seeking, and political surveillance (Kaye, 2005). People may also read blogs to connect to their communities of interest (Dave et al., 2004; Mishne, 2006), or just for fun/ enjoyment (Baumer et al., 2008; Kaye, 2005). Some studies have also revealed interesting behavioural patterns of blog readers. For example, people often look for variety of opinio"
W14-3107,D09-1026,0,0.00774011,"and more generic in other cases. As such, the topic model needs to be revised based 49 on user feedback to better support her analysis tasks. Thus, our goal is to support a human-inthe-loop topic modeling for asynchronous conversations via interactive visualization. There have been some recent works for incorporating user supervision in probabilistic topic models (e.g., Latent Dirichlet Allocation (LDA)) by adding constraints in the form of must-link and cannot-link (Andrzejewski et al., 2009; Hu et al., 2011), or in the form of a one-to-one mapping between LDA’s latent topics and user tags (Ramage et al., 2009). The feedback from users has been also integrated through visualizations, that steers a semi-supervised topic model (Choo et al., 2013). In contrast to the above-mentioned methods that are designed for generic documents, we are focusing on how our topic modeling approach that is specific to asynchronous conversations, can be steered by the end-users. We are planning to combine a visual interface for expressing the user’s intention via a set of actions, and a semi-supervised version of the topic model that can be iteratively refined from such user actions. A set of possible topic revision oper"
W14-3107,P11-1026,0,0.0245055,"l model and current tasks, the topic modeling results may require to be more specific in some cases, and more generic in other cases. As such, the topic model needs to be revised based 49 on user feedback to better support her analysis tasks. Thus, our goal is to support a human-inthe-loop topic modeling for asynchronous conversations via interactive visualization. There have been some recent works for incorporating user supervision in probabilistic topic models (e.g., Latent Dirichlet Allocation (LDA)) by adding constraints in the form of must-link and cannot-link (Andrzejewski et al., 2009; Hu et al., 2011), or in the form of a one-to-one mapping between LDA’s latent topics and user tags (Ramage et al., 2009). The feedback from users has been also integrated through visualizations, that steers a semi-supervised topic model (Choo et al., 2013). In contrast to the above-mentioned methods that are designed for generic documents, we are focusing on how our topic modeling approach that is specific to asynchronous conversations, can be steered by the end-users. We are planning to combine a visual interface for expressing the user’s intention via a set of actions, and a semi-supervised version of the t"
W14-3107,P13-1048,1,0.848055,"of communicating for most people, other conversational modalities such as blogs, microblogs (e.g., Twitter) and discussion fora have quickly become widely popular. Since the nature of data and tasks may vary significantly from one domain to the other, rather than trying to build an one-sizefit-all interface, we follow a design methodology that is driven by modeling the tasks and usage characteristics in a specific domain. In this work, we focus on blogs, where people can express their thoughts and engage in online discussions. Due to the large number of comments with complex thread structure (Joty et al., 2013b), mining and visualizing blog conversations can become a challenging problem. However, the visualization can be effective for other threaded discussions (e.g., news stories, Youtube comments). Users: As shown in Table 1, blog users can be categorized into two groups based on their activities: (a) participants who already contributed to the conversations, and (b) non-participants who wish to join the conversations or analyze the conversations. Depending on different user groups the tasks might vary as well, something that needs to be taken into account in the design process. For example, imag"
W14-3107,J11-2001,0,0.0150223,"FQG captures the reply relations between text fragments, which are extracted by analyzing the actual body of the comments, thus provides a finer representation of the conversation than the reply-to structure. Similarly, the topic labels are found by using their best unsupervised graph-based ranking model (i.e., BiasedCorank) that extracts representative keyphrases for each topical segment by combining informative clues from initial sentences of the segment and the fine-grain conversational structure, i.e., the FQG. For sentiment analysis, we apply the Semantic Orientation CALculator (SO-CAL) (Taboada et al., 2011), which is a lexicon-based approach (i.e., unsupervised) for determining sentiment of a text. Its performance is consistent across various domains and on completely unseen data, thus making a suitable tool for our purpose. We define five different polarity intervals (-2 to +2), and for each comment we count how many sentences fall in any of these polarity intervals to compute the polarity distribution for that comment. While designing and implementing ConVis, we have been mainly working with blog conversations from two different sources: Slashdot1 — a technology related blog site, and Daily Ko"
W14-3352,E06-1032,0,0.104635,"translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to the combination other metrics fr"
W14-3352,W07-0734,0,0.111099,"Missing"
W14-3352,D08-1024,0,0.0117943,"se years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous"
W14-3352,W14-3336,0,0.123967,"Missing"
W14-3352,2003.mtsummit-papers.9,0,0.0555949,"part raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to th"
W14-3352,P07-1098,0,0.0527917,"words in an Elementary Discourse Unit (EDU) are grouped under a predefined tag EDU, to which the nuclearity status of the EDU is attached: nucleus vs. satellite. Coherence relations, such as Attribution, Elaboration, and Enablement, between adjacent text spans constitute the internal nodes of the tree. Like the EDUs, the nuclearity statuses of the larger discourse units are attached to the relation labels. Notice that with this representation the tree kernel can easily be extended to find subtree matches at the word level, i.e., by including an additional layer of dummy leaves as was done in (Moschitti et al., 2007). We applied the same solution in our representations. Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolutio"
W14-3352,2003.mtsummit-papers.10,0,0.311329,"trics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Ne"
W14-3352,2012.amta-papers.6,0,0.0421525,"@qf.org.qa Abstract Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design bette"
W14-3352,P03-1021,0,0.00737295,"orms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Associatio"
W14-3352,P02-1040,0,0.100492,"nally, we add other metrics from the A SIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mea"
W14-3352,2006.amta-papers.25,0,0.163958,"ent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that are based on new representations of the DTs. In the remainder of this section, we will focus on the individual DT representations that"
W14-3352,P14-1065,1,0.494914,"Missing"
W14-3352,D07-1080,0,0.0139657,"hat participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, w"
W14-3352,D11-1125,0,0.0213356,"segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that"
W14-3352,D12-1083,1,\N,Missing
