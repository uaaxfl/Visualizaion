2021.starsem-1.30,Adversarial Training for Machine Reading Comprehension with Virtual Embeddings,2021,-1,-1,4,1,1014,ziqing yang,Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics,0,"Adversarial training (AT) as a regularization method has proved its effectiveness on various tasks. Though there are successful applications of AT on some NLP tasks, the distinguishing characteristics of NLP tasks have not been exploited. In this paper, we aim to apply AT on machine reading comprehension (MRC) tasks. Furthermore, we adapt AT for MRC tasks by proposing a novel adversarial training method called PQAT that perturbs the embedding matrix instead of word vectors. To differentiate the roles of passages and questions, PQAT uses additional virtual P/Q-embedding matrices to gather the global perturbations of words from passages and questions separately. We test the method on a wide range of MRC tasks, including span-based extractive RC and multiple-choice RC. The results show that adversarial training is effective universally, and PQAT further improves the performance."
2021.mrqa-1.10,Bilingual Alignment Pre-Training for Zero-Shot Cross-Lingual Transfer,2021,-1,-1,5,1,1014,ziqing yang,Proceedings of the 3rd Workshop on Machine Reading for Question Answering,0,"Multilingual pre-trained models have achieved remarkable performance on cross-lingual transfer learning. Some multilingual models such as mBERT, have been pre-trained on unlabeled corpora, therefore the embeddings of different languages in the models may not be aligned very well. In this paper, we aim to improve the zero-shot cross-lingual transfer performance by proposing a pre-training task named Word-Exchange Aligning Model (WEAM), which uses the statistical alignment information as the prior knowledge to guide cross-lingual word prediction. We evaluate our model on multilingual machine reading comprehension task MLQA and natural language interface task XNLI. The results show that WEAM can significantly improve the zero-shot performance."
2021.findings-acl.207,A Closer Look into the Robustness of Neural Dependency Parsers Using Better Adversarial Examples,2021,-1,-1,2,1,8010,yuxuan wang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.216,Dynamic Connected Networks for {C}hinese Spelling Check,2021,-1,-1,2,0.833333,8036,baoxin wang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.findings-acl.282,Learning to Bridge Metric Spaces: Few-shot Joint Learning of Intent Detection and Slot Filling,2021,-1,-1,4,1,8189,yutai hou,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.182,Don{'}t be Contradicted with Anything! {CI}-{T}o{D}: Towards Benchmarking Consistency for Task-oriented Dialogue System,2021,-1,-1,6,1,9005,libo qin,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Consistency Identification has obtained remarkable success on open-domain dialogue, which can be used for preventing inconsistent response generation. However, in contrast to the rapid development in open-domain dialogue, few efforts have been made to the task-oriented dialogue direction. In this paper, we argue that \textit{consistency problem} is more urgent in task-oriented domain. To facilitate the research, we introduce CI-ToD, a novel dataset for \textbf{C}onsistency \textbf{I}dentification in \textbf{T}ask-\textbf{o}riented \textbf{D}ialog system. In addition, we not only annotate the single label to enable the model to judge whether the system response is contradictory, but also provide more fine-grained labels (i.e., Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base Inconsistency) to encourage model to know what inconsistent sources lead to it. Empirical results show that state-of-the-art methods only achieve 51.3{\%}, which is far behind the human performance of 93.2{\%}, indicating that there is ample room for improving consistency identification ability. Finally, we conduct exhaustive experiments and qualitative analysis to comprehend key challenges and provide guidance for future directions. All datasets and models are publicly available at \url{https://github.com/yizhen20133868/CI-ToD}."
2021.emnlp-main.257,Allocating Large Vocabulary Capacity for Cross-Lingual Language Model Pre-Training,2021,-1,-1,5,1,9172,bo zheng,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Compared to monolingual models, cross-lingual models usually require a more expressive vocabulary to represent all languages adequately. We find that many languages are under-represented in recent cross-lingual language models due to the limited vocabulary capacity. To this end, we propose an algorithm VoCap to determine the desired vocabulary capacity of each language. However, increasing the vocabulary size significantly slows down the pre-training speed. In order to address the issues, we propose k-NN-based target sampling to accelerate the expensive softmax. Our experiments show that the multilingual vocabulary learned with VoCap benefits cross-lingual language model pre-training. Moreover, k-NN-based target sampling mitigates the side-effects of increasing the vocabulary size while achieving comparable performance and faster pre-training speed. The code and the pretrained multilingual vocabularies are available at https://github.com/bozheng-hit/VoCapXLM."
2021.emnlp-main.356,{D}u{R}ec{D}ial 2.0: A Bilingual Parallel Corpus for Conversational Recommendation,2021,-1,-1,5,0,9438,zeming liu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we provide a bilingual parallel human-to-human recommendation dialog dataset (DuRecDial 2.0) to enable researchers to explore a challenging task of multilingual and cross-lingual conversational recommendation. The difference between DuRecDial 2.0 and existing conversational recommendation datasets is that the data item (Profile, Goal, Knowledge, Context, Response) in DuRecDial 2.0 is annotated in two languages, both English and Chinese, while other datasets are built with the setting of a single language. We collect 8.2k dialogs aligned across English and Chinese languages (16.5k dialogs and 255k utterances in total) that are annotated by crowdsourced workers with strict quality control procedure. We then build monolingual, multilingual, and cross-lingual conversational recommendation baselines on DuRecDial 2.0. Experiment results show that the use of additional English data can bring performance improvement for Chinese conversational recommendation, indicating the benefits of DuRecDial 2.0. Finally, this dataset provides a challenging testbed for future studies of monolingual, multilingual, and cross-lingual conversational recommendation."
2021.emnlp-demo.6,N-{LTP}: An Open-source Neural Language Technology Platform for {C}hinese,2021,-1,-1,1,1,1017,wanxiang che,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations,0,"We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https://github.com/HIT-SCIR/ltp."
2021.acl-long.15,{GL}-{GIN}: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling,2021,-1,-1,5,1,9005,libo qin,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention. However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage. In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate. Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance. Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster."
2021.acl-long.136,Discovering Dialog Structure Graph for Coherent Dialog Generation,2021,-1,-1,6,0.627756,12893,jun xu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation. However, this problem is less studied in open-domain dialogue. In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems. To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph. Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system. Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence."
2021.acl-long.201,{L}ayout{LM}v2: Multi-modal Pre-training for Visually-rich Document Understanding,2021,-1,-1,10,0,2213,yang xu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672)."
2021.acl-long.264,Consistency Regularization for Cross-Lingual Fine-Tuning,2021,-1,-1,7,1,9172,bo zheng,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling."
2020.nlptea-1.5,Combining {R}es{N}et and Transformer for {C}hinese Grammatical Error Diagnosis,2020,-1,-1,11,1,15989,shaolei wang,Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications,0,"Grammatical error diagnosis is an important task in natural language processing. This paper introduces our system at NLPTEA-2020 Task: Chinese Grammatical Error Diagnosis (CGED). CGED aims to diagnose four types of grammatical errors which are missing words (M), redundant words (R), bad word selection (S) and disordered words (W). Our system is built on the model of multi-layer bidirectional transformer encoder and ResNet is integrated into the encoder to improve the performance. We also explore two ensemble strategies including weighted averaging and stepwise ensemble selection from libraries of models to improve the performance of single model. In official evaluation, our system obtains the highest F1 scores at identification level and position level. We also recommend error corrections for specific error types and achieve the second highest F1 score at correction level."
2020.findings-emnlp.58,Revisiting Pre-Trained Models for {C}hinese Natural Language Processing,2020,26,0,2,1,1015,yiming cui,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pre-trained language models. In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. https://github.com/ymcui/MacBERT"
2020.findings-emnlp.163,{AGIF}: An Adaptive Graph-Interactive Framework for Joint Multiple Intent Detection and Slot Filling,2020,-1,-1,3,1,9005,libo qin,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"In real-world scenarios, users usually have multiple intents in the same utterance. Unfortunately, most spoken language understanding (SLU) models either mainly focused on the single intent scenario, or simply incorporated an overall intent context vector for all tokens, ignoring the fine-grained multiple intents information integration for token-level slot prediction. In this paper, we propose an Adaptive Graph-Interactive Framework (AGIF) for joint multiple intent detection and slot filling, where we introduce an intent-slot graph interaction layer to model the strong correlation between the slot and intents. Such an interaction layer is applied to each token adaptively, which has the advantage to automatically extract the relevant intents information, making a fine-grained intent information integration for the token-level slot prediction. Experimental results on three multi-intent datasets show that our framework obtains substantial improvement and achieves the state-of-the-art performance. In addition, our framework achieves new state-of-the-art performance on two single-intent datasets."
2020.emnlp-main.142,Combining Self-Training and Self-Supervised Learning for Unsupervised Disfluency Detection,2020,-1,-1,3,1,15989,shaolei wang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice. There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora. In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain. Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training. Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA)."
2020.emnlp-main.634,Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting,2020,65,0,4,0,20611,sanyuan chen,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning. But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance. To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks. Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually. Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark. Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large. Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community."
2020.conll-shared.6,{HIT}-{SCIR} at {MRP} 2020: Transition-based Parser and Iterative Inference Parser,2020,-1,-1,4,0,20947,longxu dou,Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,0,"This paper describes our submission system (HIT-SCIR) for the CoNLL 2020 shared task: Cross-Framework and Cross-Lingual Meaning Representation Parsing. The task includes five frameworks for graph-based meaning representations, i.e., UCCA, EDS, PTG, AMR, and DRG. Our solution consists of two sub-systems: transition-based parser for Flavor (1) frameworks (UCCA, EDS, PTG) and iterative inference parser for Flavor (2) frameworks (DRG, AMR). In the final evaluation, our system is ranked 3rd among the seven team both in Cross-Framework Track and Cross-Lingual Track, with the macro-averaged MRP F1 score of 0.81/0.69."
2020.coling-main.589,A Sentence Cloze Dataset for {C}hinese Machine Reading Comprehension,2020,27,0,6,1,1015,yiming cui,Proceedings of the 28th International Conference on Computational Linguistics,0,"Owing to the continuous efforts by the Chinese NLP community, more and more Chinese machine reading comprehension datasets become available. To add diversity in this area, in this paper, we propose a new task called Sentence Cloze-style Machine Reading Comprehension (SC-MRC). The proposed task aims to fill the right candidate sentence into the passage that has several blanks. We built a Chinese dataset called CMRC 2019 to evaluate the difficulty of the SC-MRC task. Moreover, to add more difficulties, we also made fake candidates that are similar to the correct ones, which requires the machine to judge their correctness in the context. The proposed dataset contains over 100K blanks (questions) within over 10K passages, which was originated from Chinese narrative stories. To evaluate the dataset, we implement several baseline systems based on the pre-trained models, and the results show that the state-of-the-art model still underperforms human performance by a large margin. We release the dataset and baseline system to further facilitate our community. Resources available through https://github.com/ymcui/cmrc2019"
2020.acl-main.10,Slot-consistent {NLG} for Task-oriented Dialogue Systems with Iterative Rectification Network,2020,-1,-1,4,0,3590,yangming li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Data-driven approaches using neural networks have achieved promising performances in natural language generation (NLG). However, neural generators are prone to make mistakes, e.g., neglecting an input slot value and generating a redundant slot value. Prior works refer this to hallucination phenomenon. In this paper, we study slot consistency for building reliable NLG systems with all slot values of input dialogue act (DA) properly generated in output sentences. We propose Iterative Rectification Network (IRN) for improving general NLG systems to produce both correct and fluent responses. It applies a bootstrapping algorithm to sample training candidates and uses reinforcement learning to incorporate discrete reward related to slot inconsistency into training. Comprehensive studies have been conducted on multiple benchmark datasets, showing that the proposed methods have significantly reduced the slot error rate (ERR) for all strong baselines. Human evaluations also have confirmed its effectiveness."
2020.acl-main.98,Towards Conversational Recommendation over Multi-Type Dialogs,2020,35,0,5,0,9438,zeming liu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We focus on the study of conversational recommendation in the context of multi-type dialogs, where the bots can proactively and naturally lead a conversation from a non-recommendation dialog (e.g., QA) to a recommendation dialog, taking into account user{'}s interests and feedback. To facilitate the study of this task, we create a human-to-human Chinese dialog dataset DuRecDial (about 10k dialogs, 156k utterances), where there are multiple sequential dialogs for a pair of a recommendation seeker (user) and a recommender (bot). In each dialog, the recommender proactively leads a multi-type dialog to approach recommendation targets and then makes multiple recommendations with rich interaction behavior. This dataset allows us to systematically investigate different parts of the overall problem, e.g., how to naturally lead a dialog, how to interact with users for recommendation. Finally we establish baseline results on DuRecDial for future studies."
2020.acl-main.128,Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network,2020,-1,-1,2,1,8189,yutai hou,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot). Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels. But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets. To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores. In the few-shot setting, the emission score of CRF can be calculated as a word{'}s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model {--} TapNet, by leveraging label name semantics in representing labels. Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting."
2020.acl-main.166,Conversational Graph Grounded Policy Learning for Open-Domain Conversation Generation,2020,-1,-1,5,0.627756,12893,jun xu,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog. To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent {``}what to say{''} and {``}how to say{''}, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response. We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation. In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy. Results on two benchmark corpora demonstrate the effectiveness of this framework."
2020.acl-main.565,Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog,2020,18,0,3,1,9005,libo qin,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Recent studies have shown remarkable success in end-to-end task-oriented dialog system. However, most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. However, there has been relatively little research on how to effectively use data from all domains to improve the performance of each domain and also unseen domains. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. In addition, we propose a novel Dynamic Fusion Network (DF-Net) which automatically exploit the relevance between the target domain and each domain. Results show that our models outperforms existing methods on multi-domain dialogue, giving the state-of-the-art in the literature. Besides, with little training data, we show its transferability by outperforming prior best model by 13.9{\%} on average."
2020.acl-main.599,Document Modeling with Graph Attention Networks for Multi-grained Machine Reading Comprehension,2020,34,0,5,1,9172,bo zheng,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Natural Questions is a new challenging machine reading comprehension benchmark with two-grained answers, which are a long answer (typically a paragraph) and a short answer (one or more entities inside the long answer). Despite the effectiveness of existing methods on this benchmark, they treat these two sub-tasks individually during training while ignoring their dependencies. To address this issue, we present a novel multi-grained machine reading comprehension framework that focuses on modeling documents at their hierarchical nature, which are different levels of granularity: documents, paragraphs, sentences, and tokens. We utilize graph attention networks to obtain different levels of representations so that they can be learned simultaneously. The long and short answers can be extracted from paragraph-level representation and token-level representation, respectively. In this way, we can model the dependencies between the two-grained answers to provide evidence for each other. We jointly train the two sub-tasks, and our experiments show that our approach significantly outperforms previous systems at both long and short answer criteria."
2020.acl-demos.2,{T}ext{B}rewer: {A}n {O}pen-{S}ource {K}nowledge {D}istillation {T}oolkit for {N}atural {L}anguage {P}rocessing,2020,27,1,4,1,1014,ziqing yang,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"In this paper, we introduce TextBrewer, an open-source knowledge distillation toolkit designed for natural language processing. It works with different neural network models and supports various kinds of supervised learning tasks, such as text classification, reading comprehension, sequence labeling. TextBrewer provides a simple and uniform workflow that enables quick setting up of distillation experiments with highly flexible configurations. It offers a set of predefined distillation methods and can be extended with custom code. As a case study, we use TextBrewer to distill BERT on several typical NLP tasks. With simple configurations, we achieve results that are comparable with or even higher than the public distilled BERT models with similar numbers of parameters."
P19-1103,Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency,2019,0,20,4,0,6497,shuhuai ren,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We address the problem of adversarial attacks on text classification, which is rarely studied comparing to attacks on image classification. The challenge of this task is to generate adversarial examples that maintain lexical correctness, grammatical correctness and semantic similarity. Based on the synonyms substitution strategy, we introduce a new word replacement order determined by both the word saliency and the classification probability, and propose a greedy algorithm called probability weighted word saliency (PWWS) for text adversarial attack. Experiments on three popular datasets using convolutional as well as LSTM models show that PWWS reduces the classification accuracy to the most extent, and keeps a very low word substitution rate. A human evaluation study shows that our generated adversarial examples maintain the semantic similarity well and are hard for humans to perceive. Performing adversarial training using our perturbed datasets improves the robustness of the models. At last, our method also exhibits a good transferability on the generated adversarial examples."
K19-2007,{HIT}-{SCIR} at {MRP} 2019: A Unified Pipeline for Meaning Representation Parsing via Efficient Training and Effective Encoding,2019,0,2,1,1,1017,wanxiang che,Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 Conference on Natural Language Learning,0,"This paper describes our system (HIT-SCIR) for CoNLL 2019 shared task: Cross-Framework Meaning Representation Parsing. We extended the basic transition-based parser with two improvements: a) Efficient Training by realizing Stack LSTM parallel training; b) Effective Encoding via adopting deep contextualized word embeddings BERT. Generally, we proposed a unified pipeline to meaning representation parsing, including framework-specific transition-based parsers, BERT-enhanced word representation, and post-processing. In the final evaluation, our system was ranked first according to ALL-F1 (86.2{\%}) and especially ranked first in UCCA framework (81.67{\%})."
D19-1013,Entity-Consistent End-to-end Task-Oriented Dialogue System with {KB} Retriever,2019,0,2,3,1,9005,libo qin,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Querying the knowledge base (KB) has long been a challenge in the end-to-end task-oriented dialogue system. Previous sequence-to-sequence (Seq2Seq) dialogue generation work treats the KB query as an attention over the entire KB, without the guarantee that the generated entities are consistent with each other. In this paper, we propose a novel framework which queries the KB in two steps to improve the consistency of generated entities. In the first step, inspired by the observation that a response can usually be supported by a single KB row, we introduce a KB retrieval component which explicitly returns the most relevant KB row given a dialogue history. The retrieval result is further used to filter the irrelevant entities in a Seq2Seq response generation model to improve the consistency among the output entities. In the second step, we further perform the attention mechanism to address the most correlated KB column. Two methods are proposed to make the training feasible without labeled retrieval data, which include distant supervision and Gumbel-Softmax technique. Experiments on two publicly available task oriented dialog datasets show the effectiveness of our model by outperforming the baseline systems and producing entity-consistent responses."
D19-1169,Cross-Lingual Machine Reading Comprehension,2019,19,4,2,1,1015,yiming cui,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data.In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English. Firstly, we present several back-translation approaches for CLMRC task which is straightforward to adopt. However, to exactly align the answer into source language is difficult and could introduce additional noise. In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language. We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD. The results show consistent and significant improvements over various state-of-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. Resources available: https://github.com/ymcui/Cross-Lingual-MRC"
D19-1214,A Stack-Propagation Framework with Token-Level Intent Detection for Spoken Language Understanding,2019,0,5,2,1,9005,libo qin,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Intent detection and slot filling are two main tasks for building a spoken language understanding (SLU) system. The two tasks are closely tied and the slots often highly depend on the intent. In this paper, we propose a novel framework for SLU to better incorporate the intent information, which further guiding the slot filling. In our framework, we adopt a joint model with Stack-Propagation which can directly use the intent information as input for slot filling, thus to capture the intent semantic knowledge. In addition, to further alleviate the error propagation, we perform the token-level intent detection for the Stack-Propagation framework. Experiments on two publicly datasets show that our model achieves the state-of-the-art performance and outperforms other previous methods by a large margin. Finally, we use the Bidirectional Encoder Representation from Transformer (BERT) model in our framework, which further boost our performance in SLU task."
D19-1575,Cross-Lingual {BERT} Transformation for Zero-Shot Dependency Parsing,2019,0,9,2,1,8010,yuxuan wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results."
D19-1600,A Span-Extraction Dataset for {C}hinese Machine Reading Comprehension,2019,0,12,3,1,1015,yiming cui,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Machine Reading Comprehension (MRC) has become enormously popular recently and has attracted a lot of attention. However, the existing reading comprehension datasets are mostly in English. In this paper, we introduce a Span-Extraction dataset for Chinese machine reading comprehension to add language diversities in this area. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. We also annotated a challenge set which contains the questions that need comprehensive understanding and multi-sentence inference throughout the context. We present several baseline systems as well as anonymous submissions for demonstrating the difficulties in this dataset. With the release of the dataset, we hosted the Second Evaluation Workshop on Chinese Machine Reading Comprehension (CMRC 2018). We hope the release of the dataset could further accelerate the Chinese machine reading comprehension research. Resources are available: https://github.com/ymcui/cmrc2018"
W18-3707,{C}hinese Grammatical Error Diagnosis using Statistical and Prior Knowledge driven Features with Probabilistic Ensemble Enhancement,2018,0,0,6,0.815318,13186,ruiji fu,Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper describes our system at NLPTEA-2018 Task {\#}1: Chinese Grammatical Error Diagnosis. Grammatical Error Diagnosis is one of the most challenging NLP tasks, which is to locate grammar errors and tell error types. Our system is built on the model of bidirectional Long Short-Term Memory with a conditional random field layer (BiLSTM-CRF) but integrates with several new features. First, richer features are considered in the BiLSTM-CRF model; second, a probabilistic ensemble approach is adopted; third, Template Matcher are used during a post-processing to bring in human knowledge. In official evaluation, our system obtains the highest F1 scores at identifying error types and locating error positions, the second highest F1 score at sentence level error detection. We also recommend error corrections for specific error types and achieve the best F1 performance among all participants."
P18-1129,Distilling Knowledge for Search-based Structured Prediction,2018,33,3,2,1,3664,yijia liu,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble{'}s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks {--} transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model{'}s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures."
N18-1088,Parsing Tweets into {U}niversal {D}ependencies,2018,35,7,3,1,3664,yijia liu,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"We study the problem of analyzing tweets with universal dependencies (UD). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-of-speech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (Tweebank v2) that is four times larger than the (unlabeled) Tweebank v1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome the annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-of-the-art on other treebanks in both accuracy and speed."
K18-2005,"Towards Better {UD} Parsing: Deep Contextualized Word Embeddings, Ensemble, and Treebank Concatenation",2018,17,6,1,1,1017,wanxiang che,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This paper describes our system (HIT-SCIR) submitted to the CoNLL 2018 shared task on Multilingual Parsing from Raw Text to Universal Dependencies. We base our submission on Stanford{'}s winning system for the CoNLL 2017 shared task and make two effective extensions: 1) incorporating deep contextualized word embeddings into both the part of speech tagger and parser; 2) ensembling parsers trained with different initialization. We also explore different ways of concatenating treebanks for further improvements. Experimental results on the development data show the effectiveness of our methods. In the final evaluation, our system was ranked first according to LAS (75.84{\%}) and outperformed the other systems by a large margin."
D18-1264,An {AMR} Aligner Tuned by Transition-based Parser,2018,0,6,2,1,3664,yijia liu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the current state-of-the-art parser."
C18-1105,Sequence-to-Sequence Data Augmentation for Dialogue Language Understanding,2018,37,8,3,1,8189,yutai hou,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this paper, we study the problem of data augmentation for language understanding in task-oriented dialogue system. In contrast to previous work which augments an utterance without considering its relation with other utterances, we propose a sequence-to-sequence generation based data augmentation framework that leverages one utterance{'}s same semantic alternatives in the training data. A novel diversity rank is incorporated into the utterance representation to make the model produce diverse utterances and these diversely augmented utterances help to improve the language understanding module. Experimental results on the Airline Travel Information System dataset and a newly created semantic frame annotation on Stanford Multi-turn, Multi-domain Dialogue Dataset show that our framework achieves significant improvements of 6.38 and 10.04 F-scores respectively when only a training set of hundreds utterances is represented. Case studies also confirm that our method generates diverse utterances."
C18-1320,Sequence-to-Sequence Learning for Task-oriented Dialogue with Dialogue State Representation,2018,0,6,3,0.454545,3228,haoyang wen,Proceedings of the 27th International Conference on Computational Linguistics,0,"Classic pipeline models for task-oriented dialogue system require explicit modeling the dialogue states and hand-crafted action spaces to query a domain-specific knowledge base. Conversely, sequence-to-sequence models learn to map dialogue history to the response in current turn without explicit knowledge base querying. In this work, we propose a novel framework that leverages the advantages of classic pipeline and sequence-to-sequence models. Our framework models a dialogue state as a fixed-size distributed representation and use this representation to query a knowledge base via an attention mechanism. Experiment on Stanford Multi-turn Multi-domain Task-oriented Dialogue Dataset shows that our framework significantly outperforms other sequence-to-sequence based baseline models on both automatic and human evaluation."
P17-4003,{B}enben: A {C}hinese Intelligent Conversational Robot,2017,7,1,5,0,8543,weinan zhang,"Proceedings of {ACL} 2017, System Demonstrations",0,None
K17-3005,The {HIT}-{SCIR} System for End-to-End Parsing of {U}niversal {D}ependencies,2017,9,3,1,1,1017,wanxiang che,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This paper describes our system (HIT-SCIR) for the CoNLL 2017 shared task: Multilingual Parsing from Raw Text to Universal Dependencies. Our system includes three pipelined components: \textit{tokenization}, \textit{Part-of-Speech} (POS) \textit{tagging} and \textit{dependency parsing}. We use character-based bidirectional long short-term memory (LSTM) networks for both tokenization and POS tagging. Afterwards, we employ a list-based transition-based algorithm for general non-projective parsing and present an improved Stack-LSTM-based architecture for representing each transition state and making predictions. Furthermore, to parse low/zero-resource languages and cross-domain data, we use a model transfer approach to make effective use of existing resources. We demonstrate substantial gains against the UDPipe baseline, with an average improvement of 3.76{\%} in LAS of all languages. And finally, we rank the 4th place on the official test sets."
I17-5001,Deep Learning in Lexical Analysis and Parsing,2017,-1,-1,1,1,1017,wanxiang che,"Proceedings of the {IJCNLP} 2017, Tutorial Abstracts",0,"Neural networks, also with a fancy name deep learning, just right can overcome the above {``}feature engineering{''} problem. In theory, they can use non-linear activation functions and multiple layers to automatically find useful features. The novel network structures, such as convolutional or recurrent, help to reduce the difficulty further. These deep learning models have been successfully used for lexical analysis and parsing. In this tutorial, we will give a review of each line of work, by contrasting them with traditional statistical methods, and organizing them in consistent orders."
D17-1296,Transition-Based Disfluency Detection using {LSTM}s,2017,0,6,2,1,15989,shaolei wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art f-score of 87.5{\%} on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data."
W16-4907,{C}hinese Grammatical Error Diagnosis with Long Short-Term Memory Networks,2016,9,4,2,1,9172,bo zheng,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"Grammatical error diagnosis is an important task in natural language processing. This paper introduces our Chinese Grammatical Error Diagnosis (CGED) system in the NLP-TEA-3 shared task for CGED. The CGED system can diagnose four types of grammatical errors which are redundant words (R), missing words (M), bad word selection (S) and disordered words (W). We treat the CGED task as a sequence labeling task and describe three models, including a CRF-based model, an LSTM-based model and an ensemble model using stacking. We also show in details how we build and train the models. Evaluation includes three levels, which are detection level, identification level and position level. On the CGED-HSK dataset of NLP-TEA-3 shared task, our system presents the best F1-scores in all the three levels and also the best recall in the last two levels."
S16-1167,{S}em{E}val-2016 Task 9: {C}hinese Semantic Dependency Parsing,2016,6,5,1,1,1017,wanxiang che,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"This paper describes the SemEval-2016 Shared Task 9: Chinese semantic Dependency Parsing. We extend the traditional treestructured representation of Chinese sentence to directed acyclic graphs that can capture richer latent semantics, and the goal of this task is to identify such semantic structures from a corpus of Chinese sentences. We provide two distinguished corpora in the NEWS domain with 10,068 sentences and the TEXTBOOKS domain with 14,793 sentences respectively. We will first introduce the motivation for this task, and then present the task in detail including data preparation, data format, task evaluation and so on. At last, we briefly describe the submitted systems and analyze these results."
C16-1002,A Universal Framework for Inductive Transfer Parsing across Multi-typed Treebanks,2016,43,9,2,1,26099,jiang guo,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Various treebanks have been released for dependency parsing. Despite that treebanks may belong to different languages or have different annotation schemes, they contain common syntactic knowledge that is potential to benefit each other. This paper presents a universal framework for transfer parsing across multi-typed treebanks with deep multi-task learning. We consider two kinds of treebanks as source: the multilingual universal treebanks and the monolingual heterogeneous treebanks. Knowledge across the source and target treebanks are effectively transferred through multi-level parameter sharing. Experiments on several benchmark datasets in various languages demonstrate that our approach can make effective use of arbitrary source treebanks to improve target parsing models."
C16-1027,A Neural Attention Model for Disfluency Detection,2016,11,5,2,1,15989,shaolei wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we study the problem of disfluency detection using the encoder-decoder framework. We treat disfluency detection as a sequence-to-sequence problem and propose a neural attention-based model which can efficiently model the long-range dependencies between words and make the resulting sentence more likely to be grammatically correct. Our model firstly encode the source sentence with a bidirectional Long Short-Term Memory (BI-LSTM) and then use the neural attention as a pointer to select an ordered sub sequence of the input as the output. Experiments show that our model achieves the state-of-the-art f-score of 86.7{\%} on the commonly used English Switchboard test set. We also evaluate the performance of our model on the in-house annotated Chinese data and achieve a significantly higher f-score compared to the baseline of CRF-based approach."
C16-1120,A Unified Architecture for Semantic Role Labeling and Relation Classification,2016,26,3,2,1,26099,jiang guo,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper describes a unified neural architecture for identifying and classifying multi-typed semantic relations between words in a sentence. We investigate two typical and well-studied tasks: semantic role labeling (SRL) which identifies the relations between predicates and arguments, and relation classification (RC) which focuses on the relation between two entities or nominals. While mostly studied separately in prior work, we show that the two tasks can be effectively connected and modeled using a general architecture. Experiments on CoNLL-2009 benchmark datasets show that our SRL models significantly outperform state-of-the-art approaches. Our RC models also yield competitive performance with the best published records. Furthermore, we show that the two tasks can be trained jointly with multi-task learning, resulting in additive significant improvements for SRL."
P15-1119,Cross-lingual Dependency Parsing Based on Distributed Representations,2015,55,85,2,1,26099,jiang guo,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper investigates the problem of cross-lingual dependency parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g. English). Existing approaches typically donxe2x80x99t include lexical features, which are not transferable across languages. In this paper, we bridge the lexical feature gap by using distributed feature representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is able to incorporate additional useful features such as cross-lingual word clusters. Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly outperforms McDonald et al. (2013) augmented with projected cluster features on identical data."
N15-1012,Transition-Based Syntactic Linearization,2015,21,13,3,1,3664,yijia liu,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Syntactic linearization algorithms take a bag of input words and a set of optional constraints, and construct an output sentence and its syntactic derivation simultaneously. The search problem is NP-hard, and the current best results are achieved by bottom-up bestfirst search. One drawback of the method is low efficiency; and there is no theoretical guarantee that a full sentence can be found within bounded time. We propose an alternative algorithm that constructs output structures from left to right using beam-search. The algorithm is based on incremental parsing algorithms. We extend the transition system so that word ordering is performed in addition to syntactic parsing, resulting in a linearization system that runs in guaranteed quadratic time. In standard evaluations, our system runs an order of magnitude faster than a state-of-the-art baseline using best-first search, with improved accuracies."
P14-1113,Learning Semantic Hierarchies via Word Embeddings,2014,32,141,4,0.815318,13186,ruiji fu,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Semantic hierarchy construction aims to build structures of concepts linked by hypernymxe2x80x90hyponym (xe2x80x9cis-axe2x80x9d) relations. A major challenge for this task is the automatic discovery of such relations. This paper proposes a novel and effective method for the construction of semantic hierarchies based on word embeddings, which can be used to measure the semantic relationship between words. We identify whether a candidate word pair has hypernymxe2x80x90hyponym relation by using the word-embedding-based semantic projections between words and their hypernyms. Our result, an F-score of 73.74%, outperforms the state-of-theart methods on a manually labeled test dataset. Moreover, combining our method with a previous manually-built hierarchy extension method can further improve Fscore to 80.29%."
P14-1125,Character-Level {C}hinese Dependency Parsing,2014,30,42,3,1,6801,meishan zhang,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures. In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters. Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies. We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing. Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods."
E14-1062,Type-Supervised Domain Adaptation for Joint Segmentation and {POS}-Tagging,2014,20,27,3,1,6801,meishan zhang,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,None
D14-1012,Revisiting Embedding Features for Simple Semi-supervised Learning,2014,29,83,2,1,26099,jiang guo,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Recent work has shown success in using continuous word embeddings learned from unlabeled data as features to improve supervised NLP systems, which is regarded as a simple semi-supervised learning mechanism. However, fundamental problems on effectively incorporating the word embedding features within the framework of linear models remain. In this study, we investigate and analyze three different approaches, including a new proposed distributional prototype approach, for utilizing the embedding features. The presented approaches can be integrated into most of the classical linear models in NLP. Experiments on the task of named entity recognition show that each of the proposed approaches can better utilize the word embedding features, among which the distributional prototype approach performs the best. Moreover, the combination of the approaches provides additive improvements, outperforming the dense and continuous embedding features by nearly 2 points of F1 score."
D14-1093,Domain Adaptation for {CRF}-based {C}hinese Word Segmentation using Free Annotations,2014,23,32,3,1,3664,yijia liu,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Supervised methods have been the dominant approach for Chinese word segmentation. The performance can drop significantly when the test domain is different from the training domain. In this paper, we study the problem of obtaining partial annotation from freely available data to help Chinese word segmentation on different domains. Different sources of free annotations are transformed into a unified form of partial annotation and a variant CRF model is used to leverage both fully and partially annotated data consistently. Experimental results show that the Chinese word segmentation model benefits from free partially annotated data. On the SIGHAN Bakeoff 2010 data, we achieve results that are competitive to the best reported in the literature."
C14-1048,Learning Sense-specific Word Embeddings By Exploiting Bilingual Resources,2014,33,35,2,1,26099,jiang guo,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Recent work has shown success in learning word embeddings with neural network language models (NNLM). However, the majority of previous NNLMs represent each word with a single embedding, which fails to capture polysemy. In this paper, we address this problem by representing words with multiple and sense-specific embeddings, which are learned from bilingual parallel data. We evaluate our embeddings using the word similarity measurement and show that our approach is significantly better in capturing the sense-level word similarities. We further feed our embeddings as features in Chinese named entity recognition and obtain noticeable improvements against single embeddings."
C14-1051,Jointly or Separately: Which is Better for Parsing Heterogeneous Dependencies?,2014,35,4,2,1,6801,meishan zhang,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"For languages such as English, several constituent-to-dependency conversion schemes are proposed to construct corpora for dependency parsing. It is hard to determine which scheme is better because they reflect different views of dependency analysis. We usually obtain dependency parsers of different schemes by training with the specific corpus separately. It neglects the correlations between these schemes, which can potentially benefit the parsers. In this paper, we study how these correlations influence final dependency parsing performances, by proposing a joint model which can make full use of the correlations between heterogeneous dependencies, and finally we can answer the following question: parsing heterogeneous dependencies jointly or separately, which is better? We conduct experiments with two different schemes on the Penn Treebank and the Chinese Penn Treebank respectively, arriving at the same conclusion that jointly parsing heterogeneous dependencies can give improved performances for both schemes over the individual models."
C14-1129,Sentence Compression for Target-Polarity Word Collocation Extraction,2014,32,11,2,0,6855,yanyan zhao,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Target-polarity word (T-P) collocation extraction, a basic sentiment analysis task, relies primarily on syntactic features to identify the relationships between targets and polarity words. A major problem of current research is that this task focuses on customer reviews, which are natural or spontaneous, thus posing a challenge to syntactic parsers. We address this problem by proposing a framework of adding a sentiment sentence compression (Sent Comp) step before performing T-P collocation extraction. Sent Comp seeks to remove the unnecessary information for sentiment analysis, thereby compressing a complicated sentence into one that is shorter and easier to parse. We apply a discriminative conditional random field model, with some special sentimentrelated features, in order to automatically compress sentiment sentences. Experiments show that Sent Comp significantly improves the performance of T-P collocation extraction."
P13-1013,{C}hinese Parsing Exploiting Characters,2013,21,50,3,1,6801,meishan zhang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Characters play an important role in the Chinese language, yet computational processing of Chinese has been dominated by word-based approaches, with leaves in syntax trees being words. We investigate Chinese parsing from the character-level, extending the notion of phrase-structure trees by annotating internal structures of words. We demonstrate the importance of character-level information to Chinese processing by building a joint segmentation, part-of-speech (POS) tagging and phrase-structure parsing system that integrates character-structure features. Our joint system significantly outperforms a state-of-the-art word-based baseline on the standard CTB5 test, and gives the best published results for Chinese parsing."
P13-1106,Joint Word Alignment and Bilingual Named Entity Recognition Using Dual Decomposition,2013,31,26,2,0,39074,mengqiu wang,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Translated bi-texts contain complementary language cues, and previous work on Named Entity Recognition (NER) has demonstrated improvements in performance over monolingual taggers by promoting agreement of tagging decisions between the two languages. However, most previous approaches to bilingual tagging assume word alignments are given as fixed input, which can cause cascading errors. We observe that NER label information can be used to correct alignment mistakes, and present a graphical model that performs bilingual NER tagging jointly with word alignment, by combining two monolingual tagging models with two unidirectional alignment models. We introduce additional cross-lingual edge factors that encourage agreements between tagging and alignment decisions. We design a dual decomposition inference algorithm to perform joint decoding over the combined alignment and NER output space. Experiments on the OntoNotes dataset demonstrate that our method yields significant improvements in both NER and word alignment over state-of-the-art monolingual baselines."
N13-1006,Named Entity Recognition with Bilingual Constraints,2013,25,55,1,1,1017,wanxiang che,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Different languages contain complementary cues about entities, which can be used to improve Named Entity Recognition (NER) systems. We propose a method that formulates the problem of exploring such signals on unannotated bilingual text as a simple Integer Linear Program, which encourages entity tags to agree via bilingual constraints. Bilingual NER experiments on the large OntoNotes 4.0 Chinese-English corpus show that the proposed method can improve strong baselines for both Chinese and English. In particular, Chinese performance improves by over 5% absolute F1 score. We can then annotate a large amount of bilingual text (80k sentence pairs) using our method, and add it as uptraining data to the original monolingual NER training corpus. The Chinese model retrained on this new combined dataset outperforms the strong baseline by over 3% F1 score."
W12-6316,Micro blogs Oriented Word Segmentation System,2012,3,1,3,1,3664,yijia liu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"We present a Chinese word segmentation system submitted to the first task on CLP 2012 back-offs. Our segmenter is built using a conditional random field sequence model. We set the combination of a few annotated micro blogs and People Daily corpus as the training data. We encode special words detected by rules and information extracted from unlabeled data into features. These features are used to improve our modelxe2x80x99s performance. We also derive a micro blog specified lexicon from auto-analyzed data and use lexicon related features to assist the model. When testing on the sample data of this task, these features result in 1.8% improvement over the baseline model. Finally, our model achieves F-score of 94.07% on the bakeoffxe2x80x99s test set."
W12-6330,Multiple {T}ree{B}anks Integration for {C}hinese Phrase Structure Grammar Parsing Using Bagging,2012,9,1,2,1,6801,meishan zhang,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"We describe our method of traditional Phrase Structure Grammar (PSG) parsing in CIPS-Bakeoff2012 Task3. First, bagging is proposed to enhance the baseline performance of PSG parsing. Then we suggest exploiting another TreeBank (CTB7.0) to improve the performance further. Experimental results on the development data set demonstrate that bagging can boost the baseline F1 score from 81.33% to 84.41%. After exploiting the data of CTB7.0, the F1 score reaches 85.03%. Our final results on the official test data set show that the baseline closed system using bagging gets the F1 score of 80.17%. It outperforms the best closed system by nearly 4% which uses a single model. After exploiting the CTB7.0 data, the F1 score reaches 81.16%, demonstrating further increases of about 1%."
S12-1050,{S}em{E}val-2012 Task 5: {C}hinese Semantic Dependency Parsing,2012,15,11,1,1,1017,wanxiang che,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"The paper presents the SemEval-2012 Shared Task 5: Chinese Semantic Dependency Parsing. The goal of this task is to identify the dependency structure of Chinese sentences from the semantic view. We firstly introduce the motivation of providing Chinese semantic dependency parsing task, and then describe the task in detail including data preparation, data format, task evaluation, and so on. Over ten thousand sentences were labeled for participants to train and evaluate their systems. At last, we briefly describe the submitted systems and analyze these results."
P12-2003,A Comparison of {C}hinese Parsers for {S}tanford Dependencies,2012,39,12,1,1,1017,wanxiang che,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Stanford dependencies are widely used in natural language processing as a semantically-oriented representation, commonly generated either by (i) converting the output of a constituent parser, or (ii) predicting dependencies directly. Previous comparisons of the two approaches for English suggest that starting from constituents yields higher accuracies. In this paper, we re-evaluate both methods for Chinese, using more accurate dependency parsers than in previous work. Our comparison of performance and efficiency across seven popular open source parsers (four constituent and three dependency) shows, by contrast, that recent higher-order graph-based techniques can be more accurate, though somewhat slower, than constituent parsers. We demonstrate also that n-way jackknifing is a useful technique for producing automatic (rather than gold) part-of-speech tags to train Chinese dependency parsers. Finally, we analyze the relations produced by both kinds of parsing and suggest which specific parsers to use in practice."
P12-1071,Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars,2012,41,19,3,1,3691,zhenghua li,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We present a simple and effective framework for exploiting multiple monolingual treebanks with different annotation guidelines for parsing. Several types of transformation patterns (TP) are designed to capture the systematic annotation inconsistencies among different tree-banks. Based on such TPs, we design quasi-synchronous grammar features to augment the baseline parsing models. Our approach can significantly advance the state-of-the-art parsing accuracy on two widely used target tree-banks (Penn Chinese Treebank 5.1 and 6.0) using the Chinese Dependency Treebank as the source treebank. The improvements are respectively 1.37% and 1.10% with automatic part-of-speech tags. Moreover, an indirect comparison indicates that our approach also outperforms previous work based on treebank conversion."
C12-1103,A Separately Passive-Aggressive Training Algorithm for Joint {POS} Tagging and Dependency Parsing,2012,34,20,3,1,3691,zhenghua li,Proceedings of {COLING} 2012,0,"Recent study shows that parsing accuracy can be largely improved by the joint optimization of part-of-speech (POS) tagging and dependency parsing. However, the POS tagging task does not benefit much from the joint framework. We argue that the fundamental reason behind is because the POS features are overwhelmed by the syntactic features during the joint optimization, and the joint models only prefer such POS tags that are favourable solely from the parsing viewpoint. To solve this issue, we propose a separately passive-aggressive learning algorithm (SPA), which is designed to separately update the POS features weights and the syntactic feature weights under the joint optimization framework. The proposed SPA is able to take advantage of previous joint optimization strategies to significantly improve the parsing accuracy, but also overcome their shortages to significantly boost the tagging accuracy by effectively solving the syntax-insensitive POS ambiguity issues. Experiments on the Chinese Penn Treebank 5.1 (CTB5) and the English Penn Treebank (PTB) demonstrate the effectiveness of our proposed methodology and empirically verify our observations as discussed above. We achieve the best tagging and parsing accuracies on both datasets, 94.60% in tagging accuracy and 81.67% in parsing accuracy on CTB5, and 97.62% and 93.52% on PTB."
C12-1188,Stacking Heterogeneous Joint Models of {C}hinese {POS} Tagging and Dependency Parsing,2012,34,4,2,1,6801,meishan zhang,Proceedings of {COLING} 2012,0,"Previous joint models of Chinese part-of-speech (POS) tagging and dependency parsing are extended from either graphor transition-based dependency models. Our analysis shows that the two models have different error distributions. In addition, integration of graphand transition-based dependency parsers by stacked learning (stacking) has achieved significant improvements. These motivate us to study the problem of stacking graphand transition-based joint models. We conduct experiments on Chinese Penn Treebank 5.1 (CTB5.1). The results demonstrate that the guided transition-based joint model obtains better performance than the guided graph-based joint model. Further, we introduce a constituent-based joint model which derives the POS tag sequence and dependency tree from the output of PCFG parsers, and then integrate it into the guided transition-based joint model. Finally, we achieve the best performance on CTB5.1, 94.95% in tagging accuracy and 83.98% in parsing accuracy respectively. TITLE AND ABSTRACT IN CHINESE xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8dxe5x90x88xe5xbcx82xe7xa7x8dxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b xe8xbfx87xe5x8exbbxe7x9ax84xe4xb8xadxe6x96x87xe8xafx8dxe6x80xa7xe5x92x8cxe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe5x9fxbaxe6x9cxacxe4xb8x8axe9x83xbdxe6xa0xb9xe6x8dxaexe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe6x88x96xe8x80x85 xe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe5x88x86xe6x9ex90xe6xa8xa1xe5x9ex8bxe8xbfx9bxe8xa1x8cxe6x8bx93xe5xb1x95xe8x80x8cxe5xbdxa2xe6x88x90xe7x9ax84xe3x80x82xe6x88x91xe4xbbxacxe7x9ax84xe5x88x86xe6x9ex90xe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8exe8xbfx99xe4xb8xa4xe7xa7x8dxe4xb8x8dxe5x90x8cxe7x9ax84xe6xa8xa1xe5x9ex8b xe9x94x99xe8xafxafxe5x88x86xe5xb8x83xe5xb9xb6xe4xb8x8dxe4xb8x80xe6xa0xb7,xe8x80x8cxe4xb8x94xe5x9cxa8xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe4xb8xad,xe5xb0x86xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe6xa8xa1xe5x9ex8bxe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe6xa8xa1xe5x9ex8bxe4xbdxbfxe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe8x9ex8d xe5x90x88xe4xb9x8bxe5x90x8e,xe8x83xbdxe5xa4x9fxe6x98xbexe8x91x97xe7x9ax84xe6x8fx90xe5x8dx87xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe7x9ax84xe6x80xa7xe8x83xbd,xe8xbfx99xe4xbax9bxe4xbfx83xe4xbdxbfxe6x88x91xe4xbbxacxe8xbfx9bxe4xb8x80xe6xadxa5xe7xa0x94xe7xa9xb6xe9x87x87xe7x94xa8xe5xa0x86xe6x96xb9xe6xb3x95xe5x8exbbxe8x9ex8dxe5x90x88xe5x9fxba xe4xbax8exe5x9bxbexe7x9ax84xe5x92x8cxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8xafx8dxe6x80xa7xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe3x80x82xe6x88x91xe4xbbxacxe5x9cxa8xe4xb8xadxe6x96x87xe5xaexbexe5xb7x9exe6xa0x91xe5xbax935.1xe7x89x88xe6x9cxac(CTB5.1)xe4xb8x8a xe8xbfx9bxe8xa1x8cxe8xafx95xe9xaax8c,xe5xaex9exe9xaax8cxe7xbbx93xe6x9ex9cxe8xa1xa8xe6x98x8e,xe7x9bxb8xe6xafx94xe4xbdxbfxe7x94xa8xe5x9fxbaxe4xbax8exe5x9bxbexe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8b,xe9x87x87xe7x94xa8xe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8bxe4xb8xbaxe8xa2xabxe6x8cx87xe5xafxbcxe6xa8xa1xe5x9ex8bxe8x83xbdxe5x8fx96xe5xbex97xe8xbex83xe5xa5xbdxe7x9ax84xe6x80xa7xe8x83xbdxe3x80x82xe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5,xe6x88x91xe4xbbxacxe4xbbx8bxe7xbbx8dxe4xbax86xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1 xe5x9ex8b,xe5xaex83xe4xbbx8exe4xb8x80xe4xb8xaaxe5x8fxa5xe5xadx90xe7x9ax84xe6xa6x82xe7x8ex87xe7x9fxadxe8xafxadxe6x96x87xe6xb3x95xe5x88x86xe6x9ex90xe5x99xa8xe8xbex93xe5x87xbaxe7xbbx93xe6x9ex9cxe4xb8xadxe6x8fx90xe5x8fx96xe5x8fxa5xe5xadx90xe7x9ax84xe8xafx8dxe6x80xa7xe5xbax8fxe5x88x97xe4xbbxa5xe5x8fx8axe4xbex9dxe5xadx98xe6xa0x91xe7xbbx93 xe6x9ex9c,xe7x84xb6xe5x90x8exe6x88x91xe4xbbxacxe9x87x87xe7x94xa8xe5x9fxbaxe4xbax8exe7x9fxadxe8xafxadxe5x8fxa5xe6xb3x95xe7xbbx93xe6x9ex84xe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8bxe6x9bxb4xe8xbfx9bxe4xb8x80xe6xadxa5xe6x8cx87xe5xafxbcxe5x9fxbaxe4xbax8exe8xbdxacxe7xa7xbbxe7x9ax84xe8x81x94xe5x90x88xe6xa8xa1xe5x9ex8b,xe6x9cx80xe7xbbx88 xe6x88x91xe4xbbxacxe5x9cxa8CTB5.1xe7x9ax84xe6x95xb0xe6x8dxaexe4xb8x8axe5x8fx96xe5xbex97xe4xbax86xe6x9cx80xe5xa5xbdxe7xbbx93xe6x9ex9c,xe8xafx8dxe6x80xa7xe6xa0x87xe6xb3xa8xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb094.95%,xe5x90x8cxe6x97xb6,xe4xbex9dxe5xadx98xe5x8fxa5xe6xb3x95 xe5x87x86xe7xa1xaexe7x8ex87xe8xbexbexe5x88xb083.98%xe3x80x82"
I11-1113,A Graph-based Method for Entity Linking,2011,22,30,2,1,12264,yuhang guo,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we formalize the task of finding a knowledge base entry that a given named entity mention refers to, namely entity linking, by identifying the most xe2x80x9cimportantxe2x80x9d node among the graph nodes representing the candidate entries. With the aim of ranking these entities by their xe2x80x9cimportancexe2x80x9d, we introduce three degree-based measures of graph connectivity. Experimental results on the TACKBP benchmark data sets show that our graph-based method performs comparably with the state-of-the-art methods. We also show that using the name phrase feature outperforms the commonly used bagof-word feature for entity linking."
I11-1171,Improving {C}hinese {POS} Tagging with Dependency Parsing,2011,20,5,2,1,3691,zhenghua li,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Recent research usually models POS tagging as a sequential labeling problem, in which only local context features can be used. Due to the lack of morphological inflections, many tagging ambiguities in Chinese are difficult to handle unless consulting larger contexts. In this paper, we try to improve Chinese POS tagging by using long-distance dependencies produced by a statistical dependency parser. Experimental results show that, despite error propagation, the syntactic features can significantly improve the tagging accuracy from 93.88% to 94.41% (p < 10 xe2x88x925 ). Detailed analysis shows that these features are helpful for ambiguous pairs like {NN,VV} and {DEC,DEG}. 1"
I11-1176,Word Sense Disambiguation Corpora Acquisition via Confirmation Code,2011,11,0,1,1,1017,wanxiang che,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word Sense Disambiguation (WSD) is one of the fundamental natural language processing tasks. However, lack of training corpora is a bottleneck to construct a high accurate all-words WSD system. Annotating a large-scale corpus by experts costs enormous time and financial resources. Human Computation is a novel idea for integrating human resources behind the Web, which has been wasted, to solve practical problems that are difficult for computers. Based on human computation, we design a confirmation code system, which can not only distinguish between human beings and computers (the function of normal confirmation code system), but also annotate WSD corpora. The preliminary experimental result shows that the proposed method can annotate large-scale and high-quality WSD corpora within a short time. To the best of our knowledge, this is the first attempt to use confirmation code in natural language processing for corpora acquisition."
D11-1109,Joint Models for {C}hinese {POS} Tagging and Dependency Parsing,2011,37,57,3,1,3691,zhenghua li,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Part-of-speech (POS) is an indispensable feature in dependency parsing. Current research usually models POS tagging and dependency parsing independently. This may suffer from error propagation problem. Our experiments show that parsing accuracy drops by about 6% when using automatic POS tags instead of gold ones. To solve this issue, this paper proposes a solution by jointly optimizing POS tagging and dependency parsing in a unique model. We design several joint models and their corresponding decoding algorithms to incorporate different feature sets. We further present an effective pruning strategy to reduce the search space of candidate POS tags, leading to significant improvement of parsing speed. Experimental results on Chinese Penn Treebank 5 show that our joint models significantly improve the state-of-the-art parsing accuracy by about 1.5%. Detailed analysis shows that the joint method is able to choose such POS tags that are more helpful and discriminative from parsing viewpoint. This is the fundamental reason of parsing accuracy improvement."
S10-1091,{HIT}-{CIR}: An Unsupervised {WSD} System Based on Domain Most Frequent Sense Estimation,2010,11,1,2,1,12264,yuhang guo,Proceedings of the 5th International Workshop on Semantic Evaluation,0,This paper presents an unsupervised system for all-word domain specific word sense disambiguation task. This system tags target word with the most frequent sense which is estimated using a thesaurus and the word distribution information in the domain. The thesaurus is automatically constructed from bilingual parallel corpus using paraphrase technique. The recall of this system is 43.5% on SemEval-2 task 17 English data set.
N10-1030,Improving Semantic Role Labeling with Word Sense,2010,10,9,1,1,1017,wanxiang che,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Semantic role labeling (SRL) not only needs lexical and syntactic information, but also needs word sense information. However, because of the lack of corpus annotated with both word senses and semantic roles, there is few research on using word sense for SRL. The release of OntoNotes provides an opportunity for us to study how to use word sense for SRL. In this paper, we present some novel word sense features for SRL and find that they can improve the performance significantly."
C10-3004,{LTP}: A {C}hinese Language Technology Platform,2010,6,261,1,1,1017,wanxiang che,Coling 2010: Demonstrations,0,"LTP (Language Technology Platform) is an integrated Chinese processing platform which includes a suite of high performance natural language processing (NLP) modules and relevant corpora. Especially for the syntactic and semantic parsing modules, we achieved good results in some relevant evaluations, such as CoNLL and SemEval. Based on XML internal data representation, users can easily use these modules and corpora by invoking DLL (Dynamic Link Library) or Web service APIs (Application Program Interface), and view the processing results directly by the visualization tool."
C10-1019,Jointly Modeling {WSD} and {SRL} with {M}arkov {L}ogic,2010,22,16,1,1,1017,wanxiang che,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Semantic role labeling (SRL) and word sense disambiguation (WSD) are two fundamental tasks in natural language processing to find a sentence-level semantic representation. To date, they have mostly been modeled in isolation. However, this approach neglects logical constraints between them. We therefore exploit some pipeline systems which verify the automatic all word sense disambiguation could help the semantic role labeling and vice versa. We further propose a Markov logic model that jointly labels semantic roles and disambiguates all word senses. By evaluating our model on the OntoNotes 3.0 data, we show that this joint approach leads to a higher performance for word sense disambiguation and semantic role labeling than those pipeline approaches."
W09-1207,Multilingual Dependency-based Syntactic and Semantic Parsing,2009,17,46,1,1,1017,wanxiang che,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"Our CoNLL 2009 Shared Task system includes three cascaded components: syntactic parsing, predicate classification, and semantic role labeling. A pseudo-projective high-order graph-based model is used in our syntactic dependency parser. A support vector machine (SVM) model is used to classify predicate senses. Semantic role labeling is achieved using maximum entropy (MaxEnt) model based semantic role classification and integer linear programming (ILP) based post inference. Finally, we win the first place in the joint task, including both the closed and open challenges."
W08-2134,A Cascaded Syntactic and Semantic Dependency Parsing System,2008,5,29,1,1,1017,wanxiang che,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"We describe our CoNLL 2008 Shared Task system in this paper. The system includes two cascaded components: a syntactic and a semantic dependency parsers. A first-order projective MSTParser is used as our syntactic dependency parser. In order to overcome the shortcoming of the MSTParser, that it cannot model more global information, we add a relabeling stage after the parsing to distinguish some confusable labels, such as ADV, TMP, and LOC. Besides adding a predicate identification and a classification stages, our semantic dependency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument classification and an ILP-based post inference. Finally, we gain the overall labeled macro F1 = 82.66, which ranked the second position in the closed challenge."
I08-2109,Fast Computing Grammar-driven Convolution Tree Kernel for Semantic Role Labeling,2008,11,0,1,1,1017,wanxiang che,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Grammar-driven convolution tree kernel (GTK) has shown promising results for semantic role labeling (SRL). However, the time complexity of computing the GTK is exponential in theory. In order to speed up the computing process, we design two fast grammar-driven convolution tree kernel (FGTK) algorithms, which can compute the GTK in polynomial time. Experimental results on the CoNLL-2005 SRL data show that our two FGTK algorithms are much faster than the GTK."
S07-1034,{HIT}-{IR}-{WSD}: A {WSD} System for {E}nglish Lexical Sample Task,2007,5,7,2,1,12264,yuhang guo,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"HIT-IR-WSD is a word sense disambiguation (WSD) system developed for English lexical sample task (Task 11) of Semeval 2007 by Information Retrieval Lab, Harbin Institute of Technology. The system is based on a supervised method using an SVM classifier. Multi-resources including words in the surrounding context, the part-of-speech of neighboring words, collocations and syntactic relations are used. The final micro-avg raw score achieves 81.9% on the test set, the best one among participating runs."
P07-1026,A Grammar-driven Convolution Tree Kernel for Semantic Role Classification,2007,27,25,2,0.04471,3694,min zhang,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Convolution tree kernel has shown promising results in semantic role classification. However, it only carries out hard matching, which may lead to over-fitting and less accurate similarity measure. To remove the constraint, this paper proposes a grammardriven convolution tree kernel for semantic role classification by introducing more linguistic knowledge into the standard tree kernel. The proposed grammar-driven tree kernel displays two advantages over the previous one: 1) grammar-driven approximate substructure matching and 2) grammardriven approximate tree node matching. The two improvements enable the grammardriven tree kernel explore more linguistically motivated structure features than the previous one. Experiments on the CoNLL-2005 SRL shared task show that the grammardriven tree kernel significantly outperforms the previous non-grammar-driven one in SRL. Moreover, we present a composite kernel to integrate feature-based and tree kernel-based methods. Experimental results show that the composite kernel outperforms the previously best-reported methods."
P06-2010,A Hybrid Convolution Tree Kernel for Semantic Role Labeling,2006,30,18,1,1,1017,wanxiang che,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"A hybrid convolution tree kernel is proposed in this paper to effectively model syntactic structures for semantic role labeling (SRL). The hybrid kernel consists of two individual convolution kernels: a Path kernel, which captures predicate-argument link features, and a Constituent Structure kernel, which captures the syntactic structure features of arguments. Evaluation on the datasets of CoNLL-2005 SRL shared task shows that the novel hybrid convolution tree kernel out-performs the previous tree kernels. We also combine our new hybrid tree kernel based method with the standard rich flat feature based method. The experimental results show that the combinational method can get better performance than each of them individually."
W05-0627,Semantic Role Labeling System Using Maximum Entropy Classifier,2005,0,23,2,0.148396,1018,ting liu,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,None
I05-2023,Improved-Edit-Distance Kernel for {C}hinese Relation Extraction,2005,14,16,1,1,1017,wanxiang che,Companion Volume to the Proceedings of Conference including Posters/Demos and tutorial abstracts,0,"In this paper, a novel kernel-based method is presented for the problem of relation extraction between named entities from Chinese texts. The kernel is defined over the original Chinese string representations around particular entities. As a kernel function, the Improved-Edit-Distance (IED) is used to calculate the similarity between two Chinese strings. By employing the Voted Perceptron and Support Vector Machine (SVM) kernel machines with the IED kernel as the classifiers, we tested the method by extracting person-affiliation relation from Chinese texts. By comparing with traditional feature-based learning methods, we conclude that our method needs less manual efforts in feature transformation and achieves a better performance."
W04-1120,A New {C}hinese Natural Language Understanding Architecture Based on Multilayer Search Mechanism,2004,4,1,1,1,1017,wanxiang che,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,None
