2020.acl-main.716,W18-3219,1,0.925294,"Missing"
2020.acl-main.716,C16-1115,0,0.0205769,"label scheme (see Table 2). However, to compare with other work, we also calculate the average of the weighted F1 scores over the labels lang1 and lang2. Table 3 shows a comparison of our results and the previous state of the art. Note that, for Spanish-English and Hindi-English, the gap of improvement is reasonable, considering that similar gaps in the validation experiments are statistically significant. In contrast, in the case of Nepali-English, we cannot determine whether our improvement is marginal or substantial since the authors only provide one decimal in their scores. Nevertheless, Al-Badrashiny and Diab (2016) use a CRF with hand-crafted features (AlBadrashiny and Diab, 2016), while our approach does not require any feature engineering. 5.2 POS Tagging and NER We use LID to adapt the English pre-trained knowledge of ELMo to the code-switching setting, effectively generating CS-ELMo. Once this is achieved, we fine-tune the model on downstream NLP tasks such as POS tagging and NER. In this section, our goal is to validate whether the CS-ELMo model can improve over vanilla ELMo, multilingual BERT, and the previous state of the art for both tasks. More specifically, we use our best architecture (Exp 3."
2020.acl-main.716,D18-1347,0,0.153513,"ed tasks are good candidates for such applications, since they are usually framed as low-resource problems. However, previous research on sequence labeling for code-switching mainly focused on traditional ML techniques because they performed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tas"
2020.acl-main.716,Q17-1010,0,0.22465,"However, previous research on sequence labeling for code-switching mainly focused on traditional ML techniques because they performed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-swi"
2020.acl-main.716,W16-5805,1,0.907556,"inguistics, pages 8033–8044 c July 5 - 10, 2020. 2020 Association for Computational Linguistics CS-ELMo, an extended version of ELMo that contains a position-aware hierarchical attention mechanism over ELMo’s character n-gram representations. These enhanced representations allow the model to see the location where particular n-grams occur within a word (e.g., affixes or lemmas) and to associate such behaviors with one language or another.1 With the help of this mechanism, our models consistently outperform the state of the art on LID for Nepali-English (Solorio et al., 2014), Spanish-English (Molina et al., 2016), and HindiEnglish (Mave et al., 2018). Moreover, we conduct experiments that emphasize the importance of the position-aware hierarchical attention and the different effects that it can have based on the similarities of the code-switched languages. In the second part, we demonstrate the effectiveness of our CS-ELMo models by further fine-tuning them on tasks such as NER and POS tagging. Specifically, we show that the resulting models significantly outperform multilingual BERT and their homologous ELMo models directly trained for NER and POS tagging. Our models establish a new state of the art"
2020.acl-main.716,D14-1162,0,0.0833008,"nal dimensionality expected in the upper layers of ELMo, while it also emphasizes which n-gram order should receive more attention. 3.2 Sequence Tagging We follow Peters et al. (2018) to use ELMo for sequence labeling. They reported state-of-the-art performance on NER by using ELMo followed by a bidirectional LSTM layer and a linear-chain conditional random field (CRF). We use this architecture as a backbone for our model (see Figure 2A), but we add some modifications. The first modification is the concatenation of static English word embeddings to ELMo’s word representation, such as Twitter (Pennington et al., 2014) and fastText (Bojanowski et al., 2017) embeddings similar to Howard and Ruder (2018) and Mave et al. (2018). The idea is to enrich the context of the words by providing domain-specific embeddings and subword level embeddings. The second modification is the concatenation of the enhanced character ngram representation with the input to the CRF layer. This emphasizes even further the extracted morphological patterns, so that they are present during inference time for the task at hand (i.e., not only LID, but also NER and POS tagging). The last modification is the addition of a secondary task on"
2020.acl-main.716,N18-1202,0,0.528004,"nsive for every combination of languages. Nevertheless, code-switching often occurs in language pairs that include English (see examples in Figure 1). These aspects lead us to explore approaches where English pre-trained models can be leveraged and tailored to perform well on code-switching settings. In this paper, we study the CS phenomenon using English as a starting language to adapt our models to multiple code-switched languages, such as Nepali-English, Hindi-English and SpanishEnglish. In the first part, we focus on the task of language identification (LID) at the token level using ELMo (Peters et al., 2018) as our reference for English knowledge. Our hypothesis is that English pre-trained models should be able to recognize whether a word belongs to English or not when such models are fine-tuned with codeswitched text. To accomplish that, we introduce 8033 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8033–8044 c July 5 - 10, 2020. 2020 Association for Computational Linguistics CS-ELMo, an extended version of ELMo that contains a position-aware hierarchical attention mechanism over ELMo’s character n-gram representations. These enhanced representat"
2020.acl-main.716,petrov-etal-2012-universal,0,0.0897625,"Missing"
2020.acl-main.716,P18-2037,0,0.0195093,"formed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-switched text. Even though attention is an effective and successful mechanism in other NLP tasks, the code-switching lit"
2020.acl-main.716,W17-2619,0,0.0149883,"ls trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-switched text. Even though attention is an effective and successful mechanism in other NLP tasks, the code-switching literature barely covers such technique (Sitaram et"
2020.acl-main.716,L18-1560,0,0.0184855,"han deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-switched text. Even though attention is an effective and successful mechanism in other NLP tasks, the code-switching literature barely covers"
2020.acl-main.716,W18-3503,0,0.505083,", 2018). Moreover, we conduct experiments that emphasize the importance of the position-aware hierarchical attention and the different effects that it can have based on the similarities of the code-switched languages. In the second part, we demonstrate the effectiveness of our CS-ELMo models by further fine-tuning them on tasks such as NER and POS tagging. Specifically, we show that the resulting models significantly outperform multilingual BERT and their homologous ELMo models directly trained for NER and POS tagging. Our models establish a new state of the art for Hindi-English POS tagging (Singh et al., 2018) and Spanish-English NER (Aguilar et al., 2018). Our contributions can be summarized as follows: 1) we use transfer learning from models trained on a high-resource language (i.e., English) and effectively adapt them to the code-switching setting for multiple language pairs on the task of language identification; 2) we show the effectiveness of transferring a model trained for LID to downstream code-switching NLP tasks, such as NER and POS tagging, by establishing a new state of the art; 3) we provide empirical evidence on the importance of the enhanced character n-gram mechanism, which aligns"
2020.acl-main.716,W14-3907,1,0.913546,"g of the Association for Computational Linguistics, pages 8033–8044 c July 5 - 10, 2020. 2020 Association for Computational Linguistics CS-ELMo, an extended version of ELMo that contains a position-aware hierarchical attention mechanism over ELMo’s character n-gram representations. These enhanced representations allow the model to see the location where particular n-grams occur within a word (e.g., affixes or lemmas) and to associate such behaviors with one language or another.1 With the help of this mechanism, our models consistently outperform the state of the art on LID for Nepali-English (Solorio et al., 2014), Spanish-English (Molina et al., 2016), and HindiEnglish (Mave et al., 2018). Moreover, we conduct experiments that emphasize the importance of the position-aware hierarchical attention and the different effects that it can have based on the similarities of the code-switched languages. In the second part, we demonstrate the effectiveness of our CS-ELMo models by further fine-tuning them on tasks such as NER and POS tagging. Specifically, we show that the resulting models significantly outperform multilingual BERT and their homologous ELMo models directly trained for NER and POS tagging. Our m"
2020.acl-main.716,W18-3201,0,0.0803393,"lin et al., 2019). CS-related tasks are good candidates for such applications, since they are usually framed as low-resource problems. However, previous research on sequence labeling for code-switching mainly focused on traditional ML techniques because they performed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has"
2020.acl-main.716,W18-3220,0,0.124623,"n Section 4. 2 http://github.com/RiTUAL-UH/cs_elmo 2018; Devlin et al., 2019). CS-related tasks are good candidates for such applications, since they are usually framed as low-resource problems. However, previous research on sequence labeling for code-switching mainly focused on traditional ML techniques because they performed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (P"
2020.acl-main.716,W18-3221,0,0.0207942,"pable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-switched text. Even though attention is an effective and successful mechanism in other NLP tasks, the code-switching literature barely covers such technique (Sitaram et al., 2019). Wang et al. (2018) use a different attention method for NER, which is based on a gated cell that learns to choose appropriate monolingual embeddings according to the input text. Recently, Winata et al. (2019) proposed multilingual meta embeddings (MME) combined with self-attention (Vaswani et al., 2017). Their method establishes a state of the art on Spanish-English NER by heavily relying on monolingual embeddings for every language in the code-switched text. Our model outperforms theirs by only fine-tuning a generic CS-aware model, without relying on task-specific designs. Another contribution of our work are"
2020.acl-main.716,W19-4320,0,0.0135427,"ttings. Our work is based on ELMo (Peters et al., 2018), a large pre-trained language model that has not been applied to CS tasks before. We also use attention (Bahdanau et al., 2015) within ELMo’s convolutions to adapt it to code-switched text. Even though attention is an effective and successful mechanism in other NLP tasks, the code-switching literature barely covers such technique (Sitaram et al., 2019). Wang et al. (2018) use a different attention method for NER, which is based on a gated cell that learns to choose appropriate monolingual embeddings according to the input text. Recently, Winata et al. (2019) proposed multilingual meta embeddings (MME) combined with self-attention (Vaswani et al., 2017). Their method establishes a state of the art on Spanish-English NER by heavily relying on monolingual embeddings for every language in the code-switched text. Our model outperforms theirs by only fine-tuning a generic CS-aware model, without relying on task-specific designs. Another contribution of our work are position embeddings, which have not been considered for code-switching either. These embeddings, combined with CNNs, have proved useful in computer vision (Gehring et al., 2017); they help t"
2020.acl-main.716,W18-3214,0,0.0241483,"github.com/RiTUAL-UH/cs_elmo 2018; Devlin et al., 2019). CS-related tasks are good candidates for such applications, since they are usually framed as low-resource problems. However, previous research on sequence labeling for code-switching mainly focused on traditional ML techniques because they performed better than deep learning models trained from scratch on limited data (Yirmibes¸o˘glu and Eryi˘git, 2018; AlBadrashiny and Diab, 2016). Nonetheless, some researchers have recently shown promising results by using pre-trained monolingual embeddings for tasks such as NER (Trivedi et al., 2018; Winata et al., 2018) and POS tagging (Soto and Hirschberg, 2018; Ball and Garrette, 2018). Other efforts include the use of multilingual sub-word embeddings like fastText (Bojanowski et al., 2017) for LID (Mave et al., 2018), and cross-lingual sentence embeddings for text classification like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which is capable of handling code-switched sentences. These results show the potential of pre-trained knowledge and they motivate our efforts to further explore transfer learning in code-switching settings. Our work is based on ELMo (Peters et al., 2018), a"
2020.acl-main.716,W18-6115,0,0.043447,"Missing"
2020.acl-main.762,P18-1017,0,0.0248552,"Missing"
2020.acl-main.762,L18-1027,0,0.0593803,"Missing"
2020.acl-main.762,C18-1244,1,0.843876,"s. Then the output is fed to two dense layers yielding the class predictions. We implement our model based on the Hugging Face’s BERT implementation (Wolf et al., 2019). 5.2 Evaluation Settings We evaluate the performance by using two different evaluation metrics for this new task. Font Recall (FR) Less popular fonts could be underrepresented by the models. Therefore we need an evaluation metric that measures the performance of models in learning individual labels. Since we are dealing with an unbalanced dataset, motivated by evaluation methodology used in previous recommendation systems like Kar et al. (2018); Carneiro et al. (2007), we compute Font Recall, i.e. the average recall per font, to measure the performance of the models in learning individual labels. P|F | FR := Emoji Model In this model, we use the DeepMoji pre-trained model (Felbo et al., 2017) to generate emoji vectors by encoding the text into 2304dimensional feature vectors. We treat these features as embedding and pass them to the model with two dense layers. Deepmoji6 is a sentence-level model containing rich representations of emotional content which is trained on a 1,246 million tweet corpus in the emoji prediction task. 5 Expe"
2020.acl-main.762,L18-1010,0,0.0366152,"Missing"
2020.acl-main.762,E17-1083,0,0.0660149,"Missing"
2020.acl-main.762,D14-1162,0,0.0821799,"Missing"
2020.acl-main.762,P19-1112,1,0.773681,"sess annotators’ reliability (Yang et al., 2018; Srinivasan and Chander, 2019; Rodrigues et al., 2014). All of these methods rely on the assumption that only one answer is correct and should be considered as ground truth (Nguyen et al., 2016). Whereas in tasks like ours, sentiment analysis (Brew et al., 2010) or facial expression (Barsoum et al., 2016), the answer is likely to be more subjective due to its non-deterministic nature (Urkullu et al., 2019). We follow previous studies that successfully employed label distribution learning to handle ambiguity in the annotations (Geng et al., 2013; Shirani et al., 2019; Yang et al., 2015). 3 Font Dataset The proposed dataset includes 1,309 short text instances from Adobe Spark3 . The dataset is a collection of publicly available sample texts created by different designers. It covers a variety of topics found in posters, flyers, motivational quotes and advertisements.4 3 https://spark.adobe.com. The dataset along with the annotations can be found online: https://github.com/RiTUAL-UH/ Font-prediction-dataset 4 Choice of Fonts A vast number of fonts and typefaces are used in contemporary printed literature. To narrow down the task, we had a font expert select"
2020.acl-main.762,D17-1169,0,\N,Missing
2020.alw-1.10,W19-3515,0,0.0277437,"Missing"
2020.alw-1.10,2020.trac-1.24,0,0.0133742,"etection, which is not created based on a specific list of profane words. • We develop approaches for incorporating emotions into textual information to improve abusive language detection, and create unified 2 http://enough.org/stats_cyberbullying https://curiouscat.me 79 Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 79–88 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 deep neural models that show promising results across several relevant corpora from various domains. abusive language and hate-speech detection (Koufakou and Scott, 2020; Wiegand et al., 2018; Martins et al., 2018; Corazza et al., 2018; Alorainy et al., 2018; Gao and Huang, 2017). There is also one study that shows jointly modeling of emotion classification and abuse detection, through a multitask approach, can improve the performance of the latter task (Rajamanickam et al., 2020). Our methodology has two key differences in contrast to other existing methods: (1) Instead of using an ensemble approach, we create unified deep neural architectures that show very promising results across multiple domains, and (2) We do not use any user-level information in our mo"
2020.alw-1.10,N19-1423,0,0.00625927,"o integrate emotion information into the textual representation. For Kaggle, Bodapati’19 reports best results. However, the performance of that model compared to our best model, BERT Baseline + DeepMoji, is not significantly better under the Mcnemar test. Although none of our main models (BiLSTM + EA + DeepMoji and BiLSTM + GEA) is the winner for Kaggle, still, the best performing model across our proposed approaches and baselines (i.e., BERT Baseline + DeepMoji) has DeepMoji as part of its 2. BERTbase (uncased) contextualized embeddings trained on the BookCorpus and English Wikipedia corpus (Devlin et al., 2019).10 Based on the results shown in Table 3, it seems that BERT performs better than Glove embeddings across all datasets, despite the fact that we do not fine-tune its weights. Therefore, we use BERT as the embeddings in the rest of the experiments. Curious Cat ask.fm Kaggle Wikipedia BiLSTM + GEA F1 W F1 72.22* 90.9* 60.70* 85.2* 74.98 86.7 77.15 95.5 Table 4: Comparison between RA and GEA attention models. The starred results show significant improvement compared to the opposite model. 1. 200-dimensional Glove9 embeddings trained on Twitter Glove F1 W F1 60.16 87.1 51.69 83.9 69.73 85.1 75.60"
2020.alw-1.10,N19-1221,0,0.126789,"our model. Therefore, the model can be applied to various online platforms, even those that offer anonymity. • We introduce Gated Emotion-Aware Attention (GEA) that dynamically learns the contribution of emotion and textual information to weigh the words inside a sequence. We show that this new attention mechanism significantly outperforms the regular attention, which only utilizes textual hidden representations to learn the word weights when the input text is short and noisy. 2 Related Work Abusive language identification and hate speech detection have been addressed by many research papers (Mishra et al., 2019c; Schmidt and Wiegand, 2017). Most of the related works have employed feature engineering approaches, and use a combination of different types of lexical, syntactic, semantic, sentiment and lexicon-based features along with classic machine learning algorithms such as Support Vector Machines (SVM), and Logistic Regression (Samghabadi et al., 2018; Davidson et al., 2017; Nobata et al., 2016; Gitari et al., 2015; Van Hee et al., 2015). Due to the popularity of deep neural networks, multiple studies have recently been conducted in order to explore the performance of these models on the task of ag"
2020.alw-1.10,D17-1169,0,0.0328626,": One popular way to incorporate information into deep neural models is concatenation. In this approach, we pass the output of BiLSTM to an attention layer, same as Bahdanau et al. (2015), to aggregate the output hidden states of BiLSTM into a single vector. Within this layer, P we calculate the weighted sum of r = i αi hi , → − ← − where hi = [ hi ; hi ] is the concatenation of the forward and backward hidden states of BiLSTM. αi stands for the relative importance of words which is measured as follows: ter offensive language recognition. For capturing emotions from the text, we use DeepMoji (Felbo et al., 2017) pre-trained on Twitter data. As for the output, this model creates a representation for 64 frequently used online emojis that shows how relevant each emoji is to a given text. Figure 2 illustrates the top 5 emojis that DeepMoji assigned to one neutral and one offensive instances in our Curious Cat data. Both of these comments are very short and include the bad word “die”. We can see that DeepMoji correctly recognized the tone of the language in both examples. The colors also show the attention weights assigned by DeepMoji model. The darker colors indicate higher attention weights. Interesting"
2020.alw-1.10,W17-3013,0,0.0693843,"Missing"
2020.alw-1.10,gao-huang-2017-detecting,0,0.0152724,"emotions into textual information to improve abusive language detection, and create unified 2 http://enough.org/stats_cyberbullying https://curiouscat.me 79 Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 79–88 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 deep neural models that show promising results across several relevant corpora from various domains. abusive language and hate-speech detection (Koufakou and Scott, 2020; Wiegand et al., 2018; Martins et al., 2018; Corazza et al., 2018; Alorainy et al., 2018; Gao and Huang, 2017). There is also one study that shows jointly modeling of emotion classification and abuse detection, through a multitask approach, can improve the performance of the latter task (Rajamanickam et al., 2020). Our methodology has two key differences in contrast to other existing methods: (1) Instead of using an ensemble approach, we create unified deep neural architectures that show very promising results across multiple domains, and (2) We do not use any user-level information in our model. Therefore, the model can be applied to various online platforms, even those that offer anonymity. • We int"
2020.alw-1.10,W17-3010,1,0.918595,"lts are the most vulnerable group.1 To combat this problem at scale, automated Natural Language Processing (NLP) systems can help identify potentially abusive language. In recent years, there have been several efforts to automate the detection of offensive language across social media platforms. Lexical features have been proven to work quite well for this task (Dinakar et al., 2012; Davidson et al., 2017). However, such features introduce some bias into the systems by heavily relying on profane words, whereas reports show that most profanities are used in a neutral way in today’s teen talks (Samghabadi et al., 2017; Vidgen et al., 2019). The following examples signify the need for linguistically more sophisticated techniques beyond profanity dependent models to detect abusive language: 1 • We introduce a new corpus for the task of abusive language detection, which is not created based on a specific list of profane words. • We develop approaches for incorporating emotions into textual information to improve abusive language detection, and create unified 2 http://enough.org/stats_cyberbullying https://curiouscat.me 79 Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 79–88 c Online, Nove"
2020.alw-1.10,P19-1163,0,0.117071,"Missing"
2020.alw-1.10,W17-1101,0,0.0553548,"the model can be applied to various online platforms, even those that offer anonymity. • We introduce Gated Emotion-Aware Attention (GEA) that dynamically learns the contribution of emotion and textual information to weigh the words inside a sequence. We show that this new attention mechanism significantly outperforms the regular attention, which only utilizes textual hidden representations to learn the word weights when the input text is short and noisy. 2 Related Work Abusive language identification and hate speech detection have been addressed by many research papers (Mishra et al., 2019c; Schmidt and Wiegand, 2017). Most of the related works have employed feature engineering approaches, and use a combination of different types of lexical, syntactic, semantic, sentiment and lexicon-based features along with classic machine learning algorithms such as Support Vector Machines (SVM), and Logistic Regression (Samghabadi et al., 2018; Davidson et al., 2017; Nobata et al., 2016; Gitari et al., 2015; Van Hee et al., 2015). Due to the popularity of deep neural networks, multiple studies have recently been conducted in order to explore the performance of these models on the task of aggression identification. Most"
2020.alw-1.10,R15-1086,0,0.059025,"Missing"
2020.alw-1.10,W19-3509,0,0.0196736,"le group.1 To combat this problem at scale, automated Natural Language Processing (NLP) systems can help identify potentially abusive language. In recent years, there have been several efforts to automate the detection of offensive language across social media platforms. Lexical features have been proven to work quite well for this task (Dinakar et al., 2012; Davidson et al., 2017). However, such features introduce some bias into the systems by heavily relying on profane words, whereas reports show that most profanities are used in a neutral way in today’s teen talks (Samghabadi et al., 2017; Vidgen et al., 2019). The following examples signify the need for linguistically more sophisticated techniques beyond profanity dependent models to detect abusive language: 1 • We introduce a new corpus for the task of abusive language detection, which is not created based on a specific list of profane words. • We develop approaches for incorporating emotions into textual information to improve abusive language detection, and create unified 2 http://enough.org/stats_cyberbullying https://curiouscat.me 79 Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 79–88 c Online, November 20, 2020. 2020 As"
2020.alw-1.10,N18-1095,0,0.0179501,"ated based on a specific list of profane words. • We develop approaches for incorporating emotions into textual information to improve abusive language detection, and create unified 2 http://enough.org/stats_cyberbullying https://curiouscat.me 79 Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 79–88 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 deep neural models that show promising results across several relevant corpora from various domains. abusive language and hate-speech detection (Koufakou and Scott, 2020; Wiegand et al., 2018; Martins et al., 2018; Corazza et al., 2018; Alorainy et al., 2018; Gao and Huang, 2017). There is also one study that shows jointly modeling of emotion classification and abuse detection, through a multitask approach, can improve the performance of the latter task (Rajamanickam et al., 2020). Our methodology has two key differences in contrast to other existing methods: (1) Instead of using an ensemble approach, we create unified deep neural architectures that show very promising results across multiple domains, and (2) We do not use any user-level information in our model. Therefore, the mo"
2020.emnlp-main.454,Q18-1002,1,0.821936,"ciently capture various important story aspects in long synopses and reviews, we model our task from the perspective of Multiple Instance Learning (MIL). We assume that each synopsis and review is a bag of instances (i.e., sentences in our task), where labels are assigned at the bag level. In such cases, a prediction is made for the bag by either learning to aggregate the instance level predictions (Keeler and Rumelhart, 1992; Dietterich et al., 1997; Maron and Ratan, 1998) or jointly learning the labels for instances and the bag (hua Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015; Angelidis and Lapata, 2018; Xu and Lapata, 2019). In our setting, we choose the latter; i.e., we aggregate P (YP ) for each sentence with the combined representation of XP S and XR to compute P (YP |X). As we will show later, MIL improves prediction performance and promotes interpretability. We represent a synopsis XP S consisting of L sentences (s1 , ..., sL ) in a hierarchical manner instead of a long sequence of words. At first, for a sentence si = (w1 , ..., wT ) having T words, we create a matrix Ei where Eit is the vector representation for word wt in si . We use pre-trained Glove embeddings (Pennington et al., 2"
2020.emnlp-main.454,L18-1274,1,0.811738,"Missing"
2020.emnlp-main.454,C18-1244,1,0.688704,"eal life. This is a great mix, and the artistic style make the film memorable. violence action murder atmospheric revenge mafia family loyalty greed relationship artistic Figure 1: Example snippets from plot synopsis and review of The Godfather and tags that can be generated from these. Introduction A high-level description of stories represented by a tagset can assist consumers of story-based media (e.g., movies, books) during the selection process. Although collecting tags from users is timeconsuming and often suffers from coverage issues (Katakis et al., 2008), NLP techniques like those in Kar et al. (2018b) and Gorinski and Lapata (2018) can be employed to generate tags automatically from written narratives such as synopses. However, existing supervised approaches suffer from two significant weaknesses. Firstly, the accuracy of the extracted tags is subject to the quality of the synopses. Secondly, the tagset is predefined by what was present in the training and development sets and thus is brittle; story attributes are unbounded in principle and grow with the underlying vocabulary. To address the weaknesses presented above, we propose to exploit user reviews. We have found that movie reviews"
2020.emnlp-main.454,P97-1005,0,0.424897,"pproached as supervised learning, we push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and"
2020.emnlp-main.454,C10-1074,0,0.0469024,"subtle distinction between the reviews of typical material products (e.g. phone, TV, furniture) and story-based items (e.g. literature, film, blog). In contrast to the usual aspect based opinions (e.g. battery, resolution, color), reviews of story-based items often contain end users’ feelings, important events of stories, or genre related information, which are abstract in nature (e.g. heart-warming, slasher, melodramatic) and do not have a very specific target aspect. Extraction of such opinions about stories has been approached by previous work using reviews of movies (Zhuang et al., 2006; Li et al., 2010) and books (Lin et al., 2013). Such attempts are broadly divided into two categories. The first category deals with spotting words or phrases (excellent, fantastic, boring) used by people to express how they feel about the story. And the second category focuses on extracting important opinionated sentences from reviews and generating a summary. In our work, while the primary task is to retrieve relevant tags from a pre-defined tagset by supervised learning, our model provides the ability to mine story aspects from reviews without any direct supervision. 3 Dataset Our starting data set is the M"
2020.emnlp-main.454,C18-1167,0,0.0152374,"push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and possible feeling of consumers in a superv"
2020.emnlp-main.454,D14-1162,0,0.0871978,"dis and Lapata, 2018; Xu and Lapata, 2019). In our setting, we choose the latter; i.e., we aggregate P (YP ) for each sentence with the combined representation of XP S and XR to compute P (YP |X). As we will show later, MIL improves prediction performance and promotes interpretability. We represent a synopsis XP S consisting of L sentences (s1 , ..., sL ) in a hierarchical manner instead of a long sequence of words. At first, for a sentence si = (w1 , ..., wT ) having T words, we create a matrix Ei where Eit is the vector representation for word wt in si . We use pre-trained Glove embeddings (Pennington et al., 2014) to initialize E. Then, we encode the sentences using a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with attention (Bahdanau et al., 2015). It helps the model to create a sentence representation shi for the ith sentence in XP S by learning to put a higher weight on the words that correlate more with the target tags. The transformation is as follows: → − −−−−→ h wit = LST M (Eit ), t ∈ [1, T ] ← − ←−−−− h wit = LST M (Eit ), t ∈ [T, 1] → − ← − uit = tanh(Wwt .[ h wit , h wit ] + bw ) exp(rt ) rit = u> αit = P it vt ; t exp(rt ) Learning the Predefined Tagset Different words and senten"
2020.emnlp-main.454,E12-3002,0,0.0288464,"d learning, we push this task to an unsupervised direction to avoid the annotation burden. We verify our proposed method against multiple competitive baselines and conduct a human evaluation to confirm our tags’ effectiveness for a set of movies. 2 Background Prior art related to this paper’s work includes story analysis of movies and mining opinions from movie reviews. In this section, we briefly discuss these lines of work. Story Analysis of Movies Over the years, highlevel story characterization approaches evolved around the problem of identifying genres (Biber, 1992; Kessler et al., 1997; Petrenz, 2012; Worsham and Kalita, 2018). Genre information is helpful but not very expressive most of the time as it is a broad way to categorize items. Recent work (Gorinski and Lapata, 2018; Kar et al., 2018b) retrieves other Instances Tags per instance Reviews per movie S Sentence per document S Words per sentence R Sentence per document R Words per sentence Train 9, 746 3 72 50 21 117 27 Val 2, 437 3 74 53 21 116 27 Test 3, 046 3 72 51 21 116 27 Table 1: Statistics of the dataset. S denotes synopses and R denotes review summaries. attributes of movie storylines like mood, plot type, and possible feeli"
2020.emnlp-main.454,D19-1410,0,0.0123854,"e stories (Kar et al., 2018b). To our knowledge, this method is currently the best-performing system on our task. Pre-trained language models Large pre-trained language models (LM) built with Transformers (Vaswani et al., 2017) have shown impressive performance in a wide range of natural language understanding (NLU) tasks like natural language inference, sentiment analysis, and question-answering in the GLUE benchmark (Wang et al., 2019). However, directly fine-tuning such models for long texts like synopses and reviews is extremely memory expensive. Therefore, we employ Sentence-BERT (SBERT; Reimers and Gurevych, 2019) in our work, which is a state-of-the-art universal sentence encoder built with pre-trained BERT (Devlin et al., 2019). We use SBERT encoded sentence representations with our proposed model in Section 4 instead of training the Bi-LSTM with a word-level attention based sentence encoder. Then we use these representations to create a document representation using Bi-LSTM with sentence-level attention, keeping the rest of the model unchanged. 6 Results Quantitative Results We report the results of our experiments on the test5 set in Table 2. We mainly discuss the top-3 setting, where three tags ar"
2020.lrec-1.166,W18-5118,0,0.0345714,"e system unsuited for prediction before movie production. So, in this paper, we only rely on the script of the movie to do the prediction, and we use a bad word list based system as a baseline to compare with our proposed model. Based on a survey done on hate speech detection (Schmidt and Wiegand, 2017), several works have been conducted on detecting the offensive language in the text. These works are relevant to our research since the offensive language in dialogues can affect the suitability of movies for children. Researchers in (Singh et al., 2018; Zhang et al., 2018; Park and Fung, 2017; Mathur et al., 2018) adapt Convolutional and Recurrent Neural Networks to predict abusive language and hate speech on Twitter data. (Davidson et al., 2017) and (Nobata et al., 2016) also automatically predict hate speech in online content by extracting lexical, syntactic, sentiment, and semantic features and training traditional machine learning classifiers. In one of our baselines, we also make use of lexical and sentiment features and feed them into an SVM classifier. 3. Dataset We expand the movie script dataset collected by Shafaei et al. (2019) to include MPAA ratings. The original corpus provides scripts of"
2020.lrec-1.166,W11-1514,0,0.0352319,"r, fear, surprise) for a G-rated (“College Road Trip”) and an R-rated (“Gernika”) movie. where Wh is the weight matrix, and v and bh are the parameters of the network. 4.3. Emotion Vector The emotion in movie dialogues can help the model to better contextualize conversations among characters, and also better discriminate movies belonging to different rankings. For example, we expect G movies (the most suitable movies for children) to contain less content related to “fear” or “disgust” and instead include more “joy” and “happiness”. To extract emotion from the text, we use NRC emotion lexicon (Mohammad, 2011). This dictionary maps words to eight different emotions (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) and two sentiments (positive and negative) with binary values. We calculate the normalized count of words per emotion over the whole movie. As a result, we have a vector [e1 , e2 , ..., e10 ] for each movie, where ei is the percentage of words corresponding to emotion i. To show the truthfulness of our hypothesis about the emotion, we illustrate the average emotion scores per class (for movies with the same MPAA rating, we average all the scores per emotion). Accordi"
2020.lrec-1.166,D19-1642,0,0.0195403,"tent in movies or predicting abusive language and hate speech in online texts. The closest paper to our work is Martinez et al. (2019). In this study, the authors try to predict if a movie is violent or not using scripts. They extract sentiment, semantic, and lexical features and feed them to an RNN-based classifier to predict violence in movies. Our research is similar to this work because we also use dialogues among movie characters as the input. But, instead of extracting features from 1327 text to feed the model, we mostly use raw data to avoid error propagation due to feature extraction (Ning et al., 2019). Also, the outputs of models are different. In this paper, we predict the MPAA rating, not violence (violence is one of the many aspects of the MPAA rating). The dataset introduced by the aforementioned work is not publicly available, while we make our corpus available4 to enable research in this direction. There are some other works for violence detection in movies as well. Giannakopoulos et al. (2010) work on this topic by extracting visual and audio features from movies. Authors in (Gninkoun and Soleymani, 2011) build upon (Giannakopoulos et al., 2010) and add textual features in order to"
2020.lrec-1.166,W17-3006,0,0.0123103,"deo and audio make the system unsuited for prediction before movie production. So, in this paper, we only rely on the script of the movie to do the prediction, and we use a bad word list based system as a baseline to compare with our proposed model. Based on a survey done on hate speech detection (Schmidt and Wiegand, 2017), several works have been conducted on detecting the offensive language in the text. These works are relevant to our research since the offensive language in dialogues can affect the suitability of movies for children. Researchers in (Singh et al., 2018; Zhang et al., 2018; Park and Fung, 2017; Mathur et al., 2018) adapt Convolutional and Recurrent Neural Networks to predict abusive language and hate speech on Twitter data. (Davidson et al., 2017) and (Nobata et al., 2016) also automatically predict hate speech in online content by extracting lexical, syntactic, sentiment, and semantic features and training traditional machine learning classifiers. In one of our baselines, we also make use of lexical and sentiment features and feed them into an SVM classifier. 3. Dataset We expand the movie script dataset collected by Shafaei et al. (2019) to include MPAA ratings. The original corp"
2020.lrec-1.166,W17-1101,0,0.0115936,"ovies as well. Giannakopoulos et al. (2010) work on this topic by extracting visual and audio features from movies. Authors in (Gninkoun and Soleymani, 2011) build upon (Giannakopoulos et al., 2010) and add textual features in order to capture the ratio of swear words for violence detection. Using video and audio make the system unsuited for prediction before movie production. So, in this paper, we only rely on the script of the movie to do the prediction, and we use a bad word list based system as a baseline to compare with our proposed model. Based on a survey done on hate speech detection (Schmidt and Wiegand, 2017), several works have been conducted on detecting the offensive language in the text. These works are relevant to our research since the offensive language in dialogues can affect the suitability of movies for children. Researchers in (Singh et al., 2018; Zhang et al., 2018; Park and Fung, 2017; Mathur et al., 2018) adapt Convolutional and Recurrent Neural Networks to predict abusive language and hate speech on Twitter data. (Davidson et al., 2017) and (Nobata et al., 2016) also automatically predict hate speech in online content by extracting lexical, syntactic, sentiment, and semantic feature"
2020.lrec-1.166,W18-5106,0,0.0279093,"r words for violence detection. Using video and audio make the system unsuited for prediction before movie production. So, in this paper, we only rely on the script of the movie to do the prediction, and we use a bad word list based system as a baseline to compare with our proposed model. Based on a survey done on hate speech detection (Schmidt and Wiegand, 2017), several works have been conducted on detecting the offensive language in the text. These works are relevant to our research since the offensive language in dialogues can affect the suitability of movies for children. Researchers in (Singh et al., 2018; Zhang et al., 2018; Park and Fung, 2017; Mathur et al., 2018) adapt Convolutional and Recurrent Neural Networks to predict abusive language and hate speech on Twitter data. (Davidson et al., 2017) and (Nobata et al., 2016) also automatically predict hate speech in online content by extracting lexical, syntactic, sentiment, and semantic features and training traditional machine learning classifiers. In one of our baselines, we also make use of lexical and sentiment features and feed them into an SVM classifier. 3. Dataset We expand the movie script dataset collected by Shafaei et al. (2019) t"
2020.lrec-1.223,W18-3219,1,0.924866,"al.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other researchers have provided datasets for dialect recognition (Hamed et al., 2018), humor detection (Khandelwal et al., 2018), sub-word code-switching detecti"
2020.lrec-1.223,C16-1115,0,0.0467415,"Missing"
2020.lrec-1.223,W14-3914,0,0.024577,"evlin et al., 2018). In our analysis, we evaluate the results of the best model and describe the outstanding challenges in this benchmark. Moreover, LinCE is publicly available at ritual.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have focused mostly on core"
2020.lrec-1.223,W18-3204,0,0.145046,"Missing"
2020.lrec-1.223,W18-2501,0,0.158535,"lish (HIN-ENG). One of the most challenging aspects of this language pair is the lack of a standardized transliteration system. Speakers transliterate Hindi employing mostly ad-hoc phonological rules to use the English alphabet when writing. Using the same roman alphabet makes code-switching more convenient but the lack of an official standard for transliteration makes it difficult to process with existing resources exclusively available for Hindi with 1804 Task Corpus Languages All Posts All CMI CS Posts CS CMI Lang1 Lang2 All Tokens LID Molina et al. (2016) Solorio et al. (2014) Mave et al. (2018) Molina et al. (2016) SPA-ENG NEP-ENG HIN-ENG MSA-EA 32,651 13,011 7,421 11,243 8.29 19.85 10.14 2.82 12,380 10,029 3,317 1,326 21.86 25.75 22.68 23.89 129,065 59,037 84,752 140,057 170,793 78,360 29,958 40,759 390,953 188,784 146,722 227,354 POS Singh et al. (2018b) Soto and Hirschberg (2017) HIN-ENG SPA-ENG 1,489 42,911 20.28 24.19 1,077 41,856 28.04 24.81 12,589 178,135 9,882 92,517 33,010 333,069 NER Aguilar et al. (2018) Singh et al. (2018a) Aguilar et al. (2018) SPA-ENG HIN-ENG MSA-EA 67,223 2,079 12,335 5.49 19.99 – 17,466 1,644 – 21.16 25.28 – 163,824 13,860 – 402,923 11,391 – 808,663"
2020.lrec-1.223,W18-3217,0,0.0604443,"Missing"
2020.lrec-1.223,L18-1601,0,0.0540757,"lish (HIN-ENG). One of the most challenging aspects of this language pair is the lack of a standardized transliteration system. Speakers transliterate Hindi employing mostly ad-hoc phonological rules to use the English alphabet when writing. Using the same roman alphabet makes code-switching more convenient but the lack of an official standard for transliteration makes it difficult to process with existing resources exclusively available for Hindi with 1804 Task Corpus Languages All Posts All CMI CS Posts CS CMI Lang1 Lang2 All Tokens LID Molina et al. (2016) Solorio et al. (2014) Mave et al. (2018) Molina et al. (2016) SPA-ENG NEP-ENG HIN-ENG MSA-EA 32,651 13,011 7,421 11,243 8.29 19.85 10.14 2.82 12,380 10,029 3,317 1,326 21.86 25.75 22.68 23.89 129,065 59,037 84,752 140,057 170,793 78,360 29,958 40,759 390,953 188,784 146,722 227,354 POS Singh et al. (2018b) Soto and Hirschberg (2017) HIN-ENG SPA-ENG 1,489 42,911 20.28 24.19 1,077 41,856 28.04 24.81 12,589 178,135 9,882 92,517 33,010 333,069 NER Aguilar et al. (2018) Singh et al. (2018a) Aguilar et al. (2018) SPA-ENG HIN-ENG MSA-EA 67,223 2,079 12,335 5.49 19.99 – 17,466 1,644 – 21.16 25.28 – 163,824 13,860 – 402,923 11,391 – 808,663"
2020.lrec-1.223,R15-1033,0,0.0606436,"Missing"
2020.lrec-1.223,L18-1193,0,0.0307574,"Missing"
2020.lrec-1.223,N19-1201,0,0.0409905,"Missing"
2020.lrec-1.223,W18-3206,1,0.920936,"• Hindi-English (HIN-ENG). One of the most challenging aspects of this language pair is the lack of a standardized transliteration system. Speakers transliterate Hindi employing mostly ad-hoc phonological rules to use the English alphabet when writing. Using the same roman alphabet makes code-switching more convenient but the lack of an official standard for transliteration makes it difficult to process with existing resources exclusively available for Hindi with 1804 Task Corpus Languages All Posts All CMI CS Posts CS CMI Lang1 Lang2 All Tokens LID Molina et al. (2016) Solorio et al. (2014) Mave et al. (2018) Molina et al. (2016) SPA-ENG NEP-ENG HIN-ENG MSA-EA 32,651 13,011 7,421 11,243 8.29 19.85 10.14 2.82 12,380 10,029 3,317 1,326 21.86 25.75 22.68 23.89 129,065 59,037 84,752 140,057 170,793 78,360 29,958 40,759 390,953 188,784 146,722 227,354 POS Singh et al. (2018b) Soto and Hirschberg (2017) HIN-ENG SPA-ENG 1,489 42,911 20.28 24.19 1,077 41,856 28.04 24.81 12,589 178,135 9,882 92,517 33,010 333,069 NER Aguilar et al. (2018) Singh et al. (2018a) Aguilar et al. (2018) SPA-ENG HIN-ENG MSA-EA 67,223 2,079 12,335 5.49 19.99 – 17,466 1,644 – 21.16 25.28 – 163,824 13,860 – 402,923 11,391 – 808,663"
2020.lrec-1.223,W16-5805,1,0.917071,"anding challenges in this benchmark. Moreover, LinCE is publicly available at ritual.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other researchers have provided datasets for dialect recognition (Hamed et al"
2020.lrec-1.223,W18-0201,0,0.0269478,"Missing"
2020.lrec-1.223,2020.semeval-1.100,1,0.844343,"Missing"
2020.lrec-1.223,N18-1202,0,0.272126,"ging datasets, three NER datasets, and one sentiment analysis (SA) dataset, providing a total of ten datasets (see Table 1). Furthermore, an important contribution of LinCE is the new stratification process to provide fair and, in some cases, official splits for the tasks at hand. This required a careful inspection of the original datasets from which we list five major issues (see Section 4.5) and propose new splits for nine out of the ten datasets. In addition to the LinCE benchmark, we also provide strong baselines using popular models such as LSTMs (Hochreiter and Schmidhuber, 1997), ELMo (Peters et al., 2018), and multilingual BERT (Devlin et al., 2018). In our analysis, we evaluate the results of the best model and describe the outstanding challenges in this benchmark. Moreover, LinCE is publicly available at ritual.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identificati"
2020.lrec-1.223,petrov-etal-2012-universal,0,0.0162879,"stituency and dependency parsing. Codeswitched data is not exempted of such analysis. In fact, previous studies have shown that syntax is preserved and compliant with the syntactic rules of the individual languages when code-switching occurs (Solorio and Liu, 2008). In this benchmark, we consider the language pairs HindiEnglish and Spanish-English: • HIN-ENG. Singh et al. (2018b) provides 1,489 tweets (33,010 tokens) annotated with POS tags and three language IDs (hi for Hindi, en for English, and rest for any other token). The POS tags are annotated using the universal POS tagset proposed by Petrov et al. (2012) with the addition of two labels: PART NEG and PRON WH. The corpus does not provide training, development, and test splits due to the small number of samples. However, for the purposes of the benchmark, we propose standard splits using the stratification criteria discussed in Section 4.5. • SPA-ENG. We use the Miami Bangor corpus with the annotations provided by Soto and Hirschberg (2017). The Bangor corpus is composed of bilingual conversations from four speakers with a total of 42,911 utterances and 333,069 tokens. The corpus contains POS tags from the universal POS tagset and LID labels. Th"
2020.lrec-1.223,D18-1344,0,0.149648,"Missing"
2020.lrec-1.223,W16-5806,1,0.916887,"Missing"
2020.lrec-1.223,W15-5936,0,0.0167392,"Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other researchers have provided datasets for dialect recognition (Hamed et al., 2018), humor detection (Khandelwal et al., 2018), sub-word code-switching detection (Mager et al., 2019), among others. Despite the availability and recent growth of datasets, it is still unclear how to compare models across language pairs, domains, and general language processing tasks. In the case of language identification (LID) at th"
2020.lrec-1.223,P18-3008,0,0.399165,"ame roman alphabet makes code-switching more convenient but the lack of an official standard for transliteration makes it difficult to process with existing resources exclusively available for Hindi with 1804 Task Corpus Languages All Posts All CMI CS Posts CS CMI Lang1 Lang2 All Tokens LID Molina et al. (2016) Solorio et al. (2014) Mave et al. (2018) Molina et al. (2016) SPA-ENG NEP-ENG HIN-ENG MSA-EA 32,651 13,011 7,421 11,243 8.29 19.85 10.14 2.82 12,380 10,029 3,317 1,326 21.86 25.75 22.68 23.89 129,065 59,037 84,752 140,057 170,793 78,360 29,958 40,759 390,953 188,784 146,722 227,354 POS Singh et al. (2018b) Soto and Hirschberg (2017) HIN-ENG SPA-ENG 1,489 42,911 20.28 24.19 1,077 41,856 28.04 24.81 12,589 178,135 9,882 92,517 33,010 333,069 NER Aguilar et al. (2018) Singh et al. (2018a) Aguilar et al. (2018) SPA-ENG HIN-ENG MSA-EA 67,223 2,079 12,335 5.49 19.99 – 17,466 1,644 – 21.16 25.28 – 163,824 13,860 – 402,923 11,391 – 808,663 35,374 248,478 SA Patwa et al. (2020) SPA-ENG 18,789 20.70 18,196 21.37 65,968 144,533 286,810 Table 2: The CMI scores and the number of tokens across corpora. All Posts describes the number of posts in the corpora and All CMI is the corresponding CMI scores for su"
2020.lrec-1.223,W18-3503,0,0.441803,"ame roman alphabet makes code-switching more convenient but the lack of an official standard for transliteration makes it difficult to process with existing resources exclusively available for Hindi with 1804 Task Corpus Languages All Posts All CMI CS Posts CS CMI Lang1 Lang2 All Tokens LID Molina et al. (2016) Solorio et al. (2014) Mave et al. (2018) Molina et al. (2016) SPA-ENG NEP-ENG HIN-ENG MSA-EA 32,651 13,011 7,421 11,243 8.29 19.85 10.14 2.82 12,380 10,029 3,317 1,326 21.86 25.75 22.68 23.89 129,065 59,037 84,752 140,057 170,793 78,360 29,958 40,759 390,953 188,784 146,722 227,354 POS Singh et al. (2018b) Soto and Hirschberg (2017) HIN-ENG SPA-ENG 1,489 42,911 20.28 24.19 1,077 41,856 28.04 24.81 12,589 178,135 9,882 92,517 33,010 333,069 NER Aguilar et al. (2018) Singh et al. (2018a) Aguilar et al. (2018) SPA-ENG HIN-ENG MSA-EA 67,223 2,079 12,335 5.49 19.99 – 17,466 1,644 – 21.16 25.28 – 163,824 13,860 – 402,923 11,391 – 808,663 35,374 248,478 SA Patwa et al. (2020) SPA-ENG 18,789 20.70 18,196 21.37 65,968 144,533 286,810 Table 2: The CMI scores and the number of tokens across corpora. All Posts describes the number of posts in the corpora and All CMI is the corresponding CMI scores for su"
2020.lrec-1.223,D08-1110,1,0.779023,"ark. Moreover, LinCE is publicly available at ritual.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have focused mostly on core NLP tasks. Additionally, other researchers have provided datasets for dialect recognition (Hamed et al., 2018), humor detection (Khandelw"
2020.lrec-1.223,W14-3907,1,0.94538,"d multilingual BERT (Devlin et al., 2018). In our analysis, we evaluate the results of the best model and describe the outstanding challenges in this benchmark. Moreover, LinCE is publicly available at ritual.uh.edu/lince, and we anticipate this benchmark to continue to grow and include new tasks and language pairs as they become available. We hope that LinCE motivates future work and accelerates the progress on NLP for code-switched languages. 2. Related Work Linguistic code-switching has been studied in the context of many NLP tasks (Sitaram et al., 2019), including language identification (Solorio et al., 2014; Bali et al., 2014), part1803 of-speech tagging (Soto and Hirschberg, 2018; Soto and Hirschberg, 2017; Molina et al., 2016; Das, 2016; Solorio and Liu, 2008), named entity recognition (Aguilar et al., 2018), parsing (Partanen et al., 2018), sentiment analysis (Vilares et al., 2015), and question answering (Raghavi et al., 2015; Chandu et al., 2018). Many code-switching datasets have been made available through the shared-task series FIRE (Sequiera et al., 2015b; Choudhury et al., 2014; Roy et al., 2013) and CALCS (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), which have fo"
2020.lrec-1.223,W18-3201,0,0.0371017,"Missing"
2020.lrec-1.223,W18-3220,0,0.141068,"Missing"
2020.lrec-1.223,W15-2902,0,0.147392,"Missing"
2020.lrec-1.223,W18-5446,0,0.209918,"nd not easy to compare across each other. A slightly different trend has been marked in named entity recognition (NER). Although the main problem in NER has been the lack of datasets, it is until recently that researchers have provided a few corpora on Hindi-English (Singh et al., 2018b), Spanish-English and Modern Standard ArabicEgyptian Arabic (Aguilar et al., 2018). The participants of the 2018 CALCS competition proposed models based on standard neural NER architectures (e.g., character CNN, followed by a word-based LSTM, and CRF) (Geetha et al., 2018), including variations with attention (Wang et al., 2018b) and multi-task learning (Trivedi et al., 2018). Additionally, most of the participants exploited publicly available resources such as gazetteers as well as monolingual and multilingual embeddings (Winata et al., 2018). While the CALCS competition provided datasets on SpanishEnglish and Modern Standard Arabic-Egyptian Arabic simultaneously, the participants were allowed to provide predictions on one or both competitions. This flexibility left the question open regarding which model was overall the best across language pairs. Sentiment analysis (SA) on code-switched data has not been explored"
2020.lrec-1.223,W18-3221,0,0.279825,"nd not easy to compare across each other. A slightly different trend has been marked in named entity recognition (NER). Although the main problem in NER has been the lack of datasets, it is until recently that researchers have provided a few corpora on Hindi-English (Singh et al., 2018b), Spanish-English and Modern Standard ArabicEgyptian Arabic (Aguilar et al., 2018). The participants of the 2018 CALCS competition proposed models based on standard neural NER architectures (e.g., character CNN, followed by a word-based LSTM, and CRF) (Geetha et al., 2018), including variations with attention (Wang et al., 2018b) and multi-task learning (Trivedi et al., 2018). Additionally, most of the participants exploited publicly available resources such as gazetteers as well as monolingual and multilingual embeddings (Winata et al., 2018). While the CALCS competition provided datasets on SpanishEnglish and Modern Standard Arabic-Egyptian Arabic simultaneously, the participants were allowed to provide predictions on one or both competitions. This flexibility left the question open regarding which model was overall the best across language pairs. Sentiment analysis (SA) on code-switched data has not been explored"
2020.lrec-1.223,W18-3214,0,0.114315,"Missing"
2020.semeval-1.100,2020.semeval-1.163,0,0.0830955,"Missing"
2020.semeval-1.100,W18-3219,1,0.85107,"ma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two datasets - Spanglish an"
2020.semeval-1.100,2020.semeval-1.118,0,0.094663,"Missing"
2020.semeval-1.100,W15-4319,0,0.042123,"Missing"
2020.semeval-1.100,2020.semeval-1.172,0,0.0729464,"Missing"
2020.semeval-1.100,2020.semeval-1.182,0,0.0811279,"Missing"
2020.semeval-1.100,2020.semeval-1.175,0,0.0894491,"Missing"
2020.semeval-1.100,2020.semeval-1.121,0,0.0808882,"Missing"
2020.semeval-1.100,2020.semeval-1.119,0,0.0648513,"Missing"
2020.semeval-1.100,Q17-1010,0,0.0443846,"2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for se"
2020.semeval-1.100,2020.semeval-1.165,0,0.0668742,"Missing"
2020.semeval-1.100,W14-3908,0,0.0238306,"ttempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Co"
2020.semeval-1.100,P19-4007,0,0.0481051,"Missing"
2020.semeval-1.100,W14-5152,1,0.859608,"Missing"
2020.semeval-1.100,N19-1423,0,0.0289635,"y positive. The intention to proceed in this way is to enrich the original corpus annotations with sentiment-level labels. Moreover, the splits do not share the same distribution (i.e., development and test are more skewed than training) because we were annotating data on-demand rather than having available the entire corpus at any stage of the competition. Some annotated examples are provided in Table 2. The average CMI for the train, validation, and test sets are 21.84, 20.52, and 17.23, respectively. 5 Baseline We develop our baseline system using the pre-trained multilingual BERT (M-BERT; Devlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019"
2020.semeval-1.100,2020.semeval-1.164,0,0.0861606,"Missing"
2020.semeval-1.100,W13-1102,0,0.0133803,"are inherently multilingual environments.2 Besides, multilingual communities around the world regularly express their thoughts in social media employing and alternating different languages in the same utterance. This mixing of languages, also known as code-mixing or code-switching,3 is a norm in multilingual societies and is one of the many NLP challenges that social media has facilitated. 1.1 Code-Mixing Challenges In addition to the writing aspects in social media, such as flexible grammar, permissive spelling, arbitrary punctuation, slang, and informal abbreviations (Baldwin et al., 2015; Eisenstein, 2013), code-mixing has introduced a diverse set of linguistic challenges. For instance, multilingual speakers tend to code-mix using a single alphabet regardless of whether the languages involved belong to different writing systems ∗ 1 Equal contribution. https://ritual-uh.github.io/sentimix2020/ This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 2 Statistics show that half of the messages on Twitter are in a language other than English (Schroeder, 2010). 3 We use code-mixing and code-switching intercha"
2020.semeval-1.100,L16-1292,1,0.903347,"Missing"
2020.semeval-1.100,2020.semeval-1.171,0,0.0951278,"Missing"
2020.semeval-1.100,2020.semeval-1.176,0,0.0834599,"Missing"
2020.semeval-1.100,2020.semeval-1.125,0,0.0948821,"Missing"
2020.semeval-1.100,2020.semeval-1.166,0,0.0581426,"Missing"
2020.semeval-1.100,P18-1031,0,0.0665313,"Missing"
2020.semeval-1.100,2020.semeval-1.170,0,0.079557,"Missing"
2020.semeval-1.100,2020.semeval-1.120,0,0.0613383,"Missing"
2020.semeval-1.100,2020.semeval-1.162,0,0.0710808,"Missing"
2020.semeval-1.100,2020.semeval-1.117,0,0.09011,"Missing"
2020.semeval-1.100,2020.semeval-1.103,0,0.0585778,"Missing"
2020.semeval-1.100,2020.semeval-1.126,0,0.0563388,"Missing"
2020.semeval-1.100,2020.semeval-1.177,0,0.0948764,"Missing"
2020.semeval-1.100,S13-2053,0,0.0336157,"found on social media which contains a lot of nonstandard spellings of words and unnecessary capitalization (Das and Gamb¨ack, 2014), making the task more difficult. Naturally, the difficulty will increase as the amount of code-mixing increases. To quantify the level of code-switching between languages in a sentence, Gamb¨ack and Das (2016) introduced a measure called Code Mixing Index (CMI) which considers the number of tokens of each language in a sentence and the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identifi"
2020.semeval-1.100,W16-5805,1,0.702393,"code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language marking. We release two"
2020.semeval-1.100,2020.semeval-1.124,0,0.0681298,"Missing"
2020.semeval-1.100,2020.semeval-1.169,0,0.0479718,"Missing"
2020.semeval-1.100,P19-1493,0,0.0323392,"vlin et al. (2019)). M-BERT was trained on 104 languages’ entire Wikipedia dump and the WordPiece (Wu et al., 2016) vocabulary of this model contains 110K sub-word tokens from these 104 languages. To balance the risk of low-resource languages being under-represented or over-fitted due to small training resources during pretraining, exponentially smoothed weighting was performed on the data during pre-training data creation and vocabulary creation. Although M-BERT was trained on monolingual data from different languages, it is capable of multilingual generalization in code-switching scenarios (Pires et al., 2019). We use the Transformers (Wolf et al., 2019) library to implement our framework and we fine-tune the pre-trained BERT-Base, Multilingual Cased model separately for each of the two languages. Based on our observation on the training split for each dataset, we set the highest sequence length to 40 and 56 tokens for Spanglish and Hinglish, respectively. Then, we fine-tune the model for three epochs using AdamW (Loshchilov and Hutter, 2019) optimizer (η = 2e−5 ). 9 https://requester.mturk.com/ An assignment is done by a single annotator. 11 We use the assignment review policy ScoreMyKnownAnswers/"
2020.semeval-1.100,N16-1159,0,0.0256245,"d the number of tokens where the language switches. Finding the sentiment from code-mixed text has been attempted by some researchers. Mohammad et al. (2013) used SVM-based classifiers to detect sentiment in tweets and text messages using semantic information. Bojanowski et al. (2017) proposed a skip-gram based word representation model that classifies the sentiment of tweets and provides an extensive vocabulary list for language. Giatsoglou et al. (2017) trained lexicon-based document vectors, word embedding, and hybrid systems with the polarity of words to classify the sentiment of a tweet. Sharma et al. (2016) attempted shallow parsing of code-mixed data obtained from online social media, and Chittaranjan et al. (2014) tried word-level identification of code-mixed data to classify the sentiment. Some researchers also tried normalizing the text with lexicon lookup for sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 20"
2020.semeval-1.100,2020.semeval-1.173,0,0.077831,"Missing"
2020.semeval-1.100,D08-1102,1,0.598825,"74 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 774–790 Barcelona, Spain (Online), December 12, 2020. (i.e., language scripts). This behavior is known as transliteration, and code-mixers rely on the phonetic patterns of their writing (i.e., the actual sound) to convey their thoughts in the foreign language (i.e., the language adapted to a new script) (Sitaram et al., 2019). Another common pattern in code-mixing is the alternation of languages at the word level. This behavior often happens by inflecting words from one language with the rules of another language (Solorio and Liu, 2008). For instance, in the second example below, the word pushes is the result of conjugating the English verb push according to Spanish grammar rules for the present tense in third person (in this case, the inflection -es). The Hinglish example shows that phonetic Latin script typing is a popular practice in India, instead of using Devanagari script to write Hindi words. We capture both transliteration and word-level code-mixing inflections in the Hinglish and Spanglish corpora of this competition, respectively. AyeHI aurHI enjoyEN kareHI Eng. Trans.: come and enjoy NoSP meSP pushesEN pleaseEN En"
2020.semeval-1.100,W14-3907,1,0.910074,"sentiment analysis of code-mixed data (Sharma et al., 2015). To advance research in code-mixed language processing, few workshops have also been conducted. Four successful series of Mixed Script Information Retrieval have been organized at the Forum for Information Retrieval Evaluation (FIRE) (SahaRoy et al., 2013; Choudhury et al., 2014; Sequiera et al., 2015; Banerjee et al., 2016). Three workshops on Computational Approaches to Linguistic Code-Switching (CALCS) have been conducted which included shared tasks on language identification and Named Entity Recognition (NER) in code-mixed data (Solorio et al., 2014a; Molina et al., 2016; Aguilar et al., 2018). For our SentiMix Spanglish dataset, we adopt the SentiStrength (Vilares et al., 2015) annotation mechanism and conduct the annotation process over the unified corpus from the three CALCS workshops. 3 Task Description Although code-mixing has received some attention recently, properly annotated data is still scarce. We run a shared task to perform sentiment analysis of code-mixed tweets crawled from social media. Each tweet is classified into one of the three polarity classes - Positive, Negative, Neutral. Each tweet also has word-level language ma"
2020.semeval-1.100,2020.semeval-1.122,0,0.373251,"Missing"
2020.semeval-1.100,2020.semeval-1.168,0,0.0472143,"Missing"
2020.semeval-1.100,2020.semeval-1.167,0,0.0594151,"Missing"
2020.semeval-1.100,2020.semeval-1.181,0,0.0809492,"Missing"
2020.semeval-1.100,W15-2902,0,0.181507,"Missing"
2020.semeval-1.100,2020.semeval-1.174,0,0.044579,"Missing"
2020.semeval-1.100,2020.semeval-1.179,0,0.0543759,"Missing"
2020.semeval-1.100,2020.semeval-1.183,0,0.0848258,"Missing"
2020.semeval-1.184,2020.semeval-1.219,0,0.0964702,"ll four scores. The next system on our leader board is Hitachi, with a score of 0.814. And finally, IITK, by achieving 0.810 RANK, stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, including late-phrase words, sequences of words, and abnormal/poetic sentence structure. 7.3 Top Performing Systems and Novel Architectures In this section, we provide a brief description of the best performing and novel appro"
2020.semeval-1.184,S17-2091,0,0.0211435,"361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and prosodic features that exist in spoken data. For example, (Nakajima et al., 2014) predicted emphasized accent phrases from advertisement text"
2020.semeval-1.184,2020.semeval-1.218,0,0.0611921,"Missing"
2020.semeval-1.184,P19-4007,0,0.0426115,"Missing"
2020.semeval-1.184,2020.semeval-1.222,0,0.0627134,"Missing"
2020.semeval-1.184,2020.semeval-1.215,0,0.0659097,"Missing"
2020.semeval-1.184,2020.semeval-1.223,0,0.154922,", stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, including late-phrase words, sequences of words, and abnormal/poetic sentence structure. 7.3 Top Performing Systems and Novel Architectures In this section, we provide a brief description of the best performing and novel approaches. Table 4 shows a high level summary of these systems. ERNIE achieved the highest score by fine-tuning ERNIE 2.0 as the base mo"
2020.semeval-1.184,2020.semeval-1.190,0,0.0808437,"eads of different models like BERT, DistilBERT, GPT-2, RoBERTa, XLNet, and XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand"
2020.semeval-1.184,2020.semeval-1.214,0,0.0562082,"Missing"
2020.semeval-1.184,2020.semeval-1.224,0,0.0398041,"Missing"
2020.semeval-1.184,2020.semeval-1.216,0,0.140749,"ERT, DistilBERT, GPT-2, RoBERTa, XLNet, and XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand-crafted features Distribution"
2020.semeval-1.184,Y14-1022,0,0.288573,"ainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and prosodic features that exist in spoken data. For example, (Nakajima et al., 2014) predicted emphasized accent phrases from advertisement text information and (Mass et al., 2018) modeled word emphasis on audience-addressed speeches. 3 Data Collection The data used for this shared task is the integration of two datasets from different sources, which are created from scratch based on texts collected from the Adobe Spark and Wisdom Quotes website. The dataset used for this task can be found in the task’s data repository3 . The following are the descriptions of the two datasets. The Spark dataset is collection of 1,195 instances from Adobe Spark4 . It contains a variety of subj"
2020.semeval-1.184,N18-1202,0,0.0210204,"aged value (RANK). To better handle word duplicates, the computation is based on the position of words in a sentence rather than the actual words. Note that there were many cases where two or more tokens have the exact same probability. In this case, if the model predicts either one of the labels, we considered it as a correct answer. Table 5 shows some examples form the dataset, illustrating how the metric is computed. 6 Baseline Model We provided a baseline model for this task. This model (DL-BiLSTM-ELMo) is a sequence-labeling model that essentially utilizes ELMo contextualized embeddings (Peters et al., 2018) as well as two BiLSTM layers to label emphasis. During the training phase, the Kullback-Leibler Divergence (KL-DIV) (Kullback and Leibler, 1951) is used as the loss function. More analysis and the complete description of this model is provided in (Shirani et al., 2019). 7 Systems and Results This task attracted 197 participants and a total of 31 teams made submissions to this task. The teams that submitted papers for the SemEval-2020 proceedings are listed in Table 3. In total, 25 teams performed higher than the baseline and six teams performed lower. 13 of the 31 teams also submitted their s"
2020.semeval-1.184,2020.semeval-1.220,0,0.0498012,"Missing"
2020.semeval-1.184,2020.semeval-1.185,0,0.460143,"oints higher than the second team and 0.013 points higher than the third team. ERNIE, achieved the highest 1364 Figure 4: Pie chart showing the pre-trained models used in this task. score not only in RANK score but across all four scores. The next system on our leader board is Hitachi, with a score of 0.814. And finally, IITK, by achieving 0.810 RANK, stands in third place. 7.2 Best Paper Awards Our shared task awarded several best paper distinctions to complement the top performing systems. Here are the categories of best papers and the winners for each: • Best system description paper: IDS (Shin et al., 2020), this paper, with interesting analysis components, advances our understanding regarding the effectiveness of pre-trained language models for this specific task. • Best result interpretation paper: MIDAS (Anand et al., 2020), the authors go the extra mile to analyze the results in this paper. • Best negative results paper: UIC-NLP (Hossu and Parde, 2020), the authors performed extensive experiments with non-contextualized pre-trained models as well as a variety of hand-crafted features. Through the error analysis, the authors identified a number of common challenging patterns for the model, in"
2020.semeval-1.184,P19-1112,1,0.428485,"importantly, the insights gained from the task. 1.1 Task Definition Given a sequence of tokens C = {x1 , ..., xn }, a real number yi ∈ [0, 1] needs to be assigned for each token in the sequence, indicating the degree to which the token needs to be emphasized. In other words, we define the emphasis score yi as the probability or weight of the ith token in the sequence. Finally, during the evaluation, the final set of emphases are generated by selecting tokens with the highest values (described in Section 5). 2 Related Work We firstly introduced and formulated the task of emphasis selection in (Shirani et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 20"
2020.semeval-1.184,2020.semeval-1.217,0,0.152241,"XLM to probe their ability to identify emphasis without any fine-tuning. Their interesting findings indicate that DistilBERT is more successful in predicting emphasis while XLNet and GPT-2 perform poorly when there is no training for this task. The top non-transformer-based model, Procyon (ranked 12th), successfully proposed an ELMo-based 1366 Table 4: Main features in participating systems. An ‘N/A’ in the Ref. column means that we did not receive a system description paper for that entry. Rank 1 Team Name ERNIE Ref. (Huang et al., 2020) 2 Hitachi (Morio et al., 2020) 3 4 IITK Randomseed19 (Singhal et al., 2020) (Shatilov et al., 2020) 5 Sherry N/A 5 Sattiy N/A 7 FPAI (Guo et al., 2020) 10 BugHunter N/A 11 11 Amobee MIDAS N/A (Anand et al., 2020) 12 Procyon N/A 12 Jupyter N/A 13 CrazyRock N/A 14 CLP N/A 15 16 17 18 18 20 TextLearner Bright LAST AP T¨extmarkers EL-BERT (Yang et al., 2020) N/A (Bestgen, 2020) N/A (Glocker and Markianos Wright, 2020) (Kanani et al., 2020) 22 UIC-NLP (Hossu and Parde, 2020) 23 IDS (Shin et al., 2020) 24 YNU-HPCC (Liao et al., 2020) 1367 Best performing Methods ERNIE 2.0 + data augmentation + hand-crafted features Distribution fusion of BERT, GPT-2, RoBERTa, XLM-RoBERTa,"
2020.semeval-1.184,N03-1033,0,0.0926157,"label frequencies respectively. Words Tag Your Best Friends 4 A1 B O O O A2 O O B I A3 B O B O A4 O O B O A5 B O B O A6 O O B I A7 O O O B A8 O O B O A9 B O B I Freq. [B,I,O] [4,0,5] [0,0,9] [7,0,2] [1,3,5] Norm. Freq. [B,I,O] [0.44,0,0.55] [0,0,1] [0.77,0,0.22] [0.11,0.33,0.55] Emphasis Probs [B+I] [0.44] [0] [0.77] [0.44] Data Analysis Many systems reported performance gain by using Part of Speech Tags (POS) tags in their models. In this section, we analyze the effectiveness of this feature by closely examining the top 20 POS tags in our dataset. We used the Stanford Part-Of-Speech Tagger (Toutanova et al., 2003) to obtain POS tags for all tokens in our dataset. We divide the emphasis probabilities to four intervals (0-0.25, 0.25-0.50, 0.50-0.75 and 0.75-1.00) and compute how the POS tags are distributed in these four intervals. Figure 3: Frequencies of the top 20 POS tags in 0-0.25, 0.25-0.5, 0.5-0.75, 0.75-1.00 intervals of emphasis probabilities. The vertical values correspond to the percentage of tag counts over the total number of words in training set. Figure 3 shows the occurrence of the top 20 POS tags in four emphasis probability intervals for all token labels in our training set. POS tags li"
2020.semeval-1.184,P07-1070,0,0.01227,"d Work We firstly introduced and formulated the task of emphasis selection in (Shirani et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of"
2020.semeval-1.184,2020.semeval-1.221,0,0.0731643,"Missing"
2020.semeval-1.184,D16-1080,0,0.056644,"ni et al., 2019) in which an end-to-end label distribution learning (LDL) model in a sequence tagging architecture is proposed to 1361 model emphasis. We evaluated the model against different baselines on the Spark dataset (introduced in Section 3). Keyword or key-phrase detection may be the closest topic to emphasis selection. Keywords can capture the main topics described in a given document (Turney, 2002). Modeling keywords or key-phrases has been widely addressed in different domains such as news articles (Wan et al., 2007), scientific publications (Nguyen and Kan, 2007) and Twitter data (Zhang et al., 2016; Bellaachia and Al-Dhelaan, 2012). Keyword detection mainly focuses on finding important nouns or noun phrases (Augenstein et al., 2017). In contrast, emphasis could be applied to a subset of words with different roles in a sentence. Generally, word emphasis may use to express emotions, show contrast, capture a reader’s interest or clarify a message. Moreover, emphasis selection in social media posts deals with very short texts and the prediction needs to be made based on a single instance. In the context of expressive prosody generation, emphasis has been addressed based on acoustic and pros"
2020.trac-1.20,2020.trac-1.25,0,0.0627022,"dress this problem. Aggression is a feeling of anger that results in hostile behavior and readiness to attack. According to Kumar et al. (2018c), aggression can either be expressed in a direct, explicit manner (Overtly Aggressive) or an indirect, sarcastic manner (Covertly Aggressive). Hate-speech is used to attack a person or a group of people based on their color, gender, race, sexual orientation, ethnicity, nationality, religion (Nockleby, 2000). Misogyny or Sexism is a subset of hate-speech (Waseem and Hovy, 2016) and targets the victim based on gender or sexuality (Davidson et al., 2017; Bhattacharya et al., 2020). It is essential to identify aggression and hate-speech in social networks to protect online users against such attacks, but it is quite time-consuming to do so manually. Hence, social media companies and government agencies are focusing on building a system that can automate the identification process. However, it is difficult to draw a dis1 These authors contributed equally. https://blog.microfocus.com/how-muchdata-is-created-on-the-internet-each-day/ 2 tinguishing line between acceptable content and aggressive/hateful content due to the subjectivity of the definitions and different percept"
2020.trac-1.20,N19-1423,0,0.0844451,"Missing"
2020.trac-1.20,W17-3013,0,0.0512149,"Missing"
2020.trac-1.20,W17-2902,0,0.0878508,"Missing"
2020.trac-1.20,W18-4401,0,0.158089,"I system. Facebook published its audit report3 on civil rights, which explains its strategy to tackle abusive and hateful content. The report claims that building a complete automation system to detect hate-speech is not possible, and content moderation is unavoidable. This point brings many researchers to focus on building hate-speech/aggression detection systems since a large amount of such data is diffused in social networks. To this end, several workshops have been organized, including ‘Abusive Language Online’ (ALW) (Roberts et al., 2019), ‘Trolling, Aggression and Cyberbullying’ (TRAC) (Kumar et al., 2018b), and Semantic Evaluation (SemEval) shared task on Identifying Offensive Language in Social Media (OffensEval) (Zampieri et al., 2020). This paper presents our system for TRAC-2 Shared Task on “Aggression Identification” (sub-task A) and “Misogynistic Aggression Identification” (sub-task B), in which we propose a BERT (Devlin et al., 2018) based architecture to detect misogyny and aggression using a multi-task approach. The proposed model uses attention mechanism over BERT to get relative importance of words, followed by Fully-Connected layers, and a final classification layer for each sub-t"
2020.trac-1.20,S19-2123,0,0.0353497,"Missing"
2020.trac-1.20,W18-4414,0,0.022903,"Missing"
2020.trac-1.20,D19-1174,0,0.0403019,"Missing"
2020.trac-1.20,W18-4404,0,0.024349,"Missing"
2020.trac-1.20,W18-4418,0,0.392873,"Missing"
2020.trac-1.20,2020.trac-1.1,0,0.330248,"Missing"
2020.trac-1.20,W17-1101,0,0.0492831,"Missing"
2020.trac-1.20,N16-2013,0,0.0958162,"Missing"
2020.trac-1.20,2020.semeval-1.188,0,0.023826,"he report claims that building a complete automation system to detect hate-speech is not possible, and content moderation is unavoidable. This point brings many researchers to focus on building hate-speech/aggression detection systems since a large amount of such data is diffused in social networks. To this end, several workshops have been organized, including ‘Abusive Language Online’ (ALW) (Roberts et al., 2019), ‘Trolling, Aggression and Cyberbullying’ (TRAC) (Kumar et al., 2018b), and Semantic Evaluation (SemEval) shared task on Identifying Offensive Language in Social Media (OffensEval) (Zampieri et al., 2020). This paper presents our system for TRAC-2 Shared Task on “Aggression Identification” (sub-task A) and “Misogynistic Aggression Identification” (sub-task B), in which we propose a BERT (Devlin et al., 2018) based architecture to detect misogyny and aggression using a multi-task approach. The proposed model uses attention mechanism over BERT to get relative importance of words, followed by Fully-Connected layers, and a final classification layer for each sub-task, which predicts the class. 2. Related Work Hate-speech: The interest of NLP researchers in hatespeech, aggression, and sexism detect"
2020.trac-1.23,D17-1169,0,0.0429825,"Missing"
2020.trac-1.23,N18-1110,1,0.86169,"Missing"
2020.trac-1.23,N19-1221,0,0.017177,"by providing timely predictions. The main contributions of this work are listed as follows: • A new methodology for creating a cyberbullying corpus and the first dataset suited for the task of early cyberbullying prediction. • A new strategy to detect cyberbullying events as early as possible and the first evaluation framework that takes both the performance and the earliness of the predictions into account. 2. Related Research Although there are several works on detecting different types of online aggression (Wulczyn et al., 2016; Nobata et al., 2016; Van Hee et al., 2018; Qian et al., 2018; Mishra et al., 2019a; Mishra et al., 2019b), only a few of them address cyberbullying detection. Dinakar et al. (2012) construct a common sense knowledge base - BullySpace - with knowledge about bullying situations and a wide range of common daily topics. Xu et al. (2012) study bullying traces and formulate cyberbullying detection as different Natural Language Processing (NLP) tasks. For instance, they use latent topic modeling to analyze the topics commonly discussed in bullying comments. Some previous works investigate cyberbullying on Instagram and Vine (Hosseinmardi et al., 2014; Hosseinmardi et al., 2015; R"
2020.trac-1.23,N18-2019,0,0.0248501,"idence as possible by providing timely predictions. The main contributions of this work are listed as follows: • A new methodology for creating a cyberbullying corpus and the first dataset suited for the task of early cyberbullying prediction. • A new strategy to detect cyberbullying events as early as possible and the first evaluation framework that takes both the performance and the earliness of the predictions into account. 2. Related Research Although there are several works on detecting different types of online aggression (Wulczyn et al., 2016; Nobata et al., 2016; Van Hee et al., 2018; Qian et al., 2018; Mishra et al., 2019a; Mishra et al., 2019b), only a few of them address cyberbullying detection. Dinakar et al. (2012) construct a common sense knowledge base - BullySpace - with knowledge about bullying situations and a wide range of common daily topics. Xu et al. (2012) study bullying traces and formulate cyberbullying detection as different Natural Language Processing (NLP) tasks. For instance, they use latent topic modeling to analyze the topics commonly discussed in bullying comments. Some previous works investigate cyberbullying on Instagram and Vine (Hosseinmardi et al., 2014; Hossein"
2020.trac-1.23,W17-3010,1,0.914304,"l history of question-answer pairs for 3K users. The question field includes a question/comment posted by the other users, and the answer field consists of the reply to that question/comment provided by the owner of the account. As we mentioned earlier, for finding the cyberbullying incidents, 3 https://www.reddit.com https://ask.fm 5 https://en.wikipedia.org/wiki/Ask.fm#2016OT1 textendashpresent: Purchased by Noosphere and new cryptocurrency plans 4 we may look for the threads of messages that include high ratio of abusive comments. We use our previous system for abuse detection on ask.fm (Samghabadi et al., 2017). We utilize the ask.fm corpus proposed in the same work for training the model and label each row of our data automatically. To make the cyberbullying instances, we create a fixed-length sliding window and move it through the whole history of question-answer pairs per user. For each window sample, we calculate the ratio of offensive questions/comments that the user received inside the window. If it is greater than a pre-defined threshold, we consider the window as a potential cyberbullying event. Additionally, we check whether we can expand the potential negative window by adding more questio"
2020.trac-1.23,N12-1084,0,0.0851274,"Missing"
2021.calcs-1.15,N18-1090,0,0.0172086,"is Romanized. In another study, Pires et al. (2019) tested mBERT’s zeroshot performance on code switched data in two formats: transliterated, where Hindi words are written in the Roman script, and corrected, where Hindi words have been converted back to the Devanagari script by human annotators. Their results show a substantial increase in zero shot performance with scriptcorrected data. Other studies have also shown improvement in perfor mance after normalization and backtransliteration on various tasks like named entity recognition and partofspeech tagging (Ball and Garrette, 2018; Bhat et al., 2018). Thus, there is often a need for computationally inexpensive systems to prepocess data by normalization and/or backtransliteration. We begin by providing background for the normalization and backtransliteration tasks. Then, we describe our system for normalization, graphemetophoneme, and backtransliteration. Finally, we provide results and statistics of our system against human annotated data. Our contributions include: (1) a model to normal ize phonetic typing variations, (2) a simplified backtransliteration technique, (3) a grapheme tophoneme conversion technique for romanized Hind"
2021.calcs-1.15,W19-7809,0,0.0222677,"e resources available for basis. Codeswitched data differs from monolin standard Devanagari text like Wikipedia entries gual data to a great extent which discourages use and monolingual models for Hindi. of existing NLP technologies on codeswitched Recent trends in NLP research on code text. Codeswitching also combines the syntax and switching have explored the performance of large lexicon of the languages used, making it difficult pretrained models on codeswitching tasks. State for monolingual models to adapt to codeswitched oftheart multilingual models are typically trained data (Çetinoğlu and Çöltekin, 2019). on standard script text like Wikipedia and struggle In textual codeswitching, text is frequently at adapting to transliterated noisy codeswitched romanized1 due to various technical constraints. input. Transfer learning has emerged as a promis This is especially true in the case of HindiEnglish ing method to adapt monolingual models trained since the Devanagari script for Hindi is not widely on high resource languages like English to code available or efficient on modern technology. Fig switched data. Large pretrained models like multi ure 1 shows an example of a codeswitched Hindi"
2021.calcs-1.15,2020.lrec-1.223,1,0.823412,"okens. Since the Roman script doesn’t cover all the consonants required for transcribing Hindi, there are multiple ways of transcribing the same phoneme. However, prepocessing by nor malization reduces the variation to a large extent. An example of graphemetophoneme is provided in Table 6. Original Translation IPA let’s go bhaaiHIN abhiHIN kitnaaHIN wait karogeHIN let’s go brother how long will you wait lɛts gəʊ baɪ kɪtnɑ weɪt kəɾoːɡeː Table 6: An example of Grapheme to Phoneme 7 Released Datasets We use our system to backtransliterate the Hindi English corpora from the LinCE6 benchmark (Aguilar et al., 2020). The NER corpus is from Singh et al. (2018a) and has 2,079 tweets while the POS tagging corpus is from Singh et al. (2018b) and has 1,489 tweets. Some statistics about the datasets are presented in Table 7. 8 Conclusion and Future Work Our method can easily be extended to other lan guages that employ variations of the Devanagari 3 https://github.com/anoopkunchukuttan/ crowd-indic-transliteration-data 4 https://github.com/libindic/indic-trans 5 https://github.com/dmort27/epitran 6 https://ritual.uh.edu/lince/home 122 Task NER POS Corpus Singh et al. (2018a) Singh et al. (2018b) Hindi 13,860 1"
2021.calcs-1.15,2020.acl-main.716,1,0.7498,"crying, the film will be fun! Figure 1: An example of a codeswitched Hindi English tweet. English text appears in italics and Hindi text is underlined. during time of purchase or adapt to using the stan dard QWERTY layout for transliterating multiple scripts. Since most users need to use English in their daily life, it is impractical to choose a differ ent keyboard layout. The ease of convenience due to Latin script keyboard layouts and the lack of a standardized transliteration process leads users to employ adhoc phonetic transcription rules when transcribing Hindi in the Roman script (Aguilar and Solorio, 2020). Variations in transliteration 1 Introduction and the informality of social media adds noise Linguistic codeswitching (CS) is the phenomenon which makes HindiEnglish codeswitched data in of mixing two or more languages in the context of creasingly different from standard script text and a single utterance. Multilingual speakers around harder to process. Further, transliteration also pre the world engage in codeswitching on a regular vents from leveraging the resources available for basis. Codeswitched data differs from monolin standard Devanagari text like Wikipedia entries gual data"
2021.calcs-1.15,D18-1347,0,0.0228785,"their codeswitched input is Romanized. In another study, Pires et al. (2019) tested mBERT’s zeroshot performance on code switched data in two formats: transliterated, where Hindi words are written in the Roman script, and corrected, where Hindi words have been converted back to the Devanagari script by human annotators. Their results show a substantial increase in zero shot performance with scriptcorrected data. Other studies have also shown improvement in perfor mance after normalization and backtransliteration on various tasks like named entity recognition and partofspeech tagging (Ball and Garrette, 2018; Bhat et al., 2018). Thus, there is often a need for computationally inexpensive systems to prepocess data by normalization and/or backtransliteration. We begin by providing background for the normalization and backtransliteration tasks. Then, we describe our system for normalization, graphemetophoneme, and backtransliteration. Finally, we provide results and statistics of our system against human annotated data. Our contributions include: (1) a model to normal ize phonetic typing variations, (2) a simplified backtransliteration technique, (3) a grapheme tophoneme conversion techniqu"
2021.calcs-1.15,W05-0909,0,0.240118,"g all possible variations. Instead, we treat normalization as a general machine translation problem. We train a character level sequenceto sequence model for normalization following the ar chitecture of Sutskever et al. (2014). The model is comprised of a Long ShortTerm Memory(LSTM) encoder and LSTM decoder. We use the Keras li brary (Chollet, 2015) for training the model. Ta ble 1 compares our model’s performance with the baselines provided by Makhija et al. (2020). We evaluate our system using Word Error Rate (Nießen et al., 2000), BLEU score (Papineni et al., 2002), and METEOR score (Banerjee and Lavie, 2005). Model (Makhija et al., 2020) Ours WER 15.55 18.5 BLEU 71.21 80.48 METEOR 0.50 0.56 Table 1: Results showing the effectiveness of the nor malization model using the WER, BLEU, and ME TEOR metrics. It is likely that some of the errors are due to inconsistencies in the transcription scheme in the hinglishNorm dataset since it is annotated by hu mans. One such instance is the long vowel आ which is normalized to “aa” through most of the data. However, in some instances, the annotators normalize it to “a”. For example, “bt control to krna pdega” from the training data is normalized to “but cont"
2021.calcs-1.15,N19-1423,0,0.0533563,"Missing"
2021.calcs-1.15,khapra-etal-2014-transliteration,0,0.0647956,"Missing"
2021.calcs-1.15,P07-2045,0,0.0115952,"Missing"
2021.calcs-1.15,N15-3017,0,0.0205888,"repeating the respec tive consonant. For example, इज़ज़त can be transliterated as izzat or izat. • Slang and abbreviations: We define some commonly used slang and abbreviations for both Hindi and English. Some examples in clude: btw &gt; by the way wassup &gt; what’s up? Besides the above, there are other non standard variations observed in transliterated Hindi as well. These variations make it difficult to properly transliterate text using simple phonetic mappings due to the lack of a standard transliteration scheme. Numerous schemes like WX notation (Chaitanya et al., 1996), BrahmiNetITRANS (Kunchukuttan et al., 2015), and others have been introduced. However, none of these have been widely em ployed by the general public. 4 Methodology We follow a two step system to transliterate Ro manized Hindi to the Devanagari orthography. First, we normalize the input using a sequenceto sequence model. Then, for the backtransliteration task, we syllabify the token and transcribe to De vanagari. For the graphemetophoneme task, we directly map the normalized tokens into the inter national phonetic alphabet (IPA). 5 Data Rule based systems are not the most efficient solu tion to normalization since they are no"
2021.calcs-1.15,2020.coling-industry.13,0,0.0383182,"phonetic alphabet (IPA). 5 Data Rule based systems are not the most efficient solu tion to normalization since they are not capable of capturing all possible variations. Instead, we treat normalization as a general machine translation problem. We train a character level sequenceto sequence model for normalization following the ar chitecture of Sutskever et al. (2014). The model is comprised of a Long ShortTerm Memory(LSTM) encoder and LSTM decoder. We use the Keras li brary (Chollet, 2015) for training the model. Ta ble 1 compares our model’s performance with the baselines provided by Makhija et al. (2020). We evaluate our system using Word Error Rate (Nießen et al., 2000), BLEU score (Papineni et al., 2002), and METEOR score (Banerjee and Lavie, 2005). Model (Makhija et al., 2020) Ours WER 15.55 18.5 BLEU 71.21 80.48 METEOR 0.50 0.56 Table 1: Results showing the effectiveness of the nor malization model using the WER, BLEU, and ME TEOR metrics. It is likely that some of the errors are due to inconsistencies in the transcription scheme in the hinglishNorm dataset since it is annotated by hu mans. One such instance is the long vowel आ which is normalized to “aa” through most of the data. Howe"
2021.calcs-1.15,W18-6107,0,0.0172793,"with word variations, spelling mistakes, and grammatical errors. Since the Latin script does not possess all the consonants and vowels required to transliterate Hindi, users come up with the most convenient ways to transcribe Hindi. Common variations in transliterated Hindi are: Related Work Normalization. Research in phonetic typing vari ations when transliterating Hindi has gained in creasing attention recently due to the presence of codeswitched data on social media. Singh et al. (2018c) proposed a normalization model us ing skipgram and clustering techniques for Hindi English data. Mandal and Nanmaran (2018) pre sented the first sequencetosequence model for normalizing BengaliEnglish codeswitched data. Transliteration. Previous work in Hindi transliter ation has fallen in two classes: rule based systems and machine translation based approaches. Multi ple libraries like indictransliteration2 exist for sim ple transliteration tasks using rule based systems; • Ambiguous consonant transliteration: For consonants not covered by the Roman script, users rely on the most appropriate translitera tion available which leads to multiple sounds being transliterated to the same grapheme in the roman"
2021.calcs-1.15,L18-1429,0,0.0287842,"media. Before the advent of neu ral machine translation, statistical machine trans lation tools such as Moses (Koehn et al., 2007) were deployed for transliteration. Neural machine translation based approaches have continued to treat transliteration as a translation problem and ap plied methods such as sequencetosequence learn ing successfully. For instance, Bhat et al. (2018) proposed a three step encoderdecoder model for normalization and transliteration of HindiEnglish codeswitched text. GraphemetoPhoneme. Graphemetophomene (G2P) is an important task for speech recogni tion. Mortensen et al. (2018) presented a multi lingual G2P system for transcribing a multitude of languages using simple mappings. G2P for stan dard Hindi is a straightforward task using simple phonetic mappings. However, for nonstandard transliterated Hindi, it can be tricky to generate ac curate phonemic representations. 3 Background User generated codeswitched data is noisy and rid dled with word variations, spelling mistakes, and grammatical errors. Since the Latin script does not possess all the consonants and vowels required to transliterate Hindi, users come up with the most convenient ways to transcribe Hin"
2021.calcs-1.15,niessen-etal-2000-evaluation,0,0.062899,"efficient solu tion to normalization since they are not capable of capturing all possible variations. Instead, we treat normalization as a general machine translation problem. We train a character level sequenceto sequence model for normalization following the ar chitecture of Sutskever et al. (2014). The model is comprised of a Long ShortTerm Memory(LSTM) encoder and LSTM decoder. We use the Keras li brary (Chollet, 2015) for training the model. Ta ble 1 compares our model’s performance with the baselines provided by Makhija et al. (2020). We evaluate our system using Word Error Rate (Nießen et al., 2000), BLEU score (Papineni et al., 2002), and METEOR score (Banerjee and Lavie, 2005). Model (Makhija et al., 2020) Ours WER 15.55 18.5 BLEU 71.21 80.48 METEOR 0.50 0.56 Table 1: Results showing the effectiveness of the nor malization model using the WER, BLEU, and ME TEOR metrics. It is likely that some of the errors are due to inconsistencies in the transcription scheme in the hinglishNorm dataset since it is annotated by hu mans. One such instance is the long vowel आ which is normalized to “aa” through most of the data. However, in some instances, the annotators normalize it to “a”. For exam"
2021.calcs-1.15,P02-1040,0,0.114775,"ion since they are not capable of capturing all possible variations. Instead, we treat normalization as a general machine translation problem. We train a character level sequenceto sequence model for normalization following the ar chitecture of Sutskever et al. (2014). The model is comprised of a Long ShortTerm Memory(LSTM) encoder and LSTM decoder. We use the Keras li brary (Chollet, 2015) for training the model. Ta ble 1 compares our model’s performance with the baselines provided by Makhija et al. (2020). We evaluate our system using Word Error Rate (Nießen et al., 2000), BLEU score (Papineni et al., 2002), and METEOR score (Banerjee and Lavie, 2005). Model (Makhija et al., 2020) Ours WER 15.55 18.5 BLEU 71.21 80.48 METEOR 0.50 0.56 Table 1: Results showing the effectiveness of the nor malization model using the WER, BLEU, and ME TEOR metrics. It is likely that some of the errors are due to inconsistencies in the transcription scheme in the hinglishNorm dataset since it is annotated by hu mans. One such instance is the long vowel आ which is normalized to “aa” through most of the data. However, in some instances, the annotators normalize it to “a”. For example, “bt control to krna pdega” from"
2021.calcs-1.15,N18-1202,0,0.0292957,"h resource languages like English to code available or efficient on modern technology. Fig switched data. Large pretrained models like multi ure 1 shows an example of a codeswitched Hindi lingual BERT (henceforth, mBERT) (Devlin et al., English tweet. As we can see in the example, key 2019) have shown robust crosslingual zeroshot board layouts force users to choose a single script performance with codeswitching data. Aguilar 1 and Solorio (2020) demonstrated the crosslingual Throughout this paper, we use romanized to mean transliterated to the Roman script transfer ability of ELMo (Peters et al., 2018) , 119 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 119–124 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_015 which was trained on English, to SpanishEnglish, HindiEnglish, and NepaliEnglish codeswitched data. They observe that mBERT is outperformed by their model (CSELMo) for HindiEnglish, pos sibly due to the fact that mBERT is trained on Hindi in Devanagari and their codeswitched input is Romanized. In another study, Pires et al. (2019) tested mBERT’s zeroshot performanc"
2021.calcs-1.15,P19-1493,0,0.0117388,"n script transfer ability of ELMo (Peters et al., 2018) , 119 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 119–124 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_015 which was trained on English, to SpanishEnglish, HindiEnglish, and NepaliEnglish codeswitched data. They observe that mBERT is outperformed by their model (CSELMo) for HindiEnglish, pos sibly due to the fact that mBERT is trained on Hindi in Devanagari and their codeswitched input is Romanized. In another study, Pires et al. (2019) tested mBERT’s zeroshot performance on code switched data in two formats: transliterated, where Hindi words are written in the Roman script, and corrected, where Hindi words have been converted back to the Devanagari script by human annotators. Their results show a substantial increase in zero shot performance with scriptcorrected data. Other studies have also shown improvement in perfor mance after normalization and backtransliteration on various tasks like named entity recognition and partofspeech tagging (Ball and Garrette, 2018; Bhat et al., 2018). Thus, there is often a need for"
2021.calcs-1.15,P18-3008,0,0.2111,"Missing"
2021.emnlp-main.434,N19-1078,0,0.0185744,"itecture to transform the data representation from a high-resource to a low-resource domain by learning the patterns (e.g. style, noise, abbreviations, etc.) in the text that differentiate them and a shared feature space where both domains are aligned. We experiment with diverse datasets and show that transforming the data to the low-resource domain representation achieves significant improvements over only using data from highresource domains. 1 1 Introduction Named entity recognition (NER) has seen significant performance improvements with the recent advances of pre-trained language models (Akbik et al., 2019; Devlin et al., 2019). However, the high performance of such models usually relies on the size and quality of training data. When used under low-resource or even zero-resource scenarios, those models struggle to generalize over diverse domains (Fu et al., 2020), and the performance drops dramatically due to the lack of annotated data. Unfortunately, annotating more data is often expensive and time-consuming, and it requires expert domain knowledge. Moreover, annotated data can quickly become outdated in domains where language changes rapidly (e.g, social media), leadFigure 1: Examples from On"
2021.emnlp-main.434,2020.coling-main.343,0,0.10477,"jhwani and Preotiuc-Pietro, 2020). A common approach to alleviate the limitations mentioned above is data augmentation, where automatically generated data can increase the size and diversity in the training set, while resulting in model performance gains. But data augmentation in the context of NER is still understudied. Approaches that directly modify words in the training set (e.g, synonym replacement (Zhang et al., 2015) and word swap (Wei and Zou, 2019)) can inadvertently result in incorrectly labeled entities after modification. Recent work in NER for low resource scenarios is promising (Dai and Adel, 2020; Ding et al., 2020) but it is limited to same domain settings and improvements decrease drastically with smaller sizes of training data. To facilitate research in this direction, we investigate leveraging data from high-resource domains by projecting it into low-resource domains. Based on our observations, the text in different domains usually presents unique patterns (e.g. style, noise, 1 abbreviations, etc.). As shown in Figure 1, the We release the code at https://github.com/ RiTUAL-UH/style_NER. text in the newswire domain is long and formal, 5346 Proceedings of the 2021 Conference on Emp"
2021.emnlp-main.434,N19-1423,0,0.168869,"m the data representation from a high-resource to a low-resource domain by learning the patterns (e.g. style, noise, abbreviations, etc.) in the text that differentiate them and a shared feature space where both domains are aligned. We experiment with diverse datasets and show that transforming the data to the low-resource domain representation achieves significant improvements over only using data from highresource domains. 1 1 Introduction Named entity recognition (NER) has seen significant performance improvements with the recent advances of pre-trained language models (Akbik et al., 2019; Devlin et al., 2019). However, the high performance of such models usually relies on the size and quality of training data. When used under low-resource or even zero-resource scenarios, those models struggle to generalize over diverse domains (Fu et al., 2020), and the performance drops dramatically due to the lack of annotated data. Unfortunately, annotating more data is often expensive and time-consuming, and it requires expert domain knowledge. Moreover, annotated data can quickly become outdated in domains where language changes rapidly (e.g, social media), leadFigure 1: Examples from Ontonotes 5.0 dataset an"
2021.emnlp-main.434,2020.emnlp-main.488,0,0.564244,"Pietro, 2020). A common approach to alleviate the limitations mentioned above is data augmentation, where automatically generated data can increase the size and diversity in the training set, while resulting in model performance gains. But data augmentation in the context of NER is still understudied. Approaches that directly modify words in the training set (e.g, synonym replacement (Zhang et al., 2015) and word swap (Wei and Zou, 2019)) can inadvertently result in incorrectly labeled entities after modification. Recent work in NER for low resource scenarios is promising (Dai and Adel, 2020; Ding et al., 2020) but it is limited to same domain settings and improvements decrease drastically with smaller sizes of training data. To facilitate research in this direction, we investigate leveraging data from high-resource domains by projecting it into low-resource domains. Based on our observations, the text in different domains usually presents unique patterns (e.g. style, noise, 1 abbreviations, etc.). As shown in Figure 1, the We release the code at https://github.com/ RiTUAL-UH/style_NER. text in the newswire domain is long and formal, 5346 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.434,P17-2090,0,0.0289271,"NER in low-resource settings. Data augmentation aims to increase the size of training data by slightly modifying the copies of already existing data or adding newly generated 3 Proposed Method synthetic data from existing data (Hou et al., 2018; In this work, we propose a novel neural architecture Wei and Zou, 2019). It has become more practical for NLP tasks in recent years, especially in low- to augment the data by transforming the text from a high-resource domain to a lower-resource domain resource scenarios where annotated data is limited for the NER task. The overall neural architecture (Fadaee et al., 2017; Xia et al., 2019). Without is shown in Figure 2. collecting new data, this technique reduces the cost of annotation and boosts the model performance. We consider two unparalleled datasets: one from Previous work has studied the data augmenta- the source domain Dsrc and one from the target tion for both token-level tasks (Sahin ¸ and Steed- domain Dtgt . We first linearize all sentences by man, 2018; Gao et al., 2019) and sequence-level inserting every entity label before the correspond5347 (a) Denoising Reconstruction (b) Detransforming Reconstruction Figure 2: The general architecture of ou"
2021.emnlp-main.434,P19-1555,0,0.0219719,"nsforming the text from a high-resource domain to a lower-resource domain resource scenarios where annotated data is limited for the NER task. The overall neural architecture (Fadaee et al., 2017; Xia et al., 2019). Without is shown in Figure 2. collecting new data, this technique reduces the cost of annotation and boosts the model performance. We consider two unparalleled datasets: one from Previous work has studied the data augmenta- the source domain Dsrc and one from the target tion for both token-level tasks (Sahin ¸ and Steed- domain Dtgt . We first linearize all sentences by man, 2018; Gao et al., 2019) and sequence-level inserting every entity label before the correspond5347 (a) Denoising Reconstruction (b) Detransforming Reconstruction Figure 2: The general architecture of our proposed method. (a) Figure shows the architecture for denoising reconstruction, which aims to reconstruct each input sentence from its noisy version in its corresponding domain. (b) Figure shows the details of reconstructing each input sentence from its transformed version in its corresponding domain. We call this detransforming reconstruction. ing word. At each iteration, we randomly pair tation. Otherwise, the mod"
2021.emnlp-main.434,C18-1105,0,0.0239402,"uation (Liu et al., 2020b). In our work, we focus on cross-domain data augmentation. The proposed method aims to map data from a high-resource domain to a low-resource domain. By learning the textual patterns of the data from different domains, our proposed method transform the data from one domain to another and boosts the model performance with the generated data for NER in low-resource settings. Data augmentation aims to increase the size of training data by slightly modifying the copies of already existing data or adding newly generated 3 Proposed Method synthetic data from existing data (Hou et al., 2018; In this work, we propose a novel neural architecture Wei and Zou, 2019). It has become more practical for NLP tasks in recent years, especially in low- to augment the data by transforming the text from a high-resource domain to a lower-resource domain resource scenarios where annotated data is limited for the NER task. The overall neural architecture (Fadaee et al., 2017; Xia et al., 2019). Without is shown in Figure 2. collecting new data, this technique reduces the cost of annotation and boosts the model performance. We consider two unparalleled datasets: one from Previous work has studied"
2021.emnlp-main.434,P19-1236,0,0.026285,"replacement). Zhang et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel approach using adversarial learning to generate high-quality synthetic data, which is applicable to both supervised and semi-supervised settings. In cross-domain settings, NER models struggle to generalize over diverse genres (Rijhwani and Preotiuc-Pietro, 2020; Fu et al., 2020). Most existing work mainly studies domain adaptation (Liu et al., 2020a; Jia et al., 2019; Wang et al., 2020; Liu et al., 2020b) which aims to adapt a neural model from a source domain to achieve better performance on the data from the target domain. Liu et al. (2020a) proposed a zero-resource cross-domain framework to learn the general representations of named entities. Jia et al. (2019) studied the knowledge of domain difference and presented a novel parameter generation network. Other efforts include the different domain adaptation settings (Wang et al., 2020) and effective cross-domain evaluation (Liu et al., 2020b). In our work, we focus on cross-domain data augmentation. The"
2021.emnlp-main.434,2020.repl4nlp-1.1,0,0.0230656,"tity type with the replacement). Zhang et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel approach using adversarial learning to generate high-quality synthetic data, which is applicable to both supervised and semi-supervised settings. In cross-domain settings, NER models struggle to generalize over diverse genres (Rijhwani and Preotiuc-Pietro, 2020; Fu et al., 2020). Most existing work mainly studies domain adaptation (Liu et al., 2020a; Jia et al., 2019; Wang et al., 2020; Liu et al., 2020b) which aims to adapt a neural model from a source domain to achieve better performance on the data from the target domain. Liu et al. (2020a) proposed a zero-resource cross-domain framework to learn the general representations of named entities. Jia et al. (2019) studied the knowledge of domain difference and presented a novel parameter generation network. Other efforts include the different domain adaptation settings (Wang et al., 2020) and effective cross-domain evaluation (Liu et al., 2020b). In our work, we focus on cross-domain dat"
2021.emnlp-main.434,2020.acl-main.212,0,0.0204307,"arize, we make the following contributions: 1. We propose a novel neural architecture that can learn the textual patterns and effectively transform the text from a high-resource to a low-resource domain. 2. We systematically evaluate our proposed method on two datasets, including six different domains and ten different domain pairs, and show the effectiveness of cross-domain data augmentation for the NER task. 3. We empirically explore our approach in lowresource scenarios and expose the case where our approach could benefit the low-resource NER task 2 Related work tasks (Wang and Yang, 2015; Min et al., 2020). Related to data augmentation on NER, Dai and Adel (2020) conducted a study that primarily focuses on the simple data augmentation methods such as synonym replacement (i.e., replace the token with one of its synonyms) and mention replacement (i.e., randomly replace the mention with another one that has the same entity type with the replacement). Zhang et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel approach using adv"
2021.emnlp-main.434,W13-3516,0,0.00996542,"periments In this section, we will introduce the cross-domain mapping experiment and the NER experiment. In the cross-domain mapping experiment, we analyze the reconstruction and generation capability of the proposed model. We then tested our proposed method and evaluated the data generated from our model on the NER task. Details of the data set, experimental setup, and results are described below. 4.1 Datasets Ontonotes 5.0 Dataset We use subsets from five different domains, including Broadcast Conversation (BC), Broadcast News (BN), Magazine (MZ), Newswire (NW), and Web Data (WB). Following Pradhan et al. (2013), we use the same splits and remove the repeated sequences from each dataset. Temporal Twitter Dataset This dataset was collected from Social Media (SM) domain. It includes tweets from 2014 to 2019, with 2,000 samples from each year. We use the data from 2014 to 2018 as the training set. Following Rijhwani and PreotiucPietro (2020), we use 500 samples from 2019 as the validation set and another 1,500 samples from 2019 as the test set. Cross-domain Mapping In this section, we describe the experimental settings of our proposed cross-domain autoencoder model and report the evaluation results. Cro"
2021.emnlp-main.434,2020.acl-main.680,0,0.351874,"verse domains (Fu et al., 2020), and the performance drops dramatically due to the lack of annotated data. Unfortunately, annotating more data is often expensive and time-consuming, and it requires expert domain knowledge. Moreover, annotated data can quickly become outdated in domains where language changes rapidly (e.g, social media), leadFigure 1: Examples from Ontonotes 5.0 dataset and Temporal Twitter dataset. The language variations and abbreviations in the text from social media domain make it clearly different from the formal text in newswire domain. ing to the temporal drift problem (Rijhwani and Preotiuc-Pietro, 2020). A common approach to alleviate the limitations mentioned above is data augmentation, where automatically generated data can increase the size and diversity in the training set, while resulting in model performance gains. But data augmentation in the context of NER is still understudied. Approaches that directly modify words in the training set (e.g, synonym replacement (Zhang et al., 2015) and word swap (Wei and Zou, 2019)) can inadvertently result in incorrectly labeled entities after modification. Recent work in NER for low resource scenarios is promising (Dai and Adel, 2020; Ding et al.,"
2021.emnlp-main.434,D18-1545,0,0.0515747,"Missing"
2021.emnlp-main.434,E99-1023,0,0.523711,"arn a algorithm. compressed representation of an input based on the 3.1 Data Pre-processing domain it comes from in an unsupervised way. We inject noise into each input sentence by shuffling, Following Ding et al. (2020), we perform sentence dropping, or masking some words. The encoder is linearization so that the model can learn the distritrained to capture the textual semantics and learn bution and the relationship of words and labels. In the pattern that makes each sentence different from this work, we use the standard BIO schema (Tjong sentences in other domains. Then we train the de- Kim Sang and Veenstra, 1999). Given a sequence coder by minimizing a training objective that mea- of words w = {w1 , w2 , ..., wn } and a sequence sures its ability to reconstruct each sentence from of labels l = {l1 , l2 , ..., ln }, we first linearize the its noisy version in its corresponding domain. In words with labels by putting every label li before detransforming reconstruction, the goal is to trans- the corresponding word wi . Then we generate a form sentences from one domain to another domain new sentence x = {l1 , w1 , l2 , w2 , ..., ln , wn } and based on their textual semantics. We first transform drop all O"
2021.emnlp-main.434,2020.acl-main.750,0,0.0334861,"g et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel approach using adversarial learning to generate high-quality synthetic data, which is applicable to both supervised and semi-supervised settings. In cross-domain settings, NER models struggle to generalize over diverse genres (Rijhwani and Preotiuc-Pietro, 2020; Fu et al., 2020). Most existing work mainly studies domain adaptation (Liu et al., 2020a; Jia et al., 2019; Wang et al., 2020; Liu et al., 2020b) which aims to adapt a neural model from a source domain to achieve better performance on the data from the target domain. Liu et al. (2020a) proposed a zero-resource cross-domain framework to learn the general representations of named entities. Jia et al. (2019) studied the knowledge of domain difference and presented a novel parameter generation network. Other efforts include the different domain adaptation settings (Wang et al., 2020) and effective cross-domain evaluation (Liu et al., 2020b). In our work, we focus on cross-domain data augmentation. The proposed method ai"
2021.emnlp-main.434,D15-1306,0,0.030576,"-domain data. To summarize, we make the following contributions: 1. We propose a novel neural architecture that can learn the textual patterns and effectively transform the text from a high-resource to a low-resource domain. 2. We systematically evaluate our proposed method on two datasets, including six different domains and ten different domain pairs, and show the effectiveness of cross-domain data augmentation for the NER task. 3. We empirically explore our approach in lowresource scenarios and expose the case where our approach could benefit the low-resource NER task 2 Related work tasks (Wang and Yang, 2015; Min et al., 2020). Related to data augmentation on NER, Dai and Adel (2020) conducted a study that primarily focuses on the simple data augmentation methods such as synonym replacement (i.e., replace the token with one of its synonyms) and mention replacement (i.e., randomly replace the mention with another one that has the same entity type with the replacement). Zhang et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel"
2021.emnlp-main.434,2020.emnlp-main.691,0,0.0367461,"ross-domain data augmentation for the NER task. 3. We empirically explore our approach in lowresource scenarios and expose the case where our approach could benefit the low-resource NER task 2 Related work tasks (Wang and Yang, 2015; Min et al., 2020). Related to data augmentation on NER, Dai and Adel (2020) conducted a study that primarily focuses on the simple data augmentation methods such as synonym replacement (i.e., replace the token with one of its synonyms) and mention replacement (i.e., randomly replace the mention with another one that has the same entity type with the replacement). Zhang et al. (2020) studied sequence mixup (i.e., mix eligible sequences in the feature space and the label space) to improve the data diversity and enhance sequence labeling for active learning. Ding et al. (2020) presented a novel approach using adversarial learning to generate high-quality synthetic data, which is applicable to both supervised and semi-supervised settings. In cross-domain settings, NER models struggle to generalize over diverse genres (Rijhwani and Preotiuc-Pietro, 2020; Fu et al., 2020). Most existing work mainly studies domain adaptation (Liu et al., 2020a; Jia et al., 2019; Wang et al., 20"
2021.findings-acl.377,W09-1115,0,0.0449813,"understand quickly. Benchmarking The Task Instead of providing baselines for the proposed dataset, we organized a shared task and invited researchers to work on the new corpus. Section 6 describes the top-performing methods. By examining the challenges of the 4314 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4314–4320 August 1–6, 2021. ©2021 Association for Computational Linguistics dataset, we provide different analysis components. 2 Related Work 3 Prior work explored automatically generating presentation slides from documents such as scientific articles (Beamer and Girju, 2009; Wang et al., 2017; Hu and Wan, 2013; Shibata and Kurohashi, 2005; Sravanthi et al., 2009). These projects assume that a slide page is a summarization of some part of the paper, and many summarization methods have been proposed to improve the effectiveness. Other studies provide guidelines or alternatives to traditional designs to communicate a presentation’s content more effectively (Alley and Robertshaw, 2004; Jennings, 2009; Alley et al., 2006; Atkinson, 2005; Doumont, 2005). These create slides with sentence headlines and visual elements to reinforce ideas and increase the audience’s rete"
2021.findings-acl.377,D19-1371,0,0.0937184,", 2020), XLNet(Yang et al., 2019), XLMRoBERTa and BERT (Devlin et al., 2019). Comparing the results of all seven models, XLMRoBERTa performed the best. Besides pre-trained language models, UBRI-604 leveraged lexical features such as capitalized words and punctuation, for further improvement. DeepBlueAI team stood in second place (0.519), a RANK score that was 0.006 lower than the first team’s. DeepBlueAI introduced an ensemble Transformer-based model with two fully-connected layers combined with POS tags embedding and hand-crafted features. The ensemble model takes advantage of BERT, SciBERT (Beltagy et al., 2019) and ERNIE 2.0 pre-trained language models by taking the average of the scores predicted by these models. 7 CAD21 shared task: https://competitions.c odalab.org/competitions/27419 4317 Lastly, Cisco (Ghosh et al.), with a score 0.001 lower than the second team, ranked third. Cisco explored two approaches based on BiLSTM+ELMo (Shirani et al., 2019) architecture and Transformerbased pre-trained models with the base model of RoBERTa and XLNet. They enriched the ELMo contextual embedding in BiLSTM+ELMo model by incorporating a character-level BiLSTM Network. Their results show an increase of 0.026"
2021.findings-acl.377,P19-4007,0,0.0603597,"Missing"
2021.findings-acl.377,N19-1423,0,0.029042,"tures (such as words with capital letters and punctuation) were explored to improve the models’ performance. We describe and compare top-performing approaches next. The top-performing team, UBRI-604 (Hu et al., 2021), by proposing end-to-end Transformer-based approach, ranked in the first place with RANK score of (0.525). Different rich Transformer-based pre-trained language models were explored during the experiment, such as ALBERT (Lample and Conneau, 2019), GPT-2 (Radford and Wu, 2019), RoBERTa (Liu et al., 2019), ERNIE 2.0 (Sun et al., 2020), XLNet(Yang et al., 2019), XLMRoBERTa and BERT (Devlin et al., 2019). Comparing the results of all seven models, XLMRoBERTa performed the best. Besides pre-trained language models, UBRI-604 leveraged lexical features such as capitalized words and punctuation, for further improvement. DeepBlueAI team stood in second place (0.519), a RANK score that was 0.006 lower than the first team’s. DeepBlueAI introduced an ensemble Transformer-based model with two fully-connected layers combined with POS tags embedding and hand-crafted features. The ensemble model takes advantage of BERT, SciBERT (Beltagy et al., 2019) and ERNIE 2.0 pre-trained language models by taking th"
2021.findings-acl.377,2020.semeval-1.190,0,0.0154764,"s Selection for written text in visual media. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challe"
2021.findings-acl.377,2020.semeval-1.216,0,0.017681,"ten text in visual media. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challenges due to differen"
2021.findings-acl.377,N18-1202,0,0.022139,"on the evaluation phase used an ensemble of XLNet and RoBERTa, giving them third place. They boosted the model further in the Post Evaluation phase by ensembling XLNet and BiLSTM+ELMo models and incorporating hand-crafted features like POS and Keyphrase. We used the same baseline model (DLBiLSTM+ELMo) introduced in Shirani et al. (2019) to better show the challenges of PSED dataset. This model achieved RANK score of 0.475 (Table 4) which is 0.275 lower than the reported score by Shirani et al. (0.75).8 With a sequence-labeling architecture, this model utilizes ELMo contextualized embeddings (Peters et al., 2018) and two BiLSTM layers to label emphasis. The Kullback-Leibler Divergence (KL-DIV) (Kullback and Leibler, 1951) is used as the loss function during the training phase. 7 Discussion The PSED dataset contains slides with different lengths. To better examine how the length of slides can affect the prediction, we performed an error analysis to examine this relationship. We divided the test set into three groups based on the instances’ lengths, namely &lt;60, 60–90, and >90 tokens. Then we computed the average Matchm scores over all shared task submissions, four in total, for every example in each gro"
2021.findings-acl.377,I05-1066,0,0.107551,"g baselines for the proposed dataset, we organized a shared task and invited researchers to work on the new corpus. Section 6 describes the top-performing methods. By examining the challenges of the 4314 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4314–4320 August 1–6, 2021. ©2021 Association for Computational Linguistics dataset, we provide different analysis components. 2 Related Work 3 Prior work explored automatically generating presentation slides from documents such as scientific articles (Beamer and Girju, 2009; Wang et al., 2017; Hu and Wan, 2013; Shibata and Kurohashi, 2005; Sravanthi et al., 2009). These projects assume that a slide page is a summarization of some part of the paper, and many summarization methods have been proposed to improve the effectiveness. Other studies provide guidelines or alternatives to traditional designs to communicate a presentation’s content more effectively (Alley and Robertshaw, 2004; Jennings, 2009; Alley et al., 2006; Atkinson, 2005; Doumont, 2005). These create slides with sentence headlines and visual elements to reinforce ideas and increase the audience’s retention of the information during presentation. Many applications pr"
2021.findings-acl.377,P19-1112,1,0.922551,"commendations to enhance the slides’ communication power could improve authoring even more. Our goal is predicting emphasis words in presentation slides. Emphasis uses special formatting like boldface or italics to make words stand out. Well-designed emphasis can significantly increase the viewers’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presen"
2021.findings-acl.377,2020.acl-main.762,1,0.795688,"rs’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presentation slides. They can have different structures, and authors may follow traditional styles, or modern styles with more visual content. Slides cover a wide range of topics, from technical, marketing, and legal presentations to children’s illustrations. The requirement to generalize to differen"
2021.findings-acl.377,2020.semeval-1.184,1,0.745753,"rs’ retention by guiding their focus to a few words (Alley and Robertshaw, 2004). Instead of reading the entire slide, they can read only the emphasized parts, keeping their attention on the speaker and their speech, as Figure 1 illustrates.1 The Emphasis Selection (ES) task was initially introduced by Shirani et al. (2019) with a focus on 1 Source: Web Marketing for Fundraisers: Get Found, Get Traffic, Get Ahead (http://www.fundraising123.o rg/files/web-marketing-for-fundraisers-g et-found-get-traffic-get-ahead652.pdf) short written text in social media, and later became a SemEval 2020 task (Shirani et al., 2020b). In this paper, we focus on presentation slides, introducing a new corpus as well as automated emphasis prediction approaches. We are among the first to use the content of the slides to provide automated design assistance. Task Characteristics Emphasis selection poses new challenges specific to presentation slides. They can have different structures, and authors may follow traditional styles, or modern styles with more visual content. Slides cover a wide range of topics, from technical, marketing, and legal presentations to children’s illustrations. The requirement to generalize to differen"
2021.findings-acl.377,2020.semeval-1.217,0,0.0156287,"edia. The proposed model with an end-to-end sequence tagging architecture utilizes label distribution learning (LDL) (Geng, 2016) to handle the task’s subjectivity, and predicts emphasis scores for short written texts. They trained and evaluated the model against a collection of social media short texts from Adobe Spark2 . Later on in SemEval 2020 (Shirani et al., 2020b), 31 teams proposed novel approaches to model emphasis more effectively. The organizers augmented the social media dataset with a large dataset of short quotations. Top-performing teams (Huang et al., 2020; Morio et al., 2020; Singhal et al., 2020) used rich contextualized pre-trained language models such as ERNIE 2.0 (Sun et al., 2020), XLMRoBERTa (Conneau et al., 2019), XLNet (Yang et al., 2019), and T5 (Raffel et al., 2019). This study focuses on a new domain, presentation slides, where emphasis serves a different purpose than in social media. For social media the main purpose is to draw the audience’s attention, while for presentations, the main purpose is to help the audience better understand the content. Identifying emphasis in presentations brings unique 2 https://spark.adobe.com challenges due to differences in topic, length, a"
2021.findings-emnlp.141,W18-3219,1,0.858329,"Missing"
2021.findings-emnlp.141,2020.lrec-1.223,1,0.840959,"ting from scratch—thus, saving precious computational time and resources. Besides, the subword vocabulary provides enough character-level patterns to learn from already-segmented tokens. We integrate our module with mBERT’s transformer layers even further by continuing to train with the pretraining data and the MLM objective. Once our char2subword is adapted to the pre-trained language model, we evaluate the overall model performance by fine-tuning it on downstream tasks. We show our method’s effectiveness by outperforming mBERT on the social media linguistic code-switching (LinCE) benchmark (Aguilar et al., 2020), where the fine-tuning domain deviates substantially from the pre-training domain. The results show that the char2subword module can also capture intra-word code-switching. At the sentence level, the model can relate words from the same language to support language prediction. We highlight our main contributions as follows: 1. We introduce char2subword, a new parameterefficient and open-vocabulary module that extends the domain-constrained and fixed vocabulary in mBERT (or any pre-trained model relying on subwords) while preserving the semantics of the multilingual embedding space. 2. We show"
2021.findings-emnlp.141,2020.acl-main.716,1,0.744782,"Mikolov et al., 2013) and GloVe (Pen- level LSTMs (Ling et al., 2015a). Most successnington et al., 2014). They showed the capability of ful contextualized word embeddings built out of arranging words in a continuous high-dimensional characters are the language models ELMo (Peters space encoding semantic relationships and mean- et al., 2018) and Flair (Akbik et al., 2018). Building (Goldberg and Levy, 2014). However, rare ing models from characters can easily adapt to sowords are weakly represented in such space, and cial media domains (Akbik et al., 2019), including 1641 code-switching data (Aguilar and Solorio, 2020). Subword models Sennrich et al. (2016) proposed subword tokenization using the byte-pair encoding (BPE) algorithm to balance the use of characters and words. BPE automatically chooses a vocabulary of subwords given the desired vocabulary size. This procedure recursively builds subwords upon characters using the word frequencies (Sennrich et al., 2016). Another greedy variation of BPE can select the longest prefix to segment words (Wu et al., 2016). Alternatively to the greedy versions, the segmentation can happen in a stochastic way; drawing segmentation candidates at different points of a wo"
2021.findings-emnlp.141,N19-1078,0,0.0195638,"ntermediate word representations from characterword2vec (Mikolov et al., 2013) and GloVe (Pen- level LSTMs (Ling et al., 2015a). Most successnington et al., 2014). They showed the capability of ful contextualized word embeddings built out of arranging words in a continuous high-dimensional characters are the language models ELMo (Peters space encoding semantic relationships and mean- et al., 2018) and Flair (Akbik et al., 2018). Building (Goldberg and Levy, 2014). However, rare ing models from characters can easily adapt to sowords are weakly represented in such space, and cial media domains (Akbik et al., 2019), including 1641 code-switching data (Aguilar and Solorio, 2020). Subword models Sennrich et al. (2016) proposed subword tokenization using the byte-pair encoding (BPE) algorithm to balance the use of characters and words. BPE automatically chooses a vocabulary of subwords given the desired vocabulary size. This procedure recursively builds subwords upon characters using the word frequencies (Sennrich et al., 2016). Another greedy variation of BPE can select the longest prefix to segment words (Wu et al., 2016). Alternatively to the greedy versions, the segmentation can happen in a stochastic"
2021.findings-emnlp.141,C18-1139,0,0.0229372,"networks (Srivastava et al., 2015), as well as ground-breaking advances in NLP relied on word character-based LSTM language models that build embedding representations from methods like intermediate word representations from characterword2vec (Mikolov et al., 2013) and GloVe (Pen- level LSTMs (Ling et al., 2015a). Most successnington et al., 2014). They showed the capability of ful contextualized word embeddings built out of arranging words in a continuous high-dimensional characters are the language models ELMo (Peters space encoding semantic relationships and mean- et al., 2018) and Flair (Akbik et al., 2018). Building (Goldberg and Levy, 2014). However, rare ing models from characters can easily adapt to sowords are weakly represented in such space, and cial media domains (Akbik et al., 2019), including 1641 code-switching data (Aguilar and Solorio, 2020). Subword models Sennrich et al. (2016) proposed subword tokenization using the byte-pair encoding (BPE) algorithm to balance the use of characters and words. BPE automatically chooses a vocabulary of subwords given the desired vocabulary size. This procedure recursively builds subwords upon characters using the word frequencies (Sennrich et al.,"
2021.findings-emnlp.141,W15-4319,0,0.0445518,"Missing"
2021.findings-emnlp.141,D16-1047,0,0.0232621,"e character compositionality capabilities of our module by handling noise robustly at the character level while being language-independent and flexible to different tokenization. 3. We analyze the advantages of our model on downstream tasks and demonstrate its practical use and adaptability to other domains despite of vocabulary changes. OOV words are not representable. To alleviate that, researchers proposed word representations using recursive neural networks guided by morphology (Luong et al., 2013), as well as morpheme embeddings as a prior distribution over probabilistic word embeddings (Bhatia et al., 2016). Regardless, the challenges persist in noisy text, where users do not follow the canonical word forms (Eisenstein, 2013b). Such problems are aggravated in social media due to the inherently multilingual environment. More words per language are required, while the spelling noise is persistent across languages. Character representations While characterlevel systems proved strong for text classification (Conneau et al., 2017), they were not as successful on multilingual tasks like neural machine translation (NMT) initially (Neubig et al., 2013; Chung et al., 2016). Even when the performance was"
2021.findings-emnlp.141,2020.findings-emnlp.414,0,0.125139,"haracters, and nanotechnologi do not resemble any of its morphemes. Kudo, 2018; Wang et al., 2019). However, BPE and its variants are sensitive to small perturbations in the text, potentially distorting the sentences’ meaning (Jones et al., 2020) (see Figure 1). Moreover, this tokenization process is rigid to changes such as adding more subwords to the vocabulary or correcting the segmentation splits. That is because the tokenization relies on the original corpus where the vocabulary was generated (e.g., Wikipedia), resulting in a fixed set of subword pieces tied to an embedding lookup table (Bostrom and Durrett, 2020). Although these aspects are not a problem with clean and properly formatted text, that is not the case when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in"
2021.findings-emnlp.141,D18-1461,0,0.0180095,"like neural machine translation (NMT) initially (Neubig et al., 2013; Chung et al., 2016). Even when the performance was satisfactory, such systems had to process long sequences of characters resulting in a very slow process (Costa-jussà and Fonollosa, 2016; Ling et al., 2015b). Additionally, languages have different writing systems and specific properties encoded at the character level. While some of those properties may be captured effectively on morphologically rich languages (e.g., Czech and Arabic), properties from other languages are not more impactful than using words (e.g., English) (Cherry et al., 2018). These challenges are also applicable to our case since we conduct our study on multilingual data with typologically different languages. Hybrid representations Using words or characters has shown advantages and disadvantages on both ends. Researchers tried to get the best of both worlds by combining characters and words in a hybrid architecture (Luong and Manning, 2016) where the default was based on static word embeddings that backed off to characters if the word was unknown. Parallel efforts focused on characteraware neural language models (Kim et al., 2016) 2 Related Work where the meanin"
2021.findings-emnlp.141,P16-1160,0,0.0250393,"probabilistic word embeddings (Bhatia et al., 2016). Regardless, the challenges persist in noisy text, where users do not follow the canonical word forms (Eisenstein, 2013b). Such problems are aggravated in social media due to the inherently multilingual environment. More words per language are required, while the spelling noise is persistent across languages. Character representations While characterlevel systems proved strong for text classification (Conneau et al., 2017), they were not as successful on multilingual tasks like neural machine translation (NMT) initially (Neubig et al., 2013; Chung et al., 2016). Even when the performance was satisfactory, such systems had to process long sequences of characters resulting in a very slow process (Costa-jussà and Fonollosa, 2016; Ling et al., 2015b). Additionally, languages have different writing systems and specific properties encoded at the character level. While some of those properties may be captured effectively on morphologically rich languages (e.g., Czech and Arabic), properties from other languages are not more impactful than using words (e.g., English) (Cherry et al., 2018). These challenges are also applicable to our case since we conduct ou"
2021.findings-emnlp.141,W19-4828,0,0.0163686,"ut a single vector that mimicks the associated word embedding in the dictionary. Schick and Schütze (2019) improved this method by introducing attentive mimicking to account for context, besides the surface form of the word. 3 Method Given a word w, a subword model produces a sequence of subword pieces s “ ps0 , s1 , . . . , sn q, such that the concatenation of all the segments from s fully reconstructs the word w. Regardless of whether a subword piece represents a character in a word or not, all the pieces are treated as semantic units within a sentence.2 Such pieces come 2 Previous studies (Clark et al., 2019; Rogers et al., 2020) showed that BERT learns syntax and parsing within its selffrom a rule-based system that does not take into account semantics or morphology during the tokenization. Thus, the subword tokenization has a significant impact on the semantic abstraction from upper layers in pre-trained models like mBERT. To alleviate such problems, we build word representations out of characters. The char2subword module allows flexible tokenization patterns, where the model can split by spaces, use the original tokenization method, or employ a different tokenization process as defined by the u"
2021.findings-emnlp.141,2020.acl-main.747,0,0.0647513,"Missing"
2021.findings-emnlp.141,E17-1104,0,0.0269889,"tations using recursive neural networks guided by morphology (Luong et al., 2013), as well as morpheme embeddings as a prior distribution over probabilistic word embeddings (Bhatia et al., 2016). Regardless, the challenges persist in noisy text, where users do not follow the canonical word forms (Eisenstein, 2013b). Such problems are aggravated in social media due to the inherently multilingual environment. More words per language are required, while the spelling noise is persistent across languages. Character representations While characterlevel systems proved strong for text classification (Conneau et al., 2017), they were not as successful on multilingual tasks like neural machine translation (NMT) initially (Neubig et al., 2013; Chung et al., 2016). Even when the performance was satisfactory, such systems had to process long sequences of characters resulting in a very slow process (Costa-jussà and Fonollosa, 2016; Ling et al., 2015b). Additionally, languages have different writing systems and specific properties encoded at the character level. While some of those properties may be captured effectively on morphologically rich languages (e.g., Czech and Arabic), properties from other languages are no"
2021.findings-emnlp.141,P16-2058,0,0.0204804,"isenstein, 2013b). Such problems are aggravated in social media due to the inherently multilingual environment. More words per language are required, while the spelling noise is persistent across languages. Character representations While characterlevel systems proved strong for text classification (Conneau et al., 2017), they were not as successful on multilingual tasks like neural machine translation (NMT) initially (Neubig et al., 2013; Chung et al., 2016). Even when the performance was satisfactory, such systems had to process long sequences of characters resulting in a very slow process (Costa-jussà and Fonollosa, 2016; Ling et al., 2015b). Additionally, languages have different writing systems and specific properties encoded at the character level. While some of those properties may be captured effectively on morphologically rich languages (e.g., Czech and Arabic), properties from other languages are not more impactful than using words (e.g., English) (Cherry et al., 2018). These challenges are also applicable to our case since we conduct our study on multilingual data with typologically different languages. Hybrid representations Using words or characters has shown advantages and disadvantages on both end"
2021.findings-emnlp.141,N19-1423,0,0.271465,"se when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in codetransformer-based language models such as BERT switching scenarios (Singh et al., 2018; Aguilar (Devlin et al., 2019), GPT-2 (Radford et al., 2019), et al., 2018; Molina et al., 2016; Das, 2016). RoBERTa (Liu et al., 2019), and CTRL (Keskar This paper proposes a character-to-subword et al., 2019). This method addresses the open vo(char2subword) module trained to handle rare or uncabulary problem by segmenting unseen or rare seen spellings robustly while being less restrictive words into smaller subword units while keeping to a particular tokenization method. Our method a reasonable vocabulary size (Huck et al., 2017; works as a drop-in alternative to the embedding ta* Work performed as summer intern at Sales"
2021.findings-emnlp.141,P18-1128,0,0.0287395,"Missing"
2021.findings-emnlp.141,W13-1102,0,0.0506729,"., Wikipedia), resulting in a fixed set of subword pieces tied to an embedding lookup table (Bostrom and Durrett, 2020). Although these aspects are not a problem with clean and properly formatted text, that is not the case when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in codetransformer-based language models such as BERT switching scenarios (Singh et al., 2018; Aguilar (Devlin et al., 2019), GPT-2 (Radford et al., 2019), et al., 2018; Molina et al., 2016; Das, 2016). RoBERTa (Liu et al., 2019), and CTRL (Keskar This paper proposes a character-to-subword et al., 2019). This method addresses the open vo(char2subword) module trained to handle rare or uncabulary problem by segmenting unseen or rare seen spellings robustly while being less restrictive words into s"
2021.findings-emnlp.141,N13-1037,0,0.056885,"., Wikipedia), resulting in a fixed set of subword pieces tied to an embedding lookup table (Bostrom and Durrett, 2020). Although these aspects are not a problem with clean and properly formatted text, that is not the case when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in codetransformer-based language models such as BERT switching scenarios (Singh et al., 2018; Aguilar (Devlin et al., 2019), GPT-2 (Radford et al., 2019), et al., 2018; Molina et al., 2016; Das, 2016). RoBERTa (Liu et al., 2019), and CTRL (Keskar This paper proposes a character-to-subword et al., 2019). This method addresses the open vo(char2subword) module trained to handle rare or uncabulary problem by segmenting unseen or rare seen spellings robustly while being less restrictive words into s"
2021.findings-emnlp.141,W17-4706,0,0.0368657,"Missing"
2021.findings-emnlp.141,2020.acl-main.245,0,0.0217849,"former parameters fixed– and thus, providing a practical method. Finally, we show that incorporating our module to mBERT significantly improves the performance on the social media linguistic codeswitching evaluation (LinCE) benchmark. Figure 1: Examples of subword tokenization from OOV words. The word helllo changes its meaning (e.g., hell), BUSINESS is split almost to characters, and nanotechnologi do not resemble any of its morphemes. Kudo, 2018; Wang et al., 2019). However, BPE and its variants are sensitive to small perturbations in the text, potentially distorting the sentences’ meaning (Jones et al., 2020) (see Figure 1). Moreover, this tokenization process is rigid to changes such as adding more subwords to the vocabulary or correcting the segmentation splits. That is because the tokenization relies on the original corpus where the vocabulary was generated (e.g., Wikipedia), resulting in a fixed set of subword pieces tied to an embedding lookup table (Bostrom and Durrett, 2020). Although these aspects are not a problem with clean and properly formatted text, that is not the case when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subw"
2021.findings-emnlp.141,P18-1007,0,0.152209,"vel alterations such as misspellings, word inflection, casing, and punctuation. We integrate it further with BERT through pre-training while keeping BERT transformer parameters fixed– and thus, providing a practical method. Finally, we show that incorporating our module to mBERT significantly improves the performance on the social media linguistic codeswitching evaluation (LinCE) benchmark. Figure 1: Examples of subword tokenization from OOV words. The word helllo changes its meaning (e.g., hell), BUSINESS is split almost to characters, and nanotechnologi do not resemble any of its morphemes. Kudo, 2018; Wang et al., 2019). However, BPE and its variants are sensitive to small perturbations in the text, potentially distorting the sentences’ meaning (Jones et al., 2020) (see Figure 1). Moreover, this tokenization process is rigid to changes such as adding more subwords to the vocabulary or correcting the segmentation splits. That is because the tokenization relies on the original corpus where the vocabulary was generated (e.g., Wikipedia), resulting in a fixed set of subword pieces tied to an embedding lookup table (Bostrom and Durrett, 2020). Although these aspects are not a problem with clea"
2021.findings-emnlp.141,D15-1176,0,0.0966518,"Missing"
2021.findings-emnlp.141,2020.acl-main.740,0,0.0474958,"Missing"
2021.findings-emnlp.141,2021.ccl-1.108,0,0.0669531,"Missing"
2021.findings-emnlp.141,P16-1100,0,0.0545357,"Missing"
2021.findings-emnlp.141,W13-3512,0,0.058737,"ned model relying on subwords) while preserving the semantics of the multilingual embedding space. 2. We show the character compositionality capabilities of our module by handling noise robustly at the character level while being language-independent and flexible to different tokenization. 3. We analyze the advantages of our model on downstream tasks and demonstrate its practical use and adaptability to other domains despite of vocabulary changes. OOV words are not representable. To alleviate that, researchers proposed word representations using recursive neural networks guided by morphology (Luong et al., 2013), as well as morpheme embeddings as a prior distribution over probabilistic word embeddings (Bhatia et al., 2016). Regardless, the challenges persist in noisy text, where users do not follow the canonical word forms (Eisenstein, 2013b). Such problems are aggravated in social media due to the inherently multilingual environment. More words per language are required, while the spelling noise is persistent across languages. Character representations While characterlevel systems proved strong for text classification (Conneau et al., 2017), they were not as successful on multilingual tasks like neu"
2021.findings-emnlp.141,W16-5805,1,0.659406,"ocial media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in codetransformer-based language models such as BERT switching scenarios (Singh et al., 2018; Aguilar (Devlin et al., 2019), GPT-2 (Radford et al., 2019), et al., 2018; Molina et al., 2016; Das, 2016). RoBERTa (Liu et al., 2019), and CTRL (Keskar This paper proposes a character-to-subword et al., 2019). This method addresses the open vo(char2subword) module trained to handle rare or uncabulary problem by segmenting unseen or rare seen spellings robustly while being less restrictive words into smaller subword units while keeping to a particular tokenization method. Our method a reasonable vocabulary size (Huck et al., 2017; works as a drop-in alternative to the embedding ta* Work performed as summer intern at Salesforce. ble in pre-trained language models like mBERT. It ** Work"
2021.findings-emnlp.141,2020.emnlp-demos.16,0,0.084274,"Missing"
2021.findings-emnlp.141,D14-1162,0,0.0920941,"Missing"
2021.findings-emnlp.141,N18-1202,0,0.119616,"Missing"
2021.findings-emnlp.141,D17-1010,0,0.0194889,"nseen or rare words into pieces that are in the vocabulary. The problem is that BPE can generate subword pieces that are not linguistically plausible. The BPE tokenization is not ideal for social media domains because its rules do not necessarily apply across domains, particularly the ones with substantial noise and spelling differences (Bostrom and Durrett, 2020). Compositional models The idea of composing OOV vectors has been explored before (Ling et al., 2015a; Plank et al., 2016). However, learning such vectors requires a large corpus and long computing time (i.e., processing characters). Pinter et al. (2017) proposed learning OOV words from a pretrained word embedding dictionary. They treat every word from the dictionary as a sequence of characters and output a single vector that mimicks the associated word embedding in the dictionary. Schick and Schütze (2019) improved this method by introducing attentive mimicking to account for context, besides the surface form of the word. 3 Method Given a word w, a subword model produces a sequence of subword pieces s “ ps0 , s1 , . . . , sn q, such that the concatenation of all the segments from s fully reconstructs the word w. Regardless of whether a subwo"
2021.findings-emnlp.141,P16-2067,0,0.0203862,"ingual BERT (Devlin et al., 2019). Regardless of the variant, these methods handle the out-of-vocabulary problem by breaking down unseen or rare words into pieces that are in the vocabulary. The problem is that BPE can generate subword pieces that are not linguistically plausible. The BPE tokenization is not ideal for social media domains because its rules do not necessarily apply across domains, particularly the ones with substantial noise and spelling differences (Bostrom and Durrett, 2020). Compositional models The idea of composing OOV vectors has been explored before (Ling et al., 2015a; Plank et al., 2016). However, learning such vectors requires a large corpus and long computing time (i.e., processing characters). Pinter et al. (2017) proposed learning OOV words from a pretrained word embedding dictionary. They treat every word from the dictionary as a sequence of characters and output a single vector that mimicks the associated word embedding in the dictionary. Schick and Schütze (2019) improved this method by introducing attentive mimicking to account for context, besides the surface form of the word. 3 Method Given a word w, a subword model produces a sequence of subword pieces s “ ps0 , s1"
2021.findings-emnlp.141,2020.tacl-1.54,0,0.0188006,"hat mimicks the associated word embedding in the dictionary. Schick and Schütze (2019) improved this method by introducing attentive mimicking to account for context, besides the surface form of the word. 3 Method Given a word w, a subword model produces a sequence of subword pieces s “ ps0 , s1 , . . . , sn q, such that the concatenation of all the segments from s fully reconstructs the word w. Regardless of whether a subword piece represents a character in a word or not, all the pieces are treated as semantic units within a sentence.2 Such pieces come 2 Previous studies (Clark et al., 2019; Rogers et al., 2020) showed that BERT learns syntax and parsing within its selffrom a rule-based system that does not take into account semantics or morphology during the tokenization. Thus, the subword tokenization has a significant impact on the semantic abstraction from upper layers in pre-trained models like mBERT. To alleviate such problems, we build word representations out of characters. The char2subword module allows flexible tokenization patterns, where the model can split by spaces, use the original tokenization method, or employ a different tokenization process as defined by the user. There are two mai"
2021.findings-emnlp.141,N19-1048,0,0.0172798,"across domains, particularly the ones with substantial noise and spelling differences (Bostrom and Durrett, 2020). Compositional models The idea of composing OOV vectors has been explored before (Ling et al., 2015a; Plank et al., 2016). However, learning such vectors requires a large corpus and long computing time (i.e., processing characters). Pinter et al. (2017) proposed learning OOV words from a pretrained word embedding dictionary. They treat every word from the dictionary as a sequence of characters and output a single vector that mimicks the associated word embedding in the dictionary. Schick and Schütze (2019) improved this method by introducing attentive mimicking to account for context, besides the surface form of the word. 3 Method Given a word w, a subword model produces a sequence of subword pieces s “ ps0 , s1 , . . . , sn q, such that the concatenation of all the segments from s fully reconstructs the word w. Regardless of whether a subword piece represents a character in a word or not, all the pieces are treated as semantic units within a sentence.2 Such pieces come 2 Previous studies (Clark et al., 2019; Rogers et al., 2020) showed that BERT learns syntax and parsing within its selffrom a"
2021.findings-emnlp.141,P16-1162,0,0.0633511,"LSTMs (Ling et al., 2015a). Most successnington et al., 2014). They showed the capability of ful contextualized word embeddings built out of arranging words in a continuous high-dimensional characters are the language models ELMo (Peters space encoding semantic relationships and mean- et al., 2018) and Flair (Akbik et al., 2018). Building (Goldberg and Levy, 2014). However, rare ing models from characters can easily adapt to sowords are weakly represented in such space, and cial media domains (Akbik et al., 2019), including 1641 code-switching data (Aguilar and Solorio, 2020). Subword models Sennrich et al. (2016) proposed subword tokenization using the byte-pair encoding (BPE) algorithm to balance the use of characters and words. BPE automatically chooses a vocabulary of subwords given the desired vocabulary size. This procedure recursively builds subwords upon characters using the word frequencies (Sennrich et al., 2016). Another greedy variation of BPE can select the longest prefix to segment words (Wu et al., 2016). Alternatively to the greedy versions, the segmentation can happen in a stochastic way; drawing segmentation candidates at different points of a word can improve generalization (Kudo, 20"
2021.findings-emnlp.141,W18-3503,0,0.182466,"tted text, that is not the case when the text presents substantial noise (e.g., Wikipedia vs. social media). Noisy text can result in extensive subword pieces per word (see Figure 1), preventing the models from capturing the mean1 Introduction ing effectively and adapting to such domains. This is particularly prominent on social media text (BaldByte-pair encodings (BPE) is a ubiquitous alwin et al., 2015; Eisenstein, 2013a,b), where the gorithm in the tokenization process among noise permeates even across languages and in codetransformer-based language models such as BERT switching scenarios (Singh et al., 2018; Aguilar (Devlin et al., 2019), GPT-2 (Radford et al., 2019), et al., 2018; Molina et al., 2016; Das, 2016). RoBERTa (Liu et al., 2019), and CTRL (Keskar This paper proposes a character-to-subword et al., 2019). This method addresses the open vo(char2subword) module trained to handle rare or uncabulary problem by segmenting unseen or rare seen spellings robustly while being less restrictive words into smaller subword units while keeping to a particular tokenization method. Our method a reasonable vocabulary size (Huck et al., 2017; works as a drop-in alternative to the embedding ta* Work perf"
2021.findings-emnlp.332,2020.emnlp-main.387,0,0.0999446,"Missing"
2021.findings-emnlp.332,D14-1162,0,0.0852122,"taset used in this work was developed based on the script data used in (Shafaei et al., 2019, anced. The dataset is first split and stratified using an 80/10/10 ratio for training, development, and 2020). We collected the up-to-date user ratings for test for each age-restricted aspect. The experimenage-restricted content from IMDB.com for more tal results on the test set are reported in Table 1. than 15,000 movies. The age-restricted aspects Baseline models include average GloVe embedare adopted from the Parents Guide section of each movies, and there are five aspects: Sex & Nudity, Vi- ding (Pennington et al., 2014) with SVM/Logistic olence & Gore, Profanity, Alcohol, Drugs & Smok- Regression, TextCNN (Kim, 2014), and BERT (Devlin et al., 2019). The proposed multitask model ing, and Frightening & Intense Scenes. Each of outperforms multiple baselines by a compelling the aspects has four severity levels for the users to rate on the corresponding movies from low to high, margin in all aspects. Statistical significance test shows introducing the ranking subtask does no which are None, Mild, Moderate, and Severe. In harm to model performance. this work, we pick the ratings on the website as the The proposed"
2021.findings-emnlp.332,D19-1410,0,0.0118428,"ts of movies. This problem is formulated as a multi-class classification task. The average length of the dialogue scripts is around 10,000 words, which drastically exceeds the limit of current popular Transformer-based models. To leverage the strong semantic representation capability of the Transformers, we propose to represent each utterance as the basic unit, and further encode the context with the recurrent modules. Finally, we use a fully connected layer on top of the encoded representations to produce the classification predictions. For this model, we first leverage SentenceTransformers (Reimers and Gurevych, 2019) to encode each dialogue utterance. Then, a Bi-directional LSTM encoder is deployed to model the sequential interrelations of the utterance flow. We finally apply a max-pooling operation on all time steps of the hidden states of the recurrent module to get the document representation for classification following the practice in (Howard and Ruder, 2018). We also study another strong word-level deep learning model, TextRCNN (Lai et al., 2015), to probe the significance of lexical signals. 2.1 The Multitask Ranking-Classification Framework None / Mild / Moderate / Severe Less / Equal / More cpr ["
2021.findings-emnlp.332,2020.lrec-1.166,1,0.753892,"solve the problem of predicting the severity of age-restricted content solely using the dialogue script data. Text is much more lightweight than visual data (such as images and videos), so the processing procedure can be more efficient and scalable considering the increasing fidelity of multimedia content. We initiate our exploration on movies from five aspects of contents: Sex & Nudity, Violence & Gore, Profanity, Alcohol, Drugs & Smoking, and Frightening & Intense Scenes as used in IMDB2 Parent Guide. There are a small number of previous works that studied modeling age-restricted content. (Shafaei et al., 2020) initiated the research of predicting MPAA ratings of the movies leveraging movie script and metadata. (Martinez et al., 2019) focused on violence detection using movie scripts while (Martinez et al., 2020) expanded the scope to violence, substance abuse, and sex. Both works intended to predict the severity of age-restricted content into three manually defined levels: low, mid, and high. In this work, We introduce two more aspects of interest: Frightening and Profanity. Instead of manually downgrading severity levels into three categories, we explore with a more challenging setting: rating on"
2021.socialnlp-1.14,P18-2109,0,0.0275979,"a is selected (2020) investigated the efficiency of deep neural based on the trend scores. Each data has 1,000 networks to detect trends. However, these techsamples. Table 1 shows the model performance niques are applied without taking named entities on the random data, versus the trending data. We into consideration. 166 Towards emerging named entities, recent work has mainly focus on identification and classification of unusual and previously unseen named entities. Derczynski et al. (2015) investigated the effects of data drift and the evaluation of the NER models on temporally unseen data. Agarwal et al. (2018) studied the disambiguation of named entities with explicit consideration of temporal background. Rijhwani and Preotiuc-Pietro (2020) reported improvements on performance for overlapping named entities under the impact of temporal drift. Due to the limitation of resources and lack of annotated data from social media, these NER models tend to have lower performances on emerging named entities. 5 Conclusion In this work, we propose a simple approach to update model parameters and prevent degradation performance from temporal drifts. Our approach is inspired by our observations of how Twitter dat"
2021.socialnlp-1.14,fromreide-etal-2014-crowdsourcing,0,0.0555357,"Missing"
2021.socialnlp-1.14,D16-1229,0,0.07037,"Missing"
2021.socialnlp-1.14,P16-1101,0,0.0613005,"Missing"
2021.socialnlp-1.14,2020.emnlp-demos.2,0,0.0216135,"es to our training set. For Random, we randomly select instances from the available data. For Trend, we rank all available instances from most trending to less trending based on their trending scores. We then use this ranking to select the instances. At each step, we choose the instances with the highest trending scores that have not yet been added to the training set. We experiment with 50 (Appendix B), 100 and 200 (Appendix B) instances per step to show the impact of training size. and generates contextualized word representations for each sentence. BERTweet + CRF Similar to BERT, BERTweet (Nguyen et al., 2020) is a large-scale language model with the same configuration as BERT. It is pre-trained on the corpora from the social media domain and achieves state-of-the-art results on many downstream Twitter NER tasks. 3.2 Results We empirically examine the performance of models under the influence of data evolution and temporal drift. We start with doing experiments on trending bi-grams and use the same amount of training samples at each step to eliminate the influence of training data size. Below we discuss the results of the two evaluation scenarios. trendiness score (trend), we take the model as trai"
2021.socialnlp-1.14,D14-1162,0,0.084962,"Missing"
2021.socialnlp-1.14,2020.acl-main.680,0,0.201799,"ber of new instances each time. For the trend models, we select instances based on their trend scores, regardless of the year, whereas for the random model, we select instances at random from the merged pool of data. Similar to what we did in scenario 1, we run each model 5 times and report the averaged results. The results are shown in Figure 3. Similar to scenario 1, the F1 scores of the models trained on instances selected based on their trend scores are always higher than random sampling F1 scores. In addition, scenario 2, on average, works better than scenario 1, which is consistent with Rijhwani and Preotiuc-Pietro (2020). However, this setting requires the data available from all years from the very beginning. Compared to scenario 2, scenario 1 is far more realistic because it can be more easily applied in practice. 3.3 Analysis Impact of training data size We ran additional experiments where we add different amounts of training data at each iteration (50 and 200). With less training data available, the benefits of selecting instances based on trend scores are amplified. Even if more data is available, using trend scores to select which instances to add always results in better performance than randomly choos"
2021.socialnlp-1.14,N18-1044,0,0.0212756,"of the task, and use this metric to select the most informative authors also call this language drift (Fromreide instances for retraining. We show that labeling et al., 2014; Derczynski et al., 2015). Temporal instances based on this approach can yield better drift effects are amplified in social media. Due to downstream performance than randomly sampling the ecosystem’s very nature, topics reflect events tweets for annotation. and interests of a diverse user base and are continuNote that topics such as semantic shift (Hamilton ously and rapidly evolving. To study the impact of et al., 2016; Rosenfeld and Erk, 2018) and active language drift, we focus our analysis on the case learning (Sinha et al., 2019; Kirsch et al., 2019) are of NER on Twitter data. Emerging and Trending related to the work we present here. In semantic topics are an essential part of Twitter. They change shift, the core problem is how to trace temporal quite rapidly, reflecting diverse topics and world changes in lexical semantics, including linguistic 1 drifts and cultural shifts. Unlike this task, our goal We release the code at https://github.com/ is to leverage the emergence of trends to guide an RiTUAL-UH/trending_NER. 163 Proce"
2021.wnut-1.11,W18-3219,1,0.894867,"Missing"
2021.wnut-1.11,P18-1185,1,0.871627,"Missing"
2021.wnut-1.11,W17-4418,0,0.0317405,"Missing"
2021.wnut-1.11,N18-1078,1,0.837358,"ity of platforms like Twitter, Instagram, and Snapchat, where users can create multimedia posts, images and text are frequently used together. This trend enables the use of images to improve current NER systems. Given a pair of text and image, the Multimodal NER(MNER) task’s goal is to identify and classify named entities in the text. Current work in MNER mainly focuses on the alignment between words and image regions and the fusion of textual information and visual context. Recent successful architectures for MNER mainly rely on attention mechanisms combined with different fusion techniques (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018; Arshad et al., 2019; Asgari-Chenaghlu et al., 2020; Yu et al., 2020). 1 We release the code at https://github.com/ RiTUAL-UH/multimodal_NER. 87 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 87–96 November 11, 2021. ©2021 Association for Computational Linguistics work and expose the bottleneck of existing approaches in terms of multimodal fusion. 2. We study an alternative approach to incorporate images in MNER. We use captions to represent images as text and adapt transformerbased sequence labeling"
2021.wnut-1.11,N19-1423,0,0.139237,"1) proposed a text-image relation propagation-based multimodal BERT model (RpBERT) to reduce the interference from irrelevant images, which effectively resolved failed cases mentioned in previous work (Lu et al. (2018); Arshad et al. (2019); Yu et al. (2020)). Recent advances in large pre-trained models (Lu et al., 2019; Tan and Bansal, 2019) have led to 3 Methodology In this work, we study how to represent images and how to fuse different modalities in MNER. We explore three different ways to represent images in different levels of semantic density. Then we adapt a unimodal transformer BERT (Devlin et al., 2019) and a multimodal transformer VisualBERT (Li et al., 2019) as base models. To fuse textual information with visual information, we investigate four different approaches for multimodal fusion. 3.1 Image Representation We explore three different ways to represent images in order of semantic density: i) global image features to represent the whole image using image classification that assigns each image with a single class, ii) regional image features to represent objects in the image using object detection that increases semantic density by labeling multiple objects, and iii) image captions to r"
2021.wnut-1.11,2020.coling-main.168,0,0.0342445,"20) addresses these problems by proposing a neural network to better exploit visual and textual information. However, their results revealed that their model remains to extract entities incorrectly when visual objects cannot reveal the label semantics of entities. To address the problem of semantic disparity between different modalities, Wu et al. (2020) chooses to transform object labels into word embeddings. Their dense co-attention module introduced can take the interand intra-connections between visual objects and textual entities into account, which helps extract entities more precisely. Sun et al. (2020) introduced a pre-trained multimodal language model based on Relationship Inference and Visual Attention (RIVA) for tweets, and a gated visual context based on text-image relation. Sun et al. (2021) proposed a text-image relation propagation-based multimodal BERT model (RpBERT) to reduce the interference from irrelevant images, which effectively resolved failed cases mentioned in previous work (Lu et al. (2018); Arshad et al. (2019); Yu et al. (2020)). Recent advances in large pre-trained models (Lu et al., 2019; Tan and Bansal, 2019) have led to 3 Methodology In this work, we study how to rep"
2021.wnut-1.11,D19-1514,0,0.0189357,"ties into account, which helps extract entities more precisely. Sun et al. (2020) introduced a pre-trained multimodal language model based on Relationship Inference and Visual Attention (RIVA) for tweets, and a gated visual context based on text-image relation. Sun et al. (2021) proposed a text-image relation propagation-based multimodal BERT model (RpBERT) to reduce the interference from irrelevant images, which effectively resolved failed cases mentioned in previous work (Lu et al. (2018); Arshad et al. (2019); Yu et al. (2020)). Recent advances in large pre-trained models (Lu et al., 2019; Tan and Bansal, 2019) have led to 3 Methodology In this work, we study how to represent images and how to fuse different modalities in MNER. We explore three different ways to represent images in different levels of semantic density. Then we adapt a unimodal transformer BERT (Devlin et al., 2019) and a multimodal transformer VisualBERT (Li et al., 2019) as base models. To fuse textual information with visual information, we investigate four different approaches for multimodal fusion. 3.1 Image Representation We explore three different ways to represent images in order of semantic density: i) global image features"
2021.wnut-1.11,2020.acl-main.306,0,0.39066,"text are frequently used together. This trend enables the use of images to improve current NER systems. Given a pair of text and image, the Multimodal NER(MNER) task’s goal is to identify and classify named entities in the text. Current work in MNER mainly focuses on the alignment between words and image regions and the fusion of textual information and visual context. Recent successful architectures for MNER mainly rely on attention mechanisms combined with different fusion techniques (Moon et al., 2018; Zhang et al., 2018; Lu et al., 2018; Arshad et al., 2019; Asgari-Chenaghlu et al., 2020; Yu et al., 2020). 1 We release the code at https://github.com/ RiTUAL-UH/multimodal_NER. 87 Proceedings of the 2021 EMNLP Workshop W-NUT: The Seventh Workshop on Noisy User-generated Text, pages 87–96 November 11, 2021. ©2021 Association for Computational Linguistics work and expose the bottleneck of existing approaches in terms of multimodal fusion. 2. We study an alternative approach to incorporate images in MNER. We use captions to represent images as text and adapt transformerbased sequence labeling models to connect multimodal information. 3. We provide empirical evidence to expose the situations where i"
C04-1201,W02-1033,0,0.0828513,"Missing"
C04-1201,J03-3001,0,0.16098,"ario considers as attribute information prefixes of words in combination with attributes whose values are obtained from the Internet. These Internet based attributes are targeted to extract evidence of the possible semantic class of the question. The next subsection will explain how the Internet is used to extract attributes for our question classification problem. In subsection 3.2 we present a brief description of Support Vector Machines, the learning algorithm used on our experiments. 3.1 Using Internet As Kilgarriff and Grefenstette wrote, the Internet is a fabulous linguists’ playground (Kilgarriff and Grefenstette, 2003). It has become the greatest information source available worldwide, and although English is the dominant language represented on the Internet it is very likely that one can find information in almost any desired language. Considering this, and the fact that the texts are written in natural language, we believe that new methods that take advantage of this large corpus must be devised. In this work we propose using the Internet in order to acquire information that can be used as attributes in our classification problem. This attribute information can be extracted automatically from the web and"
C04-1201,C02-1150,0,0.241031,"Missing"
C04-1201,W03-1208,0,0.0377198,"Missing"
C14-1116,P11-1030,1,0.860458,"Missing"
C14-1116,E09-1039,0,0.300969,"styles of text. Obtaining such corpora is a challenging task since most authorship attribution studies focus on a single domain. We have found two datasets that meet our criteria, one having both cross-topic and cross-genre flavor, and the other having only cross-topic flavor. The first corpus contains communication samples from 21 authors in six genres (Email, Essay, Blog, Chat, Phone Interview, and Discussion) on six topics (Catholic Church, Gay Marriage, War in Iraq, Legalization of Marijuana, Privacy Rights, and Sex Discrimination), which we call dataset 1. This dataset was obtained from Goldstein-Stewart et al. (2009). Using this dataset, it is possible to see how the performance of cross-topic AA changes across different genres. Another corpus is composed of texts published in The Guardian daily newspaper written by 13 authors in one genre on four topics (dataset 2) due Stamatatos et al. (2013). It contains opinion articles (comments) about World, U.K., Culture, and Politics. Table 1 shows some statistics about the datasets. Corpus #authors #genres #topics Dataset 1 Dataset 2 21 13 6 1 6 4 avg #docs/author 36 64 avg #sentences/doc 31.7 53 avg #words/doc 600 1034 Table 1: Some statistics about dataset 1 an"
C18-1244,I13-1171,0,0.0735415,"Missing"
C18-1244,I13-2009,0,0.0387181,"Missing"
C18-1244,E14-1023,0,0.0404477,"Missing"
C18-1244,P13-1035,0,0.0570972,"Missing"
C18-1244,C14-1008,0,0.0342289,"dules. The first module uses a convolutional neural network (CNN) to learn plot representations from synopses. The second module models the flow of emotions via a bidirectional long short-term memory (Bi-LSTM) network. And the last module contains hidden dense layers that operate on the combined representations generated by the first and second modules to predict the most likely tags for movies. (a) Convolutional Neural Network (CNN): Recent successes in different text classification problems motivated us to extract important word level features using convolutional neural networks (CNNs) (dos Santos and Gatti, 2014; Kim, 2014; Zhang et al., 2015; Kar et al., 2017; Shrestha et al., 2017). We design a model that takes word sequences as input, where each word is represented by a 300-dimensional word embedding vector. We use randomly initialized word embeddings but also experiment with the FastText7 word embeddings trained on Wikipedia using subword information. We stack 4 sets of one-dimensional convolution modules with 1024 filters each for filter sizes 2, 3, 4, and 5 to extract word-level n-gram features (Kim, 2014; Zhang et al., 2015). Each filter of size c is applied from window t to window t + c − 1 o"
C18-1244,D13-1181,0,0.134077,"Missing"
C18-1244,N15-1113,0,0.128661,"Missing"
C18-1244,D10-1008,0,0.0847849,"Missing"
C18-1244,N16-1180,0,0.0854281,"Missing"
C18-1244,S17-2150,1,0.847273,"work (CNN) to learn plot representations from synopses. The second module models the flow of emotions via a bidirectional long short-term memory (Bi-LSTM) network. And the last module contains hidden dense layers that operate on the combined representations generated by the first and second modules to predict the most likely tags for movies. (a) Convolutional Neural Network (CNN): Recent successes in different text classification problems motivated us to extract important word level features using convolutional neural networks (CNNs) (dos Santos and Gatti, 2014; Kim, 2014; Zhang et al., 2015; Kar et al., 2017; Shrestha et al., 2017). We design a model that takes word sequences as input, where each word is represented by a 300-dimensional word embedding vector. We use randomly initialized word embeddings but also experiment with the FastText7 word embeddings trained on Wikipedia using subword information. We stack 4 sets of one-dimensional convolution modules with 1024 filters each for filter sizes 2, 3, 4, and 5 to extract word-level n-gram features (Kim, 2014; Zhang et al., 2015). Each filter of size c is applied from window t to window t + c − 1 on a word sequence x1 , x2 , . . . , xn . Convolut"
C18-1244,L18-1274,1,0.680745,"Missing"
C18-1244,D14-1181,0,0.00245806,"uses a convolutional neural network (CNN) to learn plot representations from synopses. The second module models the flow of emotions via a bidirectional long short-term memory (Bi-LSTM) network. And the last module contains hidden dense layers that operate on the combined representations generated by the first and second modules to predict the most likely tags for movies. (a) Convolutional Neural Network (CNN): Recent successes in different text classification problems motivated us to extract important word level features using convolutional neural networks (CNNs) (dos Santos and Gatti, 2014; Kim, 2014; Zhang et al., 2015; Kar et al., 2017; Shrestha et al., 2017). We design a model that takes word sequences as input, where each word is represented by a 300-dimensional word embedding vector. We use randomly initialized word embeddings but also experiment with the FastText7 word embeddings trained on Wikipedia using subword information. We stack 4 sets of one-dimensional convolution modules with 1024 filters each for filter sizes 2, 3, 4, and 5 to extract word-level n-gram features (Kim, 2014; Zhang et al., 2015). Each filter of size c is applied from window t to window t + c − 1 on a word se"
C18-1244,N15-1185,0,0.0327369,"Missing"
C18-1244,E17-1114,1,0.896959,"Missing"
C18-1244,N18-2042,1,0.787141,"ffective dimensions is represented by six different activation levels that make up to 24 distinct labels called ‘elementary emotions’ that represent the total emotional state of the human mind. NRC8 emotion lexicons (Mohammad and Turney, 2013) is a list of 14,182 words9 and their binary associations with eight types of elementary emotions from the Hourglass of Emotions model (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) with polarity. These lexicons have been used effectively in tracking the emotions in literary texts (Mohammad, 2011) and predicting success of books (Maharjan et al., 2018). To model the flow of emotions throughout the plots, we divide each synopsis into N equally-sized segments based on words. For each segment, we compute the percentage of words corresponding to each emotion and polarity type (positive and negative) using the NRC emotion lexicons. More precisely, for a synopsis xX, where X denotes the entire collection of plot synopses, we create N sequences of emotion vectors using the NRC emotion lexicons as shown below: x → s1:N = [s1 , s2 , ..., sN ] (4) 8 9 National Research Council Canada Version 0.92 2882 where si is the emotion vector for segment i. We"
C18-1244,P10-1158,0,0.0619649,"Missing"
C18-1244,W11-1514,0,0.0358807,"uman emotions by Plutchik (2001). Each of these affective dimensions is represented by six different activation levels that make up to 24 distinct labels called ‘elementary emotions’ that represent the total emotional state of the human mind. NRC8 emotion lexicons (Mohammad and Turney, 2013) is a list of 14,182 words9 and their binary associations with eight types of elementary emotions from the Hourglass of Emotions model (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) with polarity. These lexicons have been used effectively in tracking the emotions in literary texts (Mohammad, 2011) and predicting success of books (Maharjan et al., 2018). To model the flow of emotions throughout the plots, we divide each synopsis into N equally-sized segments based on words. For each segment, we compute the percentage of words corresponding to each emotion and polarity type (positive and negative) using the NRC emotion lexicons. More precisely, for a synopsis xX, where X denotes the entire collection of plot synopses, we create N sequences of emotion vectors using the NRC emotion lexicons as shown below: x → s1:N = [s1 , s2 , ..., sN ] (4) 8 9 National Research Council Canada Version 0."
C18-1244,E17-2106,1,0.821921,"n plot representations from synopses. The second module models the flow of emotions via a bidirectional long short-term memory (Bi-LSTM) network. And the last module contains hidden dense layers that operate on the combined representations generated by the first and second modules to predict the most likely tags for movies. (a) Convolutional Neural Network (CNN): Recent successes in different text classification problems motivated us to extract important word level features using convolutional neural networks (CNNs) (dos Santos and Gatti, 2014; Kim, 2014; Zhang et al., 2015; Kar et al., 2017; Shrestha et al., 2017). We design a model that takes word sequences as input, where each word is represented by a 300-dimensional word embedding vector. We use randomly initialized word embeddings but also experiment with the FastText7 word embeddings trained on Wikipedia using subword information. We stack 4 sets of one-dimensional convolution modules with 1024 filters each for filter sizes 2, 3, 4, and 5 to extract word-level n-gram features (Kim, 2014; Zhang et al., 2015). Each filter of size c is applied from window t to window t + c − 1 on a word sequence x1 , x2 , . . . , xn . Convolution units of filter size"
D08-1102,C82-1023,0,0.439987,"ic boundaries shared by both languages, and the resulting monolingual fragments will conform to the grammar of the corresponding language. In this CS theory the relationship between both languages is symmetric –lexical items from one language can be replaced by the corresponding items in the second language and vice versa. Another prevalent linguistic theory argues the contrary: there is an asymmetric relation where the changes can occur only in one direction, which reflects the existence of a Matrix Language (ML), the dominant language, and an Embedded Language (EL), or subordinate language (Joshi, 1982). The Matrix Language Frame model, proposed and extended by Scotton-Myers, supports this asymmetric relation theory. This formalism prescribes that content morphemes can come from the 974 ML or the EL, whereas late system morphemes, the elements that indicate grammatical relations, can only be provided by the ML (Myers-Scotton, 1997). Until an empirical evaluation is carried out on large representative samples of discourse involving a large number of different speakers, and different language-pairs, the production of CS discourse will not be explained satisfactorily. The goal of this work is t"
D08-1102,P98-1002,0,0.457312,"ingual discourse, and corpus-driven studies about CS can also inform linguistic theories. In this paper we present exploratory work on learning to predict CS points using a machine learning approach. Such an approach can be used to reduce perplexity of language models for bilingual discourse. We believe that CS behavior can be learned by a classifier and the results presented in this paper support our belief. One of the difficult aspects of trying to predict CS points is how to evaluate the performance of the learner since switching is intrinsically motivated and there are no forced switches (Sankoff, 1998b). Therefore, standard classification measures for this task such as precision, recall, F-measure, or accuracy, are not the best approach for measuring the effectiveness of a CS predictor. To comple973 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 973–981, c Honolulu, October 2008. 2008 Association for Computational Linguistics ment the evaluation of our approach, we designed a task involving human judgements on the naturalness of automatically generated code-switched sentences. Both evaluations yielded encouraging results. The next section disc"
D08-1102,D08-1110,1,0.780716,"anguage processing tasks, including optical character recognition, text classification, named entity extraction, and many more. The premise of our paper is that machine learning algorithms can also be successful at learning how to code-switch as well as humans. At the very least we want to provide encouraging evidence that this is possible. To the best of our knowledge, there is no previous work related to the problem of automatically predicting CS points. Our machine learning framework then is inspired by existing theories of CS and existing work on part-of-speech tagging code-switched text (Solorio and Liu, 2008). In our approach, each word boundary is a potential point for switching – an instance of the learning task. It should be noted that we can only rely on the history of words preceding potential CS points in orFeature id 1 2 3 4 5 6 7 8 9 10 Description Word Language id Gold-standard POS tag BIO chunk English Tree Tagger POS English Tree Tagger prob English Tree Tagger lemma Spanish Tree Tagger POS Spanish Tree Tagger prob Spanish Tree Tagger lemma Table 1: Features explored in learning to predict CS points. der to extract meaningful features. Otherwise, if we look also into the future, we coul"
D08-1102,C98-1002,0,\N,Missing
D08-1110,carreras-padro-2002-flexible,0,0.0484014,"Missing"
D08-1110,C82-1023,0,0.54418,"n subject to judge sentences generated by a PCFG induced from training data and the language model. However, they only used one human judge. Regarding the automated POS tagging and parsing of code-mixed utterances there is little prior work. To the best of our knowledge, there is no parser, nor POS tagger, currently available for the syntactic analysis of this type of discourse. There are theoretical approaches that propose formalisms to represent the structure of code-switched utterances and describe a framework for parsing and generating mixed sentences, for example for Marathi and English (Joshi, 1982), or Hindi and English (Goyal et al., 2003). Sankoff proposed a production model of bilingual discourse that accounts for the equivalence constraint and the unpredictability of code-switching (Sankoff, 1998a; Sankoff, 1998b). His real-time production model draws on the alternation of fragments from two virtual monolingual sentences. It also accounts for other types of codeswitching such as repetition-translation and insertional code-switching. But no statistical assessment has been conducted on real corpora. Our goal is to develop a POS tagger for codeswitched utterances, which is the first st"
D08-1110,W96-0213,0,0.0249762,"y et al., 1992). Currently, there is no annotation of code-switched text of comparable size. But in contrast to the lack of linguistic resources available for Spanish-English code-mixed discourse, English and Spanish have sufficient resources, especially English. Thus, rather than starting from scratch, we will draw on existing taggers for both languages, which will reduce the amount of code-switched data needed. Some examples of POS taggers that perform reasonably well on monolingual text of each language can be found in (Brants, 2000; Brill, 1992; Carreras and Padro´ , 2002; Charniak, 1993; Ratnaparkhi, 1996; Schmid, 1994). However, these tools are designed to work on monolingual text, therefore if applied as they are to code-switched text, their accuracy will decrease by a large margin. In the following sections we will explore different methods for combining monolingual taggers. 4 Table 1: Excerpts taken from the Spanglish data set. Spanglish (a)Entonces le di´o el virus y no se lo atendi´o and the virus spread through his body. (b)Cuando yo lo vi he looked pretty bad. (c)I think she was taller than he was. Y un car´acter muy bonito tambi´en ella. Very easy going. English Translation (a)Then he"
D08-1110,P98-1002,0,0.586452,"rs have been analyzed on different language pairs, including English-French, English-Dutch, Finish-English, Arabic-French, and Spanish-English, to name a few. There is a general agreement that code-switched patterns are not generated randomly; according to these studies, they follow specific grammatical rules. Furthermore, some studies suggest that, if these rules are violated, the resulting discourse will sound unnatural (Toribio, 2001b; Toribio, 2001a). The following shows the rules governing code-switching discourse described in several studies (Poplack, 1980; Poplack, 1981; Sankoff, 1981; Sankoff, 1998a). • Switches can take place only between full word boundaries. This is also known as the free morpheme constraint. • Monolingual constructs within the sentence 1052 Although these rules are somewhat controversial, and most of the studies on this area have been conducted on small samples, we cannot ignore the fact that patterns bearing the above rules have emerged in different bilingual communities with different backgrounds. 3 Automated Processing of Code-Switched Discourse A previous work related to the processing of codeswitched text deals with language identification on English-Maltese co"
D08-1110,H92-1073,0,\N,Missing
D08-1110,C98-1002,0,\N,Missing
D08-1110,A00-1031,0,\N,Missing
D18-1375,W17-4419,1,0.785283,"book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user information was much more effective in enhancing the document representations than the product information."
D18-1375,D13-1181,0,0.155245,"ver image of two of such books. The fact that the cover has no images with just plain background, and title, leaves little information for the visual modality. Similarly, we also analyzed the books that were correctly classified by visual features only and misclassified when textual features were added. Figure 7 shows two such books. Both the cover image and the title (present in the cover) of these two books seem to be interesting and are very likely to attract a reader’s attention. 7 Related Work Prior works have shown that stylistic traits to be useful features to predict success of books (Ashok et al., 2013; Underwood and Sellers, 2016; Maharjan et al., 2017). Ashok et al. (2013) used stylistic features extracted using the first 1K sentences from books to classify highly successful literature from less successful literature. van Cranenburgh and Bod (2017) used lexical and rich syntactic tree features to distinguish the degrees of high and less literary novels. Louis and Nenkova (2013) defined genre-specific and general features to predict the article quality in science journalism domain. Maharjan et al. (2017) compared their work with Ashok et al. (2013) and presented a new dataset for the book"
D18-1375,D16-1153,0,0.0276143,"rn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user information was much more effective in enhancing the document representations than t"
D18-1375,P16-1223,0,0.0423623,"Missing"
D18-1375,D16-1171,0,0.0295022,"prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user informatio"
D18-1375,D16-1044,0,0.0231405,"of the features from different modalities. This method is similar to our proposed method, except that we do not use genre information for computing the attention weights. We compute the score(.) as vT selu(Wa hi + ba ), without the genre information. This experiment will help us understand the importance of genre in computing weights for the feature types. Bilinear Model: We combine the non-linear transformed modalities h1 , . . . , hn using a bilinear form (hi T Wb hj + bb ), where k×d ×d Wb R hi hj is the weight tensor and bb is the bias vector (Socher et al., 2013; Laha and Raykar, 2016; Fukui et al., 2016; Gao et al., 2016). This operation gives us a k-dimensional vector. In the case  of more than two modalities, we first create n2 pairs of these modalities and combine each of them using a bilinear form. The final book vector is the concatenation of the resulting vectors from each of these pairs. Bilinear models are used in the visual question answering community to fuse visual and textual information (Fukui et al., 2016). This experiment will help us understand how our proposed model compares with other state-of-the-art multimodal approaches. For all these models as well, we also performed a"
D18-1375,S17-2150,1,0.851089,". (2018) also showed that modeling sequential flow of emotions across entire books improves likability prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They u"
D18-1375,C18-1244,1,0.835301,"ed by humans, but this is error-prone, very subjective, and a non-scalable process. An alternative to the human-guided process is to design a reliable automatic system that predicts the likability of books. Such a system, we argue, must be able to take into account all of the many aspects involved in the eventual success of a book. These include not only the topic of the book and the writing style of the author, but in the case of creative writing, also include elements such as creativity, plot structure, and the flow of sentiments (Hall, 2012; Archer and Jockers, 2016; Maharjan et al., 2018; Kar et al., 2018). Other relevant aspects influencing readers’ interest for a book could be the cover and the title of the book. We believe that in addition to the ability to incorporate the different aspects, it is equally important to have a robust mechanism that gives higher weight to the most relevant aspects, while at the same time disregards the noisy or redundant aspects. Traditionally, this is achieved by searching through multiple feature combination experiments for an optimal combination of different feature types (Yang and Pedersen, 1997; Forman, 2003). The main problem with these methods is that th"
D18-1375,C16-1260,0,0.0192776,"priate weights for each of the features from different modalities. This method is similar to our proposed method, except that we do not use genre information for computing the attention weights. We compute the score(.) as vT selu(Wa hi + ba ), without the genre information. This experiment will help us understand the importance of genre in computing weights for the feature types. Bilinear Model: We combine the non-linear transformed modalities h1 , . . . , hn using a bilinear form (hi T Wb hj + bb ), where k×d ×d Wb R hi hj is the weight tensor and bb is the bias vector (Socher et al., 2013; Laha and Raykar, 2016; Fukui et al., 2016; Gao et al., 2016). This operation gives us a k-dimensional vector. In the case  of more than two modalities, we first create n2 pairs of these modalities and combine each of them using a bilinear form. The final book vector is the concatenation of the resulting vectors from each of these pairs. Bilinear models are used in the visual question answering community to fuse visual and textual information (Fukui et al., 2016). This experiment will help us understand how our proposed model compares with other state-of-the-art multimodal approaches. For all these models as well,"
D18-1375,D15-1298,0,0.0708245,"Missing"
D18-1375,D15-1044,0,0.0469492,"at book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user information was much more effective in enhancing the document representations than the product information. Similar to their idea, we fuse the"
D18-1375,N15-1010,1,0.83863,"2.1 Features For our features, we build on the work by Maharjan et al. (2017) that provides a comprehensive exploration of different hand-crafted features and neural representations. They showed that a combination of writing density (WR) (distribution The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ genre_aware_attention of word, character, sentences, and paragraphs), Book2Vec, and recurrent neural network representations (RNN) works well for books. Similar to their work, our textual features consist of word, character, and typed character n-grams (Sapkota et al., 2015), syntactic features, sentiment and sentic concepts and scores (SCS) (Cambria et al., 2014), style-related WR and readability (R), and neural representations learned using Word2Vec (Mikolov et al., 2013), Doc2Vec and RNN. We consider these categories of the textual features as different modalities or sources since they capture different aspects of a book and are generated by different processes. In addition to these features, we also add visual information extracted from the book covers. To extract the visual features, we rely on state-of-the-art visual feature extractor methods like VGG (Simo"
D18-1375,Q13-1028,0,0.0742807,"in the cover) of these two books seem to be interesting and are very likely to attract a reader’s attention. 7 Related Work Prior works have shown that stylistic traits to be useful features to predict success of books (Ashok et al., 2013; Underwood and Sellers, 2016; Maharjan et al., 2017). Ashok et al. (2013) used stylistic features extracted using the first 1K sentences from books to classify highly successful literature from less successful literature. van Cranenburgh and Bod (2017) used lexical and rich syntactic tree features to distinguish the degrees of high and less literary novels. Louis and Nenkova (2013) defined genre-specific and general features to predict the article quality in science journalism domain. Maharjan et al. (2017) compared their work with Ashok et al. (2013) and presented a new dataset for the book success prediction task. Their multitask approach with the combination of deep representations and hand-crafted features improved the classification results. Maharjan et al. (2018) also showed that modeling sequential flow of emotions across entire books improves likability prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and g"
D18-1375,E17-1114,1,0.60257,"Missing"
D18-1375,N18-2042,1,0.774548,"current process is guided by humans, but this is error-prone, very subjective, and a non-scalable process. An alternative to the human-guided process is to design a reliable automatic system that predicts the likability of books. Such a system, we argue, must be able to take into account all of the many aspects involved in the eventual success of a book. These include not only the topic of the book and the writing style of the author, but in the case of creative writing, also include elements such as creativity, plot structure, and the flow of sentiments (Hall, 2012; Archer and Jockers, 2016; Maharjan et al., 2018; Kar et al., 2018). Other relevant aspects influencing readers’ interest for a book could be the cover and the title of the book. We believe that in addition to the ability to incorporate the different aspects, it is equally important to have a robust mechanism that gives higher weight to the most relevant aspects, while at the same time disregards the noisy or redundant aspects. Traditionally, this is achieved by searching through multiple feature combination experiments for an optimal combination of different feature types (Yang and Pedersen, 1997; Forman, 2003). The main problem with these"
D18-1375,D16-1058,0,0.0351486,"oved the classification results. Maharjan et al. (2018) also showed that modeling sequential flow of emotions across entire books improves likability prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final repres"
D18-1375,C16-1153,0,0.0177166,"oved the classification results. Maharjan et al. (2018) also showed that modeling sequential flow of emotions across entire books improves likability prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final repres"
D18-1375,C16-1231,0,0.0304648,"rafted features improved the classification results. Maharjan et al. (2018) also showed that modeling sequential flow of emotions across entire books improves likability prediction of books. Iwana et al. (2016) used neural networks to learn relationships between book covers and genre. They showed that book covers tend to have carefully designed color and tone, objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to lea"
D18-1375,D17-1004,0,0.0255443,", objects, and text. Our work relies on prior works’ hand-engineered and deep learning The attention mechanism (Bahdanau et al., 2014) has been successfully applied in enhancing the document representation for several text classification (Zhang et al., 2016; Wang et al., 2016b), sentiment classification (Kar et al., 2017; Nguyen and Shirai, 2015; Wang et al., 2016a), question answering (Tan et al., 2015; Chen et al., 2016a; Hermann et al., 2015), named entity recognition (Bharadwaj et al., 2016; Aguilar et al., 2017), summarization (Rush et al., 2015), imagecaptioning (Xu et al., 2015) tasks. Zhang et al. (2017) used summary vectors and position vectors while computing the attention weights for the slot filling problem. Chen et al. (2016b) applied user preferences and product characteristics as attentions to words and sentences in reviews to learn the final representation for the sentences and reviews. They used these representation to do the sentiment classification task and showed that adding user information was much more effective in enhancing the document representations than the product information. Similar to their idea, we fuse the genre information while computing attention weights. 8 Conclu"
E17-1114,W16-5311,1,0.731504,"Missing"
E17-1114,baccianella-etal-2010-sentiwordnet,0,0.00607983,"es representing grammatical classes, like affixes, and stylistic classes, like beg-punct and midpunct which reflect the position of punctuation marks in the n-gram. The purpose of these features is to correlate success with an author’s word choice. Constituents: We computed the normalized counts of ‘SBAR’, ‘SQ’, ‘SBARQ’, ‘SINV’, and ‘S’ syntactic tag sets from the parse tree of each sentence in each book, following the method of Ganjigunte Ashok et al. (2013) to determine the syntactic style of the authors. Sentiment: We computed sentence neutrality, positive and negative, using SentiWordNet (Baccianella et al., 2010) along with the counts of nouns, verbs, adverbs, and adjectives. We averaged these scores for every 50 consecutive sentences in order to evaluate change in sentiment throughout the course of each book, because we anticipate emotions, like suspense, anger, and happiness to contribute to the success of the book. SenticNet Concepts: We extracted sentiment concepts from the books using the Sentic Concept 1219 4.2 GRU GRU Success Softmax GRU since we met that last strange GRU Genre Softmax GRU Cross-entropy Loss well how has it been with you GRU time of Input words word2vec embedding Average repres"
E17-1114,W14-4012,0,0.104682,"Missing"
E17-1114,W14-0905,0,0.0907491,"Missing"
E17-1114,P13-1166,0,0.0264283,"Missing"
E17-1114,D13-1181,0,0.281956,"eived many rejection letters before landing their first publishing deal. Many factors contribute to the eventual success of a given book. Internal factors such as plot, story line, and character development all have a role in the likability of a book. External factors such as author reputation and marketing strategy are arguably equally relevant. Some factors might even be out of the control of an author or publishing house, such as the current trends, the competition from books released simultaneously, and the historical and contextual factors inherent to society. Previous work by Ganjigunte Ashok et al. (2013) demonstrated relevant results using stylistic features to predict the success of books. Their definition of success was a function of the number of downloads from Project Gutenberg. However downloading a book is not by itself an indicator of a highly liked or a commercially successful book. We instead propose to use the rating from reviewers collected from Goodreads as a measure of success. We also propose features and deep learning techniques that have not been used before on this problem, and validate their usefulness in two different tasks: success prediction and genre classification. Our"
E17-1114,P14-1066,0,0.0383983,"Missing"
E17-1114,D10-1008,0,0.0582638,"Missing"
E17-1114,D08-1020,0,0.105718,"Missing"
E17-1114,Q13-1028,0,0.282707,"Missing"
E17-1114,W16-5806,1,0.845498,"Missing"
E17-1114,N15-1010,1,0.833185,"ed a wide range of textual features in an attempt to capture the topic, sentiment, writing style, and readability for each book. This set included both new and previously used features. We also explored techniques for automatically learning representations from text using neural networks, which have been shown to be successful in various text classification tasks (Kiros et al., 2015; LeCun et al., 2015). These techniques include word embeddings, document embeddings, and recurrent neural networks. 4.1 Hand-crafted text features Lexical: We used skip-grams, char n-grams, and typed char n-grams (Sapkota et al., 2015) with term frequency-inverse document frequency (TFIDF) as the weighting scheme. Sapkota et al. (2015) showed that classical character n-grams lose some information in merging instances of ngrams like the which could be a prefix (thesis), a suffix (breathe), or a standalone word (the). They separated character n-grams into ten categories representing grammatical classes, like affixes, and stylistic classes, like beg-punct and midpunct which reflect the position of punctuation marks in the n-gram. The purpose of these features is to correlate success with an author’s word choice. Constituents:"
E17-1114,P13-1045,0,0.0106024,"re computed using the same unified representation h. z succ and z gen represent two different linear transformations over h that map to the number of classes. 5 5.1 Experiments and Results Experiments on Goodreads dataset We merged books from different genres, and then randomly divided the data into a 70:30 training/test ratio, while maintaining the distribution of Successful and Unsuccessful classes per genre. As a preprocessing step we converted all words to lowercase and removed infrequent tokens having document frequency ≤ 2. For our tagging and parsing needs, we used the Stanford parser (Socher et al., 2013). We then trained a LibLinear Support Vector Machine (SVM)7 classifier with L2 regularization using the hand-crafted features described in Section 4. We tuned the C parameter in the training set with 3-fold grid search cross-validation over different values of 1e{-4,...,4}. With the features used by Ganjigunte Ashok et al. (2013), we obtained the highest weighted F1score of 0.659 with word bigram features. We set this value as our baseline. In order to study the effect of the multitask approach, we devised analogous experiments to our proposed multitask RNN method and predicted both genre and"
E17-1114,P16-2038,0,0.0433802,"d embedding (Mikolov et al., 2013) and Paragraph Vector (Le and Mikolov, 2014) have been shown to achieve state-of-theart performance in several text classification and sentiment classification tasks. These techniques are able to learn distributed vector representations that capture semantic and syntactic relationships between words. Collobert and Weston (2008) trained jointly a single Convolutional Neural Network (CNN) architecture on different NLP tasks and showed that multitask learning increases the generalization of the shared tasks. Other researchers (Ian Goodfellow and Courville, 2016; Søgaard and Goldberg, 2016; Attia et al., 2016) have also reached to similar conclusions. 3 Dataset We experimented with two book collections: one prepared by Ganjigunte Ashok et al. (2013)3 and the other constructed by us to evaluate a new definition of success. We refer to the first dataset as EMNLP13 and the second dataset as Goodreads. The EMNLP13 collection contained Project Gutenberg books from eight different genres. The authors created a balanced dataset containing 100 books per genre, resulting in a total of 800 books. We manually reviewed the dataset and found missing or irrelevant content in 58 books: a tota"
E17-1114,W15-0707,0,0.322289,"Missing"
E17-1114,D13-1061,0,0.0857242,"Missing"
E17-2106,P14-1062,0,0.110174,"N-gram embeddings are fed to convolutional and max pooling layers, and the final classification is done via a softmax layer applied to the final text representation (_: whitespace in the input). As can be seen from Figure 1, we use a convolutional layer with different widths w, allowing us to capture patterns that involve everything from morphemes to words. We then pool the resulting feature maps f by max-over-time pooling (Collobert et al., 2011), to obtain yk , the maximum value of each feature map fk : tional approach to CNN that uses either a sequence of words or a sequence of characters (Kalchbrenner et al., 2014; Kim, 2014; Collobert et al., 2011; Zhang et al., 2015). This CNN captures local interactions at the character level, which are then aggregated to learn high-level patterns for modeling the style of an author. The main contributions of this paper are: yk = max fk [i], k = 1 . . . m i where m is the number of feature maps. This allows us to represent the text by its most important features, independent of their position. After pooling and concatenating the feature representations yk , we obtain a compact representation of the text. Finally, this representation is passed through a fully connect"
E17-2106,P15-1150,0,0.102148,"Missing"
E17-2106,D15-1167,0,0.0390136,"Missing"
E17-2106,N16-1082,0,0.0198297,"k by Bagnall (2015) uses a multi-headed Recurrent Neural Network (RNN) character language model that gives a set of next character probabilities for each author at every step of the model. This was the best-performing system for the PAN 2015 author identification task with a macro-averaged area under the curve (AUC) of 0.628 (Stamatatos et al., 2015). Despite the promising results that CNNs and RNNs show, the results are not interpretable and few of these works attempt to analyze what the networks are actually learning. We try to get an insight into our model by using the saliency analysis by Li et al. (2016). We have also devised our own method of finding out the input n-grams that are overall most important to the model. As a solution to the problem of AA of short texts, we propose a neural network architecture that is able to learn the representation of the text starting from the character sequence. Our architecture is a CNN that uses a sequence of character n-grams as input. This contrasts with the tradiWe present a model to perform authorship attribution of tweets using Convolutional Neural Networks (CNNs) over character n-grams. We also present a strategy that improves model interpretability"
E17-2106,D13-1193,0,0.232759,"ting Systems and Industrial Engineering Dept. Universidad Nacional de Colombia Bogotá, Colombia {ssierral, fagonzalezo}@unal.edu.co Manuel Montes-y-Gómez Thamar Solorio Instituto Nacional de Dept. of Computer Science Astrofısica, Optica y Electronica University of Houston Puebla, Mexico Houston, TX, 77004 mmontesg@ccc.inoep.mx solorio@cs.uh.edu Abstract ods that dealt with AA of short text. However, we were able to find research in AA using traditional as well as related approaches. Character and word n-grams have been used as the core of many authorship attribution systems (Stamatatos, 2009; Schwartz et al., 2013; Layton et al., 2010). Character and word n-grams help determine the author of a document by capturing the syntax and style of an author. Considering deep learning approaches, we found one other work that uses CNNs for authorship attribution (Rhodes, 2015). However, they use word representations for larger texts rather than character representation for short texts. Additionally, work by Bagnall (2015) uses a multi-headed Recurrent Neural Network (RNN) character language model that gives a set of next character probabilities for each author at every step of the model. This was the best-perform"
I11-1018,P10-2008,1,0.887995,"on of meta features uses these m different vectors to produce m clustering solutions for the training data with k clusters each. That means that we end up with different arrangements of the training instances into clusters, one arrangement per modality. Note that since clustering is performed per modality, k may be different in each clustering solution. From each cluster ck in each of the m clustering solutions, we compute a centroid by averaging all the feature vectors in that cluster. In another interesting recent work on AA, Probabilistic Context-Free Grammars (PCFGs) were proposed for AA (Raghavan et al., 2010). The number of authors in the evaluation data sets was rather small (3 to 6) but it included different domains, such as poetry, football, business, travel, and cricket, and all the data was harvested from the Internet. Raghavan et al. trained a PCFG for each author independently and authorship on the test data was assigned by taking the highest likelihood score from these grammars. To overcome the data sparseness problem, they mixed treebanked data from the Wall Street Journal (WSJ). They also enriched this mostly syntactic models with lexical information by combining the output with a bagof-"
I11-1018,de-marneffe-etal-2006-generating,0,0.0115943,"Missing"
L16-1541,W10-1908,0,0.032136,"First, we present our author profiling dataset for health support forums. Our corpus is the first of its kind for author profiling of medical forum data. Second, we motivate author profile analysis in this type of data. Third, we propose a system that can predict age and gender of health forum users and present benchmarking results for future research in this area. We also discuss interesting findings about the salient topics in the different population groups. 2. Related Work Medical forum data has been used in a variety of different research works since it is a comprehensive source of data. Jha and Elhadad (2010) have tried to predict the cancer stage of a patient by using the text in their posts and their online behavior. They formulated the problem as a multi-class classification problem with four cancer stages. They used unigrams and bigrams as their text based features and also 3394 explored the use of network features with the hypothesis that patients in similar stages of cancer will interact more with each other. A combination of these three features gave them the best results. Rolia et al. (2013) tried to make predictions about the condition a person is suffering from based on similarities of t"
L18-1274,P98-1013,0,0.206915,"for the basic four affective dimensions. We used this knowledge base to compute average polarity, attention, sensitivity, aptitude, and pleasantness for the synopses. We divide the plot synopses into three equal chunks based on words and extracted these two sentiment features for each chunk. We will discuss more about chunk-based sentiment representation later. Semantic Frames: Semantic role labeling is a useful technique to assign abstract roles to the arguments of predicates or verbs of sentences. We use SEMAFOR7 framesemantic parser to parse the frame-semantic structure using the FrameNet (Baker et al., 1998) frames. For each synopsis, we use the bag of frames representation weighted by normalized frequency as feature. Word Embeddings: Word embeddings have been shown effectiveness in text classification problems by capturing semantic information. Hence, in order to capture the semantic representation of the plots, we average the word vectors of every word in the plot. We use the publicly available FastText pre-trained word embeddings8 . Agent Verbs and Patient Verbs: Actions done and received by the characters can help to identify attributes of plots. For example, if the characters of a movie kill"
L18-1274,P13-1035,0,0.0647325,"Missing"
L18-1274,C16-1251,0,0.0707987,"Missing"
L18-1274,J90-1003,0,0.294283,"exploring more scalable approaches. 1736 Figure 4: Tracking flow of emotions in the synopses of six movies. Each synopsis was divided into equally sized 20 segments based on the words and percentage of the emotions for each segment were calculated using NRC emotion lexicons. The y axis represents the percentage of emotions in each segment; whereas, the x axis represents the segments. 3.2. Correlation between Tags 3.3. To find out significant correlations in the tagset, we compute the Positive Pointwise Mutual Information (PPMI) between the tags, which is a modification over the standard PMI (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). PPMI between two tags t1 and t2 is computed by the following equation: P P M I(t1; t2) ≡ max(log2 P (t1, t2) , 0) P (t1)P (t2) (3) where, P (t1, t2) is the probability of tags t1 and t2 occurring together and P (t1) and P (t2) are the probabilities of tag t1 and t2, respectively. Figure 3 shows the heatmap correlation of PPMI values between a subset of tags. The figure shows interesting relations between the tags and supports our understanding of the real world scenario. High PPMI scores show that cute, entertaining, dramatic, and sentimental movies"
L18-1274,P93-1022,0,0.226993,"approaches. 1736 Figure 4: Tracking flow of emotions in the synopses of six movies. Each synopsis was divided into equally sized 20 segments based on the words and percentage of the emotions for each segment were calculated using NRC emotion lexicons. The y axis represents the percentage of emotions in each segment; whereas, the x axis represents the segments. 3.2. Correlation between Tags 3.3. To find out significant correlations in the tagset, we compute the Positive Pointwise Mutual Information (PPMI) between the tags, which is a modification over the standard PMI (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). PPMI between two tags t1 and t2 is computed by the following equation: P P M I(t1; t2) ≡ max(log2 P (t1, t2) , 0) P (t1)P (t2) (3) where, P (t1, t2) is the probability of tags t1 and t2 occurring together and P (t1) and P (t2) are the probabilities of tag t1 and t2, respectively. Figure 3 shows the heatmap correlation of PPMI values between a subset of tags. The figure shows interesting relations between the tags and supports our understanding of the real world scenario. High PPMI scores show that cute, entertaining, dramatic, and sentimental movies can evoke feel-good"
L18-1274,N15-1113,0,0.135138,"gs (MPST) Corpus There are several datasets that provide plots or scripts of movies. Since their utilization in this work was difficult, we created a fine-grained tagset first and collected the synopses by ourselves. For example, MM-IMDb (Arevalo et al., 2017) provides plot summaries, posters, and metadata of ≈25K movies collected from IMDb. But these plot summaries are very short to capture different attributes of http://www.imdb.com https://www.movielens.org 3 1734 http://ritual.uh.edu/mpst-2018 movies (average words per summary is 92.5 versus 986.47 in MPST). Another example is ScriptBase (Gorinski and Lapata, 2015), which provides scripts of 1,276 movies collected from IMSDb4 . But plot synopses are more readily available than the scripts and that helped us to create a bigger dataset. Finally, CMU Movie summary corpus (Bamman et al., 2013) contains ≈42K plot synopses of movies collected from Wikipedia. Due to the absence of IMDb id for these movies, we could not retrieve the tag association information for the movies in that corpus. We created the corpus using MovieLens 20M dataset, Internet Movie Data Base (IMDb), and Wikipedia. To create a good corpus, we first defined some expected properties of the"
L18-1274,W11-1514,0,0.315961,"Grind-house, Christian, and non-fiction films do not usually have romantic elements. Romantic movies are usually cute and sentimental. Autobiographical movies usually have storytelling style and they are thought-provoking and philosophical. These relations, in fact, show that the movie tags within our corpus seem to portray a reasonable view of movie types based on our understanding of possible impressions from different types of movies. Emotion Flow in the Synopses NRC Emotion Lexicons (Mohammad and Turney, 2010) have been shown effective to capture the flow of emotions in narrative stories (Mohammad, 2011). It is a list of 14,182 words5 and their binary associations with eight types of elementary emotions from the Hourglass of Emotions model (Cambria et al., 2012) (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) with polarity. In Figure 4, we try to inspect how the flows of emotions look like in different types of plots. The reason behind this investigation is to get a shallow idea about the potential feasibility of the collected plot synopses to predict tags. As general users have written the collected plot synopses and created the tags for movies on the web, there is al"
L18-1274,C94-1049,0,0.119232,"gure 4: Tracking flow of emotions in the synopses of six movies. Each synopsis was divided into equally sized 20 segments based on the words and percentage of the emotions for each segment were calculated using NRC emotion lexicons. The y axis represents the percentage of emotions in each segment; whereas, the x axis represents the segments. 3.2. Correlation between Tags 3.3. To find out significant correlations in the tagset, we compute the Positive Pointwise Mutual Information (PPMI) between the tags, which is a modification over the standard PMI (Church and Hanks, 1990; Dagan et al., 1993; Niwa and Nitta, 1994). PPMI between two tags t1 and t2 is computed by the following equation: P P M I(t1; t2) ≡ max(log2 P (t1, t2) , 0) P (t1)P (t2) (3) where, P (t1, t2) is the probability of tags t1 and t2 occurring together and P (t1) and P (t2) are the probabilities of tag t1 and t2, respectively. Figure 3 shows the heatmap correlation of PPMI values between a subset of tags. The figure shows interesting relations between the tags and supports our understanding of the real world scenario. High PPMI scores show that cute, entertaining, dramatic, and sentimental movies can evoke feel-good mood, whereas lower PP"
L18-1274,W10-0204,0,0.0685551,"g, historical, and home movies. Christian films and science fictions are also good sources of inspiration. Grind-house, Christian, and non-fiction films do not usually have romantic elements. Romantic movies are usually cute and sentimental. Autobiographical movies usually have storytelling style and they are thought-provoking and philosophical. These relations, in fact, show that the movie tags within our corpus seem to portray a reasonable view of movie types based on our understanding of possible impressions from different types of movies. Emotion Flow in the Synopses NRC Emotion Lexicons (Mohammad and Turney, 2010) have been shown effective to capture the flow of emotions in narrative stories (Mohammad, 2011). It is a list of 14,182 words5 and their binary associations with eight types of elementary emotions from the Hourglass of Emotions model (Cambria et al., 2012) (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) with polarity. In Figure 4, we try to inspect how the flows of emotions look like in different types of plots. The reason behind this investigation is to get a shallow idea about the potential feasibility of the collected plot synopses to predict tags. As general users"
N09-1006,W07-1001,0,0.127858,"both omit and substitute articles and clitics, while the dominant errors for Italian-speakers are omissions. 3 Our Approach We use language models (LMs) in our initial investigation, and later explore more complex ML algorithms to improve the results. Our ultimate goal is to discover a highly accurate ML method that can be used to assist clinicians in the task of LI identification in children. 3.1 Language Models for Predicting Language Impairment LMs are statistical models used to estimate the probability of a given sequence of words. They have been explored previously for clinical purposes. Roark et al. (2007) proposed cross entropy of LMs trained on Part-of-Speech (POS) sequences as a measure of syntactic complexity with the aim of determining mild cognitive impairment in adults. Solorio and Liu (2008) evaluated LMs on a small data set in a preliminary trial on LI prediction. The intuition behind using LMs is that they can identify atypical grammatical patterns and help discriminate the population with potential LI from the Typically Developing (TD) one. We use LMs trained on POS tags rather than on words. Using POS tags can address the data sparsity issue in LMs, and place less emphasis on the vo"
N09-1006,W08-0626,1,0.845656,"xplore more complex ML algorithms to improve the results. Our ultimate goal is to discover a highly accurate ML method that can be used to assist clinicians in the task of LI identification in children. 3.1 Language Models for Predicting Language Impairment LMs are statistical models used to estimate the probability of a given sequence of words. They have been explored previously for clinical purposes. Roark et al. (2007) proposed cross entropy of LMs trained on Part-of-Speech (POS) sequences as a measure of syntactic complexity with the aim of determining mild cognitive impairment in adults. Solorio and Liu (2008) evaluated LMs on a small data set in a preliminary trial on LI prediction. The intuition behind using LMs is that they can identify atypical grammatical patterns and help discriminate the population with potential LI from the Typically Developing (TD) one. We use LMs trained on POS tags rather than on words. Using POS tags can address the data sparsity issue in LMs, and place less emphasis on the vocabulary and more emphasis on the syntactic patterns. We trained two separate LMs using POS tags from the transcripts of TD and LI children, respectively. The language status of a child is predicte"
N15-1010,P11-1030,1,0.796217,"le-domain corpus, where there is only one topic that all authors are writing about, and a multi-domain corpus, where there are multiple different topics. The latter allows us to test the generalization of AA models, by testing them on a different topic from that used for training. The first collection is the CCAT topic class, a subset of the Reuters Corpus Volume 1 (Lewis et al., 2004). Although this collection was not gathered for the goal of doing authorship attribution studies, previous work has reported results for AA with 10 and 50 authors (Stamatatos, 2008; Plakias and Stamatatos, 2008; Escalante et al., 2011). We refer to these as CCAT 10 and CCAT 50, respectively. Both CCAT 10 and CCAT 50 belong to CCAT category (about corporate/industrial news) and are balanced across authors, with 100 documents sampled for each author. Manual inspection of the dataset revealed that some of the authors in this collection consistently used signatures at the end of documents. Also, we noticed some writers use quotations a lot. ConCCAT 10 CCAT 50 Guardian1 Guardian2 10 50 13 13 SC Category prefix suffix space-prefix space-suffix whole-word mid-word multi-word beg-punct mid-punct end-punct #docs #sentences #words /a"
N15-1010,W14-0908,0,0.265256,"Missing"
N15-1010,C08-1065,0,0.338084,"Missing"
N15-1010,J00-4001,0,0.172465,"dra,bethard}@cis.uab.edu Manuel Montes-y-G´omez Instituto Nacional de Astrof´ısica Optica y Electr´onica Puebla, Mexico mmontesg@ccc.inaoep.mx Thamar Solorio University of Houston 4800 Calhoun Rd Houston, TX, 77004, USA solorio@cs.uh.edu Abstract tential candidate authors have an important effect on the accuracy of AA approaches (Moore, 2001; Luyckx and Daelemans, 2008; Luyckx and Daelemans, 2010). We can also point out the most common features that have been used successfully in AA work, including: bag-of-words (Madigan et al., 2005; Stamatatos, 2006), stylistic features (Zheng et al., 2006; Stamatatos et al., 2000), and word and character level n-grams (Kjell et al., 1994; Keselj et al., 2003; Peng et al., 2003; Juola, 2006). Character n-grams have been identified as the most successful feature in both singledomain and cross-domain Authorship Attribution (AA), but the reasons for their discriminative value were not fully understood. We identify subgroups of character n-grams that correspond to linguistic aspects commonly claimed to be covered by these features: morphosyntax, thematic content and style. We evaluate the predictiveness of each of these groups in two AA settings: a single domain setting and"
N18-1110,J92-4003,0,0.0612878,"epresent entire phrases or documents (e.g., the most common strategy is to average the term vectors in documents). The proposed method is based on creating meta1217 words to represent documents. Clustering words into meaningful groups based on some measure of similarity to represent text is not a new concept. One of the classic approaches is term clustering in an unsupervised manner that was first investigated by (Lewis, 1992). He called his method reciprocal nearest neighbor clustering. His method consists of joining words that are similar according to a measure of similarity. In other work, Brown et al. (1992) explored the idea of discovering similarities between words to obtain clusters at different levels. One key difference with our proposal is that in (Brown et al., 1992), terms are deterministically/probabilistically associated with a discrete class, where terms that are in the same class are similar in some aspect. However in our proposed strategy, we exploit word vectors instead of a discrete random deterministic variable (e.g., soft/hard partitions of word sets). This makes possible to discover different clusters and meta-words if we change the word representation. Thus, the proposed strate"
N18-1110,W13-1607,1,0.881268,"Missing"
N18-1110,W16-0416,1,0.823822,"Missing"
N18-1127,W17-4419,1,0.939568,"LL 2003 dataset (Tjong Kim Sang and De Meulder, 2003). Although most of the work has focused on formal datasets, similar approaches have been evaluated on SM domains (Strauss et al., 2016; Derczynski et al., 2017). In the Workshop on Noisy User-generated Text (WNUT) 2016, Limsopatham and Collier (2016), the winners of the NER shared task, used a BLSTM-CRF model that induced features from an orthographic representation of the text. Later, in the WNUT 2017 shared task, the best performing system used a multitask network that transferred the learning to a CRF classifier for the final prediction (Aguilar et al., 2017). In this work we focus on addressing the challenges of the NER task found in social media environments. We propose that what is traditionally categorized as noise (i.e., misspellings, inconsistent orthography, emerging abbreviations, and slang) should be modeled as is since it is an inherent characteristic of SM text. Specifically, the proposed models attempt to address i) misspellings using subword level representations, ii) grammatical mistakes with SM-oriented Part-ofSpeech tags (Owoputi et al., 2013), iii) sounddriven text with phonetic and phonological features (Bharadwaj et al., 2016),"
N18-1127,W15-4319,0,0.0446098,"Missing"
N18-1127,W03-0420,0,0.30652,": person, corporation, and location. These entity types were originally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable"
N18-1127,D16-1153,0,0.257455,"on (Aguilar et al., 2017). In this work we focus on addressing the challenges of the NER task found in social media environments. We propose that what is traditionally categorized as noise (i.e., misspellings, inconsistent orthography, emerging abbreviations, and slang) should be modeled as is since it is an inherent characteristic of SM text. Specifically, the proposed models attempt to address i) misspellings using subword level representations, ii) grammatical mistakes with SM-oriented Part-ofSpeech tags (Owoputi et al., 2013), iii) sounddriven text with phonetic and phonological features (Bharadwaj et al., 2016), and iv) the intrinsic skewness of NER datasets by applying class weights. It is worth noting that our models do not rely on capitalization or any external resources such as gazetteers. The reasons are that capitalization is arbitrarily used on SM environments, and gazetteers are expensive resources to develop for a scenario where novel entities constantly and rapidly emerge (Derczynski et al., 2017; Augenstein et al., 2017). Based on our experiments, we have seen that a multitask variation of the proposed networks improves the results over a single-task network. Additionally, this multitask"
N18-1127,M98-1018,0,0.410802,"e word Defense because both words map to the same IPA sequence, /dIfEns/. In the third case, the model is not able to identify the NE Scout, even though the context makes it fairly easy. 6 Related work In its former years, NER systems focused on newswire text, where the goal was to identify mainly three types of entities: person, corporation, and location. These entity types were originally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugment"
N18-1127,W03-0424,0,0.118237,"entity types were originally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the"
N18-1127,W17-4418,0,0.107633,"Missing"
N18-1127,W15-4307,0,0.0478102,"Missing"
N18-1127,P15-1033,0,0.0198079,"f the network. It also uses multitask learning on the output layer. Figure 3: This is a stacked model that uses a network as feature extractor, and then it transfers the learning to a CRF classifier. The network uses multitask learning to capture the features. This vector not only encodes the phonetic and phonological features, but it also captures some morphological patterns at the character level based on the IPA representations. Then, we concatenate this vector with the word and POS tag representations: a = [xt ; pt ; ht ]. We feed this representation to another bidirectional LSTM network (Dyer et al., 2015), similar to the BLSTM described for the character level. The bidirectional LSTM generates a word-level representation that accounts for the context in the sentence using semantics, syntax, phonetics and phonological aspects. We feed this representation to a fully-connected layer: where yi represents the ith label of the xi token in the sentence. Next, we calculate the conditional probability of seeing y given the extracted features z from the network and the weights W associated to the labels: ri = BLSTM({a1 , a2 , ...an }) zi = ReLU(Wa ri + b) (1) (2) At this point, both models share the sam"
N18-1127,W03-0423,0,0.0585014,", and location. These entity types were originally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet"
N18-1127,P05-1045,0,0.223743,"Missing"
N18-1127,W03-0425,0,0.479402,"nally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the expensive detailed rul"
N18-1127,P02-1022,0,0.35172,"Missing"
N18-1127,W15-4322,0,0.0520748,"Missing"
N18-1127,C96-1079,0,0.272803,"OV words using the FastText library helped in this case. Also, from the phonetic perspective, the model treated the word Defence as if it was the word Defense because both words map to the same IPA sequence, /dIfEns/. In the third case, the model is not able to identify the NE Scout, even though the context makes it fairly easy. 6 Related work In its former years, NER systems focused on newswire text, where the goal was to identify mainly three types of entities: person, corporation, and location. These entity types were originally proposed in the 6th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Kle"
N18-1127,W03-0428,0,0.102186,"th Message Understanding Conference (MUC-6) (Grishman and Sundheim, 1996b). In MUC-7, the majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the expensive detailed rules were not; their sys"
N18-1127,N16-1030,0,0.32708,"Missing"
N18-1127,W16-3920,0,0.0124411,"le SM domains, such as Twitter, YouTube, Reddit, and StackExchange. moving the dependencies on external resources. Moreover, Ma and Hovy (2016) proposed an end-to-end BLSTM-CNN-CRF network, whose loss function is based on the maximum loglikelihood estimation of the CRF. These architectures were benchmarked on the standard CoNLL 2003 dataset (Tjong Kim Sang and De Meulder, 2003). Although most of the work has focused on formal datasets, similar approaches have been evaluated on SM domains (Strauss et al., 2016; Derczynski et al., 2017). In the Workshop on Noisy User-generated Text (WNUT) 2016, Limsopatham and Collier (2016), the winners of the NER shared task, used a BLSTM-CRF model that induced features from an orthographic representation of the text. Later, in the WNUT 2017 shared task, the best performing system used a multitask network that transferred the learning to a CRF classifier for the final prediction (Aguilar et al., 2017). In this work we focus on addressing the challenges of the NER task found in social media environments. We propose that what is traditionally categorized as noise (i.e., misspellings, inconsistent orthography, emerging abbreviations, and slang) should be modeled as is since it is"
N18-1127,P16-1101,0,0.21014,"Missing"
N18-1127,W03-0429,0,0.105973,"majority of the systems were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the expensive detailed rules were not; their systems were difficult to maintain and adapt to other domains where different rules were neede"
N18-1127,W03-0430,0,0.216641,"res and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the expensive detailed rules were not; their systems were difficult to maintain and adapt to other domains where different rules were needed. 1408 Recently, NER has been focused on noisy data as a result o"
N18-1127,C16-1328,0,0.0213637,"presentation. Even though the spellings of both phrases are significantly different, by using the phonological (articulatory) aspects of those phrases it is possible to map them to the same phonetic representation. In other words, our assumption is that social media writers heavily rely on the way that words sound while they write. We use the Epitran1 library (Bharadwaj et al., 2016), which transliterates graphemes to phonemes with the International Phonetic Alphabet (IPA). In addition to the IPA phonemes, we also use the phonological (articulatory) features generated by the PanPhon2 library (Mortensen et al., 2016). These features provide articulatory information such as the way the mouth and nasal areas are involved in the elaboration of sounds while people speak. 2.2 Models We have experimented with two models. In the first one, we use an end-to-end BLSTM-CRF network with a multitask output layer comprised of one CRF per task, similar to Yang et al. (2016). In the second one, we define a stacked model that is based on two phases: i) a multitask neural network and ii) a CRF classifier. In the first phase, the network acts as a feature extractor, and then, 1 2 for the second phase, it transfers the lear"
N18-1127,N13-1039,0,0.0607876,"Missing"
N18-1127,P16-2025,0,0.0441261,"Missing"
N18-1127,D11-1141,0,0.585559,"Missing"
N18-1127,W16-3919,0,0.0690135,"Missing"
N18-1127,W15-4321,0,0.054549,"Missing"
N18-1127,W03-0432,0,0.0891745,"s were based on heavily hand-crafted features and manually elaborated rules (Borthwick et al., 1998). Some years later, many researchers incorporated machine learning algorithms to their systems, but there was still a strong dependency on external resources and domain-specific features and rules (Tjong Kim Sang and De Meulder, 2003). In addition, the majority of the systems used Maximum Entropy (Bender et al., 2003; Chieu and Ng, 2003b; Curran and Clark, 2003; Florian et al., 2003b; Klein et al., 2003) and Hidden Markov Models (Florian et al., 2003b; Klein et al., 2003; Mayfield et al., 2003; Whitelaw and Patrick, 2003). Furthermore, McCallum and Li (2003) used a CRF combined with webaugmented lexicons. The features were selected by hand-crafted rules and refined based on their relevance to the domain of the entities. Moreover, Nothman et al. (2013) used Wikipedia resources to take advantage of structured data and reduce the human-annotated labels. In general, the results of the systems were reasonable for formal text, yet the scalability and the expensive detailed rules were not; their systems were difficult to maintain and adapt to other domains where different rules were needed. 1408 Recently, NER has bee"
N18-1127,W03-0419,0,\N,Missing
N18-2042,D13-1181,0,0.117845,"hange in trust, fear, and sadness, which relates to the main character’s getting into and out of trouble. These patterns present the emotional arcs of the story. Even though they do not reveal the actual plot, they indicate major events happening in the story. In this paper, we hypothesize that readers enjoy emotional rhythm and thus modeling emotion flows will help predicting a book’s potential success. In addition, we show that using the entire content of the book yields better results. Considering only a fragment, as done in earlier work that focuses mainly on style (Maharjan et al., 2017; Ashok et al., 2013), disregards important emotional changes. Similar to Maharjan et al. (2017), we also find that adding genre as an auxiliary task improves success prediction. Books have the power to evoke a multitude of emotions in their readers. They can make readers laugh at a comic scene, cry at a tragic scene and even feel pity or hate for the characters. Specific patterns of emotion flow within books can compel the reader to finish the book, and possibly pursue similar books in the future. Like a musical arrangement, the right emotional rhythm can arouse readers, but even a slight variation in the composi"
N18-2042,D15-1166,0,0.0241132,"recurrent The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ emotion flow 259 Proceedings of NAACL-HLT 2018, pages 259–265 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics neural network (RNN) to model the sequential flow of emotions. We aggregate the encoded sequences into a single book vector using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This final vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ∼14K words (Version 0.92) and their binary associations with eight types of elementary emotions (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) from the Hourglass of emotions model with polarity (positive and negative) (Mohammad and Turney, 2013, 2010). These lexicons have been shown to be effective in tracking emotions in literary texts (Mohammad, 2011). Figure 2: Multitask Emotion Flow Model. rize the contextual emotion flow informa"
N18-2042,P16-1223,0,0.0731501,"Missing"
N18-2042,E17-1114,1,0.557289,"Missing"
N18-2042,W11-1514,0,0.457387,"a musical arrangement, the right emotional rhythm can arouse readers, but even a slight variation in the composition might turn them away. Vonnegut (1981) discussed the potential of plotting emotions in stories on the “Beginning-End” and the “Ill Fortune-Great Fortune” axes. Reagan et al. (2016) used mathematical tools like Singular Value Decomposition, agglomerative clustering, and Self Organizing Maps (Kohonen et al., 2001) to generate basic shapes of stories. They found that stories are dominated by six different shapes. They even correlated these different shapes to the success of books. Mohammad (2011) visualized emotion densities across books of different genres. He found that the progression of emotions varies with the genre. For example, there is a stronger progression into darkness in horror stories than in comedy. Likewise, Kar et al. (2018) 2 Methodology We extract emotion vectors from different chunks of a book and feed them to a recurrent The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ emotion flow 259 Proceedings of NAACL-HLT 2018, pages 259–265 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics n"
N18-2042,W10-0204,0,0.102482,"Missing"
N18-2042,L18-1274,1,0.79213,"Missing"
N18-2042,D15-1044,0,0.0345315,"and feed them to a recurrent The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ emotion flow 259 Proceedings of NAACL-HLT 2018, pages 259–265 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics neural network (RNN) to model the sequential flow of emotions. We aggregate the encoded sequences into a single book vector using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This final vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ∼14K words (Version 0.92) and their binary associations with eight types of elementary emotions (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) from the Hourglass of emotions model with polarity (positive and negative) (Mohammad and Turney, 2013, 2010). These lexicons have been shown to be effective in tracking emotions in literary texts (Mohammad, 2011). Figure 2: Multitask Emotion Flow Model. rize the contextual"
N18-2042,D16-1058,0,0.0422732,"2018) 2 Methodology We extract emotion vectors from different chunks of a book and feed them to a recurrent The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ emotion flow 259 Proceedings of NAACL-HLT 2018, pages 259–265 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics neural network (RNN) to model the sequential flow of emotions. We aggregate the encoded sequences into a single book vector using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This final vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ∼14K words (Version 0.92) and their binary associations with eight types of elementary emotions (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) from the Hourglass of emotions model with polarity (positive and negative) (Mohammad and Turney, 2013, 2010). These lexicons have been shown to be effective in tracking emotions in literary text"
N18-2042,N16-1174,0,0.0616805,"We extract emotion vectors from different chunks of a book and feed them to a recurrent The source code and data for this paper can be downloaded from https://github.com/sjmaharjan/ emotion flow 259 Proceedings of NAACL-HLT 2018, pages 259–265 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics neural network (RNN) to model the sequential flow of emotions. We aggregate the encoded sequences into a single book vector using an attention mechanism. Attention models have been successfully used in various Natural Language Processing tasks (Wang et al., 2016; Yang et al., 2016; Hermann et al., 2015; Chen et al., 2016; Rush et al., 2015; Luong et al., 2015). This final vector, which is emotionally aware, is used for success prediction. Representation of Emotions: NRC Emotion Lexicons provide ∼14K words (Version 0.92) and their binary associations with eight types of elementary emotions (anger, anticipation, joy, trust, disgust, sadness, surprise, and fear) from the Hourglass of emotions model with polarity (positive and negative) (Mohammad and Turney, 2013, 2010). These lexicons have been shown to be effective in tracking emotions in literary texts (Mohammad, 2011)."
N18-2042,baccianella-etal-2010-sentiwordnet,0,\N,Missing
P05-2005,carreras-padro-2002-flexible,0,0.0640376,"Missing"
P05-2005,E03-1038,0,0.0363963,"Missing"
P05-2005,W03-0421,0,0.0643892,"Missing"
P05-2005,W02-2010,0,0.0142636,"on we use is automatically extracted from the documents, without human intervention. Moreover, the method performs well even without the use of the hand coded system. The experimental results are very encouraging. Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese. 1 Introduction Given the usefulness of Named Entities (NEs) in many natural language processing tasks, there has been a lot of work aimed at developing accurate named entity extractors (Borthwick, 1999; Velardi et al., 2001; Ar´evalo et al., 2002; Zhou and Su, 2002; Florian, 2002; Zhang and Johnson, 2003). Most approaches however, have very low portability, they are designed to perform well over a particular collection or type of document, and their accuracies will drop considerably when used in different domains. The reason for this is that many NE extractor systems rely heavily on complex linguistic resources, which are typically hand coded, for example regular expressions, grammars, gazetteers and the like. Our goal is to present a method that will facilitate the task of increasing the coverage of named entity extractor systems. In this setting, we assume that we h"
P05-2005,P02-1060,0,0.247452,"The only information we use is automatically extracted from the documents, without human intervention. Moreover, the method performs well even without the use of the hand coded system. The experimental results are very encouraging. Our approach even outperformed the hand coded system on NER in Spanish, and it achieved high accuracies in Portuguese. 1 Introduction Given the usefulness of Named Entities (NEs) in many natural language processing tasks, there has been a lot of work aimed at developing accurate named entity extractors (Borthwick, 1999; Velardi et al., 2001; Ar´evalo et al., 2002; Zhou and Su, 2002; Florian, 2002; Zhang and Johnson, 2003). Most approaches however, have very low portability, they are designed to perform well over a particular collection or type of document, and their accuracies will drop considerably when used in different domains. The reason for this is that many NE extractor systems rely heavily on complex linguistic resources, which are typically hand coded, for example regular expressions, grammars, gazetteers and the like. Our goal is to present a method that will facilitate the task of increasing the coverage of named entity extractor systems. In this setting, we a"
P05-2005,W03-0434,0,\N,Missing
P05-2005,W02-1111,0,\N,Missing
P05-2005,W03-2201,0,\N,Missing
P05-2005,A97-1029,0,\N,Missing
P16-1210,W06-1615,0,0.571498,"io University of Houston solorio@cs.uh.edu Manuel Montes-y-G´omez Instituto Nacional de Astrof´ısica, Optica y Electr´onica mmontesg@inaoep.mx Steven Bethard University of Alabama at Birmingham bethard@uab.edu Abstract One of the scenarios that has received limited attention is cross-domain authorship attribution, when we need to identify the author of a text but all the text with known authors is from a different topic, genre, or modality. Here we propose to solve the problem of cross-domain authorship attribution by adapting the Structural Correspondence Learning (SCL) algorithm proposed by Blitzer et al. (2006). We make the following contributions: We present the first domain adaptation model for authorship attribution to leverage unlabeled data. The model includes extensions to structural correspondence learning needed to make it appropriate for the task. For example, we propose a median-based classification instead of the standard binary classification used in previous work. Our results show that punctuation-based character n-grams form excellent pivot features. We also show how singular value decomposition plays a critical role in achieving domain adaptation, and that replacing (instead of concat"
P16-1210,P07-1033,0,0.194974,"Missing"
P16-1210,P11-1030,1,0.872381,"he standard SCL formulation. The extensions to SCL that we propose in this work are likely to yield performance improvements in other tasks where SCL has been successfully applied, such as part-of-speech tagging and sentiment analysis. We plan to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution sho"
P16-1210,E09-1039,0,0.210101,"pplied, such as part-of-speech tagging and sentiment analysis. We plan to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution should be done with care. However, the study did not attempt to construct authorship attribution models where the source and target domains differ. Goldstein-Stewart et al. (2009) per"
P16-1210,C08-1065,0,0.612836,"Missing"
P16-1210,C14-1116,1,0.326485,"to investigate this further in the future. 2 Related Work Cross-Domain Authorship Attribution Almost all previous authorship attribution studies have tackled traditional (single-domain) authorship problems where the distribution of the test data is the same as that of the training data (Madigan et al., 2005; Stamatatos, 2006; Luyckx and Daelemans, 2008; Escalante et al., 2011). However, there are a handful of authorship attribution studies that explore cross-domain authorship attribution scenarios (Mikros and Argiri, 2007; Goldstein-Stewart et al., 2009; Schein et al., 2010; Stamatatos, 2013; Sapkota et al., 2014). Here, following prior work, cross-domain is a cover term for cross-topic, cross-genre, cross-modality, etc., though most work focuses on the cross-topic scenario. Mikros and Argiri (2007) illustrated that many stylometric variables are actually discriminating topic rather than author. Therefore, the authors suggest their use in authorship attribution should be done with care. However, the study did not attempt to construct authorship attribution models where the source and target domains differ. Goldstein-Stewart et al. (2009) performed a study on cross-topic authorship attribution by concat"
P16-1210,N15-1010,1,0.371067,"-grams used in authorship attribution that might make good pivot features. 3.2.1 Untyped Character N -grams Classical character n-grams are simply the sequences of characters in the text. For example, given the text: We propose to use as pivot features the p most frequent character n-grams. For non-pivot features, we use the remaining features from prior work (Sapkota et al., 2014). These include both the remaining (lower frequency) character n-grams, as well as stop-words and bag-of-words lexical features. We call this the untyped formulation of pivot features. 3.2.2 Typed Character N -grams Sapkota et al. (2015) showed that classical character n-grams lose some information in merging together instances of n-grams like the which could be a prefix (thesis), a suffix (breathe), or a standalone word (the). Therefore, untyped character n-grams were separated into ten distinct categories. Four of the ten categories are related to affixes: prefix, suffix, space-prefix, and space-suffix. Three are wordrelated: whole-word, mid-word, and multi-word. The final three are related to the use of punctuation: beg-punct, mid-punct, and end-punct. For example, the character n-grams from the last section would instead"
P16-1210,D07-1129,0,0.0627668,"Missing"
P16-1210,N09-2046,0,0.0227359,", the assumption is that there will still be some general features that share similar characteristics in both domains. SCL has been applied to tasks such as sentiment analysis, dependency parsing, and partof-speech tagging, but has not yet been explored for the problem of authorship attribution. The common feature representation in SCL is created by learning a projection to “pivot” features from all other features. These pivot features are a critical component of the successful use of SCL, and their selection is something that has to be done carefully and specifically to the task at the hand. Tan and Cheng (2009) studied sentiment analysis, using frequently occurring sentiment words as pivot features. Similarly, Zhang et al. (2010) proposed a simple and efficient method for selecting pivot features in domain adaptive sentiment analysis: choose the frequently occurring words or wordbigrams among domains computed after applying some selection criterion. In dependency parsing, Shimizu and Nakagawa (2007) chose the presence of a preposition, a determiner, or a helping verb between two tokens as the pivot features. For partof-speech tagging, Blitzer et al. (2006) used words that occur more than 50 times in"
P19-1112,S17-2091,0,0.0190702,"one label sequence is correct and should be considered as ground truth. This is contrary to the ambiguous nature of our task, where different interpretations are possible. Our solution is to utilize label distribution learning (Subsection 3.2). LDL methods have been used before to solve various visual recognition problems such as facial age prediction (Rondeau and Alvarez, 2018; Gao et al., 2017). We are the first to introduce LDL for sequence labeling. 2 3 Related Work A large amount of work in NLP addresses finding keywords or key-phrases in long texts from scientific articles, news, etc. (Augenstein et al., 2017; Zhang et al., 2016). Keyword detection mainly focuses on finding important nouns or noun phrases. In contrast, social media text is much shorter, and users tend to emphasize a subset of words with different roles to convey specific intent. Emphasis words are not necessarily the words with the highest or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in exp"
P19-1112,N18-1202,0,0.0286867,"ps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better train and to force the network finding different activation paths, we use two dropout layers with a rate of 0.5 in the sequence and inference layers. Finetuning is performed for 160 epochs, and the reported test result corresponds to the best accuracy obtained on the validation set. 3 The implementation is available online: https:// github.com/RiTUAL-UH/emphasis-2019 1169 Model/Evals m="
P19-1112,D11-1143,0,0.110937,"Missing"
P19-1112,D16-1080,0,0.0603252,"rrect and should be considered as ground truth. This is contrary to the ambiguous nature of our task, where different interpretations are possible. Our solution is to utilize label distribution learning (Subsection 3.2). LDL methods have been used before to solve various visual recognition problems such as facial age prediction (Rondeau and Alvarez, 2018; Gao et al., 2017). We are the first to introduce LDL for sequence labeling. 2 3 Related Work A large amount of work in NLP addresses finding keywords or key-phrases in long texts from scientific articles, news, etc. (Augenstein et al., 2017; Zhang et al., 2016). Keyword detection mainly focuses on finding important nouns or noun phrases. In contrast, social media text is much shorter, and users tend to emphasize a subset of words with different roles to convey specific intent. Emphasis words are not necessarily the words with the highest or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in expressive prosody gener"
P19-1112,D17-1004,0,0.0136072,"ed results. 3 KL-Divergence Loss During the training phase, the Kullback-Leibler Divergence (KLDIV) (Kullback and Leibler, 1951) is used as the loss function. KL-DIV is a measure of how one probability distribution P is different from a second reference probability distribution Q: KL-DIV(P ||Q) = X x∈X 5 5.1 w1 Input both forward and backward directions. Having two BiLSTM layers helps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better tra"
P19-1112,Y14-1022,0,0.250052,"or lowest frequency in the text. Often a high sentiment adjective can be emphasized, such as Hot in Hot Summer. Generally, word emphasis may express emotions, show contrast, capture a reader’s interest or clarify a message. In a different context, modeling word emphasis has been addressed in expressive prosody generation. Most studies detect emphasis words based on acoustic and prosodic features that exist in spoken data (Mishra et al., 2012; Chen and Pan, 2017). More recently, few works model emphasis from text to improve expressive prosody generation in modern Text-To-Speech (TTS) systems (Nakajima et al., 2014; Mass et al., 2018). For example, (Mass et al., 2018) trained a deep neural network model on audience-addressed speeches to predict word emphasis. The dataset consists of relatively long paragraphs which are labeled by four annotators based on words that clearly stand out in a recorded speech. Many approaches have been proposed to deal with annotations coming from multiple annota3.1 Emphasis Selection Task Definition Given a sequence of words or tokens C = {x1 , ..., xn }, we want to determine the subset S of words in C that are good candidates to emphasize, where 1 ≤ |S |≤ n. 3.2 Label Distr"
P19-1112,D14-1162,0,0.082989,"d and backward directions. Having two BiLSTM layers helps to build a deeper feature extractor; having more than two does not help the performance as the model becomes too complicated. We investigate the impact of attention mechanisms to the model (Vinyals et al., 2015; Zhang et al., 2017), where attention weights ai represent the relative contribution of a specific word to the text representation. We compute ai at each output time i as follows: P (x) log Q(x) P (x) Experimental Settings and Results Training Details We use two different word representations: pretrained 100-dim GloVe embedding (Pennington et al., 2014), and 2048-dim ELMo embedding (Peters et al., 2018). We use BiLSTM layers with hidden size of 512 and 2048 when using GloVe and ELMo embeddings respectively. We use the Adam optimizer (Kingma and Ba, 2014) with the learning rate set to 0.001. In order to better train and to force the network finding different activation paths, we use two dropout layers with a rate of 0.5 in the sequence and inference layers. Finetuning is performed for 160 epochs, and the reported test result corresponds to the best accuracy obtained on the validation set. 3 The implementation is available online: https:// git"
R19-1080,N09-1003,0,0.0839805,"Missing"
R19-1080,Q13-1028,0,0.0808808,"Missing"
R19-1080,W13-3512,0,0.0912005,"Missing"
R19-1080,D13-1181,0,0.172961,"2009; Stamatatos, 2009; Sapkota et al., 2015). Rather than using plain character-based ngrams, we first annotate them with their functional roles (prefix, suffix, and whole-word). This is necessary since, for example, off is semantically distinct from when it is used as a whole word and when it is used as a suffix (e.g. trade-off ) or when it is used as a prefix (e.g. offend). Introduction Literary texts have been computationally modelled by extracting stylistic traits such as readability and writing density, flow of emotions, and even by cover images of books (Maharjan et al., 2018b,a, 2017; Ashok et al., 2013). However, modelling of authors through their work has not been explored until now. An author’s style of presenting stories has a great influence on whether a book will be liked by readers or not. We can find evidence of the effect that an author’s style has on readers in book reviews and through readers’ comments left on Goodreads1 shown in Table 1. The readers talk about the impact of the author’s writing style on their reading experience. In the first two examples, it left a positive impact on the readers, while in the last it had a negative impact. These examples provide further evidence f"
R19-1080,E17-1114,1,0.898665,"Missing"
R19-1080,Q17-1010,0,0.134807,"Missing"
R19-1080,P15-1010,0,0.0550909,"Missing"
R19-1080,N18-2042,1,0.836202,"Missing"
R19-1080,D18-1375,1,0.755395,"Missing"
R19-1080,D14-1181,0,0.00592844,"Missing"
R19-1080,E03-1053,0,0.106027,"d authorship attribution. 1 Table 1: Readers comments showing the importance of authors’ writing style writing style for the task of likability prediction of books. In this paper we propose a new approach to capture style in text by jointly learning author specific embeddings and character based n-gram embeddings. The idea of using author embeddings is motivated by reader comments as discussed above. The use of character n-gram embeddings comes from previous work on authorship attribution (AA) that has shown character n-grams to have strong prediction value for the task (Keˇselj et al., 2003; Peng et al., 2003; Koppel et al., 2009; Stamatatos, 2009; Sapkota et al., 2015). Rather than using plain character-based ngrams, we first annotate them with their functional roles (prefix, suffix, and whole-word). This is necessary since, for example, off is semantically distinct from when it is used as a whole word and when it is used as a suffix (e.g. trade-off ) or when it is used as a prefix (e.g. offend). Introduction Literary texts have been computationally modelled by extracting stylistic traits such as readability and writing density, flow of emotions, and even by cover images of books (Maharjan et al."
R19-1080,D17-1023,0,0.0598763,"Missing"
R19-1080,W16-5806,1,0.901779,"Missing"
R19-1080,N15-1010,1,0.920427,"ing the importance of authors’ writing style writing style for the task of likability prediction of books. In this paper we propose a new approach to capture style in text by jointly learning author specific embeddings and character based n-gram embeddings. The idea of using author embeddings is motivated by reader comments as discussed above. The use of character n-gram embeddings comes from previous work on authorship attribution (AA) that has shown character n-grams to have strong prediction value for the task (Keˇselj et al., 2003; Peng et al., 2003; Koppel et al., 2009; Stamatatos, 2009; Sapkota et al., 2015). Rather than using plain character-based ngrams, we first annotate them with their functional roles (prefix, suffix, and whole-word). This is necessary since, for example, off is semantically distinct from when it is used as a whole word and when it is used as a suffix (e.g. trade-off ) or when it is used as a prefix (e.g. offend). Introduction Literary texts have been computationally modelled by extracting stylistic traits such as readability and writing density, flow of emotions, and even by cover images of books (Maharjan et al., 2018b,a, 2017; Ashok et al., 2013). However, modelling of au"
R19-1080,E17-2106,1,0.896755,"Missing"
R19-1080,E17-2116,0,0.0621954,"Missing"
R19-1080,E17-1115,0,0.057271,"Missing"
S12-1036,P11-2049,0,0.0526,"Missing"
S12-1036,W10-3110,0,0.0610199,"mation of an event and disregard its negation information, and vice versa. For example, while searching for the patients with diabetes, we should not include a patient who has a clinical report saying No symptoms of diabetes were observed. Thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently. However, most of the systems developed for processing natural language data do not consider negations present in the sentences. Although various works (Morante et al., 2008; Morante and Daelemans, 2009; Li et al., 2010; Councill et al., 2010; The first task in *SEM 2012 Shared Task (Morante and Blanco, 2012) is concerned with finding the scope of negation. The task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences. Negation cue is a word, part of a word, or a combination of words that carries the negation information. Scope of negation in a sentence is the longest group of words in the sentence that is influenced by the negation cue. Negated event is the shortest group of words that is actually affected by the negation cue. In Example (1) below, word n"
S12-1036,C10-1076,0,0.0959029,"he positive information of an event and disregard its negation information, and vice versa. For example, while searching for the patients with diabetes, we should not include a patient who has a clinical report saying No symptoms of diabetes were observed. Thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently. However, most of the systems developed for processing natural language data do not consider negations present in the sentences. Although various works (Morante et al., 2008; Morante and Daelemans, 2009; Li et al., 2010; Councill et al., 2010; The first task in *SEM 2012 Shared Task (Morante and Blanco, 2012) is concerned with finding the scope of negation. The task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences. Negation cue is a word, part of a word, or a combination of words that carries the negation information. Scope of negation in a sentence is the longest group of words in the sentence that is influenced by the negation cue. Negated event is the shortest group of words that is actually affected by the negation cue. In Ex"
S12-1036,S12-1035,0,0.0913646,"ice versa. For example, while searching for the patients with diabetes, we should not include a patient who has a clinical report saying No symptoms of diabetes were observed. Thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently. However, most of the systems developed for processing natural language data do not consider negations present in the sentences. Although various works (Morante et al., 2008; Morante and Daelemans, 2009; Li et al., 2010; Councill et al., 2010; The first task in *SEM 2012 Shared Task (Morante and Blanco, 2012) is concerned with finding the scope of negation. The task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences. Negation cue is a word, part of a word, or a combination of words that carries the negation information. Scope of negation in a sentence is the longest group of words in the sentence that is influenced by the negation cue. Negated event is the shortest group of words that is actually affected by the negation cue. In Example (1) below, word no is a negation cue, the discontinuous word sequences ‘I gave him’ a"
S12-1036,W09-1105,0,0.0197342,"es, we should consider only the positive information of an event and disregard its negation information, and vice versa. For example, while searching for the patients with diabetes, we should not include a patient who has a clinical report saying No symptoms of diabetes were observed. Thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently. However, most of the systems developed for processing natural language data do not consider negations present in the sentences. Although various works (Morante et al., 2008; Morante and Daelemans, 2009; Li et al., 2010; Councill et al., 2010; The first task in *SEM 2012 Shared Task (Morante and Blanco, 2012) is concerned with finding the scope of negation. The task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences. Negation cue is a word, part of a word, or a combination of words that carries the negation information. Scope of negation in a sentence is the longest group of words in the sentence that is influenced by the negation cue. Negated event is the shortest group of words that is actually affected by the ne"
S12-1036,morante-daelemans-2012-conandoyle,0,0.028328,"emaining sections of the paper. For example, for a negation signal unnecessary, the negation cue is un, and similarly, for a negation signal needless, the negation cue is less. 276 Scope of a negation in a sentence can be a continuous sequence of words or a discontinuous set of words in the sentence. Scope of negation sometimes includes the negation word. A negation word may not have a negated event. Presence of a negated event in a sentence depends upon the facts described by the sentence. Non-factual sentences such as interrogative, imperative, and conditional do not contain negated events. Morante and Daelemans (2012) describe the details of the negation cue, scope, and negated event, and the annotation guidelines. An example of the task is shown in Table 1. 3 System Description We decompose the system to identify the scope of negation into three tasks. They are: 1. Finding the negation cue 2. Finding the scope of negation 3. Finding the negated event The scope detection and the negated event detection tasks are dependent on the task of finding the negation cue. But the scope detection and the negated event detection tasks are independent of each other. We identify the negation cues present in the test dat"
S12-1036,D08-1075,0,0.0229458,"ion retrieval, sometimes, we should consider only the positive information of an event and disregard its negation information, and vice versa. For example, while searching for the patients with diabetes, we should not include a patient who has a clinical report saying No symptoms of diabetes were observed. Thus, finding the negation and its scope is important in tasks where the negation and assertion information need to be treated differently. However, most of the systems developed for processing natural language data do not consider negations present in the sentences. Although various works (Morante et al., 2008; Morante and Daelemans, 2009; Li et al., 2010; Councill et al., 2010; The first task in *SEM 2012 Shared Task (Morante and Blanco, 2012) is concerned with finding the scope of negation. The task includes identifying: i) negation cues, ii) scope of negation, and iii) negated event for each negation present in the sentences. Negation cue is a word, part of a word, or a combination of words that carries the negation information. Scope of negation in a sentence is the longest group of words in the sentence that is influenced by the negation cue. Negated event is the shortest group of words that i"
S16-1126,P98-1013,0,0.503337,"nts, question vs. related questions, or question vs. comments of related questions — with a set of similarities computed at two different levels: lexical and semantic. This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means. Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database (Baker et al., 1998). This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this Se814 Proceedings of SemEval-2016, pages 814–821, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics mEval task (Tran et al., 2015). Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations (Franco-Salvador et al., 2014). In this task, if the question or comment includes multiple text fields, e.g. body and sub"
S16-1126,S15-2035,0,0.137294,"features such as syntactic relations and distributed word representations. Similarly to our work, the highest performing approach employed a combination of lexical and semanticbased similarity measures (Tran et al., 2015). Its semantic features included the use of probabilistic topic models, translation obfuscation-based alignments, and pre-computed distributed representations of words both generated with the word2vec1 and GloVe2 toolkits. Their lexical features included BOW, word alignments, and noun matching. They employed a regression model for classification. Another interesting approach, Hou et al. (2015), included textual features — word lengths and punctuation — in addition to syntactical-based features — Part-ofSpeech (PoS) tags. In this work we aim at differentiating from the other approaches by enhancing our ranking model with new similarity measures. These include the use of knowledge graphs obtained using the largest multilingual semantic network — BabelNet — frames from the FrameNet lexical database, and bidirectional distributed word alignments. 3 Lexical and Semantic-based Community Question Answering In this section we detailed the system that we designed for this CQA task. First in"
S16-1126,W10-1201,0,0.0229261,"Missing"
S16-1126,E14-1044,1,0.141883,"Missing"
S16-1126,W15-5403,1,0.650561,"ted the normalized count of common ngrams(n=1,2,3) between two texts. 3.1.2 Semantic Features The semantic features that we employed are the following: • Distributed representations of texts. We used the continuous Skip-gram model (Mikolov et al., 2013) of the word2vec toolkit to generate distributed representations of the words of the complete English Wikipedia.4 Next, for each text, e.g. question or comment, we averaged its word vectors in order to have a single representation of its content as this setting has shown good results in other NLP tasks (e.g. for language variety identification (Franco-Salvador et al., 2015a) and discriminating similar languages (Franco-Salvador et al., 2015b)). Finally, the similarity between texts, e.g. question vs. comment, is estimated using the cosine similarity. 3 http://www.nltk.org/ We used 200-dimensional vectors, context windows of size 10, and 20 negative words for each sample. 4 816 • Distributed word alignments. The use of word alignment strategies has been employed in the past for textual semantic relatedness (Hassan and Mihalcea, 2011). Tran et al. (2015) employed distributed representations to align the words of the question with the words of the comment. A more"
S16-1126,S15-2047,0,0.0705957,"Missing"
S16-1126,S16-1083,0,0.0963816,". 1 Introduction The key role that the Internet plays today for our society benefited the dawn of thousands of new Web social activities. Among those, forums emerged with special relevance following the paradigm of the Community Question Answering (CQA). These type of social networks allow people to post a question to other users of that community. The usage is simple, without much restrictions, and infrequently moderated. The popularity of CQA is a strong indicator that users receive some good and valuable answers. Details of the SemEval 2016 Task 3 on CQA can be found in the overview paper (Nakov et al., 2016). In this work we evaluate the three English-related Task 3 subtasks on CQA. We first represent each instance to rank — question versus (vs.) comments, question vs. related questions, or question vs. comments of related questions — with a set of similarities computed at two different levels: lexical and semantic. This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means. Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use o"
S16-1126,P02-1006,0,0.16122,"Missing"
S16-1126,S15-2038,0,0.258086,"employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database (Baker et al., 1998). This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this Se814 Proceedings of SemEval-2016, pages 814–821, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics mEval task (Tran et al., 2015). Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations (Franco-Salvador et al., 2014). In this task, if the question or comment includes multiple text fields, e.g. body and subject, similarities are estimated using all possible combinations (see Section 3.2). Finally, the ranking of instances is performed using a state-of-the-art machine-learned ranking algorithm: SVMrank . 2 Related Work Automatic question answering has been a popular interest of research in NLP from the beginning of the Internet"
S16-1126,C98-1013,0,\N,Missing
S17-2150,W16-5311,1,0.890969,"Missing"
S17-2150,C16-1251,0,0.0340114,"d search cross-validation over the values {10, 1, 0.1, 1e-02, 1e-03, 1e-04, 1e-05, 1e-06} during the training phase. 3.1.1 3.2 System 2 Hand-crafted Features Before extracting the features, we first lowercased, applied stemming and removed stopwords from the messages. We also replaced named entities (NE), and digits with common identifiers to reduce noise. Lexical: We extracted word n-grams (n=1,2,3) and character n-grams (n=3,4,5) from the messages as they are strong lexical representations (Cavnar and Trenkle, 1994; Mcnamee and Mayfield, 2004; Sureka and Jalote, 2010). Sentiment: SenticNet (Cambria et al., 2016) have been used successfully in problems related to sentiment analysis (Bravo-Marquez et al., 2014; Poria et al., 2016; Maharjan et al., 2017) as it provides a collection of concept-level opinion lexicons with scores in five dimensions (aptitude, attention, pleasantness, polarity, and sensitivity). We used both of the stemmed and non-stemmed versions of the messages to extract concepts from the knowledge base. We modeled the concepts as bag-of-concepts (BoC) and used them as binary features. We averaged the concept scores of five dimensions for each text and used them as numeric features. Word"
S17-2150,W16-5806,1,0.79743,"BoC) and used them as binary features. We averaged the concept scores of five dimensions for each text and used them as numeric features. Word Embeddings: Word embeddings have been shown to capture semantic information. Hence, in order to capture the semantic representation of the messages, we used publicly available word vecFigure 1: Architecture of System 2 Figure 1 shows the overall system architecture of our neural network model. The main motivation to use deep learning methods is the wide success these methods have achieved in various NLP tasks (Bahdanau et al., 2014; Attia et al., 2016; Samih et al., 2016; Collobert and Weston, 2008). It is a combination of two deep learning architecture based models and a multilayer perceptrons (MLP) model operating on hand-engineered fea5 https://code.google.com/archive/p/ word2vec/ 878 tures discussed in Section 3.1.1. Si → x1:T = [x1 , x2 , ..., xT ] (1) We tokenized each messages and represented them as sequences of word vectors as in Equation 1. The maximum length (T ) of the sequences was set to 18 for headlines and 33 for microblogs. These lengths were determined from the training data. The embeddings for the words were initialized using pre-trained wo"
S17-2150,E17-1114,1,0.779496,"Missing"
solorio-etal-2014-sockpuppet,P11-1030,1,\N,Missing
W06-2004,N03-2023,0,0.0540143,"Missing"
W06-2004,W97-0322,1,0.687559,"have been specified in a manually created list. Note that with smaller numbers of contexts (usually 200 or fewer), we lower the frequency threshold to two or more. In general PMI is known to have a bias towards pairs of words (bigrams) that occur a small number of times and only with each other. In this work that is a desirable quality, since that will tend to identify pairs of words that are very strongly associated with each other and also provide unique discriminating information. Pedersen, 2004), which builds upon earlier work in word sense discrimination, including (Schu¨ tze, 1998) and (Pedersen and Bruce, 1997). Our method treats each occurrence of an ambiguous name as a context that is to be clustered with other contexts that also include the same name. In this paper, each context consists of about 50 words, where the ambiguous name is generally in the middle of the context. The goal is to cluster similar contexts together, based on the presumption that the occurrences of a name that appear in similar contexts will refer to the same underlying entity. This approach is motivated by both the distributional hypothesis (Harris, 1968) and the strong contextual hypothesis (Miller and Charles, 1991). 2.1"
W06-2004,E06-2007,1,0.824583,"ntext is now represented by the vector of words with which it occurred in the feature selection data. If a word does not have a corresponding entry in the co–occurrence matrix, then it is simply removed from the context. After all the words in the context are checked, then all of the vectors that are selected are averaged together to create a vector representation of the context. Then these contexts are clustered into a pre– specified number of clusters using the k–means algorithm. Note that we are currently developing methods to automatically select the number of clusters in the data (e.g., (Pedersen and Kulkarni, 2006)), although we have not yet applied them to this particular work. 3 4 Experimental Data We use data in four languages in these experiments, Bulgarian, English, Romanian, and Spanish. The Language Salad 4.1 Raw Corpora In this paper, we explore the creation of a second order representation for a set of evaluation contexts using three different sets of feature selection data. The co–occurrence matrix may be derived from the evaluation contexts themselves, or from a separate set of contexts in a different language, or from the combination of these two (the Salad or Mix). For example, suppose we h"
W06-2004,W04-2406,1,0.773666,"experimentally) reveals that Ronaldo the soccer player tends to occur more so than any other single entity named Ronaldo, so while there is a bit more noise for Ronaldo, there is not really a significant ambiguity. For the Spanish results we only note one pair (George Bush - Tony Blair) where the Mix of English and Spanish results in the best performance. 31 the mix of English and evaluation contexts, in order to perform more accurate name discrimination. contexts based on the number of features they have in common with other evaluation contexts. In fact, results on word sense discrimination (Purandare and Pedersen, 2004) suggest that first order representations are more effective with larger number of context than second order methods. However, we see examples in these results that suggests this may not always be the case. In the Bulgarian results, the largest number of Bulgarian contexts are for NATO-USA, but the Mix performs quite a bit better than Bulgarian only. In the case of Romanian, again NATO-USA has the largest number of contexts, but the Mix still does better than Romanian only. And in Spanish, Mexico-India has the largest number of contexts and English-only does better. Thus, even in cases where w"
W06-2004,J98-1004,0,\N,Missing
W08-0626,W07-1001,0,0.0592699,"PT (s) − P PI (s)) &gt; 0 T D otherwise 116 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 116–117, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics where P PT (s) is the perplexity of the model MT over the sample s, and P PI (s) is the perplexity of the model MI over the same sample s. In other words, if the perplexity of the LM trained on syntactic patterns of children with SLI is smaller than that of the LM trained on POS patterns of TD children, then we will predict that the sample belongs to a child with SLI. In a related work, (Roark et al., 2007) explored the use of cross entropy of LMs trained on POS tags as a measure of syntactic complexity. Their results were inconsistent across language tasks, which may be due to the meaning attached to cross entropy in this setting. Unlikely patterns are a deviation from what is expected; they are not necessarily complex or syntactically rich. Table 1: Perplexity and final output of the LMs for the discrimination of SLI and TD. Sample T D1 T D2 T D3 T D4 T D5 T D6 T D7 T D8 T D9 SLI1 SLI2 average TD average SLI P PT (s) 14.73 11.37 18.35 30.23 9.42 17.37 20.32 16.40 24.35 20.21 19.70 18.06 19.95"
W08-0626,D08-1110,1,0.857465,"Missing"
W12-0413,W11-1713,0,0.107175,"Missing"
W12-0413,W97-0703,0,0.581705,"of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underage) (b) Predator/Volunteer posing as a children (c) Predator/Law enforcement officer posing as a child Our Approach We believe that lexical chains are appropriate to model the fixated discourse of the predators chats. 2. Adult/Adult (consensual relationship) 4.1 Lexical Chains A lexical chain is a sequence of semantically related terms (Morris and Hirst, 1991). It has applications in many tasks including Word Sense Disambiguation (WSD) (Galley and McKeown, 2003) and Text Summarization (Barzilay and Elhadad, 1997). To estimate semantic similarity we used two metrics: the similarity of Leacock and Chodorow (Leacock and Chodorow, 2003), and that of Resnik (Resnik, 1995). Leacock and Chodorow’s semantic similarity measure is defined as: SimL&Ch (c1 , c2 ) = −log Data The most interesting from our research point of view is data of the type 1(a), but obtaining such data is not easy. However, the data of type 1(b) is freely available at the web site www.perverted-justice.com (PJ). For our study, we have extracted chat logs from the perverted-justice website. Since the victim is not real, we considered only t"
W12-0413,J91-1002,0,0.249021,"im 13 in march and not yet? i lied a little bit b4 Predator: its all cool Predator: i can lick hard 4 5 Pendar (2007) has summarized the possible types of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underage) (b) Predator/Volunteer posing as a children (c) Predator/Law enforcement officer posing as a child Our Approach We believe that lexical chains are appropriate to model the fixated discourse of the predators chats. 2. Adult/Adult (consensual relationship) 4.1 Lexical Chains A lexical chain is a sequence of semantically related terms (Morris and Hirst, 1991). It has applications in many tasks including Word Sense Disambiguation (WSD) (Galley and McKeown, 2003) and Text Summarization (Barzilay and Elhadad, 1997). To estimate semantic similarity we used two metrics: the similarity of Leacock and Chodorow (Leacock and Chodorow, 2003), and that of Resnik (Resnik, 1995). Leacock and Chodorow’s semantic similarity measure is defined as: SimL&Ch (c1 , c2 ) = −log Data The most interesting from our research point of view is data of the type 1(a), but obtaining such data is not easy. However, the data of type 1(b) is freely available at the web site www.p"
W12-2422,U11-1012,0,0.0342062,"Missing"
W12-2422,U11-1003,0,0.0394344,"Missing"
W12-2422,U10-1012,0,0.053353,"Missing"
W12-3717,baccianella-etal-2010-sentiwordnet,0,0.0242188,"of different authors, thereby obtaining 68 files. We have also used the subset the of NPS chat corpus (Forsythand and Martell, 2007), though it is not of type 2. We have extracted chat lines only for those adult authors who had more than 30 lines written. Finally the dataset consisted of 65 authors. From each dataset we have left 20 files for testing. 6 Experiments To distinguish between predators and not predators we used a Naive Bayes classifier, already successfully utilized for analyzing chats by previous research (Lin, 2007). To extract positive and negative words, we used SentiWordNet (Baccianella et al., 2010). The features borrowed from McGhee et al. (2011), were detected with the list of words authors made available for us. Imperative sentences were detected as affirmative sentences starting with verbs. Emoticons were captured with simple regular expressions. Our dataset is imbalanced, the majority of the chat logs are from PJ. To make the experimental data more balanced, we have created 5 subsets of PJ corFeature Class Emotional Markers Features borrowed from McGhee et al. (2011) Features helpful to detect neuroticism level Features derived from pedophile’s psychological profile Other Feature Po"
W12-3717,W97-0703,0,0.0717515,"Missing"
W12-3717,W12-0413,1,0.710427,"Missing"
W12-3717,J91-1002,0,0.240367,"ges of personal and reflexive pronouns and modal obligation verbs (have to, has to, had to, must, should, mustn’t, and shouldn’t). We consider the use of imperative sentences and emoticons to capture the predators tendencies to be dominant and copy childrens’ behaviour respectively. The study of Egan et al. (Egan et al., 2011) has revealed several recurrent themes that appear in PJ chats. Among them, fixated discourse: the unwillingness of the predator to change the topic. In (Bogdanova et al., 2012) we present experiments on modeling the fixated discourse. We have constructed lexical chains (Morris and Hirst, 1991) starting with the anchor word “sex” in the first WordNet meaning: “sexual activity, sexual practice, sex, sex activity (activities associated with sexual intercourse)”. We have finally used as a feature the length of the lexical chain constructed with the Resnik similarity measure (Resnik, 1995) with the threshold = 0.7. The full list of features is presented in Table 1. 5 Datasets Pendar (2007) has summarized the possible types of chat interactions with sexually explicit content: 1. Predator/Other (a) Predator/Victim (victim is underaged) (b) Predator/Volunteer posing as a children 114 The m"
W12-3717,S07-1013,0,0.0243579,"use of positive words is expected. On the other hand, as it was described earlier, pedophiles tend to be emotionally unstable and prone to lose temper, hence they might 113 Predator: Predator: Predator: Predator: Predator: Predator: Predator: Predator: Predator: hello r u there thnx a lot thanx a lot u just wast my time drive down there can u not im any more Here the offender is angry because the pseudovictim did not call him: Predator: u didnt call Predator: i m angry with u Therefore, we have decided to use markers of basic emotions as features. At the SemEval 2007 task on “Affective Text” (Strapparava and Mihalcea, 2007) the problem of fine-grained emotion annotation was defined: given a set of news titles, the system is to label each title with the appropriate emotion out of the following list: ANGER, DISGUST, FEAR, JOY, SADNESS, SURPRISE. In this research work we only use the percentages of the markers of each emotion. We have also borrowed several features from McGhee et al. (2011): • Percentage of approach words. Approach words include verbs such as come and meet and such nouns as car and hotel. • Percentage of relationship words. These words refer to dating (e.g. boyfriend, date). • Percentage of family"
W12-3717,strapparava-valitutti-2004-wordnet,0,0.1358,"Missing"
W12-3717,W11-1713,0,0.0639967,"Missing"
W13-1107,P11-1030,1,0.874397,"Missing"
W13-1107,goldstein-stewart-etal-2008-creating,0,0.0256963,"s on data collections that were gathered originally for other purposes. Examples of this include the Reuters Corpus (Lewis et al., 2004) that has been used for benchmarking different approaches to AA (Stamatatos, 2008; Plakias and Stamatatos, 2008; Escalante et al., 2011) and the datasets used in the 2011 and 2012 authorship identification competitions from the PAN Workshop series (Argamon and Juola, 2011; Juola, 2012). Other researchers have invested efforts in creating their own AA corpus by eliciting written samples from subjects participating in their studies (Luyckx and Daelemans, 2008b; Goldstein-Stewart et al., 2008), or crawling though online websites (Narayanan et al., 2012). In contrast, in this paper we focus on data from Wikipedia, where there is a real need to identify if the comments submitted by what appear to be different users, belong to a sockpuppeteer. Data from real world scenarios like this make solving the AA problem an even more urgent and practical matter, but also impose additional challenges to what is already a difficult problem. First, the texts analyzed in the Wikipedia setting were generated by people with the actual intention of deceiving the administrators into believing they are"
W13-1107,C08-1065,0,0.0890912,". Because it is a manual process, it is time consuming and expensive. Perhaps a more serious weakness is the fact that relaying on IP addresses is not robust, as simple counter measures can fool the check users. An alternative to this process could be an automated framework that relies on the analysis of the comments to link editor accounts, as we propose in this paper. 3 Related Work Modern approaches to AA typically follow a text classification framework where the classes are the set of candidate authors. Different machine learning algorithms have been used, including memory-based learners (Luyckx and Daelemans, 2008a; Luyckx and Daelemans, 2010), Support Vector Machines (Escalante et al., 2011), and Probabilistic Context Free Grammars (Raghavan et al., 2010). Similarity-based approaches have also been successfully used for AA. In this setting, the training documents from the same author are concatenated into a single file to generate profiles from authorspecific features. Then authorship predictions are based on similarity scores. (Keselj et al., 2003; Stamatatos, 2007; Koppel et al., 2011) are examples of successful examples of this approach. Previous research has shown that low-level features, such as"
W13-1107,luyckx-daelemans-2008-personae,0,0.183779,". Because it is a manual process, it is time consuming and expensive. Perhaps a more serious weakness is the fact that relaying on IP addresses is not robust, as simple counter measures can fool the check users. An alternative to this process could be an automated framework that relies on the analysis of the comments to link editor accounts, as we propose in this paper. 3 Related Work Modern approaches to AA typically follow a text classification framework where the classes are the set of candidate authors. Different machine learning algorithms have been used, including memory-based learners (Luyckx and Daelemans, 2008a; Luyckx and Daelemans, 2010), Support Vector Machines (Escalante et al., 2011), and Probabilistic Context Free Grammars (Raghavan et al., 2010). Similarity-based approaches have also been successfully used for AA. In this setting, the training documents from the same author are concatenated into a single file to generate profiles from authorspecific features. Then authorship predictions are based on similarity scores. (Keselj et al., 2003; Stamatatos, 2007; Koppel et al., 2011) are examples of successful examples of this approach. Previous research has shown that low-level features, such as"
W13-1107,J93-2004,0,0.0521767,"ount without capital letter at the beginning: Some authors start sentences with numbers or small letters. This feature captures that writing style. An example can be “1953 was the year, ...” or, “big, bald, and brass - all applies to our man”. Quotation count: This is an authorship attribution feature where usage of quotation is counted as a feature. When quoting, not everyone uses the quotation punctuation and hence quotation marks count may help discriminate some writers from others. Parts of speech (POS) tags frequency: We took a total of 36 parts of speech tags from the Penn Treebank POS (Marcus et al., 1993) tag set into consideration. We ignored all tags related to punctuation marks as we have other features capturing these characters. Frequency of letters: We compute the frequency of each of the 26 English letters in the alphabet. The count is normalized by the total number of non-white characters in the comment. This contributed 26 features to the feature set. Function words frequency: It has been widely acknowledged that the rate of function words is a good marker of authorship. We use a list of function words taken from the function words in (Zheng et al., 2006). This list contributed 150 fe"
W13-1107,P10-2008,0,0.0174701,"t robust, as simple counter measures can fool the check users. An alternative to this process could be an automated framework that relies on the analysis of the comments to link editor accounts, as we propose in this paper. 3 Related Work Modern approaches to AA typically follow a text classification framework where the classes are the set of candidate authors. Different machine learning algorithms have been used, including memory-based learners (Luyckx and Daelemans, 2008a; Luyckx and Daelemans, 2010), Support Vector Machines (Escalante et al., 2011), and Probabilistic Context Free Grammars (Raghavan et al., 2010). Similarity-based approaches have also been successfully used for AA. In this setting, the training documents from the same author are concatenated into a single file to generate profiles from authorspecific features. Then authorship predictions are based on similarity scores. (Keselj et al., 2003; Stamatatos, 2007; Koppel et al., 2011) are examples of successful examples of this approach. Previous research has shown that low-level features, such as character n-grams are very powerful discriminators of writing styles. Although, enriching the models with other types of features can boost accur"
W13-1729,P03-1054,0,0.0075393,"used character n-grams, word n-grams, Parts of Speech (POS) tag n-grams, and perplexity of character trigrams as features. For all the features except perplexity, we used a TF-IDF weighting scheme. To reduce the number of fea226 tures, we selected only the top k features based on the document frequency in the training data. The provided dataset contained all the sentences in the essays tokenized by using ETS’s proprietary tokenizers. For the POS tags based features, we used two tagsets: Penn TreeBank (PTB) and Universal POS tags. For PTB POS tags, we tagged the text with the Stanford parser (Klein and Manning, 2003). In order to tag the sentences with Universal POS tags, we mapped the PTB POS tags to universal POS tags using the mapping described by Petrov et al. (2011). We also used perplexity values from language models in our experiments. To generate the language models and compute perplexity, we used the SRILM toolkit (Stolcke et al., 2011). We used training data to generate the language models and train the classifier. Finally, all the sentences were converted into lower case before finding the word and character n-grams. 5 Feature Sets Evaluation We performed a series of experiments using a single"
W13-1729,petrov-etal-2012-universal,0,0.093569,"Missing"
W13-1729,C12-1158,0,0.123753,"Missing"
W13-1729,W13-1706,0,0.0511898,"um, there are varieties of feature types used in native language identification, most of them combine three to nine types. Each type aims to capture specific information such as lexical and syntactic information, structural information, idiosyncrasies, or errors. 3 Shared Task Description The Native Language Identification (NLI) shared task focuses on identifying the L1 of an author based on his writing in a second language. In this case, the second language is English. The shared task had three sub-tasks: one closed training and two open training. The details about the tasks are described by Tetreault et al. (2013). For each subtask, the participants were allowed to submit up to five runs. We participated in the closed training sub-task and submitted five runs. The data sets provided for the shared task were generated from the TOEFL corpus (Blanchard et al., 2013) that contains 12, 100 English essays. The corpus comprised 11 native languages (L1s): Arabic (ARA), Chinese (CHI), French (FRE), German (GER), Hindi (HIN), Italian (ITA), Japanese (JPN), Korean (KOR), Spanish (SPA), Telugu (TEL), and Turkish (TUR), each containing 1100 essays. The corpus was divided into training, development, and test dataset"
W13-1729,W07-0602,0,0.0302507,"the grammatical structures of a second language, the inclusion of function words and dependency parsers as features seem to be helpful to find such transfers as well as error types (Tetreault et al., 2012; Brooke and Hirst, 2011; Wong et al., 2012). It is common that the analysis of the structure of 225 certain grammatical patterns is also informative to find the use or misuse of well-established grammatical structures (e.g. to distinguish between the use of verb-subject-object, subject-verb-object, and subject-object-verb), in such cases n-grams of POS tags can be used. Finally, according to Tsur and Rappoport (2007), the transfer of phonemes is useful in identifying the native language. Even though the phonemes are usually speech features, the authors suggest that this transfer can be captured by the use of character n-grams in the text. Character n-grams have been proved to be a good feature in author profiling as well since they also capture hints of style, lexical information, use of punctuation and capitalization. In sum, there are varieties of feature types used in native language identification, most of them combine three to nine types. Each type aims to capture specific information such as lexical"
W13-1729,U09-1008,0,0.118116,"Missing"
W13-1729,D12-1064,0,0.166946,"Missing"
W13-1911,A00-2018,0,0.0986201,"Missing"
W13-1911,P11-1073,0,0.0304325,"cal pattern. Our approach to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias, 2012; Guti´errez-Clell"
W13-1911,N09-1006,1,0.850652,"Missing"
W13-1911,P05-1025,0,0.0244286,"les Iglesias Temple University Philadelphia, PA 19140, USA iglesias@temple.edu ˜ Lisa Bedore and Elizabeth Pena The University of Texas at Austin Austin, TX 78712, USA lbedore,lizp@mail.utexas.edu Abstract propose a set of linguistic measures for age prediction in children that combines three traditional measures from language assessment with a set of five data-driven measures from language samples of 7 children. A common theme in this emerging line of research is the study of the syntax in those language samples. For instance, to annotate data to be used in the study of language development (Sagae et al., 2005), or to build models to map utterances to their meaning, similar to what children do during the language acquisition stage (Kwiatkowski et al., 2012). In addition, language samples are also used for neurological assessment, as for example in (Roark et al., 2007; Roark et al., 2011) where they explored features such as Yngve and Frazier scores, together with features derived from automated parse trees to model syntactic complexity and surprisal. Similar features are used in the classification of language samples to discriminate between children developing typically and children suffering from a"
W13-1911,P12-2019,0,0.0981442,"t in children. 1 Introduction The analysis of spontaneous language samples is an important task across a variety of fields. For instance, in language assessment this task can help to extract information regarding language proficiency (e.g. is the child typically developing or language impaired). In second language acquisition, language samples can help determine if a child’s proficiency is similar to that of native speakers. In recent years, we have started seeing a growing interest in the exploration of NLP techniques for the analysis of language samples in the clinical setting. For example, Sahakian and Snyder (2012) 89 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 89–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics measures have demonstrated to be valuable in the assessment of language ability considering that practitioners often only need to focus on productivity, diversity of vocabulary, and sentence organization. Although useful, these metrics only provide superficial measures of the children’s language skills that fail to capture detailed lexicosyntactic information. For example, in addition to knowing that a chil"
W13-1911,E12-1024,0,0.0267808,"in Austin, TX 78712, USA lbedore,lizp@mail.utexas.edu Abstract propose a set of linguistic measures for age prediction in children that combines three traditional measures from language assessment with a set of five data-driven measures from language samples of 7 children. A common theme in this emerging line of research is the study of the syntax in those language samples. For instance, to annotate data to be used in the study of language development (Sagae et al., 2005), or to build models to map utterances to their meaning, similar to what children do during the language acquisition stage (Kwiatkowski et al., 2012). In addition, language samples are also used for neurological assessment, as for example in (Roark et al., 2007; Roark et al., 2011) where they explored features such as Yngve and Frazier scores, together with features derived from automated parse trees to model syntactic complexity and surprisal. Similar features are used in the classification of language samples to discriminate between children developing typically and children suffering from autism or language impairment (Prud’hommeaux et al., 2011). In a similar line of research, machine learning and features inspired by NLP have been exp"
W13-1911,W11-0604,0,0.027917,"h to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias, 2012; Guti´errez-Clellen et al., 2"
W13-1911,D12-1055,0,0.0206273,"one for that grammatical pattern. Our approach to compute these new metrics does not require any special treatment on the transcripts or special purpose parsers beyond a POS tagger. On the contrary, we provide a set of measures that in addition to being easy to interpret by practitioners, are also easy to compute. 2 Background and Motivation To establish language proficiency, clinical researchers and practitioners rely on a variety of measures, such as number of different words, type-token ratio, distribution of part-of-speech tags, and mean length of sentences and words per minute (Lu, 2012; Yoon and Bhat, 2012; Chen and Zechner, 2011; Yang, 2011; Miller et al., 2006), to name a few. Most of these metrics can be categorized as low-level metrics since they only consider rates of different characteristics at the lexical level. These measures are helpful in the solution of several problems, for example, building automatic scoring models to evaluate non-native speech (Chen and Zechner, 2011). They can also be used as predictors of the rate of growth of English acquisition in specific populations, for instance, in typically developing (TD) and language impaired (LI) bilingual children (Rojas and Iglesias"
W13-1911,W11-0610,0,0.0370794,"Missing"
W13-1911,W07-1001,0,\N,Missing
W13-1914,P12-2019,0,0.025245,"development. These include speech fluency, syntax, semantics, and coherence. For such analysis, spontaneous narratives have been widely used. Narrating a story or a personal experience requires the narrator to build a mental model of the story and use the knowledge of semantics and syntax to produce a coherent narrative. Children learn from a very early age to narrate stories. The different processes involved in generating a narrative have been shown to provide insights into the language status of children. There has been some prior work on child language sample analysis using NLP techniques. Sahakian and Snyder (2012) used a set of linguistic features computed on child speech samples to create language metrics that included age prediction. Gabani et al. (2011) combined commonly used measurements in communication disorders with LDA has been used in the field of narrative analysis. Wallace et al. (2012) adapted LDA to the task of multiple narrative disentanglement, in which the aim was to tease apart narratives by assigning passages from a text to the subnarratives that they belong to. They achieved strong empirical results. In this paper, we explore the use of LDA for child narrative analysis. We aim to ans"
W13-1914,N12-1001,0,0.0610602,"Missing"
W14-3907,li-etal-2012-mandarin,1,0.84418,"Missing"
W14-3907,W14-3917,0,0.103569,"Missing"
W14-3907,W14-3909,0,0.0388692,"Missing"
W14-3907,W14-3915,0,0.0710818,"Missing"
W14-3907,D13-1084,0,0.207232,"Missing"
W14-3907,W14-3911,1,0.921109,"t exploiting. For instance, the NE lexicons might account for the best results in the NE class in both the Twitter data and the Surprise genre (see Table 4 last row for SPA-EN and second to last for SPAEN Surprise). Most systems showed considerable 67 F-measure 1 0.9 Baseline 0.894 0.892 0.888 0.838 0.8 0.7 0.6 (Jain and Bhat, 2014) (Lin et al., 2014) (Chittaranjan et al., 2014) (King et al., 2014) F-measure (a) MAN-EN Baseline Test1 0.4 Baseline Test2 0.3 0.196 0.2 0.152 0.118 0.1 (Chittaranjan et al., 2014) 0.417 0.360 0.338 0.260 (King et al., 2014) 0.095 0.048 0.044 (Jain and Bhat, 2014) (Elfardy et al., 2014) (Lin et al., 2014) F-measure (b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2 Baseline 1 0.952 0.962 (King et al., 2014) (Lin et al., 2014) 0.975 0.974 0.972 0.977 0.9 0.8 (Jain and Bhat, 2014) (Shrestha, 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (c) NEP-EN F-measure 1 0.9 Baseline 0.8 0.7 0.6 0.703 0.754 0.753 0.783 0.793 0.822 0.634 (Shrestha, 2014) (King et al., 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (Jain and Bhat, 2014) (Lin et al., 2014) (Bar and Dershowitz, 2014) (d) SPA-EN Figure 1: Prediction results on"
W14-3907,W14-3916,0,0.102327,"Missing"
W14-3907,W14-3910,0,0.0496265,"Missing"
W14-3907,C82-1023,0,0.500666,"olumbia.edu pascale@ece.ust.hk ayc2135@columbia.edu Abstract this direction. We define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably. In its spoken form, CS has probably been around ever since different languages first came in contact. Linguists have studied this phenomenon since the mid 1900s. In contrast, the Natural Language Processing (NLP) community has only recently started to pay attention to CS, with the earliest work in this area dating back to Joshi’s theoretical work proposing an approach to parsing CS data (Joshi, 1982) based on the Matrix and Embedded language framework. With the wide-spread use of social media, CS is now being used more and more in written language and thus we are seeing an increase in published papers dealing with CS. We are specifically interested in intrasentential code switched phenomena. As a result of this task, we have successfully created the first set of annotated data for several language pairs with a coherent set of labels across the languages. As the shared task results show, CS poses new research questions that warrant new NLP approaches, and thus we expect to see a significan"
W14-3907,P11-2007,0,0.0996179,"Missing"
W14-3907,N13-1131,0,0.144834,"Missing"
W14-3907,W14-3912,0,0.116613,"Missing"
W14-3907,W15-3116,0,\N,Missing
W14-3907,E14-1001,0,\N,Missing
W14-3907,W15-2902,0,\N,Missing
W14-3907,N15-1109,0,\N,Missing
W14-3907,W15-5936,0,\N,Missing
W15-1608,J08-4004,0,0.0578495,"ality control of this task contained 1,000 tweets that were annotated by two in-lab annotators. 3.3 Review and Agreement To judge the validity of the CrowdFlower annotations, one-way in-lab review was performed on small segments of the crowdsourced results. 1,000 tweets were reviewed from jobs using the PDF instruction scheme and another 500 were reviewed from the job using the inline instruction scheme. Inter-annotator agreement measures were calculated between the original and reviewed annotations for each scheme. The measures used were observed agreement, Fleiss multi-π, and Cohen multi-κ (Artstein and Poesio, 2008) calculated for the full data set, as well as observed agreement per annotation category. The CrowdFlower annotation results’ agreement with the in-lab review was above expectations. All three overall agreement measures were at or above 0.9. At the category level, agreement was high for the simpler categories, such as Lang1, Lang2, and Other, but dipped considerably for the more complicated ones such as named entities. This is consistent with the error analysis done by King and Abney (2013), where the most frequent source of error was named entities. Ambiguous and Mixed made up only approximat"
W15-1608,W12-2108,0,0.0347668,"code-switching, as well as research in statistical methods for the automated processing of code-switching. Therefore, the annotations are theory agnostic, and follow a pragmatic definition of code-switching. Finally, to show that the processing of codeswitching text requires further advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topi"
W15-1608,W10-0701,0,0.0678343,"Missing"
W15-1608,C82-1023,0,0.836204,"Missing"
W15-1608,N13-1131,0,0.445052,"In each of these projects, however, code-switching data was scarce, coming primarily from conversations. Because of complications with traditional evaluation measures, the code-switching point detection project used a new evaluation method, in which artificial code-switched content was generated and compared with genuine content (Solorio and Liu, 2008a). In the past, most language identification research has been done at the document level. Some researchers, however, have developed methods to identify languages within multilingual documents (Singh and Gorla, 2007; Nguyen and Do˘gru¨oz, 2013; King and Abney, 2013). Their test data comes from a variety of sources, including web pages, bilingual forum posts, and jumbled data from monolingual sources, but none of them are trained on code-switched data, opting instead for a monolingual training set per language. This could prove to be a problem when working on code-switched data, particularly in shorter samples such as social media data, as the codeswitching context is not present in training material. One system tackled both the problems of codeswitching and social media in language and codeswitched status identification (Lignos and Marcus, 2013). Lignos"
W15-1608,D14-1108,0,0.0598944,"Missing"
W15-1608,li-etal-2012-mandarin,0,0.0976505,"Missing"
W15-1608,P08-1099,0,0.0721365,"Missing"
W15-1608,D13-1084,0,0.338601,"Missing"
W15-1608,roberts-etal-2012-empatweet,0,0.0309149,"of a population that is almost entirely multilingual. In both cases the two languages are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #m"
W15-1608,D08-1102,1,0.905506,"rther advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topic. An earlier example is the work by Joshi (Joshi, 1982), where he proposes a system that can help to parse and generate code-switching sentences. His approach is based on the matrix language-embedded language formalism and although the paper has a good justification it lacks a"
W15-1608,D08-1110,1,0.861076,"rther advancement of our NLP technology, we present a case study in language identification with our corpora. Language identification of monolingual text has been considered a solved problem for some time now (McNamee, 2005) and even in Twitter the problem has been shown to be tractable when annotated data is available (Bergsma et al., 2012). However, as we demonstrate in this paper, when code-switching is present, the performance of state-of-the-art systems is not on par with that of monolingual sources. We predict that the difficulty increases for deeper and higher-level NLP tasks. In fact, Solorio and Liu (2008b) have shown 73 already that part-of-speech tagging performance in code-switching data is also lagging behind that observed in monolingual sources. 2 Related Work Although code-switching has not been investigated as deeply as monolingual text in the natural language processing field, there has been some work on the topic. An earlier example is the work by Joshi (Joshi, 1982), where he proposes a system that can help to parse and generate code-switching sentences. His approach is based on the matrix language-embedded language formalism and although the paper has a good justification it lacks a"
W15-1608,temnikova-etal-2014-building,0,0.0212016,"s are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet, #proud. A similar process was followed by"
W15-1608,tomlinson-etal-2014-mygoal,0,0.0233503,"ultilingual. In both cases the two languages are written using the same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet,"
W15-1608,williams-katz-2012-new,0,0.0272467,"same Latin script. This is true for Nepali, even though Devanagari is its official script, because the education system in Nepal teaches typing only for English, so for digital content like social media it is common for Nepalese speakers to type using English characters. We chose Twitter as the source of our data as the informal nature of tweets makes them a more natural source for code-switching phenomena. Many researchers have turned to Twitter as a source of data for research (i.e. (Roberts et al., 2012; Reyes et al., 2013; Tomlinson et al., 2014; Kong et al., 2014; Temnikova et al., 2014; Williams and Katz, 2012)). Typically, collecting Twitter data is a straightforward 1 2 http://www.lenafoundation.org/ https://www.dataminr.com/ 72 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 72–84, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics process involving the Twitter API, specifying the desired language, and a set of keywords or hash tags. For example, in the research on user intentions some of the hash tags used include: #mygoal, #iwon, #madskills, #imapro, #dowhatisay, #kissmyfeet, #proud. A similar process was followed by all of the previous work l"
W15-2602,P14-5010,1,0.0161718,"Missing"
W16-0322,W15-1202,0,0.0269119,"ost of the errors of our system were due to new vocabulary not present in the training set. We tried to account for this with the use of a smoothing parameter in the classifier but more work is needed in this respect. One way could be to train a word embedding using the unlabeled data in such a way that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, w"
W16-0322,W15-1206,1,0.82427,"ay that semantic similarities of words not present in the training samples can be modeled in the test set. 6 Related work In the previous versions of the workshop some systems have been proposed to solve similar challenging problems using some or similar features to the ones we used in our system. In (Mitchell et al., 2015) a system was developed for quantifying the language of schizophrenia in social media based on the LIWC lexicon. This study also showed that character ngrams over specific tweets in the user’s history can be used to separate schizophrenia sufferers from a control group. In (Pedersen, 2015) a system based on decision lists was developed to identify Twitter users who suffer from Depression or Post Traumatic Stress Disorder (PTSD). The features in this system are based on n-grams of up to 6 words. In this system, the usage of larger n-grams performed better 174 than bigrams. In our experiments, we only tried with n-grams up to length 3 and found that the best performing system in the cross-validation of the training data was obtained using bigrams. 7 Conclusion In this paper, we have briefly described our submission to the CLPsych 2016 shared task. We found that the best result wa"
W16-5311,S15-2151,0,0.0148292,"cation of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al."
W16-5311,S16-1168,0,0.0235916,"classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently wi"
W16-5311,S07-1081,0,0.0372437,"antic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2"
W16-5311,N16-2002,0,0.0163109,"many. For example, while it is relatively easy to predict ‘queen’ as the answer to this query x = king − man + woman, you cannot expect ‘contract’ as the answer to the query x = shoe − boot + lease with the same level of confidence if the relationship is expected to be either synonymy, antonymy, hyponymy, or hypernymy. In this paper we try three different methods for handling semantic classification in the shared task: word analogy, linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and Mc"
W16-5311,P05-1053,0,0.0185926,"linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we"
W16-5311,P97-1023,0,0.558863,"; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to e"
W16-5311,S12-1047,0,0.0318201,"tend to have similar contextual embeddings. This paper describes our system for the CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about thr"
W16-5311,S14-2003,0,0.0171965,"s our system for the CogALex-V Shared Task on Corpus-Based Identification of Semantic Relations. We evaluated three methods for semantic classification based on word embeddings: word analogy, linear regression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman,"
W16-5311,W14-1618,0,0.0310541,"ord 54 . This is built with the GloVe architecture from a corpus of 6B words (400K vocabulary entries) with 300 dimensions, and applying AdaGrad with context size of 20. 4 Experiments and Results In this section we outline the experiments and report the results for the three approaches we tested: word analogy, linear regression and multi-task CNN. The results reported in this section are on the training set for all labels including “FALSE” for Task-1 and “RANDOM” for Task-2. Results on the test set of our selected systems are reported in Section 5. 4.1 Word Analogy In word analogy, similar to Levy et al. (2014), we query the word vector directly to obtain the closest match to the given example using the formula: predicted_word = example_word1 − example_word2 + target_word. We iterate the query over all the examples in the training set and limit the search scope to the vocabulary items within the set (a set is the target word and all potentially related words). Then we take the average of the responses. The results in Table 2 show that this approach does not work as well for this current task. As we will show, the scores are much lower than those of the other approaches we explored here. 4.2 Linear R"
W16-5311,Q15-1016,0,0.0391388,"en words is one-to-many. For example, while it is relatively easy to predict ‘queen’ as the answer to this query x = king − man + woman, you cannot expect ‘contract’ as the answer to the query x = shoe − boot + lease with the same level of confidence if the relationship is expected to be either synonymy, antonymy, hyponymy, or hypernymy. In this paper we try three different methods for handling semantic classification in the shared task: word analogy, linear regression and multi-task CNN. Using word analogy for identifying semantic relations has been discussed in a number of papers including (Levy et al., 2015; Gladkova et al., 2016; Vylomova et al., 2015). The basic idea is to use vector-oriented reasoning based on the offsets between words (Mikolov et al., 2013b) assuming that pairs of words that share a certain semantic relation will have similar cosine distance. Linear regression classifiers, including Naive Bayes, Logistic Regression and Support Vector Machines, have been used for the identification of semantic relations. For example, GuoDong et al. (2005) used SVM to extract semantic relationships between entities relying on features extracted from lexical, syntactic, and semantic knowledge."
W16-5311,N13-1090,0,0.367689,"et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolov et al. (2013a). This architecture provided an efficient and simplified training methodology that minimizes computational complexity by doing away with the non-linear hidden layer, enabling training on much larger data than were previously possible. The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality siz"
W16-5311,D14-1162,0,0.0899215,", 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolov et al. (2013a). This architecture provided an efficient and simplified training methodology that minimizes computational complexity by doing away with the non-linear hidden layer, enabling training on much larger data than were previously possible. The public availability of word embedding training programs such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) allowed researchers to create models with different parameters and dimensionality sizes for different purposes. The evaluation data1 used in the development of the Google Continuous Bag of Words (CBOW) and skip-gram vectors (Mikolov et al., 2013a) focused on semantic similarities and coarse-grained semantic relations in the form of deterministic answers by analogy. These relationships were one-to-one including, for example, capitals (Athens: Greece - Baghdad: Iraq), currencies (India: rupee - Iran: rial), gender (king: queen - man: woman), derivation (amazing: amazingly - safe: safely), and i"
W16-5311,W14-4701,0,0.0286264,"egression, and multi-task CNNs. In all these methods, we use publicly available pre-trained English word vectors. 2 Related Work Semantic relatedness between single words (excluding phrases, sentences and multilingual parallel data) has been addressed in a number of shared tasks before, including relational similarity in SemEval-2012 (Jurgens et al., 2012), word to sense matching in SemEval-2014 (Jurgens et al., 2014), hyponym-hypernym relations in SemEval-2015 (Bordea et al., 2015), semantic taxonomy (hypernymy) in SemEval-2016 (Bordea et al., 2016), and semantic association in CogALex-2014 (Rapp and Zock, 2014). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 86 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 86–91, Osaka, Japan, December 11-17 2016. License details: http:// The idea of representing words as vectors has been studied for about three decades (Hinton et al., 1986; Rumelhart et al., 1986; Elman, 1990; Bengio et al., 2003; Kann and Schütze, 2008; Mikolov et al., 2013b). The interest in word embeddings has intensified recently with the introduction of the new log linear architecture of Mikolo"
W16-5311,P15-1061,0,0.025986,"Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect the type of the relationship: HYPER, PART_OF, SYN, ANT, or RANDOM. 3.2 Pre-Trained Word Vectors In our experiments we experimented with three large-scale, publicly available pre-trained word vectors: 1 http://www.fit.vutbr.cz/ imikolov/rnnlm/word-"
W16-5311,D15-1062,0,0.0222394,"e Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect the type of the relationship: HYPER, PART_OF, SYN, ANT, or RANDOM. 3.2 Pre-Trained Word Vectors In our experiments we experimented with three large-scale, publicly available pre-trained word vectors: 1 http://www.fit.vutbr.cz/ imikolov/rnnlm/word-test.v1.txt 87 Word 1"
W16-5311,C14-1220,0,0.030602,"yntactic, and semantic knowledge. Hatzivassiloglou and McKeown (1997) used a log-linear regression model to predict the similarity of conjoined adjectives. Snow et al. (2004) use a logistic regression classifier for hypernym pair identification. Costello (2007) used Naive Bayes to learn associations between features extracted from WordNet and predict relation membership categories. In our work, we do not use any lexical, syntactic or semantic features, other than the word embeddings and we score similarity using the well known cosine similarity metric. CNNs have also been applied to the task. Zeng et al. (2014) use a convolutional deep neural network (DNN) to extract lexical features learned from word embeddings and then fed into a softmax classifier to predict the relationship between words. Similar approaches have been applied in (Santos et al., 2015) and (Xu et al., 2015). 3 Data Description 3.1 Shared Task Data The shared task organizers provide a training set of 3,054 word pairs for 318 target words. In Task-1, we are given a pair of words and we need to determine if the words are semantically related or not. Some examples of Task-1 are shown in 1. In Task-2 participants are required to detect"
W16-5311,P16-1158,0,\N,Missing
W16-5805,K15-1005,1,0.864973,"Missing"
W16-5805,L16-1260,0,0.0396233,"ta (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2014 shared task as this year’s training corpus and development corpus, respectively. However, the previous shared task did not have the same labels we are using this year. We had in-lab annotators follow a simple mapping process: they"
W16-5805,W16-5814,0,0.137774,"ams also participated in the MSA-DA task. There was a wide variety of system architectures ranging from simple rule based systems all the way to more complex machine learning implementations. Most of the systems submitted did not change anything in the implementation to tackle one language pair or the other, which implies that the participants were highly interested in building language independent systems that could be easily scaled to multiple language pairs. In Table 5 we show a summary of the the architectures of the systems submitted by the participants. All teams, with the exception of (Chanda et al., 2016), used some sort of machine learning algorithm in their systems. The algorithm of choice by most participants was the Conditional Random Fields (CRF). This is no surprise since CRFs fit the problem nicely due to the sequence labeling nature of the task as it was evidenced in the high performance by CRFs achieved in the previous shared task. A new addition this year is the use of deep learning algorithms by two of the participants. Deep learning is now much more prevalent in NLP than it was two years ago when the previous shared task was held. (Jaech et al., 2016) used a convolutional neural ne"
W16-5805,L16-1292,0,0.188047,"Missing"
W16-5805,N15-1109,0,0.0305685,"this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2014 shared task as this year’s training corpus and development corpus, respectively. However, t"
W16-5805,E14-1001,0,0.0310435,"uial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2"
W16-5805,W16-5807,0,0.185235,"teams, with the exception of (Chanda et al., 2016), used some sort of machine learning algorithm in their systems. The algorithm of choice by most participants was the Conditional Random Fields (CRF). This is no surprise since CRFs fit the problem nicely due to the sequence labeling nature of the task as it was evidenced in the high performance by CRFs achieved in the previous shared task. A new addition this year is the use of deep learning algorithms by two of the participants. Deep learning is now much more prevalent in NLP than it was two years ago when the previous shared task was held. (Jaech et al., 2016) used a convolutional neural network (CNN) to obtain word vectors which are then fed as a sequence to a bidirectional long short term memory recurrent neural network (LSTM) to map the sequence to a label. The system submitted by (Samih et al., 2016) used the output of a pair of LSTMs along with a CRF and postprocessing to obtain the final label mapping. These systems are perhaps more complex than traditional machine learning algorithms, but the trade off for performance is evident in the results. Most of the participants included some sort of external resource in their system. Among them we ca"
W16-5805,R15-1033,0,0.120597,"Missing"
W16-5805,C82-1023,0,0.832126,"or annotation. In Table 1 we show examples of code-switched tweets that are found in our data. We have posted the annotation guidelines for SPA-ENG, but it can be generalized to the MSA-DA language pair as well. This is possible because we want to have a universal set of annotation labels that can be used to correctly annotate new data with the least amount of error possible. We keep improving the guidelines to accommodate findings from the previous shared task as well as new relevant research. 3 Related Work The earliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsi"
W16-5805,W15-3116,0,0.0386905,"ialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of"
W16-5805,W16-2013,0,0.0526771,"arliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic i"
W16-5805,L16-1667,0,0.125616,"Missing"
W16-5805,L16-1658,0,0.0258703,"tter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presen"
W16-5805,W15-5936,0,0.0808228,"tification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, En"
W16-5805,N16-1159,0,0.0350113,"he task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research"
W16-5805,W16-5816,0,0.0489599,"Missing"
W16-5805,W16-5817,0,0.054738,"Missing"
W16-5805,W14-3907,1,0.730185,"o the MSA-DA language pair as well. This is possible because we want to have a universal set of annotation labels that can be used to correctly annotate new data with the least amount of error possible. We keep improving the guidelines to accommodate findings from the previous shared task as well as new relevant research. 3 Related Work The earliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-"
W16-5805,W15-2902,0,0.148151,"Missing"
W16-5805,L16-1655,0,0.125494,"Missing"
W16-5805,W16-5818,0,0.0581395,"Missing"
W16-5806,P13-2037,0,0.158762,"Missing"
W16-5806,K15-1005,0,0.0606159,"Missing"
W16-5806,W14-3902,0,0.11852,"nguage is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limite"
W16-5806,W14-5152,0,0.0188575,"lmeyer Mohammed Attia Dept. of Computational Linguistics Dept. of Computer Science Google Inc. University of Houston Heinrich Heine University, New York City Houston, TX, 77004 Düsseldorf, Germany NY, 10011 solorio@cs.uh.edu kallmeyer@phil.hhu.de attia@google.com Abstract tweets, SMS messages, user comments on the articles, blogs, etc., this phenomenon is becoming more pervasive. Code-switching does not only occur across sentences (inter-sentential) but also within the same sentence (intra-sentential), adding a substantial complexity dimension to the automatic processing of natural languages (Das and Gambäck, 2014). This phenomenon is particularly dominant in multilingual societies (Milroy and Muysken, 1995), migrant communities (Papalexakis et al., 2014), and in other environments due to social changes through education and globalization (Milroy and Muysken, 1995). There are also some social, pragmatic and linguistic motivations for code-switching, such as the the intent to express group solidarity, establish authority (Chang and Lin, 2014), lend credibility, or make up for lexical gaps. This paper describes the HHU-UH-G system submitted to the EMNLP 2016 Second Workshop on Computational Approaches to"
W16-5806,W15-3904,0,0.0617735,"Missing"
W16-5806,elfardy-diab-2012-simplified,0,0.0155933,"ter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect. With the massive increase in code-switched writings in user-generated content, it has become imperative to develop tools and methods to handle and process this type of data."
W16-5806,C82-1023,0,0.183927,"Missing"
W16-5806,P16-1101,0,0.00704538,"at of Chang and Lin (2014) in that we use RNNs and word embeddings. The difference is that we use long-shortterm memory (LSTM) with the added advantage of the memory cells that efficiently capture longdistance dependencies. We also combine wordlevel with character-level representation to obtain morphology-like information on words. 3 Model In this section, we will provide a brief description of LSTM, and introduce the different components of our code-switching detection model. The architecture of our system, shown in Figure 1, bears resemblance to the models introduced by Huang et al. (2015), Ma and Hovy (2016), and Collobert et al. (2011). 3.1 Long Short-term Memory A recurrent neural network (RNN) belongs to a family of neural networks suited for modeling sequential data. Given an input sequence x = (x1 , ..., xn ), an RNN computes the output vector yt of each word xt by iterating the following equations from t = 1 to n: Figure 1: System Architecture. where ht is the hidden states vector, W denotes weight matrix, b denotes bias vector and f is the activation function of the hidden layer. Theoretically RNN can learn long distance dependencies, still in practice they fail due the vanishing/exploding"
W16-5806,W15-1608,1,0.900866,"Missing"
W16-5806,N13-1039,0,0.0355151,"Missing"
W16-5806,W14-3905,0,0.049232,"Missing"
W16-5806,P08-2030,0,0.0602839,"Missing"
W16-5806,L16-1658,1,0.835896,"ges, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect. With the massive increase in code-switched writings in user-generated content, it has become imperative to develop tools and methods to handle and process this type of data. Identification of languages used in the sentence is the first step in doing any kind of text analysis. For example, most data found in social media produced by bilingual people is a mixture of two languages. In order to process or translate this data t"
W16-5806,D08-1102,1,0.820571,"rmance. 1 Introduction Code-switching can be defined as the act of alternating between elements of two or more languages or language varieties within the same utterance. The main language is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approach"
W16-5806,W14-3903,0,0.0180518,"switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics 2016a; Samih and Maier, 2016b). The current shared task is limited to two scenarios: a) codeswitching between two distinct languages: SpanishEnglish, b) and two language varieties: MSAEgyptian Dialect."
W16-5806,W14-3904,0,0.050393,"Missing"
W16-5806,L16-1667,0,0.022612,"ween elements of two or more languages or language varieties within the same utterance. The main language is sometimes referred to as the ‘host language’, and the embedded language as the ‘guest language’ (Yeh et al., 2013). Code-switching is a wide-spread linguistic phenomenon in modern informal user-generated data, whether spoken or written. With the advent of social media, such as Facebook posts, Twitter It is not necessary for code-switching to occur only between two different languages like Spanish-English (Solorio and Liu, 2008), MandarinTaiwanese (Yu et al., ) and Turkish-German (Özlem Çetinoglu, 2016), but it can also happen between three languages, e.g. Bengali, English and Hindi (Barman et al., 2014), and in some extreme cases between six languages: English, French, German, Italian, Romansh and Swiss German (Volk and Clematide, 2014). Moreover, this phenomenon can occur between two different dialects of the same language as between Modern Standard Arabic (MSA) and Egyptian Dialect (Elfardy and Diab, 2012), or MSA and Moroccan Arabic (Samih and Maier, 50 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 50–59, c Austin, TX, November 1, 2016. 2016 Asso"
W16-5812,K15-1005,1,0.840177,"to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identified as lang2 are processed by the monolingual lang2 POS tagger. Finally we integrate the POS tags from both monolingual taggers creating the POS tag sequence for the sentence. Figure-1 shows a diagram representing this approach for the MSA-EGY language pair. For MSA-EGY, we used the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. It takes plain Arabic text in Arabic UTF8 encoding or Buckwalter encoding as input and outputs: 1) Class Identification (CI) of the input text to specify whether the tokens are MSA, EGY, as well as other information such as name entity, foreign word, or unknown labels per token. Furthermore, it provides the results with a confidence score; 2)Dialect Classification (DC) of the input text to specify whether it is Egyptian. For SPA-ENG, we trained language models (LM) on English and Spanish data to assign Langu"
W16-5812,N13-1049,0,0.0602647,"Missing"
W16-5812,W14-3902,0,0.166421,"Missing"
W16-5812,W13-2301,0,0.0179214,"notated using the Buckwalter (BW) POS tag set. The BW POS 102 tag set is considered one of the most popular Arabic POS tagsets. It gains its popularity from its use in the Penn Arabic Treebank (PATB) (Maamouri et al., 2004; Alkuhlani et al., 2013). It can be used for tokenized and untokenized Arabic text. The tokenized tags that are used in the PATB are extracted from the untokenized tags. The number of untokenized tags is 485 tags and generated by BAMA (Buckwalter, 2004). Both tokenized and untokenized tags use the same 70 tags and sub-tags such as nominal suffix, ADJ, CONJ, DET, and, NSUFF (Eskander et al., 2013) (Alkuhlani et al., 2013). Combining the sub-tags can form almost 170 morpheme sub-tags such NSUFF FEM SG. This is a very detailed tagset for our purposes and also for cross CS language pair comparison, i.e. in order to compare between trends in the MSA-EGY setting and the SPA-ENG setting. Accordingly, we map the BW tagset which is the output of the MADAMIRA tools to the universal tagset (Petrov et al., 2011). We apply the mapping as follows: 1) Personal, relative, demonstrative, interrogative, and indefinite pronouns are mapped to Pronoun; 2)Acronyms are mapped to Proper Nouns; 3) Complementi"
W16-5812,R15-1033,0,0.132072,"Missing"
W16-5812,J93-2004,0,0.0788385,"rsion of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identi"
W16-5812,pasha-etal-2014-madamira,1,0.874077,"Missing"
W16-5812,W15-5936,0,0.176006,"Missing"
W16-5812,D08-1110,1,0.873698,"a relatively pure monolingual tagger per language variety (MSA or EGY), trained on informal genres for both MSA and EGY. Therefore, we retrained a new version of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. pro"
W16-5812,taule-etal-2008-ancora,0,0.108198,"Missing"
W16-5812,D14-1105,0,0.331016,"m. Typically people who code switch master two (or more) languages: a common first language (lang1) and another prevalent language as a second language (lang2). The languages could be completely distinct such as Mandarin and English, or Hindi and English, or they can be variants of one another such as in the case of Modern Standard Arabic (MSA) and Arabic regional dialects (e.g. Egyptian dialect– EGY). CS is traditionally prevalent in spoken language but with the proliferation of social media such as Facebook, Instagram, and Twitter, CS is becoming ubiquitous in written modalities and genres (Vyas et al., 2014; Danet and Herring, 2007; C´ardenas-Claros and Isharyanti, 2009) CS can be observed in different linguistic levels of representation for different language pairs: phonological, morphological, lexical, syntactic, semantic, and discourse/pragmatic. It may occur within (intra-sentential) or across utterances (inter-sentential). For example, the following Arabic excerpt exhibits both lexical and syntactic CS. The speaker alternates between two variants of Arabic MSA and EGY. Arabic Intra-sentential CS:1 wlkn AjhztnA AljnA}yp lAnhA m$ xyAl Elmy lm tjd wlw mElwmp wAHdp. English Translation: Since o"
W16-6203,W16-0307,0,0.0772871,"how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a thread, or reply to a thread initiated by others. We focused on the forums Depression, Relationship Health, and Brain/Nervous System Disorders. Wh"
W16-6203,P14-5010,1,0.020172,"Missing"
W16-6203,W15-2602,1,0.830778,"While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental ailments. Users in these forums can either initiate a"
W16-6203,W14-4108,0,0.0313908,"tely predict which users will withdraw from a forum. While these contributions obviously do not represent a solution to depression, we believe they form a significant first step towards understanding how the study of social media timelines can contribute. There are several other works that have analyzed participation continuation problems in different online social paradigms using different approaches, i.e. friendship relationship among users (Ngonmang et al., 2012), psycholinguistic word usage (Mahmud et al., 2014), linguistic change (Danescu-NiculescuMizil et al., 2013), activity timelines (Sinha et al., 2014), and combinations of the above (Sadeque et al., 2015). Also there are numerous works that contributes to the mental health research (De Choudhury et al., 2016; De Choudhury, 2015; Gkotsis et al., 2016; Colombo et al., 2016; Desmet and Hoste, 2013) We believe ours is the first work to integrate language and timeline analysis for studying decreasing social interaction in depression forums. 2 Data Our data is collected from HealthBoards1 , one of the oldest and largest support group based online social networks with hundreds of support groups dedicated to people suffering from physical or mental"
W16-6203,D13-1170,0,0.00917784,"Missing"
W17-3010,baccianella-etal-2010-sentiwordnet,0,0.0459712,"tupid ass(N) dip(N) shit(N) You stupid ass. S**k my ass. Table 4: Negative patterns for detecting nastiness. The capital letters are the abbreviations for the following POS tags: L = nominal + verbal (e.g. I’m)/verbal + nominal (e.g. let’s), R = adverb, D = determiner, A = adjective, N = noun, O = pronoun (not possessive) CMU’s Part of Speech tagger7 to get the POS tags for each document. Emoticons (E): We use a normalized count of happy, sad and total emoticons as features to feed the classifier. SentiWordNet (SWN): We use sentence neutrality, positive and negative scores using SentiWordNet (Baccianella et al., 2010), average count of nouns, verbs, adverbs and adjectives (Ark Tweet NLP (Owoputi et al., 2013)) as features. LIWC (Linguistic Inquiry and Word Count): LIWC2007 (Pennebaker et al., 2007)) helps us to determine different language dimensions like the degree of positive or negative emotions, selfreferences, and casual words in each text. In this case, we use a normalized count of words separated by any of LIWC categories. Style and Writing density (WR): This category focuses on the properties of the text by considering the number of words, characters, all uppercase words, exclamations, question mar"
W17-3010,N13-1039,0,0.0942258,"Missing"
W17-3010,R15-1086,0,0.10891,"Missing"
W17-3010,N12-1084,0,0.211987,"Missing"
W17-4419,W09-1324,0,0.00897952,"Missing"
W17-4419,W16-5311,1,0.88214,"Missing"
W17-4419,E17-2026,0,0.00533772,"), who achieved the first place on WNUT-2016 shared task, use a BLSTM neural network to leverage orthographic features. We use a similar approach but we employ CNN and BLSTM in parallel instead of forwarding the CNN output to the BLSTM. Nevertheless, our main contribution resides on MultiTask Learning (MTL) and a combination of POS tags and gazetteers representation to feed the network. Recently, MTL has gained significant attention. Researchers have tried to correlate the success of MTL with label entropy, regularizers, training data size, and other aspects (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). For instance, Collobert and Weston (2008) use a multitask network for different NLP tasks and show that the multi-task setting improves generality among shared tasks. In this paper, we take advantage of the multi-task setting by adding a more general secondary task, NE segmentation, along with the primary NE categorization task. 3 and punctuation patterns. Once we have an encoded word, we represent each character with a 30-dimensional vector (Ma and Hovy, 2016). We account for a maximum length of 20 characters3 per word, applying post padding on shorter words and truncating longer words. Wor"
W17-4419,E17-1005,0,0.00879387,"patham and Collier (2016), who achieved the first place on WNUT-2016 shared task, use a BLSTM neural network to leverage orthographic features. We use a similar approach but we employ CNN and BLSTM in parallel instead of forwarding the CNN output to the BLSTM. Nevertheless, our main contribution resides on MultiTask Learning (MTL) and a combination of POS tags and gazetteers representation to feed the network. Recently, MTL has gained significant attention. Researchers have tried to correlate the success of MTL with label entropy, regularizers, training data size, and other aspects (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). For instance, Collobert and Weston (2008) use a multitask network for different NLP tasks and show that the multi-task setting improves generality among shared tasks. In this paper, we take advantage of the multi-task setting by adding a more general secondary task, NE segmentation, along with the primary NE categorization task. 3 and punctuation patterns. Once we have an encoded word, we represent each character with a 30-dimensional vector (Ma and Hovy, 2016). We account for a maximum length of 20 characters3 per word, applying post padding on shorter words and t"
W17-4419,P15-1033,0,0.00759021,"econd layer (blue) takes the input from the first convolutional layer. Global Average Pooling is applied after the second convolutional layer. and categorization tasks. We use a single-neuron layer with a sigmoid activation function for the secondary NE segmentation task, whereas for the primary NE categorization task, we employ a 13neuron6 layer with a softmax activation function. Finally, we add the losses from both tasks and feed the total loss backward during training. a word. The resulting vector is used as input for the rest of the network. Word level BLSTM: we use a Bidirectional LSTM (Dyer et al., 2015) to learn the contextual information of a sequence of words as described in Figure 2. Word embeddings are initialized with pre-trained Twitter word embeddings from a Skipgram model (Godin et al., 2015) using word2vec (Mikolov et al., 2013). Additionally, we use POS tag embeddings, which are randomly initialized using a uniform distribution. The model receives the concatenation of both POS tags and Twitter word embeddings. The BLSTM layer extracts the features from both forward and backward directions and concatenates the resulting vectors from ~ Following Ma and Hovy each direction ([~h; h])."
W17-4419,W15-4322,0,0.0117701,"Missing"
W17-4419,W16-3927,0,0.0387573,"h tags generated by the CMU Twitter POS tagger (Owoputi et al., 2013). The POS tag embeddings are represented by 100dimensional vectors. In order to capture contextual information, we account for a context window of 3 tokens on both words and POS tags, where the target token is in the middle of the window. We randomly initialize both the character features and the POS tag vectors a uniform dish q usingq i 3 3 tribution in the range − dim , + dim , where dim is the dimension of the vectors from each feature representation (He et al., 2015). Lexical representation: we use gazetteers provided by Mishra and Diesner (2016) to help the model improve its precision for well-known entities. For each word we create a binary vector of 6 dimensions (one dimension per class). Each of the vector dimensions is set to one if the word appears in the gazetteers of the related class. Methodology This section describes our system1 in three parts: feature representation, model description2 , and sequential inference. 3.1 Feature Representation We select features to represent the most relevant aspects of the data for the task. The features are divided into three categories: character, word, and lexicons. Character representatio"
W17-4419,N13-1039,0,0.0411599,"Missing"
W17-4419,N16-1030,0,0.0273628,"Missing"
W17-4419,W09-1119,0,0.227919,"is useful for higher-level Natural Language Processing (NLP) applications such as information extraction, summarization, and data mining (Chen et al., 2004; Banko et al., 2007; Aramaki et al., 2009). Learning Named Entities (NEs) from social media is a challenging task mainly because (i) entities usually represent a small part of limited annotated data which makes the task hard to generalize, and (ii) they do not follow strict rules (Ritter et al., 2011; Li et al., 2012). 2 Related Work Traditional NER systems use hand-crafted features, gazetteers and other external resources to perform well (Ratinov and Roth, 2009). Luo et al. (2015) obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, Lample et al. (2016) use a bidirectional LSTM148 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148–153 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics CRF architecture. They obtain a state-of-theart performance without relying on hand-crafted features. Limsopatham and Colli"
W17-4419,D11-1141,0,0.0615175,"Entity Recognition (NER) aims at identifying different types of entities, such as people names, companies, location, etc., within a given text. This information is useful for higher-level Natural Language Processing (NLP) applications such as information extraction, summarization, and data mining (Chen et al., 2004; Banko et al., 2007; Aramaki et al., 2009). Learning Named Entities (NEs) from social media is a challenging task mainly because (i) entities usually represent a small part of limited annotated data which makes the task hard to generalize, and (ii) they do not follow strict rules (Ritter et al., 2011; Li et al., 2012). 2 Related Work Traditional NER systems use hand-crafted features, gazetteers and other external resources to perform well (Ratinov and Roth, 2009). Luo et al. (2015) obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, Lample et al. (2016) use a bidirectional LSTM148 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148–153 c Copenhagen, Denmark, September 7, 2017. 2017"
W17-4419,W16-3920,0,0.480916,"tinov and Roth, 2009). Luo et al. (2015) obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, Lample et al. (2016) use a bidirectional LSTM148 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148–153 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics CRF architecture. They obtain a state-of-theart performance without relying on hand-crafted features. Limsopatham and Collier (2016), who achieved the first place on WNUT-2016 shared task, use a BLSTM neural network to leverage orthographic features. We use a similar approach but we employ CNN and BLSTM in parallel instead of forwarding the CNN output to the BLSTM. Nevertheless, our main contribution resides on MultiTask Learning (MTL) and a combination of POS tags and gazetteers representation to feed the network. Recently, MTL has gained significant attention. Researchers have tried to correlate the success of MTL with label entropy, regularizers, training data size, and other aspects (Mart´ınez Alonso and Plank, 2017; B"
W17-4419,P16-2038,0,0.0397585,"Missing"
W17-4419,D15-1104,0,0.0223284,"l Natural Language Processing (NLP) applications such as information extraction, summarization, and data mining (Chen et al., 2004; Banko et al., 2007; Aramaki et al., 2009). Learning Named Entities (NEs) from social media is a challenging task mainly because (i) entities usually represent a small part of limited annotated data which makes the task hard to generalize, and (ii) they do not follow strict rules (Ritter et al., 2011; Li et al., 2012). 2 Related Work Traditional NER systems use hand-crafted features, gazetteers and other external resources to perform well (Ratinov and Roth, 2009). Luo et al. (2015) obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, Lample et al. (2016) use a bidirectional LSTM148 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 148–153 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics CRF architecture. They obtain a state-of-theart performance without relying on hand-crafted features. Limsopatham and Collier (2016), who achi"
W17-4419,P16-1101,0,0.0347653,"the success of MTL with label entropy, regularizers, training data size, and other aspects (Mart´ınez Alonso and Plank, 2017; Bingel and Søgaard, 2017). For instance, Collobert and Weston (2008) use a multitask network for different NLP tasks and show that the multi-task setting improves generality among shared tasks. In this paper, we take advantage of the multi-task setting by adding a more general secondary task, NE segmentation, along with the primary NE categorization task. 3 and punctuation patterns. Once we have an encoded word, we represent each character with a 30-dimensional vector (Ma and Hovy, 2016). We account for a maximum length of 20 characters3 per word, applying post padding on shorter words and truncating longer words. Word representation: we have two different representations at the word level. The first one uses pre-trained word embeddings trained on 400 million tweets representing each word with 400 dimensions (Godin et al., 2015)4 . The second one uses Part-of-Speech tags generated by the CMU Twitter POS tagger (Owoputi et al., 2013). The POS tag embeddings are represented by 100dimensional vectors. In order to capture contextual information, we account for a context window of"
W17-4419,E17-1114,1,0.894714,"Missing"
W17-4419,W17-4418,0,\N,Missing
W18-3206,W14-3914,0,0.0967035,"rain a character n-gram based CRF model using the above mentioned three datasets (see Section 5.2) and predict the labels for all the posts crawled from Facebook and the random tweets from Twitter. From these, we identify the posts predicted as code-switched, correct the labels where necessary, and add them to the final dataset. The F1-weighted score for this model is close to 96 percent. beled accordingly. Corpus Creation for Hindi-English. For the HIN-ENG corpus, we consider Facebook pages of prominent public figures from India. HindiEnglish bilingual users are highly active in these pages (Bali et al., 2014). We crawl posts and their comments from the Facebook public pages of various sports-persons, political figures, and movie stars. We also crawl random tweets from geographical locations Mumbai and Delhi using the Twitter API. From the crawled posts, we remove the posts in native scripts, and remove duplicate and promotional posts. We filter the posts containing URLs and those with less than 3 words. Language Pair Pair SPA-ENG HIN-ENG Tweets (Posts) 25,130 7,421 Tokens 294,261 146,722 4 In this section we provide some descriptive statistics about the corpora to understand the language distribut"
W18-3206,W14-3902,0,0.0756029,"Missing"
W18-3206,W16-5802,0,0.0510673,"Missing"
W18-3206,D12-1039,0,0.0208046,", we generate character n-grams of length 1 to 5 and filter them based on a minimum threshold frequency of 5. To capture the morphological information of the tokens, we use binary features - is digit, is special character, is all capital, is title case, begins with @ character, has accent character (for SPA-ENG only) and has apostrophe. We also use language dependent resources like lexicons and monolingual parts-of-speech (POS) taggers. For HIN-ENG, we use three different lexicons - Leipzig corpus for English, FIRE 2013 transliterated Hindi word pairs, and lexically normalized dictionary from Han et al. (2012) and the output of Twitter POS tagger and CRF++ Table 5: CS Metrics for the datasets. SPA-ENG has higher M-Index (Table 5) value indicating a balanced ratio of words from the two languages. This is consistent with the distribution of language words in the datasets (Table 2). The differences in CMI-all between 55 based Hindi POS tagger.4,5 For SPA-ENG, we use Leipzig corpus Spanish along with the other two lexicons mentioned above and the output from monolingual TreeTaggers for Spanish and English.6 Bidirectional LSTM: Long Short Term Memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997) a"
W18-3206,W14-5152,0,0.087696,"Missing"
W18-3206,W15-1608,1,0.903416,"Missing"
W18-3206,W16-5805,1,0.867523,"putational modeling of code-switched data. Language identification is important for a wide variety of end user applications such as information extraction systems, voice assistant interfaces, machine translation, as well as for tools to assist language assessment in bilingual children (Gupta et al., 2014; Chandu et al., 2017; Roy et al., 2013). Language detection, in addition, enables sociolinguistics and pragmatic studies of code-switching behavior. Code-switching in speech is well studied in Table 1: Example 1 shows code-switching between Hindi-English and Example 2 between Spanish-English (Molina et al., 2016). Word level language identification of codeswitched text is inherently difficult. First, a single code-switched instance can have mixing at the sentence or clause level, the word level, and even at the sub-word level (e.g. sir-ji, chapathis). Second, the typology of the languages involved in switching and their inter-relatedness further increase the task complexity. For example, a shared Latin influence on Spanish and English results in lexical relatedness (Smith, 2001; August et al., 2002), making Spanish-English language identification harder than Hindi-English. Third, in spite of the fact"
W18-3206,W14-5151,0,0.071175,"Missing"
W18-3206,W16-5806,1,0.954979,"he language detection task as a sequence labeling problem and explore combinations of several features using the CRF model, but we use a larger set of labels. We obtain significantly higher performance for the Hindi-English language pair than Das and Gamb¨ack (2014). Along with the traditional machine learning approach, some researchers have also used models based on artificial neural networks. Chang and Lin (2014) use an RNN architecture with pre-trained word2vec embeddings for SPA-ENG and the Nepali-English datasets from the First Shared Task on Language Identification in CodeSwitched Data. Samih et al. (2016) build an LSTM based neural network architecture for SPAENG and MSA-DA datasets from the Second Shared Task on Language Identification in CodeSwitched Data. Their model combines word and character representations initialized with pretrained word2vec embeddings. We replicate their model with softmax output layer for SPA-ENG and run similar experiments for HIN-ENG, as well as with both the corpora combined. Our result for SPA-ENG match that of Samih et al. (2016). transliterated. Transliteration is conversion of a text from one script to another. In the case of Hindi, text is converted from nati"
W18-3206,W15-5936,0,0.135377,"Missing"
W18-3206,W14-3907,1,0.921682,"Missing"
W18-3219,N18-1127,1,0.877746,"Missing"
W18-3219,L16-1669,1,0.92618,"Missing"
W18-3219,W17-4419,1,0.850073,"e NE tokens in MSA-EGY dataset is higher than the percentage of the NE tokens in ESP-ENG dataset. 4 Approaches In this section, we briefly describe the systems of the participants and discuss their results as well as the final scores. • IIT BHU (Trivedi et al., 2018). They proposed a “new architecture based on gating of character- and word-based representation of a token”. They captured the character and the word representations using a CNN and a bidirectional LSTM, respectively. They also used the Multi-Task Learning on the output layer and transfer the learning to a CRF classifier following Aguilar et al. (2017). Moreover, they fed a gazetteers representation to their model. Figure 3: MSA-EGY Data Annotation (i.e., URL, Punctuation, Number, etc) in addition to named entities. Then, we extracted and prepared all the tweets that contained “ne” for annotation. As we mentioned earlier, the IOB scheme is used as an annotation scheme to identify multiple words as a single named entity. All the URLs, Punctuation and Numbers tags are deterministically converted to “O” tag, while the tweets that include “ne” tags were given to our in-lab annotators for validation and re-annotation if needed. . Quality checks"
W18-3219,W18-3217,0,0.0703829,"are the ones described in Table 3. As stated by (Derczynski et al., 2014), the idea of the Surface Form F1-score is to capture the novel and emerging aspects that are usually encountered in social media data. Those aspects describe a fast-moving language that constantly produces new entities challenging more the recall capabilities of state-of-the-art models than the precision side. They fed the CRF with features from both external and internal resources. Additionally, they incorporated the language identification labels of the datasets from the previous versions of this workshop. • semantic (Geetha et al., 2018). They jointly trained a Bidirectional LSTM with a Conditional Random Fields on the output layer. • BATs (Janke et al., 2018). They used a Conditional Random Fields with multiple features. Some of those features were also used for neural network, but they got better results with the CRF approach. • Fraunhofer FKIE (Claeser et al., 2018). They used a Support Vector Machine (SVM) classifier with a Radial Basis kernel. They handcrafted a lot of features and also included gazetteers. 5.2 Although all the scores reported by the participants outperformed the baselines in both ENGSPA and MSA-EGY lang"
W18-3219,K15-1005,1,0.918003,"Missing"
W18-3219,W18-3213,0,0.0614809,"Missing"
W18-3219,W16-5812,1,0.89568,"Missing"
W18-3219,W18-3212,0,0.0472178,"Missing"
W18-3219,N16-1030,0,0.203476,"Missing"
W18-3219,W15-3116,0,0.0485213,"Missing"
W18-3219,P16-1101,0,0.120177,"Missing"
W18-3219,W16-5805,1,0.914454,"Missing"
W18-3219,W16-5803,0,0.0287502,"Missing"
W18-3219,W18-3220,0,0.0799552,"Missing"
W18-3219,D11-1141,0,0.0941713,"nts as a title. This is an example of what we refer to heterogeneous entity type, mean• Clitic attachment can obscure tokens, e.g. ¢l ¤ wAllh “and-God” or ”swear”. • Clitic attachment can obscure tokens, e.g. Yn¤ wmnY “and-Mona” or ”swear”. 144 N 1 MSA-EGY Samples Buckwalter Encoding:[wAllh]PER OnA HAss bqhr In [ElA’ Ebd AlftAH]PER [wmnY]PER [syf ]PER bytHAkmwA wfy AlqfS Arabic: º®  rhq HFA A ¢l ¤ years (Sang and Meulder, 2003). More recently, however, the focus has drastically moved to social media data due to the great incidence that social networks have in our daily communication (Ritter et al., 2011; Augenstein et al., 2017). The workshop on Noisy User-generated Text (W-NUT) has been a great effort towards the study of named entity recognition on noisy data. In 2016, the organizers focused on named entities from different topics to evaluate the adaptation of models from one topic to another (Strauss et al., 2016). In 2017, the organizers introduced the Surface Form F1-score metric and collected data from multiple social media platforms (Derczynski et al., 2014). The challenge not only lies on the entity types and the social media noisy but also in the distribution of the datasets and the"
W18-3219,W15-2902,0,0.15871,"Missing"
W18-3219,W03-0419,0,0.626032,"Missing"
W18-3219,L16-1655,0,0.0740953,"Missing"
W18-3219,W18-3215,0,0.0617212,"Missing"
W18-3219,W18-3221,0,0.175264,"Missing"
W18-3219,W14-3907,1,0.913831,"Missing"
W18-3219,W18-3214,0,0.187561,"Missing"
W18-3219,D08-1102,1,0.830788,"Missing"
W18-3219,D08-1110,1,0.798454,"Missing"
W18-4402,W18-4401,0,0.0605235,"Missing"
W18-4402,L18-1226,0,0.18822,"measures of 0.5921 for the English Facebook task (ranked 12th), 0.5663 for the English Social Media task (ranked 6th), 0.6292 for the Hindi Facebook task (ranked 1st), and 0.4853 for the Hindi Social Media task (ranked 2nd). 1 Introduction Users’ activities on social media is increasing at a fast rate. Unfortunately, a lot of people misuse these online platforms to harass, threaten, and bully other users. This growing aggression against social media users has caused serious effects on victims, which can even lead them to harm themselves. The TRAC 2018 Shared Task on Aggression Identification (Kumar et al., 2018a) aims at developing a classifier that could make a 3-way classification of a given data instance between “Overtly Aggressive”, “Covertly Aggressive”, and “Non-aggressive”. We present here the different systems we submitted to the shared task, which mainly use lexical and semantic features to distinguish different levels of aggression over multiple datasets from Facebook and other social media that cover both English and Hindi texts. 2 Related Work In recent years, several studies have been done towards detecting abusive and hateful language in online texts. Some of these works target differe"
W18-4402,W18-3206,1,0.882046,"Missing"
W18-4402,W17-3010,1,0.894058,"Missing"
W18-4402,D14-1121,0,0.0189965,"tribution over all sentences and use them as feature vector. LIWC (Linguistic Inquiry and Word Count): LIWC2007 (Pennebaker et al., 2007) includes around 70 word categories to analyze different language dimensions. In our approach, we only use the categories related to positive or negative emotions and self-references. To build the feature vectors in this case, we use a normalized count of words separated by any of the mentioned categories. This feature is only applicable to English data. Gender Probability: Following the approach in Waseem (2016) we use the Twitter based lexicon presented in Sap et al. (2014) to calculate the probability of gender. We also convert these probabilities to binary gender by considering the positive cases as female and the rest as male. We make the feature vectors with the probability of the gender and binary gender for each message. This feature is not applicable to Hindi corpus. 4 Experiments and Results 4.1 Experimental Settings For both datasets, we trained several classification models using different combinations of features discussed in 3.3. Since this is a multi-class classification task, we use a one-versus-rest classifier which trains a separate classifier fo"
W18-4402,D13-1170,0,0.00583489,"Missing"
W18-4402,R15-1086,0,0.144805,"Missing"
W18-4402,N16-2013,0,0.152232,"Missing"
W18-4402,W16-5618,0,0.0237403,"e calculate the mean and standard deviation of sentiment distribution over all sentences and use them as feature vector. LIWC (Linguistic Inquiry and Word Count): LIWC2007 (Pennebaker et al., 2007) includes around 70 word categories to analyze different language dimensions. In our approach, we only use the categories related to positive or negative emotions and self-references. To build the feature vectors in this case, we use a normalized count of words separated by any of the mentioned categories. This feature is only applicable to English data. Gender Probability: Following the approach in Waseem (2016) we use the Twitter based lexicon presented in Sap et al. (2014) to calculate the probability of gender. We also convert these probabilities to binary gender by considering the positive cases as female and the rest as male. We make the feature vectors with the probability of the gender and binary gender for each message. This feature is not applicable to Hindi corpus. 4 Experiments and Results 4.1 Experimental Settings For both datasets, we trained several classification models using different combinations of features discussed in 3.3. Since this is a multi-class classification task, we use a"
