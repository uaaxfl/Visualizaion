2020.acl-main.268,N18-1172,0,0.0182567,"lar to our approach is the work of Bjerva (2017), who estimates the effect of an auxiliary task in MTL with information-theoretic measures. As the method requires the same datasets to be tagged with multiple tasks in parallel, at least one task must be automatically taggable with almost perfect results. He shows a correlation of conditional entropy and mutual information with a change in accuracy compared to STL. Results on the semantic task of Bjerva et al. (2016); Mart´ınez Alonso and Plank (2017) indicate that mutual information for helpful auxiliary tasks is higher than for harmful tasks. Augenstein et al. (2018) propose an architecture that learns label embeddings for natural language classification tasks and find that label embeddings indicate gains or harms of MTL. Ruder et al. (2019) correlate task properties with performance differences and learned meta-network parameters of their proposed sluice networks. They find that MTL gains are higher for smaller training datasets and that sluice networks learn to share more in case of higher variance in the training data. Opposed to previous approaches, our methods can compare same-task datasets and are not restricted to datasets with parallel labels. As"
2020.acl-main.268,P17-2054,0,0.0892573,"om/uhh-lt/seq-tag-sim 2 2.1 Related work Neural multi-task and transfer learning Multi-task learning (MTL) is a technique to learn multiple tasks jointly (Caruana, 1997). Depending on the setting, either all tasks are equally important, or only the performance on the main task is of interest, which shall be improved with additional training data. MTL has been successfully applied in natural language processing for various sequence tagging tasks (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Mart´ınez Alonso and Plank, 2017; Kaiser et al., 2017; Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Kim et al., 2017; Yang et al., 2017; Changpinyo 2971 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2971–2985 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2018; Liu et al., 2018; Schulz et al., 2018). These approaches use hard parameter sharing in the hidden layers of neural learning architectures, where the same weights are updated from several tasks. The majority of works combined a main task with a single, supervised auxiliary task. In transfer learning, a model is pre-trained on an auxiliary dataset to increas"
2020.acl-main.268,C18-1251,0,0.114924,"Missing"
2020.acl-main.268,W09-3302,0,0.103974,"Missing"
2020.acl-main.268,D14-1179,0,0.0145369,"Missing"
2020.acl-main.268,benikova-etal-2014-nosta,1,0.891453,"Missing"
2020.acl-main.268,D17-1302,0,0.139462,"lated work Neural multi-task and transfer learning Multi-task learning (MTL) is a technique to learn multiple tasks jointly (Caruana, 1997). Depending on the setting, either all tasks are equally important, or only the performance on the main task is of interest, which shall be improved with additional training data. MTL has been successfully applied in natural language processing for various sequence tagging tasks (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Mart´ınez Alonso and Plank, 2017; Kaiser et al., 2017; Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Kim et al., 2017; Yang et al., 2017; Changpinyo 2971 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2971–2985 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2018; Liu et al., 2018; Schulz et al., 2018). These approaches use hard parameter sharing in the hidden layers of neural learning architectures, where the same weights are updated from several tasks. The majority of works combined a main task with a single, supervised auxiliary task. In transfer learning, a model is pre-trained on an auxiliary dataset to increase the main task pe"
2020.acl-main.268,P15-1046,0,0.029919,"k performance. In contrast, Mart´ınez Alonso and Plank (2017) show results that auxiliary tasks with few labels and a uniform label distribution perform better for MTL in neural sequence tagging: Auxiliary tasks having many labels or high entropy harm the main task performance. While Ruder et al. (2019) confirm these findings, Bjerva (2017) found no evidence of label entropy correlating with MTL performance. Mart´ınez Alonso and Plank (2017) found a difference between two POS datasets when used as auxiliary data because converting one to another tagset changes the effect of MTL significantly. Kim et al. (2015) propose a method using label embeddings to map labels from auxiliary datasets to the target tagset so that MTL can be treated as single-task learning (STL) with an increased amount of training data. Bingel and Søgaard (2017) predict MTL performance from dataset and STL learning features and found the learning curve to be much more important. From the dataset features, the number of labels on the main task and the auxiliary label entropy showed predictive potential. Most similar to our approach is the work of Bjerva (2017), who estimates the effect of an auxiliary task in MTL with information-"
2020.acl-main.268,C18-1176,0,0.021614,"Missing"
2020.acl-main.268,U15-1010,0,0.0355209,"Missing"
2020.acl-main.268,N18-2006,0,0.493557,"rest, which shall be improved with additional training data. MTL has been successfully applied in natural language processing for various sequence tagging tasks (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Mart´ınez Alonso and Plank, 2017; Kaiser et al., 2017; Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Kim et al., 2017; Yang et al., 2017; Changpinyo 2971 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2971–2985 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2018; Liu et al., 2018; Schulz et al., 2018). These approaches use hard parameter sharing in the hidden layers of neural learning architectures, where the same weights are updated from several tasks. The majority of works combined a main task with a single, supervised auxiliary task. In transfer learning, a model is pre-trained on an auxiliary dataset to increase the main task performance. Howard and Ruder (2018) showed knowledge transfer based on large-scale language modeling. Before the breakthrough with BERT (Devlin et al., 2019), only partial knowledge transfer via word embeddings such as word2vec (Mikolov et al., 2013) or ELMo (Ili"
2020.acl-main.268,silveira-etal-2014-gold,0,0.0778743,"Missing"
2020.acl-main.268,P16-2038,0,0.114643,"of different auxiliary datasets, hyperparameter search and training runs with multiple random seeds have to be performed for each auxiliary 1 github.com/uhh-lt/seq-tag-sim 2 2.1 Related work Neural multi-task and transfer learning Multi-task learning (MTL) is a technique to learn multiple tasks jointly (Caruana, 1997). Depending on the setting, either all tasks are equally important, or only the performance on the main task is of interest, which shall be improved with additional training data. MTL has been successfully applied in natural language processing for various sequence tagging tasks (Søgaard and Goldberg, 2016; Bjerva et al., 2016; Plank et al., 2016; Mart´ınez Alonso and Plank, 2017; Kaiser et al., 2017; Bingel and Søgaard, 2017; Augenstein and Søgaard, 2017; Kim et al., 2017; Yang et al., 2017; Changpinyo 2971 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2971–2985 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2018; Liu et al., 2018; Schulz et al., 2018). These approaches use hard parameter sharing in the hidden layers of neural learning architectures, where the same weights are updated from several tasks. The majority"
2020.acl-main.268,J17-3005,0,0.0287312,"Missing"
2020.acl-main.268,W03-0419,0,0.486655,"Missing"
2020.cogalex-1.1,P01-1005,0,0.190796,"Missing"
2020.cogalex-1.1,N19-1423,0,0.0126671,"Catell’s (1943, p. 157) challenge of “freeing adult tests from assumptions of uniform knowledge“. For future work, we would like to proceed in three directions. First, we would like to improve the collection procedure: the corpora collected via screenshots and OCR contain a high number of artifacts stemming from non-textual material, as well as non-contiguous texts as a result from complex webpage layouts. Second, we like to increase the number of participants in future studies. Third, it would be interesting to compare our word2vec results with more recent contextual embeddings such as BERT (Devlin et al., 2019), which have been shown to achieve better performance across a wide range of natural language processing tasks than language models with static word embeddings. While it is nontrivial to use BERT’s bi-directional architecture and its masking mechanism for language modelling tasks, Salazar et al., (2020) have recently shown how to obtain prediction values for BERT and other architectures trained with masking loss. Subword representations as used in BERT may also help to compensate OCR-based errors, when only a few letters have been falsely recognized. On the downside, it is questionable whether"
2020.cogalex-1.1,goldhahn-etal-2012-building,0,0.427195,"stages of memory (e.g. Paller and Wagner, 2002). All memories start with experience, which is reflected by text corpora (e.g. Hofmann et al., 2018). The training of a language model then reflects the process of memory consolidation. The final stage is memory retrieval, which can be examined in psycholinguistic experiments. In this paper, we measure the correlation of computational language modelling and cognitive performance. We collected individual corpora from two participants reading on a tablet for two months and compared them to an extensive corpus mainly consisting of online newspapers (Goldhahn et al., 2012). To consolidate differential knowledge structures in long-term memory, word2vec models were trained from these corpora. For stimulus selection, we relied on these three language models to compute word probabilities and sentence perplexity scores for 45K sentences of a Wikipedia dump. Perplexity rank differences were used to select sentences with uncorrelated word probabilities for the three language models, allowing to estimate the independent contribution of the word probabilities in multiple regression analyses. The resulting 134 stimulus sentences were read by the participants in an eye tr"
2020.cogalex-1.1,C14-1200,0,0.349148,"and memory consolidation Norm corpora as a representative sample of human experience? When selecting a corpus as a sample of the learning experience of human participants or language models, the question arises which corpus is most representative for which person. For instance, the knowledge of young adults is better characterized by corpora consisting of books written for younger adults, while older adults are more experienced – when therefore searching for corpora that account for their performance best, more diverse fictional and literature books are chosen (Johns, Jones, & Mewhort, 2018). Rapp (2014) proposed that corpus representativeness should be measured by the Pearson correlation of corpus-derived computational measures and an external measure of human performance. The knowledge of an average reader may be well represented by balanced corpora containing all sorts of content, such as Wikipedia. However, a previous study revealed that a newspaper corpus often provides higher correlations than Wikipedia when accounting for human cloze completion probabilities, as well as eye tracking or brain-electric data (Goldhahn et al., 2012; Hofmann et al., 2017). This result pattern could on the o"
2020.coling-main.91,N19-4010,0,0.0233884,"ing representation. BERT-like models have an advantage over static embeddings as they can accommodate different embedding representation for the same word based on its context. In this task, we have used RoBERTa (A Robustly Optimized BERT Pre-training Approach), which is a replication of BERT developed by Facebook (Liu et al., 2019). Unlike BERT, RoBERTa removed the ’next sentence prediction’ functionality, allowing training on longer sequences, and dynamically changing the masking patterns. We also train and fine-tune contextual embedding models using the FLAIR framework (Akbik et al., 2018; Akbik et al., 2019) Document Embedding: Unlike word embeddings, document embeddings provide a single embedding for the entire text (sentence, paragraph, or the entire document). The FLAIR framework has document embeddings implementations, such as ‘DocumentPoolEmbeddings, which produces document embeddings from pooled word embeddings, and ‘DocumentLSTMEmbeddings, which provides document embeddings from LSTM based on word embeddings (Akbik et al., 2019). 5 Results and Discussions Table 3 shows the experimental results based on the baseline, supervised, and deep learning models. As we can see in the table, both the"
2020.coling-main.91,L16-1429,0,0.028038,"The most widely adopted approach in sentiment analysis to explore opinions is by employing very large datasets that target products and services, political, economical, social, and cultural feelings (Kauffmann et al., 2019; Caetano et al., 2018; Lennox et al., 2020). Understanding the sentiment of text content helps governments, organizations, and institutions to make correct, timely, and economical decisions (De Souza Bermejo et al., 2019). Sentiment analysis has been researched intensively for resource-rich languages such as English and German (Liu, 2012; Feldman, 2013; Tymann et al., 2019; Akhtar et al., 2016; D’Andrea et al., 2015; Wojatzki et al., 2017). However, existing models and approaches for most resource-rich languages can not easily be adapted to Amharic due to context variations in language, culture, and technology, especially for social media communication (Gangula and Mamidi, 2018). The works by Gezmu et al. (2018) and Abate and Assabie (2014) indicate that natural language processing (NLP) components, such as part of speech tagging (POS), named entity recognition (NER), and sentiment analysis are nontrivial due to the morphological, syntactic, and semantic complexity of the language."
2020.coling-main.91,N19-1423,0,0.009801,"roving over time as newer algorithms, better hardware infrastructure, and above all, a substantially large amount of free texts are being generated (Torfi et al., 2020). Unlike high-resource languages such as English https://scrapy.org/ https://developers.google.com/youtube/v3 7 https://developer.twitter.com/en 5 6 1054 and German, the impacts, limitations, and perspectives of using deep learning models in sentiment analysis for low-resource languages, particularly for Amharic, is not yet exploited. In this work, three types of embeddings, namely static (Mikolov et al., 2013), contextualized (Devlin et al., 2019), and network (Hamilton et al., 2017) embeddings are considered to build different deep learning models for the sentiment classification. Word2Vec: Word2vec helps to learn word representations (word embeddings) that employ a two-layer neural network architecture (Mikolov et al., 2013). Embeddings can be computed using a large set of texts as an input to the neural network architecture. We have used the Gensim Python Library (Řehůřek and Sojka, 2011) to train the embeddings using the default parameters. Network embeddings: Network embeddings allow representing nodes in a graph in the form of lo"
2020.coling-main.91,L18-1100,0,0.110649,"sentiment of text content helps governments, organizations, and institutions to make correct, timely, and economical decisions (De Souza Bermejo et al., 2019). Sentiment analysis has been researched intensively for resource-rich languages such as English and German (Liu, 2012; Feldman, 2013; Tymann et al., 2019; Akhtar et al., 2016; D’Andrea et al., 2015; Wojatzki et al., 2017). However, existing models and approaches for most resource-rich languages can not easily be adapted to Amharic due to context variations in language, culture, and technology, especially for social media communication (Gangula and Mamidi, 2018). The works by Gezmu et al. (2018) and Abate and Assabie (2014) indicate that natural language processing (NLP) components, such as part of speech tagging (POS), named entity recognition (NER), and sentiment analysis are nontrivial due to the morphological, syntactic, and semantic complexity of the language. The absence of well-annotated corpora and NLP resources like parsers and taggers make Amharic sentiment analysis still challenging (Gezmu et al., 2018; Pandey and Govilkar, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/lice"
2020.coling-main.91,W18-3809,0,0.0229321,"ents, organizations, and institutions to make correct, timely, and economical decisions (De Souza Bermejo et al., 2019). Sentiment analysis has been researched intensively for resource-rich languages such as English and German (Liu, 2012; Feldman, 2013; Tymann et al., 2019; Akhtar et al., 2016; D’Andrea et al., 2015; Wojatzki et al., 2017). However, existing models and approaches for most resource-rich languages can not easily be adapted to Amharic due to context variations in language, culture, and technology, especially for social media communication (Gangula and Mamidi, 2018). The works by Gezmu et al. (2018) and Abate and Assabie (2014) indicate that natural language processing (NLP) components, such as part of speech tagging (POS), named entity recognition (NER), and sentiment analysis are nontrivial due to the morphological, syntactic, and semantic complexity of the language. The absence of well-annotated corpora and NLP resources like parsers and taggers make Amharic sentiment analysis still challenging (Gezmu et al., 2018; Pandey and Govilkar, 2015). This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: htt"
2020.coling-main.91,N18-1043,0,0.0203782,"arge set of texts as an input to the neural network architecture. We have used the Gensim Python Library (Řehůřek and Sojka, 2011) to train the embeddings using the default parameters. Network embeddings: Network embeddings allow representing nodes in a graph in the form of low dimensional representation (embeddings) to maintain the relationship of nodes (Hamilton et al., 2017; Sevgili et al., 2019; Cai et al., 2018). In this paper, we first compute the network-based distributional thesaurus (DT) (Ruppert et al., 2015) and later convert the DT to a network embeddings following the approach by Jana and Goyal (2018). Contextual embeddings: With the release of Google’s Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019), word representation strategies have shifted from the traditional static embeddings to a contextualized embedding representation. BERT-like models have an advantage over static embeddings as they can accommodate different embedding representation for the same word based on its context. In this task, we have used RoBERTa (A Robustly Optimized BERT Pre-training Approach), which is a replication of BERT developed by Facebook (Liu et al., 2019). Unlike BERT, RoB"
2020.coling-main.91,S13-2052,0,0.0287051,"omplete’ tweets also need more background information as well as knowledge of other languages. For instance, in Table 2, the first ‘Mixed-Script’ example represents a tweet, which is an English sentence transliterated in ‘Fidel’ script. 3 Related Works According to Liu (2012), opinion mining is a field of study that analyzes people’s opinions, sentiments, evaluations, attitudes, and emotions from written language. It can be asserted that the expansion of digital technology and the volume of data made available by such technologies affects trends of sentiment analysis task. The work by Feldman (2013) differentiates sentiment analysis into four levels. Document-Level Sentiment Analysis: This is the simplest form of sentiment analysis and it is assumed that the document contains an opinion on one main object expressed by the author of the document. Sentence-Level Sentiment Analysis: A single document may contain multiple opinions even about the same entities. When we want to have a more generalized view of the different opinions expressed in the document about the entities, we must move to the sentence level. Aspect-Based Sentiment Analysis: The above methods work when whether the whole doc"
2020.coling-main.91,P15-4018,1,0.829868,"wo-layer neural network architecture (Mikolov et al., 2013). Embeddings can be computed using a large set of texts as an input to the neural network architecture. We have used the Gensim Python Library (Řehůřek and Sojka, 2011) to train the embeddings using the default parameters. Network embeddings: Network embeddings allow representing nodes in a graph in the form of low dimensional representation (embeddings) to maintain the relationship of nodes (Hamilton et al., 2017; Sevgili et al., 2019; Cai et al., 2018). In this paper, we first compute the network-based distributional thesaurus (DT) (Ruppert et al., 2015) and later convert the DT to a network embeddings following the approach by Jana and Goyal (2018). Contextual embeddings: With the release of Google’s Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019), word representation strategies have shifted from the traditional static embeddings to a contextualized embedding representation. BERT-like models have an advantage over static embeddings as they can accommodate different embedding representation for the same word based on its context. In this task, we have used RoBERTa (A Robustly Optimized BERT Pre-training App"
2020.coling-main.91,sabou-etal-2014-corpus,0,0.0423508,"Missing"
2020.coling-main.91,P19-2044,1,0.816867,"for the sentiment classification. Word2Vec: Word2vec helps to learn word representations (word embeddings) that employ a two-layer neural network architecture (Mikolov et al., 2013). Embeddings can be computed using a large set of texts as an input to the neural network architecture. We have used the Gensim Python Library (Řehůřek and Sojka, 2011) to train the embeddings using the default parameters. Network embeddings: Network embeddings allow representing nodes in a graph in the form of low dimensional representation (embeddings) to maintain the relationship of nodes (Hamilton et al., 2017; Sevgili et al., 2019; Cai et al., 2018). In this paper, we first compute the network-based distributional thesaurus (DT) (Ruppert et al., 2015) and later convert the DT to a network embeddings following the approach by Jana and Goyal (2018). Contextual embeddings: With the release of Google’s Bidirectional Encoder Representations from Transformer (BERT) (Devlin et al., 2019), word representation strategies have shifted from the traditional static embeddings to a contextualized embedding representation. BERT-like models have an advantage over static embeddings as they can accommodate different embedding representa"
2020.coling-main.91,E12-2021,0,0.0529048,"t the communication with our server respectively. The user interface of ASAB as it can be seen on the user’s mobile device is depicted in Figure 1c. ASAB is designed to support rewards (in the form of mobile card vouchers) as soon as the user successfully annotated enough tweets. After conducting a pilot study, the number of tweets to annotate and get a reward was set at 50. When the worker completes the task, the voucher will be displayed instantly to the user. In general, controlling the quality of the annotated data by blocking bad workers or spammers is crucial on crowdsourcing platforms (Stenetorp et al., 2012; Hovy and Lavid, 2010). The chatbot-based annotation is much more restrictive, mainly designed with built-in control mechanisms to assure annotation quality. ASAB integrates a controlling strategy in the form of control questions. For every 6 tweets, we have included one control question with a known answer. Users who have made 3 consecutive mistakes will receive a warning message. If the user still keeps on randomly annotating the tweets, he/she will be blocked after the fourth attempt. Another challenge in the crowdsourcing annotation framework is the preparation of concise annotation instr"
2020.lrec-1.722,K18-1022,0,0.0587655,"Missing"
2020.lrec-1.722,bird-etal-2008-acl,0,0.119488,"Missing"
2020.lrec-1.722,J08-4005,0,0.0564611,"Missing"
2020.lrec-1.722,N19-1423,0,0.00750394,"compilation process. Similarly, some of the errors from the system’s prediction are to be attributed to the annotation process of the test set. For example, in the sentence ”They included support for marine reserves and money for fisheries management reform.”, reserves is annotated as informal while the system identified it as formal. In general, while bootstrapping the academic resource compilation and the informal word identification tasks, a minimal intervention of human annotators would enhance the overall system. Furthermore, integration of a BERT or other contextualized embedding model (Devlin et al., 2019) could also help to improve the performance of the system. Contextualized word embeddings provide word vector representations based on their context. As the vector representation of words varies as per the context, they implicitly provide a model for word sense disambiguation (WSD). 7 Conclusion and Future Direction In the realm of academic text writing, we explored how to compile academic resources, automatically identify informal words (words that are less formal for academic writing), and provide better substitutes. We have used a generic approach to compile the academic resources, which ca"
2020.lrec-1.722,D17-1091,0,0.0341901,"Missing"
2020.lrec-1.722,L18-1039,0,0.0336358,"Missing"
2020.lrec-1.722,C18-1039,0,0.0173605,"nd Section 7 respectively. 5896 2 Previous Work In this section, we review previous work in lexical substitution, a closely related task, and discuss how the academic text rewriting system potentially differs. In essence, our system is similar to lexical substitution (LS) and text simplification tasks, in such a way that both focus on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the l"
2020.lrec-1.722,D18-1541,0,0.0326254,"Missing"
2020.lrec-1.722,E14-1057,0,0.261959,"Missing"
2020.lrec-1.722,W18-6706,1,0.750418,"arson1 . However, these resources are 1) limited to a certain domain and target writers (mostly L2 learners and students), 2) their vocabulary is fixed, thus requiring manual work for an extension, and 3) the resources are limited to uni-gram and bi-gram lists. In this work, we build academic resources that are more generic, which can be built from existing reference corpora. In addition to uni-gram and bi-gram resources, we also design a system that can produce resources up to a length of four words (quad-grams). As far as we know, the only system available to academic writing is the work of Lee et al. (2018), which addresses a different aspect, which is a sentence restructuring based on nominalizing verbal expressions. 3 Building Academic Resources In this section, we will first discuss the existing academic resources, how they are built and their limitations. Then, we will present our approach that describes the process of building academic resources from different reference corpora. Finally, we will discuss the quality of the collected resources against two evaluation measures, namely comparing with the existing resources and manually evaluating the academic fitness of resources. 1 Academic col"
2020.lrec-1.722,S07-1009,0,0.166276,"Missing"
2020.lrec-1.722,P16-2024,0,0.0646835,"Missing"
2020.lrec-1.722,P15-2070,0,0.155618,"Missing"
2020.lrec-1.722,D14-1162,0,0.0836199,"Missing"
2020.lrec-1.722,D14-1066,0,0.0193169,"2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the lexical substitution task, particularly for medical documents. They have relied on Distributional Thesaurus (DT), computed on medical texts to generate synonyms for target words. Existing resources for academic writing are limited to a precompiled list of words such as the Corpus Of Contemporary American English (COCA) (Gardner and Davies, 2013) and the New Academic Word List 1.0 (NAWL) (Browne et al., 2013) vocabulary lists. Regarding phrases (multiword expressions) for academic writing, the only available resources are the academic bi-grams compiled by Pearson1 . However, the"
2020.lrec-1.722,P15-4018,1,0.88784,"Missing"
2020.lrec-1.722,W17-5703,0,0.0432254,"Missing"
2020.lrec-1.722,C18-3005,0,0.111521,"cademic resources might vary from domain to domain as some words or phrases are extensively used in one domain over the other. The first step in building an academic writing aid tool is to collect resources that determines whether a given phrase follows the style of writing in academia. This involves analyzing a given sentence and determining if the lexemes of the sentences are well-selected academic words and phrases or not. To evaluate the resources compiled, we have to build a system, analogous to the lexical substitution and text simpliˇ fication tasks, for example, (Szarvas et al., 2013; Stajner and Saggion, 2018), that consists of informal word identification, academic candidate generation, and candidate paraphrase ranking components (see Figure 1). While it is possible to follow the same approaches as the lexical substitution and text simplification approaches for academic text rewriting tasks, the main challenge for the academic paraphrasing task is the collection of resources for academic texts. The following are the main objectives of building academic resources: 1. Identify suitable academic and non-academic datasets that are to be used to build academic resources. 2. Design a generic, domain-ind"
2020.lrec-1.722,N13-1133,1,0.906611,"style. Moreover, the academic resources might vary from domain to domain as some words or phrases are extensively used in one domain over the other. The first step in building an academic writing aid tool is to collect resources that determines whether a given phrase follows the style of writing in academia. This involves analyzing a given sentence and determining if the lexemes of the sentences are well-selected academic words and phrases or not. To evaluate the resources compiled, we have to build a system, analogous to the lexical substitution and text simpliˇ fication tasks, for example, (Szarvas et al., 2013; Stajner and Saggion, 2018), that consists of informal word identification, academic candidate generation, and candidate paraphrase ranking components (see Figure 1). While it is possible to follow the same approaches as the lexical substitution and text simplification approaches for academic text rewriting tasks, the main challenge for the academic paraphrasing task is the collection of resources for academic texts. The following are the main objectives of building academic resources: 1. Identify suitable academic and non-academic datasets that are to be used to build academic resources. 2."
2020.lrec-1.722,N03-1033,0,0.138259,"Missing"
2020.lrec-1.722,C18-1028,1,0.881095,"Missing"
2020.lrec-1.722,W16-1801,1,0.862553,"ions. Pearson has published a set of academic bi-grams7 . Words like best, almost, and way are not by themselves academic, but they can be combined with other 6 http://www.newgeneralservicelist.org/ nawl-new-academic-word-list 7 Academic collocation list: https://pearsonpte. com/organizations/resea 4.3.2 Paraphrase Generation, Selection, and Ranking Given an informal word, this step generates a list of substitution candidates. While there are different approaches to generate candidates for target words, such as using existing paraphrase resources like WordNet and Distributional thesaurus (see Yimam et al. (2016)), we depend solely on the CoInCo (Kremer et al., 2014), WordNet (Miller, 1995), and the paraphrase database (PPDB) (Pavlick et al., 2015) resources to generate candidates. Once the candidates are generated, all of the candidates, which must be academic words are retained for the paraphrase ranking component. Given a list of academic substitution candidates, the paraphrase ranking component finds the one that fits best in the context. The detailed approach is presented in Section 4.4. 5899 Figure 2: Architecture of the system. CoInCo annotation IWI dataset Pacific First Financial Corp said[par"
2020.lrec-1.722,I17-2068,1,0.904094,"Missing"
2020.lrec-1.722,yimam-etal-2017-multilingual,1,0.933203,"on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the lexical substitution task, particularly for medical documents. They have relied on Distributional Thesaurus (DT), computed on medical texts to generate synonyms for target words. Existing resources for academic writing are limited to a precompiled list of words such as the Corpus Of Contemporary American English (COCA) (Gardner and Da"
2020.lrec-1.722,W18-0507,1,0.910658,"7 respectively. 5896 2 Previous Work In this section, we review previous work in lexical substitution, a closely related task, and discuss how the academic text rewriting system potentially differs. In essence, our system is similar to lexical substitution (LS) and text simplification tasks, in such a way that both focus on the rewriting of an original text towards a given goal. Lexical substitution system mainly focuses on rewriting texts by replacing some of the words or phrases without ˇ altering the original meaning (Szarvas et al., 2013; Stajner and Saggion, 2018). The work by Guo et al. (2018) targeted text simplification based on the sequence-to-sequence deep neural network model, where its entailment and paraphrasing capabilities are improved via multi-task learning. While the complex word identification (CWI) task focuses on identifying lexical units that pose difficulties to understand the sentence (Yimam et al., 2017b; Yimam et al., 2017a; Yimam et al., 2018; Paetzold and Specia, 2016), our informal word identification (IWI) component focuses on identifying words that are not fitting or adhering to the academic style of writing. The work by Riedl et al. (2014) focuses on the l"
2020.lrec-1.722,S16-1085,0,\N,Missing
2020.lrec-1.728,D18-1523,0,0.0164621,"t al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties"
2020.lrec-1.728,P18-1001,0,0.0606807,"Missing"
2020.lrec-1.728,L18-1618,0,0.022236,"the most appropriate sense (labelled by the centroid word of a corresponding cluster). by Ustalov et al. (2018), extending it with a back-end for multiple languages, language detection, and sense browsing capabilities. 5. Evaluation We first evaluate our converted embedding models on multi-language lexical similarity and relatedness tasks, as a sanity check, to make sure the word sense induction process did not hurt the general performance of the embeddings. Then, we test the sense embeddings on WSD task. 5.1. Lexical Similarity and Relatedness Experimental Setup We use the SemR-11 datasets4 (Barzegar et al., 2018), which contain word pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in o"
2020.lrec-1.728,W06-3812,1,0.364448,"ess algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Ind"
2020.lrec-1.728,Q17-1010,0,0.301237,"ontexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polysemous word using only the information encoded in pretrained word embeddings. We construct a semantic similarity graph for words and partition it into densely connect"
2020.lrec-1.728,N19-1423,0,0.00893264,"ification problem. Knowledge-based approaches construct sense embeddings, i.e. embeddings that separate various word senses. SupWSD (Papandrea et al., 2017) is a state-of-the-art system for supervised WSD. It makes use of linear classifiers and a number of features such as POS tags, surrounding words, local collocations, word embeddings, and syntactic relations. GlossBERT model (Huang et al., 2019), which is another implementation of supervised WSD, achieves a significant improvement by leveraging gloss information. This model benefits from sentence-pair classification approach, introduced by Devlin et al. (2019) in their BERT contextualized embedding model. The input to the model consists of a context (a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an ex"
2020.lrec-1.728,L18-1550,0,0.16799,"pment of completely unsupervised and knowledge-free approaches to word sense disambiguation (WSD). They are particularly useful for under-resourced languages which do not have any resources for building either supervised and/or knowledge-based models. In this paper, we present a method that takes as input a standard pre-trained word embedding model and induces a fully-fledged word sense inventory, which can be used for disambiguation in context. We use this method to induce a collection of sense inventories for 158 languages on the basis of the original pre-trained fastText word embeddings by Grave et al. (2018), enabling WSD in these languages. Models and system are available online. Keywords: word sense induction, word sense disambiguation, word embeddings, sense embeddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning m"
2020.lrec-1.728,S13-2113,0,0.0199433,"s senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and Goldberg, 2018). Analogously to previously descr"
2020.lrec-1.728,D19-1355,0,0.0605952,"Missing"
2020.lrec-1.728,P19-1165,0,0.0213523,"(a sentence which contains an ambiguous word) and a gloss (sense definition) from WordNet. The contextgloss pair is concatenated through a special token ([SEP]) and classified as positive or negative. On the other hand, sense embeddings are an alternative to traditional word vector models such as word2vec, fastText or GloVe, which represent monosemous words well but fail for ambiguous words. Sense embeddings represent individual senses of polysemous words as separate vectors. They can be linked to an explicit inventory (Iacobacci et al., 2015) or induce a sense inventory from unlabelled data (Iacobacci and Navigli, 2019). LSTMEmbed (Iacobacci and Navigli, 2019) aims at learning sense embeddings linked to BabelNet (Navigli and Ponzetto, 2012), at the same time handling word ordering, and using pre-trained embeddings as an objective. Although it was tested only on English, the approach can be easily adapted to other languages present in BabelNet. However, manually labelled datasets as well as knowledge bases exist only for a small number of wellresourced languages. Thus, to disambiguate polysemous words in other languages one has to resort to fully unsupervised techniques. The task of Word Sense Induction (WSI)"
2020.lrec-1.728,P15-1010,0,0.0808199,"Missing"
2020.lrec-1.728,W19-7405,0,0.01615,"lysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their contexts (Athiwaratkun et al., 2018; Jain et al., 2019). Unfortunately, many widespread word embedding models, such as GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017), do not handle polysemous words. Words in these models are represented with single vectors, which were constructed from diverse sets of contexts corresponding to different senses. In such cases, their disam? Currently at Yandex. biguation needs knowledge-rich approaches. We tackle this problem by suggesting a method of posthoc unsupervised WSD. It does not require any external knowledge and can separate different senses of a polyse"
2020.lrec-1.728,S13-2049,0,0.151837,"ge in Pearson correlation score when switching from the baseline fastText embeddings to our sense vectors. The new vectors significantly improve the relatedness detection for German, Farsi, Russian, and Chinese, whereas for Italian, Dutch, and Swedish the score slightly falls behind the baseline. For other languages, the performance of sense vectors is on par with regular fastText. 5.2. Word Sense Disambiguation The purpose of our sense vectors is disambiguation of polysemous words. Therefore, we test the inventories constructed with egvi on the Task 13 of SemEval-2013 — Word Sense Induction (Jurgens and Klapaftis, 2013). The task is to identify the different senses of a target word in context in a fully unsupervised manner. 5947 lar to state-of-the-art word sense disambiguation and word sense induction models. In particular, we can see that it outperforms SenseGram on the majority of metrics. We should note that this comparison is not fully rigorous, because SenseGram induces sense inventories from word2vec as opposed to fastText vectors used in our work. 5.3. Figure 3: Absolute improvement of Pearson correlation scores of our embeddings compared to fastText. This is the averaged difference of the scores for"
2020.lrec-1.728,D15-1200,0,0.0219916,"ems, Rubyist, Rubyists, Rubys, Sadie, Sapphire, Sypro, Violet, jRuby, ruby, rubyists Table 1: Top nearest neighbours of the fastText vector of the word Ruby are clustered according to various senses of this word: programming language, gem, first name, color, but also its spelling variations (typeset in black color). vectors. Here, the definition of context may vary from window-based context to latent topic-alike context. Afterwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Ke"
2020.lrec-1.728,W14-0121,0,0.0241505,"= K. arated into two clusters. Interestingly, fastText handles typos, code-switching, and emojis by correctly associating all non-standard variants to the word they refer, and our method is able to cluster them appropriately. Both inventories were produced with K = 200, which ensures stronger connectivity of graph. However, we see that this setting still produces too many clusters. We computed the average numbers of clusters produced by our model with K = 200 for words from the word relatedness datasets and compared these numbers with the number of senses in WordNet for English and RuWordNet (Loukachevitch and Dobrov, 2014) for Russian (see Table 3). We can see that the number of senses extracted by our method is consistently higher than the real number of senses. We also compute the average number of senses per word for all the languages and different values of K (see Figure 5). The average across languages does not change much as we increase K. However, for larger K the average exceed the median value, indicating that more languages have lower number of senses per word. At the same time, while at smaller K the maximum average number of senses per word does not exceed 6, larger values of K produce outliers, e.g"
2020.lrec-1.728,C14-3003,0,0.031943,"ddings, graph clustering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vecto"
2020.lrec-1.728,D17-2016,1,0.833565,"5 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·) among words in this clus"
2020.lrec-1.728,D17-2018,0,0.0605054,"Missing"
2020.lrec-1.728,W16-1620,1,0.576845,"rwards, the resulting clusters are either used as senses directly (Kutuzov, 2018), or employed further to learn sense embeddings via Chinese Restaurant Process algorithm (Li and Jurafsky, 2015), AdaGram, a Bayesian extension of the Skip-Gram model (Bartunov et al., 2016), AutoSense, an extension of the LDA topic model (Amplayo et al., 2019), and other techniques. Word ego-network clustering is applied to semantic graphs. The nodes of a semantic graph are words, and edges between them denote semantic relatedness which is usually evaluated with cosine similarity of the corresponding embeddings (Pelevina et al., 2016) or by PMI-like measures (Hope and Keller, 2013b). Word senses are induced via graph clustering algorithms, such as Chinese Whispers (Biemann, 2006) or MaxMax (Hope and Keller, 2013a). The technique suggested in our work belongs to this class of methods and is an extension of the method presented by Pelevina et al. (2016). Synonyms and substitute clustering approaches create vectors which represent synonyms or substitutes of polysemous words. Such vectors are created using synonymy dictionaries (Ustalov et al., 2019) or context-dependent substitutes obtained from a language model (Amrami and G"
2020.lrec-1.728,D14-1162,0,0.0837391,"Missing"
2020.lrec-1.728,L18-1167,1,0.8605,"pairs with manually assigned similarity scores from 0 (words are not related) to 10 (words are fully interchangeable) for 12 languages: English (en), Arabic (ar), German (de), Spanish (es), Farsi (fa), French (fr), Italian (it), Dutch (nl), Portuguese (pt), Russian (ru), Swedish (sv), Chinese (zh). The task is to assign relatedness scores to these pairs so that the ranking of the pairs by this score is close to the ranking defined by the oracle score. The performance is measured with Pearson correlation of the rankings. Since one word can have several different senses in our setup, we follow Remus and Biemann (2018) and define the relatedness score for a pair of words as the maximum cosine similarity between any of their sense vectors. We extract the sense inventories from fastText embedding vectors. We set N = K for all our experiments, i.e. the number of vertices in the graph and the maximum number of vertices’ nearest neighbours match. We conduct experiments with N = K set to 50, 100, and 200. For each cluster Vi we create a sense vector si by averaging vectors that belong to this cluster. We rely on the methodology of (Remus and Biemann, 2018) shifting the generated sense vector to the direction of t"
2020.lrec-1.728,P15-4018,1,0.847544,"e all 50 terms are 5945 clustered, in the method presented in this section we sparsify the graph by removing 13 nodes which were not in the set of the “anti-edges” i.e. pairs of most dissimilar terms out of these 50 neighbours. Examples of anti-edges i.e. pairs of most dissimilar terms for this graph include: (Haskell, Sapphire), (Garnet, Rails), (Opal, Rubyist), (Hazel, RubyOnRails), and (Coffeescript, Opal). Labelling of Induced Senses We label each word cluster representing a sense to make them and the WSD results interpretable by humans. Prior systems used hypernyms to label the clusters (Ruppert et al., 2015; Panchenko et al., 2017), e.g. “animal” in the “python (animal)”. However, neither hypernyms nor rules for their automatic extraction are available for all 158 languages. Therefore, we use a simpler method to select a keyword which would help to interpret each cluster. For each graph node v ∈ V we count the number of anti-edges it belongs to: count(v) = |{(wi , wi ) : (wi , wi ) ∈ E ∧ (v = wi ∨ v = wi )}|. A graph clustering yields a partition of V into n clusters: V = {V1 , V2 , ..., Vn }. For each cluster Vi we define a keyword wikey as the word with the largest number of antiedges count(·)"
2020.lrec-1.728,I05-3027,0,0.0267775,"rs of GPU-accelerated computations. We release the constructed sense inventories for all the available languages. They contain all the necessary information for using them in the proposed WSD system or in other downstream tasks. 4.2. Word Sense Disambiguation System The first text pre-processing step is language identification, for which we use the fastText language identification models by Bojanowski et al. (2017). Then the input is tokenised. For languages which use Latin, Cyrillic, Hebrew, or Greek scripts, we employ the Europarl tokeniser.2 For Chinese, we use the Stanford Word Segmenter (Tseng et al., 2005). For Japanese, we use Mecab (Kudo, 2006). We tokenise Vietnamese with UETsegmenter (Nguyen and Le, 2016). All other languages are processed with the ICU tokeniser, as implemented in the PyICU project.3 After the tokenisation, the system analyses all the input words with pre-extracted sense inventories and defines the most appropriate sense for polysemous words. Figure 2 shows the interface of the system. It has a textual input form. The automatically identified language of text is shown above. A click on any of the words displays a prompt (shown in black) with the most appropriate sense of a"
2020.lrec-1.728,D18-1270,0,0.0263365,"ering 1. Introduction There are many polysemous words in virtually any language. If not treated as such, they can hamper the performance of all semantic NLP tasks (Resnik, 2006). Therefore, the task of resolving the polysemy and choosing the most appropriate meaning of a word in context has been an important NLP task for a long time. It is usually referred to as Word Sense Disambiguation (WSD) and aims at assigning meaning to a word in context. The majority of approaches to WSD are based on the use of knowledge bases, taxonomies, and other external manually built resources (Moro et al., 2014; Upadhyay et al., 2018). However, different senses of a polysemous word occur in very diverse contexts and can potentially be discriminated with their help. The fact that semantically related words occur in similar contexts, and diverse words do not share common contexts, is known as distributional hypothesis and underlies the technique of constructing word embeddings from unlabelled texts. The same intuition can be used to discriminate between different senses of individual words. There exist methods of training word embeddings that can detect polysemous words and assign them different vectors depending on their co"
2020.lrec-1.728,L18-1164,1,0.90455,"Missing"
2020.lrec-1.728,C02-1114,0,0.018722,"tained from a language model (Amrami and Goldberg, 2018). Analogously to previously described techniques, word senses are induced by clustering these vectors. 3. Algorithm for Word Sense Induction The majority of word vector models do not discriminate between multiple senses of individual words. However, a polysemous word can be identified via manual analysis of its nearest neighbours—they reflect different senses of the word. Table 1 shows manually sense-labelled most similar terms to the word Ruby according to the pre-trained fastText model (Grave et al., 2018). As it was suggested early by Widdows and Dorow (2002), the distributional properties of a word can be used to construct a graph of words that are semantically related to it, and if a word is polysemous, such graph can easily be partitioned into a number of densely connected subgraphs corresponding to different senses of this word. Our algorithm is based on the same principle. 5944 3.1. SenseGram: A Baseline Graph-based Word Sense Induction Algorithm nodes – those which should not be connected: E = {(w1 , w1 ), (w2 , w2 ), ..., (wN , wN )}. To clarify this, consider the target (ego) word w = python, its top similar term w1 = Java and the resultin"
2020.lrec-1.728,J19-3002,1,\N,Missing
2020.pam-1.13,S19-2004,1,0.82754,"hod for inducing frame-semantic resources based on a few frame-annotated sentences using lexical substitution, and (ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of frame-semantic resources. 2 We experimented with two groups of lexical substitution methods. The first"
2020.pam-1.13,P98-1013,0,0.836384,"er, the latter show comparable performance on the task of LU expansion. Substitutes for Assistance: assist, aid Substitutes for Helper: she, I, he, you, we, someone, they, it, lori, hannah, paul, sarah, melanie, pam, riley Substitutes for Benefited party: me, him, folk, her, everyone, people Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and a"
2020.pam-1.13,P13-2130,0,0.0246292,"ple Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to roles of the respective frame) of those sentences and aggregating possible substitutions into an induced frame-semantic resource. Table 1 shows one such induced example. For this purpose, we have experimented with state-of-the-art noncontextualized (static)"
2020.pam-1.13,P14-1133,0,0.0234777,"e (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications."
2020.pam-1.13,L16-1630,0,0.0404709,"Missing"
2020.pam-1.13,P14-1136,0,0.01767,"d embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inv"
2020.pam-1.13,padro-stanilovsky-2012-freeling,0,0.100483,"Missing"
2020.pam-1.13,J05-1004,0,0.426396,"formance on the task of LU expansion. Substitutes for Assistance: assist, aid Substitutes for Helper: she, I, he, you, we, someone, they, it, lori, hannah, paul, sarah, melanie, pam, riley Substitutes for Benefited party: me, him, folk, her, everyone, people Substitutes for Time: tomorrow, now, shortly, sooner, tonight, today, later Table 1: An example of the induced lexical representation (roles and LUs) of the Assistance FrameNet frame using lexical substitutes from a single seed sentence. annotated resources. Some publicly available resources are FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), yet for many languages and domains, specialized resources do not exist. Besides, due to the inherent vagueness of frame definitions, the annotation task is challenging and requires semanticists or very complex crowd-sourcing setups (Fossati et al., 2013). We suggest a different perspective on the problem: expanding the FrameNet resource automatically by using lexical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to ro"
2020.pam-1.13,P15-2036,0,0.0218338,"frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inverse problem, the case where the number of annotations is insufficient, similar to the idea of Pennacchiotti et al. (2008) who investigated the ut"
2020.pam-1.13,N18-1135,0,0.011093,"7), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013), as well as distributional thesaurus based models in the form of JoBimText (Biemann and Riedl, 2013). The second group of methods does use the context: here, we tried contextualized word embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of"
2020.pam-1.13,P14-2050,0,0.0304354,"rd annotations from the rest of the corpus marked with the same role and related to the same frame as ground truth substitutes. The final datasets for experiments contain 79, 584 records for lexical unit expansion and 191, 252 records for role expansion (cf. Tables 4 and 5). Contextualized Models Static word representations fail to handle polysemic words. This paves the way for contextaware word representation models, which can generate diverse word-probability distributions for a target word based on its context. Melamud et al. (2015) This simple model uses syntax-based skip-gram embeddings (Levy and Goldberg, 2014) of a word and its context to produce context-sensitive lexical substitutes, where the context of the word is represented by the dependency relations of the word. We use the original word and context embeddings of Melamud et al. (2015), trained on the ukWaC (Ferraresi et al., 2008) corpus. To find dependency relations, we use Stanford Parser (Chen and Manning, 2014) and collapsed the dependencies that include prepositions. Top k substitutes are produced if both the word and its context are present in the model’s vocabulary. Melamud et al. (2015) proposed four measures of contextual similarity"
2020.pam-1.13,D08-1048,0,0.0924979,"Missing"
2020.pam-1.13,P98-2127,0,0.149749,"ent words as vectors of continuous numbers, where words with similar meanings are expected to have similar vectors. Thus, to produce substitutes, we extracted the k nearest neighbors using a cosine similarity measure. We use pre-trained embeddings by authors models: fastText trained on the Common Crawl corpus, GloVe trained on Common Crawl corpus with 840 billion words, word2vec trained on Google News. All these models produce 300-dimension vectors. 96 Distributional Thesaurus (DT) In this approach, word similarities are computed using complex linguistic features such as dependency relations (Lin, 1998). The representations provided by DTs are sparser, but similarity scores based on them can be better. JoBimText (Biemann and Riedl, 2013) is a framework that offers many DTs computed on a range of different corpora. Context features for each word are ranked using the lexicographer’s mutual information (LMI) score and used to compute word similarity by feature overlap. We extract the k nearest neighbors for the target word. We use two JoBimText DTs: (i) DT built on Wikipedia with n-grams as contexts and (ii) DT built on a 59G corpus (Wikipedia, Gigaword, ukWaC, and LCC corpora combined) using d"
2020.pam-1.13,D14-1162,0,0.0836952,"exical substitution. Given a small set of seed sentences with their frame annotations, we can expand it by substituting the targets (words corresponding to lexical units of the respective frame) and arguments (words corresponding to roles of the respective frame) of those sentences and aggregating possible substitutions into an induced frame-semantic resource. Table 1 shows one such induced example. For this purpose, we have experimented with state-of-the-art noncontextualized (static) word representation models including neural word embeddings, i.e. fastText (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Kha"
2020.pam-1.13,N18-1202,0,0.0391628,"to generate meaningpreserving substitutes for this argument. Contributions of our work are (i) a method for inducing frame-semantic resources based on a few frame-annotated sentences using lexical substitution, and (ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of f"
2020.pam-1.13,S19-2003,0,0.0149138,"(ii) an evaluation of various distributional semantic models and lexical substitution methods on the ground truth from FrameNet. WordNet-based methods to automatically induce new LUs and reported their results on FrameNet. Our method is inspired by the recent work of Amrami and Goldberg (2018). They suggest to predict the substitutes vectors for target words using pre-trained ELMo (Peters et al., 2018) and dynamic symmetric patterns, then induced the word senses using clustering. Arefyev et al. (2019) takes the idea of substitute vectors from (Amrami and Goldberg, 2018) for the SemEval 2019 (QasemiZadeh et al., 2019) frame induction task and replaces ELMo with BERT (Devlin et al., 2019) for improved performance. Zhou et al. (2019) show the utility of BERT for the lexical substitution task. Lexical substitution has been used for a range of NLP tasks such as paraphrasing or text simplification, but here, we are employing it, as far as we are aware, for the first time to perform expansion of frame-semantic resources. 2 We experimented with two groups of lexical substitution methods. The first one use no context: non-contextualized neural word embedding models, i.e. fastText (Bojanowski et al., 2017), GloVe ("
2020.pam-1.13,Q15-1032,0,0.0190172,"t to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more challenging because of the high granularity of frame roles. These supervised models rely on a dataset of frame-annotated sentences such as FrameNet. FrameNet-like resources are available only for very few languages and cover only a few domains. In this paper, we venture into the inverse problem, the case where the number of annotations is insufficient, similar to the idea of Pennacchiotti et al. (2008) who investigated the utility of semantic space"
2020.pam-1.13,D07-1002,0,0.0865619,"ski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013); and distributional thesaurus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many down"
2020.pam-1.13,D18-1412,0,0.0703975,"tion (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications. To complete the comparison, we also include the lexical substitution model of Melamud et al. (2015), which uses dependency-based word and context embeddings a"
2020.pam-1.13,D17-1128,0,0.0129786,"t (Bojanowski et al., 2017), GloVe (Pennington et al., 2014), and word2vec (Mikolov et al., 2013), as well as distributional thesaurus based models in the form of JoBimText (Biemann and Riedl, 2013). The second group of methods does use the context: here, we tried contextualized word embedding model BERT (Devlin et al., 2019) and the lexical substitution model of Melamud et al. (2015). 3 Related Work Approaches to semantic frame parsing with respect to a pre-defined semantic frame resource, such as FrameNet, have received much attention in the literature (Das et al., 2010; Oepen et al., 2016; Yang and Mitchell, 2017; Peng et al., 2018), with SEMAFOR (Das et al., 2014) being a most widely known system to extract complete frame structure including target identification. Some works focus on identifying partial structures such as frame identification (Hartmann et al., 2017; Hermann et al., 2014), role labelling with frame identification (Swayamdipta et al., 2017; Yang and Mitchell, 2017), and simple role labelling (Kshirsagar et al., 2015; Roth and Lapata, 2015; Swayamdipta et al., 2018), which is considered very similar to standard PropBank (Palmer et al., 2005) style semantic role labelling, albeit more ch"
2020.pam-1.13,P13-1111,0,0.0107154,"urus, i.e. JoBimText (BieIntroduction The goal of lexical substitution (McCarthy and Navigli, 2009) is to replace a given target word in its context with meaning-preserving alternatives. In this paper, we show how lexical substitution can be used for semantic frame expansion. A semantic frame is a linguistic structure used to describe the formal meaning of a situation or event (Fillmore, 1982). Semantic frames have witnessed a wide range of applications; such as question answering (Shen and Lapata, 2007; Berant and Liang, 2014; Khashabi et al., 2018), machine translation (Gao and Vogel, 2011; Zhai et al., 2013), and semantic role labelling (Do et al., 2017; Swayamdipta et al., 2018). The impact, however, is limited by the scarce availability of 95 Proceedings of the Probability and Meaning Conference (PaM 2020), pages 95–103 c October 14-15 2020 2020 Association for Computational Linguistics mann and Riedl, 2013); and compared their results with contextualized word representations of the state-of-the-art BERT model (Devlin et al., 2019), which has set a new benchmark performance on many downstream NLP applications. To complete the comparison, we also include the lexical substitution model of Melamud"
2020.peoples-1.8,Q17-1010,0,0.00535986,"tigate the frequency of the power motive with the self-regulatory level 4, which we expect to be higher. At the same time, we will also analyze the other motives and levels to see which ones are now less frequent and to what extent. Furthermore, we compare different linguistic features and statistics from 2019 to 2020 to see, if any of these show differences that might indicate possible biases in the data. Our Bi-LSTM model was set to be trained within 3 epochs and with a batch size of 32 instances. The model was constructed having 3 hidden layers and utilized pre-trained fasttext embeddings (Bojanowski et al., 2017), as this character-based or word fragment-based language representation has shown to be less prone to noisy data and words that have not been observed yet like e.g. spelling mistakes or slang – both often observable in social media data. The fasttext embeddings had 300 dimensions and were trained on a Twitter corpus, ideally matching the task at hand.10 Explorative experiments with different parameter combinations have shown that a drop-out rate of .3 and step width of .001 produced good results. The cross-entropy loss was reduced rather quickly and oscillated at 1.1 when we stopped training"
2020.peoples-1.8,N19-1357,0,0.0173049,"echanism (Young et al., 2018) can capture the intermediate importance of algorithmic decisions made by the network. It can be employed for enhanced results but also investigated for researching algorithmic decisions. However, it is debated upon, whether this algorithmic importance can serve as an explanation. Even though oftentimes, the algorithmic importance is correlated with an explanation for the task (i.e. does a model for image recognition of animals look at the animals or the backgrounds of the images?), there are cases, where algorithmic importance and explanation for the task differ (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019)). Since automatically labeling implicit motives is a sequential problem revolving around identifying the first verbal enactment of a motive (see Section 3, we decided to employ a Bi-LSTM with an attention mechanism (Schuster and Paliwal, 1997). 5 GermEval is a series of shared task evaluation campaigns that focus on Natural Language Processing for the German language. 6 https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/germeval-2020cognitive-motive.html 7 The data can be retrieved via https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/ germeval-"
2020.peoples-1.8,W19-3014,1,0.735522,"an implicit motive (see Section 3) dictionary approach to automatically determine risks of war outbreaks from different novels and historic documents, identifying widening gaps between the so-called power motive and affiliation motive in near-war situations. Overall, the work on automated classification of implicit motive data or the use of NLP for the assertion of psychological traits in general is rather sparse or relies on rather outdated methods, as this application domain can be considered a niche (Schultheiss and Brunstein, 2010; Johannßen and Biemann, 2019; Johannßen and Biemann, 2018; Johannßen et al., 2019). One recent event in this area was the GermEval 2020 Task 1 on the Classification and Regression of Cognitive and Motivational Style from Text (Johannßen et al., 2020). The best participating team reached a macro f1 score of 70.40 on the task of classifying implicit motives combined with self-regulating levels, resulting in 30 target classes. However, behavioral outcomes from automatically classified implicit motives have – to our knowledge – not yet been researched. 3 Implicit Motives and Self-regulatory Levels Implicit motives can reveal intrinsic, unconscious human desires, and thus avoid"
2020.peoples-1.8,W19-4724,0,0.0159256,"ated work in Section 2, we will first introduce the concept of implicit motives and self-regulating levels in more details in Section 3 and the social unrest predictors thereafter in Section 4. The data utilized for experiments is described in Section 5 and the methodology in Section 6. Thereafter, we will present the results in Section 7 and discuss their impacts in Section 8. Lastly, we will draw a conclusion in Section 9. 2 Related Work Conflict predictions from natural language are rarely encountered applications and have mainly been about content analysis and less about crowd psychology. Kutuzov et al. (2019) used one-to-X analogy reasoning based on word embeddings for predicting previous armed conflict situations from printed news. Johansson et al. (2011) performed named entity recognition (NER) and extracted events via Hidden Markov Models (HMM) and neural networks, which were combined with human intelligence reports to identify current global areas of conflicts, that, in turn, were utilized mainly for world map visualizations. Investigation of personality traits has mainly been focussing on so-called explicit methods. For these, questionnaires are filled out either by interviewers, through obse"
2020.peoples-1.8,2020.acl-main.472,0,0.0165438,"ly by participants. One of the most broadly utilized psychometrics is the Big Five inventory, even though its validity is controversial (Block, 1995). The five-factory theory of personality (later named Big Five) identifies five personality traits, namely openness to experiences, conscientiousness, extraversion, agreeableness and neuroticism (McCrae and Costa Jr., 1999; Goldberg, 1981). This Big Five inventory was utilized by Tighe and Chegn (2018) for analyzing these five traits of Filipino speakers. Some studies perform natural language processing (NLP) for investigating personality traits. Lynn et al. (2020) utilized an attention mechanism for deciding upon important parts of an instance when assigning the five-factor inventory classes. The Myers-Briggs Type Indicator (MBTI) is a broadly utilized adaption of the Big Five inventory, which Yamada et al. (2019) employed for asserting the personality traits within tweets.4 The research field of psychology has moved further towards automated language assertions during the past years. One standard methodology is the utilization of the tool linguistic inquiry and word count (LIWC), developed by Pennebaker et al. (1999). The German version of LIWC was de"
2020.peoples-1.8,W18-1115,0,0.0964014,"Missing"
2020.peoples-1.8,D19-1002,0,0.0124524,"2018) can capture the intermediate importance of algorithmic decisions made by the network. It can be employed for enhanced results but also investigated for researching algorithmic decisions. However, it is debated upon, whether this algorithmic importance can serve as an explanation. Even though oftentimes, the algorithmic importance is correlated with an explanation for the task (i.e. does a model for image recognition of animals look at the animals or the backgrounds of the images?), there are cases, where algorithmic importance and explanation for the task differ (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019)). Since automatically labeling implicit motives is a sequential problem revolving around identifying the first verbal enactment of a motive (see Section 3, we decided to employ a Bi-LSTM with an attention mechanism (Schuster and Paliwal, 1997). 5 GermEval is a series of shared task evaluation campaigns that focus on Natural Language Processing for the German language. 6 https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/germeval-2020cognitive-motive.html 7 The data can be retrieved via https://www.inf.uni-hamburg.de/en/inst/ab/lt/resources/data/ germeval-2020-cognitive-motive.html 79"
2020.peoples-1.8,P19-2024,0,0.0183481,"ly openness to experiences, conscientiousness, extraversion, agreeableness and neuroticism (McCrae and Costa Jr., 1999; Goldberg, 1981). This Big Five inventory was utilized by Tighe and Chegn (2018) for analyzing these five traits of Filipino speakers. Some studies perform natural language processing (NLP) for investigating personality traits. Lynn et al. (2020) utilized an attention mechanism for deciding upon important parts of an instance when assigning the five-factor inventory classes. The Myers-Briggs Type Indicator (MBTI) is a broadly utilized adaption of the Big Five inventory, which Yamada et al. (2019) employed for asserting the personality traits within tweets.4 The research field of psychology has moved further towards automated language assertions during the past years. One standard methodology is the utilization of the tool linguistic inquiry and word count (LIWC), developed by Pennebaker et al. (1999). The German version of LIWC was developed by Wolf et al. (2008). It includes 96 target classes, some of which are rather simple linguistic features (word count, words longer than six characters, frequency of punctuation), and psychological categories such as anxiety, familiarity, or occup"
2020.semeval-1.213,P19-4007,0,0.068237,"Missing"
2020.semeval-1.213,N19-1423,0,0.222659,"long with the average label probability and the variance of the five classifier predictions. Since there is no way that such weak labels themselves carry more useful information to a machine learning system than the original dataset on which the five classifiers were trained, we decided not to use any of the weakly labeled information. Instead, for our classification systems, we rely on the 2019 OLID dataset only. However, the OffensEval 2020 dataset is an ample source to build models using unsupervised learning, particularly for domain-adaptation of a pre-trained language model such as BERT (Devlin et al., 2019) or its successors which are based on the transformer neural network architecture. Unfortunately, training a transformer-based language model in an unsupervised manner is incredibly resource-consuming, making it impractical to learn from large datasets without access to larger GPU clusters or TPU hardware. Regarding this, the contribution of our paper is two-fold: This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1638 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 16"
2020.semeval-1.213,S19-2011,0,0.249467,"rnational License. creativecommons.org/licenses/by/4.0/. License details: http:// 1638 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1638–1644 Barcelona, Spain (Online), December 12, 2020. 1. We evaluate to what extent different pre-trained transformer-based neural network models can be fine-tuned to detect offensive language and its sub-categories. An ensemble based on the ALBERT (Lan et al., 2019) model achieves the best overall performance. 2. We study how an additional fine-tuning step with masked language modeling (MLM) of the best individual model RoBERTa (Liu et al., 2019b) conducted on in-domain data affects the model performance. An ensemble of models trained with this strategy was submitted as our official contribution to the OffensEval 2020 shared task for the English language and achieved first place in the competition. 2 Related Work Offensive language detection: Nowadays, a number of public datasets are available to train machine classifiers for detecting English offensive language. Unfortunately, underlying data sources, category definitions, data sampling strategies, and annotation guidelines differ to a large extent between these datasets. Hence, res"
2020.semeval-1.213,W17-1101,0,0.0293086,"ontribution to the OffensEval 2020 shared task for the English language and achieved first place in the competition. 2 Related Work Offensive language detection: Nowadays, a number of public datasets are available to train machine classifiers for detecting English offensive language. Unfortunately, underlying data sources, category definitions, data sampling strategies, and annotation guidelines differ to a large extent between these datasets. Hence, results of different datasets are hardly comparable, and training sets usually cannot be combined to obtain a more robust classification system. Schmidt and Wiegand (2017), and Fortuna and Nunes (2018) conducted insightful surveys on this rapidly growing field. Mandl et al. (2019), Struß et al. (2019), and Basile et al. (2019) recently organized shared tasks on the topic. Although winning systems can achieve striking prediction accuracy, OLD is far from being a solved problem. Prediction performance usually drops severely if the target data comprises different characteristics than the training data. Gr¨ondahl et al. (2018), for instance, show that many machine learning architectures can be fooled easily just by adding the word “love” to an offensive tweet to ma"
2020.semeval-1.213,S19-2137,1,0.868493,"019) highlight that linguistic information alone is not enough in many cases to decide whether a tweet is hateful or not. Also context information, e.g. about tweeting users themselves (Ribeiro et al., 2018), or mentioned users in tweets (Wiedemann et al., 2018) can be a useful feature for automatic OLD. Pre-trained language models for text classification: Transfer learning with deep neural networks, in general, has proven to be superior over supervised learning for text classification, especially for small training data situations. This is illustrated exemplarily in our last year’s approach (Wiedemann et al., 2019) to the OLD SemEval shared task which employed unsupervised pre-training of a recurrent neural network architecture with a topic cluster prediction task. Practically all winners of the aforementioned shared task competitions employ some form of a fine-tuned bidirectional transformer-based language model, a neural network architecture for which Devlin et al. (2019) published with BERT the seminal work. This architecture has been proven highly successful for transfer learning. A base model is pretrained with a MLM task and a next-sentence prediction (NSP) task in an unsupervised manner on very l"
2020.semeval-1.213,N19-1144,0,0.0694843,"Missing"
2020.semeval-1.213,S19-2010,0,0.1289,"Missing"
2020.semeval-1.213,S19-2007,0,\N,Missing
2021.eacl-demos.23,W06-3812,1,0.627865,"hbourhood graphs. In sum, SCoT offers a new type of neighbourhood graph that is different to all known implementations of neighbourhood or so-called ego graphs (Mitra et al., 2015). The variants are implemented with a similar pattern: each algorithm first searches for the nodes and then for the edges. Then, the algorithm merges those nodes and edges that refer to the same words in different intervals. It encodes the time-based scores in the nodes and edges. 2.3 Sense clustering The advantage of NGoTs is that they need to be clustered only once. For this, we use the Chinese Whispers algorithm (Biemann, 2006). The key characteristics of the algorithm are that it is nondeterministic, has a linear time-complexity and runs with a fixed number of iterations that result in a stable partition of the graph. We set the number of iterations to 15 in order to increase the chances of the algorithm of reaching a stable plateau. However, there may be more than one stable solution. We have thus implemented the possibility to recluster the graph in order to see whether multiple solutions exist. If one wants to break a tie, it is recommended to slightly reduce the density of the graph and to cluster again. This s"
2021.eacl-demos.23,S13-1035,0,0.492931,"on’ or ‘inflation’ (Biemann et al., 2020). In the recent decade, the interest in dynamic networks has increased. (Rosetti and Cazabet, 2018). This has also stimulated new graph-based approaches to analysing vocabulary change (Mitra et al., 2015; Riedl et al., 2014). Such research is a key interest of linguists (Tahmasebi et al., 2018; Nulty, 2017) and scholars in the humanities (Koselleck, 1989; Mueller and Schmieder, 2016; Friedrich and Biemann, 2016). Traditionally, scholars have determined such changes through close reading. However, the growing availability of ever larger digital corpora (Goldberg and Orwant, 2013) and the increasing speed of sense changes in social media (Stilo and Velardi, 2017) have boosted the significance of new research (Nulty, 2017). Of particular importance in the research on lexical change is the unsupervised approach of word sense induction (WSI). WSI enables the development of data-driven hypotheses. The approach induces meaning from the bottom upwards and can be used with a diachronic angle. Several implementations for diachronic WSI exist (Tahmasebi et al., 2018). While many of them represent word meaning by dense vector embeddings, sparse models with network representation"
2021.eacl-demos.23,riedl-etal-2014-distributed,1,0.835808,"Missing"
2021.eacl-demos.23,J17-1005,0,0.0264311,"ic networks has increased. (Rosetti and Cazabet, 2018). This has also stimulated new graph-based approaches to analysing vocabulary change (Mitra et al., 2015; Riedl et al., 2014). Such research is a key interest of linguists (Tahmasebi et al., 2018; Nulty, 2017) and scholars in the humanities (Koselleck, 1989; Mueller and Schmieder, 2016; Friedrich and Biemann, 2016). Traditionally, scholars have determined such changes through close reading. However, the growing availability of ever larger digital corpora (Goldberg and Orwant, 2013) and the increasing speed of sense changes in social media (Stilo and Velardi, 2017) have boosted the significance of new research (Nulty, 2017). Of particular importance in the research on lexical change is the unsupervised approach of word sense induction (WSI). WSI enables the development of data-driven hypotheses. The approach induces meaning from the bottom upwards and can be used with a diachronic angle. Several implementations for diachronic WSI exist (Tahmasebi et al., 2018). While many of them represent word meaning by dense vector embeddings, sparse models with network representations still play an important role. The use of sparse, human-readable models enables a b"
2021.eacl-demos.36,D19-5405,0,0.0329609,"Missing"
2021.eacl-demos.36,D19-6606,0,0.0217934,"ject, and (4) entering extracted comparative structures in templates. 5.1 most representative sentences and display it in the proper form. To create an answer, CAM: Bullet points mentions a “winner” defined by CAM with respect to aspects if they exist. It also takes the top-3 sentences supporting each of the objects and produces a list for highlighting the advantages and disadvantages of each object in comparison. Generation Methods Pre-trained Language Models Pre-trained language models have been shown to contain commonsense knowledge, so they can be successfully used for question answering (Andrews and Witteveen, 2019) and for generating sensible and coherent continuation of text. Therefore, we use Transformersbased CTRL (Keskar et al., 2019) models for answering comparative questions. CTRL allows explicit control codes to vary the domain and the content of the text. We use the Links control code, which forces the model to produce text similar to online news and reports. We feed into CTRL phrase “Links Which is better in respect to the aspect: object1 or object2 ?” and a row question from the input. We also vary the maximum number of tokens generated by CTRL. We experiment with different length set, includi"
2021.eacl-demos.36,N19-1423,0,0.0136652,"t question. We test several common baselines starting with simple one-layer bidirectional LSTM described by (Arora et al., 2017) where the input is encoded with GloVe (Pennington et al., 2014) embeddings. For some further improvements, we add Conditional Random Field (Sutton and McCallum, 2012) Objects Aspects Predicates 0.925 0.829 0.654 0.631 0.582 0.685 0.563 0.487 0.475 0.328 0.894 0.869 0.825 0.766 0.730 to LSTM and use context-based ELMO (Peters et al., 2018) embeddings for token representations. We also experiment with Transformers (Vaswani et al., 2017) using a pre-trained BERT model (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), which is its modification yielding better performance. For every classifier, during training, we tune hyperparameters by varying a batch size (from 16 to 100) and a learning rate (from 10−6 to 10−2 ). To find a proper converge of the training process, we apply two types of learning rate schedulers: Linear With Warmup and Slanted Triangular. For the model with the highest achieved F1 (RoBERTa), we employ stochastic weight ensembling (Goodfellow et al., 2015; Garipov et al., 2018), i.e., we interpolate between the weights obtained by training a certain model with"
2021.eacl-demos.36,W16-3622,0,0.0499051,"Missing"
2021.eacl-demos.36,W06-3812,1,0.222676,"f edges and sets the node weights to be proportional to the importance of adjacent edges. To make the graph sparse, we remove the edges with a score below average. We create separate graphs for sentences supporting each of the objects. We apply TextRank to each of them and then cluster them. Clustering divides the nodes in graphs by semantic similarity and thus allows identifying groups of sentences supporting a particular idea. Then, we apply TextRank again to each of the clusters separately and select the three most characteristic sentences from each cluster as produced by Chinese Whispers (Biemann, 2006), an iterative clustering algorithm, which assigns vertices to the most common class among their neighbors. Argumentative sentences selected in this way are displayed as a bullet-list after declaring the “winner” object of comparison. Document-Retrieval-Based Method To compose an answer, CAM: First snippets takes the first sentence related to the “winner” object in CAM output. Then it finds a document corresponding to this sentence and extracts the surrounding context. The obtained context consists of 3 sentences and is considered to be a system answer. 306 Table 3: Evaluation of generation me"
2021.eacl-demos.36,D18-1426,0,0.0198934,"template-based generation to Transformers-based language models. The main contributions of our work are threefold: (i) we design an evaluation framework for comparative QA, featuring a dataset based on Yahoo! Answers; (ii) we test several strategies for identification of comparative structures and for answer generation; (iii) we develop an online demo using three answer generation approaches. A demo of the system is available online.1 Besides, we release our code and data. 2 Related Work Text Generation Most of the current text natural language generation tasks (Duˇsek and Jurˇc´ıcˇ ek, 2016; Freitag and Roy, 2018) are based on se1 https://skoltech-nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-"
2021.eacl-demos.36,C08-1031,0,0.0748995,", 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, comparative information needs are partially satisfied by several kinds of industrial systems mentioned above. Schildw¨achter et al. (2019) proposed Comparative Argumentative Machine (CAM)2 , which a comparison system based on extracting and ranking arguments from the web. The authors have conducted a user study on 34 comparison topics, showing that the system is faster and more confident at finding arguments when answering comparative questions in contrast to a keyword-based search. Wachsmuth et al. (2017) pre"
2021.eacl-demos.36,P18-1013,0,0.013115,"nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid question"
2021.eacl-demos.36,P19-1206,0,0.0144812,"g generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapa"
2021.eacl-demos.36,kessler-kuhn-2014-corpus,0,0.0490592,"Missing"
2021.eacl-demos.36,W08-1404,0,0.0565599,"ture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comp"
2021.eacl-demos.36,2021.ccl-1.108,0,0.0522553,"Missing"
2021.eacl-demos.36,P04-3020,0,0.0233581,"(See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or definition questions: we focus on comparative questions. Sun et al. (2006) proposed one of the first works on automatic comparative web search, where each object was submitted as a separate query, then obtained results were compared. Opinion mining of comparative sentences is discussed by Ganapathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, compara"
2021.eacl-demos.36,P19-1220,0,0.0116277,"s 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fan and Fang, 2017). Comparative QA According to Li and Roth (2006), questions can be divided into 6 coarse and 50 fine-grained categories, such as factoid questions, list questions, or d"
2021.eacl-demos.36,W19-4516,1,0.918932,"y in order to get good results. Figure 2: The interface of the Comparative Question Answering System (CoQAS). with respect to an aspect specified by the user. First, using the Elasticsearch BM25, CAM retrieves sentences containing the two compared objects and the comparison aspect from the Common Crawl-based corpus featuring 14.3 billion sentences (Panchenko et al., 2018). Then, CAM classifies the sentences as comparative or not and identifies the “winner” of the two compared objects in the sentence context. Besides, it extracts aspects and predicates from the retrieved comparative sentences (Panchenko et al., 2019). Finally, CAM outputs a list of argumentative pro/con sentences and shows the “winner” of the comparison along with the comparison aspects. web interface in action. In the NLG module, we use several approaches to response generation: an information retrievalbased approach and an approach built upon pretrained language models. These techniques provide different answers: the first is more structured, and the second one is based on experience and opinions. Therefore, we allow a user to choose a generation model from different types: CAM, CTRL, and Snippets (cf. Figure 2). Finally, for integratio"
2021.eacl-demos.36,L18-1286,1,0.865385,"ions with Python OBJ very easy.The downside is, that you have to be careful when using it. If you’re not careful, you’ll end up writing code which will crash your computer if something goes wrong. You also need to know how to use libraries like numpy in order to get good results. Figure 2: The interface of the Comparative Question Answering System (CoQAS). with respect to an aspect specified by the user. First, using the Elasticsearch BM25, CAM retrieves sentences containing the two compared objects and the comparison aspect from the Common Crawl-based corpus featuring 14.3 billion sentences (Panchenko et al., 2018). Then, CAM classifies the sentences as comparative or not and identifies the “winner” of the two compared objects in the sentence context. Besides, it extracts aspects and predicates from the retrieved comparative sentences (Panchenko et al., 2019). Finally, CAM outputs a list of argumentative pro/con sentences and shows the “winner” of the comparison along with the comparison aspects. web interface in action. In the NLG module, we use several approaches to response generation: an information retrievalbased approach and an approach built upon pretrained language models. These techniques provi"
2021.eacl-demos.36,D14-1162,0,0.0862706,"Missing"
2021.eacl-demos.36,N18-1202,0,0.0466749,"e question components (objects, aspects, predicates, or none) is a sequence-labeling task, where the classifier should tag respective tokens in an input question. We test several common baselines starting with simple one-layer bidirectional LSTM described by (Arora et al., 2017) where the input is encoded with GloVe (Pennington et al., 2014) embeddings. For some further improvements, we add Conditional Random Field (Sutton and McCallum, 2012) Objects Aspects Predicates 0.925 0.829 0.654 0.631 0.582 0.685 0.563 0.487 0.475 0.328 0.894 0.869 0.825 0.766 0.730 to LSTM and use context-based ELMO (Peters et al., 2018) embeddings for token representations. We also experiment with Transformers (Vaswani et al., 2017) using a pre-trained BERT model (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), which is its modification yielding better performance. For every classifier, during training, we tune hyperparameters by varying a batch size (from 16 to 100) and a learning rate (from 10−6 to 10−2 ). To find a proper converge of the training process, we apply two types of learning rate schedulers: Linear With Warmup and Slanted Triangular. For the model with the highest achieved F1 (RoBERTa), we employ stochasti"
2021.eacl-demos.36,P17-1099,0,0.0340107,"ext Generation Most of the current text natural language generation tasks (Duˇsek and Jurˇc´ıcˇ ek, 2016; Freitag and Roy, 2018) are based on se1 https://skoltech-nlp.github.io/coqas 302 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 302–311 April 19 - 23, 2021. ©2021 Association for Computational Linguistics quence to sequence model architecture (Sutskever et al., 2014). The existing generation methods are developed by employing attention mechanism (Bahdanau et al., 2015) and pointer-generator network (See et al., 2017). More recent work on text generation focus on generating natural language using multitask learning from multi-document or multi-passage sources (Hsu et al., 2018; Nishida et al., 2019). However, in our generation task, we have a list of arguments used to build the final answer. This makes our task similar to unsupervised summarization. There exist several approaches for tackling the latter task, e.g. graph-based (Litvak and Last, 2008) and neural models (Isonuma et al., 2019; Coavoux et al., 2019). A common approach to summarization is based on the TextRank graph algorithm (Mihalcea, 2004; Fa"
2021.eacl-demos.36,W17-5106,0,0.0201308,"apathibhotla and Liu (2008) and Jindal and Liu (2006), yet with no connection to argumentation mining. Instead, comparative information needs are partially satisfied by several kinds of industrial systems mentioned above. Schildw¨achter et al. (2019) proposed Comparative Argumentative Machine (CAM)2 , which a comparison system based on extracting and ranking arguments from the web. The authors have conducted a user study on 34 comparison topics, showing that the system is faster and more confident at finding arguments when answering comparative questions in contrast to a keyword-based search. Wachsmuth et al. (2017) presented args.me, a search engine for retrieving pro and con arguments given for a given controversial topic. The input to this system is not structured but rather a query in a free textual form. The Touch´e shared task on argument retrieval at CLEF (Bondarenko et al., 2020b, 2021) featured a related track. The task was to retrieve from a large web corpus documents answering comparative question queries like “What IDE is better for Java: NetBeans or Eclipse?”. Objects: - Python - MATLAB Python Pros: - ... - ... Cons: - ... CAM / args.me / Touché ... Aspects: - Deep Learning MATLAB Pros: - .."
2021.eacl-demos.36,D18-1259,0,0.0723619,"Missing"
2021.eacl-demos.36,P13-4001,1,0.8493,"Missing"
2021.eacl-demos.8,2020.coling-main.484,1,0.81355,"Missing"
2021.eacl-demos.8,N19-1423,0,0.0123023,"uman in the loop (Bailey and Chopra, 2018), who annotates and enlarges the training set to improve the automatic classification iteratively. Annotators can sort the user comments according to the uncertainty score to keep the annotation process most rewarding (Andersen et al., 2020). Forum 4.0 uses the label probability as the uncertainty value. Uncertain instances are those whose classifi4 import user comment https://spiegel.de/ 66 Forum 4.0 employs transfer learning (Howard and Ruder, 2018) by using the embeddings of wellestablished pre-trained language models, for example, BERT embeddings (Devlin et al., 2019), as machine learning features for the classification model. Subsequently, all existing models classify the new user comment batch. 4 For the comment embeddings, we used two different multi-lingual pre-trained language models to embed the comments: (1) BERT (Devlin et al., 2019) is based on a transformer architecture, which learns contextual relations between sub-(word) units in a text. We used an average token embedding of the four last layers of the BERT model as the comment embeddings. (2) SentenceBERT (S-BERT) (Reimers and Gurevych, 2019) is based on a modification of the BERT network and"
2021.eacl-demos.8,P18-1031,0,0.164142,"arguments, collect resonance about their articles, or identify and contact experts or persons concerned for follow-up stories (Loosen et al., 2018). However, the quality of the comments varies significantly, and their amount is sometimes overwhelming, which makes manual monitoring and analysis a real challenge (Pagano and Maalej, 2013; Park et al., 2016a). In this work, we propose Forum 4.0, an opensource user comment analysis framework to semiautomatically analyze a large number of user comments for domain experts from various domains. Forum 4.0 leverages a combination of transfer learning (Howard and Ruder, 2018), human-in-theloop (Bailey and Chopra, 2018), and active learning (Settles, 2012) strategies to automatically analyze the comments’ content. To enable replication and further research, we share Forum 4.0’s source code, With the increasing number of user comments in diverse domains, including comments on online journalism and e-commerce websites, the manual content analysis of these comments becomes time-consuming and challenging. However, research showed that user comments contain useful information for different domain experts, which is thus worth finding and utilizing. This paper introduces"
2021.eacl-demos.8,gao-huang-2017-detecting,0,0.0217746,"p feature”, or “missing or requested features”. The domain expert further annotates app reviews, compiling a training set. Forum 4.0 trains a model and classifies the other app reviews for the domain expert to analyze. Usage of Forum 4.0 We describe exemplary usage scenarios of Forum 4.0 for journalists and product managers in their respective online journalism and app development domains and introduce Forum 4.0’s user interface. 2.1 Online Journalism The manual effort for comment moderation in online journalism is high (Park et al., 2016b). One the one hand, media outlets filter hate speech (Gao and Huang, 2017), as it might negatively affect their credibility (Naab et al., 2020). On the other hand, user comments can also be useful for different journalistic purposes (Diakopoulos, 2015). For example, journalists can obtain new perspectives and opinions on an article, learn from users’ described personal experiences, or identify potential interview partners among the commenting users (Loosen et al., 2018). Journalists can also aggregate user comments to identify and visualize their audience’s opinion on current news topics (Wang et al., 2013). Users can also point out errors in reporting, contribute a"
2021.eacl-demos.8,D19-1410,0,0.0148231,"pre-trained language models, for example, BERT embeddings (Devlin et al., 2019), as machine learning features for the classification model. Subsequently, all existing models classify the new user comment batch. 4 For the comment embeddings, we used two different multi-lingual pre-trained language models to embed the comments: (1) BERT (Devlin et al., 2019) is based on a transformer architecture, which learns contextual relations between sub-(word) units in a text. We used an average token embedding of the four last layers of the BERT model as the comment embeddings. (2) SentenceBERT (S-BERT) (Reimers and Gurevych, 2019) is based on a modification of the BERT network and infers semantically meaningful sentence embeddings. We used a lightweight logistic regression model as a classifier due to performance requirements for quick updates of machine labels during human-in-the-loop coding. To assess the feasibility of our architecture, we further timed the model’s training and evaluation. To mitigate the noise of our results, we performed 50 rounds for each experiment. The line plots show the average results of all rounds and the standard deviation. Machine Learning Experiments To preliminary evaluate the applicabi"
2021.findings-emnlp.218,J81-4005,0,0.643925,"Missing"
2021.findings-emnlp.218,P98-1013,0,0.450235,"n between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs encode. Different probing experiments An example of semantic attributes and their valhave been proposed to study the drawbacks of ues is the relation that exists between old, age and PTLMs in areas such as the biomedical domain (Jin date of birth, or the relation between rich, wealth et al., 2019), syntax (Hewitt and Manning, 2019), and net worth. Looking up rich on FrameNet (Fillsemantic and syntactic sentence structures (Ten- more, 1982; Baker et al., 1998) would not result in a ney et al., 2019; Peters et al., 2018b), prenomial frame by itself, but would evoke the semantic frame anaphora (Sorodoc et al., 2020), linguistics (Be- wealthiness (Figure 1). In WordNet (Fellbaum, linkov et al., 2017; Clark et al., 2020; Tenney et al., 1998) these associations are called an attribute2019) and commonsense knowledge (Petroni et al., value relation, where the attribute is a noun for 2019; Davison et al., 2019; Talmor et al., 2020). which adjectives express values. For instance, the 2554 Findings of the Association for Computational Linguistics: EMNLP 2021"
2021.findings-emnlp.218,P17-1080,0,0.0504894,"Missing"
2021.findings-emnlp.218,D19-1109,0,0.0242516,"019), syntax (Hewitt and Manning, 2019), and net worth. Looking up rich on FrameNet (Fillsemantic and syntactic sentence structures (Ten- more, 1982; Baker et al., 1998) would not result in a ney et al., 2019; Peters et al., 2018b), prenomial frame by itself, but would evoke the semantic frame anaphora (Sorodoc et al., 2020), linguistics (Be- wealthiness (Figure 1). In WordNet (Fellbaum, linkov et al., 2017; Clark et al., 2020; Tenney et al., 1998) these associations are called an attribute2019) and commonsense knowledge (Petroni et al., value relation, where the attribute is a noun for 2019; Davison et al., 2019; Talmor et al., 2020). which adjectives express values. For instance, the 2554 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2554–2559 November 7–11, 2021. ©2021 Association for Computational Linguistics Patterns A, [MASK] and B. What’s [VALUE] or [MASK], A or B? What’s [VALUE] and [MASK], A or B? A is [MASK], thus they have a [VALUE] A is [VALUE], thus it has more [MASK] per m2 . To know which is [VALUE], A or B, you need [MASK]. What’s [VALUE], thus has a higher [MASK], A or B ? We need the [MASK] to know what’s [VALUE], A or B. We need the [VALUE] to know who"
2021.findings-emnlp.218,N19-1423,0,0.189185,"evokes the frame wealthiness, while “old"" evokes age. The relation between rich and high net worth could be defined as a value-attribute pair, where rich is the value of an expression that represents an attribute: wealthiness/net worth. LU stands for lexical unit, FN1_Sent represents and Finished_Initial refer to which FrameNet version the lexical unit is from. Introduction In this paper, we expand on this line of research by probing PTLMs to investigate if they cover seGiven the ability of pretrained language models mantic attributes and their values. The closest work (PTLMs), such as BERT (Devlin et al., 2019), to create useful text representations, they have be- to ours has been proposed by Ribeiro et al. (2020), come the standard choice when building NLP ap- where they investigate if PTLMs capture checkplications (Peters et al., 2018a; Devlin et al., 2019; lists such as: red, green and yellow. In contrast to them, we focus on finding out whether pretrained Radford and Narasimhan, 2018). However, there language models capture the correlation between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs enco"
2021.findings-emnlp.218,N19-1419,0,0.0202339,"to them, we focus on finding out whether pretrained Radford and Narasimhan, 2018). However, there language models capture the correlation between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs encode. Different probing experiments An example of semantic attributes and their valhave been proposed to study the drawbacks of ues is the relation that exists between old, age and PTLMs in areas such as the biomedical domain (Jin date of birth, or the relation between rich, wealth et al., 2019), syntax (Hewitt and Manning, 2019), and net worth. Looking up rich on FrameNet (Fillsemantic and syntactic sentence structures (Ten- more, 1982; Baker et al., 1998) would not result in a ney et al., 2019; Peters et al., 2018b), prenomial frame by itself, but would evoke the semantic frame anaphora (Sorodoc et al., 2020), linguistics (Be- wealthiness (Figure 1). In WordNet (Fellbaum, linkov et al., 2017; Clark et al., 2020; Tenney et al., 1998) these associations are called an attribute2019) and commonsense knowledge (Petroni et al., value relation, where the attribute is a noun for 2019; Davison et al., 2019; Talmor et al., 20"
2021.findings-emnlp.218,W19-2011,0,0.0443991,"Missing"
2021.findings-emnlp.218,P15-1036,0,0.0274865,"fferent pretrained language models: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al.). noun weight is an attribute, for which the adjectives light and heavy express values. Another example of these kind of associations is rich, which could be associated with wealthiness and net worth. Knowledge bases (KBs), such as Wikidata (Vrandeˇci´c and Krötzsch, 2014), constitute a valuable resource for collecting attributes and their values. In general, KBs have been shown to help improve multiple NLP application as they contain structured information (Annervaz et al., 2018; Nakashole and Mitchell, 2015; Rahman and Ng, 2011; Ratinov and Roth, 2009). As matter of fact, it is fairly simple to answer factoid questions such as “How old is Joe Biden?” using Wikidata, by Patterns The Wikipedia page of Jeff Bezos consimply looking up his date of birth on Wikidata. tains the statement: Bezos was named the richest An important step to make this happen is to match man in modern history after his net worth increased old and date of birth to each other. Similarly, to to X. This type of sentences appears frequently on check “how rich is Jeff Bezos?"", we only need to the web, which is why we construct a d"
2021.findings-emnlp.218,N18-1202,0,0.0528832,"worth. LU stands for lexical unit, FN1_Sent represents and Finished_Initial refer to which FrameNet version the lexical unit is from. Introduction In this paper, we expand on this line of research by probing PTLMs to investigate if they cover seGiven the ability of pretrained language models mantic attributes and their values. The closest work (PTLMs), such as BERT (Devlin et al., 2019), to create useful text representations, they have be- to ours has been proposed by Ribeiro et al. (2020), come the standard choice when building NLP ap- where they investigate if PTLMs capture checkplications (Peters et al., 2018a; Devlin et al., 2019; lists such as: red, green and yellow. In contrast to them, we focus on finding out whether pretrained Radford and Narasimhan, 2018). However, there language models capture the correlation between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs encode. Different probing experiments An example of semantic attributes and their valhave been proposed to study the drawbacks of ues is the relation that exists between old, age and PTLMs in areas such as the biomedical domain (Jin d"
2021.findings-emnlp.218,D18-1179,0,0.0232133,"worth. LU stands for lexical unit, FN1_Sent represents and Finished_Initial refer to which FrameNet version the lexical unit is from. Introduction In this paper, we expand on this line of research by probing PTLMs to investigate if they cover seGiven the ability of pretrained language models mantic attributes and their values. The closest work (PTLMs), such as BERT (Devlin et al., 2019), to create useful text representations, they have be- to ours has been proposed by Ribeiro et al. (2020), come the standard choice when building NLP ap- where they investigate if PTLMs capture checkplications (Peters et al., 2018a; Devlin et al., 2019; lists such as: red, green and yellow. In contrast to them, we focus on finding out whether pretrained Radford and Narasimhan, 2018). However, there language models capture the correlation between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs encode. Different probing experiments An example of semantic attributes and their valhave been proposed to study the drawbacks of ues is the relation that exists between old, age and PTLMs in areas such as the biomedical domain (Jin d"
2021.findings-emnlp.218,D19-1250,0,0.04756,"Missing"
2021.findings-emnlp.218,P11-1082,0,0.0283929,"dels: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al.). noun weight is an attribute, for which the adjectives light and heavy express values. Another example of these kind of associations is rich, which could be associated with wealthiness and net worth. Knowledge bases (KBs), such as Wikidata (Vrandeˇci´c and Krötzsch, 2014), constitute a valuable resource for collecting attributes and their values. In general, KBs have been shown to help improve multiple NLP application as they contain structured information (Annervaz et al., 2018; Nakashole and Mitchell, 2015; Rahman and Ng, 2011; Ratinov and Roth, 2009). As matter of fact, it is fairly simple to answer factoid questions such as “How old is Joe Biden?” using Wikidata, by Patterns The Wikipedia page of Jeff Bezos consimply looking up his date of birth on Wikidata. tains the statement: Bezos was named the richest An important step to make this happen is to match man in modern history after his net worth increased old and date of birth to each other. Similarly, to to X. This type of sentences appears frequently on check “how rich is Jeff Bezos?"", we only need to the web, which is why we construct a dataset based extract"
2021.findings-emnlp.218,P19-1452,0,0.0382343,"Missing"
2021.findings-emnlp.218,2020.emnlp-demos.6,0,0.0477528,"Missing"
2021.findings-emnlp.218,P19-1139,0,0.0229809,"te Wikidata entities and their values according to the semantic frames of each entity and every word that is evoked by a value. One advantage of PTLMs is their capability to perform well on specific tasks and domains that were not part of their training regime via fine-tuning, i.e. the retraining of a pretrained model with domain-specific examples. We argue that for entities and their values, resources such as FrameNet and WordNet, while paired with massive resources such as Wikidata could be used to fine-tune PTLMs towards more semantically-based objectives, as a complementary work to ERNIE (Zhang et al., 2019), which showed that fine-tuning PTLMs towards knowledge graphs helps enhancing language representation with external knowledge. 5 Conclusion We demonstrated that PTLMs are unable to capture semantic similarity between different words that refer to the same concepts. While PTLMs have been shown to improve the quality of many tasks and are not easy to train, our probing experiments show that an improvement is necessary. All the examples we extracted from Wikidata show that by enabling PTLMs to capture more semantically-based information by fine-tuning towards more semanticallybased objectives li"
2021.findings-emnlp.218,W09-1119,0,0.049607,"al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al.). noun weight is an attribute, for which the adjectives light and heavy express values. Another example of these kind of associations is rich, which could be associated with wealthiness and net worth. Knowledge bases (KBs), such as Wikidata (Vrandeˇci´c and Krötzsch, 2014), constitute a valuable resource for collecting attributes and their values. In general, KBs have been shown to help improve multiple NLP application as they contain structured information (Annervaz et al., 2018; Nakashole and Mitchell, 2015; Rahman and Ng, 2011; Ratinov and Roth, 2009). As matter of fact, it is fairly simple to answer factoid questions such as “How old is Joe Biden?” using Wikidata, by Patterns The Wikipedia page of Jeff Bezos consimply looking up his date of birth on Wikidata. tains the statement: Bezos was named the richest An important step to make this happen is to match man in modern history after his net worth increased old and date of birth to each other. Similarly, to to X. This type of sentences appears frequently on check “how rich is Jeff Bezos?"", we only need to the web, which is why we construct a dataset based extract his net worth from Wikida"
2021.findings-emnlp.218,2020.acl-main.442,0,0.0141678,"d be defined as a value-attribute pair, where rich is the value of an expression that represents an attribute: wealthiness/net worth. LU stands for lexical unit, FN1_Sent represents and Finished_Initial refer to which FrameNet version the lexical unit is from. Introduction In this paper, we expand on this line of research by probing PTLMs to investigate if they cover seGiven the ability of pretrained language models mantic attributes and their values. The closest work (PTLMs), such as BERT (Devlin et al., 2019), to create useful text representations, they have be- to ours has been proposed by Ribeiro et al. (2020), come the standard choice when building NLP ap- where they investigate if PTLMs capture checkplications (Peters et al., 2018a; Devlin et al., 2019; lists such as: red, green and yellow. In contrast to them, we focus on finding out whether pretrained Radford and Narasimhan, 2018). However, there language models capture the correlation between has recently been a rising amount of research that semantic attributes and their values. uses probes to understand the level of linguistics PTLMs encode. Different probing experiments An example of semantic attributes and their valhave been proposed to st"
2021.findings-emnlp.218,2020.acl-main.384,0,0.0327104,"encode. Different probing experiments An example of semantic attributes and their valhave been proposed to study the drawbacks of ues is the relation that exists between old, age and PTLMs in areas such as the biomedical domain (Jin date of birth, or the relation between rich, wealth et al., 2019), syntax (Hewitt and Manning, 2019), and net worth. Looking up rich on FrameNet (Fillsemantic and syntactic sentence structures (Ten- more, 1982; Baker et al., 1998) would not result in a ney et al., 2019; Peters et al., 2018b), prenomial frame by itself, but would evoke the semantic frame anaphora (Sorodoc et al., 2020), linguistics (Be- wealthiness (Figure 1). In WordNet (Fellbaum, linkov et al., 2017; Clark et al., 2020; Tenney et al., 1998) these associations are called an attribute2019) and commonsense knowledge (Petroni et al., value relation, where the attribute is a noun for 2019; Davison et al., 2019; Talmor et al., 2020). which adjectives express values. For instance, the 2554 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2554–2559 November 7–11, 2021. ©2021 Association for Computational Linguistics Patterns A, [MASK] and B. What’s [VALUE] or [MASK], A or B? What’s [VA"
2021.findings-emnlp.218,2020.tacl-1.48,0,0.0323105,"nd Manning, 2019), and net worth. Looking up rich on FrameNet (Fillsemantic and syntactic sentence structures (Ten- more, 1982; Baker et al., 1998) would not result in a ney et al., 2019; Peters et al., 2018b), prenomial frame by itself, but would evoke the semantic frame anaphora (Sorodoc et al., 2020), linguistics (Be- wealthiness (Figure 1). In WordNet (Fellbaum, linkov et al., 2017; Clark et al., 2020; Tenney et al., 1998) these associations are called an attribute2019) and commonsense knowledge (Petroni et al., value relation, where the attribute is a noun for 2019; Davison et al., 2019; Talmor et al., 2020). which adjectives express values. For instance, the 2554 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2554–2559 November 7–11, 2021. ©2021 Association for Computational Linguistics Patterns A, [MASK] and B. What’s [VALUE] or [MASK], A or B? What’s [VALUE] and [MASK], A or B? A is [MASK], thus they have a [VALUE] A is [VALUE], thus it has more [MASK] per m2 . To know which is [VALUE], A or B, you need [MASK]. What’s [VALUE], thus has a higher [MASK], A or B ? We need the [MASK] to know what’s [VALUE], A or B. We need the [VALUE] to know who is [MASK] A or B A is"
2021.konvens-1.15,2020.lrec-1.6,0,0.0136427,"ected tree where each node represents a mention is used to model the coreferences in a document. For determining antecedents, both local and non-local handcrafted features are employed. They created the current state-of-the-art approach for German news datasets, evaluating their system on the SemEval-2010 shared task and on version 10 of the T¨uBa-D/Z dataset. The domain of literature has, for both German and English, received increased attention in recent years with regard to coreference resolution. Roesiger et al. (2018) considered the domain specific challenges and phenomena of literature. Bamman et al. (2020) released an English dataset and Krug et al. (2018) released a German dataset (see Section 3.2 for details). While Krug (2020) performed coreference resolution on German literary data, Toshniwal et al. (2020) used the English dataset. Krug (2020) compare various approaches to coreference resolution on German historic novels using the DROC dataset (Krug et al., 2018). Their bestperforming system in a gold-mention scenario uses a rule-based Stanford Sieve approach (Lee et al., 2011), iteratively applying rules starting from the most precise rule, going to less precise rules. When mention spans a"
2021.konvens-1.15,P14-1035,0,0.0298144,"g documents such as literary works. Our code and ready-to-use models are publicly available. 1 Introduction Coreference resolution is the task of resolving text spans in documents that refer to the same entities. These are grouped into mention-clusters with each cluster representing one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overh"
2021.konvens-1.15,P14-1005,0,0.0696539,"Missing"
2021.konvens-1.15,2020.coling-main.598,0,0.0344865,"Missing"
2021.konvens-1.15,D16-1245,0,0.0380786,"Missing"
2021.konvens-1.15,P16-1061,0,0.0600069,"ared to existing entity representations and assigned to those that match them best. This way memory usage and computational effort can be reduced, as it is proportional to the number of entities, rather than the square of the number of mentions. 2 Related Work Relevant prior work can be put into two distinct categories: (a) Neural, state-of-the-art coreference resolution developed primarily on English (b) Coreference resolution applied to German. Most neural coreference resolution models perform a ranking of antecedents based on the pairwise scores of mention candidates (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017), at this only relying on local decisions that may not be globally optimal to form coherent entities (Lee et al., 2018). This general architecture has been improved on in multiple ways. To address the issue of global optimization, Clark and Manning (2016b) and Wiseman et al. (2016) create entity representations during the ranking step. Lee et al. (2018); Kantor and Globerson (2019) iteratively refine mention representations with associated antecedent information, performing what they refer to as higher-order inference. While the end-to-end coreference model of Lee et al. (2"
2021.konvens-1.15,2020.tacl-1.5,0,0.0132161,"LSTM (Hochreiter and Schmidhuber, 1997) to produce span representations, Lee et al. (2018) see a 3.2 F1 score increase on the English CoNLL-2012 shared task by additionally using ELMo (Peters et al., 2018) embeddings. Lee et al. (2018) also modify the model to perform coarse-to-fine antecedent pruning enabling an efficient computation and potentially allowing the processing of longer documents. Joshi et al. (2019) and Kantor and Globerson (2019) improve upon this by using BERT (Devlin et al., 2019) embeddings instead of the LSTM-based representations and gain another 3.3 F1 points. Recently, Joshi et al. (2020) presented a model optimized for span representations named SpanBERT and saw another 2.5 point increase in F1 score, which has been reproduced by Xu and Choi (2020). Wu et al. (2020) have taken a different approach to coreference resolution; they outperform previous state of the art by 3.5 F1 points in part due to the ability to recover missed mentions by framing the task as a question-answering problem. Toshniwal et al. (2020); Xia et al. (2020) both introduce incremental approaches to coreference resolution. Instead of comparing mention pairs like Lee et al. (2017), they compare mentions wit"
2021.konvens-1.15,D19-1588,0,0.176417,"senting one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overhead; before [ her ] 1 was [ another long passage ] 2, and [ the White Rabbit ] 3 was still in sight, hurrying down [ it ] 2. Figure 1: Coreference gold annotations for “Alice’s Adventures in Wonderland” (annotations from Bamman et al., 2020) first end-to-end neural system (L"
2021.konvens-1.15,P19-1066,0,0.0170026,"lish (b) Coreference resolution applied to German. Most neural coreference resolution models perform a ranking of antecedents based on the pairwise scores of mention candidates (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017), at this only relying on local decisions that may not be globally optimal to form coherent entities (Lee et al., 2018). This general architecture has been improved on in multiple ways. To address the issue of global optimization, Clark and Manning (2016b) and Wiseman et al. (2016) create entity representations during the ranking step. Lee et al. (2018); Kantor and Globerson (2019) iteratively refine mention representations with associated antecedent information, performing what they refer to as higher-order inference. While the end-to-end coreference model of Lee et al. (2017) uses a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to produce span representations, Lee et al. (2018) see a 3.2 F1 score increase on the English CoNLL-2012 shared task by additionally using ELMo (Peters et al., 2018) embeddings. Lee et al. (2018) also modify the model to perform coarse-to-fine antecedent pruning enabling an efficient computation and potentially allowing the processing o"
2021.konvens-1.15,R11-1025,0,0.0122611,"ke Lee et al. (2017), they compare mentions with entity representations, with the entity representations being produced from a linear combination of their mentions. Both approaches work by iteratively processing all mentions and scoring each mention with regard to a set of entities; as a result, an evaluation of the full cross-product of mentions is not necessary. The two approaches differ slightly in how they handle the introduction of new entities. For coreference resolution on German texts, published work predates the age of neural networks in natural language processing. The CorZu system (Klenner and Tuggener, 2011; Tuggener and Klenner, 2014) is a rule-based incremental entitymention model that has been extended with Markov Logic Networks for the antecedent selection. Roesiger and Kuhn (2016) adapted the English system of Bj¨orkelund and Kuhn (2014) to German. A directed tree where each node represents a mention is used to model the coreferences in a document. For determining antecedents, both local and non-local handcrafted features are employed. They created the current state-of-the-art approach for German news datasets, evaluating their system on the SemEval-2010 shared task and on version 10 of the"
2021.konvens-1.15,W11-1902,0,0.32813,"These are grouped into mention-clusters with each cluster representing one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overhead; before [ her ] 1 was [ another long passage ] 2, and [ the White Rabbit ] 3 was still in sight, hurrying down [ it ] 2. Figure 1: Coreference gold annotations for “Alice’s Adventures in Wonderland” (annota"
2021.konvens-1.15,D17-1018,0,0.328881,"each cluster representing one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overhead; before [ her ] 1 was [ another long passage ] 2, and [ the White Rabbit ] 3 was still in sight, hurrying down [ it ] 2. Figure 1: Coreference gold annotations for “Alice’s Adventures in Wonderland” (annotations from Bamman et al., 2020) first end-to"
2021.konvens-1.15,N18-2108,0,0.138948,"Missing"
2021.konvens-1.15,W99-0212,0,0.0167637,"we make use of two distinct model architectures: a mention linking-based and an incremental entity-based approach that should scale to very long documents such as literary works. Our code and ready-to-use models are publicly available. 1 Introduction Coreference resolution is the task of resolving text spans in documents that refer to the same entities. These are grouped into mention-clusters with each cluster representing one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal"
2021.konvens-1.15,2020.lrec-1.7,0,0.357648,"a variant of the DROC dataset with all singleton mentions removed. Using Welch’s t-test we can infer that the pertrained version does, on average, perform better for the no singleton variant (p < 0.005). As a result we will use the news-pretrained model variant in all our further experiments. This finding is also supported by the recent publication by (Xia and Durme, 2021) which establishes that, especially for short datasets, using pretrained weights is beneficial. We are unsure if further significant improvements could be gained by pre-training on additional datasets, for example GerDraCor (Pagel and Reiter, 2020), given that T¨uBa-D/Z is already a large dataset. Table 5 shows how two configuration parameters affect the coarse-to-fine model’s performance. The two options enable different features, where “segment info” describes how many BERT segments lie between the current and candidate mention while “token info” describes the token distance from the candidate mention to the document start. Further, “token info” encodes the length of the candidate mention span. This experiment was performed as 80 Segment Token 7 3 7 3 7 7 3 3 Coarse-To-Fine Incremental 61.11 ± 0.57 62.31 ± 0.27 61.70 ± 0.22 59.93 ± 0."
2021.konvens-1.15,N18-1202,0,0.0249131,"dress the issue of global optimization, Clark and Manning (2016b) and Wiseman et al. (2016) create entity representations during the ranking step. Lee et al. (2018); Kantor and Globerson (2019) iteratively refine mention representations with associated antecedent information, performing what they refer to as higher-order inference. While the end-to-end coreference model of Lee et al. (2017) uses a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to produce span representations, Lee et al. (2018) see a 3.2 F1 score increase on the English CoNLL-2012 shared task by additionally using ELMo (Peters et al., 2018) embeddings. Lee et al. (2018) also modify the model to perform coarse-to-fine antecedent pruning enabling an efficient computation and potentially allowing the processing of longer documents. Joshi et al. (2019) and Kantor and Globerson (2019) improve upon this by using BERT (Devlin et al., 2019) embeddings instead of the LSTM-based representations and gain another 3.3 F1 points. Recently, Joshi et al. (2020) presented a model optimized for span representations named SpanBERT and saw another 2.5 point increase in F1 score, which has been reproduced by Xu and Choi (2020). Wu et al. (2020) have"
2021.konvens-1.15,W12-4501,0,0.255627,"lors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overhead; before [ her ] 1 was [ another long passage ] 2, and [ the White Rabbit ] 3 was still in sight, hurrying down [ it ] 2. Figure 1: Coreference gold annotations for “Alice’s Adventures in Wonderland” (annotations from Bamman et al., 2020) first end-to-end neural system (Lee et al., 2017) has shown that neural systems are key to state-of-theart performance. Coreference resolution on German using neural networks h"
2021.konvens-1.15,W09-2411,0,0.0940301,"Missing"
2021.konvens-1.15,L16-1024,0,0.153277,"er to the same entities. These are grouped into mention-clusters with each cluster representing one entity. Figure 1 shows coreference annotations on a literary text with different entities being denoted by both subscripts and colors. Tasks such as question answering (Morton, 1999) or text summarization (Steinberger et al., 2007) can rely on coreference resolution as part of the language processing pipeline. Bamman et al. (2014) demonstrated that coreference resolution is also applicable to literary analysis. The task has recently seen large improvements as systems moved from rule-based (e.g. Roesiger and Kuhn, 2016; Lee et al., 2011) to neural approaches (e.g. Lee et al., 2017; Joshi et al., 2019). This advancement from a CoNLL-F1-score of 57.8, achieved by a rule-based system in the original CoNLL-2012 shared task (Pradhan et al., 2012), to 67.2 in the ∗ denotes equal contribution [ Alice ] 1 was not a bit hurt, and [ she ] 1 jumped up on to [ her ] 1 feet in a moment: [ she ] 1 looked up, but it was all dark overhead; before [ her ] 1 was [ another long passage ] 2, and [ the White Rabbit ] 3 was still in sight, hurrying down [ it ] 2. Figure 1: Coreference gold annotations for “Alice’s Adventures in"
2021.konvens-1.15,W18-4515,0,0.0198857,"ger and Kuhn (2016) adapted the English system of Bj¨orkelund and Kuhn (2014) to German. A directed tree where each node represents a mention is used to model the coreferences in a document. For determining antecedents, both local and non-local handcrafted features are employed. They created the current state-of-the-art approach for German news datasets, evaluating their system on the SemEval-2010 shared task and on version 10 of the T¨uBa-D/Z dataset. The domain of literature has, for both German and English, received increased attention in recent years with regard to coreference resolution. Roesiger et al. (2018) considered the domain specific challenges and phenomena of literature. Bamman et al. (2020) released an English dataset and Krug et al. (2018) released a German dataset (see Section 3.2 for details). While Krug (2020) performed coreference resolution on German literary data, Toshniwal et al. (2020) used the English dataset. Krug (2020) compare various approaches to coreference resolution on German historic novels using the DROC dataset (Krug et al., 2018). Their bestperforming system in a gold-mention scenario uses a rule-based Stanford Sieve approach (Lee et al., 2011), iteratively applying"
2021.konvens-1.15,2020.emnlp-main.685,0,0.249242,"or and Globerson (2019) improve upon this by using BERT (Devlin et al., 2019) embeddings instead of the LSTM-based representations and gain another 3.3 F1 points. Recently, Joshi et al. (2020) presented a model optimized for span representations named SpanBERT and saw another 2.5 point increase in F1 score, which has been reproduced by Xu and Choi (2020). Wu et al. (2020) have taken a different approach to coreference resolution; they outperform previous state of the art by 3.5 F1 points in part due to the ability to recover missed mentions by framing the task as a question-answering problem. Toshniwal et al. (2020); Xia et al. (2020) both introduce incremental approaches to coreference resolution. Instead of comparing mention pairs like Lee et al. (2017), they compare mentions with entity representations, with the entity representations being produced from a linear combination of their mentions. Both approaches work by iteratively processing all mentions and scoring each mention with regard to a set of entities; as a result, an evaluation of the full cross-product of mentions is not necessary. The two approaches differ slightly in how they handle the introduction of new entities. For coreference resolut"
2021.konvens-1.15,P15-1137,0,0.0248965,"/tree/konvens ing compared to existing entity representations and assigned to those that match them best. This way memory usage and computational effort can be reduced, as it is proportional to the number of entities, rather than the square of the number of mentions. 2 Related Work Relevant prior work can be put into two distinct categories: (a) Neural, state-of-the-art coreference resolution developed primarily on English (b) Coreference resolution applied to German. Most neural coreference resolution models perform a ranking of antecedents based on the pairwise scores of mention candidates (Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017), at this only relying on local decisions that may not be globally optimal to form coherent entities (Lee et al., 2018). This general architecture has been improved on in multiple ways. To address the issue of global optimization, Clark and Manning (2016b) and Wiseman et al. (2016) create entity representations during the ranking step. Lee et al. (2018); Kantor and Globerson (2019) iteratively refine mention representations with associated antecedent information, performing what they refer to as higher-order inference. While the end-to-end coreferen"
2021.konvens-1.15,N16-1114,0,0.045684,"Missing"
2021.konvens-1.15,2020.acl-main.622,0,0.0679392,"(Peters et al., 2018) embeddings. Lee et al. (2018) also modify the model to perform coarse-to-fine antecedent pruning enabling an efficient computation and potentially allowing the processing of longer documents. Joshi et al. (2019) and Kantor and Globerson (2019) improve upon this by using BERT (Devlin et al., 2019) embeddings instead of the LSTM-based representations and gain another 3.3 F1 points. Recently, Joshi et al. (2020) presented a model optimized for span representations named SpanBERT and saw another 2.5 point increase in F1 score, which has been reproduced by Xu and Choi (2020). Wu et al. (2020) have taken a different approach to coreference resolution; they outperform previous state of the art by 3.5 F1 points in part due to the ability to recover missed mentions by framing the task as a question-answering problem. Toshniwal et al. (2020); Xia et al. (2020) both introduce incremental approaches to coreference resolution. Instead of comparing mention pairs like Lee et al. (2017), they compare mentions with entity representations, with the entity representations being produced from a linear combination of their mentions. Both approaches work by iteratively processing all mentions and"
2021.konvens-1.15,2021.emnlp-main.425,0,0.0585966,"Missing"
2021.konvens-1.15,2020.emnlp-main.695,0,0.067141,"Missing"
2021.konvens-1.15,2020.emnlp-main.686,0,0.232202,"tionally using ELMo (Peters et al., 2018) embeddings. Lee et al. (2018) also modify the model to perform coarse-to-fine antecedent pruning enabling an efficient computation and potentially allowing the processing of longer documents. Joshi et al. (2019) and Kantor and Globerson (2019) improve upon this by using BERT (Devlin et al., 2019) embeddings instead of the LSTM-based representations and gain another 3.3 F1 points. Recently, Joshi et al. (2020) presented a model optimized for span representations named SpanBERT and saw another 2.5 point increase in F1 score, which has been reproduced by Xu and Choi (2020). Wu et al. (2020) have taken a different approach to coreference resolution; they outperform previous state of the art by 3.5 F1 points in part due to the ability to recover missed mentions by framing the task as a question-answering problem. Toshniwal et al. (2020); Xia et al. (2020) both introduce incremental approaches to coreference resolution. Instead of comparing mention pairs like Lee et al. (2017), they compare mentions with entity representations, with the entity representations being produced from a linear combination of their mentions. Both approaches work by iteratively processing"
2021.konvens-1.4,W17-1101,0,0.0412544,"Missing"
2021.konvens-1.4,gao-huang-2017-detecting,0,0.029397,"Missing"
2021.konvens-1.4,N12-1084,0,0.0338311,"r, ethnicity, gender, sexual orientation, nationality, or religion (Nockleby et al., 2000; Davidson et al., 2017). Some examples given by Schmidt and Wiegand (2017) are: • Go fucking kill yourself and die already a useless ugly pile of shit scumbag. • The Jew Faggot Behind The Financial Collapse. ∗ Equal contribution • Hope one of those bitches falls over and breaks her leg. Several sensitive comments on social media platforms have led to crime against minorities (Williams et al., 2020). Hate speech can be considered as an umbrella term that different authors have coined with different names. Xu et al. (2012); Hosseinmardi et al. (2015); Zhong et al. (2016) referred it by the term cyberbully-ing, while Davidson et al. (2017) used the term offensive language to some expressions that can be strongly impolite, rude or use of vulgar words towards an individual or group that can even ignite ﬁghts or be hurtful. Use of words like f**k, n*gga, b*tch is common in social media comments, song lyrics, etc. Although these terms can be treated as obscene and inappropriate, some people also use them in non-hateful ways in different contexts (Davidson et al., 2017). This makes it challenging for all hate speech"
2021.naacl-demos.12,P13-4001,1,0.491332,"Missing"
2021.naacl-demos.12,S19-2010,0,0.0341701,"Missing"
2021.naacl-demos.12,Q17-1010,0,0.0583071,"Missing"
2021.naacl-main.351,W18-0518,0,0.0139136,"interpretable. 2.2 Aspects of word complexity of readability formulas (Dale and Chall, 1948; Kincaid et al., 1975; Dubay, 2004). Frequency, another factor often considered in readability and text simplification approaches (Rudell, 1993; De Belder and Moens, 2010), was shown to correlate and cause word familiarity, which in its turn contributes to higher word recognition and lower reaction times (Connine et al., 1990; Morrel-Samuels and Krauss, 1992). Notably, word length and frequency have been widely used in CWI systems, and are reported to be good, cross-linguistic predictors of complexity (Bingel and Bjerva, 2018). Other factors considered important for word complexity include a variety of psycholinguistic properties, including word’s age of acquisition, concreteness, and imagability (Carroll and White, 1973; Zevin and Seidenberg, 2002; Begg and Paivio, 1969). At the same time, not all factors are equally applicable to all groups of readers: for instance, while frequency may be an important factor for second language learners, other populations may be more affected by the length of a word or the occurrence of certain character combinations (Rudell, 1993; Rello et al., 2013). Yet, little is still known"
2021.naacl-main.351,C18-1021,0,0.0686439,"ions, such as children, non-native ciency level of a non-native reader. To facilispeakers, or readers with particular cognitive imtate reproducibility of our approach and foster pairments. Thus, personalization in LS typically further research into these aspects, we release results in specialized simplification tools aimed at a dataset of complex words annotated by readers with different backgrounds. certain groups of readers (Carroll et al., 1998; Rello et al., 2013; Evans et al., 2014), with only a few 1 Introduction systems addressing adaptation to the readers’ needs in a more dynamic way (Bingel et al., 2018; Yimam Complex word identification (CWI) is the first step and Biemann, 2018a,b; Scarton and Specia, 2018). in a lexical simplification (LS) pipeline, concerned with identification of words in text that are in need Despite CWI being one of the key steps in an of further simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler"
2021.naacl-main.351,P11-2087,0,0.095553,"e distribution of features for complex words across differing proficiency levels. • Finally, we release a CWI dataset annotated by readers with different backgrounds. 2 2.1 Background Models of Complex Word Identification CWI was established as an essential step in LS in Shardlow (2013), which demonstrated that without this step, LS systems tend to over- or undersimplify, thus rendering the output less useful for the readers. Early approaches to this task considered simplification of all words (Devlin and Tait, 1998; Bott et al., 2012) and use of frequency-based thresholds (Zeng et al., 2005; Biran et al., 2011), however Shardlow (2013) shows that classification algorithms are more precise in identification of complex words than both these approaches. Recent shared tasks on CWI (Paetzold and Specia, 2016c; Yimam et al., 2018) helped it gain popularity in the NLP community as they provide researchers with shared data and benchmarks. Most systems participating in the shared tasks addressed CWI with classical machine learning algorithms, with the best-performing systems using ensemblebased approaches. Current state-of-the-art results on CWI are achieved by a sequence-labeling model of Gooding and Kochma"
2021.naacl-main.351,C12-1023,0,0.0169042,"the notion of complexity for native and non-native audiences. • We analyse the distribution of features for complex words across differing proficiency levels. • Finally, we release a CWI dataset annotated by readers with different backgrounds. 2 2.1 Background Models of Complex Word Identification CWI was established as an essential step in LS in Shardlow (2013), which demonstrated that without this step, LS systems tend to over- or undersimplify, thus rendering the output less useful for the readers. Early approaches to this task considered simplification of all words (Devlin and Tait, 1998; Bott et al., 2012) and use of frequency-based thresholds (Zeng et al., 2005; Biran et al., 2011), however Shardlow (2013) shows that classification algorithms are more precise in identification of complex words than both these approaches. Recent shared tasks on CWI (Paetzold and Specia, 2016c; Yimam et al., 2018) helped it gain popularity in the NLP community as they provide researchers with shared data and benchmarks. Most systems participating in the shared tasks addressed CWI with classical machine learning algorithms, with the best-performing systems using ensemblebased approaches. Current state-of-the-art"
2021.naacl-main.351,P11-2117,0,0.0395578,"able 5, with label 1 denoting complex words and label 0 used for non-complex words. It is worth noting that the groups of annotators labelling portions of the dataset were not fixed. Within each group, the proficiency distribution varied, with some containing no annotators from a given class. 4 • SubIMDB: using the SubIMDB corpus (Paetzold and Specia, 2016b), the word frequencies are calculated from the ‘Movies and Series for Children’ section. The top 1, 000 most frequent words are then included. • Simple Wikipedia (SimpWiki): a list of the top 6, 368 words contained in the Simple Wikipedia (Coster and Kauchak, 2011). • Ogden’s Basic English: the top 1, 000 words from Ogden’s Basic English list (Ogden, 1968). • Cambridge Advanced Learner’s Dictionary (CALD):2 the entries contained in the Cambridge Advanced Learner’s Dictionary. Method We firstly show that when predicting word complexity, the needs of sub-groups differ and are best predicted using models targeting them specifically. We demonstrate that the best performing models for a sub-group are trained with the annotations of that group using a classical machine learning approach. Secondly, we analyse the correlation of features with the number of anno"
2021.naacl-main.351,W14-1215,0,0.037987,"Missing"
2021.naacl-main.351,S13-1035,0,0.0118393,"e word are included sourced from the MCR Psycholinguistic Database (Wilson, 1988). POS & Dependency Parse Relations: The target sentence is parsed using the NLPCore pipeline. Following this, the number of dependency relations are counted to produce a feature. The part-ofspeech tag for the word is additionally included. List-Based Features: A set of binary features are used that indicate the presence of the target word in a given list. The source of each list is outlined below: Complexity Features Word Frequency: The frequency of the target word is estimated using the Google dataset of ngrams (Goldberg and Orwant, 2013). Additionally, the Thorndike-Lorge written frequency derived from Thorndike and Lorge (1944) is obtained from the MCR Psycholinguistic Database (Wilson, 1988). Psycholinguistic Features: Finally, the following features are extracted from the MCR Psycholinguistic Database (Wilson, 1988): To gain fundamental insights into the performance across proficiency groups, we run experiments using the C AMB system by Gooding and Kochmar (2018) as it achieved the best results across all binary and two probabilistic tracks in the CWI 2018 shared task (Yimam et al., 2018). Furthermore, the code for this sy"
2021.naacl-main.351,W18-0520,1,0.796533,". The source of each list is outlined below: Complexity Features Word Frequency: The frequency of the target word is estimated using the Google dataset of ngrams (Goldberg and Orwant, 2013). Additionally, the Thorndike-Lorge written frequency derived from Thorndike and Lorge (1944) is obtained from the MCR Psycholinguistic Database (Wilson, 1988). Psycholinguistic Features: Finally, the following features are extracted from the MCR Psycholinguistic Database (Wilson, 1988): To gain fundamental insights into the performance across proficiency groups, we run experiments using the C AMB system by Gooding and Kochmar (2018) as it achieved the best results across all binary and two probabilistic tracks in the CWI 2018 shared task (Yimam et al., 2018). Furthermore, the code for this system has been made publicly available by the authors. The C AMB system relies on 27 features in total. Feature types include lexical, syntactic, frequency-based and other aspects of information about individual words, outlined below. Lexical Features: For each target word, the word itself as well as the length and number of syllables (obtained using the Datamuse API) is included. Additionally, the number of senses, hypernyms and hypo"
2021.naacl-main.351,P19-1109,1,0.866892,"h identification of words in text that are in need Despite CWI being one of the key steps in an of further simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler alternative, ing and widely used datasets on CWI present a e.g. flooded, in the next step (Paetzold and Specia, homogeneous view on word complexity, merging 2016a; Gooding and Kochmar, 2019b): annotations from various groups of readers (Paetzold and Specia, 2016c; Yimam et al., 2018). From (1) Water engulfed Beringia. the cognitive perspective, little is still known about ↓ the challenges that particular readers face when deWater flooded Beringia. veloping their reading skills and about the factors It has been shown that accurate CWI can sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the developme"
2021.naacl-main.351,D19-1491,1,0.897909,"h identification of words in text that are in need Despite CWI being one of the key steps in an of further simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler alternative, ing and widely used datasets on CWI present a e.g. flooded, in the next step (Paetzold and Specia, homogeneous view on word complexity, merging 2016a; Gooding and Kochmar, 2019b): annotations from various groups of readers (Paetzold and Specia, 2016c; Yimam et al., 2018). From (1) Water engulfed Beringia. the cognitive perspective, little is still known about ↓ the challenges that particular readers face when deWater flooded Beringia. veloping their reading skills and about the factors It has been shown that accurate CWI can sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the developme"
2021.naacl-main.351,C18-1019,0,0.0155329,"om various groups of readers (Paetzold and Specia, 2016c; Yimam et al., 2018). From (1) Water engulfed Beringia. the cognitive perspective, little is still known about ↓ the challenges that particular readers face when deWater flooded Beringia. veloping their reading skills and about the factors It has been shown that accurate CWI can sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the development system output (Lee and Yeung, 2018). In addi- of reading abilities: whether a reader is a native tion, CWI has been shown to be an important com- speaker of the language, and if not, what is the ponent in readability assessment systems (Mad- reader’s level of language proficiency. We use the dela and Xu, 2018) and in vocabulary acquisition data from Yimam et al. (2017a), which contains modules of educational applications (Zaidi et al., English sentences where complex words are anno2020). However, an important aspect of CWI and tated by native and non-native speakers of English, LS that is often neglected is that text complexity"
2021.naacl-main.351,D18-1410,0,0.0366055,"Missing"
2021.naacl-main.351,L16-1491,0,0.277643,"vel of language Lexical complexity is a highly subjective noproficiency) would not necessarily be challenging tion, yet this factor is often neglected in lexical for readers with other backgrounds (for example, simplification and readability systems which more proficient readers) (Bingel, 2018). A numuse a “one-size-fits-all"" approach. In this paber of factors may contribute to that, including per, we investigate which aspects contribute to the notion of lexical complexity in various the reader’s age and level of language proficiency, groups of readers, focusing on native and nonamong others (Paetzold and Specia, 2016c). LS native speakers of English, and how the notion systems often aim to address the needs of specific of complexity changes depending on the profireader populations, such as children, non-native ciency level of a non-native reader. To facilispeakers, or readers with particular cognitive imtate reproducibility of our approach and foster pairments. Thus, personalization in LS typically further research into these aspects, we release results in specialized simplification tools aimed at a dataset of complex words annotated by readers with different backgrounds. certain groups of readers (Carrol"
2021.naacl-main.351,C16-1157,0,0.339574,"vel of language Lexical complexity is a highly subjective noproficiency) would not necessarily be challenging tion, yet this factor is often neglected in lexical for readers with other backgrounds (for example, simplification and readability systems which more proficient readers) (Bingel, 2018). A numuse a “one-size-fits-all"" approach. In this paber of factors may contribute to that, including per, we investigate which aspects contribute to the notion of lexical complexity in various the reader’s age and level of language proficiency, groups of readers, focusing on native and nonamong others (Paetzold and Specia, 2016c). LS native speakers of English, and how the notion systems often aim to address the needs of specific of complexity changes depending on the profireader populations, such as children, non-native ciency level of a non-native reader. To facilispeakers, or readers with particular cognitive imtate reproducibility of our approach and foster pairments. Thus, personalization in LS typically further research into these aspects, we release results in specialized simplification tools aimed at a dataset of complex words annotated by readers with different backgrounds. certain groups of readers (Carrol"
2021.naacl-main.351,P18-2113,0,0.0220431,"with particular cognitive imtate reproducibility of our approach and foster pairments. Thus, personalization in LS typically further research into these aspects, we release results in specialized simplification tools aimed at a dataset of complex words annotated by readers with different backgrounds. certain groups of readers (Carroll et al., 1998; Rello et al., 2013; Evans et al., 2014), with only a few 1 Introduction systems addressing adaptation to the readers’ needs in a more dynamic way (Bingel et al., 2018; Yimam Complex word identification (CWI) is the first step and Biemann, 2018a,b; Scarton and Specia, 2018). in a lexical simplification (LS) pipeline, concerned with identification of words in text that are in need Despite CWI being one of the key steps in an of further simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler alternative, ing and widely used datasets on CWI present a e.g. flooded, in the next step (Paetzold and Sp"
2021.naacl-main.351,P13-3015,0,0.0810504,"ation tools aimed at a dataset of complex words annotated by readers with different backgrounds. certain groups of readers (Carroll et al., 1998; Rello et al., 2013; Evans et al., 2014), with only a few 1 Introduction systems addressing adaptation to the readers’ needs in a more dynamic way (Bingel et al., 2018; Yimam Complex word identification (CWI) is the first step and Biemann, 2018a,b; Scarton and Specia, 2018). in a lexical simplification (LS) pipeline, concerned with identification of words in text that are in need Despite CWI being one of the key steps in an of further simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler alternative, ing and widely used datasets on CWI present a e.g. flooded, in the next step (Paetzold and Specia, homogeneous view on word complexity, merging 2016a; Gooding and Kochmar, 2019b): annotations from various groups of readers (Paetzold and Specia, 2016c; Yimam et al., 2018). From (1) Water e"
2021.naacl-main.351,shardlow-2014-open,0,0.0507176,"Missing"
2021.naacl-main.351,S12-1046,0,0.0351151,"ontext has been marked as complex by 10 non-native and 1 native annotator. This indicates that the word might be more challenging for a nonnative audience than for native in the given context. Figure 3 shows the highest correlated features for the native and non-native groups, all of which are significant (p < .001). Overall, the correlations for the native class are higher than for non-native which is likely due to a more united perspective of complexity. This follows as individuals with a similar first language or educational background are more likely to annotate the same words as complex (Specia et al., 2012). For both classes, the feature with the highest 4444 correlation is that of word length: the positive correlation shows that the longer the word, the more likely it will belong to the complex class. Following this, for the native class we see that the number of syllables is second. Whilst the length of a word and the number of syllables are highly correlated (0.64), it is interesting to note that the number of syllables correlates more highly with the native notion of complexity than for non-native. This may be explained by the fact that syllable and phoneme awareness plays an independent rol"
2021.naacl-main.351,D18-2009,1,0.864491,"Missing"
2021.naacl-main.351,W18-0507,1,0.826451,"rther simplification (Shardlow, 2013). For in- LS pipeline in need of adaptation to readers’ prostance, in example (1) a CWI system might identify files, this is rarely addressed in practice (Lee and engulfed as a complex word, which would allow an Yeung, 2018; Bingel, 2018). For instance, existLS system to replace it with a simpler alternative, ing and widely used datasets on CWI present a e.g. flooded, in the next step (Paetzold and Specia, homogeneous view on word complexity, merging 2016a; Gooding and Kochmar, 2019b): annotations from various groups of readers (Paetzold and Specia, 2016c; Yimam et al., 2018). From (1) Water engulfed Beringia. the cognitive perspective, little is still known about ↓ the challenges that particular readers face when deWater flooded Beringia. veloping their reading skills and about the factors It has been shown that accurate CWI can sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the development system output (Lee and Yeung, 2018). In addi- of reading abilities: whether a reader is a na"
2021.naacl-main.351,I17-2068,1,0.937004,"sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the development system output (Lee and Yeung, 2018). In addi- of reading abilities: whether a reader is a native tion, CWI has been shown to be an important com- speaker of the language, and if not, what is the ponent in readability assessment systems (Mad- reader’s level of language proficiency. We use the dela and Xu, 2018) and in vocabulary acquisition data from Yimam et al. (2017a), which contains modules of educational applications (Zaidi et al., English sentences where complex words are anno2020). However, an important aspect of CWI and tated by native and non-native speakers of English, LS that is often neglected is that text complexity is spanning three different levels of language profinot an objective notion homogeneous across vari- ciency. We investigate which aspects contribute to ous target populations: what is challenging for a the notion of lexical complexity for readers with 4439 Proceedings of the 2021 Conference of the North American Chapter of the Assoc"
2021.naacl-main.351,yimam-etal-2017-multilingual,1,0.939575,"sig- contributing to their vocabulary acquisition. nificantly reduce errors in simplification (ShardIn this paper, we investigate factors focusing on low, 2014), thus improving the quality of an LS the two key background aspects in the development system output (Lee and Yeung, 2018). In addi- of reading abilities: whether a reader is a native tion, CWI has been shown to be an important com- speaker of the language, and if not, what is the ponent in readability assessment systems (Mad- reader’s level of language proficiency. We use the dela and Xu, 2018) and in vocabulary acquisition data from Yimam et al. (2017a), which contains modules of educational applications (Zaidi et al., English sentences where complex words are anno2020). However, an important aspect of CWI and tated by native and non-native speakers of English, LS that is often neglected is that text complexity is spanning three different levels of language profinot an objective notion homogeneous across vari- ciency. We investigate which aspects contribute to ous target populations: what is challenging for a the notion of lexical complexity for readers with 4439 Proceedings of the 2021 Conference of the North American Chapter of the Assoc"
2021.naacl-srw.21,N19-1423,0,0.0196895,"kiCaps Subset for Multi-Modal Image-Retrieval); b) an in-depth analysis and comparison of WISMIR to other multimodal datasets used for image-retrieval; c) a textimage retrieval evaluation of state-of-the-art imageretrieval models on WISMIR. 2 Related Work During the last few years, there were significant breakthroughs in various computer vision tasks and models (Kirillov et al., 2020; Güler et al., 2018) as well as in the field of natural language processing. Especially with the recent dawn of transformers, models are increasingly capable of understanding text’s semantics (Brown et al., 2020; Devlin et al., 2019; Yang et al., 2019). This progress of uni-modal models also led to a great leap forward in multi-modal visio-linguistic models, which are starting to leverage the power of transformers to 3 https://flickr.com/ work with text and images simultaneously. One of the several multi-modal tasks where these models pushed the boundaries is text-image retrieval, which we want to make use of in our language learner scenario. For this task, the model learns a metric function Φk,l : R|Sk |×|Il |→ [0, 1] that measures the similarity of sentence Sk and image Il . The goal is to find the best matching image"
2021.naacl-srw.21,W18-1814,0,0.151957,"eval are MS COCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). Both datasets were created by crowdsourcing workers with the task to find short, simple and descriptive captions for images taken from Flickr3 . We argue that sentences slightly advanced language learners might not comprehend are presumably more complex than the captions from COCO or Flickr30k. Hence we further claim that current models will perform poorly on more complex data. The contributions of this work to verify these hypotheses are: a) the collection of a multi-modal dataset based on WikiCaps (Schamoni et al., 2018), which we call WISMIR (WIkiCaps Subset for Multi-Modal Image-Retrieval); b) an in-depth analysis and comparison of WISMIR to other multimodal datasets used for image-retrieval; c) a textimage retrieval evaluation of state-of-the-art imageretrieval models on WISMIR. 2 Related Work During the last few years, there were significant breakthroughs in various computer vision tasks and models (Kirillov et al., 2020; Güler et al., 2018) as well as in the field of natural language processing. Especially with the recent dawn of transformers, models are increasingly capable of understanding text’s seman"
2021.naacl-srw.21,P18-1238,0,0.0274986,"Missing"
2021.naacl-srw.21,D19-1514,0,0.0161559,"e of tasks on typical datasets like COCO or Flickr30k, early-fusion models are not applicable in real-world information retrieval systems with large pool of images because it would require tremendous computational power and is therefore infeasible in time-critical applications. As opposed to early-fusion models, in latefusion models, the textual and visual modalities get forwarded through separate transformers for each modality. Later, the output of the textual transformer and the output of the visual transformer get fused depending on the model’s specific implementation. For example, LXMERT (Tan and Bansal, 2019) and VilBERT (Lu et al., 2019) compute the fused cross-modality output with a third crossmodal transformer that takes the separate and unimodal transformers’ outputs as inputs. Other latefusion models specially designed to solve multimodal retrieval tasks like TERN (Messina et al., 2020) and TERAN (Nicola et al., 2020) use a more computationally efficient way. A significant advantage of late-fusion models over early-fusion models is that the output embeddings of the uni-modal transformers can be pre-computed and indexed. In real-world applications, with a large pool of images, this can save en"
2021.naacl-srw.21,Q14-1006,0,0.0144619,"8; Xie et al., 2019; Albahiri and Alhaj, 2020). This paper presents initial research towards leveraging machine learning technology within a language learner context to improve human reading. 1 2 https://babbel.com/ https://duolingo.com/ In this scenario, the aim is to support a user’s reading comprehension of arbitrary text by enhancing it with context-specific visual clues discovered by state-of-the-art multi-modal Transformers used within text-image retrieval. The most popular training datasets for current models applied on text-image retrieval are MS COCO (Lin et al., 2014) and Flickr30k (Young et al., 2014; Plummer et al., 2015). Both datasets were created by crowdsourcing workers with the task to find short, simple and descriptive captions for images taken from Flickr3 . We argue that sentences slightly advanced language learners might not comprehend are presumably more complex than the captions from COCO or Flickr30k. Hence we further claim that current models will perform poorly on more complex data. The contributions of this work to verify these hypotheses are: a) the collection of a multi-modal dataset based on WikiCaps (Schamoni et al., 2018), which we call WISMIR (WIkiCaps Subset for Mul"
2021.naacl-srw.5,burchardt-etal-2006-salsa,0,0.107754,"ature, such as news (Doddington et al., 2004; Chambers and Jurafsky, 2008). More recently, Sims et al. (2019) created a new dataset of annotated literary texts. 2.2 Event Sequences Semantic Frame Induction Semantic frames, in the context of FrameNet (Baker et al., 1998), are definitions of word senses where each sense can be evoked by multiple different words. The “Commerce_buy” frame, for example, can be evoked by the verbs “buy”, “aquire” and “purchase”, among others. FrameNet is an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame lexicon provided by FrameNet. The induction of specific frames has received much attention (Gildea and Jurafsky, 2000; Das et al., 2014). Generally, frame-semantic parsing is split into two sub-tasks of relevance to us: (i) target detection, the discovery of predicates evoking frames, and (ii) frame induction, the classification tasks of deciding which frame a predicate evokes (Das et al., 2014, p. 19). For the SemEval-2007 shared task (Pradhan et al., 2007), the work by Johansson and Nugues (2007) relies on the FrameNet lexicon specifying all possible frames for a predica"
2021.naacl-srw.5,P08-1090,0,0.50709,"many Chris Biemann Language Technology Group Universität Hamburg, Germany hatzel@informatik.uni-hamburg.de biemann@informatik.uni-hamburg.de Abstract can be the subject and object relating to a given verb, in conjunction with the verb’s lemma (Chambers and Jurafsky, 2008). If one only wants to include events involving a single character in a story, it is necessary to consider only those predicates with arguments coreferring to the character. The narrative coherence assumption says that “verbs sharing coreferring arguments are semantically connected by virtue of narrative discourse structure” (Chambers and Jurafsky, 2008). Verbs connected in this way are, under the assumption, considered to be part of the same so-called narrative chain (Chambers and Jurafsky, 2008). Previous work has focused on finding chains as representations of narratives in short documents, combining individual narrative chains, each focused on one character, into a schema involving multiple chains and thereby multiple characters (Chambers and Jurafsky, 2009). While the overall narrative in a long document could be regarded as a large schema, a variety of sub-schemas exists describing each scene using individual events. As a result, a typi"
2021.naacl-srw.5,P09-1068,0,0.33046,"coreferring to the character. The narrative coherence assumption says that “verbs sharing coreferring arguments are semantically connected by virtue of narrative discourse structure” (Chambers and Jurafsky, 2008). Verbs connected in this way are, under the assumption, considered to be part of the same so-called narrative chain (Chambers and Jurafsky, 2008). Previous work has focused on finding chains as representations of narratives in short documents, combining individual narrative chains, each focused on one character, into a schema involving multiple chains and thereby multiple characters (Chambers and Jurafsky, 2009). While the overall narrative in a long document could be regarded as a large schema, a variety of sub-schemas exists describing each scene using individual events. As a result, a typical document in our domain contains multiple schemas. Figure 1 illustrates a potential separation of an event sequence into schemas. For each event EnC in any given text we know, based on coreference resolution, which entities C are involved with it (i.e.: occur as its arguments). Intuitively a separation boundary is preferably found between nonconnected events. The verbs “leaving” and “arriving”, for example, ar"
2021.naacl-srw.5,N19-1423,0,0.0667933,"ating several Our initial approach mirrors the one described as binary features based on concepts from narratology “Bottom-up Prototype” by Sikos and Padó (2019). (Schmid, 2014). These features capture such crite- In this approach, for each frame, the average vector ria as reversibility, unexpectedness, and relevance representation of all training examples is computed, of events. with the resulting centroid representing the entire frame. With this approach, using BERT-based em4.2 Frame Identification for Event beddings, assigning frames based on the closest Representation centroid embedding, (Devlin et al., 2019) we only Initially, we assume each verb to evoke a frame and barely reached double-digit results (in terms of to represent an event, thereby addressing target de- frame classification F1-score) without lexical unit tection using a parser-based heuristic. One notable filtering while predicting German SALSA frames. exception, to the assumption of all verbs evoking These current results are not comparable with exframes, is stative verbs, “Water is cold” does not isting ones that we are aware of but we will make describe an event. Other cases such as inductive sure to apply our approach to existin"
2021.naacl-srw.5,doddington-etal-2004-automatic,0,0.254164,"s as found in the domain of long-form literature. Our approach is closely related to the one by Chambers and Jurafsky (2008) and Chambers and Jurafsky (2009), extending their approach to use vector representations over verb forms and to the operation on longer texts with multiple schemas. {A} E7 Figure 1: One possible separation of the events into four schemas splits the event up into a shopping, a transportation, a social gathering, and another transportation schema. 2 2.1 Related Work Event Processing The detection of events has mostly focused on domains outside of literature, such as news (Doddington et al., 2004; Chambers and Jurafsky, 2008). More recently, Sims et al. (2019) created a new dataset of annotated literary texts. 2.2 Event Sequences Semantic Frame Induction Semantic frames, in the context of FrameNet (Baker et al., 1998), are definitions of word senses where each sense can be evoked by multiple different words. The “Commerce_buy” frame, for example, can be evoked by the verbs “buy”, “aquire” and “purchase”, among others. FrameNet is an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame"
2021.naacl-srw.5,P98-1013,0,0.54956,"and to the operation on longer texts with multiple schemas. {A} E7 Figure 1: One possible separation of the events into four schemas splits the event up into a shopping, a transportation, a social gathering, and another transportation schema. 2 2.1 Related Work Event Processing The detection of events has mostly focused on domains outside of literature, such as news (Doddington et al., 2004; Chambers and Jurafsky, 2008). More recently, Sims et al. (2019) created a new dataset of annotated literary texts. 2.2 Event Sequences Semantic Frame Induction Semantic frames, in the context of FrameNet (Baker et al., 1998), are definitions of word senses where each sense can be evoked by multiple different words. The “Commerce_buy” frame, for example, can be evoked by the verbs “buy”, “aquire” and “purchase”, among others. FrameNet is an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame lexicon provided by FrameNet. The induction of specific frames has received much attention (Gildea and Jurafsky, 2000; Das et al., 2014). Generally, frame-semantic parsing is split into two sub-tasks of relevance to us: (i) ta"
2021.naacl-srw.5,O98-4002,0,0.57565,"he prediction of masked events in event sequence contexts. We propose building on sequences of event embeddings to form schema representations, thereby summarizing sections of documents using a fixed-size representation. This approach will give rise to comparisons of sections such as chapters up to the comparison of entire literary works on the level of their schema structure, paving the way to a computational approach to quantitative literary research. 1 Introduction Events generally describe any change of state (Hogenboom et al., 2016) and are often used in information extraction scenarios (Gaizauskas and Wilks, 1998; Niklaus et al., 2018). The modeling of sequences of events has the potential of aiding literary scientists in understanding narrative patterns and devices. Determining which events in a narrative are crucial is challenging and relates to a variety of related tasks, such as summarization, comparison, or even story generation. Understanding the contexts of an event requires modeling its arguments and semantics. A simple representation 32 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 32–39 June 6–11, 2021. ©2021 Association for Computational Linguistics {B} E0 {B} E1 {B} E2 {B"
2021.naacl-srw.5,P00-1065,0,0.237322,"nnotated literary texts. 2.2 Event Sequences Semantic Frame Induction Semantic frames, in the context of FrameNet (Baker et al., 1998), are definitions of word senses where each sense can be evoked by multiple different words. The “Commerce_buy” frame, for example, can be evoked by the verbs “buy”, “aquire” and “purchase”, among others. FrameNet is an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame lexicon provided by FrameNet. The induction of specific frames has received much attention (Gildea and Jurafsky, 2000; Das et al., 2014). Generally, frame-semantic parsing is split into two sub-tasks of relevance to us: (i) target detection, the discovery of predicates evoking frames, and (ii) frame induction, the classification tasks of deciding which frame a predicate evokes (Das et al., 2014, p. 19). For the SemEval-2007 shared task (Pradhan et al., 2007), the work by Johansson and Nugues (2007) relies on the FrameNet lexicon specifying all possible frames for a predicate, with their model only deciding between the defined options. To handle predicates not covered 2.4 Coreference Resolution Coreference re"
2021.naacl-srw.5,Y09-1038,0,0.064718,"Missing"
2021.naacl-srw.5,2020.emnlp-main.351,0,0.0299295,"he task of, given its surrounding events, predicting an event in a narrative chain). Chambers and Jurafsky (2009) extend the concept of narrative chains to narrative schemas, which involve more than one character and capture the interactions of different chains. Our approach is an extension of this work in that we aim to extract multiple schemas from a single long document. We assume that a document contains the descriptions of multiple processes or scenarios where each forms a schema. Distinguishing real from generated event chains has been used in discriminative setups for story generation. Goldfarb-Tarrant et al. (2020) use event sequences as a building block to allow language models to generate globally consistent stories based on short prompts. Their model is trained to discern shuffled event sequences (using different shuffling strategies) from real ones. Guan et al. (2020) generate common-sense stories based on external knowledge bases. To our knowledge, no existing event modeling literature operates on longer chains of events as found in the domain of long-form literature. Our approach is closely related to the one by Chambers and Jurafsky (2008) and Chambers and Jurafsky (2009), extending their approac"
2021.naacl-srw.5,W16-1007,0,0.0240483,"cases such as inductive sure to apply our approach to existing datasets (e.g. generalizations like “Metal expands in the heat” Pradhan et al., 2007) in the future to facilitate comare more difficult to handle and may require ma- parisons. To retain the wider applicability of our chine learning approaches. Our initial approaches embeddings, while improving results, we decided will only rely on the text order of events; we choose to use an approach similar to the “Bottom-up plus not to apply temporal ordering approaches (Mir- Top-down Prototype” one taken by QasemiZadeh roshandel et al., 2009; Mostafazadeh et al., 2016). et al. (2019). We train a BERT network to decide Concerns over insufficient coverage in the frame if a pair of lexical units in their contexts evoke the annotation data are motivated by an assumed di- same frame. Unlike QasemiZadeh et al. (2019), we verse vocabulary in the domain of German litera- rely on embedding similarity to frame centroids at ture. We separate coverage issues with frame re- evaluation time. 35 Model Name Max F1-Score bert-german bert-dbmdz multilingual-bert bert-electra IMS HotCoref DE1 2016). Our preliminary results show that the existing approach by Xu and Choi (2020)"
2021.naacl-srw.5,C18-1326,0,0.0152635,"ts in event sequence contexts. We propose building on sequences of event embeddings to form schema representations, thereby summarizing sections of documents using a fixed-size representation. This approach will give rise to comparisons of sections such as chapters up to the comparison of entire literary works on the level of their schema structure, paving the way to a computational approach to quantitative literary research. 1 Introduction Events generally describe any change of state (Hogenboom et al., 2016) and are often used in information extraction scenarios (Gaizauskas and Wilks, 1998; Niklaus et al., 2018). The modeling of sequences of events has the potential of aiding literary scientists in understanding narrative patterns and devices. Determining which events in a narrative are crucial is challenging and relates to a variety of related tasks, such as summarization, comparison, or even story generation. Understanding the contexts of an event requires modeling its arguments and semantics. A simple representation 32 Proceedings of NAACL-HLT 2021: Student Research Workshop, pages 32–39 June 6–11, 2021. ©2021 Association for Computational Linguistics {B} E0 {B} E1 {B} E2 {B} E3 {B} E4 {A,B} E5 {A"
2021.naacl-srw.5,2020.lrec-1.7,0,0.0181485,"sults listed for comparison. 4.3 Coreference Resolution 4.4 Coreference resolution is required to extract chains of events sharing a specific entity. Our initial results are promising, showing that current neural approaches using modern embeddings perform very well on German data. In the experiments we present in this proposal, we train and evaluate German coreference models on the TüBa-D/Z dataset (Telljohann et al., 2004), adapting English approaches that are trained on OntoNotes (Pradhan and Ramshaw, 2017). We intend to train and evaluate further on the DROC (Krug et al., 2018) and DraCor (Pagel and Reiter, 2020) datasets adapting our models to perform character based coreference resolution. In the context of event extraction, the focus on characters could benefit us by irrelevant events being discarded, on the other hand, the removal of noncharacter related events relevant to the plot (e.g.: an earthquake) could be detrimental. Table 1 shows our best results for each model on the validation set (with which early stopping is performed). All models were tested in their base variant. We use the training, validation, and test splits suggested by Roesiger and Kuhn (2016). Multilingual BERT (Devlin et al.,"
2021.naacl-srw.5,2020.tacl-1.5,0,0.0464176,"Missing"
2021.naacl-srw.5,S07-1016,0,0.0201845,"s an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame lexicon provided by FrameNet. The induction of specific frames has received much attention (Gildea and Jurafsky, 2000; Das et al., 2014). Generally, frame-semantic parsing is split into two sub-tasks of relevance to us: (i) target detection, the discovery of predicates evoking frames, and (ii) frame induction, the classification tasks of deciding which frame a predicate evokes (Das et al., 2014, p. 19). For the SemEval-2007 shared task (Pradhan et al., 2007), the work by Johansson and Nugues (2007) relies on the FrameNet lexicon specifying all possible frames for a predicate, with their model only deciding between the defined options. To handle predicates not covered 2.4 Coreference Resolution Coreference resolution is the task of identifying spans of text referring to the same entity within a document. Spans of text that refer to an entity are called mentions, in the sentence “[Alice] got up to greet [her] friend.”, for example, both “Alice” and “her” refer to the same entity. The output of a coreference system is a set of mentions for each enti"
2021.naacl-srw.5,D19-1588,0,0.0129275,"heir model only deciding between the defined options. To handle predicates not covered 2.4 Coreference Resolution Coreference resolution is the task of identifying spans of text referring to the same entity within a document. Spans of text that refer to an entity are called mentions, in the sentence “[Alice] got up to greet [her] friend.”, for example, both “Alice” and “her” refer to the same entity. The output of a coreference system is a set of mentions for each entity in the text. With the recent success of 33 contextual embedding based coreference resolution approaches (Xu and Choi, 2020; Joshi et al., 2019, 2020) and its adaptation to longer documents on English data (Xia et al., 2020; Toshniwal et al., 2020), it seems possible that learning-based approaches could outperform rule-based ones, even on documents the length of entire novels. For English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Roesiger and Kuhn, 201"
2021.naacl-srw.5,P14-2006,0,0.0214293,"er related events relevant to the plot (e.g.: an earthquake) could be detrimental. Table 1 shows our best results for each model on the validation set (with which early stopping is performed). All models were tested in their base variant. We use the training, validation, and test splits suggested by Roesiger and Kuhn (2016). Multilingual BERT (Devlin et al., 2019) performs about on par with the two older German models but is outperformed by the more recently released Electra model2 . On the test set, our approach also performs well, reaching an F1 score of 75.44 using the evaluation script by Pradhan et al. (2014). Existing German results on the same data, using the same prediction setup (i.e. without using gold mentions), reach a maximum F1 score of 48.54 (Roesiger and Kuhn, Narrative Schemas As mentioned in Section 1, as a first step a schema segmentation needs to be performed. From surfacelevel features (like paragraphs) to content-based ones (like perplexity of event sequence models), we will openly explore different approaches. The evaluation of segmentations will pose a challenge, due to the lack of evaluation data; we will start with manual evaluation, potentially extending it to metric-based ev"
2021.naacl-srw.5,W12-4501,0,0.0491658,"and “her” refer to the same entity. The output of a coreference system is a set of mentions for each entity in the text. With the recent success of 33 contextual embedding based coreference resolution approaches (Xu and Choi, 2020; Joshi et al., 2019, 2020) and its adaptation to longer documents on English data (Xia et al., 2020; Toshniwal et al., 2020), it seems possible that learning-based approaches could outperform rule-based ones, even on documents the length of entire novels. For English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Roesiger and Kuhn, 2016) are based on rule-based systems. LitBank (Bamman et al., 2020) is a dataset of English novels with coreference annotations. Recent approaches by Xia et al. (2020) and Toshniwal et al. (2020) have evaluated their approach on this dataset. Krug et al. (2015) have approached the domain of German literature using rule-based coreference resolution. They point out issues"
2021.naacl-srw.5,W15-0711,0,0.0213909,"English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Roesiger and Kuhn, 2016) are based on rule-based systems. LitBank (Bamman et al., 2020) is a dataset of English novels with coreference annotations. Recent approaches by Xia et al. (2020) and Toshniwal et al. (2020) have evaluated their approach on this dataset. Krug et al. (2015) have approached the domain of German literature using rule-based coreference resolution. They point out issues with machine learning approaches, namely the fact that literary text is very different from the news data usually used for training, and provide a corpus for evaluation (Krug et al., 2018). The availability and quality of pre-trained embeddings as well as the absence of very large annotated German literary datasets are hindrances to applying state-of-theart English approaches. Recently neural networks, however, have been found to perform similarly to rule-based approaches in our doma"
2021.naacl-srw.5,2020.lrec-1.431,0,0.0112125,"different literary genres). To address these, we will explore the role of segmentation for processing documents in sections, the viability of incremental processing, and the role of pre-training and unsupervised fine-tuning. Aside from intrinsic evaluations of schemas based on their similarity and predicates based on them constituting events, we plan to derive summaries from the schema structure and compare them to human-generated summaries in literary lexicons (e.g. Arnold, 2009). sources into two categories, expecting both to occur with our data: (i) missing frames where, as pointed out by Yong and Torrent (2020), some semantics may not be covered, and (ii) missing lexical units where not-before-seen verbs evoke known frames. While previous work by Yong and Torrent (2020) addressed missing frame coverage concerns by generating new frames, our approach does not necessitate discrete frame representations, rather we see multiple potential benefits to using continuous rep4 Methodology resentations instead. Vector representations for different frames may model their semantic distances, From the research questions, two immediate direcdifferent frames of communication such as “Statetions emerge: event extrac"
2021.naacl-srw.5,S19-2003,0,0.0262696,"Missing"
2021.naacl-srw.5,L16-1024,0,0.132798,"20; Joshi et al., 2019, 2020) and its adaptation to longer documents on English data (Xia et al., 2020; Toshniwal et al., 2020), it seems possible that learning-based approaches could outperform rule-based ones, even on documents the length of entire novels. For English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Roesiger and Kuhn, 2016) are based on rule-based systems. LitBank (Bamman et al., 2020) is a dataset of English novels with coreference annotations. Recent approaches by Xia et al. (2020) and Toshniwal et al. (2020) have evaluated their approach on this dataset. Krug et al. (2015) have approached the domain of German literature using rule-based coreference resolution. They point out issues with machine learning approaches, namely the fact that literary text is very different from the news data usually used for training, and provide a corpus for evaluation (Krug et al., 2018). The availability and quality of pre-train"
2021.naacl-srw.5,W19-0425,0,0.012965,"0). ample “she spoke loudly” should evoke. Lastly, Event annotations will, in cooperation with liter- continuous representations are a good fit for proary scientists, be created on a small subset of this cessing neural models, no additional embedding data. In this subset, all verbs will be annotated, layer is needed. indicated whether or not they represent an event. For any verb that does represent an event, a set of binary features will be recorded, indicating several Our initial approach mirrors the one described as binary features based on concepts from narratology “Bottom-up Prototype” by Sikos and Padó (2019). (Schmid, 2014). These features capture such crite- In this approach, for each frame, the average vector ria as reversibility, unexpectedness, and relevance representation of all training examples is computed, of events. with the resulting centroid representing the entire frame. With this approach, using BERT-based em4.2 Frame Identification for Event beddings, assigning frames based on the closest Representation centroid embedding, (Devlin et al., 2019) we only Initially, we assume each verb to evoke a frame and barely reached double-digit results (in terms of to represent an event, thereby"
2021.naacl-srw.5,P19-1353,0,0.0246517,"ely related to the one by Chambers and Jurafsky (2008) and Chambers and Jurafsky (2009), extending their approach to use vector representations over verb forms and to the operation on longer texts with multiple schemas. {A} E7 Figure 1: One possible separation of the events into four schemas splits the event up into a shopping, a transportation, a social gathering, and another transportation schema. 2 2.1 Related Work Event Processing The detection of events has mostly focused on domains outside of literature, such as news (Doddington et al., 2004; Chambers and Jurafsky, 2008). More recently, Sims et al. (2019) created a new dataset of annotated literary texts. 2.2 Event Sequences Semantic Frame Induction Semantic frames, in the context of FrameNet (Baker et al., 1998), are definitions of word senses where each sense can be evoked by multiple different words. The “Commerce_buy” frame, for example, can be evoked by the verbs “buy”, “aquire” and “purchase”, among others. FrameNet is an annotated dataset, marking for each predicate the frame that it evokes. A German frame resource called SALSA (Burchardt et al., 2006) builds on the frame lexicon provided by FrameNet. The induction of specific frames ha"
2021.naacl-srw.5,telljohann-etal-2004-tuba,0,0.0314505,"72.71 73.36 75.86 48.54 Table 1: Preliminary F1 scores for German coreference resolution on the TüBa-D/Z 10 validation set for different underlying embeddings using early stopping, with previous results listed for comparison. 4.3 Coreference Resolution 4.4 Coreference resolution is required to extract chains of events sharing a specific entity. Our initial results are promising, showing that current neural approaches using modern embeddings perform very well on German data. In the experiments we present in this proposal, we train and evaluate German coreference models on the TüBa-D/Z dataset (Telljohann et al., 2004), adapting English approaches that are trained on OntoNotes (Pradhan and Ramshaw, 2017). We intend to train and evaluate further on the DROC (Krug et al., 2018) and DraCor (Pagel and Reiter, 2020) datasets adapting our models to perform character based coreference resolution. In the context of event extraction, the focus on characters could benefit us by irrelevant events being discarded, on the other hand, the removal of noncharacter related events relevant to the plot (e.g.: an earthquake) could be detrimental. Table 1 shows our best results for each model on the validation set (with which e"
2021.naacl-srw.5,2020.emnlp-main.685,0,0.0287957,"ce Resolution Coreference resolution is the task of identifying spans of text referring to the same entity within a document. Spans of text that refer to an entity are called mentions, in the sentence “[Alice] got up to greet [her] friend.”, for example, both “Alice” and “her” refer to the same entity. The output of a coreference system is a set of mentions for each entity in the text. With the recent success of 33 contextual embedding based coreference resolution approaches (Xu and Choi, 2020; Joshi et al., 2019, 2020) and its adaptation to longer documents on English data (Xia et al., 2020; Toshniwal et al., 2020), it seems possible that learning-based approaches could outperform rule-based ones, even on documents the length of entire novels. For English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Roesiger and Kuhn, 2016) are based on rule-based systems. LitBank (Bamman et al., 2020) is a dataset of English novels with cor"
2021.naacl-srw.5,2020.emnlp-main.695,0,0.0469975,"Missing"
2021.naacl-srw.5,2020.emnlp-main.686,0,0.120617,"a predicate, with their model only deciding between the defined options. To handle predicates not covered 2.4 Coreference Resolution Coreference resolution is the task of identifying spans of text referring to the same entity within a document. Spans of text that refer to an entity are called mentions, in the sentence “[Alice] got up to greet [her] friend.”, for example, both “Alice” and “her” refer to the same entity. The output of a coreference system is a set of mentions for each entity in the text. With the recent success of 33 contextual embedding based coreference resolution approaches (Xu and Choi, 2020; Joshi et al., 2019, 2020) and its adaptation to longer documents on English data (Xia et al., 2020; Toshniwal et al., 2020), it seems possible that learning-based approaches could outperform rule-based ones, even on documents the length of entire novels. For English, the CoNLL-2012 shared task, based on the OntoNotes 5.0 dataset, is universally used for evaluation (Pradhan et al., 2012). The improvement in performance on this task in the recent past has largely been attributed to the improvements in underlying embeddings (Xu and Choi, 2020). Existing approaches on German news-domain data (Ro"
2021.nodalida-main.43,P15-1153,0,0.0211576,"dstein et al., 2000; Radev et al., 2000). Later, Conroy et al. (2006) and Shen and Li (2010), attempt the MDS task with a topic and set-based methodology, respectively. Initial attempts for abstractive multi-document summarization are made by McKeown and Radev (1995) and Radev and McKeown (1998). Barzilay and McKeown (2005) use sentence-fusion for text generation to create summaries across different documents. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MD"
2021.nodalida-main.43,P16-1046,0,0.0133588,"mmaries across different documents. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work"
2021.nodalida-main.43,P06-2020,0,0.212693,"Missing"
2021.nodalida-main.43,N19-1423,0,0.0148611,"ies from Wikipedia and search for corresponding source documents on the internet. On an average, a summary is linked to 4.73 source documents. 4 Methodology We consider the state-of-the-art BART model (Lewis et al., 2020) for the multi-document summarization (MDS) task. First, we use only pre-trained BART, and next, we fine-tune the pre-trained BART model using each of the three datasets separately and analyze the performances. The details about the BART model are described below. Description of BART model BART (Lewis et al., 2020) generalizes the concepts of bidirectional encoders from BERT (Devlin et al., 2019) and autoregressive decoders from GPT-2 (Radford et al., 2019). The model is trained with text corrupted through an arbitrary noising function and a sequence-to-sequence model that learns to reconstruct the original text. The encoder reads the sequential input e.g. a document to summarize while the decoder generates the outputs autoregressively. Both layers are connected by cross-attention where each decoder layer focuses on specific aspects over the final state of the encoder output creating sequences, closely connected to the initial input. The bidirectional encoder architecture takes all pr"
2021.nodalida-main.43,P19-1102,0,0.145565,"recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work by Raffel et al. (2020) attempted to explore fine-tuning, whereas, in another work, Liu and Lapata (2019) fine-tune BERT for summarization. Later, Hokamp et al. (2020) adapt and fine-tune BART on MDS. Approaches regarding a systematic error analysis of t"
2021.nodalida-main.43,D18-1443,0,0.0320501,"Missing"
2021.nodalida-main.43,W00-0405,0,0.0819356,"Missing"
2021.nodalida-main.43,N18-1065,0,0.0328707,"Missing"
2021.nodalida-main.43,N09-1041,0,0.0838141,"igate the extractiveness of the generated summaries. 2 Related Work Early approaches on extractive MDS apply term frequency-inverse document frequency (TF-IDF) (McKeown et al., 1999; Goldstein et al., 2000; Radev et al., 2000). Later, Conroy et al. (2006) and Shen and Li (2010), attempt the MDS task with a topic and set-based methodology, respectively. Initial attempts for abstractive multi-document summarization are made by McKeown and Radev (1995) and Radev and McKeown (1998). Barzilay and McKeown (2005) use sentence-fusion for text generation to create summaries across different documents. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different"
2021.nodalida-main.43,2020.emnlp-main.33,0,0.0359042,"Missing"
2021.nodalida-main.43,D18-1446,0,0.0288441,"Missing"
2021.nodalida-main.43,2020.acl-main.703,0,0.444086,"rehend this information. Capturing salient details from multiple sources to produce an abridged version is described as MultiDocument Summarization (MDS) (Nenkova and McKeown, 2011) and can be carried out in both an abstractive or extractive manner. MDS has recently become one of the most interesting research topics in the field of Natural Language Processing (NLP). As per the literature, whilst the state-of-theart models (Gehrmann et al., 2018; Liu et al., 2018) heavily rely on large datasets, recent advances with pre-trained language model systems (Ziegler et al., 2019; Raffel et al., 2020; Lewis et al., 2020) have shown great potential for the summarization task. While there have been studies to gradually improve the performance of MDS for the English language, MDS for other languages has rarely been attempted. There has also been a lack of in-depth error analysis for the MDS task. In this study, we attempt to analyze and address these issues. Our main contributions are the following: Firstly, we reproduce recent pre-trained and fine-tuned results for multi-document summarization with the BART model, introduced by Lewis et al. (2020), on two English datasets. Further, we adapt the model for the Ge"
2021.nodalida-main.43,2020.acl-main.555,0,0.0107601,"der architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work by Raffel et al. (2020) attempted to explore fine-tuning, whereas, in another work, Liu and Lapata (2019) fine-tune BERT for summarization. Later, Hokamp et al. (2020) adapt and fine-tune BART on MDS. Approaches regarding a systematic error analysis of those models were introduced by Huang et al. (2020) who compared BART to other abstractive and extractive methods. In another dir"
2021.nodalida-main.43,W04-1013,0,0.0451008,"Missing"
2021.nodalida-main.43,D19-1387,0,0.0118411,"s are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work by Raffel et al. (2020) attempted to explore fine-tuning, whereas, in another work, Liu and Lapata (2019) fine-tune BERT for summarization. Later, Hokamp et al. (2020) adapt and fine-tune BART on MDS. Approaches regarding a systematic error analysis of those models were introduced by Huang et al. (2020) who compared BART to other abstractive and extractive methods. In another direction, attempts have also been made for single-document summarization for nonEnglish text. For instance, single-document summarization of text in German language was done by Parida and Motlicek (2019) who utilized transformer models for abstractive summarization on two datasets — SwissText 20191 and Common Crawl2 . Evalu"
2021.nodalida-main.43,K16-1028,0,0.0260313,"uments. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work by Raffel et al. (2020) att"
2021.nodalida-main.43,P11-5003,0,0.126249,"Missing"
2021.nodalida-main.43,D19-1616,0,0.0154874,"LP tasks. For summarization, one such work by Raffel et al. (2020) attempted to explore fine-tuning, whereas, in another work, Liu and Lapata (2019) fine-tune BERT for summarization. Later, Hokamp et al. (2020) adapt and fine-tune BART on MDS. Approaches regarding a systematic error analysis of those models were introduced by Huang et al. (2020) who compared BART to other abstractive and extractive methods. In another direction, attempts have also been made for single-document summarization for nonEnglish text. For instance, single-document summarization of text in German language was done by Parida and Motlicek (2019) who utilized transformer models for abstractive summarization on two datasets — SwissText 20191 and Common Crawl2 . Evaluation of summarization models to non-English data was done by Tauchmann and Mieskes (2020) who applied an automatic evaluation paradigm on the German heterogeneous dataset DBS (Benikova et al., 2016). Since our main focus is on multi-document summarization, we do not explore the literature of single-document summarization extensively. 3 Datasets For our experiments we use three datasets that exhibit extractive characteristics: two English datasets — CNN/DM (Hermann et al.,"
2021.nodalida-main.43,J98-3005,0,0.26955,"re we point out general errors and cross-lingual error similarities regarding factfulness and topic delimitation. Additionally, we also investigate the extractiveness of the generated summaries. 2 Related Work Early approaches on extractive MDS apply term frequency-inverse document frequency (TF-IDF) (McKeown et al., 1999; Goldstein et al., 2000; Radev et al., 2000). Later, Conroy et al. (2006) and Shen and Li (2010), attempt the MDS task with a topic and set-based methodology, respectively. Initial attempts for abstractive multi-document summarization are made by McKeown and Radev (1995) and Radev and McKeown (1998). Barzilay and McKeown (2005) use sentence-fusion for text generation to create summaries across different documents. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and a"
2021.nodalida-main.43,W00-0403,0,0.635853,"Missing"
2021.nodalida-main.43,D15-1044,0,0.0164147,"t summarization are made by McKeown and Radev (1995) and Radev and McKeown (1998). Barzilay and McKeown (2005) use sentence-fusion for text generation to create summaries across different documents. Haghighi and Vanderwende (2009) build a model based on word frequency and Latent Dirichlet Allocation (LDA) for MDS whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to le"
2021.nodalida-main.43,C10-1111,0,0.038424,"Missing"
2021.nodalida-main.43,P17-1108,0,0.0370306,"Missing"
2021.nodalida-main.43,2020.lrec-1.822,0,0.034588,"adapt and fine-tune BART on MDS. Approaches regarding a systematic error analysis of those models were introduced by Huang et al. (2020) who compared BART to other abstractive and extractive methods. In another direction, attempts have also been made for single-document summarization for nonEnglish text. For instance, single-document summarization of text in German language was done by Parida and Motlicek (2019) who utilized transformer models for abstractive summarization on two datasets — SwissText 20191 and Common Crawl2 . Evaluation of summarization models to non-English data was done by Tauchmann and Mieskes (2020) who applied an automatic evaluation paradigm on the German heterogeneous dataset DBS (Benikova et al., 2016). Since our main focus is on multi-document summarization, we do not explore the literature of single-document summarization extensively. 3 Datasets For our experiments we use three datasets that exhibit extractive characteristics: two English datasets — CNN/DM (Hermann et al., 2015), Multi-News (Fabbri et al., 2019) and one German dataset — auto-hMDS (Zopf, 2018). 1 2 https://www.swisstext.org/ http://commoncrawl.org/ CNN/DailyMail This dataset is an English single-document summarizati"
2021.nodalida-main.43,K17-1045,0,0.0122358,"S whereas phrase selection and merging approaches have also been tried (Bing et al., 2015) for the same. In recent years, neural network architecture is being adapted for several NLP tasks, especially with the approach of using encoder-decoder architecture. Here, relevant work includes Rush et al. (2015), who propose an attention model for combining extractive and abstractive methods, which is supplemented with document-wide contextualization by Cheng and Lapata (2016) and Nallapati et al. (2016). In a different direction, several graph-based approaches are explored as well (Tan et al., 2017; Yasunaga et al., 2017). Liu et al. (2018) show the feasibility of using Wikipedia as an MDS dataset whereas Fabbri et al. (2019) apply a pointer-generator network with a transformer model complemented with Maximal Marginal Relevance (MMR). Li et al. (2020) explores graph representation and proposes to leverage graphs for abstractive MDS. Most recently, fine-tuning pre-trained language models have gained a lot of attention for NLP tasks. For summarization, one such work by Raffel et al. (2020) attempted to explore fine-tuning, whereas, in another work, Liu and Lapata (2019) fine-tune BERT for summarization. Later, H"
2021.nodalida-main.43,L18-1510,0,0.013464,"ained and fine-tuned BART model on the Multi-News dataset. We observe the fine-tuned model outperform the H I -M AP (Fabbri et al., 2019) model, whereas it produces comparable performance with BART-DYNE (Hokamp et al., 2020). Note that, the fine-tuned BART model considers all source documents for the MDS task whereas the model by Hokamp et al. (2020) only takes one (DYNE-1) or five source documents (DYNE-5) into account, which otherwise simplifies the task. Table 3 shows the results on the German auto-hMDS dataset of pre-trained and finetuned BART models in comparison to baselines proposed by Zopf (2018). We prepare two baseline models as well. The first one is trivial by extracting ‘Top-5 Sentences’ based on the frequency of 5.2 Error Analysis Even though the BART model produces satisfactory performance for multi-document summarization for both languages, there is still scope for improvement. Hence, we investigate cases further, where even the fine-tuned BART model goes wrong. We perform this analysis for both English and German languages. To start with, we observe some interesting cases for which the model does not generate the desired gold summary due to the fact that some information in t"
2021.privatenlp-1.4,C18-1139,0,0.0272656,"in four different phases. First, we observe the basic NER model performance without using FL as well as DP. In the second phase, we incorporate DP in the optimizer 31 Common Crawl corpus) from spaCy library3 , the dimension of which is 300. The performance of these two models (BI-LSTM-TDDL, BI-LSTMCRF) are presented in Table 1. Note that, since our objective is not to produce a state-of-the-art performance for NER tasks, we do not attempt to tune the hyper-parameters to obtain the best possible performance. However, the F-measure obtained by BI-LSTM-TDDL is comparable to the state-ofthe-art (Akbik et al., 2018) F-measure of 0.9309. On the other hand, we put our effort into analyzing the behavior of these NER models’ performance in privacy preserving framework. With DP, Without FL: Next, we incorporate a Differentially Private Stochastic gradient descent (DP-SGD) optimizer for training4 . The performance of both the models with varying noisemultiplier (a hyperparameter to add noise) are presented in Table 2. Note that, by increasing the value of the NER model and analyze its behavior. Third, we analyze the performance of the NER model in an FL framework, but none of the optimizers of the clients are"
benikova-etal-2014-nosta,chrupala-klakow-2010-named,0,\N,Missing
benikova-etal-2014-nosta,C00-1027,0,\N,Missing
benikova-etal-2014-nosta,S07-1008,0,\N,Missing
benikova-etal-2014-nosta,D09-1015,0,\N,Missing
benikova-etal-2014-nosta,W03-0419,0,\N,Missing
benikova-etal-2014-nosta,J96-2004,0,\N,Missing
benikova-etal-2014-nosta,rossler-2004-corpus,0,\N,Missing
benikova-etal-2014-nosta,taule-etal-2008-ancora,0,\N,Missing
benikova-etal-2014-nosta,W12-3606,0,\N,Missing
benikova-etal-2014-nosta,P13-4001,1,\N,Missing
biemann-2012-turk,W10-2309,1,\N,Missing
biemann-2012-turk,S07-1016,0,\N,Missing
biemann-2012-turk,N06-2015,0,\N,Missing
biemann-2012-turk,W09-2412,0,\N,Missing
biemann-2012-turk,S07-1009,0,\N,Missing
biemann-2012-turk,H92-1045,0,\N,Missing
biemann-etal-2004-automatic,J93-1007,0,\N,Missing
biemann-etal-2004-automatic,P95-1026,0,\N,Missing
biemann-etal-2004-automatic,A00-1031,0,\N,Missing
biemann-etal-2004-automatic,kunze-2000-extension,0,\N,Missing
biemann-etal-2008-asv,W06-3812,1,\N,Missing
biemann-etal-2008-asv,C92-3150,0,\N,Missing
biemann-etal-2008-asv,C94-1084,0,\N,Missing
biemann-etal-2008-asv,P06-3002,1,\N,Missing
biemann-etal-2008-asv,P06-4018,0,\N,Missing
biemann-etal-2008-asv,quasthoff-etal-2006-corpus,1,\N,Missing
biemann-etal-2008-asv,W02-0109,0,\N,Missing
C04-1178,W97-0313,0,\N,Missing
C04-1178,W02-2023,0,\N,Missing
C04-1178,W02-1028,0,\N,Missing
C04-1178,P98-2182,0,\N,Missing
C04-1178,C98-2177,0,\N,Missing
C12-1017,N07-1014,1,0.796898,"based on local operations only. Each node manipulates the local structure in its vicinity in order to thrive the local motif signature towards the average local motif signature of an optimal network. 2.2 Complex Networks of Natural Language The structure of natural language networks has been extensively investigated, see e.g. (Masucci and Rodgers, 2006) and references therein. (Ferrer-i-Cancho and Solé, 2001) have shown that co-occurrence networks of natural language are scale-free small world graphs. Whereas scale-freeness seems to be a consequence of the Zipfian word-frequency distribution (Biemann, 2007), Steyvers and Tenenbaum (2005) find the small-world property in co-occurrence networks and lexical-semantic resources, which indicates that co-occurrence networks reflect semantic properties. There is only very little work on operationalizing complex network analysis for natural language processing applications. (Pardo et al., 2006) evaluate the quality of automatic summaries by analyzing the degree distributions of networks generated from words at different fixed offsets in the text, and (Amancio et al., 2012) characterize texts for authorship attribution by quantifying their consistency ind"
C12-1017,biemann-etal-2004-automatic,1,0.728725,"atively, our results are stable across vocabulary sizes between 1, 000 and 20, 000, as long as the most frequent words are considered. Co-occurrence contexts We consider two different kinds of contexts: co-occurrence as immediate neighbors in a sequence, and co-occurrence within a sentence (sequences as limited by BoS and EoS). Thus, for each corpus of text, composed of sentences, we can compute the co-occurrence graph by connecting word nodes with edges, if words co-occur. Edges are directed in the case of neighbor-based co-occurrence, and undirected for the sentence-based case. It is known (Biemann et al., 2004) that sentence-based co-occurrences, besides capturing collocations, often reflect semantic relations and capture topical dependencies between words. Significance threshold Since mere co-occurrence results in a large number of edges and very dense networks, we apply a significance test that measures the deviation of the actual co-occurrence frequency from the co-occurrence frequency that would have been observed if the two co-occurring words would be distributed independently. Here, we use the log-likelihood test (Dunning, 1993) to prune the network: We only draw edges between word nodes, if t"
C12-1017,J92-4003,0,0.261066,"Missing"
C12-1017,J93-1003,0,0.070357,", and undirected for the sentence-based case. It is known (Biemann et al., 2004) that sentence-based co-occurrences, besides capturing collocations, often reflect semantic relations and capture topical dependencies between words. Significance threshold Since mere co-occurrence results in a large number of edges and very dense networks, we apply a significance test that measures the deviation of the actual co-occurrence frequency from the co-occurrence frequency that would have been observed if the two co-occurring words would be distributed independently. Here, we use the log-likelihood test (Dunning, 1993) to prune the network: We only draw edges between word nodes, if the words co-occur with a significance value above a certain threshold. For our experiments, we used a threshold of 10.834 . During preliminary experiments, we have found the reported results to be stable across a wide range of significance thresholds. See (Biemann and Quasthoff, 2009) for an analysis of global properties of significant co-occurrence graphs of natural language. The co-occurrence graph was computed using the TinyCC5 corpus production engine (Quasthoff et al., 2006). 3.3 Network Analysis Transitivity Let G = (V, E)"
C12-1017,J10-4005,0,0.012672,"structure is dependent on the sentence length distribution. We have found in further experiments, that the general picture of results is stable for corpora of different sizes, starting from about ten thousand sentences. Text generation with n-gram models For the scope of this work, we chose n-gram models, which are the standard workhorses of language modeling. A language model assigns a probability to a sequence of words, based on a probabilistic model of language. This can be used to pick the most probable/fluent amongst several alternatives, e.g. in a statistical machine translation system (Koehn, 2010). An n-gram language model (cf. (Manning and Schütze, 1999)) over sequences of words is defined by a Markov chain of order (n − 1), where the probability of the next word only depends Qon the (n − 1) previous words, and the probability of a sentence is defined as P(w1 ...w k ) = i=1..k P(w i |w i−1 ..w i−n+1 ). We add special symbols, BoS and EoS, to indicate sentence beginning and end. Then we generate sentences word by word, starting from a sequence of (n − 1) BoS-symbols, according to the probability distribution over the vocabulary. As soon as the EoS symbol is generated, we generate the n"
C12-1017,quasthoff-etal-2006-corpus,0,0.0142256,"buted independently. Here, we use the log-likelihood test (Dunning, 1993) to prune the network: We only draw edges between word nodes, if the words co-occur with a significance value above a certain threshold. For our experiments, we used a threshold of 10.834 . During preliminary experiments, we have found the reported results to be stable across a wide range of significance thresholds. See (Biemann and Quasthoff, 2009) for an analysis of global properties of significant co-occurrence graphs of natural language. The co-occurrence graph was computed using the TinyCC5 corpus production engine (Quasthoff et al., 2006). 3.3 Network Analysis Transitivity Let G = (V, E) be an undirected graph. A closed triangle is a set of three nodes such that all three possible edges do exist. On the other hand, a triple is any set of three nodes and two edges (in other words, a chain of two edges). The transitivity of T (G) of G is three times the total number of closed triangles divided by the total number of triples, as defined by (Newman et al., 2002). This can be calculated by iterating over every node v and counting the triangles and triples in which v is incident to two edges: P δ(v) T (G) = P v∈V k(v) (1) v∈V 2 wit"
C12-1017,J12-3007,0,0.0134115,"ll fail to show the same patterns as real language, especially regarding semantic properties such as synonymy and polysemy. In applications like Machine Translation, where language models are used to rank alternatives rather than free generation and are thus bound to the cohesive structure of the source language text, the shortcomings discussed in this paper do not necessarily impede the performance on the task. In fact, preliminary experiments involving comparisons of real translations with automatic translations of the same text did not result in motif profile differences. However, as e.g. (Tan et al., 2012) point out, there is a need for more coherent language models even in these applications: e.g. speech-to-text in noisy environments might greatly benefit from better language models. Our computational studies with regard to co-occurrence graphs based on sentences and neighboring words indicate that language models based on n-grams reflect local syntax well, but fail to model semantic cohesion and topicality. Further, these language models do not have means of regulation for referring to the same concept several times. While these results confirm the common intuition of n-grams, we present the"
C12-1017,N03-1031,0,\N,Missing
C12-1017,W12-2700,0,\N,Missing
C12-1109,W06-3814,0,0.0243746,"Missing"
C12-1109,S07-1070,0,0.0660507,"Missing"
C12-1109,S12-1059,1,0.0274292,"cal gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from our MRD (“a fixed charge for borrowing money”) has no words in common with the context, and thus would not be selected by an overlap-based WSD algorithm. But with the addition of ten lexical expansions per content word (shown in smaller text), we increase the number of overlapping word pairs (shown in boldface) to seven. Observe also th"
C12-1109,D07-1108,0,0.022479,"algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with"
C12-1109,de-marneffe-etal-2006-generating,0,0.00424528,"Missing"
C12-1109,J93-1003,0,0.37538,"orpus from the Leipzig Corpora Collection3 (Biemann et al., 2007) with the Stanford parser (de Marneffe et al., 2006) and used collapsed dependencies to extract features for words: each dependency triple (w1 , r, w2 ) denoting a directed dependency of type r between words w1 and w2 results in a feature (r, w2 ) characterizing w1 , and a feature (w1 , r) characterizing w2 . Words are thereby represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all p"
C12-1109,P05-1050,0,0.0337179,"onyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context"
C12-1109,D12-1100,0,0.0137786,"by represented by the concatenation of the surface form and the POS as assigned by the parser. After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 300 most salient features per word. The similarity of two words is given by the number of their common features (which we will shortly illustrate with an example). The pruning operation greatly reduces run time at thesaurus construction, rendering memory reduction techniques like Goyal et al. (2012) unnecessary. Despite its simplicity and the basic count of feature overlap, we found this setting to be equal to or better than more complex weighting schemes in word similarity evaluations. Across all parts of speech, the DT contains five or more similar terms for a vocabulary of over 150 000 words. To illustrate the DT, Table 1 shows the top three most similar words to the noun paper, together with the features which determine the similarities. Amongst their 300 most salient features as determined by the significance measure, newspaper and paper share 45, book and paper share 33, and articl"
C12-1109,E12-1059,1,0.0654393,"tion— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as part of the Lichtenberg"
C12-1109,E12-1039,0,0.0310549,"ted from the gloss, synonyms, and example sentences provided by WordNet, plus the 1784 same information for all senses in a direct semantic relation. This setup specifically targets situations where such a resource serves as the sense inventory but no large sense-annotated corpus is available for supervised WSD (thus precluding use of the most frequent sense backoff). This is the case for many languages, where wordnets but not manually tagged corpora are available, and also for domain-specific WSD using the English WordNet. Whereas other approaches in this setting (Ponzetto and Navigli, 2010; Henrich et al., 2012) aim at improving WSD accuracy through the combination of several lexical resources, we restrict ourselves to WordNet and bridge the lexical gap with non-supervised, data-driven methods. How one computes the overlap between two strings was left unspecified by Lesk; we therefore adopt the simple approach of removing occurrences of the target word, treating both strings as bags of case-insensitive word tokens, and taking the cardinality of their intersection. We do not preprocess the texts by lemmatization or stop word filtering, since the terms in the distributional thesaurus are likewise unpro"
C12-1109,S01-1004,0,0.0130406,"g error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,P10-1116,0,0.026512,"d to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms."
C12-1109,P98-2127,0,0.555994,"mensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), and clustering for word sense induction as features (Agirre et al., 2006; Biemann, 2012); the importance of bridging the lexical gap is reflected in all those recent advances, be it in knowledge-based or supervised WSD scenarios. In this paper, we employ a source of semantic similarity whose application to automatic WSD has never before been explored: using a distributional thesaurus, or DT (Lin, 1998), we expand the lexical representations of the context and sense definition with additional terms. On this expanded representation, we are able to apply the well-known overlap-based methods to text similarity without any modification. Lexical expansion has already proven useful in semantic text similarity evaluations (Bär et al., 2012), which is a task related to matching sense definitions to contexts. The intuition behind our approach is depicted in Figure 1: say we wish to disambiguate the word interest in the sentence, “The loan interest is paid monthly.” The correct sense definition from o"
C12-1109,W03-2408,0,0.0264845,"h problem in computational linguistics. Approaches to WSD can be classified according to what lexical resources are used: knowledgebased techniques rely only on machine-readable dictionaries (MRDs), lexical semantic resources (LSRs), and untagged corpora, whereas supervised approaches instead or additionally use manually annotated training examples. Though supervised systems generally perform better, their use is restricted to scenarios where a sufficient amount of hand-crafted training data is available. Estimates for the amount of time required to produce such training data are pessimistic (Mihalcea and Chklovski, 2003); this knowledge acquisition bottleneck is the principal motivation behind research into semi-supervised and knowledge-based WSD. The latter have the advantage that, unlike manually annotated corpora, MRDs and LSRs do exist for many languages and domains. In the past, however, knowledge-based approaches have suffered from a variant of the lexical gap problem: when matching a sense description to a given context of a disambiguation target, it is often the case that the description and context do not have much vocabulary in common. We propose a new method to bridge this lexical gap which is base"
C12-1109,S07-1006,0,0.211195,"Missing"
C12-1109,P10-1023,0,0.0215006,"ith the correct sense description— but they might well result in assigning incorrect senses. A straightforward improvement would alter the lexical expansion mechanism as to be sensitive to the context—something that is captured, for example, by LDA sampling (Blei et al., 2003). A further extension would be to have the number of lexical expansions depend on the DT similarity score (be it static or 1793 contextualized) instead of the fixed number we used here. In the future, we would like to examine the interplay of lexical expansion methods in WSD systems with richer knowledge resources (e.g., Navigli and Ponzetto (2010); Gurevych et al. (2012)) and apply our approach to other languages with fewer lexical resources. Also, it seems promising to apply lexical expansion techniques to text similarity, text segmentation, machine translation, and semantic indexing. Acknowledgments We thank Richard Steuer for computing and providing us access to the distributional thesaurus. This work has been supported by the Hessian research excellence program Landes-Offensive zur Entwicklung Wissenschaftlich-ökonomischer Exzellenz (LOEWE) as part of the research center Digital Humanities, and also by the Volkswagen Foundation as"
C12-1109,S01-1005,0,0.140023,"Missing"
C12-1109,P10-1154,0,0.556073,"entences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms); their “extended” Lesk algorithm was found to be a great improvement over the original algorithm. Subsequent researchers (e.g., Ponzetto and Navigli (2010)) have combined the “simplified” and “extended” approaches into a “simplified extended” algorithm, in which augmented definitions are compared not with each other, but with the target word context. Many successful approaches to automatic WSD in recent years rely on distributional information to model the “topicality” of the context and the sense definition.1 They include using vectorspace dimensionality reduction techniques like LSA (Gliozzo et al., 2005) or LDA (Cai et al., 2007; Li et al., 2010), additionally collected text material per sense as in topic signatures (Martínez et al., 2008), a"
C12-1109,rapp-2004-freely,0,0.0251874,"e to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost conviction allegation pay suspicion count part paid paying pay pays owed generated invested spent collected raised reimbursed for monthly. annual weekly yearly quarterly hefty daily regular additional substantial recent borrowing spending borrow lending borrowed debt investment raising inflows inve"
C12-1109,W04-0811,0,0.111045,"Missing"
C12-1109,S01-1037,0,0.158399,"d of sampling error that is inevitable when representing a large vocabulary with a small fixed number of dimensions or topics. On the other hand, while vector-space models do a good job at ranking candidates according to their similarity,2 they fail to efficiently generate a top-ranked list of possible expansions: due to its size, it is infeasible to rank the full vocabulary every time. Lexical expansion methods based on distributional similarity, however, generate a short list of highly similar candidates. 1 Distributional information was also used in a much older, semi-automatic approach by Tugwell and Kilgarriff (2001). In their technique, “word sketches” consisting of common patterns of usage of a word were extracted from a large POS-tagged corpus and presented to a human operator for manual sense annotation. The pattern–sense associations were then used as input to bootstrapping WSD algorithm. 2 See Rapp (2004) for an early success of vector-space models on a semantic task. 1783 The interest : loan mortgage loans debt financing mortgages credit lease bond grant funding a fixed interest solved hefty resolved monthly additional existing reduced done current substantial is charge charges counts charging cost"
C12-1109,vasilescu-etal-2004-evaluating,0,0.521787,"hich the word is found. This variant avoids the combinatorial explosion of word sense combinations the original version suffers from when trying to disambiguate multiple words in a text. Both the original and simplified versions of the Lesk algorithm suffer from low coverage due to the lexical gap problem: because the context and definitions are usually quite short, it is often the case that there are no overlapping content words at all. Various solutions to the problem have been proposed, with varying degrees of success. Lesk himself proposed increasing the size of the context window, though Vasilescu et al. (2004) found that performance was generally better for smaller contexts. Lesk also proposed augmenting the definitions with example sentences provided by some dictionaries; Kilgarriff and Rosenzweig (2000) found that including them (for simplified Lesk) led to significantly better performance than using the definitions alone. Banerjee and Pedersen (2002) observed that, where there exists a lexical resource like WordNet (Fellbaum, 1998) which also provides semantic relations between senses, 1782 these can be used to augment definitions with those from related senses (such as hypernyms and hyponyms);"
C12-1109,S07-1053,0,\N,Missing
C12-1109,C98-2122,0,\N,Missing
C14-1136,Q13-1007,0,0.0271361,"Missing"
C14-1136,P07-1051,0,0.0593946,"Missing"
C14-1136,W06-2920,0,0.17957,"Missing"
C14-1136,D10-1096,0,0.0372269,"Missing"
C14-1136,P07-3008,0,0.0308969,"Missing"
C14-1136,W02-0908,0,0.016802,"to relate the preposition and the nominal in prepositional phrases, and how to handle punctuation, cf. Nivre and K¨ubler (2006), Schwartz et al. (2011). When it comes to utilizing syntactic structures, however, it is more important that they are consistent across different sentences than that they adhere to specific syntactic theories and conventions. Here, we choose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsing for preprocessing corpora for the purpose of computing distributional similarities. Since it is generally accepted (e.g. (Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an important role for the quality of distributional thesauri, and comparing words along their syntactic contexts does rely on the existence of such a structure rather than its actual representation, we believe that distributional similarities are an excellent test bed for addressing the following two research questions: (1) How do unsupervised parsers compare to supervised parsers when used as feature providers for building DisThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added"
C14-1136,D08-1094,0,0.0359773,"Missing"
C14-1136,P10-2036,0,0.0222129,"Missing"
C14-1136,W11-1705,0,0.0261369,"Missing"
C14-1136,W97-0802,0,0.0779546,"Missing"
C14-1136,C08-1042,0,0.0312137,"Missing"
C14-1136,N09-1012,0,0.0652131,"Missing"
C14-1136,P02-1017,0,0.118944,"Missing"
C14-1136,P04-1061,0,0.139538,"Missing"
C14-1136,P97-1009,0,0.159692,"juncts, how to relate the preposition and the nominal in prepositional phrases, and how to handle punctuation, cf. Nivre and K¨ubler (2006), Schwartz et al. (2011). When it comes to utilizing syntactic structures, however, it is more important that they are consistent across different sentences than that they adhere to specific syntactic theories and conventions. Here, we choose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsing for preprocessing corpora for the purpose of computing distributional similarities. Since it is generally accepted (e.g. (Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an important role for the quality of distributional thesauri, and comparing words along their syntactic contexts does rely on the existence of such a structure rather than its actual representation, we believe that distributional similarities are an excellent test bed for addressing the following two research questions: (1) How do unsupervised parsers compare to supervised parsers when used as feature providers for building DisThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and pro"
C14-1136,P98-2127,0,0.598869,"Missing"
C14-1136,P13-1028,0,0.0368621,"Missing"
C14-1136,de-marneffe-etal-2006-generating,0,0.0227296,"Missing"
C14-1136,P08-1006,0,0.0603391,"Missing"
C14-1136,C12-1111,0,0.0505413,"Missing"
C14-1136,D10-1120,0,0.035696,"Missing"
C14-1136,N04-3012,0,0.146979,"Missing"
C14-1136,D13-1089,1,0.858253,"Missing"
C14-1136,P11-1067,0,0.0192217,"rsers cannot be straightforwardly mapped to linguistically-inspired categories as defined in a treebank. But also when considering only unlabeled syntactic annotations, an unsupervised parser is hardly to blame if it does not adhere to sometimes arbitrary conventions: e.g. for dependencies, it is not a priori clear how to connect auxiliary and main verbs, where to attach the complementizer of subordinate clauses, how to represent a conjunction and its conjuncts, how to relate the preposition and the nominal in prepositional phrases, and how to handle punctuation, cf. Nivre and K¨ubler (2006), Schwartz et al. (2011). When it comes to utilizing syntactic structures, however, it is more important that they are consistent across different sentences than that they adhere to specific syntactic theories and conventions. Here, we choose a task that makes only intermediary use of syntactic structures: we employ unsupervised parsing for preprocessing corpora for the purpose of computing distributional similarities. Since it is generally accepted (e.g. (Lin, 1997; Curran and Moens, 2002)), that syntactic preprocessing plays an important role for the quality of distributional thesauri, and comparing words along the"
C14-1136,C04-1146,0,0.0940048,"Missing"
C14-1136,P07-1049,0,\N,Missing
C14-1136,W06-2924,0,\N,Missing
C14-1136,C08-1050,0,\N,Missing
C14-1136,C98-2122,0,\N,Missing
C14-1136,D09-1098,0,\N,Missing
C16-1196,D10-1036,0,0.0227481,"ith websockets. This provides a bi-directional communication channel, where audio is streamed to the server application and partial and full sentence hypothesis and boundaries are simultaneously returned as JSON objects. 3.2 Keyphrase Extraction A keyphrase, as opposed to a single keyword, can consist of one or more keywords that refer to one concept. We first precompute a DRUID (Riedl and Biemann, 2015) dictionary on a recent Wikipedia dump with scores for single adjectives or nouns and noun phrases. The restriction to only use adjectives and nouns is a common one in keyword detection, c.f. (Liu et al., 2010). DRUID is a state-of-the-art unsupervised measure for multiword expressions using distributional semantics. Intuitively, DRUID finds multiword expressions by combining an uniqueness measure for phrases with a measure for their incompleteness. Uniqueness in this context is based on the assumption that multiword expressions (MWEs) can often be substituted by a single word without considerably changing the meaning of a sentence. The uniqueness measure uq(t) is computed with a distributional thesaurus, as the ratio of all similar unigrams of a term t divided by the number of n-grams similar to t."
C16-1196,D15-1290,1,0.287455,"Missing"
C16-1196,rousseau-etal-2014-enhancing,0,0.0754621,"Missing"
C16-2049,C16-1196,1,0.516756,"Missing"
C16-2049,D15-1290,1,0.821801,"use of kaldi-gstreamer-server1 , which wraps a Kaldi model into a streaming server that can be accessed with websockets. This provides a bi-directional communication channel, where audio is streamed to the server application and partial and full sentence hypothesis and boundaries are simultaneously returned. Keyword and keyphrase extraction. Once a full sentence has been hypothesized, new keywords and keyphrases are extracted in the current sentence, if available. A keyphrase, as opposed to a single keyword, can consist of serveral words that refer to one concept. We first precompute a DRUID (Riedl and Biemann, 2015) dictionary on a recent Wikipedia dump with scores for noun phrases. DRUID is a state-of-the-art unsupervised measure for multiword expressions (MWEs) using distributional semantics and precomputed dictionaries for English can be downloaded from the JoBimText project website2 . All keyphrases with a DRUID score over a certain threshold (e.g. 0.7, see also Section 4) and all remaining words that are adjectives and nouns, as determined by an off-the-shelf part of speech (POS)-tagger3 , are used as candidate terms. Candidate term ranking. We score candidate terms according to a Word2Vec (Mikolov"
C16-2049,rousseau-etal-2014-enhancing,0,0.0365592,"Missing"
C18-1028,C12-1023,0,0.0241116,"orporating graded judgments. The graded judgments are obtained from the number of workers suggesting the candidate for the given CP target. 5.1 Features To train the learning-to-rank model, we have designed a set of features that are important for text simplification. Based on the works of (Yimam et al., 2017; Horn et al., 2014), we have implemented the following list of features for the ranking model, which are partially derived from the candidate generating resources. • Frequency and length: Due to the common use of these features in selecting the most simple lexical substitution candidate (Bott et al., 2012), we use three length features specifically the number of vowels, syllables, and characters and three frequency features: the frequency of the word in Simple Wikipedia, the frequency of the word in the document, and the frequency of the word in the Google Web 1T 5-Grams. • Lexical and distributional thesaurus resources: We also use the number of similar words to the CPs and candidate suggestion based on lexical resources such as WordNet and distributional thesaurus as possible features. The features are normalized and scaled using the featran’s3 min-max scaler tool. • PPDB 2.0 and simple PPDB:"
C18-1028,P11-2117,0,0.0260955,"presents SOLAR, a framework of scalable online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for a biomedical entity annotation task. Most text simplification approaches employ basic machine translation models from parallel corpora (Xu et al., 2016; Štajner et al., 2017) and using simple Wikipedia for English (Coster and Kauchak, 2011). Simple PPDB (Pavlick and Callison-Burch, 2016) is such a resource that is built automatically, using machine translation techniques on a large number of parallel corpora. The work by Lasecki et al. (2015) shows that using crowdsourcing for text simplification is a valid approach. We also conducted our adaptive text simplification experiment on the Amazon Mechanical Turk crowdsourcing platform using a specialized text simplification tool. 3 Design of the Par4Sim System Par4Sim is a text simplification tool based on an adaptive paraphrasing paradigm. Unlike the traditional text simplification"
C18-1028,P14-2075,0,0.0593468,"urceforge.net/p/lemur/wiki/RankLib/ 336 Normalized Discounted Cumulative Gain (NDCG) (Järvelin and Kekäläinen, 2002; Wang et al., 2013) is a family of ranking measures such as mean average precision (MAP) and Precision at K. NDCG is wellsuited for our experiment for its capability of incorporating graded judgments. The graded judgments are obtained from the number of workers suggesting the candidate for the given CP target. 5.1 Features To train the learning-to-rank model, we have designed a set of features that are important for text simplification. Based on the works of (Yimam et al., 2017; Horn et al., 2014), we have implemented the following list of features for the ranking model, which are partially derived from the candidate generating resources. • Frequency and length: Due to the common use of these features in selecting the most simple lexical substitution candidate (Bott et al., 2012), we use three length features specifically the number of vowels, syllables, and characters and three frequency features: the frequency of the word in Simple Wikipedia, the frequency of the word in the document, and the frequency of the word in the Google Web 1T 5-Grams. • Lexical and distributional thesaurus r"
C18-1028,N10-1062,0,0.0200401,"ystem’s performance. It further shows that the approach helps to reduce the manual annotation efforts. Žliobaitė et al. (2016) revealed that supervised machine learning approaches stationed with static datasets face problems during deployment in a real-world application due to concept drift (Tsymbal, 2004) as applications start generating data streams continually. Applications with such properties include spam filtering and intrusion detection. Stream-based learning and online learning (Bottou, 1998) are alternative setups to a batch-mode adap332 tive learning system. For example, the work by Levenberg et al. (2010) shows that the deployment of stream-based learning for statistical machine translation improves the performance of their system when new sentence pairs are incorporated from a stream. The work by Wang et al. (2015) presents SOLAR, a framework of scalable online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for"
C18-1028,P11-1027,0,0.0199251,"iword expression and phrase resources, which are obtained from the work of Yimam et al. (2016b). We trained the Phrase2Vec embeddings with 200 1 https://sites.google.com/view/cwisharedtask2018/ (Yimam et al., 2018) 335 dimensions using skip-gram training and a window size of 5. We retrieve the top 10 similar words to the CPs as candidates. Obviously, the number of candidate suggestions obtained from these different resources is enormous and we should limit the size before providing to the ranker model. The candidates are ordered by a language model score. We trained a tri-gram language-model (Pauls and Klein, 2011) using the Wikipedia articles. The number of candidates is limited to 10; these are re-ranked using the learning-to-rank model. For each HIT, we provide between 5 and 10 sentences for simplification. A HIT is then assigned to 10 workers as we need a graded relevance to train the learning-to-rank model (see Section 5). In the experiment, we make sure that a HIT is submitted only in one iteration and most importantly, during the evaluation of the ranking model performance, we make sure that the training data from previous iterations should not contain HITs from the current iteration. In this exp"
C18-1028,P16-2024,0,0.320418,"online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for a biomedical entity annotation task. Most text simplification approaches employ basic machine translation models from parallel corpora (Xu et al., 2016; Štajner et al., 2017) and using simple Wikipedia for English (Coster and Kauchak, 2011). Simple PPDB (Pavlick and Callison-Burch, 2016) is such a resource that is built automatically, using machine translation techniques on a large number of parallel corpora. The work by Lasecki et al. (2015) shows that using crowdsourcing for text simplification is a valid approach. We also conducted our adaptive text simplification experiment on the Amazon Mechanical Turk crowdsourcing platform using a specialized text simplification tool. 3 Design of the Par4Sim System Par4Sim is a text simplification tool based on an adaptive paraphrasing paradigm. Unlike the traditional text simplification approaches, Par4Sim mimics a normal text editor"
C18-1028,P15-2070,0,0.195556,"Missing"
C18-1028,P17-2016,0,0.0304617,"re incorporated from a stream. The work by Wang et al. (2015) presents SOLAR, a framework of scalable online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for a biomedical entity annotation task. Most text simplification approaches employ basic machine translation models from parallel corpora (Xu et al., 2016; Štajner et al., 2017) and using simple Wikipedia for English (Coster and Kauchak, 2011). Simple PPDB (Pavlick and Callison-Burch, 2016) is such a resource that is built automatically, using machine translation techniques on a large number of parallel corpora. The work by Lasecki et al. (2015) shows that using crowdsourcing for text simplification is a valid approach. We also conducted our adaptive text simplification experiment on the Amazon Mechanical Turk crowdsourcing platform using a specialized text simplification tool. 3 Design of the Par4Sim System Par4Sim is a text simplification tool based on an adaptive"
C18-1028,P15-1163,0,0.0307476,"lems during deployment in a real-world application due to concept drift (Tsymbal, 2004) as applications start generating data streams continually. Applications with such properties include spam filtering and intrusion detection. Stream-based learning and online learning (Bottou, 1998) are alternative setups to a batch-mode adap332 tive learning system. For example, the work by Levenberg et al. (2010) shows that the deployment of stream-based learning for statistical machine translation improves the performance of their system when new sentence pairs are incorporated from a stream. The work by Wang et al. (2015) presents SOLAR, a framework of scalable online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for a biomedical entity annotation task. Most text simplification approaches employ basic machine translation models from parallel corpora (Xu et al., 2016; Štajner et al., 2017) and using simple Wikipedia for English"
C18-1028,W16-1801,1,0.306433,"spam filtering and intrusion detection. Stream-based learning and online learning (Bottou, 1998) are alternative setups to a batch-mode adap332 tive learning system. For example, the work by Levenberg et al. (2010) shows that the deployment of stream-based learning for statistical machine translation improves the performance of their system when new sentence pairs are incorporated from a stream. The work by Wang et al. (2015) presents SOLAR, a framework of scalable online learning algorithms for ranking, to tackle the poor scalability problem of batch and offline learning models. The work by Yimam et al. (2016a) uses MIRA (Margin Infused Relaxed Algorithm) (Crammer and Singer, 2003), a perceptron-based online learning algorithm to generate suggestions in an iterative and interactive annotation setup for a biomedical entity annotation task. Most text simplification approaches employ basic machine translation models from parallel corpora (Xu et al., 2016; Štajner et al., 2017) and using simple Wikipedia for English (Coster and Kauchak, 2011). Simple PPDB (Pavlick and Callison-Burch, 2016) is such a resource that is built automatically, using machine translation techniques on a large number of paralle"
C18-1028,I17-2068,1,0.564959,"l, more specifically adaptive paraphrasing based on usage data for the task of text simplification. The main technical challenge is the integration of an adaptive paraphrasing model into a text simplification writing aid tool in order to be able to attain whether it is possible to gain NLP component quality through adaptive learning on usage data. The writing aid tool for text simplification using an adaptive paraphrasing model (Par4Sim) consists of several components. The first component in the pipeline determines the complex or difficult words or phrases (CPs), which is based on the work of Yimam et al. (2017). Once the CPs are identified, the second component produces candidate suggestions from different paraphrase resources. Candidates that do not fit the context in the sentence are filtered or excluded based on a language model score. Finally, an adaptive ranking model reorders candidates based on their simplicity and provides the ranked candidates to the user within an interactive writing aid tool. This work is aiming to answer the following three research questions: RQ1: How can an adaptive paraphrase ranking model be integrated into a text simplification writing aid tool? RQ2: How can an adap"
C18-1028,W18-0507,1,0.718504,"Phrase2Vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news text (Graff, 2002). Mikolov et al. (2013) pointed out that it is possible to extend the word-based model to a phrase-based model using a data-driven approach where each phrase or multi-word expressions are considered as individual tokens during the training process. We have used a total of 79,349 multiword expression and phrase resources, which are obtained from the work of Yimam et al. (2016b). We trained the Phrase2Vec embeddings with 200 1 https://sites.google.com/view/cwisharedtask2018/ (Yimam et al., 2018) 335 dimensions using skip-gram training and a window size of 5. We retrieve the top 10 similar words to the CPs as candidates. Obviously, the number of candidate suggestions obtained from these different resources is enormous and we should limit the size before providing to the ranker model. The candidates are ordered by a language model score. We trained a tri-gram language-model (Pauls and Klein, 2011) using the Wikipedia articles. The number of candidates is limited to 10; these are re-ranked using the learning-to-rank model. For each HIT, we provide between 5 and 10 sentences for simplifi"
cholakov-etal-2014-lexical,E12-1039,0,\N,Missing
cholakov-etal-2014-lexical,W02-0816,0,\N,Missing
cholakov-etal-2014-lexical,P06-1057,0,\N,Missing
cholakov-etal-2014-lexical,N13-1133,1,\N,Missing
D13-1089,N09-1003,0,0.0991462,"Missing"
D13-1089,P01-1005,0,0.0119188,"computation for very large corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora. 1 2 Introduction Using larger data to estimate models for machine learning applications as well as for applications of Natural Language Processing (NLP) has repeatedly shown to be advantageous, see e.g. (Banko and Brill, 2001; Brants et al., 2007). In this work, we tackle the influence of corpus size for building a distributional thesaurus (Lin, 1998). Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability. We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora. For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the numb"
D13-1089,D07-1090,0,0.0148065,"rge corpora feasible on comparably small computational resources. We demonstrate this by releasing a DT for the whole vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora. 1 2 Introduction Using larger data to estimate models for machine learning applications as well as for applications of Natural Language Processing (NLP) has repeatedly shown to be advantageous, see e.g. (Banko and Brill, 2001; Brants et al., 2007). In this work, we tackle the influence of corpus size for building a distributional thesaurus (Lin, 1998). Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability. We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora. For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features"
D13-1089,J90-1003,0,0.443207,"Missing"
D13-1089,W02-1029,0,0.135128,"l thesaurus (Lin, 1998). Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability. We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora. For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases. This makes standard similarity calculations as proposed in (Lin, 1998; Curran, 2002; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasiRelated Work A variety of approaches to compute DTs have been proposed to tackle issues regarding size and runtime. The reduction of the feature space seems to be one possibility, but still requires the computation of such reduction cf. (Blei et al., 2003; Golub and Kahan, 1965). Other approaches use randomised indexing for storing counts or hashing functions to approximate counts and measures (Gorman and Curran, 2006; Goyal et al., 2010; Sahlgren, 2006). Another possibility is the usage of distributed processing like MapRedu"
D13-1089,S13-1035,0,0.287588,"adequacy for large corpora in Section 5. 4 Evaluation The evaluation is performed using a recent dump of English Wikipedia, containing 36 million sentences and a newspaper corpus, compiled from 120 million sentences (about 2 Gigawords) from Leipzig Corpora Collection (Richter et al., 2006) and the Gigaword corpus (Parker et al., 2011). The DTs are based on collapsed dependencies from the Stanford Parser (Marneffe et al., 2006) in the holing operation. For all DTs we use the pruning parameters s=0, p=1000 and w=1000. In a final evaluation, we use the syntactic n-grams built from Google Books (Goldberg and Orwant, 2013). To show the impact of corpus size, we downsampled our corpora to 10 million, 1 million and 100,000 sentences. We compare our results against DTs calculated using Lin’s (Lin, 1998) measure and the best measure proposed by Curran (2002) (see Table 1). Our evaluation is performed using the same 1000 frequent and 1000 infrequent nouns as previously employed by Weeds et al. (2004). We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget’s 1911 thesaurus, Moby Thesaurus, Merriam Webster’s Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ"
D13-1089,P06-1046,0,0.0568805,"red and the number of context features increases. This makes standard similarity calculations as proposed in (Lin, 1998; Curran, 2002; Lund and Burgess, 1996; Weeds et al., 2004) computationally infeasiRelated Work A variety of approaches to compute DTs have been proposed to tackle issues regarding size and runtime. The reduction of the feature space seems to be one possibility, but still requires the computation of such reduction cf. (Blei et al., 2003; Golub and Kahan, 1965). Other approaches use randomised indexing for storing counts or hashing functions to approximate counts and measures (Gorman and Curran, 2006; Goyal et al., 2010; Sahlgren, 2006). Another possibility is the usage of distributed processing like MapReduce. In (Pantel et al., 2009; Agirre et al., 2009) a DT is computed using MapReduce on 200 quad core nodes (for 5.2 billion sentences) respectively 2000 cores (1.6 Terawords), an amount of hardware only available to commercial search engines. Whereas Agirre uses a χ2 test to measure the information between terms and context, Pantel uses the Pointwise Mutual Information (PMI). Then, both approaches use the cosine similarity to calculate the similarity between terms. Furthermore, Pantel d"
D13-1089,W11-1705,0,0.152698,"Missing"
D13-1089,W10-2808,0,0.0355394,"Missing"
D13-1089,P97-1009,0,0.74699,"Here, they propose a pruning scheme similar to ours, but do not explicitly evaluate its effect. The evaluation of DTs has been performed in extrinsic and intrinsic manner. Extrinsic evaluations have been performed using e.g. DTs for automatic 884 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 884–890, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics set expansion (Pantel et al., 2009) or phrase polarity identification (Goyal and Daum´e, 2011). In this work we will concentrate on intrinsic evaluations: Lin (1997; 1998) introduced two measures using WordNet (Miller, 1995) and Roget’s Thesaurus. Using WordNet, he defines context features (synsets a word occurs in Wordnet or subsets when using Roget’s Thesaurus) and then builds a gold standard thesaurus using a similarity measure. Then he evaluates his generated Distributional Thesaurus (DT) with respect to the gold standard thesauri. Weeds et al. (2004) evaluate various similarity measures based on 1000 frequent and 1000 infrequent words. Curran (2004) created a gold standard thesaurus by manually extracting entries from several English thesauri for 70"
D13-1089,P98-2127,0,0.757436,"e vocabulary of Google Books syntactic n-grams. Evaluating against lexical resources using two measures, we show that our approach produces higher quality DTs than previous approaches, and is thus preferable in terms of speed and quality for large corpora. 1 2 Introduction Using larger data to estimate models for machine learning applications as well as for applications of Natural Language Processing (NLP) has repeatedly shown to be advantageous, see e.g. (Banko and Brill, 2001; Brants et al., 2007). In this work, we tackle the influence of corpus size for building a distributional thesaurus (Lin, 1998). Especially, we shed light on the interaction of similarity measures and corpus size, as well as aspects of scalability. We shortly introduce the JoBimText framework for distributional semantics and show its scalability for large corpora. For the computation of the data we follow the MapReduce (Dean and Ghemawat, 2004) paradigm. The computation of similarities between terms becomes challenging on large corpora, as both the numbers of terms to be compared and the number of context features increases. This makes standard similarity calculations as proposed in (Lin, 1998; Curran, 2002; Lund and"
D13-1089,de-marneffe-etal-2006-generating,0,0.0342703,"Missing"
D13-1089,N04-3012,0,0.106519,"best measure proposed by Curran (2002) (see Table 1). Our evaluation is performed using the same 1000 frequent and 1000 infrequent nouns as previously employed by Weeds et al. (2004). We create a gold standard, by extracting reasonable entries of these 2000 nouns using Roget’s 1911 thesaurus, Moby Thesaurus, Merriam Webster’s Thesaurus, the Big Huge Thesaurus and the OpenOffice Thesaurus and employ the inverse ranking measure (Curran, 2002) to evaluate the DTs. Furthermore, we introduce a WordNet-based method. To calculate the similarity between two terms, we use the WordNet::Similarity path (Pedersen et al., 2004) measure. While its absolute scores are hard to interpret due to inhomogenity in the gran886 ularity of WordNet, they are well-suited for relative comparison. The score between two terms is inversely proportional to the shortest path between all the synsets of both terms. The highest possible score is one, if two terms share a synset. We compare the average score of the top five (or ten) entries in the DT for each of the 2000 selected words for our comparison. 5 Results First, we inspect the results of Curran’s measure using the Wikipedia and newspaper corpus for the frequent nouns, shown in F"
D13-1089,P07-2011,0,0.14926,"Missing"
D13-1089,C04-1146,0,0.114882,"Missing"
D13-1089,C98-2122,0,\N,Missing
D13-1089,D09-1098,0,\N,Missing
D15-1290,abeille-barrier-2004-enriching,0,0.0375308,"Missing"
D15-1290,bouamor-etal-2012-identifying,0,0.0144648,"(called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate. Distributional semantics is mostly used to detect compositionality of MWEs (Salehi et al., 2014; Katz and Giesbrecht, 2006). Most approaches therefore compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. The similarity between the vectors is then used as degree for compositionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term (cf. (Bouamor et al., 2012; Anastasiou, 2010)). Whereas this follows the same intuition as our uniqueness measure, we do not require any bilingual corpora. Regarding the evaluation, mostly precision at k (P @k) and recall at k (R@k) are applied (e.g. (Evert, 2005; Frantzi et al., 1998; Lossio-Ventura et al., 2014)). Another general approach is using the average precision (AP), which is also used in Information Retrieval (IR) (Thater et al., 2009) and has also been applied by Ramisch et al. (2012). 3 Baselines and Previous Approaches We will evaluate our method by comparing our MWE ranking to multiword lists that have b"
D15-1290,P14-1006,0,0.0246735,"for a range of task-based MWE definitions. Most previous MWE ranking approaches use the following mechanisms to determine multiwordness: part-of-speech (POS) tags, word/multiword frequency and significance of co-occurrence of the parts. In this paper we do not want to introduce ”yet another ranking function” but rather an additional mechanism, which performs ranking based on distributional semantics. Distributional semantics has already been used for MWE identification, but mainly to discriminate between compositional and noncompositional MWEs (Schone and Jurafsky, 2001; Salehi et al., 2014; Hermann and Blunsom, 2014). Here we introduce a new concept to describe the multiwordness of a term by its uniqueness. Using the uniqueness score we measure how likely a term in context can be replaced by a single word. This measure is motivated by the semiotic consideration that due to parsimony concepts are often expressed as single words. Furthermore, we implement a context-aware punishment, called incompleteness, which degrades the score of terms that seem incomplete regarding their contexts. Both concepts are combined into a single score we call DRUID, which is calculated based on a distributional thesaurus. In th"
D15-1290,W06-1203,0,0.0137237,"multiwordness was first proposed by Pecina (2010), who, however, restricts his experiments to twoword MWEs for the Czech language only. Korkontzelos (2010) comparatively evaluates several MWE ranking measures. The best MWE extractor reported in his work is the scorer by (Nakagawa and Mori, 2002; Nakagawa and Mori, 2003), who use the un-nested frequency (called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate. Distributional semantics is mostly used to detect compositionality of MWEs (Salehi et al., 2014; Katz and Giesbrecht, 2006). Most approaches therefore compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. The similarity between the vectors is then used as degree for compositionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term (cf. (Bouamor et al., 2012; Anastasiou, 2010)). Whereas this follows the same intuition as our uniqueness measure, we do not require any bilingual corpora. Regarding the evaluation, mostly precision at k (P @k) and recall at k (R@k) are applied (e.g. (Evert, 2005; Frantzi et a"
D15-1290,C94-1084,0,0.0779718,"Missing"
D15-1290,W02-1407,0,0.744061,"the frequency of a candidate when placing wildcards for each word. A newer method is introduced in Lossio-Ventura et al. (2014), which reranks scores based on an extension of the C-value, which uses a POS-based probability and an inverse document frequency. Using different measures and learning a classifier that predicts the multiwordness was first proposed by Pecina (2010), who, however, restricts his experiments to twoword MWEs for the Czech language only. Korkontzelos (2010) comparatively evaluates several MWE ranking measures. The best MWE extractor reported in his work is the scorer by (Nakagawa and Mori, 2002; Nakagawa and Mori, 2003), who use the un-nested frequency (called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate. Distributional semantics is mostly used to detect compositionality of MWEs (Salehi et al., 2014; Katz and Giesbrecht, 2006). Most approaches therefore compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. The similarity between the vectors is then used as degree for compositionality. In machine translation, words are sometimes considered as"
D15-1290,W12-3301,0,0.234821,"Missing"
D15-1290,W12-3311,0,0.0447457,"h and also examine the effect of corpus size on MWE extraction. Additionally, we report on results without using any linguistic preprocessing except tokenization. 2 Related Work The generation of MWE dictionaries has drawn much attention in the field of Natural Language Processing (NLP). Early computational approaches (e.g. Justeson and Katz (1995)) use POS sequences as MWE extractors. Other approaches, relying on word frequency, statistically verify the hypothesis whether the parts of the MWE occur more often together than would be expected by chance (Manning and Sch¨utze, 1999; Evert, 2005; Ramisch, 2012). One of the first measures that consider context information (cooccurrences) are the C-value and the NC-value introduced by Frantzi et al. (1998). These methods first extract candidates using POS information 2430 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2430–2440, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and then compute scores based on the frequency of the MWE and the frequency of nested MWE candidates. The method described by Wermter and Hahn (2005) computes a score by multiplying the frequ"
D15-1290,E14-1050,0,0.0790465,"asure that works well for a range of task-based MWE definitions. Most previous MWE ranking approaches use the following mechanisms to determine multiwordness: part-of-speech (POS) tags, word/multiword frequency and significance of co-occurrence of the parts. In this paper we do not want to introduce ”yet another ranking function” but rather an additional mechanism, which performs ranking based on distributional semantics. Distributional semantics has already been used for MWE identification, but mainly to discriminate between compositional and noncompositional MWEs (Schone and Jurafsky, 2001; Salehi et al., 2014; Hermann and Blunsom, 2014). Here we introduce a new concept to describe the multiwordness of a term by its uniqueness. Using the uniqueness score we measure how likely a term in context can be replaced by a single word. This measure is motivated by the semiotic consideration that due to parsimony concepts are often expressed as single words. Furthermore, we implement a context-aware punishment, called incompleteness, which degrades the score of terms that seem incomplete regarding their contexts. Both concepts are combined into a single score we call DRUID, which is calculated based on a dis"
D15-1290,W01-0513,0,0.039005,"Missing"
D15-1290,seddah-etal-2012-ubiquitous,0,0.179932,"Missing"
D15-1290,W09-2506,0,0.0708666,"en the vectors is then used as degree for compositionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term (cf. (Bouamor et al., 2012; Anastasiou, 2010)). Whereas this follows the same intuition as our uniqueness measure, we do not require any bilingual corpora. Regarding the evaluation, mostly precision at k (P @k) and recall at k (R@k) are applied (e.g. (Evert, 2005; Frantzi et al., 1998; Lossio-Ventura et al., 2014)). Another general approach is using the average precision (AP), which is also used in Information Retrieval (IR) (Thater et al., 2009) and has also been applied by Ramisch et al. (2012). 3 Baselines and Previous Approaches We will evaluate our method by comparing our MWE ranking to multiword lists that have been annotated in corpora. Here, we introduce an upper bound and two baseline methods and give a brief description of the competitive methods used in this paper. Most of these methods require a list of candidate terms T , usually extracted with POS sequences (see Section 5). 3.1 Upper Bound We use a perfect ranking as upper bound, where we rank all positive candidates before all negative ones. 3.2 Lower Baseline and Frequ"
D17-2016,J13-3008,0,0.00623738,"Missing"
D17-2016,D12-1129,1,0.834113,"o the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtained by averaging vectors of words sharing the same class. 3.2 an ambiguous word and its context. The output of the system is a ranked list of all word senses of the ambiguous word ordered by relevance to the input context. By default, only the best matching sense is displayed. The user can quickly understand the meaning of each induced sense by looking at the hypernym and the image representing the sense. Faralli and Navigli (2012) showed that Web search engines can be used to acquire information about word senses. We assign an image to each word in the cluster by querying an image search API9 using a query composed of the ambiguous word and its hypernym, e.g. “jaguar animal”. The first hit of this query is selected to represent the induced word sense. Interpretability of each sense is further ensured by providing to the user the list of related senses, the list of the most salient context clues, and the sense usage examples (cf. Figure 2). Note that all these elements are obtained without manual intervention. Finally,"
D17-2016,C92-2082,0,0.232587,"ollections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based"
D17-2016,S13-2049,0,0.0191872,"ical context clues. Each of these elements is extracted automatically. The reasons of the predictions are provided in terms of common sparse features of the input sentence and a sense representation (E). The induced senses are linked to BabelNet using the method of Faralli et al. (2016) (F). Figure 3: All words disambiguation mode: results of disambiguation of all nouns in a sentence. 94 # Words # Senses Avg. Polysemy # Contexts 863 2,708 3.13 11,712 WSD Model Inventory Features Table 1: Evaluation dataset based on BabelNet. methods for WSD, including participants of the SemEval 2013 Task 13 (Jurgens and Klapaftis, 2013) and two unsupervised knowledge-free WSD systems based on word sense embeddings (Bartunov et al., 2016; Pelevina et al., 2016). These evaluations were based on the “lexical sample” setting, where the system is expected to predict a sense identifier of the ambiguous word. In this section, we perform an extra evaluation that assesses how well hypernyms of ambiguous words are assigned in context by our system. Namely, the task is to assign a correct hypernym of an ambiguous word, e.g. “animal” for the word “Jaguar” in the context “Jaguar is a large spotted predator of tropical America”. This task"
D17-2016,P13-4007,0,0.351132,"Missing"
D17-2016,Q14-1019,0,0.219564,"retable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements representing word senses, such as hypernyms, usage examples, and images. We present a WSD system that bridges the ga"
D17-2016,D14-1113,0,0.0192822,"nces are used as sense usage examples. For more details about the model induction process refer to (Panchenko et al., 2017). Currently, the following WSD models induced from a text corpus are available: Word senses based on cluster word features. This model uses the cluster words from the induced word sense inventory as sparse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user inte"
D17-2016,E17-1009,1,0.647167,"ioned features enabling interpretability. For instance, systems based on sense embeddings are based on dense uninterpretable vectors. Therefore, the meaning of a sense can be interpreted only on the basis of a list of related senses. We present a system that brings interpretability of the knowledge-based sense representations into the world of unsupervised knowledge-free WSD models. The contribution of this paper is the first system for word sense induction and disambiguation, which is unsupervised, knowledge-free, and interpretable at the same time. The system is based on the WSD approach of Panchenko et al. (2017) and is designed to reach interpretability level of knowledge-based systems, such as Babelfy (Moro et al., 2014), within an unsupervised knowledgefree framework. Implementation of the system is open source.1 A live demo featuring several disambiguation models is available online.2 Interpretability of a predictive model is a powerful feature that gains the trust of users in the correctness of the predictions. In word sense disambiguation (WSD), knowledge-based systems tend to be much more interpretable than knowledge-free counterparts as they rely on the wealth of manually-encoded elements repr"
D17-2016,W16-1620,1,0.761499,"parse features that represent the sense. Word senses based on context word features. This representation is based on a sum of word vectors of all cluster words in the induced sense inventory weighted by distributional similarity scores. Knowledge-Free and Unsupervised Systems Neelakantan et al. (2014) proposed a multi-sense extension of the Skip-gram model that features an open implementation. AdaGram (Bartunov et al., 2016) is a system that learns sense embeddings using a Bayesian extension of the Skip-gram model and provides WSD functionality based on the induced sense inventory. SenseGram (Pelevina et al., 2016) is a system that transforms word embeddings to sense embeddings via graph clustering and uses them for WSD. Other methods to learn sense embeddings were proposed, but these do not feature open implementations for WSD. Among all listed systems, only Babelfy implements a user interface supporting interpretable visualization of the disambiguation results. 3 Unsupervised Knowledge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction"
D17-2016,W06-3812,1,0.134336,"tecture of the WSD system. As one may observe, no human labor is used to learn interpretable sense representations and the corresponding disambiguation models. Instead, these are induced from the input text corpus using the JoBimText approach (Biemann and Riedl, 2013) implemented using the Apache Spark framework4 , enabling seamless processing of large text collections. Induction of a WSD model consists of several steps. First, a graph of semantically related words, i.e. a distributional thesaurus, is extracted. Second, word senses are induced by clustering of an ego-network of related words (Biemann, 2006). Each discovered word sense is represented as a cluster of words. Next, the induced sense inventory is used as a pivot to generate sense representations by aggregation of the context clues of cluster words. To improve interpretability of the sense clusters they are labeled with hypernyms, which are in turn extracted from the input corpus using Hearst (1992) patterns. Finally, the obtained WSD model is used to retrieve a list of sentences that characterize each sense. Sentences that mention a given word are disambiguated and then ranked by prediction confidence. Top sentences are used as sense"
D17-2016,P15-1173,0,0.0206782,"Missing"
D17-2016,biemann-2012-turk,1,0.85795,"Missing"
D17-2016,P17-1145,1,0.786442,"dge-Free Interpretable WSD This section describes (1) how WSD models are learned in an unsupervised way from text and (2) how the system uses these models to enable human interpretable disambiguation in context. 3 Induction of the WSD Models 4 https://github.com/alvations/pywsd 92 http://spark.apache.org Super senses based on cluster word features. To build this model, induced word senses are first globally clustered using the Chinese Whispers graph clustering algorithm (Biemann, 2006). The edges in this sense graph are established by disambiguation of the related words (Faralli et al., 2016; Ustalov et al., 2017). The resulting clusters represent semantic classes grouping words sharing a common hypernym, e.g. “animal”. This set of semantic classes is used as an automatically learned inventory of super senses: There is only one global sense inventory shared among all words in contrast to the two previous traditional “per word” models. Each semantic class is labeled with hypernyms. This model uses words belonging to the semantic class as features. Super senses based on context word features. This model relies on the same semantic classes as the previous one but, instead, sense representations are obtain"
D17-2016,P10-4014,0,0.042323,"ntations. Introduction The notion of word sense is central to computational lexical semantics. Word senses can be either encoded manually in lexical resources or induced automatically from text. The former knowledgebased sense representations, such as those found in the BabelNet lexical semantic network (Navigli and Ponzetto, 2012), are easily interpretable by humans due to the presence of definitions, usage examples, taxonomic relations, related words, and images. The cost of such interpretability is that every element mentioned above is encoded Knowledge-Based and/or Supervised Systems IMS (Zhong and Ng, 2010) is a supervised allwords WSD system that allows users to integrate additional features and different classifiers. By default, the system relies on the linear support vector machines with multiple features. The AutoExtend (Rothe and Sch¨utze, 2015) approach can be used to learn embeddings for lexemes and synsets 1 2 91 https://github.com/uhh-lt/wsd http://jobimtext.org/wsd Proceedings of the 2017 EMNLP System Demonstrations, pages 91–96 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Induction of the WSD Models (Scala/Spark): §3.1 WSD Model Index RES"
D18-2009,W14-0311,0,0.0282805,"le for the adaptive model. The adaptive ranking model is a learning-torank machine learning algorithm, which is used to re-rank candidate suggestions provided for the target unit. We rely on existing paraphrase resources such as PPDB 2.0, WordNet, distributional thesaurus and word embeddings (see Section 2.1.1) to generate candidate suggestions. Some other examples for adaptive NLP setups include: 1) online learning for ranking, example Yogatama et al. (2014) who tackle the pairwise learning-to-ranking problem via a scalable online learning approach, 2) adaptive machine translation (MT), e.g. Denkowski et al. (2014) describe a framework for building adaptive MT systems that learn from post-editor feedback, and 3) incremental learning for spam filtering, e.g. Sheu et al. (2017) use a window-based technique to estimate for the condition of concept drift for each incoming new email. We have evaluated our approach with a lexical simplification task use-case. The lexical simplification task contains complex word identification (adaptive target identification) and simpler candidate selection (adaptive ranking) components. As far as our knowledge concerned, PAR 4S EM is the first tool in the semantic and NLP re"
D18-2009,C18-1028,1,0.818702,"4S EM consists of different technologies, machine learning setups, resources, and configurations, we opted to provide a Docker-based installation and deployment options. While it is possible to fully install the tool on ones own server, we also provide an API access for the whole backend services. This allows users to quickly and easily install the frontend component and relay on our API service calls for the rest of the communications. 3 Figure 6: Learning curve showing the increase of NDCG@10 score over 9 iterations. candidates are presented using a default languagemodel-based ranking. In (Yimam and Biemann, 2018) we have shown that the adaptive paraphrasing system adopts very well to text simplification, improving the NDCG (Wang et al., 2013) score from 60.66 to 75.70. Figure 6 shows the learning curve for the different iterations conducted in the experiment. Use-case – Adaptive Text Simplification using Crowdsourcing An appropriate use case for adaptive paraphrasing is lexical text simplification. Lexical simplification aims to reduce the complexity of texts due to difficult words or phrases in the text (Siddharthan, Advaith, 2014). We have used PAR 4S EM particularly for text simplification task wit"
D18-2009,W18-0507,1,0.844091,"Missing"
D18-2009,W16-1801,1,0.839461,"ons (Biemann and Riedl, 2013), to extract candidate suggestions that are semantically similar to the target unit. Phrase2Vec: We train a Phrase2Vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news text (Graff, 2002). Mikolov et al. (2013) pointed out that it is possible to extend the word based embeddings model to phrase-based model using a datadriven approach where each phrase or multi-word expressions are considered as individual tokens during the training process. We have used a total of 79,349 multiword expression and phrase resources as given in Yimam et al. (2016). We train the Phrase2Vec embeddings with 200 dimensions using skip-gram training and a window size of 5. We have retrieved the top 10 similar words to the target units as candidate suggestions. suggestions using a learning-to-rank adaptive machine learning model. Figure 2 displays the process of the adaptive models while Figure 3 displays the pipeline (as a loop) used in the generations of the adaptive models. Figure 2: The main and sub-processes of target and ranking adaption components of PAR 4S EM. 2.1.2 Adaptive Machine Learning PAR 4S EM incorporates two adaptive machine learning models."
D18-2009,I17-2068,1,0.792487,"M is available at https://ltmaggie.informatik. uni-hamburg.de/par4sem. 2 end and returns responses to the frontend. Figure 1 shows the three main components of PAR 4S EM and their interactions. Figure 1: The main components of PAR 4S EM 2.1 The Backend Component The backend component consists of several modules. For the adaptive paraphrasing system, the first component is to identify possible target units (such as single words, phrases, or multi-word expressions). For our lexical simplification use-case, the target units identification component is instantiated with the datasets obtained from Yimam et al. (2017a,b, 2018). The adaptive target identification unit then keeps on learning from the usage data (when the user highlights portions of the text to get paraphrase candidate suggestions). Once target units are marked or recognized (by the target unit identification system), the next step is to generate possible candidate suggestion for the target unit (paraphrase candidates). The candidate suggestion module includes candidate generation and candidate ranking sub-modules. Section 2.1.1 discusses our approaches to generating and ranking paraphrase candidates in detail. 2.1.1 Paraphrasing Resources P"
D18-2009,P15-2070,0,0.0232422,"Missing"
D18-2009,yimam-etal-2017-multilingual,1,0.697679,"M is available at https://ltmaggie.informatik. uni-hamburg.de/par4sem. 2 end and returns responses to the frontend. Figure 1 shows the three main components of PAR 4S EM and their interactions. Figure 1: The main components of PAR 4S EM 2.1 The Backend Component The backend component consists of several modules. For the adaptive paraphrasing system, the first component is to identify possible target units (such as single words, phrases, or multi-word expressions). For our lexical simplification use-case, the target units identification component is instantiated with the datasets obtained from Yimam et al. (2017a,b, 2018). The adaptive target identification unit then keeps on learning from the usage data (when the user highlights portions of the text to get paraphrase candidate suggestions). Once target units are marked or recognized (by the target unit identification system), the next step is to generate possible candidate suggestion for the target unit (paraphrase candidates). The candidate suggestion module includes candidate generation and candidate ranking sub-modules. Section 2.1.1 discusses our approaches to generating and ranking paraphrase candidates in detail. 2.1.1 Paraphrasing Resources P"
D18-2009,Q14-1015,0,0.0179238,"ords, phrases or multi-word expressions), that need to be paraphrased. When the user highlights target words (usage data), it is considered as a training example for the adaptive model. The adaptive ranking model is a learning-torank machine learning algorithm, which is used to re-rank candidate suggestions provided for the target unit. We rely on existing paraphrase resources such as PPDB 2.0, WordNet, distributional thesaurus and word embeddings (see Section 2.1.1) to generate candidate suggestions. Some other examples for adaptive NLP setups include: 1) online learning for ranking, example Yogatama et al. (2014) who tackle the pairwise learning-to-ranking problem via a scalable online learning approach, 2) adaptive machine translation (MT), e.g. Denkowski et al. (2014) describe a framework for building adaptive MT systems that learn from post-editor feedback, and 3) incremental learning for spam filtering, e.g. Sheu et al. (2017) use a window-based technique to estimate for the condition of concept drift for each incoming new email. We have evaluated our approach with a lexical simplification task use-case. The lexical simplification task contains complex word identification (adaptive target identifi"
D18-2014,goldhahn-etal-2012-building,0,0.0352943,"ovides automatically learned rules for temporal tagging in more than 200 languages. Extracted timestamps can be used to select and filter documents. 11 Keyterm Extraction To further summarize document contents in addition to named entities, we automatically extract keyterms and phrases from documents. For this, we implemented a keyterm extraction library for the 40 languages also supported in the previous step.15 Our approach is based on a statistical comparison of document contents with generic reference data. Reference data for each language is retrieved from the Leipzig Corpora Collection (Goldhahn et al., 2012), which provides large representative corpora for language statistics. We employ log-likelihood significance as described in (Rayson et al., 2004) to measure the overuse of terms (i.e. keyterms) in our target documents compared to the generic reference data. Ongoing sequences of keyterms in target documents are concatenated to key phrases if they occur regularly in that exact same order. Regularity is determined with the Dice coefficient. This simple method allows to reliably extract multiword units such as “stock market” or “machine learning” in the documents. Since this method also extracts"
D18-2014,D15-1063,0,0.0393719,"Missing"
D18-2014,P16-4028,1,0.837228,"Missing"
E17-1009,D14-1110,0,0.0391811,"e amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of con"
E17-1009,de-marneffe-etal-2006-generating,0,0.0297452,"Missing"
E17-1009,J13-3008,0,0.232296,"Missing"
E17-1009,S13-2050,0,0.0233307,"del Precision Recall F-score Precision Recall F-score Dependencies Dependencies Exp. 0.728 0.687 0.343 0.633 0.466 0.659 0.432 0.414 0.190 0.379 0.263 0.396 Dependencies + LM Dependencies Exp. + LM 0.689 0.684 0.681 0.676 0.685 0.680 0.426 0.412 0.422 0.408 0.424 0.410 Table 2: Effect of the feature expansion: performance on the full (on the left) and the sense-balanced (on the right) TWSI datasets. The models were trained on the Wikipedia corpus using the coarse inventory (1.96 senses per word). The best results overall are underlined. 4.2 generated using word embeddings6 . The AI-KU system (Baskaya et al., 2013) directly clusters test contexts using the k-means algorithm based on lexical substitution features. The Unimelb system (Lau et al., 2013) uses one hierarchical topic model to induce and disambiguate senses of one word. The UoS system (Hope and Keller, 2013) induces senses by building an ego-network of a word using dependency relations, which is subsequently clustered using the MaxMax clustering algorithm. The La Sapienza system (Jurgens and Klapaftis, 2013), relies on WordNet for the sense inventory and disambiguation. In contrast to the TWSI evaluation, the most fine-grained model yields the"
E17-1009,J93-1003,0,0.524247,"syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words, e.g. “office • and”, (2) two preceding words “new office •”, and (3) two succeeding words, e.g. “• and chairs”. We use the conditional probabilities of the resulting trigrams as word-feature weights. Our approach was inspired by the knowledgebased system Babelfy (Moro et al., 2014). While the inventory of Babelfy is interpretable as it relies on BabelNet, the system provides no underlying reasons"
E17-1009,D12-1129,1,0.811774,"scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, for which we here show the first one, e.g. “table (furniture)” or “table (data)”. Faralli and Navigli (2012) showed that web search engines can be used to bootstrap senserelated information. To further improve interpretability of induced senses, we assign an image to each word in the cluster (see Figure 2) by queryWord Sense Induction We induce a sense inventory by clustering of egonetwork of similar words. In our case, an inventory represents senses by a word cluster, such as “chair, bed, bench, stool, sofa, desk, cabinet” for the “furniture” sense of the word “table”. The sense induction processes one word t of the distributional thesaurus T per iteration. First, we retrieve nodes of the ego-netwo"
E17-1009,W06-3812,1,0.735982,"s, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived using corpus-bas"
E17-1009,biemann-2012-turk,1,0.869612,"sociated with the senses are retrieved using a search engine:“table data” and “table furniture”. 4 2. Sense feature representation. Each sense in our model is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector repres"
E17-1009,C92-2082,0,0.102793,"y a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can vice versa be characterized by a distribution over words. We use this duality to compute feature similarities using the same mechanism and explore their use in disambiguation below. 3.3 3.4 Labeling Induced Senses with Hypernyms and Images Each sense cluster is automatically labeled to improve its interpretability. First, we extract hypernyms from the input corpus using Hearst (1992) patterns. Second, we rank hypernyms relevant to the cluster by a product of two scores: Pthe hypernym relevance score, calculated as w∈cluster sim(t, w)f req(w, h), and Pthe hypernym coverage score, calculated Here the as w∈cluster min(f req(w, h), 1). sim(t, w) is the relatedness of the cluster word w to the target word t, and the f req(w, h) is the frequency of the hypernymy relation (w, h) as extracted via patterns. Thus, a high-ranked hypernym h has high relevance, but also is confirmed by several cluster words. This stage results in a ranked list of labels that specify the word sense, fo"
E17-1009,N15-1059,0,0.0197223,"word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are"
E17-1009,P12-1092,0,0.0637845,"lly from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words"
E17-1009,P15-1010,0,0.0282997,"es rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-p"
E17-1009,P14-2050,0,0.0238238,"rameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions over features, features can"
E17-1009,J98-1001,0,0.0520264,"Missing"
E17-1009,D15-1200,0,0.0789216,"for Word Sense Induction and Disambiguation Alexander Panchenko‡ , Eugen Ruppert‡ , Stefano Faralli† , Simone Paolo Ponzetto† and Chris Biemann‡ ‡ † Language Technology Group, Computer Science Dept., University of Hamburg, Germany Web and Data Science Group, Computer Science Dept., University of Mannheim, Germany {panchenko,ruppert,biemann}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation"
E17-1009,S13-2049,0,0.14673,"el is characterized by a list of sparse features ordered by relevance to the sense. Figure 2 (2) shows most salient dependency features to senses of the word “table”. These feature representations are obtained by aggregating features of sense cluster words. Experiments We use two lexical sample collections suitable for evaluation of unsupervised WSD systems. The first one is the Turk Bootstrap Word Sense Inventory (TWSI) dataset introduced by Biemann (2012). It is used for testing different configurations of our approach. The second collection, the SemEval 2013 word sense induction dataset by Jurgens and Klapaftis (2013), is used to compare our approach to existing systems. In both datasets, to measure WSD performance, induced senses are mapped to gold standard senses. In experiments with the TWSI dataset, the models were trained on the Wikipedia corpus4 while in experiments with the SemEval datasets models are trained on the ukWaC corpus (Baroni et al., 2009) for a fair comparison with other participants. In systems based on dense vector representations, there is no straightforward way to get the most salient features of a sense, which makes the analysis of learned representations problematic. 3. Disambiguat"
E17-1009,N16-1082,0,0.0684958,"n}@informatik.uni-hamburg.de {faralli,simone}@informatik.uni-mannheim.de Abstract Word sense induction from domain-specific corpora is a supposed to solve this problem. However, most approaches to word sense induction and disambiguation, e.g. (Sch¨utze, 1998; Li and Jurafsky, 2015; Bartunov et al., 2016), rely on clustering methods and dense vector representations that make a WSD model uninterpretable as compared to knowledge-based WSD methods. Interpretability of a statistical model is important as it lets us understand the reasons behind its predictions (Vellido et al., 2011; Freitas, 2014; Li et al., 2016). Interpretability of WSD models (1) lets a user understand why in the given context one observed a given sense (e.g., for educational applications); (2) performs a comprehensive analysis of correct and erroneous predictions, giving rise to improved disambiguation models. The contribution of this paper is an interpretable unsupervised knowledge-free WSD method. The novelty of our method is in (1) a technique to disambiguation that relies on induced inventories as a pivot for learning sense feature representations, (2) a technique for making induced sense representations interpretable by labeli"
E17-1009,P03-1054,0,0.0189642,"g them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another word. We extract the list of words that significantly co-occur in a sentence with the target word in the input corpus based on the log-likelihood as word-feature weight (Dunning, 1993). Language Model Feature. This type of features are based on a trigram model with Kneser-Ney smoothing (Kneser and Ney, 1995). In particular, a word is represented by (1) right and left context words,"
E17-1009,W02-0811,0,0.0372423,"resting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical"
E17-1009,C12-1109,1,0.808148,"ervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e"
E17-1009,S13-2051,0,0.175444,"med by assigning the sense with the highest overlap between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation is this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) presents a comprehensive study of several graph-based WSI methods including Chinese Whispers, HyperLex, curvature clustering (Dorow et al., 2005). Besides, 87 3 authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relations extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures impact of the WSI in the context of an information retrieval via clustering and di"
E17-1009,S15-2049,0,0.00630219,"re. Experiments show that our model performs on par with state-of-the-art word sense embeddings and other unsupervised systems while offering the possibility to justify its decisions in human-readable form. 1 Introduction A word sense disambiguation (WSD) system takes as input a target word t and its context C. The system returns an identifier of a word sense si from the word sense inventory {s1 , ..., sn } of t, where the senses are typically defined manually in advance. Despite significant progress in methodology during the two last decades (Ide and V´eronis, 1998; Agirre and Edmonds, 2007; Moro and Navigli, 2015), WSD is still not widespread in applications (Navigli, 2009), which indicates the need for further progress. The difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017."
E17-1009,W02-1006,0,0.0721617,"later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic"
E17-1009,Q14-1019,0,0.377872,"l, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and Pedersen, 2002; Pedersen et al., 2005; Miller et al., 2012). More recently, several methods were proposed to learn sense embeddings on the basis of the sense inventory of a lexical resource (Chen et al., 2014; Rothe and Sch¨utze, 2015; Camacho-Collados et al., 2015; Iacobacci et al., 2015; Nieto Pi˜na and Johansson, 2016). Unsupervised knowledge-free approaches use neither handcrafted lexical resources nor handannotated sense-labeled corpora. Instead, they induce word"
E17-1009,P10-1023,1,0.451602,"lar, we extract three types of features: Interpretable approaches. The need in methods that interpret results of opaque statistical models is widely recognised (Vellido et al., 2011; Vellido et al., 2012; Freitas, 2014; Li et al., 2016; Park et al., 2016). An interpretable WSD system is expected to provide (1) a human-readable sense inventory, (2) human-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to Babel"
E17-1009,W16-1620,1,0.723784,"difficulty of the problem largely stems from the lack of domain-specific training data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their"
E17-1009,D14-1113,0,0.0333137,"ng data. A fixed sense inventory, such as the one of WordNet (Miller, 1995), may contain irrelevant senses for the given application and at the same time lack relevant domain-specific senses. 1 http://www.jobimtext.org/wsd 86 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 86–98, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Related Work 2014; Bartunov et al., 2016; Li and Jurafsky, 2015; Pelevina et al., 2016). Comparisons of the AdaGram (Bartunov et al., 2016) to (Neelakantan et al., 2014) on three SemEval word sense induction and disambiguation datasets show the advantage of the former. For this reason, we use AdaGram as a representative of the state-of-the-art word sense embeddings in our experiments. In addition, we compare to SenseGram, an alternative sense embedding based approach by Pelevina et al. (2016). What makes the comparison to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre"
E17-1009,W97-0323,0,0.288821,"son to the later method interesting is that this approach is similar to ours, but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingu"
E17-1009,P15-1173,0,0.186255,"Missing"
E17-1009,W16-1401,0,0.0436871,"Missing"
E17-1009,P15-4018,1,0.84569,"-readable reasons why in a given context c a given sense si was detected. Lexical resources, such as WordNet, solve the first problem by providing manually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert,"
E17-1009,J07-2002,0,0.101588,"Missing"
E17-1009,W12-0502,1,0.771556,"ated by the absence of meta-parameters and its comparable performance on the WSI task to the state-of-theart (Di Marco and Navigli, 2013). 2013) as it yields comparable performance on semantic similarity to state-of-the-art dense representations (Mikolov et al., 2013) compared on the WordNet as gold standard (Riedl, 2016), but is interpretable as word are represented by sparse interpretable features. Namely we use dependencybased features as, according to prior evaluations, this kind of features provides state-of-the-art semantic relatedness scores (Pad´o and Lapata, 2007; Van de Cruys, 2010; Panchenko and Morozova, 2012; Levy and Goldberg, 2014). First, features of each word are ranked using the LMI metric (Evert, 2005). Second, the word representations are pruned keeping 1000 most salient features per word and 1000 most salient words per feature. The pruning reduces computational complexity and noise. Finally, word similarities are computed as a number of common features for two words. This is again followed by a pruning step in which only the 200 most similar terms are kept to every word. The resulting word similarities are browsable online.2 Note that while words can be characterized with distributions ov"
E17-1009,J98-1004,0,0.194444,"Missing"
E17-1009,L16-1421,1,0.84598,"nually-crafted definitions of senses, examples of usage, hypernyms, and synonyms. The BabelNet (Navigli and Ponzetto, 2010) integrates all these sense representations, adding to them links to external resources, such as Wikipedia, topical category labels, and images representing the sense. The unsupervised models listed above do not feature any of these representations making them much less interpretable as compared to the knowledge-based models. Ruppert et al. (2015) proposed a system for visualising sense inventories derived in an unsupervised way using graph-based distributional semantics. Panchenko (2016) proposed a method for making sense inventory of word sense embeddings interpretable by mapping it to BabelNet. Dependency Features. These feature represents a word by a syntactic dependency such as “nn(•,writing)” or “prep at(sit,•)”, extracted from the Stanford Dependencies (De Marneffe et al., 2006) obtained with the the PCFG model of the Stanford parser (Klein and Manning, 2003). Weights are computed using the Local Mutual Information (LMI) (Evert, 2005). One word is represented with 1000 most significant features. Co-occurrence Features. This type of features represents a word by another"
E17-1009,C14-1016,0,0.0529438,"supervised WSD methods fall into two main categories: context clustering and word ego-network clustering. Context clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the tar"
E17-1009,C02-1114,0,0.0171122,"text clustering approaches, e.g. (Pedersen and Bruce, 1997; Sch¨utze, 1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. Multi-prototype extensions of the skipgram model (Mikolov et al., 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., Word ego-network clustering methods (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013) cluster graphs of words semantically related to the ambiguous word. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego-network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller, 2013). Graph edges represent semantic relations between words derived u"
E17-1009,P10-4014,0,0.0897347,"but instead of sparse representations the authors rely on word embeddings, making their approach less interpretable. Multiple designs of WSD systems were proposed (Agirre and Edmonds, 2007; Navigli, 2009). They vary according to the level of supervision and the amount of external knowledge used. Most current systems either make use of lexical resources and/or rely on an explicitly annotated sense corpus. Supervised approaches use a sense-labeled corpus to train a model, usually building one submodel per target word (Ng, 1997; Lee and Ng, 2002; Klein et al., 2002; Wee, 2010). The IMS system by Zhong and Ng (2010) provides an implementation of the supervised approach to WSD that yields state-of-the-art results. While supervised approaches demonstrate top performance in competitions, they require large amounts of senselabeled examples per target word. Knowledge-based approaches rely on a lexical resource that provides a sense inventory and features for disambiguation and vary from the classical Lesk (1986) algorithm that uses word definitions to the Babelfy (Moro et al., 2014) system that uses harnesses a multilingual lexicalsemantic network. Classical examples of such approaches include (Banerjee and P"
E17-1009,W97-0322,0,\N,Missing
E17-1056,D13-1018,1,0.87749,"ntrastMedium Algorithm: Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links Stefano Faralli1 , Alexander Panchenko2 , Chris Biemann2 and Simone Paolo Ponzetto1 1 Data and Web Science Group, University of Mannheim, Germany Language Technology Group, University of Hamburg, Germany {stefano,simone}@informatik.uni-mannheim.de {panchenko,biemann}@informatik.uni-hamburg.de 2 Abstract closed information extraction approaches (Dutta et al., 2014). The use of an encyclopedia-centric (e.g., Wikipedia-based) dictionary of entities leads to poor coverage of domain-specific terminologies (Faralli and Navigli, 2013). This can be alleviated by constructing knowledge bases of ever increasing coverage and complexity from the Web (Wu et al., 2012; Gupta et al., 2014; Dong et al., 2014) or by community efforts (Bollacker et al., 2008). However, the focus on large size and wide coverage at entity level has led all these resources to avoid the complementary problem of curating and maintaining a clean taxonomic backbone with as minimal supervision as possible. That is, no resource, to date, integrates structured information from existing wide-coverage knowledge graphs with empirical evidence from text for the ex"
E17-1056,S15-2151,1,0.925043,"Missing"
E17-1056,P99-1016,0,0.48042,"cit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. Co"
E17-1056,N15-1151,0,0.0311339,"Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar, 2015, inter alia); In this paper, we focus on a different, yet complementary task, which is a necessary step when inducing novel KBs from scratch, namely extracting clean taxonomies from noisy knowlSome aspects of the proposed approach – namely, the propagation of the nodes’ weights through the graph, which we metaphorically represent as the flow of a contrast medium across nodes (Section 3.3) – are somewhat similar in spirit to spreading activation (Collins and Loftus, 1975) and random walks on graphs (Lov´asz, 1993) approaches. However, in contrast to spreading activation approaches we leverage"
E17-1056,S16-1206,1,0.913086,"oisy edges, the wrongly-acquired relations between unrelated concepts or out-of-domain relations, e.g., Jaguar Cars isa Feline; iii) cycles of hypernymy relations, such as those derived from counts over very large corpora (Seitner et al., 2016), e.g., jaguar (Panthera onca) → feline → animal → jaguar (Panthera onca). We accordingly define the task of extracting a clean taxonomy from a NKG as that of pruning the cycles, as well as the noisy edges and nodes, from the hypernymy subgraph T of G. 3.2 vised methods. To this end, we use the linked disambiguated distributional KBs from Faralli et al. (2016)1 , which are built in three steps: 1) Learning a JoBimText model. Initially, a sense inventory is created from a large text collection using the pipeline of the JoBimText project (Biemann and Riedl, 2013).2 The resulting structure contains disambiguated protoconcepts (i.e., senses), their similar and related terms, as well as aggregated contextual clues per proto-concept. 2) Disambiguation of related terms. Similar terms and hypernyms associated with a protoconcept are fully disambiguated based on the partial disambiguation from step (1). The result is a proto-conceptualization (PCZ), where a"
E17-1056,C92-2082,0,0.315937,"rmation from existing wide-coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present Contr"
E17-1056,L16-1056,1,0.855973,"e been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as, for instance, a set of manual"
E17-1056,P06-1101,0,0.34143,"mapping (Navigli and Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011;"
E17-1056,D10-1108,0,0.212701,"coverage knowledge graphs with empirical evidence from text for the explicit goal of building full-fledged taxonomies consisting of a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms"
E17-1056,P09-1113,0,0.091549,"d Ponzetto, 2012; Faralli et al., 2016, inter alia) or by relying on ground truth information from the Linguistic Linked Open Data cloud (Chiarcos et al., 2012). Knowledge Bases (KBs) can be created in many different ways depending on the availability of external resources and specific application needs. Recently, much work in Natural Language Processing focused on Knowledge Base Completion (Nickel et al., 2016a, KBC), the task of enriching and refining existing KBs. Many different methods have been explored for KBC, including exploitation of resources such as text corpora (Snow et al., 2006; Mintz et al., 2009; Aprosio et al., 2013) or other KBs (Wang et al., 2012; Bryl and Bizer, 2014) for acquiring additional knowledge. Alternative approaches, in contrast, primarily rely on existing information from the KB itself (Socher et al., 2013; Nickel et al., 2016b) used as ground-truth to simultaneously learn continuous representations of KB concepts and relations, which are used to infer additional KB relations. Finally, Open Information Extraction methods looked at ways to extract large amounts of facts from Web-scale corpora in order to acquire open-domain KBs (Etzioni et al., 2011; Faruqui and Kumar,"
E17-1056,velardi-etal-2012-new,1,0.878279,"Missing"
E17-1056,J13-3007,1,0.916583,"a clean and fully-connected directed acyclic graph (DAG). This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain-specific knowledge with dozens of scientific, industrial and social applications (Glass and Vessey, 1995). In taxonomy induction, the required domain knowledge can be acquired with many different methods for hypernym extraction, ranging from simple lexical patterns (Hearst, 1992; Oakes, 2005; Kozareva and Hovy, 2010) to statistical and machine learning techniques (Caraballo, 1999; Agirre et al., 2000; Ritter et al., 2009; Velardi et al., 2013). Recent efforts, such as Microsoft’s Probase (Wu et al., 2012) or the WebIsaDB (Seitner et al., 2016) similarly focus on ‘local’ extraction of single hypernym relations, and do not address the problem of how to combine these single relations into a coherent taxonomy. When taxonomies are automatically acquired, their cleaning (also called “pruning”) becomes a mandatory step (Velardi et al., 2013). In this paper, we present ContrastMedium, an algorithm that transforms noisy semantic networks into full-fledged, clean taxonomies. ContrastMedium is able to identify the embedded taxonomy structure"
E17-1056,D11-1142,0,\N,Missing
E17-2087,W11-2501,0,0.385952,"; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 2011), a corpus-wide classification of relations would need to classify all possible word pairs, which is computationally expensive for large vocabularies. Besides, Levy et al. (2015) discovered a tendency to lexical memorization of such approaches hampering the generalization. Methods based on projection learning take one hyponym word vector as an input and output a word vector in a topological vicinity of hypernym word vectors. Scaling this to the vocabulary, there is only one such operation per word. Mikolov et al. (2013a) used projection learning for bilingual word translation. Vuli´c and Korho"
E17-2087,S12-1012,0,0.0418583,"on the model of Fu et al. (2014), our regularizers can be straightforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS ("
E17-2087,P14-1113,0,0.405509,"ry Ustalov† , Nikolay Arefyev§ , Chris Biemann‡ , and Alexander Panchenko‡ † Ural Federal University, Institute of Natural Sciences and Mathematics, Russia Moscow State University, Faculty of Computational Mathematics and Cybernetics, Russia ‡ University of Hamburg, Deptartment of Informatics, Language Technology Group, Germany dmitry.ustalov@urfu.ru, narefjev@cs.msu.ru {biemann,panchenko}@informatik.uni-hamburg.de § Abstract The contribution of this paper is a novel approach for hypernymy extraction based on projection learning. Namely, we present an improved version of the model proposed by Fu et al. (2014), which makes use of both positive and negative training instances enforcing the asymmetry of the projection. The proposed model is generic and could be straightforwardly used in other relation extraction tasks where both positive and negative training samples are available. Finally, we are the first to successfully apply projection learning for hypernymy extraction in a morphologically rich language. An implementation of our approach and the pre-trained models are available online.1 We present a new approach to extraction of hypernyms based on projection learning and word embeddings. In contr"
E17-2087,N15-1098,1,0.924597,"l language processing tasks ranging from construction of taxonomies (Snow et al., 2006; Panchenko et al., 2016a) to query expansion (Gong et al., 2005) and question answering (Zhou et al., 2013). Automatic extraction of hypernyms from text has been an active area of research since manually constructed high-quality resources featuring hypernyms, such as WordNet (Miller, 1995), are not available for many domain-language pairs. The drawback of pattern-based approaches to hypernymy extraction (Hearst, 1992) is their sparsity. Approaches that rely on the classification of pairs of word embeddings (Levy et al., 2015) aim to tackle this shortcoming, but they require candidate hyponym-hypernym pairs. We explore a hypernymy extraction approach that requires no candidate pairs. Instead, the method performs prediction of a hypernym embedding on the basis of a hyponym embedding. 1 http://github.com/nlpub/projlearn 543 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 543–550, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics experiments. Nayak (2015) performed evaluations of distributional h"
E17-2087,goldhahn-etal-2012-building,0,0.0174841,"distinct vocabulary to avoid the lexical overfitting. This results in 25 067 training, 8 192 validation, and 8 310 test examples. The validation and test sets contain hypernyms from Wiktionary, while the training set is composed of hypernyms and synonyms coming from both sources. 4.3 4 Experiment 2: The English Language We performed the evaluation on two datasets. EVALution Dataset. In this evaluation, word embeddings were trained on a 6.3 billion token text collection composed of Wikipedia, ukWaC (Ferraresi et al., 2008), Gigaword (Graff, 2003), and news corpora from the Leipzig Collection (Goldhahn et al., 2012). We used the skipgram model with the context window size of 8 tokens and 300-dimensional vectors. We use the EVALution dataset (Santus et al., 2015) for training and testing the model, composed of 1 449 hypernyms and 520 synonyms, where hypernyms are split into 944 training, 65 validation and 440 test pairs. Similarly to the first experiment, we extracted extra training hypernyms using the Hearst patterns, but in contrast to Russian, they did not improve the results significantly, so we left them out for English. A reason for such difference could be the more complex morphological system of R"
E17-2087,C92-2082,0,0.235916,"it is natural to use both positive and negative training examples in supervised relation extraction, the impact of negative examples on hypernym prediction was not studied so far. In this paper, we show that explicit negative examples used for regularization of the model significantly improve performance compared to the stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b),"
E17-2087,P10-1134,0,0.127561,"stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction of taxonomies (Snow et al., 2006;"
E17-2087,heylen-etal-2008-modelling,0,0.110215,"e-projection of the hyponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemented in the TensorFlow f"
E17-2087,S15-1021,0,0.0385093,"neighbor regularization requires explicit negative examples, while asymmetric regularization does not. to English. Therefore, extra training samples are needed for Russian (embeddings of Russian were trained on a non-lemmatized corpus). Combined Dataset. To show the robustness of our approach across configurations, this dataset has more training instances, different embeddings, and both synonyms and co-hyponyms as negative samples. We used hypernyms, synonyms and cohyponyms from the four commonly used datasets: EVALution, BLESS (Baroni and Lenci, 2011), ROOT09 (Santus et al., 2016) and K&H+N (Necsulescu et al., 2015).The obtained 14 528 relations were split into 9 959 training, 1 631 validation and 1 625 test hypernyms; 1 313 synonyms and cohyponyms were used as negative samples. We used the standard 300-dimensional embeddings trained on the 100 billion tokens Google News corpus (Mikolov et al., 2013b). 5 Conclusion In this study, we presented a new model for extraction of hypernymy relations based on the projection of distributional word vectors. The model incorporates information about explicit negative training instances represented by relations of other types, such as synonyms and co-hyponyms, and enf"
E17-2087,P16-1226,0,0.287855,"Missing"
E17-2087,S16-1206,1,0.871651,"Missing"
E17-2087,P06-1101,0,0.0605713,"and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction of taxonomies (Snow et al., 2006; Panchenko et al., 2016a) to query expansion (Gong et al., 2005) and question answering (Zhou et al., 2013). Automatic extraction of hypernyms from text has been an active area of research since manually constructed high-quality resources featuring hypernyms, such as WordNet (Miller, 1995), are not available for many domain-language pairs. The drawback of pattern-based approaches to hypernymy extraction (Hearst, 1992) is their sparsity. Approaches that rely on the classification of pairs of word embeddings (Levy et al., 2015) aim to tackle this shortcoming, but they require candidate hyponym-"
E17-2087,W09-1122,0,0.0140739,"performance compared to the stateof-the-art approach of Fu et al. (2014) on three datasets from different languages. 1 2 Related Work Path-based methods for hypernymy extraction rely on sentences where both hyponym and hypernym co-occur in characteristic contexts, e.g., “such cars as Mercedes and Audi”. Hearst (1992) proposed to use hand-crafted lexical-syntactic patterns to extract hypernyms from such contexts. Snow et al. (2004) introduced a method for learning patterns automatically based on a set of seed hyponym-hypernym pairs. Further examples of path-based approaches include (Tjong Kim Sang and Hofmann, 2009) and (Navigli and Velardi, 2010). The inherent limitation of the path-based methods leading to sparsity issues is that hyponym and hypernym have to co-occur in the same sentence. Methods based on distributional vectors, such as those generated using the word2vec toolkit (Mikolov et al., 2013b), aim to overcome this sparsity issue as they require no hyponymhypernym co-occurrence in a sentence. Such methods take representations of individual words as an input to predict relations between them. Introduction Hypernyms are useful in many natural language processing tasks ranging from construction o"
E17-2087,P16-1024,0,0.0636052,"Missing"
E17-2087,W11-2502,1,0.771569,"yponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemented in the TensorFlow framework (Abadi et"
E17-2087,P16-1158,0,0.141805,"htforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 2011), a corpus-wide classification of relati"
E17-2087,C14-1097,0,0.324294,"Missing"
E17-2087,2005.jeptalnrecital-recital.1,0,0.178527,"ve, that rely on re-projection of the hyponym vector (xΦΦ), we also tested two regularizers without re-projection, denoted as xΦ. The neighbor regularizer in this variation is defined as follows: X 1 R= (xΦ · z)2 . |N | (x,z)∈N Linguistic Constraints via Regularization In our case, this regularizer penalizes relatedness of the predicted hypernym xΦ to the synonym z. The asymmetric regularizer without re-projection is defined in a similar way. The nearest neighbors generated using distributional word vectors tend to contain a mixture of synonyms, hypernyms, co-hyponyms and other related words (Wandmacher, 2005; Heylen et al., 2008; Panchenko, 2011). In order to explicitly provide examples of undesired relations to the model, we propose two improved versions of the baseline model: asymmetric regularization that uses inverted relations as negative examples, and neighbor regularization that uses relations of other types as negative examples. For that, we add a regularization term to the loss function: 1 X Φ∗ = arg min kxΦ − yk2 + λR, Φ |P| 3.3 Training of the Models To learn parameters of the considered models we used the Adam method (Kingma and Ba, 2014) with the default meta-parameters as implemente"
E17-2087,W15-4208,0,0.0426051,"est sets contain hypernyms from Wiktionary, while the training set is composed of hypernyms and synonyms coming from both sources. 4.3 4 Experiment 2: The English Language We performed the evaluation on two datasets. EVALution Dataset. In this evaluation, word embeddings were trained on a 6.3 billion token text collection composed of Wikipedia, ukWaC (Ferraresi et al., 2008), Gigaword (Graff, 2003), and news corpora from the Leipzig Collection (Goldhahn et al., 2012). We used the skipgram model with the context window size of 8 tokens and 300-dimensional vectors. We use the EVALution dataset (Santus et al., 2015) for training and testing the model, composed of 1 449 hypernyms and 520 synonyms, where hypernyms are split into 944 training, 65 validation and 440 test pairs. Similarly to the first experiment, we extracted extra training hypernyms using the Hearst patterns, but in contrast to Russian, they did not improve the results significantly, so we left them out for English. A reason for such difference could be the more complex morphological system of Russian, where each word has more morphological variants compared Discussion of Results. Figure 1 (left) shows performance of the three projection lea"
E17-2087,C14-1212,0,0.312339,"(2014), our regularizers can be straightforwardly integrated into the model of Yamane et al. (2016). Two branches of methods relying on distributional representations emerged so far. Methods based on word pair classification take an ordered pair of word embeddings (a candidate hyponym-hypernym pair) as an input and output a binary label indicating a presence of the hypernymy relation between the words. Typically, a binary classifier is trained on concatenation or subtraction of the input embeddings, cf. (Roller et al., 2014). Further examples of such methods include (Lenci and Benotto, 2012; Weeds et al., 2014; Levy et al., 2015; Vylomova et al., 2016). HypeNET (Shwartz et al., 2016) is a hybrid approach which is also based on a classifier, but in addition to two word embeddings a third vector is used. It represents path-based syntactic information encoded using an LSTM model (Hochreiter and Schmidhuber, 1997). Their results significantly outperform the ones from previous pathbased work of Snow et al. (2004). An inherent limitation of classification-based approaches is that they require a list of candidate words pairs. While these are given in evaluation datasets such as BLESS (Baroni and Lenci, 20"
E17-2087,L16-1722,0,\N,Missing
E17-2087,C16-1176,0,\N,Missing
I17-2068,P15-2011,1,0.911458,"Missing"
I17-2068,W14-1214,0,0.0544009,"Missing"
I17-2068,P14-2075,0,0.413365,"with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We collect a new CWI d"
I17-2068,C12-1023,0,0.444938,"Missing"
I17-2068,P13-1151,0,0.311838,"r genre and also explore if the native and non-native user groups share the same lexical-semantic simplification needs. 2 Related Work Previous datasets relied on Simple Wikipedia and edit histories as a ‘gold standard’ annotation of CWs, despite the fact that the use of Simple Wikipedia as a ‘gold standard’ for text simplification has been disputed (Amancio and Specia, 2014; Xu et al., 2015). Currently, the largest CWI dataset is the SemEval-2016 (Task 11) dataset (Paetzold and Specia, 2016a). It consists of 9,200 sentences collected from previous datasets (Shardlow, 2013; Horn et al., 2014; Kauchak, 2013). For the creation of the SemEval-2016 CWI dataset, annotators were asked to annotate (only) one word within a given sentence as complex or not. In the training set (200 sentences), each target word was annotated by 20 people, whilst in the test set (9,000 sentences) each target word was annotated by a single annotator from a pool of 400 annotators. The goal of the shared task was to predict the complexity of a word for a non-native speaker based on the annotations of a larger group of nonnative speakers. This introduced strong biases and inconsistencies in the test set, resulting in very low"
I17-2068,E99-1042,0,0.284754,"t LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we"
I17-2068,P11-2117,0,0.10862,"aˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences an"
I17-2068,S16-1151,0,0.0616333,"(F-score) system Features We use four different categories of features. Frequency and length features: Due to the common use of these features in selecting the most simple lexical substitution candidate (Bott et al., ˇ 2012; Glavaˇs and Stajner, 2015), we use three length features: the number of vowels (vow), syllables (syl), and characters (len) and three frequency features: the frequency of the word in Simple Wikipedia (sim), the frequency of the word in the paragraph (of HIT) (wf p), and the frequency of the word in the Google Web 1T 5-Grams (wbt). Syntactic features: Based on the work of Davoodi and Kosseim (2016), the part of speech (POS) tag influences the complexity of the word. We used POS tags (pos) predicted by the Stanford POS tagger (Toutanova et al., 2003). Word embeddings features: Following the work ˇ of Glavaˇs and Stajner (2015), as well as Paetzold and Specia (2016b), we train a word2vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news texts (Graff, 2002). We train 200-dimensional embeddings using skip-gram training and a window size of 5. We use the word2vec representations of CPs as a feature (emb), and also compute the cosine similarities betw"
I17-2068,P15-4015,0,0.0681752,"a certain level of difficulty and output a text in a simplified form without changing its meaning. Most LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally writte"
I17-2068,E09-1027,0,0.1301,"Missing"
I17-2068,W16-4912,0,0.127728,"lification (LS) of texts have been proposed. LS systems take as input a text of a certain level of difficulty and output a text in a simplified form without changing its meaning. Most LS systems have the functionality of replacing potentially complex words with synonyms or related words that are easier to understand and yet still fit into context. Some of these systems treat all content words in a text as potentially difficult words, e.g. (Horn et al., ˇ 2014; Glavaˇs and Stajner, 2015). Other systems try to detect complex words first and then perform the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We"
I17-2068,W13-2908,0,0.423947,"the replacement with simpler words, e.g. (Paetzold and Specia, 2016b), which seems to significantly improve the results (Paetzold and Specia, 2015). Most LS systems focus on simplifying news articles (Alu´ısio et al., 2008; Carroll et al., 1999; ˇ Saggion et al., 2015; Glavaˇs and Stajner, 2015). However, only small amounts of newswire texts are available that contain annotations for manual simplifications. Most LS systems rely on sentence alignments between English Wikipedia and English Simple Wikipedia (Coster and Kauchak, 2011). Thus, existing CWI datasets cover mostly the Wikipedia Genre (Shardlow, 2013; Horn et al., 2014; Paetzold and Specia, 2016a). We collect a new CWI dataset (CWIG3G2) covering three genres: professionally written news articles, amateurishly written news articles (WikiNews), and Wikipedia articles. Then, we test whether or not the complex word (CW) annotations collected on one genre can be used for Complex word identification (CWI) is an important task in text accessibility. However, due to the scarcity of CWI datasets, previous studies have only addressed this problem on Wikipedia sentences and have solely taken into account the needs of non-native English speakers. We"
I17-2068,N03-1033,0,0.00671505,"the most simple lexical substitution candidate (Bott et al., ˇ 2012; Glavaˇs and Stajner, 2015), we use three length features: the number of vowels (vow), syllables (syl), and characters (len) and three frequency features: the frequency of the word in Simple Wikipedia (sim), the frequency of the word in the paragraph (of HIT) (wf p), and the frequency of the word in the Google Web 1T 5-Grams (wbt). Syntactic features: Based on the work of Davoodi and Kosseim (2016), the part of speech (POS) tag influences the complexity of the word. We used POS tags (pos) predicted by the Stanford POS tagger (Toutanova et al., 2003). Word embeddings features: Following the work ˇ of Glavaˇs and Stajner (2015), as well as Paetzold and Specia (2016b), we train a word2vec model (Mikolov et al., 2013) using English Wikipedia and the AQUAINT corpus of English news texts (Graff, 2002). We train 200-dimensional embeddings using skip-gram training and a window size of 5. We use the word2vec representations of CPs as a feature (emb), and also compute the cosine similarities between the vector representations of CP and the paragraph (cosP ) and the sentence which contains it (cosS). The paragraph and sentence representations are c"
I17-2068,S16-1146,0,0.110144,"Missing"
I17-2068,Q15-1021,0,0.0339024,"Missing"
I17-2068,yimam-etal-2017-multilingual,1,0.489571,"Missing"
J12-1007,W02-1021,0,\N,Missing
J15-2006,W08-2222,0,0.0376988,"tch, team B scored at least three goals. To enable these and other kinds of inferences in machines, these need to be equipped with a domain ontology that formalizes domain knowledge and domain-specific reasoning, as well as with a mechanism to construct formal representations of natural language text. The key idea of this book is to place the ontology at the center of such an interpretation process: The domain and all its relevant semantic distinctions are defined in the ontology, thus the natural language semantic parser must only be aware of these. As opposed to generic tools such as Boxer (Bos 2008) that turn natural language into formal representations, driving the formalization of NL directly by its target ontology ensures that it can be directly consumed by further layers of interpretation, such as reasoners. The remainder of the first chapter gives a very short summary of the state of affairs in semantic interpretation and semantic parsing in NLP. Although this summary is sufficient to highlight what is missing for drawing inferences on top of natural language statements, it is—with a mere two pages— necessarily incomplete and omits even mainstream approaches such as frame semantic p"
J15-2006,W09-3726,0,0.0174519,"id contexts in terms of constituents) it anchors, whereas the corresponding ontology grammar links each ontological concept to the projections of all lexical items that verbalize this concept. Thus, each concept is associated with an exhaustive enumeration of patterns it is expressed in. On the semantic level, Discourse Representation Theory (DRT; Kamp and Reyle 1993) is chosen as a formalism for semantic operations such as coreference and quantification. DRT and LTAG are subsequently paired in a representation called DUDES (Dependency-based Underspecified Discourse Representation structures; Cimiano 2009). Here, the DRTinspired representations are again linked to ontology concepts. Alhough the choice of framework seems largely rooted in previous works of the first author, it is clearly stated that the connection to the ontology could also be realized for other syntactic (e.g., LFG, HPSG) and semantic (e.g., GLUE, MRS) frameworks. Chapter 4 deals with the representation of the ontological lexicons, which specify the interpretation of lexical items with respect to the target ontology. The declarative LEMON lexicon model for ontologies, following closely the approach to the lexicon of Ontological"
J18-3005,abeille-barrier-2004-enriching,0,0.11227,"Missing"
J18-3005,W11-0815,0,0.0805771,"Missing"
J18-3005,P08-2064,0,0.069191,"Missing"
J18-3005,biemann-etal-2008-asv,1,0.74834,"s. Corpus-driven approaches are usually informed using frequency lists (Koehn and Knight 2003), probabilistic models (Schiller 2005), parallel corpora (Koehn and Knight 2003; Macherey et al. 2011), or periphrases (i.e., 518 Riedl and Biemann Using Semantics for Granularities of Tokenization reformulations) in large monolingual corpora (Holz and Biemann 2008). As with other NLP tasks, supervised approaches are usually superior to unsupervised approaches if sufficient training material is available. A straightforward yet effective supervised decompounding system is contained in the ASV Toolbox (Biemann et al. 2008), which uses trie-based (Morrison 1968; Witschel and Biemann 2005) datastructures for recursively splitting compounds based on training set splits. Alfonseca, Bilac, and Pharies (2008) combine several signals, including web anchor text, in an SVM-based supervised splitter. More recently, Shapiro (2016) proposes another supervised method that trains a morphology component on compounds and uses a language model and handcrafted constraints in order to split compounds. The method is evaluated on a Finnish data set. A widely used German decompounder is JWS, which is based on word lists of compound"
J18-3005,bouamor-etal-2012-identifying,0,0.0711994,"Missing"
J18-3005,W15-5703,0,0.104692,"wrong splits correct split correct split + missing splits F1 = 2 · (12) precision · recall precision + recall 15 We set  = 0.01. In the range of  = [0.0001, 1] we observe marginally higher scores using smaller values. 16 Although our method mostly does not assume language knowledge, we uppercase the first letter of each wi , when we apply our method on German nouns. 17 Available at: http://webcorpora.org/. 18 The sentences are extracted from: http://lib.rus.ec. 509 Computational Linguistics Volume 44, Number 3 As unsupervised baselines we use the semantic analogy-based splitter (SAS) from (Daiber et al. 2015)19 and the split ranking by Koehn and Knight (2003), called KK. 4.3 Data Sets For the intrinsic evaluation, we chose data sets of various languages. We use one small German data set for tuning the parameters of the methods. This data set consists of 700 manually labeled German nouns from different frequency bands created by Holz and Biemann (2008). For the evaluation, we consider two larger German data sets. The first data set comprises 158,653 nouns from the German newspaper magazine c’t and was created by Marek (2006).20 As second data set we use a noun compound data set of 54,571 nouns from"
J18-3005,C94-1084,0,0.631027,"Missing"
J18-3005,R11-1058,0,0.030606,"4.3 Data Sets For the intrinsic evaluation, we chose data sets of various languages. We use one small German data set for tuning the parameters of the methods. This data set consists of 700 manually labeled German nouns from different frequency bands created by Holz and Biemann (2008). For the evaluation, we consider two larger German data sets. The first data set comprises 158,653 nouns from the German newspaper magazine c’t and was created by Marek (2006).20 As second data set we use a noun compound data set of 54,571 nouns from GermaNet,21 which has been constructed by Henrich and Hinrichs (2011).22 While converting these data sets for the task of compound splitting, we do not separate words in the gold standard, which is made up of prepositions (e.g., the word Abgang [outflow] is not split into Ab-gang [off walk]). In addition, we apply our method to a Dutch data set of 21,997 compound nouns and an Afrikaans data set that consists of 77,651 compound nouns. Both data sets have been proposed by van Zaanen et al. (2014). Furthermore, we perform an evaluation on a recent Finnish data set proposed by Shapiro et al. (2017) that comprises 20,001 words. In contrast to the other data set it d"
J18-3005,P14-1006,0,0.0793837,"Missing"
J18-3005,P90-1034,0,0.670984,"e conclusion in Section 7 and give an overview of future work in Section 8. 2. Using Distributional Semantics for Fine- and Coarse-Grained Tokenization Both methods described in this article have in common that they rely on distributional semantics, which is based on the distributional hypothesis that was conceived by Harris (1951). This hypothesis states that words that occur in a similar context tend to have similar meaning. Many methods have implemented that assumption in order to compute word similarities using various contexts (e.g., neighboring words, words with syntactic dependencies) (Hindle 1990; Grefenstette 1994; Lin 1998). Usually, words are not only similar to synonyms but also to hypernyms, antonyms, or related terms. For the task of splitting words, the similarity to hypernyms is interesting, as compounds are often similar to more general terms, which are stems of the compound. For example, the word Hefeweizenbier [yeast wheat beer] is most similar to the term Bier [beer] or Weizenbier [wheat beer], which are words that are nested in the more specific word. 485 Computational Linguistics Volume 44, Number 3 Such information is beneficial when it comes to the task of splitting co"
J18-3005,W06-1203,0,0.0580652,"learning a classifier that predicts the multiwordness was first proposed by Pecina (2010), who, however, restricts his experiments to two-word MWEs for the Czech language only. Korkontzelos (2010) comparatively evaluates several MWE ranking measures. The best MWE extractor reported in his work is the scorer by Nakagawa and Mori (2002, 2003), who use the un-nested frequency (called marginal frequency) of each candidate and multiply these by the geometric mean of the distinct neighbor of each word within the candidate. Distributional semantics is mostly used to detect compositionality of MWEs (Katz and Giesbrecht 2006; Salehi, Cook, and Baldwin 2014). For this, most approaches compare the context vector of a MWE with the combined vectors based on the constituent words of the MWE. Then, the similarity between the vectors is used as the degree of compositionality. In machine translation, words are sometimes considered as multiwords if they can be translated as single term (cf. Bouamor, Semmar, and Zweigenbaum 2012; Anastasiou 2010). Although this follows the same intuition as our uniqueness measure described in Section 3.2.1, we do not require any bilingual corpora, but rather test if a multiword can likely"
J18-3005,E03-1076,0,0.176689,"t a method for splitting close compounds. Examples for such close compounds include, for example, dishcloth (English), pancake (English), Hefeweizen (German for wheat beer), bijenzwerm (Dutch for swarm of bees) or hiilikuitu (Finnish for carbon fibre). Similar to MWEs, compounds are created by combining existing words, although in close compounds the stems are not separated by white space. Detecting the single stems, called decompounding, showed impact in several natural language processing (NLP) applications like automatic speech recognitions (Adda-Decker and Adda 2000), machine translation (Koehn and Knight 2003), or information retrieval (IR) (Monz and de Rijke 2001) and is perceived as a crucial component for the processing of languages that are productive with respect to this phenomenon. For both the detection of MWEs and the decompounding of words, most existing approaches rely either on supervised methods or use language-dependent part-ofspeech (POS) information. In this work, we present two knowledge-free and unsupervised (and therefore language-independent) methods that rely on information gained by distributional semantic models that are computed using large unannotated corpora, namely, word2v"
J18-3005,P14-2050,0,0.271174,"re. The CBOW model is learned during the task of predicting a word by its context words. For this, the input layer is defined by the contexts of a word. As output layer we use the center word. The prediction is performed using a single hidden layer that represents the semantic model with the specified dimensions. For the computation of word2vec models, we use 500 dimensions, 5 negative samples, and a word window of 5. Because the implementation by Mikolov et al. (2013)1 does not support the computation of similarities between all n-grams within a corpus, we use the word2vecf implementation by Levy and Goldberg (2014).2 This implementation allows specifying terms and contexts directly and features the functionality to retrieve the most relevant contexts for a word. In order to extract a DT from models computed with word2vec and word2vecf, we compute the cosine similarity between all terms and extract, for each term, the 200 most similar terms. As opposed to the mainstream of using dense vector representations, the approach by Biemann and Riedl (2013), called JoBimText, uses a sparse count-based context representation that nevertheless scales to arbitrary amounts of data (Riedl and Biemann 2013). Furthermor"
J18-3005,P97-1009,0,0.0942942,"imilar to food-related terms like hamburger or sandwich. As shown in the remainder of this article, the information of distributional semantics is beneficial for the tasks of identifying of MWEs but also for the task of compound splitting. In this work, we compute semantic similarities using the dense vector-based CBOW model from word2vec (Mikolov et al. 2013) and a symbolic graph-based approach called JoBimText (Biemann and Riedl 2013). In order to use both models within the word splitting and the word merging task, we transform them to a so-called distributional thesaurus (DT) as defined by Lin (1997). A DT can be considered as a dictionary where for each word the top n most similar words are listed, ordered by their similarity score. The CBOW model is learned during the task of predicting a word by its context words. For this, the input layer is defined by the contexts of a word. As output layer we use the center word. The prediction is performed using a single hidden layer that represents the semantic model with the specified dimensions. For the computation of word2vec models, we use 500 dimensions, 5 negative samples, and a word window of 5. Because the implementation by Mikolov et al."
J18-3005,P98-2127,0,0.434446,"ve an overview of future work in Section 8. 2. Using Distributional Semantics for Fine- and Coarse-Grained Tokenization Both methods described in this article have in common that they rely on distributional semantics, which is based on the distributional hypothesis that was conceived by Harris (1951). This hypothesis states that words that occur in a similar context tend to have similar meaning. Many methods have implemented that assumption in order to compute word similarities using various contexts (e.g., neighboring words, words with syntactic dependencies) (Hindle 1990; Grefenstette 1994; Lin 1998). Usually, words are not only similar to synonyms but also to hypernyms, antonyms, or related terms. For the task of splitting words, the similarity to hypernyms is interesting, as compounds are often similar to more general terms, which are stems of the compound. For example, the word Hefeweizenbier [yeast wheat beer] is most similar to the term Bier [beer] or Weizenbier [wheat beer], which are words that are nested in the more specific word. 485 Computational Linguistics Volume 44, Number 3 Such information is beneficial when it comes to the task of splitting compounds, as we shall see subse"
J18-3005,P11-1140,0,0.173253,"k (R@k) are applied (e.g., Frantzi, Ananiadou, and Tsujii 1998; Evert 2005; Lossio-Ventura et al. 2014). Another general approach is using the AP, which is also used in IR (Thater, Dinu, and Pinkal 2009) and has also been applied by Ramisch, De Araujo, and Villavicencio (2012). 6.2 Related Work on Splitting Words Approaches to automatic decompounding can be classified into corpus-driven approaches and supervised approaches. Corpus-driven approaches are usually informed using frequency lists (Koehn and Knight 2003), probabilistic models (Schiller 2005), parallel corpora (Koehn and Knight 2003; Macherey et al. 2011), or periphrases (i.e., 518 Riedl and Biemann Using Semantics for Granularities of Tokenization reformulations) in large monolingual corpora (Holz and Biemann 2008). As with other NLP tasks, supervised approaches are usually superior to unsupervised approaches if sufficient training material is available. A straightforward yet effective supervised decompounding system is contained in the ASV Toolbox (Biemann et al. 2008), which uses trie-based (Morrison 1968; Witschel and Biemann 2005) datastructures for recursively splitting compounds based on training set splits. Alfonseca, Bilac, and Pharie"
J18-3005,P11-2000,0,0.180917,"Missing"
J18-3005,W09-2501,0,0.0196936,"Missing"
J18-3005,C16-1196,1,0.887088,"Missing"
J18-3005,W02-1407,0,0.33378,"ing and Schutze 1999, page 163) is a statistical test for the significance of co-occurrence of two words. It relies on the probabilities of the term and its single words. The probability of a word p(w) is defined as the frequency of the term divided by the total number of terms of the same length. The t-test statistic is computed using Equation (4) with freq(. ) being the total frequency of all unigrams. Q p(w1 . . . wn ) − ni=1 p(wi ) t(w1 . . . wn ) ≈ p p(w1 . . . wn )/freq(. ) (4) We then use this score to rank the candidate terms. 3.1.5 Marginal Frequency-Based Geometric Mean (FGM) Score. Nakagawa and Mori (2002, 2003) presented another method that is inspired by the C/NC-value and outperformed a modified C-value measure.4 It is composed of two scoring mechanisms for the candidate term t, as shown in Equation (5). FGM(t) = GM(t) · MF(t) (5) The first term in the equation is the geometric mean GM(. ) of the number of distinct direct left l(. ) and right r(. ) neighboring words for each single word ti within t.   1 2|t| X   GM(t) = (|l(ti ) |+ 1)(|r(ti ) |+ 1) (6) ti ∈t These neighboring words are extracted directly from the corpus; the method relies on neither candidate lists nor POS tags. In cont"
J18-3005,W12-3311,0,0.0242488,"(see Figure 5), we also observe that the best results 517 Computational Linguistics Volume 44, Number 3 are obtained using MWE information in combination with compounds and content words. 6. Related Work 6.1 Related Work on Merging Words The generation of MWE dictionaries has drawn much attention in the field of NLP. Early computational approaches (e.g., Justeson and Katz 1995) use POS sequences as MWE extractors. Other approaches, relying on word frequency, statistically verify the hypothesis whether the parts of the MWE occur more often together than would be expected by chance (Evert 2005; Ramisch 2012). One of the first measures that consider context information (co-occurrences) are the C-value and the NC-value, introduced by Frantzi, Ananiadou, and Tsujii (1998). These methods first extract candidates using POS information and then compute scores based on the frequency of the MWE and the frequency of nested MWE candidates. The method described by Wermter and Hahn (2005) is based on the limited modifiability of MWEs. For this, they introduce a measure that combines frequencies of modifications of the candidate, where modifications are considered as occurrences of the candidate where a singl"
J18-3005,W12-3301,0,0.078015,"Missing"
J18-3005,D13-1089,1,0.895467,"Missing"
J18-3005,D15-1290,1,0.801092,"Missing"
J18-3005,N16-1075,1,0.888955,"Missing"
J18-3005,W17-6933,1,0.907728,"t a DT from models computed with word2vec and word2vecf, we compute the cosine similarity between all terms and extract, for each term, the 200 most similar terms. As opposed to the mainstream of using dense vector representations, the approach by Biemann and Riedl (2013), called JoBimText, uses a sparse count-based context representation that nevertheless scales to arbitrary amounts of data (Riedl and Biemann 2013). Furthermore, this approach has achieved competitive results to dense vector space models like CBOW and SKIP-gram (Mikolov et al. 2013) in word similarity evaluations (Riedl 2016; Riedl and Biemann 2017). To keep the preprocessing language independent, we keep only words in a context window for both approaches, as opposed to, for example, dependency-parsing-based contexts. For the task of MWE identification we do not only represent single words but also n-grams using single-word contexts. For the task of decompounding, only unigrams are considered. Based on the frequencies of words/n-grams and contexts, we calculate the lexicographer’s mutual information (LMI) significance score (Evert 2005) between terms and features and remove all context features that co-occur with more than 1,000 terms, a"
J18-3005,W03-1604,0,0.148631,"Missing"
J18-3005,E14-1050,0,0.0498926,"Missing"
J18-3005,D15-1036,0,0.0848795,"Missing"
J18-3005,W01-0513,0,0.273476,"Missing"
J18-3005,seddah-etal-2012-ubiquitous,0,0.0532575,"Missing"
J18-3005,C16-1061,0,0.0966355,", more words get split than using the KK method. Whereas the KK approach identifies most compounds correctly, many compounds are not detected at all. Here, our method performs best using JoBimText. For Dutch, no trained models for JWS and ASV are available. Thus, we did not use these tools but compare to the NL splitter, achieving a competitive precision but lower recall. This is caused by many short split candidates that are not detected due to the ml parameter. However, our method still significantly beats the KK baseline. Furthermore, we show results based on a Finnish data set proposed by Shapiro (2016). Whereas her method performs better in terms of recall in comparison to SECOS, 24 We perform a Wilcoxon signed-rank test between the F1 scores of each candidate assuming p &lt; 0.01. However, we only obtain a p-value below 0.5. 511 Computational Linguistics Volume 44, Number 3 Table 18 Results based on manually created data sets for German, Dutch, Afrikaans, and Finnish. We mark the best results in bold font and use an asterisk (*) to show if a method performs significantly better than the baseline methods and use two asterisks (**) when a single method outperforms all others significantly. Data"
J18-3005,W09-2506,0,0.0627737,"Missing"
J18-3005,C92-4173,0,0.455738,"mmons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 44, Number 3 1. Introduction If we take Ron Kaplan’s motivation for tokenization seriously that the “stream of characters in a natural language text must be broken up into distinct meaningful units” (Kaplan 2005) to enable natural language processing beyond the character level, then tokenization is more than the low-level preprocessing task of treating interpunctuation, hyphenation, and enclitics. Rather, tokenization also should aspire to produce meaningful units, or, as Webster and Kit (1992) define it, tokens should be linguistically significant and methodologically useful. In practice, however, tokenizers are rather not concerned with meaning or significance—placed right at the beginning of any NLP pipeline and usually implemented in a rule-based fashion, they are merely workhorses to enable higher levels of processing, which includes a reasonable split of the input into word tokens and some normalization to cater to the sensitivity of subsequent processing components. Although it is clear that the methodological utility of a specific tokenization depends on the overall task, it"
J18-3005,van-zaanen-etal-2014-development,0,0.0626574,"Missing"
J18-3005,N16-1078,0,0.0468055,"Missing"
J19-3002,2016.gwc-1.10,1,0.89418,"where nodes are words and edges are synonymy relationships. Synsets represent word senses and are building blocks of thesauri and lexical ontologies, such as WordNet (Fellbaum 1998). These resources are crucial for many NLP applications that require common sense reasoning, such as information retrieval (Gong, Cheang, and Hou U 2005), sentiment analysis (Montejo-R´aez et al. 2014), and question answering (Kwok, Etzioni, and Weld 2001; Zhou et al. 2013). For most languages, no manually constructed resource is available that is comparable to the English WordNet in terms of coverage and quality (Braslavski et al. 2016). For instance, Kiselev, Porshnev, and Mukhin (2015) present a comparative analysis of lexical resources available for the Russian language, concluding that there is no resource compared with WordNet in terms of completeness and availability for Russian. This lack of linguistic resources for many languages strongly motivates the development of new methods for automatic construction of WordNet-like resources. In this section, we apply WATSET for unsupervised synset induction from a synonymy graph and compare it with state-of-the-art graph clustering algorithms run on the same task. 441 Computat"
J19-3002,E17-2036,0,0.0177047,"an observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard created by human experts. 6.1.2 Flat Cuts of the WordNet Taxonomy. The second type of semantic classes used in our study are more semantically specific and defined as subtrees of WordNet at some fixed 462 Ustalov et al. WATSET: Local-Global Graph"
J19-3002,N13-1104,0,0.067555,"Missing"
J19-3002,W03-1022,0,0.139544,"tly. Examples of concrete semantic classes include sets of animals (dog, cat, . . . ), vehicles (car, motorcycle, . . . ), and fruit trees (apple tree, peach tree, . . . ). In this experiment, we use a gold standard derived from a reference lexicographical database, namely, WordNet (Fellbaum 1998). 35 The examples are from the file triw2v-watset-n30-top-top-triples.txt is available in the “Downloads” section of our GitHub repository at https://github.com/uhh-lt/triframes. 461 Computational Linguistics Volume 45, Number 3 Figure 14 A summary of the noun semantic classes in WordNet supersenses (Ciaramita and Johnson 2003). This allows us to benchmark the ability of WATSET to reconstruct the semantic lexicon of such a reliable reference resource that has been widely used in NLP for many decades. 6.1.1 WordNet Supersenses. The first data set used in our experiments consists of 26 broad semantic classes, also known as supersenses in the literature (Ciaramita and Johnson 2003): person, communication, artifact, act, group, food, cognition, possession, location, substance, state, time, attribute, object, process, tops, phenomenon, event, quantity, motive, animal, body, feeling, shape, plant, and relation. This syste"
J19-3002,N16-1172,0,0.0539531,"Missing"
J19-3002,E17-2028,0,0.0465007,"Missing"
J19-3002,J14-1002,0,0.075833,"Missing"
J19-3002,de-marneffe-etal-2006-generating,0,0.0378146,"Missing"
J19-3002,J13-3008,0,0.072588,"Missing"
J19-3002,E03-1020,0,0.152312,"of networks of linguistic data, such as word co-occurrence graphs, especially those that were used for induction of linguistic structures such as word senses. Markov Clustering (MCL; van Dongen 2000) is a hard clustering algorithm, that is, a method that partitions nodes of the graph in a set of disjoint clusters. This method is based on simulation of stochastic flow in graphs. MCL simulates random walks within a graph by the alternation of two operators, called expansion and inflation, which recompute the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows 2003). Chinese Whispers (CW; Biemann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logari"
J19-3002,P18-1128,0,0.152993,"443 Computational Linguistics Volume 45, Number 3 Table 7 Statistics of the gold standard data sets used in our experiments. Resource Language # words # synsets # pairs WordNet BabelNet English 148,730 11,710,137 117,659 6,667,855 152,254 28,822,400 RuWordNet YARN Russian 110,242 9,141 49,492 2,210 278,381 48,291 toolkit (Seabold and Perktold 2010).19 Since the hypothesis tested by the McNemar’s test is whether the results from both algorithms are similar against the alternative that they are not, we use the p-value of this test to assess the significance of the difference between F1 -scores (Dror et al. 2018). We consider the performance of one algorithm to be higher than the performance of another if its F1 -score is larger and the corresponding p-value is smaller than a significance level of 0.01. Gold Standards. We conduct our evaluation on four lexical semantic resources for two different natural languages. Statistics of the gold standard data sets are present in Table 7. We report the number of lexical units (# words), synsets (# synsets), and the generated synonymy pairs (# pairs). We use WordNet,20 a popular English lexical database constructed by expert lexicographers (Fellbaum 1998). Word"
J19-3002,N15-1184,0,0.0489558,"eights based on the cosine of the angle between Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -"
J19-3002,P16-1191,0,0.120597,"Missing"
J19-3002,J02-3001,0,0.346615,"pairs over a certain threshold are added to output synsets. We compare this approach to five other state-of-the-art graph clustering algorithms described in Section 2.1 as the baselines. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking do"
J19-3002,goldhahn-etal-2012-building,0,0.0267265,"Missing"
J19-3002,E12-1059,0,0.186249,"Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavlick et al. 2015). Tian et al. (2014) introduced a"
J19-3002,Q16-1015,0,0.0324022,"Missing"
J19-3002,C92-2082,0,0.235054,"Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approa"
J19-3002,S13-2113,0,0.0987206,"mann 2006, 2012) is a hard clustering algorithm for weighted graphs, which can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority of labels among the neighboring nodes. The algorithm has a hyperparameter that controls graph weights, which can be set to three values: (1) CWtop sums 425 Computational Linguistics Volume 45, Number 3 over the neighborhood’s classes; (2) CWlin downgrades the influence of a neighboring node by its degree; or (3) CWlog by the logarithm of its degree. MaxMax (Hope and Keller 2013a) is a fuzzy clustering algorithm particularly designed for the word sense induction task. In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity. The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: All transitive children of this root form a cluster and the roots are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster. Clique Per"
J19-3002,P11-1147,0,0.0258674,"Missing"
J19-3002,P12-1092,0,0.0191533,"arse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual wor"
J19-3002,S17-1025,0,0.346463,"Missing"
J19-3002,S13-2049,0,0.285653,"p between the instance’s context words and the words of the sense cluster. V´eronis (2004) compiles a corpus with contexts of polysemous nouns using a search engine. 427 Computational Linguistics Volume 45, Number 3 A word graph is built by drawing edges between co-occurring words in the gathered corpus, where edges below a certain similarity threshold were discarded. His HyperLex algorithm detects hubs of this graph, which are interpreted as word senses. Disambiguation in this experiment is performed by computing the distance between context words and hubs in this graph. Di Marco and Navigli (2013) present a comprehensive study of several graph-based WSI methods, including CW, HyperLex, and curvature clustering (Dorow et al. 2005). Additionally, the authors propose two novel algorithms: Balanced Maximum Spanning Tree Clustering and Squares (B-MST), and Triangles and Diamonds (SquaT++). To construct graphs, authors use first-order and second-order relationships extracted from a background corpus as well as keywords from snippets. This research goes beyond intrinsic evaluations of induced senses and measures the impact of the WSI in the context of an information retrieval via clustering a"
J19-3002,S18-2016,0,0.036049,"Missing"
J19-3002,P14-1097,0,0.0305612,"Missing"
J19-3002,P03-1009,0,0.239319,"Missing"
J19-3002,Q13-1015,0,0.175147,"s (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci , Cj ∈ C, whereas fuzzy clustering algorithms permit cluster overlapping, tha"
J19-3002,D13-1064,0,0.0354668,"Missing"
J19-3002,D15-1200,0,0.369011,"st dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense"
J19-3002,S10-1011,0,0.187061,"ailable implementations have been used. During the evaluation, we delete clusters equal to or larger than the threshold of 150 words, as they can hardly represent any meaningful synset. Only the clusters produced by the MaxMax algorithm were actually affected by this threshold. Quality Measure. To evaluate the quality of the induced synsets, we transform them into synonymy pairs and computed precision, recall, and F1 -score on the basis of the overlap of these synonymy pairs with the synonymy pairs from the gold standard data sets. The F1 -score calculated this way is known as paired F-score (Manandhar et al. 2010; Hope and Keller 2013a). Let C be the set of obtained synsets and CG be the set of gold synsets. Given a synset containing n &gt; 1 words, we generate n(n2−1) pairs of synonyms, so we transform C into a set of pairs P and CG into a set of gold pairs PG . We then compute the numbers of positive and negative answers as follows: TP = |P ∪ PG | (9) FP = |P  PG | (10) FN = |PG  P| (11) where TP is the number of true positives, FP is the number of false positives, and FN is the number of false negatives. As a result, we use the standard definitions of precision 2·Pr·Re TP as Pr = TPTP +FP , recall a"
J19-3002,N13-1051,0,0.418879,"nsformed into a triframe, which is a triple that is composed of the subjects fs ⊆ V, the verbs fv ⊆ V, and the objects fo ⊆ V. For example, the triples shown in Figure 9 will form a triframe ({man, people, woman}, {make, earn}, {profit, money} ). 5.2 Evaluation Currently, there is no universally accepted approach for evaluating unsupervised frame induction methods. All the previously developed methods were evaluated on completely different incomparable setups and used different input corpora (Titov and 454 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Klementiev 2012; Materna 2013; O’Connor 2013, etc.). We propose a unified methodology by treating the complex multi-stage frame induction task as a straightforward triple clustering task. 5.2.1 Experimental Setup. We compare our method, Triframes WATSET, to several available state-of-the-art baselines applicable to our data set of triples (Section 2.3). LDAFrames by Materna (2012, 2013) is a frame induction method based on topic modeling. Higher-Order Skip-Gram (HOSG) by Cotterell et al. (2017) generalizes the Skip-Gram model (Mikolov et al. 2013) by extending it from word-context co-occurrence matrices to tensors factori"
J19-3002,P09-1045,0,0.0378997,"ed approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semanti"
J19-3002,P07-3015,0,0.0440453,"Algorithm for Fuzzy Graph Clustering In this section, we present WATSET, a meta-algorithm for fuzzy graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clust"
J19-3002,C14-2023,0,0.0313933,"Missing"
J19-3002,W12-1901,0,0.0610861,"Missing"
J19-3002,D14-1113,0,0.136393,"s are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to succes"
J19-3002,P07-1068,0,0.0417874,"ng semantic classes from text, also known as semantic lexicon induction, has also been extensively explored in previous works. This is because inducing semantic classes directly from text has the potential to avoid the limited coverage problems of knowledge bases like Freebase, DBpedia (Bizer et al. 2009), or BabelNet (Navigli and Ponzetto 2012), which rely on Wikipedia (Hovy, Navigli, and Ponzetto 2013), as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization."
J19-3002,R15-1061,0,0.0343153,"Missing"
J19-3002,nivre-etal-2006-maltparser,0,0.0381336,"Missing"
J19-3002,S16-1206,1,0.914841,"ure 14 shows the distribution of the 82,115 noun synsets from WordNet 3.1 across the supersenses. In our experiments in this section, these classes are used as gold standard clustering of word senses as recorded in WordNet. One can observe a Zipfian-like power-law (Zipf 1949) distribution with a few clusters, such as artifact and person, accounting for a large fraction of all nouns in the resource. Overall, in this experiment we decided to focus on nouns, as the input distributional thesauri used in this experiment (as presented in Section 6.2) are most studied for modeling of noun semantics (Panchenko et al. 2016b). The WordNet supersenses were applied later also for word sense disambiguation as a system of broad sense labels (Flekova and Gurevych 2016). For BabelNet, there is a similar data set called BabelDomains (Camacho-Collados and Navigli 2017) produced by automatically labeling BabelNet synsets with 32 different domains based on the topics of Wikipedia featured articles. Despite the larger size, however, BabelDomains provides only a silver standard (being semi-automatically created). We thus opt in the following to use WordNet supersenses only, because they provide instead a gold standard creat"
J19-3002,L18-1286,1,0.835468,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,L18-1244,1,0.90656,"zareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we directly apply the WATSET algorithm to obtain the resulting distributional semantic classes instead of using a sophisticated parametric pipeline that performs a sequence of clustering and pruning steps. Another related strain of research to semantic class induction is d"
J19-3002,P04-1035,0,0.0483492,"graph clustering. Given a graph connecting potentially ambiguous objects (e.g., words), WATSET induces a set of unambiguous overlapping clusters (communities) by disambiguating and grouping the ambiguous objects. WATSET is a meta-algorithm that uses existing hard clustering algorithms for graphs to obtain a fuzzy clustering (e.g., soft clustering). In computational linguistics, graph clustering is used for addressing problems such as word sense induction (Biemann 2006), lexical chain computing (Medelyan 2007), Web search results diversification (Di Marco and Navigli 2013), sentiment analysis (Pang and Lee 2004), and cross-lingual semantic relationship induction (Lewis and Steedman 2013b); more applications can be found in the book by Mihalcea and Radev (2011). Definitions. Let G = (V, E) be an undirected simple graph,5 where V is a set of nodes and E ⊆ V 2 is a set of undirected edges. We denote a subset of nodes Ci ⊆ V as a cluster. S A graph clustering algorithm then is a function C LUSTER : (V, E) → C such that V = Ci ∈C Ci . We distinguish two classes of graph clustering algorithms: hard clustering algorithms (partitionings) produce non-overlapping clusters, that is, Ci ∩ Cj = ∅ ⇐⇒ i 6= j, ∀Ci ,"
J19-3002,N04-1041,0,0.0348718,", as well as to allow for resource induction across domains (Hovy et al. 2011). Information about semantic classes, in turn, has been shown to benefit such high-level NLP tasks as coreference (Ng 2007). Induction of semantic classes as a research direction in the field of NLP starts, to the best of our knowledge, with Lin and Pantel (2001), where sets of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal superv"
J19-3002,P15-2070,0,0.0389199,"Missing"
J19-3002,W97-0322,0,0.281913,"es belonging to the same community and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from th"
J19-3002,W16-1620,1,0.894029,"conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a monolingual word sense induction using only such a paraphrase corpus as Paraphrase Database (Pavli"
J19-3002,D14-1162,0,0.0797303,"Missing"
J19-3002,N18-1202,0,0.0153532,"een Skip-Gram word vectors (Mikolov et al. 2013), we should note that such an approach assigns high values of similarity not just to synonymous words, but to antonymous and generally any lexically related words. This is a common problem with lexical embedding spaces, which we tried to evade by explicitly using a synonymy dictionary as an input. For example, “audio play” and “radio play,” or “accusative” and “oblique,” are semantically related expressions, but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments,"
J19-3002,N10-1013,0,0.0170646,"1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisinger and Mooney (2010) presented a multi-prototype vector space. Sparse tf–idf vectors are clustered, using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are comm"
J19-3002,W17-6933,1,0.844526,"n.01, x-axis.n.01, y-axis.n.01, z-axis.n.01, major axis.n.01, minor axis.n.01, optic axis.n.01, principal axis.n.01, semimajor axis.n.01, semiminor axis.n.01 specifically, the dimensions of the vector space represent salient syntactic dependencies of each word extracted using a dependency parser. For this, we use the JoBimText framework for computation of count-based distributional models from raw text collections (Biemann and Riedl 2013).36 Although similar graphs could be derived also from neural distributional models, such as Word2Vec (Mikolov et al. 2013), it was shown in Riedl (2016) and Riedl and Biemann (2017) that the quality of syntactically-based graphs is generally superior. The JoBimText framework involves several steps. First, it takes an unlabeled input text corpus and performs dependency parsing so as to extract features representing each word. Each word is represented by a bag of syntactic dependencies such as conj and(Ruby, · ) or prep in(code, · ), extracted from the dependencies of MaltParser (Nivre, Hall, and Nilsson 2006), which are further collapsed using the tool by Ruppert et al. (2015) in the notation of Stanford Dependencies (de Marneffe, MacCartney, and Manning 2006). Next, sema"
J19-3002,P10-1044,0,0.102279,"Missing"
J19-3002,N16-1173,0,0.0331879,"Missing"
J19-3002,J98-1004,0,0.710235,"nity and builds a new network whose nodes are the communities. These steps are repeated to maximize modularity of the clustering result. 2.2 Word Sense Induction Word Sense Induction is an unsupervised knowledge-free approach to Word Sense Disambiguation (WSD): It uses neither handcrafted lexical resources nor hand-annotated sense-labeled corpora. Instead, it induces word sense inventories automatically from corpora. Unsupervised WSD methods fall into two main categories: context clustering and word ego network clustering. ¨ Context clustering approaches, such as Pedersen and Bruce (1997) and Schutze (1998), represent an instance usually by a vector that characterizes its context, where the definition of context can vary greatly. These vectors of each instance are then clustered. ¨ Schutze (1998) induced sparse sense vectors by clustering context vectors, using the expectation-maximization algorithm. This approach is fitted with a similarity-based WSD mechanism. Pantel and Lin (2002) used a two-staged Clustering by Committee algorithm. In the first stage, it uses average-link clustering to find small and tight clusters, which are used to iteratively identify committees from these clusters. Reisi"
J19-3002,D07-1002,0,0.0693088,"nes. 2.3 Semantic Frame Induction Frame Semantics was originally introduced by Fillmore (1982) and further developed in the FrameNet project (Baker, Fillmore, and Lowe 1998). FrameNet is a lexical resource composed of a collection of semantic frames, relationships between them, and a corpus of frame occurrences in text. This annotated corpus gave rise to the development of frame parsers using supervised learning (Gildea and Jurafsky 2002; Erk and Pado´ 2006; Das et al. 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata 2007) and Textual Entailment (Burchardt et al. 2009; Ben Aharon, Szpektor, and Dagan 2010). However, frame-semantic resources are arguably expensive and time-consuming to build because of difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with"
J19-3002,M92-1001,0,0.293429,"rdan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group"
J19-3002,D08-1061,0,0.0384586,"of similar words are clustered into concepts. This approach performs a hard clustering and does not label clusters, but these drawbacks are addressed by Pantel and Lin (2002), where words can belong to several clusters, thus representing senses. Pantel and Ravichandran (2004) aggregate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to impr"
J19-3002,W02-1028,0,0.251425,"ate hypernyms per cluster, which come from Hearst (1992) patterns. Pattern-based approaches were further developed using 3 https://tac.nist.gov/2010/Summarization. 429 Computational Linguistics Volume 45, Number 3 graph-based methods using a PageRank-based weighting (Kozareva, Riloff, and Hovy 2008), random walks (Talukdar et al. 2008), or heuristic scoring (Qadir et al. 2015). Other approaches use probabilistic graphical models, such as the ones proposed by Ritter, Mausam, and Etzioni (2010) and Hovy et al. (2011). To ensure the overall quality of extraction pattern with minimal supervision, Thelen and Riloff (2002) explored a bootstrapping approach, later extended by McIntosh and Curran (2009) with bagging and distributional similarity to minimize the semantic drift problem of iterative bootstrapping algorithms. As an alternative to pattern-based methods, Panchenko et al. (2018b) show how to apply semantic classes to improve hypernymy extraction and taxonomy induction. Like in our experiments in Section 6, it uses a distributional thesaurus as input, as well as multiple pre- and post-processing stages to filter the input graph and disambiguate individual nodes. In contrast to Pachenko et al., here we di"
J19-3002,C14-1016,0,0.0229688,"usters. Whereas most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al. 2013; Pennington, Socher, and Manning 2014), there are several approaches that produce word sense embeddings. Multi-prototype extensions of the Skip-Gram model (Mikolov et al. 2013) that use no predefined sense inventory learn one embedding word vector per one word sense and are commonly fitted with a disambiguation mechanism (Huang et al. 2012; Apidianaki and Sagot 2014; 426 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications Neelakantan et al. 2014; Tian et al. 2014; Li and Jurafsky 2015; Bartunov et al. 2016; Cocos and Callison-Burch 2016; Pelevina et al. 2016; Thomason and Mooney 2017). Huang et al. (2012) introduced multiple word prototypes for dense vector representations (embeddings). Their approach is based on a neural network architecture; during training, all contexts of the word are clustered. Apidianaki and Sagot (2014) use an aligned parallel corpus and WordNet for English to perform cross-lingual word sense disambiguation to produce French synsets. However, Cocos and Callison-Burch (2016) showed that it is possible to successfully perform a m"
J19-3002,P11-1145,0,0.0800752,"Missing"
J19-3002,E12-1003,0,0.0220272,"he CPA corpus (Hanks and Pustejovsky 2005). Although Ritter, Mausam, and Etzioni (2010) have applied LDA for inducing structures similar to frames, their study is focused on the extraction of mutually related frame arguments. ProFinder (Cheung, Poon, and Vanderwende 2013) is another generative approach that also models both frames and roles as latent topics. The evaluation was performed on the in-domain information extraction task MUC-4 (Sundheim 1992) and on the text summarization task TAC-2010.3 Modi, Titov, and Klementiev (2012) build on top of an unsupervised semantic role labeling model (Titov and Klementiev 2012). The raw text of sentences from the FrameNet data is used for training. The FrameNet gold annotations are then used to evaluate the labeling of the obtained frames and roles, effectively clustering instances known during induction. Kawahara, Peterson, and Palmer (2014) harvest a huge collection of verbal predicates along with their argument instances and then apply the Chinese Restaurant Process clustering algorithm to group predicates with similar arguments. The approach was evaluated on the verb cluster data set of Korhonen, Krymolowski, and Marx (2003). These and some other related approac"
J19-3002,W09-1127,0,0.152335,"ifficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks. Consequently, such resources exist only for a few languages (Boas 2009) and even English is lacking domainspecific frame-based resources. Possible inroads are cross-lingual semantic annotation 2 http://ontopt.dei.uc.pt. 428 Ustalov et al. WATSET: Local-Global Graph Clustering with Applications transfer (Pado´ and Lapata 2009; Hartmann, Eckle-Kohler, and Gurevych 2016) or linking FrameNet to other lexical-semantic or ontological resources (Narayanan et al. 2003; Tonelli and Pighin 2009; Laparra and Rigau 2010; Gurevych et al. 2012, inter alia). One inroad for overcoming these issues is automatizing the process of FrameNet construction through unsupervised frame induction techniques, as investigated by the systems described next. LDA-Frames (Materna 2012, 2013) is an approach to inducing semantic frames using a latent Dirichlet allocation (LDA) by Blei, Ng, and Jordan (2003) for generating semantic frames and their respective frame-specific semantic roles at the same time. The authors evaluated their approach against the CPA corpus (Hanks and Pustejovsky 2005). Although Ritt"
J19-3002,P17-1145,1,0.797757,"but really not synonyms. Such a problem can be addressed using techniques such as retrofitting (Faruqui et al. 2015) and contextualization (Peters et al. 2018). However, one limitation of all the approaches considered in this section is the dependence on the completeness of the input dictionary of synonyms. In some parts of the input synonymy graph, important bridges between words can be missing, leading to smaller-than-desired synsets. A promising extension of the present methodology is using distributional models to enhance connectivity of the graph by cautiously adding extra relationships (Ustalov et al. 2017). Cross-Resource Evaluation. In order to estimate the upper bound of precision, recall, and F1 -score in our synset induction experiments, we conducted a cross-resource evaluation between the used gold-standard data sets (Table 12). Similarly to the experimental setup described in Section 4.2.1, we transformed synsets from every data set into sets of synonymy pairs. Then, for every pair of gold standard data sets, we computed the pairwise precision, recall, and F1 -score by assessing synset-induced synonymy pairs of one data set on the pairs of another data set. As a result, we see that the lo"
J19-3002,P18-2010,1,0.815639,", Panchenko, and Biemann (2017), including an analysis of its computational complexity and run-time. We also describe a simplified version of WATSET that does not use the context similarity measure for propagating links in the original graph to the appropriate senses in the disambiguated graph. Three subsequent sections present different applications of the algorithm. Section 4 applies WATSET for unsupervised synset induction, referencing results by Ustalov, Panchenko, and Biemann. Section 5 shows frame induction with WATSET on the basis of a triclustering approach, as previously described by Ustalov et al. (2018). Section 6 presents new experiments on semantic class induction with WATSET. Section 7 concludes with the final remarks and pointers for future work. Table 1 shows several examples of linguistic structures on which we conduct experiments described in this article. With the exception of the type of input graph and the hyper-parameters of the WATSET algorithm, the overall pipeline remains similar in every described application. For instance, in Section 4 the input of the clustering algorithm is a graph of ambiguous synonyms and the output is an induced linguistic 1 This article builds upon and"
J19-3002,C02-1114,0,0.164904,"n, and semantic relatedness tasks, but yield no improvement for named entity recognition and sentiment analysis. Thomason and Mooney (2017) performed multi-modal word sense induction by combining both language and vision signals. In this approach, word embeddings are learned from the ImageNet corpus (Deng et al. 2009) and visual features are obtained from a deep neural network. Running a k-means algorithm on the joint feature set produces WordNet-like synsets. Word ego network clustering methods cluster graphs of words semantically related to the ambiguous word (Lin 1998; Pantel and Lin 2002; Widdows and Dorow 2002; Biemann 2006; Hope and Keller 2013a). An ego network consists of a single node (ego), together with the nodes they are connected to (alters), and all the edges among those alters (Everett and Borgatti 2005). In our case, such a network is a local neighborhood of one word. Nodes of the ego network can be (1) words semantically similar to the target word, as in our approach, or (2) context words relevant to the target, as in the UoS system (Hope and Keller 2013b). Graph edges represent semantic relationships between words derived using corpus-based methods (e.g., distributional semantics) or g"
J19-3002,P13-1086,0,0.0803084,"Missing"
J19-3002,zesch-etal-2008-extracting,0,0.076042,"Missing"
J19-3002,erk-pado-2006-shalmaneser,0,\N,Missing
J19-3002,bauer-etal-2012-dependency,0,\N,Missing
J19-3002,N10-1137,0,\N,Missing
J19-3002,W06-3812,1,\N,Missing
J19-3002,P10-2045,0,\N,Missing
J19-3002,P98-1013,0,\N,Missing
J19-3002,C98-1013,0,\N,Missing
J19-3002,P08-1119,0,\N,Missing
J19-3002,D09-1098,0,\N,Missing
L16-1572,baroni-bernardini-2004-bootcat,0,0.43811,"Amazons common crawl (Common Crawl Foundation, 2011) provide unprocessed html data from web crawls, which are further refined e.g. in (Pomikálek et al., 2012; Buck et al., 2014). However, the data is largely collected without notions of topical interest. If an interest in a particular topic exists, corpora have to undergo extensive document filtering with simple and/or complex text classification methods. This leads to a lot of downloaded data being discarded with lots of computational resources being unnecessarily wasted. One approach to work around these issues is to use the BootCat method (Baroni and Bernardini, 2004), which collects, based on keyword lists, web documents by sending combinations of keywords to a search engine provider. Here, one particular disadvantage is the use of a search engine provider as a black box, which makes it dependent on a) the general availability of the service, and b) ranking of the results based on the provider’s (possibly subjective) choice (Kilgarriff, 2007). This paper introduces a simple tool for focused crawling, which makes efficient use of computational resources as it downloads mainly websites of interest, i.e. those belonging to a certain topic. The domain of inte"
L16-1572,biemann-etal-2008-asv,1,0.788774,"is, after downloading 300M tokens, the unfocused crawl’s usable German data amounts to 61M tokens and the focused crawl’s yields 277M tokens, which increases harvest by a factor of over 4.5. 6. Conclusion & Future Work In this work, we presented a straightforward solution for corpus expansion as a focused web crawling approach using N-gram language models and perplexity as means for assessing the relevance of a web page and its outgoing web links. Experiments revealed that the methodology is able to improve model performance by using more domain specific 12 We use JLani from the ASV-toolbox (Biemann et al., 2008) for automatic language identification. 3609 f (no oov) nf (oov) nf (no oov) 1000 1000 900 900 800 800 PP PP f (oov) 700 600 500 500 +10M +30M +100M additional corpus size in #tokens 400 +1M +300M (a) size downloaded 100K size (de) f size (de) nf 300K f (no oov) nf (oov) nf (no oov) 700 600 400 +1M f (oov) +10M +30M +100M additional corpus size in #tokens +300M (b) 1M 3M 10M 30M 100M 300M 87, 105 267, 371 867, 686 2, 661, 758 9, 122, 831 27, 783, 058 93, 799, 389 277, 303, 711 34, 250 95, 167 216, 432 557, 515 2, 305, 689 9, 452, 318 26, 817, 917 61, 122, 895 (c) Figure 2: Perplexity on test s"
L16-1572,buck-etal-2014-n,0,0.0597234,"Missing"
L16-1572,medelyan-etal-2006-language,0,0.0812269,"from the web, http://corporafromtheweb. org/ Focused Crawling The term ‘focused crawling’ (Chakrabarti et al., 1999) also known as ‘topical crawling’ (Menczer et al., 2004) refers to the process of crawling the web in a guided way with a focus on a specific topic. The task is to decide which link to follow and which not, or in which order to follow links before actually downloading the content of their respective destination, all of which happens during crawling time (Chakrabarti et al., 1999). This can either be seen as a classification task (McCallum et al., 1999; Chakrabarti et al., 1999; Medelyan et al., 2006) with binary decisions (yes or no), or as a ranking problem with a priority queue. Our approach differs from existing approaches in that we employ a language model and perplexity as a measure of relevance for a particular web page, whereas other approaches use features such as individual components of the URL itself, e.g. server, path, query strings, etc., the surrounding context of the extracted hyperlink, text of the parent webpage, include lexical resources like WordNet (Fellbaum, 1998), and many more (Safran et al., 2012). One major advantage of the proposed methodology is the deliberate o"
L16-1572,pomikalek-etal-2012-building,0,0.0348449,"Missing"
L16-1572,schafer-bildhauer-2012-building,0,0.0937035,"strates that language models trained on focused crawls obtain better perplexity scores on in-domain corpora. We distribute the focused crawler as open source software. Keywords: web crawling, focused, topical, in-domain, web-corpora, language model, perplexity 1. Introduction 2. With increasing power of computational resources and algorithms to efficiently process more and more data in less time, the demand for larger text collections grows and the web as a huge and dynamic resource is nowadays the main source of any kind of data. The WaCKy1 corpora (Baroni et al., 2009) or the COW 2 project (Schäfer and Bildhauer, 2012) are just some examples for languageprocessing-oriented web corpora. ClueWeb (Callan et al., 2009) or Amazons common crawl (Common Crawl Foundation, 2011) provide unprocessed html data from web crawls, which are further refined e.g. in (Pomikálek et al., 2012; Buck et al., 2014). However, the data is largely collected without notions of topical interest. If an interest in a particular topic exists, corpora have to undergo extensive document filtering with simple and/or complex text classification methods. This leads to a lot of downloaded data being discarded with lots of computational resourc"
L16-1656,biemann-etal-2008-asv,1,0.872484,"Missing"
L16-1656,P06-4018,0,0.0261885,"Missing"
L16-1656,E14-2026,0,0.0255152,"RuTes (Loukashevich, 2011) is an on-going project since 1994 aimed at creating a hierarchical linguistic resource. It was created through an automatic extraction and a subsequent manual correction of terms and relations retrieved from the normative documents of the Russian Federation. It is available under the Attribution-Non-Commercial-ShareAlike 3.0 licence5 . There are further manually or semiautomatic created ontologies for the Russian language, such as RussNet, commercial projects by the enterprises UIS Rossija and Novosoft (Suhonov and Yablonskij, 2004), and Yet Another RussNet (YARN) (Braslavski et al., 2014), however, they are either unavailable or under development. 2.3.2. Automatically or semiautomatically created databases DBpedia (Lehmann et al., 2014), BabelNet (Navigli and Ponzetto, 2012), Freebase (Bollacker et al., 2008), and Yet Another Great Ontology (YAGO) (Suchanek et al., 2007) use rules to extract information from Wikipedia and other sources. The Never Ending Language Learning (NELL) (Zimmermann et al., 2013) shared knowledge base tries to continuously grow by reading in new resources. The seed knowledge base was an ontology and a set of rules. 3. 3.1. Corpus Preprocessing The descr"
L16-1656,C92-2082,0,0.238017,"ruse, 1986; Winston et al., 1987). 2.2. Hearst Patterns Many of the below listed knowledge bases and ontologies make use of patterns to automatically extract semantic relations from continuous text. Based on the previously described assumption of semantic relations involving rulegenerated representation, Hearst was one of the first to create such patterns for the automatic detection of hypernym relations between nouns. The patterns were created by thorough observation of texts and the setting of the contained relations. Attempts to build analogous patterns for holonymy were barren of results (Hearst, 1992). 2.3. Knowledge Bases containing Semantic Relations Knowledge bases containing semantic relations were created in various ways. In the following, both manually created databases such as WordNet and its German and Russian counterparts GermaNet and RuTes, as well as automatically created bases, such as BabelNet and NELL, are presented. Table 1 gives a size comparison of those databases. The sizes were retrieved from the respective webpages. 2.3.1. Manually created knowledge bases The collection of the manually created database WordNet started in 1985 (Miller, 1995; Fellbaum, 1998; Fellbaum, 201"
L16-1656,R11-1058,0,0.0553158,"d literary texts in English, German and Russian. The dataset was manually annotated in a double-annotator setting by students with linguistic background and subsequently curated. In the next step, the relations were extended using properties of the relations, e.g. transitivity of synonymy. The setting of the multilingual and multi-genre corpus enabled us to analyse the universality of the context notion. The analysis was performed by comparing our dataset to the largest manually created or revised knowledge bases in the respective languages, i.e. WordNet (Miller and Fellbaum, 1991), GermaNet (Henrich and Hinrichs, 2011) and RuTes (Loukashevich, 2011). Annotated relations not present in these databases were further analysed and manually classified according to the reason why they were not present in the respective database. This study highlights the significance of contextuality of semantic relations and quantitatively assesses this previously neglected phenomenon in a multi-lingual and multi-genre dataset. This paper is structured as following: Section 2 discusses related work, Section 3 presents the creation and the parameters of the corpus created for this study, Section 4 shows the evaluation and analysis"
L16-1656,R11-2017,0,0.014358,"periority in size. Thus, we further assume that it most representatively shows the gaps in this kind of knowledge base. This is on the one hand a coverage lack of relations due to miscellaneous reasons, but on the other hand due to the negligence of contextual relations, which are relevant to information representation. 4.2. Comparison with Pattern-created Taxonomies As described above, the automatic classification and extraction of semantic relations of words is preferably done by the use of patterns. The first and most popular patterns are that of Hearst (1992), which were later enhanced by Klaussner and Zhekova (2011). The implementation of JoBimText15 (Biemann and Riedl, 2013) of those patterns was applied to the English source texts that were annotated for Sem4158 15 http://www.jobimtext.org RelData. As the Hearst Patterns and their extensions are composed for English hyperonymy, only the hypernym relations of the English subset were considered. Those were not lemmatised as the Hearst Patterns produce both lemmatised and inflected forms of nominals. The pattern extractor selected 112 hypernym relations using the described patterns, whereas the English subset of SemRelData contains 553. Only 8 relations w"
L16-1656,S16-1206,1,0.854388,"Missing"
L18-1093,P98-1013,0,0.595481,"(bs) ∪ Bf (F )) and A.c(w) equals the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the pro"
L18-1093,E17-1056,1,0.906433,"usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and frame semantics builds upon and 1 587 https://framenet.icsi.berkel"
L18-1093,Q14-1019,0,0.0297903,"the number of occurrences of the word w in A. For instance, with respect to the excerpts of Tables 3 and 4 we obtain w(Communication, bn:00085007v) = 15.0. 3. Using the Enriched Representations for Word Frame Disambiguation We evaluate our extensions of Framester profiles following the experimental setting of Gangemi et al. (2016b), and compare the extended and the original profiles in a task of Word Frame Disambiguation (WFD). 3.1. Dataset: FrameNet Full Text Documents To create a silver standard we processed all 108 documents from the FrameNet 1.7 dataset (Baker et al., 1998) with BabelFy (Moro et al., 2014)3 . By combining the original frame annotations with the automatically generated entity links we collected a total of 81,706 annotations, which we use in our experimental setting as a silver standard. 3.2. Word Frame Disambiguation Following the WFD approach described in Gangemi et al. (2016b) we implemented a simple word frame disambiguator, where for each provided annotation in our silver standard we try to predict a frame label only on the basis of the BabelNet synsets generated through BabelFy. In order to provide the most suitable frame label F for the provided BabelNet synset label bs: 1"
L18-1093,W17-1909,1,0.92743,"we hear someone READ a text, our understanding of what we hear is usually spontaneous – the rules by which we interpret meaning. Here, the reference to the ‘Reading aloud’ frame from FrameNet1 could be triggered on the basis of the occurrences of the verbs in the sentence: hear, understanding and interpret. Such connections, in turn, could be provided by a hybrid resource where distributional representations from text have been explicitly linked to semantic knowledge repositories, since hybrid resources of this kind have been shown in the past to improve performance on lexical understanding (Panchenko et al., 2017) as well as taxonomy learning and cleaning (Faralli et al., 2017). In this paper, we bridge the gap between distributional and frame semantics by linking distributional semantic representations to Framester, a knowledge graph that acts as a hub between resources like FrameNet, BabelNet and DBpedia, among others. As a result of this, we introduce a new lexical resource that enriches the Framester knowledge graph with distributional features extracted from text, and show how this hybrid resource yields better results on the task of recognizing frames in running text. Joining distributional and f"
L18-1093,D17-1270,0,0.0292348,"Missing"
L18-1093,C98-1013,0,\N,Missing
L18-1164,S10-1011,0,0.460828,"any language for which a tokenizer, part-of-speech tagger, lemmatizer, and a sense inventory are available. The rest of the paper is organized as follows. Section 2 reviews related work. Section 3 presents the Watasense word sense disambiguation system, presents its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-"
L18-1164,P13-4007,0,0.0247526,"arch methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the seman"
L18-1164,Q14-1019,0,0.0545051,"The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vecto"
L18-1164,S07-1006,0,0.112434,"Missing"
L18-1164,D17-2016,1,0.945865,"for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce ou"
L18-1164,W16-1620,1,0.943397,"ory and offers the variety of WordNet-based algorithms. Babelfy (Moro et al., 2014) is a WSD system that uses BabelNet, a large-scale multilingual lexical ontology available for most natural languages. Due to the broad coverage of BabelNet, Babelfy offers entity linking as part of the WSD functionality. Panchenko et al. (2017b) present an unsupervised WSD system that is also knowledge-free: its sense inventory is induced based on the JoBimText framework, and disambiguation is performed by computing the semantic similarity between the context and the candidate senses (Biemann and Riedl, 2013). Pelevina et al. (2016) proposed a similar approach to WSD, but based on dense vector representations (word embeddings), called SenseGram. Similarly to SenseGram, our WSD system is based on averaging of word embeddings on the basis of an automatically induced sense inventory. A crucial difference, however, is that we induce our sense inventory from synonymy dictionaries and not distributional word vectors. While this requires more manually created resources, a potential advantage of our approach is that the resulting inventory contains less noise. 3. Watasense, an Unsupervised System for Word Sense Disambiguation Wa"
L18-1164,P17-1145,1,0.925562,"present system and also conduct its evaluation on three different lexical semantic resources for Russian. We found that the dense mode substantially outperforms the sparse one on all datasets according to the adjusted Rand index. Keywords: word sense disambiguation, system, synset induction 1. Introduction Word sense disambiguation (WSD) is a natural language processing task of identifying the particular word senses of polysemous words used in a sentence. Recently, a lot of attention was paid to the problem of WSD for the Russian language (Lopukhin and Lopukhina, 2016; Lopukhin et al., 2017; Ustalov et al., 2017). This problem is especially difficult because of both linguistic issues – namely, the rich morphology of Russian and other Slavic languages in general – and technical challenges like the lack of software and language resources required for addressing the problem. To address these issues, we present Watasense, an unsupervised system for word sense disambiguation. We describe its architecture and conduct an evaluation on three datasets for Russian. The choice of an unsupervised system is motivated by the absence of resources that would enable a supervised system for under-resourced languages. W"
L18-1164,P10-4014,0,0.0428215,"s its architecture, and describes the unsupervised word sense disambiguation methods bundled with it. Section 4 evaluates the system on a gold standard for Russian. Section 5 concludes with final remarks. 2. Related Work Although the problem of WSD has been addressed in many SemEval campaigns (Navigli et al., 2007; Agirre et al., 2010; Manandhar et al., 2010, inter alia), we focus here on word sense disambiguation systems rather than on the research methodologies. Among the freely available systems, IMS (“It Makes Sense”) is a supervised WSD system designed initially for the English language (Zhong and Ng, 2010). The system uses a support vector machine classifier to infer the particular sense of a word in the sentence given its contextual sentence-level features. Pywsd is an implementation of several popular WSD algorithms implemented in a library for the Python programming language.1 It offers both the classical Lesk algorithm for WSD and path-based algorithms that heavily use the WordNet and similar lexical ontologies. DKPro WSD (Miller et al., 2013) is a generalpurpose framework for WSD that uses a lexical ontology as the sense inventory and offers the variety of WordNet-based algorithms. Babelfy"
L18-1167,W06-3812,1,0.866038,"ctor. 3.1. the sense inventory (i.e. the collection of synsets) Sv for the word v; we refer to a particular synset or sense k of v as Svk . We want to stress that v is usually not contained in any of its “synsets”, i.e. Svk = Svk  v per definition. Following Riedl and Biemann (2017), we use an unsupervised sense inventory, pre-computed3 by using the J O B IM T EXT (JBT) framework (Biemann and Riedl, 2013; Riedl, 2016), which can be seen as a symbolic count based model. JBT provides a graph-based sparse word similarity model, i.e. only words, and no vectors are provided. The Chinese Whispers (Biemann, 2006, CW) algorithm is used for inducing word senses based on ego networks weighted by context similarity. Unsupervised Sense Inventory Our proposed method solely relies on pre-computed wordembeddings and a sense inventory resource. We follow the terminology in W ORD N ET and define a synset for a word v to be the set of related words that express the same concept, and the sense inventory of v to be the collection of its synsets, i.e. the different senses v can bear. Dorow and Widdows (2003), Pantel and Lin (2002), and more recently, Pelevina et al. (2016) use unsupervised WSI methods, which means"
L18-1167,W14-3348,0,0.0224184,"ferent interpretations of the word iron, and even this can never be considered to be complete as language evolves. Assigning the same vector for each distinguished sense and using them in downstream tasks such as sentiment analysis, named entity recognition, question answering or many others, is error prone by design due to obvious misinterpretations and error propagation. Sense inventories – of which W ORD N ET is probably the most well known – are required to distinguish between different word senses, and meanings, rather than words, should be represented in the vector space (Navigli, 2009; Denkowski and Lavie, 2014). We use a simple, yet effective technique to retrofit standard word embeddings to produce embedding vectors of senses using external resources as sense inventories. Our hypothesis is that retrofitting pretrained word embeddings to gain sense-aware embeddings is beneficial for word similarity computations. Using vectors of senses rather that vectors of words, we are indeed able to report substantial relative improvements for multiple word similarity tasks and for various types of retrofitted embeddings from five monolingual corpora. Because a word maps to multiple sense vectors in this scenari"
L18-1167,E03-1020,0,0.0623373,"ovides a graph-based sparse word similarity model, i.e. only words, and no vectors are provided. The Chinese Whispers (Biemann, 2006, CW) algorithm is used for inducing word senses based on ego networks weighted by context similarity. Unsupervised Sense Inventory Our proposed method solely relies on pre-computed wordembeddings and a sense inventory resource. We follow the terminology in W ORD N ET and define a synset for a word v to be the set of related words that express the same concept, and the sense inventory of v to be the collection of its synsets, i.e. the different senses v can bear. Dorow and Widdows (2003), Pantel and Lin (2002), and more recently, Pelevina et al. (2016) use unsupervised WSI methods, which means they use or create so-called unsupervised synsets referring to sense-inventories, which were induced automatically from text. The simplified procedure to compute an unsupervised synset for a particular word v is as follows: 1. compute v’s top n nearest neighbors (by some wordsimilarity notion) 2. compute a similarity score between every pairwise combination of nearest neighbors, which renders a fully connected similarity graph where λ is a scalar in [0, 1], vk is the sense vector of the"
L18-1167,N15-1184,0,0.0315936,"tegrating the sense distinction into the learning process. We use A DA G RAM (Bartunov et al., 2016) as an additional baseline because it compares favorably to the model by Neelakantan et al. (2014). A DAG RAM’s main parameter effectively regulates the maximum number of senses per word; the algorithm finds the number of senses automatically in this range, i.e. the parameter can be seen as a limit for the maximum number of induced senses. Retrofitting is the process of augmenting a given item for a new task, i.e. in our case a post-processing objective that re-adjusts existing word embeddings (Faruqui et al., 2015). Multiple objectives have been defined on this account, e.g. Faruqui et al. (2015) or Kiela et al. (2015) use lexical resources, while, for instance, Wieting et al. (2015) directly optimizes paraphrase pair alignment from PPDB2 (Ganitkevitch et al., 2013). 3. 3.2. Retrofitting Word Embeddings The main goal of retrofitting word vectors is to find individual vector representations for each sense of a word. Using a sense inventory, word vectors from a particular synset are averaged, such that each sense of a word will be represented by a single individual vector. For a word v, we average all vec"
L18-1167,W16-2506,0,0.0195634,"the two graph clustering algorithms, based on SGNS embedding vectors. SGNS- Sclnat SYMPAT- S clnat PARAGRAM SL- S clnat PARAGRAM WS- S clnat S IM L EX 999 0.42 0.48 0.67 0.57 Table 4: Selected results for native sense induction by clustering on four embeddings and S IM L EX 999. farther away (small cosine similarity, large rank). The selected terms for representative senses seem to have a similar cosine similarity to iron though. This is not an isolated incident, we have observed this effect consistently for multiple polysemous terms.15 This suggests the confirmation of the observations which Faruqui et al. (2016) or Schnabel et al. (2015) already noted: Within neural word embeddings, the frequency rank of a word’s neighbor strongly depends on the frequency of the word itself. This is clearly an issue because the frequency of a word’s sense naturally correlates with the frequency of a word’s occurrence. Table 3 shows the average number of clusters for all words across all datasets for varying τcos . Based on those results, we fix τcos = 0.8 and CW, as this best resembles the sense inventory of the JBT resource, where also CW is used, producing 3.73 senses on average. Selected results of the native clus"
L18-1167,N13-1092,0,0.0295567,"SIMVERB dataset (Gerz et al., 2016), can be interpreted as a larger version of the verb part of S IM L EX 999, containing 3, 500 verb pairs, allowing more meaningful benchmarking with more and better represented examples. 4.2. Embedding Matrices W ORD 2V EC applies a neural language modeling approach, where the goal is to predict a word wi at position i given its context ci (CBOW) or vice versa (SGNS) (Mikolov et al., 2013). A projection matrix is learned during this process, which has been shown to be beneficial in various NLP tasks. We use pre-trained word vectors provided by Mikolov et al. (2013), which were trained on Google News texts containing 6 Billion words.6 Additionally, we use the G LOV E7 (Pennington et al., 2014, global vectors) embeddings. Schwartz et al. (2015) defined the context of a word to be the symmetric pattern it occurs with, and applied W ORD 2V EC to those pairs. A symmetric pattern is a shallow pattern in the form of ’X or Y’, ’X and Y’, ’X as well as Y’, ’X rather than Y’, where particular instances of X and Y occur in both positions, e.g. ’cats and dogs’ and ’dogs and cats’ are considered instances of a symmetric pattern, while ’point of view’ for example can"
L18-1167,D16-1235,0,0.0253177,"Missing"
L18-1167,D15-1242,0,0.0182888,"dditional baseline because it compares favorably to the model by Neelakantan et al. (2014). A DAG RAM’s main parameter effectively regulates the maximum number of senses per word; the algorithm finds the number of senses automatically in this range, i.e. the parameter can be seen as a limit for the maximum number of induced senses. Retrofitting is the process of augmenting a given item for a new task, i.e. in our case a post-processing objective that re-adjusts existing word embeddings (Faruqui et al., 2015). Multiple objectives have been defined on this account, e.g. Faruqui et al. (2015) or Kiela et al. (2015) use lexical resources, while, for instance, Wieting et al. (2015) directly optimizes paraphrase pair alignment from PPDB2 (Ganitkevitch et al., 2013). 3. 3.2. Retrofitting Word Embeddings The main goal of retrofitting word vectors is to find individual vector representations for each sense of a word. Using a sense inventory, word vectors from a particular synset are averaged, such that each sense of a word will be represented by a single individual vector. For a word v, we average all vectors of the top m words in a synset Svk and add the vector v with weight λ in order to compensate for sema"
L18-1167,N16-1018,0,0.0369787,"Missing"
L18-1167,D14-1113,0,0.0367349,"n this paper, we mainly focus on embeddings generated by NNs because of their superior performance and current impact on NLP research. However, we note that our findings are also applicable to other types of embedded word vector spaces, as we shall see below. 1035 Rothe and Schütze (2015) introduced AUTO E XTEND, a supervised neural network model which enriches existing embeddings with word sense information from W ORD N ET or other sense inventories.1 Here, the sense inventory is taken from W ORD N ET but Rothe and Schütze (2015) emphasize that any lexical or semantic resource could be used. Neelakantan et al. (2014) and Bartunov et al. (2016) present approaches that gather sense information in an unsupervised way from monolingual text by integrating the sense distinction into the learning process. We use A DA G RAM (Bartunov et al., 2016) as an additional baseline because it compares favorably to the model by Neelakantan et al. (2014). A DAG RAM’s main parameter effectively regulates the maximum number of senses per word; the algorithm finds the number of senses automatically in this range, i.e. the parameter can be seen as a limit for the maximum number of induced senses. Retrofitting is the process of"
L18-1167,W16-1620,1,0.803434,"and no vectors are provided. The Chinese Whispers (Biemann, 2006, CW) algorithm is used for inducing word senses based on ego networks weighted by context similarity. Unsupervised Sense Inventory Our proposed method solely relies on pre-computed wordembeddings and a sense inventory resource. We follow the terminology in W ORD N ET and define a synset for a word v to be the set of related words that express the same concept, and the sense inventory of v to be the collection of its synsets, i.e. the different senses v can bear. Dorow and Widdows (2003), Pantel and Lin (2002), and more recently, Pelevina et al. (2016) use unsupervised WSI methods, which means they use or create so-called unsupervised synsets referring to sense-inventories, which were induced automatically from text. The simplified procedure to compute an unsupervised synset for a particular word v is as follows: 1. compute v’s top n nearest neighbors (by some wordsimilarity notion) 2. compute a similarity score between every pairwise combination of nearest neighbors, which renders a fully connected similarity graph where λ is a scalar in [0, 1], vk is the sense vector of the k th sense of word v, and u is the word vector of word u. A geome"
L18-1167,D14-1162,0,0.114671,"Missing"
L18-1167,W16-1622,0,0.0124846,"LSA embeddings trained on English corpora provided by Günther et al. (2015).9 Both models are based on a 2-Billion-word corpus and use a positive pointwise mutual information weighting scheme (PPMI) before applying singular value decomposition (SVD) with 300 target dimensions and a vocabulary of 100K words. We refer to the model based on a bag-of-word representation of documents as LSA BOW, and to the model applying a HAL-like context representation10 as LSA HAL, following the terminology of Günther et al. (2015). Many other NN embedding models have been published, e.g. (Wieting et al., 2016; Recski et al., 2016; Mrkši´c et al., 2016), however, we deliberately do not go into details here since these supervised models are out of the scope of this work; we focus on the relative improvement of monolingual embeddings by exploiting unsupervised WSI methods. We are thus independent of any manually developed resource and do not rely on the existence of parallel text. 5. Results We follow previous work and use the Spearman rankcorrelation coefficient ρ throughout the evaluation. We evaluated all datasets for all methods but restrict our discussion to the most interesting results. Selecting the top m = 5 clus"
L18-1167,W17-6933,1,0.843245,"of the top m words in a synset Svk and add the vector v with weight λ in order to compensate for semantic drift, for which we found strong indications in preliminary experiments: X vk = λv + (1 − λ) u, (1) Methodology u ∈ topm (Svk ) In the remainder of this work we will use v to refer to a word and v to refer to v’s corresponding word vector. 3.1. the sense inventory (i.e. the collection of synsets) Sv for the word v; we refer to a particular synset or sense k of v as Svk . We want to stress that v is usually not contained in any of its “synsets”, i.e. Svk = Svk  v per definition. Following Riedl and Biemann (2017), we use an unsupervised sense inventory, pre-computed3 by using the J O B IM T EXT (JBT) framework (Biemann and Riedl, 2013; Riedl, 2016), which can be seen as a symbolic count based model. JBT provides a graph-based sparse word similarity model, i.e. only words, and no vectors are provided. The Chinese Whispers (Biemann, 2006, CW) algorithm is used for inducing word senses based on ego networks weighted by context similarity. Unsupervised Sense Inventory Our proposed method solely relies on pre-computed wordembeddings and a sense inventory resource. We follow the terminology in W ORD N ET an"
L18-1167,P15-1173,0,0.342332,"us types of retrofitted embeddings from five monolingual corpora. Because a word maps to multiple sense vectors in this scenario, standard cosine similarity computations alone are not applicable anymore, we thus test a number of sense-aware comparison methodologies based on cosine similarity. In particular for word pairs involving minor/rare senses, we expect improvements in the sense-aware setting as the influence of the dominating major sense is diminished. Additionally, we compare our approach with two baseline approaches to supervised and unsupervised sense-aware embeddings: AUTO E XTEND (Rothe and Schütze, 2015) and A DAG RAM (Bartunov et al., 2016). To the best of our knowledge, we are the first to employ unsupervised word sense induction techniques for retrofitting single word vectors to the multiplicity of their meanings, creating new pseudo word-sense vectors, and using those for semantic similarity. Additionally, we test standard word sense induction (WSI) techniques using word embeddings themselves in order to make the retrofitting process selfsustained. Evidence presented below indicates that word embeddings are hardly useful in word sense induction clustering, due to the fact that their neigh"
L18-1167,P15-4018,1,0.816725,"rg max cos(u, vl ) 3. compute a word clustering, where each cluster represents a different sense of v. This general methodology has been proven to perform sufficiently well on a number of NLP tasks, whereas the details of this simplified procedure vary. The clustering represents Sense-aware word-similarity (3) l sim(u, v) = arg max cos(uk , vl ) . (4) k, l Equations (2-4) involve finding the closest senses k and l for the words u and v in vector space. We compare these measures to the standard, sense un-aware cosine similarity cos(u, v). 3 http://ltmaggie.informatik.uni-hamburg. de/jobimviz, (Ruppert et al., 2015) 1036 4. 4.1. Experimental Setup Word-similarity Benchmark Datasets Hill et al. (2014) raise the point that a strong distinction must be made between similarity and relatedness. While related words roughly fit into the same topic, similar words are more specific, they fit into the same topic and constitute (partial) substitutability. Consider for example the words student and professor, which are certainly considered related but not similar because there are only few contexts in which the two words can be exchanged, hence they are considered highly dissimilar due their antonymic nature while t"
L18-1167,D15-1036,0,0.019594,"algorithms, based on SGNS embedding vectors. SGNS- Sclnat SYMPAT- S clnat PARAGRAM SL- S clnat PARAGRAM WS- S clnat S IM L EX 999 0.42 0.48 0.67 0.57 Table 4: Selected results for native sense induction by clustering on four embeddings and S IM L EX 999. farther away (small cosine similarity, large rank). The selected terms for representative senses seem to have a similar cosine similarity to iron though. This is not an isolated incident, we have observed this effect consistently for multiple polysemous terms.15 This suggests the confirmation of the observations which Faruqui et al. (2016) or Schnabel et al. (2015) already noted: Within neural word embeddings, the frequency rank of a word’s neighbor strongly depends on the frequency of the word itself. This is clearly an issue because the frequency of a word’s sense naturally correlates with the frequency of a word’s occurrence. Table 3 shows the average number of clusters for all words across all datasets for varying τcos . Based on those results, we fix τcos = 0.8 and CW, as this best resembles the sense inventory of the JBT resource, where also CW is used, producing 3.73 senses on average. Selected results of the native clustering compared to S IM L"
L18-1167,K15-1026,0,0.0209754,"benchmarking with more and better represented examples. 4.2. Embedding Matrices W ORD 2V EC applies a neural language modeling approach, where the goal is to predict a word wi at position i given its context ci (CBOW) or vice versa (SGNS) (Mikolov et al., 2013). A projection matrix is learned during this process, which has been shown to be beneficial in various NLP tasks. We use pre-trained word vectors provided by Mikolov et al. (2013), which were trained on Google News texts containing 6 Billion words.6 Additionally, we use the G LOV E7 (Pennington et al., 2014, global vectors) embeddings. Schwartz et al. (2015) defined the context of a word to be the symmetric pattern it occurs with, and applied W ORD 2V EC to those pairs. A symmetric pattern is a shallow pattern in the form of ’X or Y’, ’X and Y’, ’X as well as Y’, ’X rather than Y’, where particular instances of X and Y occur in both positions, e.g. ’cats and dogs’ and ’dogs and cats’ are considered instances of a symmetric pattern, while ’point of view’ for example cannot be altered without losing its meaning, ’X of Y’ is thus considered an asymmetric pattern. Some symmetric patterns are considered to be particularly indicative for antonymy, e.g."
L18-1167,Q15-1025,0,0.127168,"Neelakantan et al. (2014). A DAG RAM’s main parameter effectively regulates the maximum number of senses per word; the algorithm finds the number of senses automatically in this range, i.e. the parameter can be seen as a limit for the maximum number of induced senses. Retrofitting is the process of augmenting a given item for a new task, i.e. in our case a post-processing objective that re-adjusts existing word embeddings (Faruqui et al., 2015). Multiple objectives have been defined on this account, e.g. Faruqui et al. (2015) or Kiela et al. (2015) use lexical resources, while, for instance, Wieting et al. (2015) directly optimizes paraphrase pair alignment from PPDB2 (Ganitkevitch et al., 2013). 3. 3.2. Retrofitting Word Embeddings The main goal of retrofitting word vectors is to find individual vector representations for each sense of a word. Using a sense inventory, word vectors from a particular synset are averaged, such that each sense of a word will be represented by a single individual vector. For a word v, we average all vectors of the top m words in a synset Svk and add the vector v with weight λ in order to compensate for semantic drift, for which we found strong indications in preliminary e"
L18-1167,D16-1157,0,0.0204635,"also make use of two LSA embeddings trained on English corpora provided by Günther et al. (2015).9 Both models are based on a 2-Billion-word corpus and use a positive pointwise mutual information weighting scheme (PPMI) before applying singular value decomposition (SVD) with 300 target dimensions and a vocabulary of 100K words. We refer to the model based on a bag-of-word representation of documents as LSA BOW, and to the model applying a HAL-like context representation10 as LSA HAL, following the terminology of Günther et al. (2015). Many other NN embedding models have been published, e.g. (Wieting et al., 2016; Recski et al., 2016; Mrkši´c et al., 2016), however, we deliberately do not go into details here since these supervised models are out of the scope of this work; we focus on the relative improvement of monolingual embeddings by exploiting unsupervised WSI methods. We are thus independent of any manually developed resource and do not rely on the existence of parallel text. 5. Results We follow previous work and use the Spearman rankcorrelation coefficient ρ throughout the evaluation. We evaluated all datasets for all methods but restrict our discussion to the most interesting results. Selecti"
L18-1244,P11-1062,0,0.0300176,"arge-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard cluste"
L18-1244,W06-3812,1,0.812843,"age” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a glob"
L18-1244,S15-2151,1,0.859596,"ypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical"
L18-1244,S16-1168,0,0.0609794,"thermore, we show the utility of our method in the domain taxonomy induction task, achieving the state-of-the-art results on a SemEval’16 task on taxonomy induction. Keywords: semantic classes, distributional semantics, hypernyms, co-hyponyms, word sense induction 1. Introduction Hypernyms are useful in various applications, such as question answering (Zhou et al., 2013), query expansion (Gong et al., 2005), and semantic role labelling (Shi and Mihalcea, 2005) as they can help to overcome sparsity of statistical models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017)."
L18-1244,2016.gwc-1.10,1,0.335932,"to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel,"
L18-1244,P99-1016,0,0.383204,"use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extract"
L18-1244,P14-1113,0,0.0239862,"e-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distrib"
L18-1244,D17-1185,1,0.891904,"Missing"
L18-1244,C92-2082,0,0.731648,"models. Hypernyms are also the building blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-"
L18-1244,heylen-etal-2008-modelling,0,0.0368325,"ango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semantic classes to improve"
L18-1244,N15-1098,1,0.894317,"Missing"
L18-1244,C14-2023,0,0.0820281,"of 4,870 relations using lexical split by hyponyms. All relations from Hcluster and Horig of one hyponym were included in the sample. These relations were subsequently annotated by human judges using crowdsourcing. We asked crowdworkers to provide a binary judgment about the correctness of each hypernymy relation as illustrated in Figure 7. 6.2.3. Results Overall, 298 annotators completed 4,870 unique tasks each labeled 6.9 times on average, resulting in a total of 33,719 binary human judgments about hypernyms. We obtained a fair agreement among annotators of 0.548 in terms of the Randolph κ (Meyer et al., 2014). Since CrowdFlower reports a confidence for each answer, we selected N = 3 most confident answers per pair and aggregated them using weighted majority voting. The ties were broken pessimistically, i.e. by treating a hypernym as irrelevant. Results for N ∈ 3, 5, 6 varied less than by 0.002 in terms of F-score. The task received the rating of a 4.4 out of 5.0 according to the annotator’s feedback mechanism. Table 5 presents results of the experiment. Since each pair received a binary score, we calculated Precision, Recall, and F-measure of two compared methods. Our denoising method improves the"
L18-1244,P06-2075,0,0.0444314,"ing their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for bu"
L18-1244,S15-1021,0,0.0148946,"obal Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account n"
L18-1244,D14-1113,0,0.099783,"s (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induced sense inventory sense network is scale-free, cf. (Steyvers and Tenenbaum, 2005). Our experiments show that a global clustering of this network can lead to a discovery of giant components, which are useless in our context as they represent no semantic class. To overcome this problem, we re-build the sense network as described below. 3.2. Representing Senses with Ego Networks To perform a global clustering of senses, we represent each induced sense s by a second-order ego network (Everett and Borgatti, 2005)."
L18-1244,S16-1206,1,0.906671,"Missing"
L18-1244,D17-2016,1,0.832821,"aluate our approach, we performed three experiments. A large-scale crowdsourcing study indicated a high plausibility of extracted semantic classes according to human judgment. Besides, we demonstrated that our approach helps to improve precision and recall of a hypernymy extraction method. Finally, we showed how the proposed semantic classes can be used to improve domain taxonomy induction from text. While we have demonstrated the utility of our approach for hypernym extraction and taxonomy induction, we believe that the induced semantic classes can be useful in other tasks. For instance, in (Panchenko et al., 2017) these semantic classes were used as an inventory for word sense disambiguation to deal with out of vocabulary words. 8. Acknowledgements This research was supported by Deutscher Akademischer Austauschdienst (DAAD), Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T), and by the Ministry of Education and Science of the Russian Federation Agreement no. 02.A03.21.0006. We are grateful to three anonymous reviewers for their helpful comments. Finally, we are grateful to Dirk Johannßen for providing feedback on an early version of th"
L18-1244,P06-1015,0,0.0996737,"rnymy relations using distributionally induced semantic classes, represented by clusters of induced word senses labeled with noisy hypernyms. The word postfix, such as #1, is an ID of an induced sense. The wrong hypernyms outside the cluster labels are removed, while the missing ones not present in the noisy database of hypernyms are added. 2. 2.1. Related Work Extraction of Hypernyms In her pioneering work, Hearst (1992) proposed to extract hypernyms based on lexical-syntactic patterns from text. Snow et al. (2004) learned such patterns automatically based on a set of hyponym-hypernym pairs. Pantel and Pennacchiotti (2006) presented another approach for weakly supervised extraction of similar extraction patterns. These approaches use some training pairs of hypernyms to bootstrap the pattern discovery process. For instance, Tjong Kim Sang (2007) used web snippets as a corpus for extraction of hypernyms. More recent approaches exploring the use of distributional word representations for extraction of 1541 1 https://github.com/uhh-lt/mangosteen §3 Induction of Semantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clusteri"
L18-1244,N04-1041,0,0.578714,"roposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http://www.commoncrawl.org algorithms are used to induce a proper tree from the binary relations harvested from text. 2.3. Induction of Semantic Classes This line of research starts with (Lin and Pantel, 2001), where sets of similar words are clustered into concepts. While this approach performs a hard clustering and does not label clusters, these drawbacks are addressed in (Pantel and Lin, 2002), where words can belong to several clusters, thus representing senses, and in (Pantel and Ravichandran, 2004), where authors aggregate hypernyms per cluster, which come from Hearst patterns. The main difference to our approach is that we explicitly represent senses both in clusters and in their hypernym labels, which enables us to connect our sense clusters into a global taxonomic structure. Consequently, we are the first to use semantic classes to improve hypernymy extraction. Ustalov et al. (2017b) proposed a synset induction approach based on global clustering of word senses. The authors used the graph constructed of dictionary synonyms, while we use distributionally-induced graphs of senses. 3. U"
L18-1244,C14-1097,0,0.161869,"mantic Classes Sense Ego-Networks §3.3 Global Sense Graph §3.4 Induced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regul"
L18-1244,L16-1056,1,0.932779,"rnym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et"
L18-1244,P16-1226,0,0.239693,"earning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised me"
L18-1244,steinberger-etal-2006-jrc,0,0.0164921,"used to improve results of other state-of-the-art hypernymy extraction approaches, such as HypeNET (Shwartz et al., 2016). 6.3. Experiment 3: Improving Domain Taxonomy Induction In this section, we show how the labeled semantic classes can be used for induction of domain taxonomies. 6.3.1. SemEval 2016 Task 13 We use the taxonomy extraction evaluation dataset by Bordea et al. (2016), featuring gold standard taxonomies for three domains (Food, Science, Environment) and four languages (English, Dutch, French, and Italian) on the basis of existing lexical resources, such as WordNet and Eurovoc (Steinberger et al., 2006).7 Participants were supposed to build a taxonomy provided a vocabulary of a domain. Since our other experiments were conducted on English, we used the English part of the task. The evaluation is 1547 7 http://eurovoc.europa.eu System / Domain, Dataset Food, WordNet Science, WordNet Food, Combined Science, Combined Science, Eurovoc Environment, Eurovoc WordNet 1.0000 1.0000 0.5870 0.5760 0.6243 n.a. Baseline JUNLP NUIG-UNLP QASSIT TAXI USAAR 0.0022 0.1925 n.a. n.a. 0.3260 0.0021 0.0016 0.0494 0.0027 0.2255 0.2255 0.0008 0.0019 0.2608 n.a. n.a. 0.2021 0.0000 0.0163 0.1774 0.0090 0.5757 0.3634 0"
L18-1244,P07-2042,0,0.0798013,"Missing"
L18-1244,E17-2087,1,0.887253,"us). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples and the asymmetric nature of the hypernyms. Recent approaches to hypernym extraction focused on learning supervised models based on a combination of syntactic patterns and distributional features (Shwartz et al., 2016). Note that while methods, such as (Mirkin et al., 2006) and (Shwartz et al., 2016) use distributional features for extraction of hypernyms, in contrast to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) perf"
L18-1244,P17-1145,1,0.586116,"Missing"
L18-1244,J13-3007,1,0.933854,"to our method, they do not take into account word senses and global distributional structure. Seitner et al. (2016) performed extraction of hypernyms from the web-scale Common Crawl2 text corpus to ensure high lexical coverage. In our experiments, we use this webscale database of noisy hypernyms, as the large-scale repository of automatically extracted hypernyms to date. 2.2. Taxonomy and Ontology Learning Most relevant in the context of automatic construction of lexical resource are methods for building resources from text (Caraballo, 1999; Biemann, 2005; Cimiano, 2006; Bordea et al., 2015; Velardi et al., 2013) as opposed to methods that automatically construct resources from semistructured data (Auer et al., 2007; Navigli and Ponzetto, 2012) or using crowdsourcing (Biemann, 2013; Braslavski et al., 2016). Our representation differs from the global hierarchy of words as constructed e.g. by (Berant et al., 2011; Faralli et al., 2016), as we are grouping many lexical items into a labeled sense cluster as opposed to organizing them in deep hierarchies. Kozareva and Hovy (2013) proposed a taxonomy induction method based on extraction of hypernyms using the doubly-anchored lexical patterns. Graph 2 http:"
L18-1244,P16-1158,0,0.0305763,"uced Word Senses §3.2 §3.1 Representing Senses Sense Graph Word Sense Induction Clustering of with Ego Networks Construction from Text Corpus Word Senes Global Sense Clusters §4 Labeling Sense Clusters with Hypernyms Noisy Hypernyms Text Corpus Cleansed Hypernyms Semantic Classes Figure 2: Outline of our approach: sense-aware distributional semantic classes are induced from a text corpus and then used to filter noisy hypernyms database (e.g. extracted by an external method from a text corpus). hypernyms and co-hyponyms include (Roller et al., 2014; Weeds et al., 2014; Necsulescu et al., 2015; Vylomova et al., 2016). They rely on two distributional vectors to characterize a relation between two words, e.g. on the basis of the difference of such vectors or their concatenation. Levy et al. (2015) discovered a tendency to lexical memorization of such approaches, hampering their generalization to other domains. Fu et al. (2014) relied on an alternative approach where a projection matrix is learned, which transforms a distributional vector of a hyponym to the vector of its hypernym. Ustalov et al. (2017a) improved this method by adding regularizers in the model that take into account negative training samples"
L18-1244,2005.jeptalnrecital-recital.1,0,0.0247916,"-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper is an unsupervised method for post-processing of noisy hypernymy relations based on clustering of graphs of word senses induced from text. The idea to use distributional semantics to find hypernyms seems natural and has been widely used. However, the existing methods used distributional, yet sense-unaware and local features. We are the first to use global sense-aware distributional structure via the induced semant"
L18-1244,C14-1212,0,0.297993,"uilding blocks for learning taxonomies from text (Bordea et al., 2016). Consider the following sentence: “This caf´e serves fresh mangosteen juice”. Here the infrequent word “mangosteen” may be poorly represented or even absent in the vocabulary of a statistical model, yet it can be substituted by lexical items with better representations, which carry close meaning, such as its hypernym “fruit” or one of its close co-hyponyms, e.g. “mango”. Currently available approaches to hypernymy extraction focus on the acquisition of individual binary hypernymy relations (Hearst, 1992; Snow et al., 2004; Weeds et al., 2014; Shwartz et al., 2016; Glavaˇs and Ponzetto, 2017). Frequencies of the extracted relations usually follow a power-law, with a long tail of noisy extractions containing rare words. We propose a method that performs post-processing of such noisy binary hypernyms using distributional semantics, cf. Figure 1. Namely, we use the observation that distributionally related words are often are co-hyponyms (Wandmacher, 2005; Heylen et al., 2008) and operationalize it to perform filtering of noisy relations by finding dense graphs composed of both hypernyms and co-hyponyms. The contribution of the paper"
L18-1244,C02-1114,0,0.354924,"ogy#0, language#0, format#2, app#0 Table 2: Sample of the induced sense clusters representing “fruits” and “programming language” semantic classes. Similarly to the induced word senses, the semantic classes are labeled with hypernyms. In contrast to the induced word senses, which represent a local clustering of word senses (related to a given word) semantic classes represent a global sense clustering of word senses. One sense c, such as “apple#0”, can appear only in a single cluster. of related ambiguous terms (Biemann and Riedl, 2013); (2) word sense induction via clustering of ego networks (Widdows and Dorow, 2002; Everett and Borgatti, 2005) of related words using the Chinese Whispers graph clustering algorithm (Biemann, 2006); (3) disambiguation of related words and hypernyms. The word sense inventory used in our experiment3 was extracted from a 9.3 billion tokens corpus, which is a concatenation of Wikipedia4 , ukWac (Ferraresi et al., 2008), LCC (Richter et al., 2006) and Gigaword (Graff and Cieri, 2003). Note that analogous graphs of senses can be obtained using word sense embeddings, see (Neelakantan et al., 2014; Bartunov et al., 2016). Similarly to any other distributional word graph, the induc"
L18-1286,C14-1076,0,0.0222431,"identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all versions of the Stanford, Malt (Hall et al., 2010), and Mate (Ballesteros and Bohnet, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and"
L18-1286,P01-1005,0,0.0320709,"is a combination of Wikipedia with two We compute syntactic count-based distributional representations of words using the JoBimText framework (Biemann 1821 30 https://github.com/uhh-lt/josimtext other corpora, we can reach the even better result by training the model (with exactly the same parameters) on the dependency-based features extracted from the full D EP CC corpus. This model substantially outperforms also the prior state of the art models, e.g. (Baroni et al., 2014) and (Gerz et al., 2016), on the SimVerb dataset, through the sheer size of the input corpus, as previously shown, e.g. (Banko and Brill, 2001) inter alia. 5.3.3. Differences in Performance for Test/Train Sets For the SimVerb dataset, the absolute performance on the test part (SimVerb500) is higher than the absolute performance on the train part (SimVerb300) for almost all models, including the baselines. We attribute this to a specific split of the data in the dataset: our models do not use the training data to learn verb representations. 6. Conclusion In this paper, we introduced a new web-scale corpus of English texts extracted from the C OMMON C RAWL, the largest openly available linguistically analyzed corpus to date, according"
L18-1286,P14-1023,0,0.148916,"e any copyrights as the authors of this derivative resource, but while using the D EP CC corpus you need to make sure to respect the Terms of Use of the original C OMMON C RAWL dataset it is based on.29 22 https://aws.amazon.com https://aws.amazon.com/ec2 24 https://aws.amazon.com/emr 25 https://www.elastic.co/guide/en/kibana/ current/lucene-query.html 26 23 https://www.elastic.co https://www.elastic.co/products/kibana 28 https://github.com/uhh-lt/josimtext 29 http://commoncrawl.org/terms-of-use 27 1820 Model SimVerb3500 SimVerb3000 SimVerb500 SimLex222 Wikipedia+ukWaC+BNC: Count SVD 500-dim (Baroni et al., 2014) PolyglotWikipedia: SGNS BOW 300-dim (Gerz et al., 2016) 8B: SGNS BOW 500-dim (Gerz et al., 2016) 8B: SGNS DEPS 500-dim (Gerz et al., 2016) PolyglotWikipedia:SGNS DEPS 300-dim (Gerz et al., 2016) 0.196 0.274 0.348 0.356 0.313 0.186 0.333 0.350 0.351 0.304 0.259 0.265 0.378 0.389 0.401 0.200 0.328 0.307 0.385 0.390 Wikipedia: LMI DEPS wpf-1000 fpw-2000 Wikipedia+ukWac+GigaWord: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-1000 fpw-1000 D EP CC: LMI DEPS wpf-1000 fpw-2000 D EP CC: LMI DEPS wpf-2000 fpw-2000 D EP CC: LMI DEPS wpf-5000 fpw-5000 0.283 0.376 0.400 0.404 0.399 0.382 0.284 0.368 0"
L18-1286,D14-1082,0,0.0495504,"t, 2014) parsers for English available via the DKPro Core framework. To dependency-parse texts, we selected the Malt parser, due to an optimal ratio of efficiency and effectiveness (parsing of 1 Mb of text per core in 1–4 minutes). This parser was successfully used in the past for the construction of linguistically analyzed web corpora, such as P UK WAC (Baroni et al., 2009) and ENCOW16 (Sch¨afer, 2015). While more accurate parsers exist, e.g. the Stanford parser, according to our experiments, even the neural-based version of this parser is substantially slower. On the other hand, as shown by Chen and Manning (2014), the performance of the Malt parser is only about 1.5–2.5 points below the neural-based Stanford parser. In particular, we used the stack model based on the projective transition system with the Malt.19 11 http://commoncrawl.org/2016/02/ http://index.commoncrawl.org/ CC-MAIN-2016-07 13 s3://commoncrawl/crawl-data/ CC-MAIN-2016-07 14 s3://commoncrawl/contrib/c4corpus/ CC-MAIN-2016-07 12 15 https://hadoop.apache.org https://uima.apache.org 17 https://github.com/uhh-lt/lefex 18 stanfordnlp-model-ner-en-all.3class.distsim.crf, 20.04.2015 19 The used model is de.tudarmstadt.ukp.dkpro.core.maltpars"
L18-1286,de-marneffe-etal-2006-generating,0,0.284992,"Missing"
L18-1286,W14-5201,0,0.0353294,"Missing"
L18-1286,P05-1045,0,0.0307133,"r the CC-BY license. 3.3. Linguistic Analysis of Texts Linguistic analysis consists of four stages presented in Figure 1 and is implemented using the Apache Hadoop framework15 for parallelization and the Apache UIMA framework16 for integration of linguistic analysers via the DKPro Core library (Eckart de Castilho and Gurevych, 2014).17 3.3.1. POS Tagging and Lemmatization For morphological analysis of texts, we used OpenNLP part-of-speech tagger and Stanford lemmatizer. 3.3.2. Named Entity Recognition To detect occurrences of persons, locations, and organizations we use the Stanford NER tool (Finkel et al., 2005).18 Overall, 7.48 billion occurrences of named entities were identified in the 251.92 billion tokens output corpus. 3.3.3. Dependency Parsing To make large-scale parsing of texts possible, a parser needs to be not only reasonably accurate but also fast. Unfortunately, the most accurate parsers, such as Stanford parser based on the PCFG grammar (De Marneffe et al., 2006), according to our experiments, take up to 60 minutes to process 1 Mb of text on a single core, which was prohibitively slow for our use-case (details of the hardware configuration are available in Section 3.5.). We tested all v"
L18-1286,P14-1097,0,0.38862,"Missing"
L18-1286,P14-2050,0,0.0413078,"art largescale experiments with syntax-aware models without the need of long and resource-intensive preprocessing. We built an index of sentences and their linguistic meta-data accessible though an interactive web-based search interface or via a RESTful API. In our experiments on the verb similarity task, a distributional model trained on the new corpus outperformed models trained on the smaller corpora, like Wikipedia, reaching new state of the art of verb similarity on the SimVerb3500 dataset. The corpus can be used in various contexts, ranging from training of syntax-based word embeddings (Levy and Goldberg, 2014) to unsupervised induction of word senses (Biemann et al., 2018) and frame structures (Kawahara et al., 2014). A promising direction of future work is using the proposed technology for building corpora in multiple languages. 7. Acknowledgements This research was supported by the Deutsche Forschungsgemeinschaft (DFG) under the project ”Joining Ontologies and Semantics Induced from Text” (JOIN-T). We are grateful to Amazon for providing required computational resources though the “AWS Cloud Credits for Research” program. Finally, we thank Kiril Gashteovski and three anonymous reviewers for their"
L18-1286,D14-1162,0,0.0799156,"Missing"
L18-1286,D14-1101,0,0.0153647,"Ginter, 2014). The texts were morphologically and syntactically analyzed. In addition, distributional vector space representations of the words were obtained using the word2vec toolkit (Mikolov et al., 2013). The resources were made available under an open license. GloVe (Pennington et al., 2014) is an unsupervised model for learning distributional word representations similar to word2vec. The authors distribute10 two models trained on the English part of a C OMMON C RAWL corpus (comprising respectively 42 and 820 billion of tokens), which are often used to build neural NLP systems, such as (Tsuboi, 2014). The models were trained on the C OMMON C RAWL documents texts tokenized with the Stanford tokenizer. In addition, the smaller training corpus was lowercased. 10 1817 https://nlp.stanford.edu/projects/glove The Web Term Vectors, Distributional Thesaurus §3.3 WARC web crawls §3.2 Filtered preprocessed documents §3.1 Linguistic Analysis: Preprocessing: Crawling Web Pages: lefex (Apache Hadoop) C4Corpus (Apache Hadoop) CCBot (Apache Nutch) POS Tagging (OpenNLP) §5.2 Comp. of Distributional Model: JoBimText (Apache Spark) Lemmatization (Stanford) DepCC: Dependency Parsed Corpus Named Entity Recog"
L18-1286,L16-1146,0,0.0555073,"Missing"
L18-1286,J15-4004,0,0.010721,"tes models based on bag-of-word features, while “DEPS” denotes syntax-based models. SimVerb3000 and SimVerb500 are train and test partitions of the SimVerb3500, while the SimLex222 dataset is composed of verb pairs from the SimLex999 dataset. The best results in a section are boldfaced, the best results overall are underlined. 5. Evaluation: Verb Similarity Task As an example of potential use-case, we demonstrate the utility of the corpus and the overall methodology on a verb similarity task. This task structurally is the same as the word similarity tasks based on such datasets as SimLex-999 (Hill et al., 2015). Namely, a system is given two words as input and needs to predict a scalar value which characterizes semantic similarity of the input words. While in the word similarity task the input pairs are words of various parts of speeches (nouns, adjectives, etc.), in this paper we only consider verb pairs. We chose this task since verb meaning is largely defined by the meaning of its arguments (Fillmore, 1982), therefore dependency-based features seem relevant for building distributional representations of verbs. 5.1. Datasets: SimVerb3500 and SimLex222 Recently a new challenging dataset for verb re"
L18-1286,W13-3520,0,0.087056,"Missing"
L18-1286,D16-1235,0,0.0315627,"Missing"
N07-1014,P03-1006,0,0.0260882,"previously 109 avg. sentence length indgw( X ) = ∑ weight (v, X ). w=0.4 s=0.08 w=0.4 s=0.1 w=0.17 s=0.22 w=0.3 s=0.09 x^(0.25); 10 1 10000 100000 text interval 1e+006 Figure 4: sentence length growth, plotted in average sentence length per intervals of 10,000 sentences. The straight line in the log plot indicates a polynomial growth. It should be noted that the sentence generator produces a very diverse sequence of sentences which does not deteriorate in repeating the same sentence all over again in later stages. Both word and sentence generator can be viewed as weighted finite automata (cf. Allauzen et al., 2003) with selftraining. After having defined our random text generation model, the next section is devoted to testing it according to the criteria given in section 2.1. 3 Experimental results However, the slight quantitative differences do not oppose the similar distribution of word lengths in both samples, which is reflected in a curve of similar shape in figure 6 and fits well the gamma distribution variant of (Sigurd et al., 2004). word length To measure agreement with our BNC sample, we generated random text with the sentence generator using w=0.4 and N=26 to match the English average word len"
N07-1014,C02-1117,0,\N,Missing
N07-3010,W06-3814,0,0.070014,"Missing"
N07-3010,E03-1020,0,0.0174961,"ives a poor impression on the utility of the unsupervised tagger’s output. Therefore, the tagger was evaluated indirectly in machine learning tasks, where POS tags are used as features. Biemann et al. (2007) report that for standard Named Entity Recognition, Word Sense Disambiguation and Chunking tasks, using unsupervised POS tags as features helps about as much as supervised tagging: Overall, almost no significant differences between results could be observed, supporting the initial claim. 3.3 Word Sense Induction (WSI) Co-occurrences are a widely used data source for WSI. The methodology of Dorow and Widdows (2003) was adopted: for the focus word, obtain its graph neighborhood (all vertices that are connected via edges to the focus word vertex and edges between these). Clustering this graph with CW and regarding clusters as senses, this method yields comparable results to Bordag (2006), tested using the unsupervised evaluation framework presented there. More detailed results are reported in Biemann (2006a). 4 4.1 Further Work Word Sense Disambiguation (WSD) The encouraging results in WSI enable support in automatic WSD systems. As described by Agirre et al. (2006), better performance can be expected if"
N07-3010,E06-1018,0,\N,Missing
N07-3010,N07-1014,1,\N,Missing
N07-3010,W06-3812,1,\N,Missing
N07-3010,P06-1038,0,\N,Missing
N07-3010,P06-3002,1,\N,Missing
N07-3010,P98-2127,0,\N,Missing
N07-3010,C98-2122,0,\N,Missing
N12-1064,A00-2004,0,0.818969,"we compare two early word-based algorithms with their topic-based variants, and construct our own algorithm called TopicRelated Work Based on the observation of Halliday and Hasan (1976) that the density of coherence relations is higher within segments than between segments, most algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to"
N12-1064,N09-1040,0,0.227467,"of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter α. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter βzdi . Unseen documents can be annotated with an existing TM using Bayesian inference methods (here:"
N12-1064,P03-1071,0,0.826199,"ost algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, c Montr´eal, Canad"
N12-1064,P94-1002,0,0.836349,"pics to identify the natural division of an unstructured text. In this work, we utilize semantic information from Topic Models (TMs) to inform text segmentation algorithms. For this, we compare two early word-based algorithms with their topic-based variants, and construct our own algorithm called TopicRelated Work Based on the observation of Halliday and Hasan (1976) that the density of coherence relations is higher within segments than between segments, most algorithms compute a coherence score to measure the difference of textual units for informing a segmentation decision. TextTiling (TT) (Hearst, 1994) relies on the simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) ar"
N12-1064,E06-1035,0,0.0166761,"leviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation algorithms, see (Yaari, 1997; Hsueh et al., 2006; Eisenstein, 2009). For topic modeling, we use the widely applied LDA (Blei et al., 2003). This generative probabilistic model uses a training corpus of documents to create document-topic and topic-word distributions and is parameterized by the number of topics N as well as by two hyperparameters. To generate a document d the topic proportions are drawn using a Dirichlet distribution with hyperparameter α. Adjacent for each word i a topic zdi is chosen according to a multinomial distribution using hyperparameter βzdi . Unseen documents can be annotated with an existing TM using Bayesian infer"
N12-1064,J02-1002,0,0.217816,"rch all local minima based on these similarity scores and calculate for these positions the depth score as described in TT. If the number of segments is known in advance, the segments of the n-highest depth-scores are used, otherwise the cut-off score criteria used in TT is adapted. 5 LDA inference, sampling 100 iterations. Inference is executed sentence-wise, since sentences form the minimal unit of our segmentation algorithms and we cannot use document information in the test setting. The performance of the algorithms is measured using Pk and WindowDiff (WD) metrics (Beeferman et al., 1999; Pevzner and Hearst, 2002). The C99 algorithm is initialized with a 11×11 ranking mask, as recommended in Choi (2000). TT is configured according to Choi (2000) with sequence length w=20 and block size k=6. 5.1 For evaluation, we rely on the Choi data set (Choi, 2000), which has been used in several other text segmentation approaches to ensure comparability. This data set is generated artificially using the Brown corpus and consists of 700 documents. Each document consists of 10 segments. For its generation, 3–11 sentences are sequentially extracted from a randomly selected document and merged together. While our CV ev"
N12-1064,P08-2068,0,0.218199,"s of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high computational cost. In addition to these linear topic segmentation algorithms, there are hierarchical segmentation"
N12-1064,P01-1064,0,0.730643,"he simplest coherence relation – word repetition – and computes similarities between textual units based on the similarities of word space vectors. With C99 (Choi, 2000) an algorithm was introduced that uses a matrix-based ranking and a clustering approach in order to relate the most similar textual units and to cluster groups of consecutive units into segments. Both TT and C99 characterize textual units by the words they contain. Galley et al. (2003) showed that using TF-IDF term weights in the term vector improves the performance of TT. Proposals using Dynamic Programming (DP) are given in (Utiyama and Isahara, 2001; Fragkou et al., 2004). Related to our work are the approaches described in (Misra et al., 2009; Sun et al., 2008): here, TMs are also used to alleviate the sparsity of word vectors. Misra et al. (2009) extended the DP algorithm U00 from Utiyama and Isahara (2001) us553 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 553–557, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics ing TMs. At this, the topic assignments have to be inferred for each possible segment, resulting in high"
N13-1119,P98-1013,0,0.0332009,"cheme that gets us reasonable inter-annotator agreement, inspired by the concept of cohesive harmony (Hasan, 1984), and report on an annotation project for German newswire texts. Documents from the SALSA 2.0 (Burchardt et al., 2006) corpus were chosen to form the basis for the annotation of lexical chain information. SALSA is based on the semi-automatically annotated TIGER Treebank 2.1 (Brants et al., 2002). The TIGER treebank provides manual annotations, such as lemmas, part-of-speech tags, and syntactic structure, the SALSA part of the corpus is also partially annotated with FrameNet-style (Baker et al., 1998) frame annotation. The documents are general domain news articles from a German newspaper comprising about 1,550 documents and around 50,000 sentences in total, with a median document length of 275 tokens. 3.1 Annotation Scheme In order to minimize the subjectiveness of choices by different annotators, annotation guidelines were developed comprising a total of ten pages. We decided to consider only nouns, noun compounds and non-compositional adjective noun phrases like “dirty money” as candidate terms for lexical chaining, which is consistent with the procedures of Hollingsworth and Teufel (20"
N13-1119,W97-0703,0,0.534009,"initially proposed an algorithm for lexical chaining based on Roget’s thesaurus (Roget, 1852), and manually assessed the quality of their algorithm. Hirst and St-Onge (1998) first presented a computational approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that 990 has also been utilized by the method of Medelyan (2007), where"
N13-1119,W06-3812,1,0.695397,"dis-)similarity and cosine similarity. Let G = (V, E) be the graph representation of a document with term vertices V = {v1 , . . . , vN d } and weighted edges E = {(v1 , v2 , sim12 ), . . . (vN d , vN d−1 , simNd Nd −1 )}, where simij is either the cosine or Euclidean similarity of term vectors. For simplicity, we reduce this representation to an unweighted graph by only retaining edges (of unit weight) that have a similarity above a parameter threshold sim . To identify chains as clusters in this graph, we follow Medelyan (2007) and apply the Chinese Whispers graph clustering algorithm (CW, Biemann (2006)), which finds the number of clusters automatically. The CW algorithm implementation comes with three parameters to regulate the node weight based on its degree, which influences cluster size and granularity. We test options ”top”, ”dist log” and ”dist lin”. The final chaining procedure is straightforward: The LDA-GM algorithm assigns every candidate lexical item wi of a certain document d which is assigned the same class label ci to the same chain. Level two links are drawn using the second dominant class of a vertex’s neighborhood, which is provided by the CW implementation. 4.3 LDA Top-N Me"
N13-1119,D07-1109,0,0.0242238,"cy on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, have carried out an annotation study, and report results on a corpus of man"
N13-1119,burchardt-etal-2006-salsa,0,0.0350143,"Missing"
N13-1119,D07-1108,0,0.0324077,"Missing"
N13-1119,C10-2029,0,0.0185116,"e semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have developed a methodology for evaluating the quality of lexical chains intrinsically, h"
N13-1119,W97-0802,0,0.171429,"eline is trivial, two baselines are state-of-the art knowledge-based systems adapted to German. Random: Candidate lexical items are randomly tied together to form sets of lexical chains. Level two links are created analogously. We regulate the process to yield the same average number of chains and links as in the development and test data. S&M GermaNet: Algorithm by Silber and McCoy (2002) with GermaNet as its knowledge resource. 7 as provided by Projekt Deutscher Wortschatz, http://wortschatz.uni-leipzig.de/ G&M GermaNet: Algorithm by Galley and McKeown (2003), also using GermaNet. GermaNet (Hamp and Feldweg, 1997) is a large WordNet-like resource for German, containing almost 100,000 lexical units and over 87,000 conceptual relations between synsets. While its size is only about half of WordNet, it is one of the largest nonEnglish lexical semantic resources. 5.2 Model Selection We optimize two sets of parameters: parameters for the LDA topic model (number of topics K, Dirichlet hyperparameters α and β) are optimized for the LDA-MM method only, and the same LDA model is used in the other two LDA-based methods. Parameters particular to the respective method are optimized individually. For LDA, we tested"
N13-1119,R09-1028,0,0.0198259,"Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text segmentation (Misra et al., 2009), lexical substitution (Dinu and Lapata, 2010) or word sense disambiguation (Cai et al., 2007; Boyd-Graber et al., 2007). 989 Proceedings of NAACL-HLT 2013, pages 989–999, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics In this work, we address the question, whether statistical methods for the extraction of lexical chains can yield better results than existing knowledge-based methods, especially for underresourced languages or domains, following principles of Structure Discovery (Biemann, 2012). To address this, we have"
N13-1119,P07-3015,0,0.081458,"lay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that 990 has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and Hirst (2010), who combine distributional co-occurrence information with semantic information from a lexicographic resource for extracting lexical chains and evaluate them by text segmentation. We are not aware of previous lexical chaining algorithms that do not rely on a lexicographic resource at all. A major issue in developing a new lexical chaining algorithm is the comparison to previous"
N13-1119,J91-1002,0,0.959711,"r (Halliday and Hasan, 1976). The theoretic foundation of this structure is defined as coherence and cohesion. While the former is concerned with the meaning of a text, the latter can be seen as a collection of devices for creating it. Cohesion and coherence build the basis for most of the current natural language processing problems that deal with text understanding. Lexical cohesion ties together words or phrases that Motivation for Corpus-driven Approach Previous approaches mainly focus on the use of knowledge resources like lexical semantic databases (Hirst and St-Onge, 1998) or thesauri (Morris and Hirst, 1991) as background information in order to resolve possible semantic relations. A major drawback of this strategy is the dependency on the coverage of the resource, which has a direct impact on the lexical chains. Their quality can be expected to be poor for resource-scarce languages or specialized application domains. Statistical methods to modeling language semantics have proven to deliver good results in many natural language processing applications. In particular, probabilistic topic models have been successfully used for tasks such as summarization (Gong and Liu, 2001; Hennig, 2009), text seg"
N13-1119,C94-2121,0,0.816359,"Missing"
N13-1119,W12-0703,1,0.648593,"formation in automatic lexical chainers. Specifically, we use the GibbsLDA++5 framework for topic model estimation and inference, and examine the following LDA parameters: number of topics T , Dirichlet hyperparameters for documenttopic distribution α and topic-term distribution β. We now describe three LDA-based approaches to lexical chaining. 4.1 LDA Mode Method (LDA-MM) The LDA-MM approach places all word tokens that share the same topic ID into the same chain. The point is now how to decide to which topic a word belongs to. Since single samples of topics per word exhibit a large variance (Riedl and Biemann, 2012), we follow these authors by sampling several times and using the mode (most frequently assigned) topic ID per word as the topic assignment. This strategy reduced the variance in the lccm to a tenth6 . More formally, let samples(d,w) be the vector of assignments that have been collected for a certain word w in a certain document d with each (d,w) samplesi referring to the i-th sampled topic ID for (d, w). In other words, samples(d,w) can be seen as the Markov chain for a particular word in a particular document. Further let z (d,w) be the topic ID that was most assigned to the word w with resp"
N13-1119,D07-1043,0,0.0272429,"hand. After exploring a number of measures1 , we decided on a combination of the adjusted Rand index (ARI, Hubert and Arabie (1985)) and the basic merge distance (BM D, Menestrina et al. (2010)) for our new measure. Menestrina et al. (2010) introduced a linear time algorithm for computing the generalized merge distance (GM D), which counts 1 Explored measures which are unsatisfactory for the given task are: Closest Cluster F1 (Benjelloun et al., 2009), K (Ajmera et al., 2002), Pairwise F1 (Manning et al., 2008), Variation of Information (Meil˘a, 2005), B3 (Bagga and Baldwin, 1998), VMeasure (Rosenberg and Hirschberg, 2007), Normalized Mutual Information (Strehl, 2002). The last two measures are equal. A proof of this can be found in the appendix. split and merge cluster editing operations. Using a constant factor of 1 for both splits and merges gives the basic merge distance (BM D): Considering > as the most general clustering of a dataset D, where all elements are grouped into the same cluster, and further considering ⊥ as the most specific clustering of D, where each element builds its own cluster, the lattice between > and ⊥ spans all possible clusterings and the BM D can be interpreted as the shortest path"
N13-1119,J02-4004,0,0.503731,"onal approach to lexical chaining based on WordNet showing that the lexical database is a reasonable replacement to Roget’s. The basic idea behind these algorithms is that semantically close words should be connected to form chains. Subsequent approaches mainly concentrated on disambiguation of words to WordNet concepts (WSD), since ambiguous words can lead to the overgeneration of connections. Barzilay and Elhadad (1997) improved the implicit word sense disambiguation (WSD) by keeping a list of different interpretations of the text and finally choosing the most plausible senses for chaining. Silber and McCoy (2002) introduced an efficient variant of the algorithm with linear complexity in the number of candidate terms. Galley and McKeown (2003) further improved accuracy by first performing WSD, and then using the remaining links between the disambiguated concepts only. They also introduced a socalled disambiguation graph, a representation that 990 has also been utilized by the method of Medelyan (2007), where she applied a graph clustering algorithm to the disambiguation graph to cut weak links, performing implicit WSD. A combination of statistical and knowledge-based methods is presented by Marathe and"
N13-1119,C98-1013,0,\N,Missing
N13-1133,S12-1051,0,0.00453916,"omplete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, including lexical-semantic resources and distributional similarity, n-gram and shallow syntactic features based on large, unannotated background corpora. In light of the existence of lexical resources such as WordNet (Fellbaum, 1998) or machine readable dictionarie"
N13-1133,D10-1116,0,0.0525266,"irst, the available lexical resources are seldom complete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, including lexical-semantic resources and distributional similarity, n-gram and shallow syntactic features based on large, unannotated background corpora. In light of the existence of lexical resources such as WordNet"
N13-1133,P06-1057,0,0.0937847,"chnische Universit¨at Darmstadt (4) Ubiquitous Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information http://www.nuance.com , http://www.ukp.tu-darmstadt.de Abstract Lexical substitution, a special form of contextual paraphrasing where only a single word is replaced, is closely related to word sense disambiguation (WSD): polysemous words have possible substitutions reflecting several senses, and the correct sense has to be picked to avoid spurious system behavior. However, no explicit word sense inventory is required for lexical substitution (Dagan et al., 2006). We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using"
N13-1133,de-marneffe-etal-2006-generating,0,0.00669888,"Missing"
N13-1133,D10-1113,0,0.03443,"o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. 1133 Dinu and Lapata (2010) used a bag of words latent variable model to characterize the meaning of a word as a distribution over a set of latent variables (that is, probabilistic senses). Contextualized representation of word meaning is then attained by conditioning the model on the context words in which the target word occurs. A similar approach has been evaluated for word similarity (Reisinger and Mooney, 2010a) and word sense disambiguation (Li et al., 2010). Although our main goal here is to develop a fullfledged lexical substitution system, we mainly focus on the construction of better ranking models based on su"
N13-1133,J93-1003,0,0.283666,"Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus 1136 of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26 . After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune the data, keeping only the 1000 most salient features (Fw ) per word7 . The similarity of two words is then given by the number of their common features. Our distributional thesaurus provides a list of the 1000 most salient features and a ranked list of up to 200 similar words (simw , based on the number of shared features) for all words above a certain frequency in the source corpus. We compute the following features to characterize a target word / substitution pair: • To what P extent the context c characterizes si : P c∈Fs"
N13-1133,D08-1094,0,0.0878026,"Missing"
N13-1133,P10-2017,0,0.0447386,"Missing"
N13-1133,S07-1029,0,0.78648,"unnecessary. 8 The various values for k trade off the salience of this feature for coverage: only very few substitutions have overlap in the top 1-5 similar words set, but if this happens, it is a very strong indicator of contextual fitness, whereas overlap within the top 100-200 similar words is present for much more target/substitution pairs, but it is a weaker indicator of fitness. 6 4.3.3 Local n-gram Features (from Web 1T) Syntagmatic coherence, measured as the n-gram frequency of the context with the candidate substitution serves as the basis of ranking in the best Semeval 2007 system (Giuliano et al., 2007), which is also our baseline method here. We use the same ngrams as features in our supervised model: • 1-5-gram frequencies in a sliding window around t: f req(cl si cr )/f req(cl tcr ), normalized w.r.t t • 1-5-gram frequencies in a sliding window P around t: f req(cl si cr )/ f req(cl Scr ), normalized w.r.t. S • for each of x in {’and’, ’or’, ’,’}, 3-5gram frequencies in a sliding window around t: f req(cl txsi cr )/f req(cl tcr ) (how frequently the target and candidate are part of a list or conjunctive phrase) 4.3.4 Shallow Syntactic Features We also use part of speech information (from"
N13-1133,S07-1091,0,0.474545,"Missing"
N13-1133,P10-1116,0,0.00868198,"Missing"
N13-1133,P98-2127,0,0.0443239,"s IDs of the words’ hypernyms as features, which can capture more general semantics (the word to replace is ’animate’, ’abstract’, etc.). The following features were extracted from WordNet: • number of senses of t and si in WordNet • the sense numbers of t and si which are synonymous (in case they are direct synonyms, c.f. WN sense numbers encode sense frequencies) • binary features for synset IDs of the hypernyms of the synset containing t and si (this feature type did not significantly improve results) 4.3.2 Corpus-based Features In order to create a Distributional Thesaurus (DT) similar to Lin (1998), we parsed a source corpus 1136 of 120M sentence English newspaper texts from the LCC5 (Richter et al., 2006) with the Stanford parser (de Marneffe et al., 2006) and used dependencies to extract features for words: each dependency triple (w1, r, w2) denoting a dependency of type r between words w1 and w2 results in a feature (r, w2) characterizing w1, and a feature (w1, r) characterizing w26 . After counting the frequency of each feature for each word, we apply a significance measure (log-likelihood test (LL), (Dunning, 1993)), rank features per word according to their significance, and prune"
N13-1133,S07-1050,0,0.386383,"rget words, instances with at least one non-multiword possible substitution, average size of candidate sets, and number of instances with no good candidate and frequency of different labels. The labels denote how many annotators proposed a particular word as substitution in the given context and can be interpreted as a measure of goodness: the higher the value, the better the candidate fits in the context. Similarly, the label 0 denotes the total number of negative examples in our datasets, i.e. bad substitutions – words that belong to the can3 This candidate set was found best for WordNet by Martinez et al. (2007). 1134 source # words #inst avg. set # empty #0 #1 #2 #3 #4 #5+ LexSub WN Gold St. 201 201 2002 2002 21 17 508 17 39465 27300 1302 4698 582 1251 308 571 212 319 129 179 TWSI WN Gold St. 908 1007 22543 24643 7.5 22 11165 620 151538 443993 10678 77417 4171 17585 2069 5629 74 325 121 411 Table 1: Details of the datasets: WN=WordNet didate set for a particular target word, but are not listed as good substitutions in the given context in the dataset. 4 Methodology 4.1 Experimental Setup and Evaluation We follow previous works in lexical substitution and evaluate our models using the Generalized Ave"
N13-1133,S07-1009,0,0.767833,"e target word and then to rank this set of possible substitutions according to their contextual fitness. The task to generate a high quality set of possible substitutions is challenging in itself, for two reasons. First, the available lexical resources are seldom complete in listing synonyms. Second, manually annotated substitutions show that not all synonyms of a word are appropriate in a given context, and many good substitutions have other lexical relation than synonymy to the original word. Introduction In recent years, the task of automatically providing lexical substitutions in context (McCarthy and Navigli, 2007) received much attention. The premise to be able to replace words in a sentence without changing its meaning gave rise to applications like linguistic steganography (Topkara et al., 2006; Chang and Clark, 2010), semantic text similarity (Agirre et al., 2012), and plagiarism detection (Gipp et al., 2011). In this work, we present a supervised lexical substitution system that, unlike the usual lexical sample supervised approaches, can produce substitutions for targets that are not contained in the training material. We reach this by using non-lexical features from heterogeneous evidence, includi"
N13-1133,P05-3014,0,0.0229166,"voting scheme, do not need training data per 1132 target. The combination of different signals, however, has to be done manually. Unsupervised systems that rely on distributional similarity (Thater et al., 2011) or topic models (Li et al., 2010) are single signals in this sense, and their development is guided by the performance and observations on standard datasets. Such signals, however, can also be kept simple avoiding any task-specific optimization and can be integrated in a single model for all words using a limited amount of training data and delexicalized features, as in Senselearner (Mihalcea and Csomai, 2005) for weakly supervised all-words disambiguation. This way, task specific development can be replaced by a machine learning component and the resulting model applies also to unseen words, similar to the knowledge-based approaches. 2.1 Full Lexical Substitution Systems Related works that address the lexical substitution problem according to the settings established by the English Lexical Substitution Task (McCarthy and Navigli, 2007) at Semeval 2007 (LexSub) typically employ a simple ranking strategy based on local n-gram frequencies and focus on finding an optimal source of possible substitutio"
N13-1133,D10-1114,0,0.00406824,"re used to characterize words according to their inverse selectional preference statistics for typical dependency relations. The representation of a word in its context is computed via combining the basic representation of a word with the inverse selectional preference vectors of its related words from the context. Ranking is done by comparing vectors of possible substitutions with the substitution target. Thater et al. (2010) took a similar approach but used second order co-occurrence vectors and report improved performance. An exemplar-based approach is presented by Erk and Pad´o (2010) and Reisinger and Mooney (2010b) to model word meaning with respect to its context: instead of representing the word and the context as separate vectors and combining them, a set of word occurrences in similar contexts is picked first, and then only these exemplars are used to represent the word in context. While this approach provides good results with relatively simple and transparent models, each occurrence of a word has a unique representation (that can only be computed at testing time), and it is computationally expensive to scale these models to a large number of examples. 1133 Dinu and Lapata (2010) used a bag of wo"
N13-1133,N10-1013,0,0.104571,"and a different learning setup: we train a model for contextualization, rather than to combine substitutions from several different resources. A recent work by Sinha and Mihalcea (2011) used an approach based on graph centrality to rank the candidates and achieved comparable performance to n-gram-frequency-based ranking. To summarize, the use of n-gram frequencies for ranking and WordNet as the (most appropriate single) source of synonyms is competitive to more complex solutions and provides a simple and strong lexical substitution system. This motivated the follow-up work by Chang and Clark (2010) to use WordNet and n-grams in a linguistic steganography application and this motivates us to use this method as our baseline. 2.2 Ranking Word Meaning in Context Another prominent line of related work focused solely on the accurate ranking of a pre-given set of possible synonyms, according to their plausibility as a substitution in a given context. Typically, lexical substitution data is used for evaluation purposes, taking the candidate substitutions directly from the test data. This choice is motivated by the assumption that better semantic models should rank nearsynonyms more accurately a"
N13-1133,R09-1073,0,0.285512,"Missing"
N13-1133,P10-1097,0,0.0860226,"Missing"
N13-1133,I11-1127,0,0.0486716,"Missing"
N13-1133,C98-2122,0,\N,Missing
N15-1098,J10-4006,0,0.065573,"okens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two w"
N15-1098,W11-2501,0,0.453212,"different goal in mind, affecting word-pair generation and annotation. For example, 2 Following Caron (2001), we used the square root √ of the eigenvalue matrix Σk for representing words: Mk = Uk Σk . 3 http://bitbucket.org/yoavgo/word2vecf 971 both of Baroni’s datasets are designed to capture hypernyms, while other datasets try to capture broader notions of lexical inference (e.g. causality). Table 1 provides metadata on each dataset, and the description below explains how each one was created. (Kotlerman et al., 2010) Manually annotated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailin"
N15-1098,E12-1004,0,0.750923,"y. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexic"
N15-1098,W09-0215,0,0.133534,"f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on context"
N15-1098,P14-1113,0,0.552171,"inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical anal"
N15-1098,S13-1035,0,0.0223919,"Missing"
N15-1098,C92-2082,0,0.839194,")) = (~x~y · x~s y~s ) 2 (~xx~s · ~y y~s ) 1−α 2 (4) While these methods reduce match error – match error = 0.618 · recall versus the previous regression curve of match error = 0.935 · recall – their overall performance is only incrementally better than that of linear methods (Table 5). This improvement is also, partially, a result of the nonlinearity introduced in these kernels. 974 7 The Limitations of Contextual Features A (de)motivating example can be seen in §4.2. A typical y often has such+1 as a dominant feature, whereas x tends to appear with such−2 . These features are relics of the Hearst (1992) pattern “y such as x”. However, contextual features of single words cannot capture the joint occurrence of x and y in that pattern; instead, they record only this observation as two independent features of different words. In that sense, contextual features are inherently handicapped in capturing relational information, requiring supervised methods to harness complementary information from more sophisticated features, such as textual patterns that connect x with y (Snow et al., 2005; Turney, 2006). Acknowledgements This work was supported by the Adolf Messer Foundation, the Google Research Aw"
N15-1098,S12-1047,0,0.0690378,"notated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a f"
N15-1098,P14-2050,1,0.0441489,"ntation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two words (x, y) and a label whether x entails y. Note that each dataset"
N15-1098,W14-1610,1,0.664954,"SS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; We"
N15-1098,P98-2127,0,0.433791,"ions over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets f"
N15-1098,C94-1049,0,0.186491,"Missing"
N15-1098,J07-2002,0,0.0122586,"ikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dat"
N15-1098,E14-1054,0,0.287818,"ds, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical analysis reveals that said methods ignore the interaction between x and y, explaining our empirical findings. We modify them accordingly by incorporating the similarity between"
N15-1098,C14-1097,0,0.772314,"sks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these super"
N15-1098,E14-4008,0,0.286564,"other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney an"
N15-1098,P93-1034,0,0.626283,",657 Table 1: Datasets evaluated in this work. 2.1 Word Representations We built 9 word representations over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows differe"
N15-1098,J06-3003,0,0.298018,"Missing"
N15-1098,W03-1011,0,0.661401,"as causality (f lu → f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, ba"
N15-1098,C14-1212,0,0.797224,"4) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), only x (~x), and only y (~y ). For each composition, we trained two types of classifiers, tuning hyperparameters with a validation set: logistic regression with L1 or L2 regularization, and SVM with a linear kernel or quadratic kernel. 3 Negative Results Based on the above setup, we present three negative empirical results, which challenge the claim that the methods presented in §2.3 are learning a relation between x and y. In addition to our setup, these results were also reproduced in preliminary experDataset Kotlerman 2010 Bless 2011 Baroni 2012 Turney 2014 Levy 2014 Lex"
N15-1098,C98-2122,0,\N,Missing
N16-1075,P08-2064,0,0.467422,"Missing"
N16-1075,biemann-etal-2008-asv,1,0.85745,"corpus-driven approaches and supervised approaches. Corpus-driven approaches are usually informed by a frequency list (Koehn and Knight, 2003), by a probabilistic model (Schiller, 2005), by parallel corpora (Koehn and Knight, 2003; Macherey et al., 2011) or by the existence of periphrases (i.e. reformulations) in large monolingual corpora (Holz and Biemann, 2008). As with other tasks, supervised approaches are usually superior to unsupervised approaches if sufficient training material is provided. A straightforward yet effective supervised decompounding system is contained in the ASV Toolbox (Biemann et al., 2008), which uses trie-based datastructures for recursively splitting compounds based on learned splits. Alfonseca 617 Proceedings of NAACL-HLT 2016, pages 617–622, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics et al. (2008) combine several signals, including web anchor text, in an SVM-based supervised splitter. A widely used German decompounder is JWordSplitter1 , which is based on word lists of compound parts as well as manually crafted blacklists and whitelists. The NL Splitter2 uses similar technology for Dutch compound decomposition. An unsupervised"
N16-1075,W15-5703,0,0.208498,"A widely used German decompounder is JWordSplitter1 , which is based on word lists of compound parts as well as manually crafted blacklists and whitelists. The NL Splitter2 uses similar technology for Dutch compound decomposition. An unsupervised approach is presented in (Koehn and Knight, 2003): out of several splits as given by matching parts of the compound to a vocabulary list, they pick the split with the highest geometric mean of word frequencies, which is entirely corpus-driven but ignores semantic relations between the compound and its parts. Another unsupervised system is proposed by Daiber et al. (2015). They propose an analogy-based approach, which relies on word embeddings. Decompounding is evaluated either intrinsically or in a task that benefits from it, e.g. IR (Monz and de Rijke, 2001), MT (Koehn and Knight, 2003; Macherey et al., 2011) or ASR (Adda-Decker and Adda, 2000; Ordelman et al., 2003). 3 Method The introduced method, called SECOS (SEmantic COmpound Splitter)3 , is based on the hypothesis that compounds are similar to their constituting word units. Our method is based on a distributional thesaurus (DT) that is computed, based on the distributional hypothesis (Harris, 1951), us"
N16-1075,R11-1058,0,0.148858,"al to generate splits based on all three candidate sets and use the geometric mean scoring as outlined above to select the best split as decomposition of a word. 4 Datasets For testing the performance of our method, we chose four datasets. The first dataset was manually labeled by Holz and Biemann (2008) and consists of 700 German nouns from different frequency bands. The second dataset consists of 158,653 nouns from the German newspaper magazine c’t6 and was created by Marek (2006). As third dataset we use a noun compound dataset of 54,571 nouns from GermaNet7 , which has been constructed by Henrich and Hinrichs (2011).8 While converting these datasets for the task of compound splitting, we do not separate words in the gold standard, which comprise of prepositions, e.g. the word Abgang (outflow) is not split into Ab-gang (off walk). To show the language independency of our method, we apply it to a 5 Whereas our method mostly does not assume language knowledge, we uppercase the first letter of each wi , when we apply our method on German texts. 6 http://heise.de/ct 7 available at: http://www.sfs.uni-tuebingen. de/lsd/documents/compounds/split_ compounds_from_GermaNet10.0.txt 8 We follow Schiller (2005) and r"
N16-1075,E03-1076,0,0.423122,"ntly outperforms two unsupervised baselines. For Dutch, our method performs only slightly below a rule-based optimized compound splitter. 1 2 Introduction Germanic and agglutinative languages (e.g. German, Swedish, Finnish, Korean) have a productive morphology that allows the formation of not spaceseparated compounds in a much larger extent than e.g. in English. The task of separating such compounds into their corresponding single word (sub-) units is called compound splitting or decompounding. Decompounding showed impact in several NLP applications, e.g. ASR (Adda-Decker and Adda, 2000), MT (Koehn and Knight, 2003) or IR (Monz and de Rijke, 2001), and is generally perceived as a crucial component for the processing of respective languages. However, most existing systems rely Related Work Approaches to automatic decompounding can be classified into corpus-driven approaches and supervised approaches. Corpus-driven approaches are usually informed by a frequency list (Koehn and Knight, 2003), by a probabilistic model (Schiller, 2005), by parallel corpora (Koehn and Knight, 2003; Macherey et al., 2011) or by the existence of periphrases (i.e. reformulations) in large monolingual corpora (Holz and Biemann, 20"
N16-1075,P11-1140,0,0.714702,"nding. Decompounding showed impact in several NLP applications, e.g. ASR (Adda-Decker and Adda, 2000), MT (Koehn and Knight, 2003) or IR (Monz and de Rijke, 2001), and is generally perceived as a crucial component for the processing of respective languages. However, most existing systems rely Related Work Approaches to automatic decompounding can be classified into corpus-driven approaches and supervised approaches. Corpus-driven approaches are usually informed by a frequency list (Koehn and Knight, 2003), by a probabilistic model (Schiller, 2005), by parallel corpora (Koehn and Knight, 2003; Macherey et al., 2011) or by the existence of periphrases (i.e. reformulations) in large monolingual corpora (Holz and Biemann, 2008). As with other tasks, supervised approaches are usually superior to unsupervised approaches if sufficient training material is provided. A straightforward yet effective supervised decompounding system is contained in the ASV Toolbox (Biemann et al., 2008), which uses trie-based datastructures for recursively splitting compounds based on learned splits. Alfonseca 617 Proceedings of NAACL-HLT 2016, pages 617–622, c San Diego, California, June 12-17, 2016. 2016 Association for Computati"
N16-1075,van-zaanen-etal-2014-development,0,0.195815,"Missing"
N18-3027,S16-1024,0,0.0247511,"lines. Data Preprocessing Before training, HTML tags, URLs and e-mails were removed using regular expressions, as early models showed a huge bias towards HR contact emails and job agencies that include boilerplate URLs in the job description footers. All special characters like non-alphabetical characters, interpunctuation and bullet points were removed. Initial semantic models required large vocabularies due to writing variations of the same word. For instance, the term Java occurs three times: Java, java and JAVA. Hence, we lowercase job posting texts and replace numbers with a placeholder (Abdelwahab and Elmaghraby, 2016). Finally, the document is stemmed using Snowball stemmer2 . 3.3 Offline Evaluation Setup The two jobs with the titles Java Developer, Hamburg and Java Backend Developer, Stuttgart are examples of two very similar job postings with different locations. Due to the location difference they fit to two different types of users: those who live close to Stuttgart and those close to Hamburg. For the creation of our dataset we consider the following: if there is no user co-interaction between two jobs, they will not be considered similar in the dataset. The same applies to similar jobs postings with l"
N18-3027,P14-2050,0,0.0172838,"require huge amounts of memory. Word2Vec (Mikolov et al., 2013) has become a standard method that builds dense vector representations, which are the weights of a neural network layer predicting neighboring words. To retrieve a document representation, we compute the average of all vectors of the words in the documents. Word2Vec was also used for recommender systems to re-rank items based on vector correlations (Musto et al., 2015; Ozsoy, 2016). A modification that allows the usage of predicting arbitrary context in order to compute word representation is named Word2VecF and was introduced by Levy and Goldberg (2014). Document embedding techniques like Doc2Vec (Le and Mikolov, 2014) assigns each document a single vector, which gets adjusted with respect to all words in the document and all document vectors in the dataset. In an attempt to reduce Doc2Vec complexity and training corpus size dependencies, Doc2VecC (Chen, 2017) uses the same architecture as Word2Vec’s, 3 Method We hypothesize that job offers are semantically similar if the words used in its description are semantically similar. In addition, metadata of job offers like e.g. location of employee, title or qualifications are relevant for similar"
N18-3027,W08-0906,0,0.0298005,"or document similarity exist, but do not focus on job postings. For the task of document similarity, the 20 Newsgroups (Lang, 1995) and TREC-AP (Lewis et al., 1996) datasets are commonly used. Here the task is to assign documents to a predefined category. Thus, the task is more related to document clustering than information retrieval of similar documents. Also related are semantic text similarity tasks, where two sentences have to be scored regarding their similarity with a score between 0 and 5 (Baudiˇs et al., 2016). Paraphrasing is another aspect that is important for document similarity. Bernhard and Gurevych (2008) introduced a dataset for paraphrasing both questions and answers in order to enhance the results for the information retrieval. Related work was done by Fazel-Zarandi and Fox (2009), who introduced a method for matching jobs with job seekers. Whereas this fits to the RecSys 2016 task, this does not cover job posting retrieval of similar jobs. Furthermore, supervised approaches exist that predict jobs to candidate users e.g. Poch et al. (2014). In addition, Kessler et al. (2008) introduced a dataset based on French job offers and presented a system for ranking relevant jobs to candidates based"
N18-3027,Q15-1016,0,0.0803905,"Missing"
N18-3027,poch-etal-2014-ranking,0,0.0267937,"heir similarity with a score between 0 and 5 (Baudiˇs et al., 2016). Paraphrasing is another aspect that is important for document similarity. Bernhard and Gurevych (2008) introduced a dataset for paraphrasing both questions and answers in order to enhance the results for the information retrieval. Related work was done by Fazel-Zarandi and Fox (2009), who introduced a method for matching jobs with job seekers. Whereas this fits to the RecSys 2016 task, this does not cover job posting retrieval of similar jobs. Furthermore, supervised approaches exist that predict jobs to candidate users e.g. Poch et al. (2014). In addition, Kessler et al. (2008) introduced a dataset based on French job offers and presented a system for ranking relevant jobs to candidates based on a jobs-tocandidates similarity metric. uation consisting of similar job postings based on user co-interactions. Then, we construct an evaluation metric based on the classification of similar and non-similar items. Testing multiple embedding models and weighting functions, the best performance is achieved when building embeddings based on the job description with an increased weight for words that appear in the job title. Finally, the model"
N18-3027,W15-1830,0,0.0483358,"Missing"
N18-3027,E17-1072,0,0.029318,"Missing"
N19-4017,H93-1012,0,0.68931,"Missing"
N19-4017,C16-1320,0,0.0506585,"Missing"
N19-4017,N18-5004,0,0.0495354,"orm of relevant documents and various 1 https://aminer.org/lab-datasets/ expertfinding/ 2 https://tides.umiacs.umd.edu/webtrec/ trecent/parsed_w3c_corpus.html 3 TextREtrieval Conference: https://trec.nist. gov/ 98 Proceedings of NAACL-HLT 2019: Demonstrations, pages 98–104 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics thors, and publication venues. The CareerMap6 (Wu et al., 2018), which is now a component of ArnetMiner, visualizes a scholar’s career trajectory, which is extracted from ArnetMiner’s publication database. The CL Scholar7 system (Singh et al., 2018) mines textual and network information for knowledge graph construction and question answering using natural language or keywords. CSSeer8 (Chen et al., 2013) is a keyphrasebased recommendation system for expert finding based on the CiteSeerX9 digital library and Wikipedia. It extracts keyphrases from the title and abstract of documents in CiteSeerX and utilizes this information to infer the author’s expertise. The Expert2B´ol`e system (Yang et al., 2009) features generic expert finding as well as b´ol`e search, which aims at identifying the top supervisors in a given field. The authors argue"
N19-4017,D18-2014,1,0.819836,"an author’s name. Note that not every author has a Wikidata or Google Scholar entry, and some authors have multiple entries. In total we count approximately 9K authors with a matching Google Scholar entity, and 14K authors with matching Wikidata entities, of which 1.5K authors can be linked to exactly one Wikidata entity. Our heuristic does not distinguish between Wikidata entities and shows them all, whereas only the first Google Scholar entity is selected. tions over time, h-index and i10-index, and more. Keywords are extracted for each document in the corpus using a keyword extractor tool (Wiedemann et al., 2018)11 , which provides results as a ranked list of keywords12 . In order to provide keywords for each author, the keywords of each document that an author has written are aggregated and ranked by document frequency. Lastly, the profile view shows information such as awards, educational degrees, employers (current and previous) as extracted from Wikidata. 3.2 The document generation model by Balog et al. (2012) is widely used as a baseline to compare expert finding methods. In their original paper, Balog et al. (2012) present two models: Model1, the candidate generation model and Model2, the docum"
P06-3002,W06-3812,1,0.654457,"ub-clusters. In contrast, unsupervised part-of-speech induction means the induction of the tag set, which implies finding the number of classes in an unguided way. 1.3 2.1 Let us consider a weighted, undirected graph G(V,E) (v∈V vertices, (vi,vj,wij)∈E edges with weights wij). Vertices represent entities (here: words); the weight of an edge between two vertices indicates their similarity. As the data here is collected in feature vectors, the question arises why it should be transformed into a graph representation. The reason is, that graph-clustering algorithms such as e.g. (van Dongen, 2000; Biemann 2006), find the number of clusters automatically 1 . Further, outliers are handled naturally in that framework, as they are represented as singleton nodes (without edges) and can be excluded from the clustering. A threshold s on similarity serves as a parameter to influence the number of non-singleton nodes in the resulting graph. For assigning classes, we use the Chinese Whispers (CW) graph-clustering algorithm, which has been proven useful in NLP applications as described in (Biemann 2006). It is time-linear with respect to the number of edges, making its application viable even for graphs with s"
P06-3002,C04-1052,0,\N,Missing
P06-3002,E03-1009,0,\N,Missing
P06-3002,E95-1020,0,\N,Missing
P09-2062,P06-3002,1,0.848394,"on here, however, lies in the comparison of the topology of the syntactic and semantic DSNs, which, to the best of our knowledge, has not been explored previously. 245 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245–248, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2 Network Construction The syntactic and semantic DSNs are constructed from a raw text corpus. This work is restricted to the study of English DSNs only1 . Syntactic DSN: We define our syntactic network in a similar way as previous works in unsupervised parts-of-speech induction (cf. (Sch¨utze, 1995; Biemann, 2006)): The most frequent 200 words in the corpus (July 2008 dump of English Wikipedia) are used as features in a word window of ±2 around the target words. Thus, each target word is described by an 800-dimensional feature vector, containing the number of times we observe one of the most frequent 200 words in the respective positions relative to the target word. In our experiments, we collect data for the most frequent 1000 and 5000 target words, arguing that all syntactic classes should be represented in those. A similarity measure between target words is defined by the cosine between the feature"
P09-2062,J93-1003,0,0.0424733,"e construction of this network is inspired by (Lin, 1998). Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al., 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Nouncoordination, NN-compound, Adj-Mod. These lexicalized relations act as features for the nouns. Verbs are recorded together with their subcategorization frame, i.e. the same verb lemmas in different subcat frames would be treated as if they were different verbs. We compute log-likelihood significance between features and target nouns (as in (Dunning, 1993)) and keep only the most significant 200 features per target word. Each feature f gets a feature weight that is inversely proportional to the logarithm of the number of target words it applies on. The similarity of two target nouns is then computed as the sum of the feature weights they share. For our analysis, we restrict the graph to the most frequent 5000 target common nouns and keep only the 200 highest weighted edges per target noun. Note that the degree of a node can Figure 1: The spectrum of the syntactic and semantic DSNs of 1000 nodes. still be larger than 200 if this node is containe"
P09-2062,E09-1067,1,0.803279,"actic and semantic distributional patterns of the words of a language? This study is an initial attempt to answer this fundamental and Spectral analysis is the backbone of several techniques, such as multi-dimensional scaling, principle component analysis and latent semantic analysis, that are commonly used in NLP. In recent times, there have been some work on spectral analysis of linguistic networks as well. Belkin and Goldsmith (2002) applied spectral analysis to understand the struture of morpho-syntactic networks of English words. The current work, on the other hand, is along the lines of Mukherjee et al. (2009), where the aim is to understand not only the principles of organization, but also the global topology of the network through the study of the spectrum. The most important contribution here, however, lies in the comparison of the topology of the syntactic and semantic DSNs, which, to the best of our knowledge, has not been explored previously. 245 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 245–248, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP 2 Network Construction The syntactic and semantic DSNs are constructed from a raw text corpus. This work is restricted t"
P09-2062,P02-1035,0,0.012507,"In our experiments, we collect data for the most frequent 1000 and 5000 target words, arguing that all syntactic classes should be represented in those. A similarity measure between target words is defined by the cosine between the feature vectors. The syntactic graph is formed by inserting the target words as nodes and connecting nodes with edge weights equal to their cosine similarity if this similarity exceeds a threshold t = 0.66. Semantic DSN: The construction of this network is inspired by (Lin, 1998). Specifically, we parsed a dump of English Wikipedia (July 2008) with the XLE parser (Riezler et al., 2002) and extracted the following dependency relations for nouns: Verb-Subject, Verb-Object, Nouncoordination, NN-compound, Adj-Mod. These lexicalized relations act as features for the nouns. Verbs are recorded together with their subcategorization frame, i.e. the same verb lemmas in different subcat frames would be treated as if they were different verbs. We compute log-likelihood significance between features and target nouns (as in (Dunning, 1993)) and keep only the most significant 200 features per target word. Each feature f gets a feature weight that is inversely proportional to the logarithm"
P09-2062,E95-1020,0,0.208277,"Missing"
P09-2062,nath-etal-2008-unsupervised,1,\N,Missing
P09-2062,W02-0605,0,\N,Missing
P09-2062,P98-2127,0,\N,Missing
P09-2062,C98-2122,0,\N,Missing
P13-4001,J96-2004,0,0.449292,"de of Figure 3) which can be used to navigate to the sentences for curation. • Annotation layers: Annotators usually work on one or two annotations layers, such as part-of-speech and dependency or named entity annotation. Overloading the annotation page by displaying all annotation layers makes the annotation and visualization process slower. WebAnno provides an option to configure visible/editable annotation layers. 3.1.5 Monitoring WebAnno provides a monitoring component, to track the progress of a project. The project manager can check the progress and compute agreement with Kappa and Tau (Carletta, 1996) measures. The progress is visualized using a matrix of annotators and documents displaying which documents the annotators have marked as complete and which documents the curator adjudicated. Figure 4 shows the project progress, progress of individual annotator and completion statistics. • Immediate persistence: Every annotation is sent to the back end immediately and persisted there. An explicit interaction by the user to save changes is not required. 3.1.3 Workflow WebAnno implements a simple workflow to track the state of a project. Every annotator works on a 3 Figure 3: Curation user inter"
P13-4001,heid-etal-2010-corpus,0,0.152534,"annotation project is conducted by a project manager (cf. Figure 1) in a project definition form. It supports creating a project, loading un-annotated or preannotated documents in different formats2 , adding annotator and curator users, defining tagsets, and configuring the annotation layers. Only a project manager can administer a project. Figure 2 illustrates the project definition page with the tagset editor highlighted. System Architecture of WebAnno 1 Available for download at (this paper is based on v0.3.0): webanno.googlecode.com/ 2 Formats: plain text, CoNLL (Nivre et al., 2007), TCF (Heid et al., 2010), UIMA XMI (Ferrucci and Lally, 2004) The overall architecture of WebAnno is depicted in Figure 1. The modularity of the architecture, 2 Figure 2: The tagset editor on the project definition page 3.1.2 Annotation Annotation is carried out with an adapted version of the brat editor, which communicates with the server via Ajax (Wang et al., 2008) using the JSON (Lin et al., 2012) format. Annotators only see projects they are assigned to. The annotation page presents the annotator different options to set up the annotation environment, for customization: separate version of the document, which is"
P13-4001,N03-4009,0,0.0575767,"is carried out with locally downloaded software. An interface to crowdsourcing platforms is missing. The GATE Teamware system is heavily targeted towards template-based information extraction. It sets a focus on the integration of automatic annotation components rather than on the interface for manual annotation. Besides, the overall application is rather complex for average users, requires considerable training and does not offer an alternative simplified interface as it would be required for crowdsourcing. General-purpose annotation tools like MMAX2 (M¨uller and Strube, 2006) or WordFreak (Morton and LaCivita, 2003) are not web-based and do not provide annotation project management. They are also not sufficiently flexible regarding different annotation layers. The same holds for specialized tools for single annotation layers, which we cannot list here for the sake of brevity. With the brat rapid annotation tool (Stenetorp et al., 2012), for the first time a web-based opensource annotation tool was introduced, which supports collaborative annotation for multiple annotation layers simultaneously on a single copy of the document, and is based on a client-server architecture. However, the current version of"
P13-4001,E12-2021,0,0.344972,"Missing"
P13-4001,D07-1096,0,\N,Missing
P14-1096,W06-3812,1,0.756833,"ected by comparing its senses obtained from two different time periods. Since we aim to detect this change automatically, we require distributional representations corresponding to word senses for different time periods. We, therefore, utilize the basic hypothesis of unsupervised sense induction to induce the sense clusters over various time periods and then compare these clusters to detect sense change. The basic premises of the ‘unsupervised sense induction’ are briefly described below. 4.1 Unsupervised sense induction We use the co-occurrence based graph clustering framework introduced in (Biemann, 2006). The algorithm proceeds in three basic steps. Firstly, a co-occurrence graph is created for every target word found in DT. Next, the neighbourhood/ego graph is clustered using the Chinese Whispers (CW) algorithm (see (McAuley and Leskovec, 2012) for similar approaches). The algorithm, in particular, produces a set of clusters for each target word by decomposing its open neighborhood. We hypothesize that each different cluster corresponds to a particular sense of the target word. For a detailed description, the reader is referred to (Biemann, 2011). If a word undergoes sense change, this can b"
P14-1096,W09-3401,0,0.0195066,"ouns, verbs and adjectives as opposed to events that are characterized mostly by named entities. Other similar works on dynamic topic modelling can be found in (Blei and Lafferty, 2006; Wang and McCallum, 2006). Google books n-gram viewer1 is a phrase-usage graphing tool which charts the yearly count of selected letter combinations, words, or phrases as found in over 5.2 million digitized books. It only reports frequency of word usage over the years, but does not give any correlation among them as e.g., in (Heyer et al., 2009), and does not analyze their senses. A few approaches suggested by (Bond et al., 2009; P¨aa¨ kk¨o and Lind´en, 2012) attempt to augment WordNet synsets primarily using methods of annotation. Another recent work by Cook et al. (2013) attempts to induce word senses and then identify novel senses by comparing two different corpora: the “focus corpora” (i.e., a recent version of the corpora) and the “reference corpora” (older version of the corpora). However, this method is limited as it only considers two time points to 1021 1 https://books.google.com/ngrams identify sense changes as opposed to our approach which is over a much larger timescale, thereby, effectively allowing us t"
P14-1096,P12-2051,0,0.245702,"Missing"
P14-1096,D13-1089,1,0.848106,"ized books made available through the Google Book project (Goldberg and Orwant, 2013). The Google Book syntactic n-grams dataset provides dependency fragment counts by the years. However, instead of using the plain syntactic n-grams, we use a far richer representation of the data in the form of a distributional thesaurus (Lin, 1997; Rychl´y and Kilgarriff, 2007). In specific, we prepare a distributional thesaurus (DT) for each of the time periods separately and subsequently construct the required networks. We briefly outline the procedure of thesauri construction here referring the reader to (Riedl and Biemann, 2013) for further details. In this approach, we first extract each word and a set of its context features, which are formed by labeled and directed dependency parse edges as provided in the dataset. Following this, we compute the frequencies of the word, the context and the words along with their context. Next we calculate the lexicographer’s mutual information LMI (Kilgarriff, 2004) between a word and its features and retain only the top 1000 ranked features for every word. Finally, we construct the DT network as follows: each word is a node in the network and the edge weight between two nodes is"
P14-1096,S13-1035,0,0.0544826,"ncial institution” and the “shore of a river.” Automatic discovery and disambiguation of word senses from a given text is an important and challenging problem which has been extensively studied in the literature (Jones, 1986; Ide and Veronis, 1998; Sch¨utze, 1998; Navigli, 2009). However, another equally important aspect that has not been so far well investigated corresponds to one or more changes that a word might undergo in its sense. This particular aspect is getting increasingly attainable as more and more time-varying text data become available in the form of millions of digitized books (Goldberg and Orwant, 2013) gathered over the last centuries. As a motivating example one could consider the word “sick” – while according to the standard English dictionaries the word is normally used to refer to some sort of illness, a new meaning of “sick” referring to something that is “crazy” or “cool” is currently getting popular in the English vernacular. This change is further interesting because while traditionally “sick” has been associated to something negative in general, the current meaning associates positivity with it. In fact, a rock band by the name of “Sick Puppies” has been founded which probably is i"
P14-1096,riedl-etal-2014-distributed,1,0.877385,"Missing"
P14-1096,J98-1001,0,0.0387463,"scribe the datasets and outline the process of co-occurrence graph construction. In Section 4 we present an approach based on graph clustering to identify the time-varying sense clusters and in Section 5 we present the split-merge based approach for tracking word sense changes. Evaluation methods are summarized in Section 6. Finally, conclusions and further research directions are outlined in Section 7. 2 Related work Word sense disambiguation as well as word sense discovery have both remained key areas of research right from the very early initiatives in natural language processing research. Ide and Veronis (1998) present a very concise survey of the history of ideas used in word sense disambiguation; for a recent survey of the state-of-the-art one can refer to (Navigli, 2009). Some of the first attempts to automatic word sense discovery were made by Karen Sp¨arck Jones (1986); later in lexicography, it has been extensively used as a pre-processing step for preparing mono- and multi-lingual dictionaries (Kilgarriff and Tugwell, 2001; Kilgarriff, 2004). However, as we have already pointed out that none of these works consider the temporal aspect of the problem. In contrast, the current study, is inspire"
P14-1096,P97-1009,0,0.0334398,"hm in order to test its overall accuracy and performance. 3 Datasets and graph construction In this section, we outline a brief description of the dataset used for our experiments and the graph construction procedure. The primary source of data have been the millions of digitized books made available through the Google Book project (Goldberg and Orwant, 2013). The Google Book syntactic n-grams dataset provides dependency fragment counts by the years. However, instead of using the plain syntactic n-grams, we use a far richer representation of the data in the form of a distributional thesaurus (Lin, 1997; Rychl´y and Kilgarriff, 2007). In specific, we prepare a distributional thesaurus (DT) for each of the time periods separately and subsequently construct the required networks. We briefly outline the procedure of thesauri construction here referring the reader to (Riedl and Biemann, 2013) for further details. In this approach, we first extract each word and a set of its context features, which are formed by labeled and directed dependency parse edges as provided in the dataset. Following this, we compute the frequencies of the word, the context and the words along with their context. Next we"
P14-1096,P07-2011,0,0.138454,"Missing"
P14-1096,J98-1004,0,0.084685,"Missing"
P14-5016,W09-0715,0,0.0241276,"Missing"
P14-5016,W07-1501,0,0.00902618,"(Bontcheva et al., 2013) automation component is most similar to our work. It is based either on plugins and externally trained classification models, or uses web services. Thus, it is highly task specific and requires extensive configuration. The automatic annotation suggestion component in our tool, in contrast, is easily configurable and adaptable to different annotation tasks and allows the use of annotations from the current annotation project. 2 Custom annotation layers Generic annotation data models are typically directed graph models (e.g. GATE, UIMA CAS (G¨otz and Suhre, 2004), GrAF (Ide and Suderman, 2007)). In addition, an annotation schema defines possible kinds of annotations, their properties and relations. While these models offer great expressiveness and flexibility, it is difficult to adequately transfer their power into a convenient annotation editor. For example, one schema may prescribe that the part-of-speech tag is a property on a Token annotation, another one may prescribe that the tag is a separate annotation, which is linked to the token. An annotator should not be exposed to these details in the UI and should be able to just edit a part-of-speech tag, ignorant of the internal re"
P14-5016,N03-4009,0,0.188268,"Missing"
P14-5016,petrov-etal-2012-universal,0,0.0283548,"Missing"
P14-5016,P13-2097,0,0.062934,"Missing"
P14-5016,benikova-etal-2014-nosta,1,\N,Missing
P14-5016,E12-2021,0,\N,Missing
P14-5016,P13-4001,1,\N,Missing
P15-4018,C92-2082,0,0.316149,"g in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8 . 4 4.2 In the demonstrator, users can enter sentences or phrases, run different holing operations on the input and browse distributional models. The demonstrator can be accessed online12 . Figure 2 shows the application layout. First, the user can decide on the model, which consists of a corpus and a holing operation, and select it via the dropdown holing operation selector (3). Then, the user enters a sentence in the text field (1) and activates the processing by clicking the Parse button (2). The holing output"
P15-4018,P98-2127,0,0.708515,"els Eugen Ruppert and Manuel Kaufmann and Martin Riedl and Chris Biemann FG Language Technology Computer Science Department, Technische Universit¨at Darmstadt, Hochschulsstrasse 10, D-62489 Darmstadt, Germany {eugen.ruppert,riedl,biem}@cs.tu-darmstadt.de, mtk@kisad.de Abstract While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: This paper introduces a web-based visualization framework for graph-based distributional semantic models. The visualization supports a wide range of data structures, including term similarities, similarities of contexts, support of multiword expressions, sense clusters for terms and sense labels. In contrast to other browsers of semantic resources, our visualization accepts input sentences, which are subsequently processed with languageindependent or language-dependent ways to comp"
P15-4018,J10-4006,0,0.0352303,"computation of the models, is fully available open source under a permissive license. Introduction Statistical semantics has emerged as a field of computational linguistics, aiming at automatically computing semantic representations for words, sentences and phrases from (large) corpora. While early approaches to distributional semantics were split into symbolic, graph-based approaches (Lin, 1998) and vector-based approaches (Sch¨utze, 1993), recent trends have mainly concentrated on optimizing the representation of word meanings in vector spaces and how these account for compositionality (cf. Baroni and Lenci (2010); Turney and Pantel (2010)). 103 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 103–108, c Beijing, China, July 26-31, 2015. 2015 ACL and AFNLP 2 Related Work A novel aspect of J O B IM V IZ is that it incorporates several different aspects of symbolic methods (distributional thesaurus, context feature scores, sense clusters), and all of these methods are derived from the input corpus alone, without relying on external resources, which are not available for every language/domain. Furthermore, we provide domain-based sense clusterings with is-a labels (cf. Figure 4), which is not p"
P15-4018,W13-5002,1,0.875848,"Missing"
P15-4018,C02-1007,0,0.0627186,"Missing"
P15-4018,W06-3812,1,0.828312,"e hand, domain- and languageindependent similarity computations using general holing operations, while on the other hand allowing complex, language-specific processing in the holing operations for higher quality models (e.g. using parsing and lemmatization). The second part consists of the similarity computation, which relies on MapReduce (Dean and Ghemawat, 2004) for scalability and employs efficient pruning strategies to keep runtime feasible even for very large corpora. After the computation of similarities, sense clusters for each term are computed using Chinese Whispers graph clustering (Biemann, 2006), as described in Biemann et al. (2013). Furthermore, the sense clusters are also labeled with hypernyms (is-a relations) derived from Hearst patterns (Hearst, 1992), implemented using the UIMA Ruta (Kluegl et al., 2014) pattern matching engine8 . 4 4.2 In the demonstrator, users can enter sentences or phrases, run different holing operations on the input and browse distributional models. The demonstrator can be accessed online12 . Figure 2 shows the application layout. First, the user can decide on the model, which consists of a corpus and a holing operation, and select it via the dropdown ho"
P15-4018,W02-1029,0,0.0336922,"ppert and Manuel Kaufmann and Martin Riedl and Chris Biemann FG Language Technology Computer Science Department, Technische Universit¨at Darmstadt, Hochschulsstrasse 10, D-62489 Darmstadt, Germany {eugen.ruppert,riedl,biem}@cs.tu-darmstadt.de, mtk@kisad.de Abstract While dense vector representations, obtained by singular value decomposition (cf. Rapp (2002)) or neural embeddings (Mikolov et al., 2010), have gained popularity due to successes in modelling semantic and relational similarity, we propose to revisit graph-based approaches to distributional semantics in the tradition of Lin (1998), Curran (2002) and Biemann and Riedl (2013) – at least as an additional alternative – for the following reasons: This paper introduces a web-based visualization framework for graph-based distributional semantic models. The visualization supports a wide range of data structures, including term similarities, similarities of contexts, support of multiword expressions, sense clusters for terms and sense labels. In contrast to other browsers of semantic resources, our visualization accepts input sentences, which are subsequently processed with languageindependent or language-dependent ways to compute term-contex"
P15-4018,de-marneffe-etal-2006-generating,0,\N,Missing
P15-4018,C98-2122,0,\N,Missing
P16-1012,cholakov-etal-2014-lexical,1,0.810523,"er equivalent datasets have been produced for other languages. Evalita 2009 posed a lexical substitution task for Italian (Toral, 2009, &quot;EL09&quot;). Participants were free to obtain a list of substitution candidates in any way, most commonly Italian WordNet1 was used. A WeightedSense baseline provided by the organizers proved very strong, as all systems scored below it. This baseline is obtained by aggregating differently weighted semantic relations from multiple human-created lexical resources (Ruimy et al., 2002). A German version of the lexical substitution task was organized at GermEval 2015 (Cholakov et al., 2014; Miller et al., 2015, &quot;GE15&quot;). Likewise, WeightedSense was able to beat both of two participating systems in oot evaluations (Miller et al., 2015). A variation for cross-lingual lexical substitution was proposed by Mihalcea et al. (2010), in which substitute words are required in a different language than the source sentence. The sentence context as well as the target word were given in English, whereas the substitute words should be provided in Spanish (annotators were fluent in both languages). This variant is motivated by direct application in Machine Translation systems, or as an aid for"
P16-1012,S12-1059,1,0.818327,"g for Supervised Lexical Substitution Gerold Hintz and Chris Biemann Research Training Group AIPHES / FG Language Technology Computer Science Department, Technische Universität Darmstadt {hintz,biem}@lt.informatik.tu-darmstadt.de Abstract utilized in downstream tasks that require paraphrasing of input text. Examples of such use cases include text simplification, text shortening, and summarization. Furthermore, lexical substitution can be regarded as an alternative to WSD in downstream tasks requiring word disambiguation. For example, it was successfully applied in Semantic Textual Similarity (Bär et al., 2012). A given list of substitution words can be regarded as a vector representation modeling the meaning of a word in context. As opposed to WSD systems, this is not reliant on a predefined sense inventory, and therefore does not have to deal with issues of coverage, or sense granularity. On the other hand, performing lexical substitution is more complex than WSD, as a system has to both generate and rank a list of substitution candidates per instance. Over the last decade, a number of shared tasks in lexical substitution has been organized and a wide range of methods have been proposed. Although"
P16-1012,D08-1094,0,0.0962337,"3 0.69 0.73 Table 1: Degree of variation in gold answers supervised approaches for determining appropriate substitutes. For the English SE07 task, systems mostly consider substitution candidates from WordNet (Fellbaum, 1998) and cast lexical substitution into a ranking task. Experiments may also be performed by pooling the set of candidates from the gold data, evaluating a pure ranking variant. Early approaches use a contextualized word instance representation and rank candidates according to their similarity to this representation. Effective representations are syntactic vector space models (Erk and Padó, 2008; Thater et al., 2011), which use distributional sparse vector representations based on the syntactic context of words. Performance improvement could be shown for different models, including the use of graph centrality algorithms on directional word similarity graphs (Sinha and Mihalcea, 2011), and clustering approaches on word instance representations (Erk and Padó, 2010). Multiple systems have built upon the distributional approach. Extensions include the use of LDA topic models (Ó Séaghdha and Korhonen, 2014), and probabilistic graphical models (Moon and Erk, 2013). The current state of the"
P16-1012,P10-2017,0,0.152488,"a pure ranking variant. Early approaches use a contextualized word instance representation and rank candidates according to their similarity to this representation. Effective representations are syntactic vector space models (Erk and Padó, 2008; Thater et al., 2011), which use distributional sparse vector representations based on the syntactic context of words. Performance improvement could be shown for different models, including the use of graph centrality algorithms on directional word similarity graphs (Sinha and Mihalcea, 2011), and clustering approaches on word instance representations (Erk and Padó, 2010). Multiple systems have built upon the distributional approach. Extensions include the use of LDA topic models (Ó Séaghdha and Korhonen, 2014), and probabilistic graphical models (Moon and Erk, 2013). The current state of the art combines a distributional model with the use of n-gram language models (Melamud et al., 2015a). They define the context vector of each word in a background corpus as a substitute vector, which is a vector of suitable filler words for the current n-gram context. They then obtain a contextualized paraphrase vector by computing a weighted average of substitute vectors in"
P16-1012,S07-1029,0,0.0358721,"aches can be extended to generalize across multiple languages without much effort. Training a supervised system on different language data emphasizes that the learned model is sufficiently generic to be language independent. Our feature space constructed from heterogeneous evidence consists of many features that perform relatively weakly on their own. The resulting ranking model captures redundancy between these signals. Finally, Table 6 shows our results in comparison to previous work. Note that we omit some participating systems from the original SE07 task. The reason we did not list IRST2 (Giuliano et al., 2007) is that for out-of-ten results, the system outputs the same substitute multiple times and the evaluation scheme gives credit for each copy of the substitute. Our (and other) systems do not tamper with the metric in this way, and only yield a set of substitutes. UNT (Hassan et al., 2007) uses a much richer set of knowledge bases, not all of them easily available, to achieve slightly better oot scores. From our experiments, we list both a model trained per language, as well as a universal model trained on all data. The latter beats nearly all other approaches on the full lexical substitution ta"
P16-1012,D07-1086,0,0.0316756,"Missing"
P16-1012,W97-0802,0,0.174557,"as yielded by a classifier model. We have tried multiple classifiers but have found no significant improvement over a maximum entropy baseline3 . Our second setup is a learningto-rank framework, adapted from (Szarvas et al., 2013b). Here, we are not restricted to a pointwise ranking model, but consider pairwise and listwise models4 . 4.1 Candidate selection We confirm earlier research (Sinha and Mihalcea, 2009) on the high quality of selecting candidates from lexical resources. We thus base our candidate selection on prevalently used resources: WordNet (Fellbaum, 1998) for English, GermaNet (Hamp and Feldweg, 1997) for German and MultiWordNet (Pianta et al., 2002) for Italian. For all resources, we consider all possible senses for a given target word and obtain all synonyms, hypernyms and hyponyms and their transitive hull. Thus, for the hypernymy and hyponymy relation, we follow the respective edges in the graph collecting all nodes (synsets) along the path. For each synset, we extract all lemmas as substitution candidates. Although restricting candidates incurs a relatively low upper bound on system recall, we still obtain best results using this rather conservative filter. Table 2 shows the upper bou"
P16-1012,S07-1091,0,0.096596,"s of many features that perform relatively weakly on their own. The resulting ranking model captures redundancy between these signals. Finally, Table 6 shows our results in comparison to previous work. Note that we omit some participating systems from the original SE07 task. The reason we did not list IRST2 (Giuliano et al., 2007) is that for out-of-ten results, the system outputs the same substitute multiple times and the evaluation scheme gives credit for each copy of the substitute. Our (and other) systems do not tamper with the metric in this way, and only yield a set of substitutes. UNT (Hassan et al., 2007) uses a much richer set of knowledge bases, not all of them easily available, to achieve slightly better oot scores. From our experiments, we list both a model trained per language, as well as a universal model trained on all data. The latter beats nearly all other approaches on the full lexical substitution task, despite not being optimized for a single language. Although omission of MWEs is common practice for SE07, it is unclear if this was 9 State of the art baseline, according to previous reported results, c.f. Table 6 125 English German Italian SemEval ’07 best-P GAP best-P GAP best-P GA"
P16-1012,D14-1113,0,0.0316829,"E07 task had a third evaluation setting MWE, in which systems had to correctly identify which target words were part of a multiword expression. 1 Italian WordNet has later been migrated into MultiWordNet (MWN), which is used in this work. 119 dataset lean model for lexical substitution based solely on syntactic word embeddings. As we leverage this model as a feature in our approach, we will elaborate on this in Section 4. Another approach for applying word embeddings to lexical substitution is their direct extension with multiple word senses, which can be weighted according to target context (Neelakantan et al., 2014). Biemann (2013) first showed that the lexical substitution task can be solved very well when sufficient amount of training data is collected per target. An approach based on crowdsourcing human judgments achieved the best performance on the S07 dataset to day. However, judgments had to be collected for each lexical item, and as a consequence the approach can not scale to an open vocabulary. As an alternative to per-word supervised systems trained on target instances per lexeme, allwords systems aim to generalize over all lexical items. Szarvas et al. (2013a) proposed such a system by using de"
P16-1012,P14-2050,0,0.0517199,"y, occurring between t and s. We have also experimented with graded variants for transitive relations, such as encoding n-th level hypernymy, but have not observed any gain from this feature variation. • sum of salience score of context features of s overlapping with the sentence context • binary occurrence of s in top-k similar words of t (k =100, 200) With the exception of the last feature, these measures are scaled to [0, 1] over the set of all substitute candidates. 4.4 Syntactic word embeddings We adapt the unsupervised approach by (Melamud et al., 2015a) as a set of features. We follow (Levy and Goldberg, 2014) to construct dependency-based word embeddings; we obtain syntactic contexts by running a syntactic dependency parser6 , and computing word embeddings using dependency edges as context features7 . The resulting dense vector representations for words and context live within the same vector space. We compute the semantic similarity between a target and a substitute word from the cosine similarity in the word embedding space, as well as the first-order target-to-context similarity. For a given target word t and substitute s, let Ct be the syntactic context of t and c ∈ Ct a single context – i.e."
P16-1012,J14-3005,0,0.0201452,"ilarity to this representation. Effective representations are syntactic vector space models (Erk and Padó, 2008; Thater et al., 2011), which use distributional sparse vector representations based on the syntactic context of words. Performance improvement could be shown for different models, including the use of graph centrality algorithms on directional word similarity graphs (Sinha and Mihalcea, 2011), and clustering approaches on word instance representations (Erk and Padó, 2010). Multiple systems have built upon the distributional approach. Extensions include the use of LDA topic models (Ó Séaghdha and Korhonen, 2014), and probabilistic graphical models (Moon and Erk, 2013). The current state of the art combines a distributional model with the use of n-gram language models (Melamud et al., 2015a). They define the context vector of each word in a background corpus as a substitute vector, which is a vector of suitable filler words for the current n-gram context. They then obtain a contextualized paraphrase vector by computing a weighted average of substitute vectors in the background corpus, based on their similarity to the current target instance. In contrast to traditional sparse vector representations obt"
P16-1012,S07-1009,0,0.705747,"ution task and datasets. Section 3 shows related work of systems addressing each of these tasks. In Section 4 we describe our method for building a supervised system capable of transfer learning. Section 5 shows our experimental results and discussion. Finally in Section 6 we give a conclusion and outlook to future work. 2 ists a larger crowd-sourced dataset of 1012 nouns (Biemann, 2013, &quot;TWSI&quot;), as well as an all-words dataset in which all words in each sentence are annotated with lexical expansions (Kremer et al., 2014). Evaluation of lexical substitution adheres to metrics defined by SE07 (McCarthy and Navigli, 2007), who provide two evaluation settings2 ; best evaluating only a system’s “best guess” of a single target substitute and oot, an unordered evaluation of up to ten substitutes. Thater et. al (2009) proposed to use Generalized Average Precision (GAP), to compare an output ranking rather than unordered sets of substitutes. Lexical substitution datasets and evaluation The lexical substitution task was first defined at SemEval 2007 (McCarthy and Navigli, 2007, &quot;SE07&quot;). A lexical sample of target word is selected from different word classes (nouns, verbs, and adjectives). Through annotation, a set of"
P16-1012,petrov-etal-2012-universal,0,0.0920418,"Missing"
P16-1012,N15-1050,0,0.112425,"last decade, a number of shared tasks in lexical substitution has been organized and a wide range of methods have been proposed. Although many approaches are in fact languageindependent, most existing work is tailored to a single language and dataset. In this work, we investigate lexical substitution as a multilingual task, and report experimental results for English, German and Italian datasets. We consider a supervised approach to lexical substitution, which casts the task as a ranking problem (Szarvas et al., 2013b). We adapt state-of-the-art unsupervised features (Biemann and Riedl, 2013; Melamud et al., 2015a) in a delexicalized ranking framework and perform transfer learning experiments by training a ranker model from a different language. Finally, we demonstrate the utility of aggregating data from different languages and train our model on this single multilingual dataset. We are able to improve the state of the art for the full task on all datasets. The remainder of this paper is structured as follows. In Section 2 we elaborate on the lexical subWe propose a framework for lexical substitution that is able to perform transfer learning across languages. Datasets for this task are available in a"
P16-1012,W15-1501,0,0.136979,"last decade, a number of shared tasks in lexical substitution has been organized and a wide range of methods have been proposed. Although many approaches are in fact languageindependent, most existing work is tailored to a single language and dataset. In this work, we investigate lexical substitution as a multilingual task, and report experimental results for English, German and Italian datasets. We consider a supervised approach to lexical substitution, which casts the task as a ranking problem (Szarvas et al., 2013b). We adapt state-of-the-art unsupervised features (Biemann and Riedl, 2013; Melamud et al., 2015a) in a delexicalized ranking framework and perform transfer learning experiments by training a ranker model from a different language. Finally, we demonstrate the utility of aggregating data from different languages and train our model on this single multilingual dataset. We are able to improve the state of the art for the full task on all datasets. The remainder of this paper is structured as follows. In Section 2 we elaborate on the lexical subWe propose a framework for lexical substitution that is able to perform transfer learning across languages. Datasets for this task are available in a"
P16-1012,ruimy-etal-2002-clips,0,0.0423486,"g a ranking of substitutes. The use of SE07 has become a de-facto standard for system comparison, however equivalent datasets have been produced for other languages. Evalita 2009 posed a lexical substitution task for Italian (Toral, 2009, &quot;EL09&quot;). Participants were free to obtain a list of substitution candidates in any way, most commonly Italian WordNet1 was used. A WeightedSense baseline provided by the organizers proved very strong, as all systems scored below it. This baseline is obtained by aggregating differently weighted semantic relations from multiple human-created lexical resources (Ruimy et al., 2002). A German version of the lexical substitution task was organized at GermEval 2015 (Cholakov et al., 2014; Miller et al., 2015, &quot;GE15&quot;). Likewise, WeightedSense was able to beat both of two participating systems in oot evaluations (Miller et al., 2015). A variation for cross-lingual lexical substitution was proposed by Mihalcea et al. (2010), in which substitute words are required in a different language than the source sentence. The sentence context as well as the target word were given in English, whereas the substitute words should be provided in Spanish (annotators were fluent in both lang"
P16-1012,S10-1002,0,0.0165642,"Italian WordNet1 was used. A WeightedSense baseline provided by the organizers proved very strong, as all systems scored below it. This baseline is obtained by aggregating differently weighted semantic relations from multiple human-created lexical resources (Ruimy et al., 2002). A German version of the lexical substitution task was organized at GermEval 2015 (Cholakov et al., 2014; Miller et al., 2015, &quot;GE15&quot;). Likewise, WeightedSense was able to beat both of two participating systems in oot evaluations (Miller et al., 2015). A variation for cross-lingual lexical substitution was proposed by Mihalcea et al. (2010), in which substitute words are required in a different language than the source sentence. The sentence context as well as the target word were given in English, whereas the substitute words should be provided in Spanish (annotators were fluent in both languages). This variant is motivated by direct application in Machine Translation systems, or as an aid for human-based translation. There also exDataset comparison The proposed lexical substitution datasets (SE07, EL09, GE15) differ in their degree of ambiguity of target items. If a dataset contains mostly target words that are unambiguous, su"
P16-1012,R09-1073,0,0.0174753,"ach substitute to obtain a score in (0, 1] if a substitute s occurs in the gold data, 0 otherwise. The ranking of substitutes per target is obtained by considering the posterior likelihood of the positive label as yielded by a classifier model. We have tried multiple classifiers but have found no significant improvement over a maximum entropy baseline3 . Our second setup is a learningto-rank framework, adapted from (Szarvas et al., 2013b). Here, we are not restricted to a pointwise ranking model, but consider pairwise and listwise models4 . 4.1 Candidate selection We confirm earlier research (Sinha and Mihalcea, 2009) on the high quality of selecting candidates from lexical resources. We thus base our candidate selection on prevalently used resources: WordNet (Fellbaum, 1998) for English, GermaNet (Hamp and Feldweg, 1997) for German and MultiWordNet (Pianta et al., 2002) for Italian. For all resources, we consider all possible senses for a given target word and obtain all synonyms, hypernyms and hyponyms and their transitive hull. Thus, for the hypernymy and hyponymy relation, we follow the respective edges in the graph collecting all nodes (synsets) along the path. For each synset, we extract all lemmas a"
P16-1012,N13-1090,0,0.0334583,"as a substitute vector, which is a vector of suitable filler words for the current n-gram context. They then obtain a contextualized paraphrase vector by computing a weighted average of substitute vectors in the background corpus, based on their similarity to the current target instance. In contrast to traditional sparse vector representations obtained through distributional methods, a recent trend is the use of low-dimensional dense vector representations. The use of such vector representations or word embeddings has been popularized by the continuous bag-of-words (CBOW) and Skip-gram model (Mikolov et al., 2013a). Melamud et al. (2015b) show a simple and knowledge4 Method description We subdivide lexical substitution into two subtasks; candidate selection and ranking. For a given target t, we consider a list of possible substi120 tutes s ∈ Ct , where Ct is a static per-target candidate list. Our method is agnostic to the creation of this static resource, which can be obtained either by an unsupervised similarity-based approach, or from a lexical resource. In particular, candidates obtained at this stage do not disambiguate possible multiple senses of t, and are filtered and ordered in the ranking st"
P16-1012,N13-1133,1,0.915777,"Missing"
P16-1012,W09-2506,0,0.434731,"Missing"
P16-1012,I11-1127,0,0.135767,"Degree of variation in gold answers supervised approaches for determining appropriate substitutes. For the English SE07 task, systems mostly consider substitution candidates from WordNet (Fellbaum, 1998) and cast lexical substitution into a ranking task. Experiments may also be performed by pooling the set of candidates from the gold data, evaluating a pure ranking variant. Early approaches use a contextualized word instance representation and rank candidates according to their similarity to this representation. Effective representations are syntactic vector space models (Erk and Padó, 2008; Thater et al., 2011), which use distributional sparse vector representations based on the syntactic context of words. Performance improvement could be shown for different models, including the use of graph centrality algorithms on directional word similarity graphs (Sinha and Mihalcea, 2011), and clustering approaches on word instance representations (Erk and Padó, 2010). Multiple systems have built upon the distributional approach. Extensions include the use of LDA topic models (Ó Séaghdha and Korhonen, 2014), and probabilistic graphical models (Moon and Erk, 2013). The current state of the art combines a distri"
P16-1012,E14-1057,0,\N,Missing
P16-4028,P03-1020,0,0.150902,"Missing"
P17-1145,W06-3812,1,0.924091,"zy Graph Clustering Disambiguated Ambiguous Sense Inventory Weighted Graph Weighted Graph Local Clustering: Global Clustering: Disambiguation of Graph Construction Word Sense Induction Synset Induction Neighbors Background Corpus Word Similarities Synonymy Dictionary Synsets Figure 1: Outline of the WATSET method for synset induction. MCL simulates random walks within a graph by alternation of two operators called expansion and inflation, which recompute the class labels. Notably, it has been successfully used for the word sense induction task (Dorow and Widdows, 2003). Chinese Whispers (CW) (Biemann, 2006) is a hard clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighboring nodes. The algorithm has a meta-parameter that controls graph weights that can be set to three values: (1) top sums over the neighborhood’s classes; (2) nolog downgrades the influence of a neighboring node by its degree or by (3) log of its degree. Clique Percolation Method (CPM) (Palla et al., 2005) is a fuzzy clustering algorithm for unweighted"
P17-1145,E03-1020,0,0.556916,"of WordNet and Wikipedia. UBY (Gurevych et al., 2012) is a general-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and"
P17-1145,E12-1059,0,0.116633,"t induction. The induced resource eliminates the need in manual synset construction and can be used to build WordNet-like semantic networks for under-resourced languages. An implementation of our method along with induced lexical resources is available online.6 2 Related Work Methods based on resource linking surveyed by Gurevych et al. (2016) gather various existing lexical resources and perform their linking to obtain a machine-readable repository of lexical semantic knowledge. For instance, BabelNet (Navigli and Ponzetto, 2012) relies in its core on a linking of WordNet and Wikipedia. UBY (Gurevych et al., 2012) is a general-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Kell"
P17-1145,heylen-etal-2008-modelling,0,0.114052,"are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, 6 co-hyponymy, antonymy, etc. (Heylen et al., 2008; Panchenko, 2011); (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset. In our synset induction method, we use word ego network clustering similarly as in word sense induction approaches, but apply them to a graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges a extracted from manually-created resources. According to the best of our knowledge, most ex"
P17-1145,S13-2049,0,0.115655,"the gold standard datasets. 4.2 Evaluation Metrics To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. Given a synset containing n words, we generate a set of n(n−1) pairs of syn2 onyms. The F-score calculated this way is known as Paired F-score (Manandhar et al., 2010; Hope and Keller, 2013). The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (Jurgens and Klapaftis, 2013), is its straightforward interpretability. 4.3 Language English Russian Input Dictionary of Synonyms For each language, we constructed a synonymy graph using openly available language resources. The statistics of the graphs used as the input in the further experiments are shown in Table 2. https://code.google.com/p/word2vec 13 http://www.dialog-21.ru/en/ evaluation/2015/semantic_similarity 14 http://russe.nlpub.ru/downloads # words 243 840 83 092 # synonyms 212 163 211 986 Table 2: Statistics of the input datasets. Russian. We use the 500-dimensional word embeddings trained using the skip-gram"
P17-1145,S10-1011,0,0.110618,"words 148 730 11 710 137 110 242 9 141 # synsets 117 659 6 667 855 49 492 2 210 # synonyms 152 254 28 822 400 278 381 48 291 Table 1: Statistics of the gold standard datasets. 4.2 Evaluation Metrics To evaluate the quality of the induced synsets, we transformed them into binary synonymy relations and computed precision, recall, and F-score on the basis of the overlap of these binary relations with the binary relations from the gold standard datasets. Given a synset containing n words, we generate a set of n(n−1) pairs of syn2 onyms. The F-score calculated this way is known as Paired F-score (Manandhar et al., 2010; Hope and Keller, 2013). The advantage of this measure compared to other cluster evaluation measures, such as Fuzzy B-Cubed (Jurgens and Klapaftis, 2013), is its straightforward interpretability. 4.3 Language English Russian Input Dictionary of Synonyms For each language, we constructed a synonymy graph using openly available language resources. The statistics of the graphs used as the input in the further experiments are shown in Table 2. https://code.google.com/p/word2vec 13 http://www.dialog-21.ru/en/ evaluation/2015/semantic_similarity 14 http://russe.nlpub.ru/downloads # words 243 840 83"
P17-1145,W11-2502,1,0.770689,"are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related words obtained using distributional methods (Baroni and Lenci, 2010; Biemann and Riedl, 2013), the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, 6 co-hyponymy, antonymy, etc. (Heylen et al., 2008; Panchenko, 2011); (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset. In our synset induction method, we use word ego network clustering similarly as in word sense induction approaches, but apply them to a graph of semantically clean synonyms. Methods based on clustering of synonyms, such as our approach, induce the resource from an ambiguous graph of synonyms where edges a extracted from manually-created resources. According to the best of our knowledge, most experiments either e"
P17-1145,E17-1009,1,0.881247,"on for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of semantically related wo"
P17-1145,W16-1620,1,0.802218,"ral-purpose specification for the representation of lexical-semantic resources and links between them. The main advantage of our approach compared to the lexical resources is that no manual synset encoding is required. Methods based on word sense induction try to induce sense representations without the need for any initial lexical resource by extracting semantic relations from text. In particular, word sense induction (WSI) based on word ego networks clusters graphs of semantically related words (Lin, 1998; Pantel and Lin, 2002; Dorow and Widdows, 2003; V´eronis, 2004; Hope and Keller, 2013; Pelevina et al., 2016; Panchenko et al., 2017a), where each cluster corresponds to a word sense. An ego network consists of a single node (ego) together with the nodes they are connected to (alters) and all the edges among those alters (Everett and Borgatti, 2005). In the case of WSI, such a network is a local neighborhood of one word. Nodes of the ego network are the words which are semantically similar to the target word. Such approaches are able to discover homonymous senses of words, e.g., “bank” as slope versus “bank” as organisation (Di Marco and Navigli, 2012). However, as the graphs are usually composed of"
P17-1145,zesch-etal-2008-extracting,0,0.0541351,"Missing"
P17-1145,J13-3008,0,\N,Missing
P18-2010,P98-1013,0,0.679639,"ully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation"
P18-2010,bauer-etal-2012-dependency,0,0.336218,"Missing"
P18-2010,P10-2045,0,0.591263,"Missing"
P18-2010,W06-3812,1,0.865724,"56 Algorithm 1 Triframes frame induction Input: an embedding model v ∈ V → ~v ∈ Rd , a set of SVO triples T ⊆ V 3 , the number of nearest neighbors k ∈ N, a graph clustering algorithm C LUSTER. Output: a set of triframes F . 1: S ← {t → ~ t ∈ R3d : t ∈ T } 0 0 ~ 2: E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E)"
P18-2010,J02-3001,0,0.262426,"automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas,"
P18-2010,E12-1059,0,0.0392429,"chmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques. Introduction Recent ye"
P18-2010,Q16-1015,0,0.188294,"Missing"
P18-2010,S17-1025,0,0.132917,"ional Linguistics The contributions of this paper are three-fold: (1) we are the first to apply triclustering algorithms for unsupervised frame induction, (2) we propose a new approach to triclustering, achieving state-of-the-art performance on the frame induction task, (3) we propose a new method for the evaluation of frame induction enabling straightforward comparison of approaches. In this paper, we focus on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major i"
P18-2010,P14-1097,0,0.532208,"r a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclustering. In this work, we cast the frame induction problem as a triclustering task (Zhao and Zaki, 2005; Ignatov et al., 2015), namely a generalization of standard clustering and biclustering (Cheng and Church, 2000), aiming at simultaneously clustering objects along three dimensions (cf. Table 1). First, using triclustering allows to avoid sequential nature of frame induction approaches, e.g. (Kawahara et al., 2014), where two independent clusterings are needed. Second, benchmarking frame induction as triclustering against other methods on dependency triples allows to abstract away the evaluation of the frame induction algorithm from other factors, e.g., the input corpus or pre-processing steps, thus allowing a fair comparison of different induction models. 55 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 55–62 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics The contributions of this paper are three"
P18-2010,P03-1009,0,0.454771,"the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles and the SVO triples, arguing that it still can be useful, as frame roles are primarily expressed by subjects and objects, giving rise to semantic structures extracted in an unsup"
P18-2010,L18-1286,1,0.87147,"E ← {(t, t ) ∈ T 2 : t0 ∈ NNS k (t), t 6= t } 3: F ← ∅ 4: for all C ∈ C LUSTER(T, E) do 5: fs ← {s ∈ V : (s, v, o) ∈ C} 6: fv ← {v ∈ V : (s, v, o) ∈ C} 7: fo ← {o ∈ V : (s, v, o) ∈ C} 8: F ← F ∪ {(fs , fv , fo )} 9: return F ate sense-aware representation that is clustered using the Chinese Whispers (CW) hard clustering algorithm (Biemann, 2006). We chose WATSET due to its performance on the related synset induction task, its fuzzy nature, and the ability to find the number of frames automatically. 3 Evaluation Input Corpus. In our evaluation, we use triple frequencies from the DepCC dataset (Panchenko et al., 2018) , which is a dependency-parsed version of the Common Crawl corpus, and the standard 300-dimensional word embeddings model trained on the Google News corpus (Mikolov et al., 2013). All evaluated algorithms are executed on the same set of triples, eliminating variations due to different corpora or pre-processing. G = (T, E) by constructing the edge set E ⊆ T 2 . For that, we compute k ∈ N nearest neighbors of each triple vector ~t ∈ R3d and establish cosine similarity-weighted edges between the corresponding triples. Then, we assume that the triples representing similar contexts appear in simil"
P18-2010,N16-1173,0,0.0731203,"us on the simplest setup with subject-verbobject (SVO) triples and two roles, but our evaluation framework can be extended to more roles. In contrast to the recent approaches like the one by Jauhar and Hovy (2017), our approach induces semantic frames without any supervision, yet capturing only two core roles: the subject and the object of a frame triggered by verbal predicates. Note that it is not generally correct to expect that the SVO triples obtained by a dependency parser are necessarily the core arguments of a predicate. Such roles can be implicit, i.e., unexpressed in a given context (Schenk and Chiarcos, 2016). Keeping this limitation in mind, we assume that the triples obtained from a Web-scale corpus cover most core arguments sufficiently. cluster dataset of Korhonen et al. (2003). A major issue with unsupervised frame induction task is that these and some other related approaches, e.g., (O’Connor, 2013), were all evaluated in completely different incomparable settings, and used different input corpora. In this paper, we propose a methodology to resolve this issue. 2 The Triframes Algorithm Our approach to frame induction relies on graph clustering. We focused on a simple setup using two roles an"
P18-2010,laparra-rigau-2010-extended,0,0.0592218,"data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised frame induction techniques."
P18-2010,D07-1002,0,0.442772,"FrameNet construction through unsupervised frame induction techniques. Introduction Recent years have seen much work on Frame Semantics (Fillmore, 1982), enabled by the availability of a large set of frame definitions, as well as a manually annotated text corpus provided by the FrameNet project (Baker et al., 1998). FrameNet data enabled the development of wide-coverage frame parsers using supervised learning (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006; Das et al., 2014, inter alia), as well as its application to a wide range of tasks, ranging from answer extraction in Question Answering (Shen and Lapata, 2007) and Textual Entailment (Burchardt et al., 2009; Ben Aharon et al., 2010). However, frame-semantic resources are arguably expensive and time-consuming to build due to difficulties in defining the frames, their granularity and domain, as well as the complexity of the construction and annotation tasks requiring expertise in the underlying knowledge. Consequently, such resources exist only for a few languages (Boas, 2009) and even English is lacking domain-specific frame-based resources. Possible inroads are cross-lingual semantic annotation transfer (Pad´o and Lapata, 2009; Hartmann Triclusterin"
P18-2010,M92-1001,0,0.788705,"Missing"
P18-2010,N13-1051,0,0.267798,"Missing"
P18-2010,P11-1145,0,0.0980018,"Missing"
P18-2010,E12-1003,0,0.34924,"Missing"
P18-2010,W09-1127,0,0.141963,"of clustering for triadic data. Our replicable benchmarks demonstrate that the proposed graph-based approach, Triframes, shows state-of-the art results on this task on a FrameNet-derived dataset and performing on par with competitive methods on a verb class clustering task. 1 Role Subject Verb Object Lexical Units (LU) kidnapper, alien, militant snatch, kidnap, abduct son, people, soldier, child Table 1: Example of a LU tricluster corresponding to the “Kidnapping” frame from FrameNet. et al., 2016) or linking FrameNet to other lexicalsemantic or ontological resources (Narayanan et al., 2003; Tonelli and Pighin, 2009; Laparra and Rigau, 2010; Gurevych et al., 2012, inter alia). But while the arguably simpler task of PropBankbased Semantic Role Labeling has been successfully addressed by unsupervised approaches (Lang and Lapata, 2010; Titov and Klementiev, 2011), fully unsupervised frame-based semantic annotation exhibits far more challenges, starting with the preliminary step of automatically inducing a set of semantic frame definitions that would drive a subsequent text annotation. In this work, we aim at overcoming these issues by automatizing the process of FrameNet construction through unsupervised fr"
P18-2010,P17-1145,1,0.73043,"frames F as presented in Algorithm 1. The hyper-parameters of the algorithm are the number of nearest neighbors for establishing edges (k) and the graph clustering algorithm C LUSTER. During the concatenation of the vectors for words forming triples, the (|T |× 3d)-dimensional vector space S is created. Thus, given the triple t ∈ T , we denote the k nearest neighbors extraction procedure of its concatenated embedding from S as NNSk (~t) ⊆ T . We used k = 10 nearest neighbors per triple. To cluster the nearest neighbor graph of SVO triples G, we use the WATSET fuzzy graph clustering algorithm (Ustalov et al., 2017). It treats the vertices T of the input graph G as the SVO triples, induces their senses, and constructs an intermediDatasets. We cast the complex multi-stage frame induction task as a straightforward triple clustering task. We constructed a gold standard set of triclusters, each corresponding to a FrameNet frame, similarly to the one illustrated in Table 1. To construct the evaluation dataset, we extracted frame annotations from the over 150 thousand sentences from the FrameNet 1.7 (Baker et al., 1998). Each sentence contains data about the frame, FEE, and its arguments, which were used to ge"
P18-2010,erk-pado-2006-shalmaneser,0,\N,Missing
P18-2010,J14-1002,0,\N,Missing
P18-2010,E17-2028,0,\N,Missing
P19-1316,P19-1474,1,0.87498,"Missing"
P19-1316,W03-1812,0,0.478278,"meaning of the phrase can be derived from the meanings of its constituent words. To motivate its importance, e.g., in machine translation, noncompositional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3"
P19-1316,W11-1304,1,0.776988,"tion more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e"
P19-1316,W07-1106,0,0.0524878,"mprovements on benchmark datasets in unsupervised and supervised settings. 3. We publicly release our Poincar´e embeddings trained on pattern extractions on a very large corpus. 2 Related Work Some of the initial efforts on compositionality prediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 201"
P19-1316,P13-4006,0,0.017524,"ls as provided, with the vector dimension size of 750 (PPMI-SVD, W2V) and 500 (GloVe)2 . PPMI-SVD baseline: For each word, its neighboring nouns and verbs in a symmetric sliding window of w words in both directions, using a linear decay weighting scheme with respect to its distance d to the target (Levy et al., 2015) are extracted. The representation of a word is a vector containing the positive pointwise mutual information (PPMI) association scores between the word and its contexts. Note that, for each target word, contexts that appear less than 1000 times are discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), using default configurations except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations; minimum word count threshold of 5. GloVe baseline: The count-based DSM of Pennington et al. (2014), implementing a factorizati"
P19-1316,W15-0904,0,0.0132577,"s a recently introduced resource created for evaluation (Ramisch et al., 2016) that extends the Reddy dataset with an additional 90 English nominal compounds, amounting to a total of 180 nominal compounds. Consistent with RD, the scores range from 0 (idiomatic) to 5 (compositional) and are annotated through Mechanical Turk and averaged over the annotators. The additional 90 entries are adjective-noun pairs, balanced with respect to compositionality. Farahmand (FD): This dataset contains 1042 English compounds extracted from Wikipedia with binary non-compositionality judgments by four experts (Farahmand et al., 2015). In evaluations we use the sum of all the judgments to have a single numeral compositionality score, ranging from 0 (compositional) to 4 (idiomatic). We optimize our method on subsets of the datasets for pairs and constituents with available Poincar´e embeddings in order to measure the direct impact of our method, which comprises 79, 146 and 780 datapoints for the three sets RD-R, RD++-R and FD-R, respectively. We subsequently report scores on the full datasets RD-F (90), RD++-F (180) and FD-F (1042) for the sake of fair comparison to previous works. In cases where no Poincar´e embeddings are"
P19-1316,W18-0905,0,0.0128343,"ds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form"
P19-1316,goldhahn-etal-2012-building,0,0.0259866,"ous representations of symbolic data by simultaneously capturing hierarchy and similarity. As per this proposed Poincar´e ball model, let β d = {x ∈ R : kxk &lt; 1} (3) be the open d-dimensional unit ball, where k.k denotes the Euclidean norm. The list of hyponym-hypernym pairs was obtained by applying lexical-syntactic patterns described by Hearst (1992) on the corpus prepared by Panchenko et al. (2016). This corpus is a concatenation of the English Wikipedia (2016 dump), Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and English news corpora from the Leipzig Corpora Collection (Goldhahn et al., 2012). The lexical-syntactic patterns proposed by Hearst (1992) and further extended and implemented in the form of FSTs by Panchenko et al. (2012)1 for extracting (noisy) hyponym-hypernym pairs are given as follows – (i) such NP as NP, NP[,] and/or NP; (ii) NP such as NP, NP[,] and/or NP; (iii) NP, NP [,] or other NP; (iv) NP, NP [,] and other NP; (v) NP, including NP, NP [,] and/or NP; (vi) NP, especially NP, NP [,] and/or NP. Pattern extraction on the corpus yields a list of 27.6 million hyponym-hypernym pairs along with the frequency of their occurrence in the corpus. We normalize the frequency"
P19-1316,C92-2082,0,0.530466,"example, ‘art school’ and ‘school’ have one common hypernym ‘educational institution’ whereas ‘hot dog’ has no common hypernym with ‘hot’ or ‘dog’, apart from very abstract concepts such as ‘physical entity’. Of course, this only holds for noun phrases, where taxonomic relations between nouns apply. To represent hypernymy information we use Poincar´e embeddings (Nickel and Kiela, 2017) for learning hierarchical representations of symbolic data by embedding them into a hyperbolic space. To this end, we extract hyponym-hypernym pairs by applying well-known lexical-syntactic patterns proposed by Hearst (1992) on a large corpus and train Poincar´e embeddings on a list of hyponymhypernym pairs. Relying on two types of representations, i.e., dense vectors in the Euclidean space and the novel hyperbolic Poincar´e embeddings, we interpolate their similarity predictions in a novel compositionality score metric that takes both distributional and hypernymy information into account. We evaluate our proposed metric on three well-accepted English datasets, i.e., Reddy (Reddy et al., 2011), Reddy++ (Ramisch et al., 2016) and Farahmand (Farahmand et al., 2015), demonstrating a performance boost when including"
P19-1316,P16-1187,0,0.0999607,"in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Lingu"
P19-1316,D13-1147,0,0.0203407,"ual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis o"
P19-1316,J19-1001,0,0.0209654,"c distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages considering the context of the expressions (Flor and Klebanov, 2018; Bizzoni et al., 2018; Peng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e embeddings: in their case for inducing taxonomies from the text. In both works, hyperbolic embeddings are trained using relations harvested using Hearst patterns, like in our work. The usefulness of hyperbolic embeddings was also shown beyond text processing: Khrulkov et al. (2019) successfully applied them"
P19-1316,P19-1313,0,0.0240964,"ng et al., 2018), see also a respective shared task (Biemann and Giesbrecht, 2011). In one of the recent attempts, Cordeiro et al. (2016) conduct an analysis of several DSMs (word2vec, GloVe, PPMI) with variations of hyper-parameters and produce the state-of-the-art results in the compositionality prediction task, which is extended further for different languages by Cordeiro et al. (2019). We take their work as our baseline and carry forward our investigation to improve the state-of-the-art performance by introducing the 3264 hyponymy-hypernymy information in the form of Poincar´e embeddings. Le et al. (2019) and Aly et al. (2019) also showed usefulness the use of Poincar´e embeddings: in their case for inducing taxonomies from the text. In both works, hyperbolic embeddings are trained using relations harvested using Hearst patterns, like in our work. The usefulness of hyperbolic embeddings was also shown beyond text processing: Khrulkov et al. (2019) successfully applied them for hierarchical relations in image classification tasks. 3 Methodology Our aim is to produce a compositionality score for a given two-word noun phrase w1 w2 . As per our hypothesis, the proposed compositionality score metri"
P19-1316,Q15-1016,0,0.0481565,"seline, where authors apply several distributional semantic models and their variants by tuning hyperparameters like the dimension of vectors, the window-size during training and others. We resort to PPMI-SVD, two variants of word2vec (CBOW and SkipGram) and GloVe as our baselines. We use these models as provided, with the vector dimension size of 750 (PPMI-SVD, W2V) and 500 (GloVe)2 . PPMI-SVD baseline: For each word, its neighboring nouns and verbs in a symmetric sliding window of w words in both directions, using a linear decay weighting scheme with respect to its distance d to the target (Levy et al., 2015) are extracted. The representation of a word is a vector containing the positive pointwise mutual information (PPMI) association scores between the word and its contexts. Note that, for each target word, contexts that appear less than 1000 times are discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), usin"
P19-1316,W03-1810,0,0.149849,"s on compositionality prediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic"
P19-1316,D07-1039,0,0.108452,"Missing"
P19-1316,P08-1028,0,0.0877299,"rediction were undertaken by Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that"
P19-1316,S16-1206,1,0.884605,"Missing"
P19-1316,D14-1162,0,0.0900465,"e discarded. The Dissect toolkit (Dinu et al., 2013) is then used in order to build a PPMI matrix and its dimensionality is reduced using singular value decomposition (SVD) to factorize the matrix. word2vec baseline: This DSM is prepared using the well-known word2vec (Mikolov et al., 2013) in both variants CBOW (W2V-CBOW) and Skip-Gram (W2V-SG), using default configurations except for the following: no hierarchical softmax; negative sampling of 25; frequent-word downsampling weight of 10−6 ; runs 15 training iterations; minimum word count threshold of 5. GloVe baseline: The count-based DSM of Pennington et al. (2014), implementing a factorization of the co-occurrence count matrix is used for the task. The configurations are the default ones, except for the following: internal cutoff parameter xmax = 75; builds co-occurrence matrix in 15 iterations; minimum word count threshold of 5. Other baseline models proposed by Reddy et al. (2011), Salehi et al. (2014), Salehi et al. (2015) report results only on Reddy dataset (since the other two datasets have been introduced later) whereas Yazdani et al. (2015) perform their evaluation only on the Farahmand dataset for their supervised model. In addition, this supe"
P19-1316,P16-2026,0,0.0142342,"Datasets To evaluate our proposed models (both supervised and unsupervised) we use three gold standard datasets for English on compositionality detection and describe them in the following. 3266 Reddy (RD): This dataset contains compositionality judgments for 90 compounds in a scale of literality from 0 (idiomatic) to 5 (compositional), obtained by averaging crowdsourced judgments on these pairs (Reddy et al., 2011). For evaluation, we use only the global compositionality score, ignoring individual word judgments. Reddy++ (RD++): This is a recently introduced resource created for evaluation (Ramisch et al., 2016) that extends the Reddy dataset with an additional 90 English nominal compounds, amounting to a total of 180 nominal compounds. Consistent with RD, the scores range from 0 (idiomatic) to 5 (compositional) and are annotated through Mechanical Turk and averaged over the annotators. The additional 90 entries are adjective-noun pairs, balanced with respect to compositionality. Farahmand (FD): This dataset contains 1042 English compounds extracted from Wikipedia with binary non-compositionality judgments by four experts (Farahmand et al., 2015). In evaluations we use the sum of all the judgments to"
P19-1316,I11-1024,0,0.337019,"ional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2"
P19-1316,E14-1050,0,0.261156,"translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3263–3274 c Florence, Italy, July 28 - August 2, 2019. 2019 Association"
P19-1316,N15-1099,0,0.0363598,"Missing"
P19-1316,D12-1110,0,0.0529303,"hrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language m"
P19-1316,S10-1049,0,0.0130872,"y Baldwin et al. (2003), who use LSA to calculate the similarity between a phrase and its components, whereas Venkatapathy and Joshi (2005) extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax (Cook et al., 2007) and selectional preferences (McCarthy et al., 2007). Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such"
P19-1316,H05-1113,0,0.269682,"can be derived from the meanings of its constituent words. To motivate its importance, e.g., in machine translation, noncompositional phrases must be translated as a unit; in word sense disambiguation, assigning one of the constituent word’s senses to the whole phrase should be avoided for idiomatic phrases; semantic parsing also requires to correctly identify complex predicates and their arguments in this way. A significant amount of effort has gone into operationalizing dense-vector distributional semantic models (DSMs) of different flavors such as count-based models (Baldwin et al. (2003); Venkatapathy and Joshi (2005); McCarthy et al. (2007)), word embeddings based on word2vec (both CBOW and SkipGram) and similar (Reddy et al. (2011); Salehi et al. (2014); Cordeiro et al. (2016, 2019)), and multi-sense skip-gram models for compositionality prediction (Salehi et al., 2015). All these attempts are based on the hypothesis that the composition of the representation of constituent words will be closer to the representation of the entire phrase in case of compositional phrases as compared to the non-compositional ones (Choueka, 1988). Observing that the distributional information 3263 Proceedings of the 57th Ann"
P19-1316,D15-1201,0,0.200919,"researched extensively (McCarthy et al., 2003; Mitchell and Lapata, 2008; Tratz and Hovy, 2010). Reddy et al. (2011) define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings (Socher et al., 2012; Salehi et al., 2015). Salehi et al. (2014) also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction. In a recent attempt, Yazdani et al. (2015) tries to learn semantic composition and finds that complex functions such as polynomial projection and neural networks can model semantic composition more effectively than the commonly used additive and multiplicative functions. Kiela and Clark (2013) detect non-compositionality using concepts of mutual information. Lioma et al. (2015) replace the context vectors with language models and compute their Kullback–Leibler divergence to approximate their semantic distance. In another stream, researchers have also attempted to classify idiomatic vs. non-idiomatic expressions in different languages"
P19-1325,P18-1011,0,0.031699,"follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two resources: the FB15k-237 (Toutanova and Chen, 2015) dataset contains 14,951 entities/nodes and is derived from Freebase (Bollacker et al., 2008); the DB100k (Ding et al., 2018) dataset contains 99,604 entities/nodes and is derived from DBPe3352 dia (Auer et al., 2007). It is important to note that both datasets were used to evaluate approaches that learn knowledge graph embeddings, e.g. (Lin et al., 2015; Xie et al., 2016; Joulin et al., 2017) on the task on knowledge base completion (KBC), to predict missing KB edges/relations between nodes/entities. The specificity of our model is that it learns a given graph similarity metric, which is not provided in these datasets. Therefore, we use only the graphs from these datasets, computing the shortest path distances betw"
P19-1325,J15-4004,0,0.0950214,"Missing"
P19-1325,S19-1014,1,0.822582,"ning metric embeddings for three types of graphs (WordNet, FreeBase, and DBPedia), based on several similarity measures. Second, in an extrinsic evaluation on the Word Sense Disambiguation (WSD) task (Navigli, 2009) we replace several original measures with their vectorized counterparts in a known graph-based WSD algorithm by Sinha and Mihalcea (2007), reaching comparable levels of performance with the graph-based algorithms while maintaining computational gains. The main contribution of this paper is the demonstration of the effectiveness and efficiency of the path2vec node embedding method (Kutuzov et al., 2019). This method learns dense vector embeddings of nodes V based on a user-defined custom similarity measure sim, e.g. the shortest path distance or any other similarity measure. While our method is able to closely approximate quite different similarity measures as we show 1 https://github.com/uhh-lt/path2vec 3349 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3349–3355 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics on WordNet-based measures and therefore can be used in lieu of these measures in NLP compo"
P19-1325,W04-0807,0,0.0870945,"LCH (WordNet) LCH (path2vec) 0.547↓0.000 0.527↓0.020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publi"
P19-1325,S15-2049,0,0.0141756,".020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two res"
P19-1325,S01-1005,0,0.0243681,"ph-based vs vector-based measures LCH (WordNet) LCH (path2vec) 0.547↓0.000 0.527↓0.020 0.494↓0.000 0.472↓0.022 0.550↓0.000 0.536↓0.014 ShP (WordNet) ShP (path2vec) 0.548↓0.000 0.534↓0.014 0.495↓0.000 0.489↓0.006 0.550↓0.000 0.563↑0.013 WuP (WordNet) WuP (path2vec) 0.547↓0.000 0.543↓0.004 0.487↓0.000 0.489↑0.002 0.542↓0.000 0.545↑0.003 Various baseline graph embeddings trained on WordNet TransR node2vec DeepWalk FSE 0.540 0.503 0.528 0.536 0.466 0.467 0.476 0.476 0.536 0.489 0.552 0.523 Table 2: F1 scores of a graph-based WSD algorithm on WordNet versus its vectorized counterparts. Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Moro and Navigli, 2015). The raw WordNet similarities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically,"
P19-1325,D14-1162,0,0.110934,"vi ,vj )∈Bn log σ(−vi · v batch of positive training samples, Bn is the batch of the generated negative samples, and σ is the sigmoid function. At this, Skip-gram uses only local information, never creating the full co-occurrence count matrix. In our path2vec model, the target dot product values sij are not binary, but can take arbitrary values in the [0...1] range, as given by the custom distance metric. Further, we use only a single embedding matrix with vector representations of the graph nodes, not needing to distinguish target and context. Another related model is Global Vectors (GloVe) (Pennington et al., 2014), which learns co-occurrence probabilities in a given corpus. The objective function to be minimized in GloVe model is L = P f (s )(v · v ˜ − log sij + bi + bj )2 , ij i j (vi ,vj )∈B where sij counts the co-occurrences of words vi and vj , bi and bj are additional biases for each word, and f (sij ) is a weighting function handling rare co-occurrences. Like the Skip-gram, GloVe also uses two embedding matrices, but it relies only on global information, pre-aggregating global word co-occurrence counts. Computing Training Similarities In general case, our model requires computing pairwise node s"
P19-1325,P15-2002,0,0.174842,"he depth of the two nodes in the taxonomy and the depth of their most specific ancestor node. For instance, for LCH this procedure took about 30 hours on an Intel Xeon E5-2603v4@1.70GHz CPU using 10 threads. We pruned similarities to the first 50 most similar ‘neighbors’ of each synset and trained path2vec on this dataset. Discussion of Results Figure 1 presents computation times for pairwise similarities between one synset and all other 82,115 WordNet noun synsets. We compare running times of calculating two original graph-based metrics to Hamming distance between 128D FSE binary embeddings (Subercaze et al., 2015) and to dot product between their dense vectorized 300D counterparts (using CPU). Using float vectors (path2vec) is 4 orders of magnitude faster than operating directly on graphs, and 2 orders faster than Hamming distance. The dot product computation is much faster as compared to shortest path computation (and other complex walks) on a large graph. Also, lowdimensional vector representations of nodes take much less space than the pairwise similarities between all the nodes. The time complexity of calculating the shortest path between graph nodes (as in ShP or LCH) is in the best case linear in"
P19-1325,W15-4007,0,0.0139358,"arities have a small edge over their vector approximations in the majority of the cases yet the path2vec models consistently closely follow them while outperforming other graph embedding baselines: We indicate the differences with respect to the original with a subscript number. 5 5.1 Evaluation on Knowledge Base Graphs Experimental Settings To show the utility of our model besides the WordNet graph, we also applied it to two graphs derived from knowledge bases (KBs). More specifically, we base our experiments on two publicly available standard samples from these two resources: the FB15k-237 (Toutanova and Chen, 2015) dataset contains 14,951 entities/nodes and is derived from Freebase (Bollacker et al., 2008); the DB100k (Ding et al., 2018) dataset contains 99,604 entities/nodes and is derived from DBPe3352 dia (Auer et al., 2007). It is important to note that both datasets were used to evaluate approaches that learn knowledge graph embeddings, e.g. (Lin et al., 2015; Xie et al., 2016; Joulin et al., 2017) on the task on knowledge base completion (KBC), to predict missing KB edges/relations between nodes/entities. The specificity of our model is that it learns a given graph similarity metric, which is not"
P19-1474,baroni-bernardini-2004-bootcat,0,0.0267867,"cally applicable to (noisy) taxonomies, yielding an improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three"
P19-1474,S15-2151,0,0.272488,"IS-A relation can be improved by studying how pattern-based and distributional approaches complement each other; ii) there is only limited success of pure deep learn1 https://github.com/uhh-lt/Taxonomy_ Refinement_Embeddings 4811 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4811–4817 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ing paradigms here, mostly because it is difficult to design a single objective function for this task. On the two recent TExEval tasks at SemEval for taxonomy extraction (Bordea et al., 2015, 2016), attracting a total of 10 participating teams, attempts to primarily use a distributional representation failed. This might seem counterintuitive, as taxonomies are surely modeling semantics and thus their extraction should benefit from semantic representations. The 2015 winner INRIASAC (Grefenstette, 2015) performed relation discovery using substring inclusion, lexicalsyntactic patterns and co-occurrence information based on sentences and documents from Wikipedia. The winner in 2016, TAXI (Panchenko et al., 2016), harvests hypernyms with substring inclusion and Hearst-style lexical-sy"
P19-1474,S16-1168,0,0.175656,"onym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue"
P19-1474,P14-1113,0,0.151137,"ll model is defined as:   ||u − v||2 d(u, v) = arcosh 1 + 2 . (1 − ||u||2 )(1 − ||v||2 ) This Poincar´e distance enables us to capture the hierarchy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected compone"
P19-1474,P19-1313,0,0.155064,"web crawling. The only submission to the TExEval 2016 task that relied exclusively on distributional semantics to induce hypernyms by adding a vector offset to the corresponding hyponym (Pocostales, 2016) achieved only modest results. A more refined approach to applying distributional semantics by Zhang et al. (2018) generates a hierarchical clustering of terms with each node consisting of several terms. They find concepts that should stay in the same cluster using embedding similarity – whereas, similar to the TExEval task, we are interested in making distinctions between all terms. Finally, Le et al. (2019) also explore using Poincar´e embeddings for taxonomy induction, evaluating their method on hypernymy detection and reconstructing WordNet. However, in contrast to our approach that filters and attaches terms, they perform inference. 3 Taxonomy Refinement using Hyperbolic Word Embeddings We employ embeddings using distributional semantics (i.e. word2vec CBOW) and Poincar´e embeddings (Nickel and Kiela, 2017) to alleviate the largest error classes in taxonomy extraction: the existence of orphans – disconnected nodes that have an overall connectivity degree of zero and outliers – a child node th"
P19-1474,N15-1098,1,0.935588,"he-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue by introducing a series of simple and parameter-free refinement steps that employ word embeddings in order to improve existing domain-specific taxonomies, induced from text using tradi"
P19-1474,S16-1204,0,0.0196188,"17 16.5 15.6 15.6 16.3 16.2 15.6 Environment 16.2 16.4 15.8 Science Food JUNLP Figure 2: F1 results for the systems on all domains. Vocabulary sizes: environment (|V |= 261), science (|V |= 453), food (|V |= 1556). Bold numbers are significantly different results to the original system with p &lt; 0.05. 4 Evaluation Proposed methods are evaluated on the data of SemEval2016 TExEval (Bordea et al., 2016) for submitted systems that created taxonomies for all domains of the task4 , namely the task-winning system TAXI (Panchenko et al., 2016) as well as the systems USAAR (Tan et al., 2016) and JUNLP (Maitra and Das, 2016). TAXI harvests 4 http://alt.qcri.org/semeval2016/ task13/index.php Comparison to Baselines Figure 2 shows comparative results for all datasets and measures for every system. The Root method, which connects all orphans to the root of the taxonomy, has the highest connectivity but falls behind in scores significantly. Word2vec CBOW embeddings partly increase the scores, however, the effect appears to be inconsistent. Word2vec embeddings connect more orphans to the taxonomy (cf. Table 2), albeit with mixed quality, thus the interpretation of word similarity as co-hyponymy does not seem to be app"
P19-1474,goldhahn-etal-2012-building,0,0.0408558,"refinement pipeline. In our experiments, we use the output of three different systems. The refinement method is generically applicable to (noisy) taxonomies, yielding an improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relatio"
P19-1474,S15-2152,0,0.139878,", supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space. 1 Introduction The task of taxonomy induction aims at creating a semantic hierarchy of entities by using hyponym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation"
P19-1474,C92-2082,0,0.7644,"over distributional semantic representations, supporting the hypothesis that they can better capture hierarchical lexical-semantic relationships than embeddings in the Euclidean space. 1 Introduction The task of taxonomy induction aims at creating a semantic hierarchy of entities by using hyponym-hypernym relations – called taxonomy – from text corpora. Compared to many other domains of natural language processing that make use of pre-trained dense representations, state-ofthe-art taxonomy learning is still highly relying on traditional approaches like extraction of lexicalsyntactic patterns (Hearst, 1992) or co-occurrence information (Grefenstette, 2015). Despite the success of pattern-based approaches, most taxonomy induction systems suffer from a significant number of disconnected terms, since the extracted relationships are too specific to cover most words (Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many o"
P19-1474,heylen-etal-2008-modelling,0,0.0250853,"s to capture the hierarchy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected components that have children are re-connected to the most similar parent in the taxonomy or to the taxonomy root if no distributed re"
P19-1474,S16-1202,0,0.190789,"(Wang et al., 2017; Bordea et al., 2016). The use of distributional semantics for hypernym identification and relation representation has thus received increasing attention (Shwartz et al., 2016). However, Levy et al. (2015) observe that many proposed supervised approaches instead learn prototypical hypernyms (that are hypernyms to many other terms), not taking into account the relation between both terms in classification. Therefore, past applications of distributional semantics appear to be rather unsuitable to be directly applied to taxonomy induction as the sole signal (Tan et al., 2015; Pocostales, 2016). We address that issue by introducing a series of simple and parameter-free refinement steps that employ word embeddings in order to improve existing domain-specific taxonomies, induced from text using traditional approaches in an unsupervised fashion. We compare two types of dense vector embeddings: the standard word2vec CBOW model (Mikolov et al., 2013a,b), that embeds terms in Euclidean space based on distributional similarity, and the more recent Poincar´e embeddings (Nickel and Kiela, 2017), which capture similarity as well as hierarchical relationships in a hyperbolic space. The source"
P19-1474,L16-1572,1,0.835287,"improved taxonomy extraction system overall. 3.1 Domain-specific Poincar´e Embedding Training Dataset Construction To create domain-specific Poincar´e embeddings, we use noisy hypernym relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three are removed to filter noise. Besides further r"
P19-1474,L16-1056,0,0.0200324,"m relationships extracted from a combination of general and domain-specific corpora. For the general domain, we extracted 59.2 GB of text from English Wikipedia, Gigaword (Parker et al., 2009), ukWac (Ferraresi et al., 2008) and LCC news corpora (Goldhahn et al., 2012). The domain-specific corpora consist of web pages, selected by using a combination of BootCat (Baroni and Bernardini, 2004) and focused crawling (Remus and Biemann, 2016). Noisy IS-A relations are extracted with lexicalsyntactic patterns from all corpora by applying PattaMaika2 , PatternSim (Panchenko et al., 2012), and WebISA (Seitner et al., 2016), following (Panchenko et al., 2016).3 The extracted noisy relationships of the common and domain-specific corpora are further processed separately and combined afterward. To limit the number of terms and relationships, we restrict the IS-A relationships on pairs for which both entities are part of the taxonomy’s vocabulary. Relations with a frequency of less than three are removed to filter noise. Besides further removing every reflexive relationship, only the more frequent pair of a symmetric relationship is kept. Hence, the set of cleaned relationships is transformed into being antisymmetri"
P19-1474,P16-1226,0,0.201626,"Missing"
P19-1474,S16-1203,0,0.0333196,"Missing"
P19-1474,S15-2155,0,0.0474667,"Missing"
P19-1474,D17-1123,0,0.32268,"Missing"
P19-1474,C14-1212,0,0.029562,"archy and similarity between words simultaneously. It increases exponentially with the depth of the hierarchy. So while the distance of a leaf node to most other nodes in the hierarchy is very high, nodes on abstract levels, such as the root, have a comparably small distance to all nodes in the hierarchy. The word2vec embeddings have no notion of hierarchy and hierarchical relationships cannot be represented with vector offsets across the vocabulary (Fu et al., 2014). When applying word2vec, we use the observation that distributionally similar words are often co-hyponyms (Heylen et al., 2008; Weeds et al., 2014). 3.2 Relocation of Outlier Terms Poincar´e embeddings are used to compute and store a rank rank(x, y) between every child and parent of the existing taxonomy, defined as the index of y in the list of sorted Poincar´e distances of all entities of the taxonomy to x. Hypernymhyponym relationships with a rank larger than the mean of all ranks are removed, chosen on the basis of tests on the 2015 TExEval data (Bordea et al., 2015). Disconnected components that have children are re-connected to the most similar parent in the taxonomy or to the taxonomy root if no distributed representation exists."
P19-1584,Q17-1010,0,0.0204524,"dam 1.0 Table 1: Hyperparameter configuration of our deidentification model. We compare three different approaches: a nonprivate de-identification classifier and two privacyenabled extensions, automatic pseudonymization (Section 4) and adversarially learned representations (Section 5). Our non-private system as well as the privacyenabled extensions are based on a bidirectional LSTM-CRF architecture that has been proven to work well in sequence tagging (Huang et al., 2015; Lample et al., 2016) and de-identification (Dernoncourt et al., 2017; Liu et al., 2017). We only use pre-trained FastText (Bojanowski et al., 2017) or GloVe (Pennington et al., 2014) word embeddings, not explicit character embeddings, as we suspect that these may allow easy re-identification of private information if used in shared representations. In place of learned character features, we provide the casing feature from Reimers and Gurevych (2017) as an additional input. The feature maps words to a one-hot representation of their casing (numeric, mainly numeric, all lower, all upper, initial upper, contains digit, or other). Table 1 shows our raw de-identification model’s hyperparameter configuration that was determined through a rando"
P19-1584,D18-1002,0,0.0744235,"e classifier and the adversary are deep learning models with shared initial layers. A gradient reversal layer is used to worsen the representation for the adversary during back-propagation: when training the adversary, the adversary-specific part of the network is optimized for the adversarial task but the shared part is updated against the gradient to make the shared representation less suitable for the adversary. Although initially conceived for use in domain adaptation, DANNs and similar adversarial deep learning models have recently been used to obfuscate demographic attributes from text (Elazar and Goldberg, 2018; Li et al., 2018) and subject identity (Feutry et al., 2018) from images. Elazar and Goldberg (2018) warn that when a representation is learned using gradient reversal methods, continued adversary training on the frozen representation may allow adversaries to break representation privacy. To test whether the unwanted information is not extractable from the generated information anymore, adversary training needs to continue on the frozen representation after finishing training the system. Only if after continued adversary training the information cannot be recovered, we have evidence that it r"
P19-1584,P82-1020,0,0.438244,"Missing"
P19-1584,L18-1708,0,0.0268521,"Missing"
P19-1584,P18-2005,0,0.0714687,"ary are deep learning models with shared initial layers. A gradient reversal layer is used to worsen the representation for the adversary during back-propagation: when training the adversary, the adversary-specific part of the network is optimized for the adversarial task but the shared part is updated against the gradient to make the shared representation less suitable for the adversary. Although initially conceived for use in domain adaptation, DANNs and similar adversarial deep learning models have recently been used to obfuscate demographic attributes from text (Elazar and Goldberg, 2018; Li et al., 2018) and subject identity (Feutry et al., 2018) from images. Elazar and Goldberg (2018) warn that when a representation is learned using gradient reversal methods, continued adversary training on the frozen representation may allow adversaries to break representation privacy. To test whether the unwanted information is not extractable from the generated information anymore, adversary training needs to continue on the frozen representation after finishing training the system. Only if after continued adversary training the information cannot be recovered, we have evidence that it really is not conta"
P19-1584,D14-1162,0,0.0821152,"figuration of our deidentification model. We compare three different approaches: a nonprivate de-identification classifier and two privacyenabled extensions, automatic pseudonymization (Section 4) and adversarially learned representations (Section 5). Our non-private system as well as the privacyenabled extensions are based on a bidirectional LSTM-CRF architecture that has been proven to work well in sequence tagging (Huang et al., 2015; Lample et al., 2016) and de-identification (Dernoncourt et al., 2017; Liu et al., 2017). We only use pre-trained FastText (Bojanowski et al., 2017) or GloVe (Pennington et al., 2014) word embeddings, not explicit character embeddings, as we suspect that these may allow easy re-identification of private information if used in shared representations. In place of learned character features, we provide the casing feature from Reimers and Gurevych (2017) as an additional input. The feature maps words to a one-hot representation of their casing (numeric, mainly numeric, all lower, all upper, initial upper, contains digit, or other). Table 1 shows our raw de-identification model’s hyperparameter configuration that was determined through a random hyperparameter search. 4 Automati"
P19-1584,N16-1030,0,0.0246394,"ropout Variational dropout Dropout after LSTM Optimizer Gradient norm clipping FastText, GloVe Yes 32 2 128 0.1 0.25 0.5 Nadam 1.0 Table 1: Hyperparameter configuration of our deidentification model. We compare three different approaches: a nonprivate de-identification classifier and two privacyenabled extensions, automatic pseudonymization (Section 4) and adversarially learned representations (Section 5). Our non-private system as well as the privacyenabled extensions are based on a bidirectional LSTM-CRF architecture that has been proven to work well in sequence tagging (Huang et al., 2015; Lample et al., 2016) and de-identification (Dernoncourt et al., 2017; Liu et al., 2017). We only use pre-trained FastText (Bojanowski et al., 2017) or GloVe (Pennington et al., 2014) word embeddings, not explicit character embeddings, as we suspect that these may allow easy re-identification of private information if used in shared representations. In place of learned character features, we provide the casing feature from Reimers and Gurevych (2017) as an additional input. The feature maps words to a one-hot representation of their casing (numeric, mainly numeric, all lower, all upper, initial upper, contains dig"
P19-1584,N15-2012,0,0.0134499,"patients. The PHI that may not be shared includes potentially identifying information such as names, geographic identifiers, dates, and account numbers; the American Health Insurance Portability Accountability Act1 (HIPAA, 1996) defines 18 categories of PHI. De-identification is the task of finding and labeling PHI in medical text as a step toward sanitization. As the information to be removed is very sensitive, sanitization always requires final human verification. Automatic deidentification labeling can however significantly speed up the process, as shown for other annotation tasks in e.g. Yimam (2015). Trying to create an automatic classifier for deidentification leads to a “chicken and egg problem” (Uzuner et al., 2007): without a comprehensive training set, an automatic de-identification classifier cannot be developed, but without access to automatic de-identification, it is difficult to share large corpora of medical text in a privacypreserving way for research (including for training the classifier itself). The standard method of data protection compliant sharing of training data for a de-identification classifier requires humans to pseudonymize protected information with substitutes i"
P19-2044,E06-1002,0,0.167272,"Missing"
P19-2044,K18-1050,0,0.43043,"¨at Hamburg, Hamburg, Germany Skolkovo Institute of Science and Technology, Moscow, Russia ? Diffbot Inc., Menlo Park, CA, USA {sevgili,panchenko,biemann}@informatik.uni-hamburg.de Abstract the entity disambiguation task. The goal of an ED system is resolving the ambiguity of entity mentions, such as Mars, Galaxy, and Bounty are all delicious. It is hard for an algorithm to identify whether the entity is an astronomical structure1 or a brand of milk chocolate2 . Current neural approaches to EL/ED attempt to use context and word embeddings (and sometimes entity embeddings on mentions in text) (Kolitsas et al., 2018; Sun et al., 2015). Whereas these and most other previous approaches employ embeddings trained from text, we aim to create entity embeddings based on structured data (i.e. hyperlinks) using graph embeddings and integrate them into the ED models. Graph embeddings aim at representing nodes in a graph, or subgraph structure, by finding a mapping between a graph structure and the points in a low-dimensional vector space (Hamilton et al., 2017). The goal is to preserve the features of the graph structure and map these features to the geometric relationships, such as distances between different nod"
P19-2044,D07-1074,0,0.112723,"Missing"
P19-2044,K16-1026,0,0.0139628,"entity (Mihalcea and Csomai, 2007; Strube and Ponzetto, 2006; Bunescu and Pas¸ca, 2006). Similarly, Milne and Witten (2008) define a measurement of entityentity relatedness. Current state-of-the-art approaches are based on neural networks (Huang et al., 2015; Ganea and Hofmann, 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on the knowledge graph for r"
P19-2044,D17-1277,0,0.132996,"Missing"
P19-2044,P10-1154,0,0.10386,"Missing"
P19-2044,N18-1167,0,0.0222984,", 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on the knowledge graph for representing structured information. 3 Learning Graph-based Entity Vectors In order to make information from a semantic graph available for an entity linking system, we make use of graph embeddings. We use DeepWalk (Perozzi et al., 2014) to create the representation of entities in the DB"
P19-2044,P11-1138,0,0.225398,"Missing"
P19-2044,roder-etal-2014-n3,0,0.14207,"Missing"
P19-2044,K16-1025,0,0.0501088,"a mention and a candidate entity (Mihalcea and Csomai, 2007; Strube and Ponzetto, 2006; Bunescu and Pas¸ca, 2006). Similarly, Milne and Witten (2008) define a measurement of entityentity relatedness. Current state-of-the-art approaches are based on neural networks (Huang et al., 2015; Ganea and Hofmann, 2017; Kolitsas et al., 2018; Sun et al., 2015), where are based on character, word and/or entity embeddings created by a neural network with a motivation of their capability to automatically induce features, as opposed to hand-crafting them. Then, they all use these embeddings in neural EL/ED. Yamada et al. (2016) and Fang et al. (2016) utilize structured data modelling entities and words in the same space and mapping spans to entities based on the similarity in this space. They expand the objective function of word2vec (Mikolov et al., 2013a,b) and use both text and structured information. Radhakrishnan et al. (2018) extend the work of Yamada et al. (2016) by creating their own graph based on co-occurrences statistics instead of using the knowledge graph directly. Contrary to them, our model learns a mapping of spans and entities, which reside in different spaces and use graph embeddings trained on th"
P19-2045,W17-2339,0,0.166803,"Neural Networks (CNNs) and different types of Recurrent Neural Networks (RNNs) (Goodfellow et al., 2016; Kim, 2014), most notably long short-term memory units (LSTMs, Hochreiter and Schmidhuber, 1997) have shown to be highly efficient in TC tasks. For HMC, Cerri et al. (2014) use concatenated multi-layer perceptrons (MLP), where each MLP is associated to one level of the class hierarchy. Kowsari et al. (2017) use multiple concatenated deep learning architectures (CNN, LSTM, and MLP) to HMC on a dataset with a rather shallow hierarchy with only two levels. Similar to Kiritchenko et al. (2005), Baker and Korhonen (2017) treat the HMC task as a multi-label classification problem that considers every label in the hierarchy, but they additionally leverage the cooccurrence of labels within the hierarchy to initialize the weights of their CNN’s final layer (Kurata et al., 2016). Capsule Networks: Capsule networks encapsulate features into groups of neurons, so-called capsules (Hinton et al., 2011; Sabour et al., 2017). Originally introduced for a handwritten digit image classification task where each digit has been associated with a capsule, capsules have shown to learn more robust representations for each class"
P19-2045,N19-1226,0,0.0202546,"tion, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structured categories, they have not yet been applied to HMC. This work aims to fill the gap by applying and thoroughly analyzing capsules’ properties at HMC. of a vertical search webpage for books and thus presents a real-world scenario task. Secondly, we thoroughly analyze the properties of capsule networks for HMC. To the best of our knowledge, capsule networks have not yet been applied and tested in the HMC domain. 2 Related Work Neural networks for"
P19-2045,Q17-1010,0,0.0315413,"Missing"
P19-2045,W18-6237,0,0.0134522,"f categories. By connecting a BiLSTM to a capsule network for relation extraction, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structured categories, they have not yet been applied to HMC. This work aims to fill the gap by applying and thoroughly analyzing capsules’ properties at HMC. of a vertical search webpage for books and thus presents a real-world scenario task. Secondly, we thoroughly analyze the properties of capsule networks for HMC. To the best of our knowledge, capsule networks have not yet been"
P19-2045,W18-4412,0,0.0137501,"regarding the selection of categories. By connecting a BiLSTM to a capsule network for relation extraction, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structured categories, they have not yet been applied to HMC. This work aims to fill the gap by applying and thoroughly analyzing capsules’ properties at HMC. of a vertical search webpage for books and thus presents a real-world scenario task. Secondly, we thoroughly analyze the properties of capsule networks for HMC. To the best of our knowledge, capsule"
P19-2045,D14-1181,0,0.00485116,"we thoroughly analyze the properties of capsule networks for HMC. To the best of our knowledge, capsule networks have not yet been applied and tested in the HMC domain. 2 Related Work Neural networks for HMC: In hierarchical multi-label classification (HMC) samples are assigned one or multiple class labels, which are organized in a structured label hierarchy (Silla and Freitas, 2011). For text classification (TC), we treat a document as a sample and its categories as labels. Convolutional Neural Networks (CNNs) and different types of Recurrent Neural Networks (RNNs) (Goodfellow et al., 2016; Kim, 2014), most notably long short-term memory units (LSTMs, Hochreiter and Schmidhuber, 1997) have shown to be highly efficient in TC tasks. For HMC, Cerri et al. (2014) use concatenated multi-layer perceptrons (MLP), where each MLP is associated to one level of the class hierarchy. Kowsari et al. (2017) use multiple concatenated deep learning architectures (CNN, LSTM, and MLP) to HMC on a dataset with a rather shallow hierarchy with only two levels. Similar to Kiritchenko et al. (2005), Baker and Korhonen (2017) treat the HMC task as a multi-label classification problem that considers every label in"
P19-2045,D18-1348,0,0.0156204,"t extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structured categories, they have not yet been applied to HMC. This work aims to fill the gap by applying and thoroughly analyzing capsules’ properties at HMC. of a vertical search webpage for books and thus presents a real-world scenario task. Secondly, we thoroughly analyze the properties of capsule networks for HMC. To the best of our knowledge, capsule networks have not yet been applied and tested in the HMC domain. 2 Related Work Neural networks for HMC: In hierarchical multi-label classification (HMC) samp"
P19-2045,N16-1063,0,0.0394553,"Missing"
P19-2045,D18-1486,0,0.0833523,"(2018) show that capsule networks can outperform traditional neural networks for TC by a great margin when training on single-labeled and testing on multi-labeled documents of the Reuters-21578 dataset since the routing of capsules behaves like a parallel attention mechanism regarding the selection of categories. By connecting a BiLSTM to a capsule network for relation extraction, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structured categories, they have not yet been applied to HMC. This work aims"
P19-2045,D18-1120,0,0.0241946,"dings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 323–330 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics (2018) show that capsule networks can outperform traditional neural networks for TC by a great margin when training on single-labeled and testing on multi-labeled documents of the Reuters-21578 dataset since the routing of capsules behaves like a parallel attention mechanism regarding the selection of categories. By connecting a BiLSTM to a capsule network for relation extraction, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and"
P19-2045,D18-1350,0,0.124875,"28 - August 2, 2019. 2019 Association for Computational Linguistics (2018) show that capsule networks can outperform traditional neural networks for TC by a great margin when training on single-labeled and testing on multi-labeled documents of the Reuters-21578 dataset since the routing of capsules behaves like a parallel attention mechanism regarding the selection of categories. By connecting a BiLSTM to a capsule network for relation extraction, Zhang et al. (2018) show that capsule networks improve at extracting n-ary relations, with n > 2, per sentence and thus confirm the observation of (Zhao et al., 2018) in a different context. For multi-task learning, Xiao et al. (2018) use capsule networks to improve the differentiation between tasks. They encapsulate features in different capsules and use the routing algorithm to cluster features for each task. Further applications to NLP span aggression, toxicity and emotion detection (Srivastava et al., 2018; Rathnayaka et al., 2018), embedding creation for knowledge graph completion (Nguyen et al., 2019), and knowledge transfer of user intents (Xia et al., 2018). Despite the suitable properties of capsule networks to classify into hierarchical structure"
P19-3031,P17-1002,0,0.19588,"e tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note that the IBM data contains many duplicate claims; it was originally publi"
P19-3031,J17-1004,0,0.168622,"laims. tional Random Field (Lafferty et al., 2001) to capture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers"
P19-3031,N16-1030,0,0.0448872,".3 The API routes free text inputs to the respective selected model to be tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note tha"
P19-3031,P14-2050,0,0.0111321,"lementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 Web Discourse Approach SVMhmm TARGER (GloVe) F1 22.90 24.20 Table 3: Comparison of TARGER’s p"
P19-3031,C18-1176,0,0.397908,"ture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloV"
P19-3031,P16-1101,0,0.0180999,"n an Elasticsearch BM25F-index of the DepCC (details in Section 3.2). The online usage is handled via a Flask-based web app whose API accepts AJAX requests from the Web UI component or via API calls (details in Sections 3.3 and 3.4). The web interface is based on the named entity visualiser displaCy ENT.3 The API routes free text inputs to the respective selected model to be tagged with argument information or it routes keyword-based queries to the index to retrieve sentences in which the query terms match argument units. 3.1 Neural Sequence Tagger We implement a BiLSTM-CNN-CRF neural tagger (Ma and Hovy, 2016) for identifying argumentative units and for classifying them as claims or premises. The BiLSTM-CNN-CRF method is a popular sequence tagging approach and achieves (near) state-of-the-art performance for tasks like named entity recognition and part-of-speech tagging (Ma and Hovy, 2016; Lample et al., 2016); it has also been used for argument mining before (Eger et al., 2017). The general method relies on pre-computed word embeddings, a single bidirectional-LSTM/GRU recurrent layer, convolutional character-level embeddings to capture out-of-vocabulary words, and a first-order Condi3 github.com/e"
P19-3031,L18-1008,0,0.0157992,"(2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 Web Discourse Approach SVMhmm TARGE"
P19-3031,L18-1286,1,0.859356,"lack of freely available tools that enable users, especially non-experts, to make use of the field’s recent advances. In this paper, we close this gap by introducing TARGER: a system with a userfriendly web interface1 that can extract argumentative units in free input texts in real-time using 1 ltdemos.informatik.uni-hamburg.de/targer models trained on recent argument mining corpora with a highly configurable and efficient neural sequence tagger. TARGER’s web interface and API also allow for very fast keyword-based argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating por"
P19-3031,D14-1162,0,0.0822942,"lementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolov et al., 2018), or dependency-based embeddings (Levy and Goldberg, 2014) (overview in Table 2). For training, the following variations were used for hyperparameter tuning: optimizer [SGD, Adam], learning rate [0.001, 0.05, 0.01], Data Embeddings Essays Tagger Essays Essays Essays fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours WebD WebD WebD fastText Dependency GloVe (Reimers and Gurevych, 2017) (Reimers and Gurevych, 2017) Ours IBM IBM fastText GloVe (Reimers and Gurevych, 2017) Ours Approach STagBLCC TARGER (GloVe) F1 64.74 64.54 We"
P19-3031,N18-1202,0,0.0112408,"ntative (e.g., punctuation) while the vast majority are tokens in claims and premises (but the only 150 different claims are heavily duplicated). Not surprisingly—given the class imbalance and duplication—, the resulting trained TARGER models “optimistically” identify some argumentative units in almost every input text. We still provide the models as a starting point with the intention to de-duplicate the data and to add more non-argumentative text passages for a more balanced / realistic training scenario. work, we plan to integrate contextualized embeddings with ELMo- and BERT-based models (Peters et al., 2018; Devlin et al., 2018). Finally, by looking at our experimental results as well as tagging examples for free input texts or the DepCC web data, we noticed that despite the recent advances in argument mining, there is still considerable headroom to improve in-domain, but especially out-of-domain argument tagging performance. Free input texts of different styles or genres taken from the web are tagged very inconsistently by the different models. More research on domain adaptation and transfer learning (Ruder, 2019) in the scenario of argument mining needs to address this issue. 4.2 TARGER @ TREC"
P19-3031,D17-1035,0,0.0240147,"i3 github.com/explosion/displacy-ent Claims Premises Major Claims Backing Refutations Rebuttals None Combined Essays WebD IBM 22,443 67,157 10,966 47,619 3,670 20,906 10,775 867 2,247 46,352 8,073,589 35,349,501 3,710,839 148,185 84,817 47,133,929 Table 1: Number of tokens per category in the training datasets. Note that the IBM data contains many duplicate claims; it was originally published as a dataset to identify relevant premises for 150 claims. tional Random Field (Lafferty et al., 2001) to capture dependencies between adjacent tags. Besides the existing BiLSTM-CNN-CRF implementation of Reimers and Gurevych (2017), we also use an own Python 3.6 / PyTorch 1.0 implementation that does not contain any third-party dependencies, has native vectorized code for efficient training and evaluation, and supports several input data formats as well as evaluation functions. The different argument tagging models currently usable through TARGER’s API are trained on the persuasive essays (Essays) (Eger et al., 2017), the web discourse (WebD) (Habernal and Gurevych, 2017), and the IBM Debater (IBM) (Levy et al., 2018) datasets (characteristics in Table 1). The models use GloVe (Pennington et al., 2014), fastText (Mikolo"
P19-3031,D15-1050,0,0.0322126,"mains and use cases, such as an application to search engine ranking that we also describe shortly. 1 Introduction Argumentation is a multi-disciplinary field that extends from philosophy and psychology to linguistics as well as to artificial intelligence. Recent developments in argument mining apply natural language processing (NLP) methods to argumentation (Palau and Moens, 2011; Lippi and Torroni, 2016a) and are mostly focused on training classifiers on annotated text fragments to identify argumentative text units, such as claims and premises (Biran and Rambow, 2011; Habernal et al., 2014; Rinott et al., 2015). More specifically, current approaches mainly focus on three tasks: (1) detection of sentences containing argumentative units, (2) detection of the argumentative units’ boundaries inside sentences, and (3) identifying relations between argumentative units. Despite vital research in argument mining, there is a lack of freely available tools that enable users, especially non-experts, to make use of the field’s recent advances. In this paper, we close this gap by introducing TARGER: a system with a userfriendly web interface1 that can extract argumentative units in free input texts in real-time"
P19-3031,N19-5004,0,0.0139533,"n to integrate contextualized embeddings with ELMo- and BERT-based models (Peters et al., 2018; Devlin et al., 2018). Finally, by looking at our experimental results as well as tagging examples for free input texts or the DepCC web data, we noticed that despite the recent advances in argument mining, there is still considerable headroom to improve in-domain, but especially out-of-domain argument tagging performance. Free input texts of different styles or genres taken from the web are tagged very inconsistently by the different models. More research on domain adaptation and transfer learning (Ruder, 2019) in the scenario of argument mining needs to address this issue. 4.2 TARGER @ TREC Common Core Track Acknowledgments As a proof of concept, we used TARGER’s model pre-trained on essays with dependencybased embeddings in a TREC 2018 Common Core track submission (Bondarenko et al., 2018). The TARGER API served as a subroutine in a pipeline axiomatically re-ranking (Hagen et al., 2016) BM25F retrieval results with respect to their argumentativeness (presence/absence of arguments). For the Washington Post corpus used in the track, the dependency-based essays model best tagged argumentative units i"
P19-3031,N18-5005,0,0.215224,"g.de/targer models trained on recent argument mining corpora with a highly configurable and efficient neural sequence tagger. TARGER’s web interface and API also allow for very fast keyword-based argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating portals. Finally, MARGOT (Lippi and Torroni, 2016b) provides argument tagging for free-text inputs. However, answer times of MARGOT are rather slow for single input sentences (&gt;5 seconds) and the F1 scores of 17.5 for claim detection and 16.7 for evidence detection are slightly worse compared to our approach (see Section 4"
P19-3031,W17-5106,0,0.134112,"ed argument retrieval from a pre-tagged version of the Common Crawl-based DepCC (Panchenko et al., 2018). The native PyTorch implementation underlying TARGER has no external depencies and is available as open source software:2 it can easily be incorporated into any existing NLP pipeline. 2 Related Work There are three publicly available systems offering some functionality similar to TARGER. ArgumenText (Stab et al., 2018) is an argument search engine that retrieves argumentative sentences from the Common Crawl and labels them as pro or con given a keyword-based user query. Similarly, args.me (Wachsmuth et al., 2017) retrieves pro and con arguments from 300,000 arguments crawled from debating portals. Finally, MARGOT (Lippi and Torroni, 2016b) provides argument tagging for free-text inputs. However, answer times of MARGOT are rather slow for single input sentences (&gt;5 seconds) and the F1 scores of 17.5 for claim detection and 16.7 for evidence detection are slightly worse compared to our approach (see Section 4.1). TARGER offers a real-time retrieval functionality similar to ArgumenText and fast real-time freetext argument tagging with the option of switching between different pre-trained state-of-the-art"
R15-1027,W13-5002,1,0.667425,"pted the training and evaluation data and splits from the CoNLL-2011 shared task on coreference resolution (Pradhan et al., 2011), which contains 2,999 documents from the OntoNotes v4.0 corpus (Hovy et al., 2006), and took the number and gender data from the task. Training and testing was performed with predicted mentions on the AUTO set of automatically preprocessed documents. We used the Berkeley System in version 1.0 and a DT created from 120M sentences of news texts (n = 200) using a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features are neighbor terms in a dependency parse, complemented by the dependency label and governing direction (e.g. governing#amod#preliminary#JJ). For evaluation, we used the standard coreference metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (entity/φ4 CEAF; Luo, 2005), as well as their average, computed with the reference scorer v7 (Pradhan et al., 2014). Additionally, we evaluate precision and recall on system-bridging mentions,2 i.e. mentions that appear as bridgi"
R15-1027,D09-1120,0,0.0284254,"lass uses manually crafted resources like WordNet or Wikipedia (Poesio et al., 2004; Ponzetto and Strube, 2006). Despite their quality, 2 Related Work Our work is very similar to Bansal and Klein (2012), who created, among others, features based on IS-As, distributional clusters, and pronoun contexts. However, we chose to use a DT’s list of similar words instead of clustering, and dependency relations as context features instead of N-gram neighborhood. We will compare our approach to Bansal and Klein’s features below. Distributional methods for coreference resolution are mostly pattern-based (Haghighi and Klein, 2009; Kobdani et al., 2011). Recent work by Recasens et al. (2013) used news events as context and exploited rewordings of the same story in different sources. Semantic similarity for the resolution of bridging mentions has been employed by Poesio et al. (1998), Gasperin et al. (2004), and Versley (2007), yet all three works are applied to oracle anaphoric 192 Proceedings of Recent Advances in Natural Language Processing, pages 192–199, Hissar, Bulgaria, Sep 7–9 2015. used both for training and querying the DT. The DT lists for each term the n semantically most similar terms, where n is the expans"
R15-1027,C92-2082,0,0.152132,"are their limited size, slow growth and general-purpose nature. In contrast, using unsupervised/semi-supervised methods for generating knowledge is only limited by the size of input data and adapts to the target domain. We present features exploiting automatically obtained distributional knowledge, following the distributional hypothesis formulated by Harris (1954) that words in similar contexts bear similar meanings. For that we resort to a distributional thesaurus (DT; Lin, 1998) listing semantically similar terms, as well as hyponym-hypernym relations (IS-As) acquired with Hearst patterns (Hearst, 1992), both made available by the JoBimText Project (Biemann and Riedl, 2013). When added to the state-of-the-art Berkeley Coreference Resolution System (Durrett and Klein, 2013), these features show a significant positive impact on bridging mentions. We explore the impact of adding distributional knowledge to a state-of-the-art coreference resolution system. By integrating features based on word and context expansions from a distributional thesaurus (DT), automatically mined IS-A relationships and shallow syntactical clues into the Berkeley system (Durrett and Klein, 2013), we are able to increase"
R15-1027,N06-2015,0,0.0666912,"the expansion size parameter, and semantic similarity is defined as the number of shared significant contexts. mentions, thus not facing spurious mentions, i.e. phrases that are non-referring in the gold standard. Ng (2007) and Lee et al. (2012) made use of Lin’s theasurus in a fully-featured system, but with a smaller expansion size (5 and 10 words, respectively). 3 4 Method Experimental Setting We adapted the training and evaluation data and splits from the CoNLL-2011 shared task on coreference resolution (Pradhan et al., 2011), which contains 2,999 documents from the OntoNotes v4.0 corpus (Hovy et al., 2006), and took the number and gender data from the task. Training and testing was performed with predicted mentions on the AUTO set of automatically preprocessed documents. We used the Berkeley System in version 1.0 and a DT created from 120M sentences of news texts (n = 200) using a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features are neighbor terms in a dependency parse, complemented by the dependency label and go"
R15-1027,P11-1079,0,0.0539912,"Missing"
R15-1027,W04-3250,0,0.0126238,"6.02 56.08 54.31 54.45 53.88 54.44† 54.09 54.44 59.30 59.51 B&K (2012) CO —”— +D UMMY 69.30 68.57 66.11 66.56† 67.67 67.55 58.10 57.12 53.62 54.12† 55.77 55.58 54.31 53.70 53.63 53.80 53.97 53.75 59.14 58.96 Table 1: Metric results achieved by the baseline, dummy setting, and incrementally adding features to the baseline (P = prior expansion, I = IS-A, C = C-expansion and A = attribute features). Also comparing to the Bansal and Klein (2012) co-occurrence feature. Scores with a dagger (†) are significantly better than the BASELINE (paired bootstrap resampling test with N = 10000 and p = 0.05 (Koehn, 2004)). and T = {t1 , . . . , tn } the set of terms for which there exists a cj ∈ C such that the pair (ti , cj ) is a member of the model. We sort the members of T in the descending order of their probability P (ti |C) and take the first 200 elements as the target term’s C-expansion. Defining P (ti |C), we assume conditional independence Q and calculate the plus-one-smoothed MLE as cj ∈C (sig(ti , P cj ) + 1)/(V + sig(∗, cj )), with sig(·, ·) returning the significance value of a term-feature pair stored in the model, and V as the vocabulary size. The coreference feature IN C-EXPANSION then return"
R15-1027,P12-1041,0,0.359322,"in, 2013), these features show a significant positive impact on bridging mentions. We explore the impact of adding distributional knowledge to a state-of-the-art coreference resolution system. By integrating features based on word and context expansions from a distributional thesaurus (DT), automatically mined IS-A relationships and shallow syntactical clues into the Berkeley system (Durrett and Klein, 2013), we are able to increase its F1 score on bridging mentions, i.e. coreferent mentions with non-identical heads, by 8.29 points. Our semantic features improve over the Web-based features of Bansal and Klein (2012). Since bridging mentions are a hard but infrequent class of coreference, this leads to merely small improvements in the overall system. 1 Introduction Automatically recognizing coreference – relating lexical items that refer to the same entity or context in a text – is an important semantic processing step for text understanding tasks such as fact extraction, information retrieval, and entity linking. A common problem of coreference systems is their inability to resolve bridging mentions, i.e. coreferent mentions with non-identical heads (Vieira and Poesio, 2000). For example, a system requir"
R15-1027,D13-1027,0,0.0144542,"ese features are subsumed by our set or provide additional value, we also show the results of combining both in Table 2. The decrease in precision on the test set suggests that the Web features introduce too much noise to the system. 7 Error Analysis Error Span Conflated entities Divided entities Extra entities Missing entities Extra mention Missing mention BASELINE PICA ∆ 399 1303 1626 521 881 577 862 404 1319 1593 559 820 618 842 +5 +16 -33 +38 -61 +41 -20 Table 3: Development set error counts comparison As shown by an automatic classification of errors by the Berkeley Coreference Analyser (Kummerfeld and Klein, 2013) in Table 3, our system is prone to create spurious entities and mentions. The problem arises from semantic relations in the DT that are actually indicators of non-coreference (e.g. antonymy, co-hyponymy), but nevertheless ranked high. The similarity measure does not differentiate between these relations. This produced links like Taipei – South Korea and the men – the women. Since the hypothesis that a mention is non-referring has low probability if it begins with a determiner, the system desperately “searches” for an antecedent. Because of our semantic features, the system achieves higher con"
R15-1027,N04-1038,0,0.0463725,"conditional independence Q and calculate the plus-one-smoothed MLE as cj ∈C (sig(ti , P cj ) + 1)/(V + sig(∗, cj )), with sig(·, ·) returning the significance value of a term-feature pair stored in the model, and V as the vocabulary size. The coreference feature IN C-EXPANSION then returns the rank of t1 in t2 ’s C-expansion with PRIOR’s result semantics. If t1 is from a closed word class, it is first mapped to the first open word class term from its own C-expansion. Unlike typical takes on selectional preference, we expand all mention heads, not only pronouns, to take their contextual role (Bean and Riloff, 2004) into account and to have at least some semantic knowledge for out-of-vocabulary terms. 4. Attribute features inspired by Vieira and Poesio (2000, 556 f.;560) guessing properties of mentions from dependency relations in the text. We consider as attributes all words in a copula, appositive, relative clause, or compound relation to a mention’s head and added the following features: • ATTR IS-IS-A = {true, false}: Its value is true if t1 is among any IS-A set of any attribute of t2 , false otherwise. If true, adds an additional version with the lexicalized IS-A. Figure 1 illustrates the first thr"
R15-1027,P06-1005,0,0.030977,"candidate antecedent. For our experiments, we used the system’s F I NAL feature set. Regarding anaphoricity and the candidate antecedent, it uses the mention’s size in words, syntactic uni- and bigrams of the head, as well as lexicalizations of the head, first, last, preceding, and following word as features. Pairwise features are the distance between the two mentions, once as the number of sentences and once as the number of mentions; whether one mention is within the boundaries of the other; whether they belong to the same speaker; the candidate antecedent’s number and gender using data by Bergsma and Lin (2006); the syntactic uni- and bigrams of both mentions; mention string match or containment; head string match or containment. See Durrett and Klein (2013) for a detailed description of the feature set. The Berkeley System expands the feature space by feature conjunctions: If a pairwise feature f fires for current mention mc and antecedent mention ma , features f ∧ type(c) and f ∧ type(c) ∧ type(a) are also activated, where type(·) returns a mention type literal based on the head’s POS. For pronouns, this is the citation form; for proper and common nouns, PROPER and NOMINAL are returned, respective"
R15-1027,W11-1902,0,0.0738981,"Missing"
R15-1027,D12-1045,0,0.0171421,"s been employed by Poesio et al. (1998), Gasperin et al. (2004), and Versley (2007), yet all three works are applied to oracle anaphoric 192 Proceedings of Recent Advances in Natural Language Processing, pages 192–199, Hissar, Bulgaria, Sep 7–9 2015. used both for training and querying the DT. The DT lists for each term the n semantically most similar terms, where n is the expansion size parameter, and semantic similarity is defined as the number of shared significant contexts. mentions, thus not facing spurious mentions, i.e. phrases that are non-referring in the gold standard. Ng (2007) and Lee et al. (2012) made use of Lin’s theasurus in a fully-featured system, but with a smaller expansion size (5 and 10 words, respectively). 3 4 Method Experimental Setting We adapted the training and evaluation data and splits from the CoNLL-2011 shared task on coreference resolution (Pradhan et al., 2011), which contains 2,999 documents from the OntoNotes v4.0 corpus (Hovy et al., 2006), and took the number and gender data from the task. Training and testing was performed with predicted mentions on the AUTO set of automatically preprocessed documents. We used the Berkeley System in version 1.0 and a DT create"
R15-1027,J00-4003,0,0.623142,"over the Web-based features of Bansal and Klein (2012). Since bridging mentions are a hard but infrequent class of coreference, this leads to merely small improvements in the overall system. 1 Introduction Automatically recognizing coreference – relating lexical items that refer to the same entity or context in a text – is an important semantic processing step for text understanding tasks such as fact extraction, information retrieval, and entity linking. A common problem of coreference systems is their inability to resolve bridging mentions, i.e. coreferent mentions with non-identical heads (Vieira and Poesio, 2000). For example, a system requires semantic knowledge to detect the hypernymic relationship that holds between mentions like a preliminary agreement and the pact. Similarly, modeling selectional preference relies on information beyond the pronoun context itself. There are two different kinds of approaches employed in the past to make this knowledge available as features to a coreference resolution system. The first class uses manually crafted resources like WordNet or Wikipedia (Poesio et al., 2004; Ponzetto and Strube, 2006). Despite their quality, 2 Related Work Our work is very similar to Ban"
R15-1027,W11-0607,0,0.0127226,"t1 is among any of the IS-As of any cluster of t2 , false otherwise. • SHARED IS-As: Calculates the Dice index (Dice, 1945) between each IS-A cluster of t1 and each of t2 and returns the maximum value. Since the data contains some noisy IS-As like bit (originating from is a bit), we added an additional lexicalized feature for SHARED IS-A = true with the shared IS-A that has the highest frequency in the model. 3. A feature targeting the context of a mention’s head to model selectional preference. For this, we define a context-based expansion (Cexpansion). Similar to verb argument expectations (Lenci, 2011), we compose a list of the most likely words appearing in a given context, but do not restrict ourselves to verbs. We exploit the fact that term-context pairs are provided in the JoBimText model (Biemann and Riedl, 2013). Let C be the set of context features of a mention head in the text • PRIOR: Its value is 0 if t1 = t2 , 2 if expansion(t2 ) = ∅, -1 if t1 ∈ / expansion(t2 ), and rank(t1 , expansion(t2 )) 194 B3 Test Development MUC CEAFe P R F1 P R F1 P R F1 Average BASELINE D UMMY 69.88 69.81 63.25 63.92† 66.40 66.74† 61.86 61.86 52.98 53.74† 57.08 57.52† 57.69 58.03 54.31 55.23† 55.95 56.6"
R15-1027,M95-1005,0,0.460456,"AUTO set of automatically preprocessed documents. We used the Berkeley System in version 1.0 and a DT created from 120M sentences of news texts (n = 200) using a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features are neighbor terms in a dependency parse, complemented by the dependency label and governing direction (e.g. governing#amod#preliminary#JJ). For evaluation, we used the standard coreference metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (entity/φ4 CEAF; Luo, 2005), as well as their average, computed with the reference scorer v7 (Pradhan et al., 2014). Additionally, we evaluate precision and recall on system-bridging mentions,2 i.e. mentions that appear as bridging to the system, but not necessarily to a human. Let head(mi ) return the predicted head of the i-th mention in a document, C(mi ) be the (gold or system) coreference chain of mi , and C ∗ (mi ) = hmj : mj ∈ C(mi ) ∧ j &lt; i ∧ head(mj ) is a nouni be the sequence of noun antecedents of mi . A mention mi is system-bridging if hea"
R15-1027,W11-1909,0,0.0492652,"Missing"
R15-1027,lin-etal-2010-new,0,0.0266222,"Missing"
R15-1027,P98-2127,0,0.404366,"act they may decrease the performance when added to the system (Lee et al., 2011; Zhou et al., 2011). Further disadvantages are their limited size, slow growth and general-purpose nature. In contrast, using unsupervised/semi-supervised methods for generating knowledge is only limited by the size of input data and adapts to the target domain. We present features exploiting automatically obtained distributional knowledge, following the distributional hypothesis formulated by Harris (1954) that words in similar contexts bear similar meanings. For that we resort to a distributional thesaurus (DT; Lin, 1998) listing semantically similar terms, as well as hyponym-hypernym relations (IS-As) acquired with Hearst patterns (Hearst, 1992), both made available by the JoBimText Project (Biemann and Riedl, 2013). When added to the state-of-the-art Berkeley Coreference Resolution System (Durrett and Klein, 2013), these features show a significant positive impact on bridging mentions. We explore the impact of adding distributional knowledge to a state-of-the-art coreference resolution system. By integrating features based on word and context expansions from a distributional thesaurus (DT), automatically min"
R15-1027,H05-1004,0,0.0425985,"in version 1.0 and a DT created from 120M sentences of news texts (n = 200) using a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features are neighbor terms in a dependency parse, complemented by the dependency label and governing direction (e.g. governing#amod#preliminary#JJ). For evaluation, we used the standard coreference metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (entity/φ4 CEAF; Luo, 2005), as well as their average, computed with the reference scorer v7 (Pradhan et al., 2014). Additionally, we evaluate precision and recall on system-bridging mentions,2 i.e. mentions that appear as bridging to the system, but not necessarily to a human. Let head(mi ) return the predicted head of the i-th mention in a document, C(mi ) be the (gold or system) coreference chain of mi , and C ∗ (mi ) = hmj : mj ∈ C(mi ) ∧ j &lt; i ∧ head(mj ) is a nouni be the sequence of noun antecedents of mi . A mention mi is system-bridging if head(mi ) is a noun, C ∗ (mi ) 6= ∅, and for all m ∈ C ∗ (mi ) it holds"
R15-1027,P04-1019,0,0.0486899,"ir inability to resolve bridging mentions, i.e. coreferent mentions with non-identical heads (Vieira and Poesio, 2000). For example, a system requires semantic knowledge to detect the hypernymic relationship that holds between mentions like a preliminary agreement and the pact. Similarly, modeling selectional preference relies on information beyond the pronoun context itself. There are two different kinds of approaches employed in the past to make this knowledge available as features to a coreference resolution system. The first class uses manually crafted resources like WordNet or Wikipedia (Poesio et al., 2004; Ponzetto and Strube, 2006). Despite their quality, 2 Related Work Our work is very similar to Bansal and Klein (2012), who created, among others, features based on IS-As, distributional clusters, and pronoun contexts. However, we chose to use a DT’s list of similar words instead of clustering, and dependency relations as context features instead of N-gram neighborhood. We will compare our approach to Bansal and Klein’s features below. Distributional methods for coreference resolution are mostly pattern-based (Haghighi and Klein, 2009; Kobdani et al., 2011). Recent work by Recasens et al. (20"
R15-1027,N06-1025,0,0.0420299,"ve bridging mentions, i.e. coreferent mentions with non-identical heads (Vieira and Poesio, 2000). For example, a system requires semantic knowledge to detect the hypernymic relationship that holds between mentions like a preliminary agreement and the pact. Similarly, modeling selectional preference relies on information beyond the pronoun context itself. There are two different kinds of approaches employed in the past to make this knowledge available as features to a coreference resolution system. The first class uses manually crafted resources like WordNet or Wikipedia (Poesio et al., 2004; Ponzetto and Strube, 2006). Despite their quality, 2 Related Work Our work is very similar to Bansal and Klein (2012), who created, among others, features based on IS-As, distributional clusters, and pronoun contexts. However, we chose to use a DT’s list of similar words instead of clustering, and dependency relations as context features instead of N-gram neighborhood. We will compare our approach to Bansal and Klein’s features below. Distributional methods for coreference resolution are mostly pattern-based (Haghighi and Klein, 2009; Kobdani et al., 2011). Recent work by Recasens et al. (2013) used news events as cont"
R15-1027,W11-1901,0,0.0228239,"the DT. The DT lists for each term the n semantically most similar terms, where n is the expansion size parameter, and semantic similarity is defined as the number of shared significant contexts. mentions, thus not facing spurious mentions, i.e. phrases that are non-referring in the gold standard. Ng (2007) and Lee et al. (2012) made use of Lin’s theasurus in a fully-featured system, but with a smaller expansion size (5 and 10 words, respectively). 3 4 Method Experimental Setting We adapted the training and evaluation data and splits from the CoNLL-2011 shared task on coreference resolution (Pradhan et al., 2011), which contains 2,999 documents from the OntoNotes v4.0 corpus (Hovy et al., 2006), and took the number and gender data from the task. Training and testing was performed with predicted mentions on the AUTO set of automatically preprocessed documents. We used the Berkeley System in version 1.0 and a DT created from 120M sentences of news texts (n = 200) using a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features ar"
R15-1027,P14-2006,0,0.174621,"sing a dependency parse holing system (Biemann and Riedl, 2013, 72 f.) and including IS-As clustered into senses (Gliozzo et al., 2013).1 Its terms are composed of a single word’s lemma and its POS tag (e.g. pact#NN), while context features are neighbor terms in a dependency parse, complemented by the dependency label and governing direction (e.g. governing#amod#preliminary#JJ). For evaluation, we used the standard coreference metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe (entity/φ4 CEAF; Luo, 2005), as well as their average, computed with the reference scorer v7 (Pradhan et al., 2014). Additionally, we evaluate precision and recall on system-bridging mentions,2 i.e. mentions that appear as bridging to the system, but not necessarily to a human. Let head(mi ) return the predicted head of the i-th mention in a document, C(mi ) be the (gold or system) coreference chain of mi , and C ∗ (mi ) = hmj : mj ∈ C(mi ) ∧ j &lt; i ∧ head(mj ) is a nouni be the sequence of noun antecedents of mi . A mention mi is system-bridging if head(mi ) is a noun, C ∗ (mi ) 6= ∅, and for all m ∈ C ∗ (mi ) it holds that head(m) 6= head(mi ). A bridging mention mi from the gold chain CG is a true positi"
R15-1027,N13-1110,0,0.013497,"Poesio et al., 2004; Ponzetto and Strube, 2006). Despite their quality, 2 Related Work Our work is very similar to Bansal and Klein (2012), who created, among others, features based on IS-As, distributional clusters, and pronoun contexts. However, we chose to use a DT’s list of similar words instead of clustering, and dependency relations as context features instead of N-gram neighborhood. We will compare our approach to Bansal and Klein’s features below. Distributional methods for coreference resolution are mostly pattern-based (Haghighi and Klein, 2009; Kobdani et al., 2011). Recent work by Recasens et al. (2013) used news events as context and exploited rewordings of the same story in different sources. Semantic similarity for the resolution of bridging mentions has been employed by Poesio et al. (1998), Gasperin et al. (2004), and Versley (2007), yet all three works are applied to oracle anaphoric 192 Proceedings of Recent Advances in Natural Language Processing, pages 192–199, Hissar, Bulgaria, Sep 7–9 2015. used both for training and querying the DT. The DT lists for each term the n semantically most similar terms, where n is the expansion size parameter, and semantic similarity is defined as the"
R15-1027,D07-1052,0,0.0253468,"we chose to use a DT’s list of similar words instead of clustering, and dependency relations as context features instead of N-gram neighborhood. We will compare our approach to Bansal and Klein’s features below. Distributional methods for coreference resolution are mostly pattern-based (Haghighi and Klein, 2009; Kobdani et al., 2011). Recent work by Recasens et al. (2013) used news events as context and exploited rewordings of the same story in different sources. Semantic similarity for the resolution of bridging mentions has been employed by Poesio et al. (1998), Gasperin et al. (2004), and Versley (2007), yet all three works are applied to oracle anaphoric 192 Proceedings of Recent Advances in Natural Language Processing, pages 192–199, Hissar, Bulgaria, Sep 7–9 2015. used both for training and querying the DT. The DT lists for each term the n semantically most similar terms, where n is the expansion size parameter, and semantic similarity is defined as the number of shared significant contexts. mentions, thus not facing spurious mentions, i.e. phrases that are non-referring in the gold standard. Ng (2007) and Lee et al. (2012) made use of Lin’s theasurus in a fully-featured system, but with"
R15-1027,C98-2122,0,\N,Missing
R15-1027,D13-1203,0,\N,Missing
riedl-etal-2014-distributed,W10-2309,1,\N,Missing
riedl-etal-2014-distributed,C04-1146,0,\N,Missing
riedl-etal-2014-distributed,P14-1096,1,\N,Missing
riedl-etal-2014-distributed,P97-1009,0,\N,Missing
riedl-etal-2014-distributed,N04-3012,0,\N,Missing
riedl-etal-2014-distributed,D13-1089,1,\N,Missing
riedl-etal-2014-distributed,S13-1035,0,\N,Missing
riedl-etal-2014-distributed,N13-1133,1,\N,Missing
riedl-etal-2014-distributed,C12-1109,1,\N,Missing
S12-1059,S12-1051,0,0.406442,": Official results on the test data for the top 5 participating runs out of 89 which were achieved on the known datasets MSRpar, MSRvid, and SMTeuroparl, as well as on the surprise datasets OnWN and SMTnews. We report the ranks (#1 : ALL, #2 : ALLnrm, #3 : Mean) and the corresponding Pearson correlation r according to the three offical evaluation metrics (see Sec. 6). The provided baseline is shown at the bottom of this table. metrics ALL (r = .823)4 and Mean (r = .677), and #2 for ALLnrm (r = .857). An exhaustive overview of all participating systems can be found in the STS task description (Agirre et al., 2012). 7 Conclusions and Future Work In this paper, we presented the UKP system, which performed best across the three official evaluation metrics in the pilot Semantic Textual Similarity (STS) task at SemEval-2012. While we did not reach the highest scores on any of the single datasets, our system was most robust across different data. In future work, it would be interesting to inspect the performance of a system that combines the output of all participating systems in a single linear model. We also propose that two major issues with the datasets are tackled in future work: (a) It is unclear how t"
S12-1059,C10-1005,0,0.0431732,"Missing"
S12-1059,P02-1020,0,0.0272336,"antic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on su"
S12-1059,C04-1051,0,0.0935903,"per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations,"
S12-1059,W99-0625,0,0.381092,"ifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, . . . , 10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetok"
S12-1059,O97-1002,0,0.48473,"the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Sema"
S12-1059,P07-2045,0,0.00189328,"n cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense disambiguation (Biemann, 2012). This system automatically provides substitutions for a set of about 1,000 frequent English nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translation"
S12-1059,2005.mtsummit-papers.11,0,0.00230974,"nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translations. 2.4 Measures Related to Structure and Style In our system, we also used measures which go beyond content and capture similarity along the structure and style dimensions inherent to texts. However, as we report later on, for this content1 www.wiktionary.org 0-5-grams, grow-diag-final-and alignment, m"
S12-1059,P98-2127,0,0.0293197,"nt to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analys"
S12-1059,W01-0515,0,0.305307,"nes a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, . . . , 15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the t"
S12-1059,R11-1063,0,0.0123301,"counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1 . 436 Textual Entailment We experimented with using the BIUTEE textual entailment system (Stern and Dagan, 2011) for generating entailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one additional feature per POS tag. However, only the feature based on cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense"
S12-1059,W07-1401,0,\N,Missing
S12-1059,C98-2122,0,\N,Missing
S13-2007,D10-1115,0,0.0186256,"ations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dict"
S13-2007,S13-2018,0,0.0375986,"Missing"
S13-2007,S13-2016,0,0.0556694,"Missing"
S13-2007,W10-2805,0,0.0217452,"3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are nat"
S13-2007,S13-2020,0,0.0615235,"Missing"
S13-2007,P08-1028,0,0.0683415,"s a core problem, since satisfactory performance in computing the similarity of full sentences depends on similarity computations on shorter sequences. 3.1 1. the act or state of touching; a touching or meeting, as of two things or people. Background and Description This subtask is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Diction"
S13-2007,P03-1048,0,0.0187643,"nts were allowed to use or ignore the training data, i.e. the systems could be supervised or unsupervised. Unsupervised systems were allowed to use the training data for development and parameter tuning. Since this is a core task, participating systems were not be able to use dictionaries or other prefabricated lists. Instead, they were allowed to use distributional similarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsuper"
S13-2007,S13-2019,0,0.0932092,"tional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 .825 .857 .797 .854 .881 .893 .937"
S13-2007,D11-1094,0,0.0586444,"Missing"
S13-2007,S13-2017,0,0.0241557,"Missing"
S13-2007,S13-2008,0,0.0216165,"milarity models, selectional preferences, measures of semantic similarity etc. Participating system responses were scored in terms of standard information retrieval measures: accuracy (A), precision (P), recall (R) and F1 score (Radev et al., 2003). Systems were encouraged to submit at most 3 solutions for each language, but submissions for fewer languages were accepted. Five research teams participated. Ten system runs were submitted for English, one for German (on data set: German - no names) and one for Italian. Table 2 illustrates the results of the evaluation process. The teams of (HsH) (Wartena, 2013), CLaC (Siblini and Kosseim, 2013), UMCC DLSI-(EPS) (D´avila et al., 2013), and ITNLP, the Harbin Institute of Technology, approached the task in a supervised way, while MELODI (Van de Cruys et al., 2013) participated with two unsupervised approaches. Interestingly, Language Rank Participant Id 1 3 2 4 5 6 7 8 9 10 HsH CLaC CLaC CLaC MELODI UMCC DLSI-(EPS) ITNLP MELODI ITNLP ITNLP German 1 Italian 1 English run Id A R P rej. R rej. P F1 1 3 2 1 lvw 1 3 dm 1 2 .803 .794 .794 .788 .748 .724 .703 .689 .663 .659 .752 .707 .695 .638 .614 .613 .501 .481 .392 .427 .837 .856 .867 .910 .838 .787 .840 ."
S13-2007,C10-1142,1,0.346352,"task is based on the assumption that we first need a basic set of functions to compose the meaning of two words, in order to construct more complex models that compositionally determine the meaning of sentences, as a second step. For compositional distributional semantics, the need for these basic functions is discussed in Mitchell and Lapata (2008). Since then, many models have been proposed for addressing the task (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Guevara, 2010), but still comparative analysis is in general based on comparing sequences that consist of two words. As in Zanzotto et al. (2010), this subtask proposes to compare the similarity of a 2-word sequence and a single word. This is important as it is the basic step to analyse models that can compare any word sequences of different length. The development and testing set for this subtask were built based on the idea described in Zanzotto et al. (2010). Dictionaries were used as sources of 41 Figure 1: The definition of contact in a sample dictionary positive training examples. Dictionaries are natural repositories of equivalences between words under definition and sequences of words used for defining them. Figure 1 presents t"
S13-2007,zesch-etal-2008-extracting,1,0.80164,"Missing"
S16-1174,esuli-sebastiani-2006-sentiwordnet,0,0.0269477,"is defined by considering whether the current token is present in dependency relations ‘nsubj’, ‘dep’, ‘amod’, ‘nmod’ and ‘dobj’ or not. • Character N-grams: We use all substrings up to length 5 of the current token as features. • Orthographic feature: This feature checks whether the current token starts with the capitalized letter or not. • DT features: We use the top 5 DT expansions of current token as the features. • Expansion Score: OTEs have opinion around them. Opinions are regularly lexicalized with words found in sentiment lexicons. We calculate sentiment score based on SentiWordNet5 (Esuli and Sebastiani, 2006) in English language. For Non-English language, we use our induced lexicons. We calculate sentiment score by considering the window size of 10 (preceding 5 and following 5 tokens of the target one). 5 http://sentiwordnet.isti.cnr.it/ 1131 We additionally extract the following features only for English language. • Chunk information: To identify the boundaries of multi-word OTEs, we use chunk information of the current token as the features. • Lemma: Lemmatization trims the inflectional forms and derivationally related forms of a token to a common base form. • WordNet: We use top 4 noun synsets"
S16-1174,W14-5117,1,0.852868,"Missing"
S16-1174,P97-1023,0,0.0961654,"Entries Positive Negative Neutral 2005 4789 12953 4120 3314 5923 8496 2992 9338 10339 5993 18308 7636 2175 1737 7869 12480 4306 3217 8849 7697 2945 1900 2515 1382 6547 1838 1916 4467 9077 1447 Table 2: Expansion statistics for induced lexicons. Common entries denote the number of words which are present both in the seed lexicon and the induced lexicon. of more than 50 in the background corpus (English8 , French9 , Spanish10 , Dutch11 , Russian12 , Arabic13 ). Finally, we compute the normalized positive, negative and neutral score for each word similar to (Kumar et al., 2015), and inspired by (Hatzivassiloglou and McKeown, 1997). The core assumption is that words tend to be semantically more similar to words of same sentiment. Hence, words appearing more in the expansions of positive (negative/neutral) words get assigned a higher positive (negative/neutral) sentiment score, Here, in difference to (Kumar et al., 2015), we compute normalized positive, negative and neutral scores rather than assigning one of the polarity class to the words. It should be noted that the volume of induced lexicon depends on two factors: (i) number of words in the seed lexicon that have expansions and (ii) pruning threshold for obtaining th"
S16-1174,R15-2003,1,0.756371,"ther than English, we use the universal parser2 for tokenization and parsing. Since we deal with the OTE as a sequence labelling problem, it is necessary to identify the boundary of OT properly. We follow the standard BIO notation, where ‘BASP’, ‘I-ASP’ and ‘O’ represent the beginning, intermediate and outside tokens of a multi-word OTE respectively. e.g. In, ‘Chow (B-ASP) fun (I-ASP) was (O) very (O) dry (O) . (O)’, ’Chow Fun’ is the OTE. 2.2 Features for Aspect Category Detection • Domain Dependency Graph: We use the aspects list produced by Domain Dependency Graph (DDG) for each domain by (Kohail, 2015). The idea is to detect topics underlying a mixed-domain dataset, aggregate individual dependency relations between domain-specific content words, weigh them with tf-idf and produce a DDG by selecting the highest-ranked words and their dependency relations. Since the domains are already given, no topic modeling is required. However, only one domain was provided for French and Spanish, we used ex1 nlp.stanford.edu/software/corenlp.shtml 2 http://www.undl.org/unlsys/uparser/UP. htm 1130 Token drinks price fresh laptop toshiba touchpad DT Expansion beers, wines, coffee, liquids, beverage prices,"
S16-1174,N15-1078,0,0.0106109,"he current token with Stanford CoreNLP tool, and use the NERsequence labels in BIO-scheme as features. 2.4 Features for Sentiment Polarity Classification • Lexical Acquisition: We use lexical expansion for inducing sentiment words based on distributional hypothesis. We observe that for rare words, unseen instances and limited coverage of available lexicons, the distributional expansion can provide a useful backoff technique, also cf. (Govind et al., 2014). For all languages, we construct a polarity lexicon using an external corpus and seed sentiment lexicon. For seed lexicons, we use English (Salameh et al., 2015) and Arabic (Salameh et al., 2015) versions of Bing Liu’s lexicon (Hu and Liu, 2004) for English and Arabic respectively, VU sentiment lexicon6 for French, Dutch and Spanish, a lexicon by (Panchenko, 2014) for Russian, and SentiTurkNet (Dehkharghani et al., 2015) and NRC Emotion for Turkish7 . For inducing a lexicon, we obtain the top 100 DT expansion of each word in the seed lexicon. Next we accept candidate terms that a) occur in the expansions of at least 10 seed terms, b) have a corpus frequency 6 https://github.com/opener-project/ VU-sentiment-lexicon 7 http://saifmohammad.com/WebPages/ N"
S16-1174,S14-2038,0,0.123164,"ation of the current, preceding two and following two tokens as the features. • Head Word and its PoS: We use the head word of the noun phrase and PoS information of the head word. • Prefix and Suffix: We use prefix and suffix of length up to four characters. • Frequent Aspect Term: We build a list of frequently occurring OTEs from the training set. An OTE is considered to be frequent if it appears at least four times in the training corpus. We define a binary feature for the presence or absence of extracted OTEs. • Dependency Relations: In English language, features are defined in line with (Toh and Wang, 2014). For other languages, feature is defined by considering whether the current token is present in dependency relations ‘nsubj’, ‘dep’, ‘amod’, ‘nmod’ and ‘dobj’ or not. • Character N-grams: We use all substrings up to length 5 of the current token as features. • Orthographic feature: This feature checks whether the current token starts with the capitalized letter or not. • DT features: We use the top 5 DT expansions of current token as the features. • Expansion Score: OTEs have opinion around them. Opinions are regularly lexicalized with words found in sentiment lexicons. We calculate sentiment"
S16-1174,S14-2077,0,0.0153565,"are provided in Table 2. We compute the sum of positive, negative and 8 https://snap.stanford.edu/data/ web-Amazon.html 9 http://wacky.sslmit.unibo.it/doku.php? id=corpora 10 http://corporafromtheweb.org/escow14/ 11 http://corporafromtheweb.org/nlcow14/ 12 lib.ruc.ecebooks 13 http://corpora2.informatik.uni-leipzig. de 1132 neutral scores of tokens using induced lexicon for that language as features. In addition, scores as given in the seed lexicon are also used as features. For English, we also computed these features from different lexicons: AFINN (Nielsen, 2011), NRC Hashtag, Sentiment 140 (Zhu et al., 2014), NRC Emotion (Mohammad and Turney, 2013) and Bing Liu (Hu and Liu, 2004). • Word N-gram: All unigrams and bigrams tokens are extracted from the training set are used as a binary feature, where 1 and 0 indicates the presence and absence of n-grams in the review. • Entity-Attribute Pair: We use E#A pair as a binary feature for sentiment classification. 3 Datasets, Experimental Results and Discussions For feature selection and hyperparameter tuning, we perform five-fold cross-validation on the training set. For Slot 1 and Slot 3, we use supervised classification using Support Vector Machine (SVM"
S16-1206,baroni-bernardini-2004-bootcat,0,0.0434616,"awl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows that such small in-domain corpora can be already useful for taxonomy extraction, we assumed that better results can be obtained if bigger dom"
S16-1206,S16-1168,0,0.255599,"Missing"
S16-1206,S16-1205,0,0.309212,"and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization procedure based on supervised learning, TAXI outperforms QASSIT in terms of comparisons with the gold standard. Possible reasons why our method performs better are (1) QASSIT use no substring features, (2) this team relies on s"
S16-1206,goldhahn-etal-2012-building,0,0.0150556,"e use three general purpose corpora in our approach presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is aro"
S16-1206,S15-2152,0,0.0318236,"co-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used features based on substrings and co-occurrence statistics (Grefenstette, 2015). Lefever et al. (2015) reached the second place gathered hypernyms from patterns, substrings and WordNet. Tan et al. (2015) used word embeddings, reaching the third place. 3 Taxonomy Induction Method Our approach is characterized by scalability and simplicity, assuming that being able to process larger input data is more important than the sophisticated extraction inference. Our approach to taxonomy induction takes as input a set of domain terms and general-domain text corpora and outputs a taxonomy. It consists of four steps. Firstly, we crawl domain-specific corpora based on terminology of"
S16-1206,C92-2082,0,0.718669,"some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated i"
S16-1206,R11-2017,0,0.141238,"Missing"
S16-1206,P10-1150,0,0.0134154,"ncrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-syntactic patterns to harvest hypernyms from the Web. The extracted hypernym relation graph is subsequently pruned. Veraldi et al. (2013) proposed a graph-based algorithm to learn a taxonomy from textual definitions, extracted from a corpus and the Web. An optimal branching algorithm is used to induce a taxonomy. Finally, Bordea et al. (2015) introduced the first shared task on Taxonomy Extraction Evaluation to provide a common ground for evaluation. Six systems participated in the competition. The top system in this challange used fea"
S16-1206,S15-2157,0,0.0549555,"ch presented in Table 1: Wikipedia, 59G and CommonCrawl1 . 1 https://commoncrawl.org 1321 Wikipedia 59G CommonCrawl FocusedCrawl Food FocusedCrawl Environment FocusedCrawl Science EN 11.0 59.2 168000.0 ‡ 22.8 23.9 8.8 FR 3.2 – – 7.9 8.9 5.4 NL 1.4 – – 3.4 2.0 6.6 IT 3.0 – – 3.6 7.1 5.1 Table 1: Corpora sizes used in our system in GB, where ‡is the size of the crawl archive. The second corpus is a concatenation of the English Wikipedia, Gigaword (Parker et al., 2009), ukWaC (Ferraresi et al., 2008) and a news corpora from the Leipzig Collection (Goldhahn et al., 2012). Domain-Specific Corpora. Lefever (2015) showed the usefulness for taxonomy extraction of domain dependent corpora crawled from the Web using BootCat (Baroni and Bernardini, 2004). This method takes terms as input, which are randomly combined into sequences of a pre-defined length, and sent to a Web search engine. The search results, i.e. the returned URLs, compose a domaindependent corpus. The number of input terms, the number of queries and the amount of desired URLs impact the size of the corpus. With 1,000 web queries and 10 URLs per query, the expected size of the resulting corpus is around 300 MB. While Lefever (2015) shows th"
S16-1206,P10-1023,1,0.702747,", to adapt the method to a new domain or language, only a small amount of manual labour is needed. 2 1 Introduction In this paper, we describe TAXI – a taxonomy induction method first presented at the SemEval 2016 challenge on Taxonomy Extraction Evaluation (Bordea et al., 2016). We consider taxonomy induction as a process that should – as much as possible – be driven solely on the basis of raw text processing. While some labeled examples might be utilized to tune the extraction and induction process, we avoid relying on structured lexical resources such as WordNet (Miller, 1995) or BabelNet (Navigli and Ponzetto, 2010). We rather envision a situation where a taxonomy shall be induced in a new domain or a new language for which such resources do not Related Work The extraction of taxonomic relationships from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which ar"
S16-1206,S16-1202,0,0.035602,"eline BestComp TAXI 0 0 0 0.009 0.016 0.189 64.28 178.22 64.94 40.50 34.89 1.00 0.009 0.016 0.189 n.a. 0.298 0.625 Table 3: Overall scores obtained by averaging the results over domains (Environment, Science, Food) and languages (NL, FR, IT) for the multilingual setting. The BestComp lists the respective best scores across four our competitors. The best scores excluding the baseline are set in boldface. Definitions of the measures are available in Section 4. that improves structure of the resource. These united mechanisms are not used in other submissions to the challenge. The NUIG-UNLP team (Pocostales, 2016) relies on vector directionality in dense word embedding spaces. Such approximation of patterns based on distributional similarity provided good recall, but attained low precision. The QASSIT team (Cleuziou and Moreno, 2016), who ranked second in the competition, uses patterns to extract hypernym candidates, but they rely solely on the Wikipedia. Subsequently, an optimization technique based on genetic algorithms is used to learn the parametrization of a so-called pretopological space, which leads to desired structural properties of the resulting taxonomy. While we use simpler optimization pro"
S16-1206,L16-1572,1,0.831085,"already useful for taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,L16-1056,1,0.0775824,"PatternSim. This system was used to process English and French corpora. It encodes patterns in the form of finite state transducers implemented with the Unitex corpus processor.2 PatternSim relies on 10 English patterns yielding average precision of top 5 extracted semantic relations per word of 0.69 (Panchenko et al., 2012). For French, 9 hypernym extraction patterns are used providing precision at top 5 of 0.63 (Panchenko et al., 2013). WebISA. In addition to PattaMaika and PatternSim, we used a publicly available database of English hypernym relations extracted from the CommonCrawl corpus (Seitner et al., 2016). We used 108 million hypernym relations with frequency above one. This collection of relations was harvested using a regexp-based implementation of 59 patterns collected from the literature. Combination of hypernyms. Result of the extraction are 18 collections of hypernym relations listed in Table 2. Even the huge WebISA collection extracted from tens of terabytes of text does not provide hypernyms for all rare taxonomic terms, such as “ground and whole bean coffee” and “black sesame rice cake”. On the other hand, most of the collections contain many noisy relations. For instance, frequent re"
S16-1206,P06-1101,0,0.455322,"se sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning task is defined as the problem of finding the taxonomy that maximizes the probability of individ1320 Proceedings of SemEval-2016, pages 1320–1327, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics ual relations extracted by the classifiers. Kozareva and Hovy (2010) start from a set of root terms and use Hearst-like lexico-synt"
S16-1206,S15-2155,0,0.132777,"Missing"
S16-1206,S16-1203,0,0.384121,"r taxonomy extraction, we assumed that better results can be obtained if bigger domain-specific corpora are used. We therefore follow a different approach based on focused crawling, where BootCat is used only for initialization of seed URLs. We use the provided taxonomy terms as input for the BootCat method, generate 1,000 random triples, and use the retrieved URLs as a starting point for further crawling. Focused crawling is an extension to standard web crawling where URLs, expected to point to relevant web documents, are prioritized for download (Chakrabarti et al., 1999). Remus and Biemann (2016) introduced a focused crawling approach based on language modeling. The idea is that relevant web documents refer to other relevant web documents, where the relevance of a web document is computed by considering a statistical n-gram language model of a small, initially provided, domain-defining corpus. We provide a domain-defining corpus for each category by using Wikipedia articles, that are directly contained in the matching Wikipedia category. For example, for the the Food domain we used the Wikipedia articles of Category:Foods to build a language model of the Food domain. The language mode"
S16-1206,J13-3007,1,0.79304,"= j; (ti , tj ) ∈ T × T }. The pairs classified using the positive class are added to the taxonomy. nodes with out degree equal to zero, and the presence of cycles. Second, system outputs were compared against the corresponding domain gold standards and performances are evaluated in terms of Fscore. Here precision and recall are based on the number of edges in common with the gold standard taxonomy over the number of system edges and over the number of gold standard edges respectively. To better compare against gold standard taxonomies the task included the evaluation of a cumulative measure (Velardi et al., 2013), namely Cumulative Fowlkes & Mallows Measure (F&M), where the similarity between the system and the reference taxonomies are measured as the combination of the hierarchical cluster similarities. Finally, the organizers performed manual quality assessment to estimate the precision of the hypernyms. To compute this measure, annotators labeled a sample of 100 hypernym relations as correct or wrong. The taxonomy extraction was evaluated on four languages, namely English, Dutch, French and Italian, and three different domains (Food, Science and Environment). A detailed description of the evaluatio"
S16-1206,P09-1031,0,0.0656634,"ips from text is a long-standing challenge in ontology learning, see e.g. Biemann (2005) for a survey. The literature on hypernym extraction offers a high variability of methods, from simple lexical patterns (Hearst, 1992; Oakes, 2005), similar to those used in our method, to complex statistical techniques (Agirre et al., 2000; Ritter et al., 2009). Snow et al. (2004) use sentences that contain two terms which are known to be hypernyms. They parse sentences and extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features and applied to text corpora. Yang and Callan (2009) presented a semisupervised taxonomy induction framework that integrates co-occurrence, syntactic dependencies, lexical-syntactic patterns and other features to learn an ontology metric, calculated in terms of the semantic distance for each pair of terms in a taxonomy. Terms are incrementally clustered on the basis of their ontology metric scores. Snow et al. (2006) perform incremental construction of taxonomies using a probabilistic model. They combine evidence from multiple supervised classifiers trained on large training datasets of hyponymy and co-hyponymy relations. The taxonomy learning"
S16-1206,S15-2151,1,\N,Missing
S17-2009,P05-1045,0,0.0125919,"inding the focus of the question and comment is important in measuring if the comment specifically covers the aspects of the question. We extract keywords from the texts using the RAKE keyword extraction algorithm (Rose et al., 2010), and derive features from the keyword match between question and comment. We also use the relative importance of common keywords as feature values. In case of factoid questions, or especially in subtask B, Named Entity Recognition becomes an important tool for computing the relevance of a text. We extract named entities using the Stanford Named Entity Recognizer (Finkel et al., 2005) and classify words into seven entity categories including PERSON, LOCATION, ORGANIZATION, DATE, MONEY, PERCENT, and TIME. We compute if both question and comment have named entities, and if these belong to the same The algorithm to construct this dynamic graph is given in Algorithm 1. We simultaneously construct two graphs, a user graph and a dialogue graph. Initially, the user graph has the question node and the dialogue graph is empty. We add new users to the graphs according to the timestamp of their occurrence in the thread. For each new comment, we add edges to each previous user and the"
S17-2009,S16-1137,0,0.0215787,"Missing"
S17-2009,P15-2113,0,0.0524853,"Missing"
S17-2009,S15-2048,0,0.0400066,"Missing"
S17-2009,S17-2003,0,0.0764522,"Missing"
S17-2009,S16-1172,0,0.0219561,"Missing"
S17-2009,S15-2036,0,0.134421,"Missing"
S17-2009,S16-1135,0,0.0366377,"Missing"
S17-2025,S16-1103,0,0.0149865,"t approaches often focus on training regression models on traditional lexical surface overlap features. Recently, deep learning models have achieved very promising results in semantic textual sim∗ *These authors contributed equally to this work 175 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 175–179, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics ilarity. The top three best performing systems from STS 2016 used sophisticated deep learning based models (Rychalska et al., 2016; Brychc´ın and Svoboda, 2016; Afzal et al., 2016). The highest correlation score was obtained by Rychalska et al. (2016). They proposed a textual similarity model that combines recursive auto-encoders (RAE) from deep learning with WordNet award penalty, which helps to adjusts the Euclidean distance between word vectors. 3 Figure 1: Unsupervised sentence alignment System Description Our contribution in the STS shared task includes three different systems: supervised, unsupervised and supervised-unsupervised ensemble. Our models are mainly developed to measure semantic similarity between monolingual sentences in English. For the cross-lingual"
S17-2025,S16-1091,0,0.0534931,"ty (Agirre et al., 2017). Most of the state-of-the-art approaches often focus on training regression models on traditional lexical surface overlap features. Recently, deep learning models have achieved very promising results in semantic textual sim∗ *These authors contributed equally to this work 175 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 175–179, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics ilarity. The top three best performing systems from STS 2016 used sophisticated deep learning based models (Rychalska et al., 2016; Brychc´ın and Svoboda, 2016; Afzal et al., 2016). The highest correlation score was obtained by Rychalska et al. (2016). They proposed a textual similarity model that combines recursive auto-encoders (RAE) from deep learning with WordNet award penalty, which helps to adjusts the Euclidean distance between word vectors. 3 Figure 1: Unsupervised sentence alignment System Description Our contribution in the STS shared task includes three different systems: supervised, unsupervised and supervised-unsupervised ensemble. Our models are mainly developed to measure semantic similarity between monoli"
S17-2025,S16-1081,0,0.151757,"salama,biemann}@informatik.uni-hamburg.de Abstract completely unrelated and 5 signifying semantic equivalence. Final performance is measured by computing the Pearson’s correlation (ρ) between machine-assigned semantic similarity scores and gold standard scores provided by human annotators. Since last year, the STS task have been extended to involve additional subtasks for crosslingual STS. Similar to the monolingual STS task, the cross-lingual task requires the semantic similarity measurement for two snippets of text that are written in different languages. In contrast to last year’s edition (Agirre et al., 2016), the task is organized into 6 sub-tracks and a primary track, which is the average of all of the secondary sub-tracks results. Secondary sub-tracks involve scoring similarity for monolingual sentence pairs in one language (Arabic, English, Spanish), and cross-lingual sentence pairs from the combination of two different languages (Arabic-English, Spanish-English, Turkish-English). Our paper proposes both supervised and unsupervised systems to automatically scoring semantic similarity between monolingual and cross-lingual short sentences. The two systems are then combined with an average ensemb"
S17-2025,S15-2027,0,0.0286733,"tch with score 1. As a last step of the alignment process, we handle the words that have not been matched in the preceding steps. The solution uses Glove word embeddings (Pennington et al., 2014) to calculate the matching score. Glove (840B tokens, 2.2M vocab) represent the word embeddings in 300d vector. We calculate the cosine distance between the unmatched words and all the words in the other sentence. Using a greedy strategy, we pick up the best match of each word. The global similarity is calculated using a weighted matches scores as shown in equation (1). Unsupervised Model Inspired by (Sultan et al., 2015; Brychc´ın and Svoboda, 2016), our unsupervised solution calculates a similarity score based on the alignment of the input pair of sentences. As presented in Figure 1, given a pair of sentences S1, S2, the alignment task builds a set of matched pair of words match(wi , wj ) where wi is a word in sentence S1, and wj is a word in sentence S2. Each matched pair has a score on the scale [0, 1]. This matching score indicates the strength of the semantic similarity between the aligned pair of words, with 1 representing the highest similarity match. As shown in Figure 2, after preprocessing, the sys"
S17-2025,S12-1051,0,0.0617926,"formation retrieval, paraphrase identification and plagiarism detection (Agirre et al., 2016). In an attempt to support the research efforts in STS, the SemEval STS shared Task (Agirre et al., 2017) offers an opportunity for developing creative new sentence-level semantic similarity approaches and to evaluate them on benchmark datasets. Given a pair of sentences, the task is to provide a similarity score on a scale of 0..5 according to the extent to which the two sentences are considered semantically similar, with 0 indicating that the semantics of the sentences are 2 Related Work Since 2012 (Agirre et al., 2012), the STS shared task has been one of the official shared tasks in SemEval and has attracted many researchers from the computational linguistics community (Agirre et al., 2017). Most of the state-of-the-art approaches often focus on training regression models on traditional lexical surface overlap features. Recently, deep learning models have achieved very promising results in semantic textual sim∗ *These authors contributed equally to this work 175 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 175–179, c Vancouver, Canada, August 3 - 4, 2017. 201"
S17-2025,S16-1089,0,0.286624,"Missing"
S17-2025,R15-2003,1,0.843942,"ained on a recent Wikipedia dump. To guarantee topic distribution stability, we run LDA for 100 repeated inferences. Then for each token, we assign the most frequent topic ID (Riedl and Biemann, 2012). VII NE Similarity: We measure similarity based on the shared named entities between the pair of text. VIII Unsupervised Dependency Alignment score: Using a Glove word embedding, we include the score of the cosine similarity between the syntactic heads of the matched words aligned in the unsupervised model (Sec. 3.2), as presented in equation (2). P score = V Dependency-Graph Features: Following Kohail (2015), each sentence S is converted into a graph using dependency relations obtained from the parser. We define the dependency graph GS = {VS , ES }, where the graph vertices VS = {w1 , w2 , . . . , wn } represent the tokens in a sentence, and ES is a set of edges. Each edge eiy represents a directed dependency relation between wi and wy . We calculate TfIdf on three levels and weight our dependency graph using the following conditions: Word TfIdf: Considering only those words that satisfy the condition: TfIdf (wi ) > α1 Pair TfIdf: Word pair are filtered based on T f Idf (c w ) ∗ Cos sim(c wi , w"
S17-2025,D14-1162,0,0.0814511,"mber of training examples is 14 619. We use StanfordCoreNLP1 pipeline to tokenize, lemmatize, dependency parse, and annotate the dataset for lemmas, part-of-speech (POS) tags, and named entities (NE). Stopwords are removed for the purpose of topic modeling and TfIdf computation. 3.2 (lemmas), and words that share similar WordNet hierarchy (synonyms, hyponyms, and hypernyms). We consider these two types of aligning as exact match with score 1. As a last step of the alignment process, we handle the words that have not been matched in the preceding steps. The solution uses Glove word embeddings (Pennington et al., 2014) to calculate the matching score. Glove (840B tokens, 2.2M vocab) represent the word embeddings in 300d vector. We calculate the cosine distance between the unmatched words and all the words in the other sentence. Using a greedy strategy, we pick up the best match of each word. The global similarity is calculated using a weighted matches scores as shown in equation (1). Unsupervised Model Inspired by (Sultan et al., 2015; Brychc´ın and Svoboda, 2016), our unsupervised solution calculates a similarity score based on the alignment of the input pair of sentences. As presented in Figure 1, given a"
S17-2025,W12-0703,1,0.794722,"n and Riedl (2013). III POS Tags Longest Common Subsequence: We measure the length of the longest common subsequence of POS tags between sentence pairs. Additionally, we also average this length by dividing it by the total number of tokens in each sentence separately. IV Topic Similarity Feature: To model the topical similarity between two documents, we use Latent Dirichlet Allocation (LDA, (Blei et al., 2003))2 model trained on a recent Wikipedia dump. To guarantee topic distribution stability, we run LDA for 100 repeated inferences. Then for each token, we assign the most frequent topic ID (Riedl and Biemann, 2012). VII NE Similarity: We measure similarity based on the shared named entities between the pair of text. VIII Unsupervised Dependency Alignment score: Using a Glove word embedding, we include the score of the cosine similarity between the syntactic heads of the matched words aligned in the unsupervised model (Sec. 3.2), as presented in equation (2). P score = V Dependency-Graph Features: Following Kohail (2015), each sentence S is converted into a graph using dependency relations obtained from the parser. We define the dependency graph GS = {VS , ES }, where the graph vertices VS = {w1 , w2 , ."
S17-2153,baccianella-etal-2010-sentiwordnet,0,0.145786,"Missing"
S17-2153,P13-2005,0,0.0331544,"mic system of a country. Therefore, a reliable and prompt delivery of information plays an important role in the financial market. Up until the last decade printed/television news were the major source of stock marketrelated information. However, with the introduction of micro-blogging websites (e.g. Twitter etc.) the trend has been shifted. The rise of Twitter and StockTwits has given the people and organizations an opportunity to vent out their feelings and views. This information can be used by an individual or an organization to make an informed prediction related to any company or stock (Si et al., 2013). This opens a new avenue for sentiment analysis in the financial domain of microblogs and news. News headlines are a short piece of text describing the nature of an article. Due to space constraints, headlines normally follow a compact writing style, known as headlinese, which limits 894 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 894–898, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Track 1 Message: Score: Span: Cashtag: Track 2 Message: Score: Company: Microblogs Putting on a little $F short, prevail"
S17-2153,S17-2089,0,0.0852026,"Missing"
S17-2153,P97-1023,0,0.632546,"Missing"
S17-2153,J09-3003,0,0.44437,"Missing"
S17-2153,S13-2053,0,0.0274224,"lysis (Khanarian and Alwarez-Melis, 2012). Moreover, each tweet can have reference to multiple company names (or stock symbols) and the expressed sentiment can be different towards different companies. Hence, there is a need to perform fine-grained sentiment analysis wherein, generally, a context is used to decide the relevant portion of a tweet for a particular company. Another inherent challenge with the microblog and news data is the use of short languages, hashtag, emoticons and embedded URL. Special attention should be given to these as they can provide some important hidden information (Mohammad et al., 2013). Example - #bullishMarket and #increasingProfit can reflect positive sentiment. These are some of the major challenges associated with fine-grained sentiment analysis of microblogging and news data. The SemEval-2017 task 5 (Fine-Grained Sentiment Analysis on Financial Microblogs and News) has two tracks (Cortis et al., 2017). For both the tracks, the overall aim was to assign a sentiment score to a cashtag/company over a continuous range of -1 (very negative/bearish) to 1 (very positive/bullish). First track involves finding a sentiment score towards a given ‘cashtag’ (stock symbol preceded b"
S18-1163,N15-1098,1,0.827444,"• total score: sum of the scores of w1 −f eat if prediction by score is 1, of w2 − f eat otherwise; 3.1 Experiments Choosing the Training Set During the practice phase, we noticed that the training set and the validation set show very different distributions. Running 5-fold cross validation experiments on either dataset, we obtained very high scores (sometimes close to 0.95). However, such scores did not generalize to the other dataset, where they dropped to about 0.60. This was only partially due to lexical memorization (some lexemes were present in multiple triples of the same dataset, cf. Levy et al. (2015); Santus et al. (2016b)). In fact, investigating the frequency of the words in the triples, we found that, on average, in our index, the first and the second words, w1 and w2, were about four times more frequent in the validation than in the training set (respectively 4.7M and 5.4M versus 0.9M and 1M ); similarly, the third word (i.e. f eat) was almost twice more frequent in the validation than in the training set (i.e. 3.9M versus 2.9M ). When the test set was made available, we could verify that its frequency distribution resembled the one in the validation set, with the first and second wor"
S18-1163,P14-1023,0,0.508854,"ana over watermelon). The system relies on an XGB classifier trained on carefully engineered graph-, pattern- and word embeddingbased features. It participated in the SemEval2018 Task 10 on Capturing Discriminative Attributes, achieving an F1 score of 0.73 and ranking 2nd out of 26 participant systems. 1 Introduction The recent introduction of popular software packages for training neural word embeddings (Mikolov et al., 2013a,b; Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead"
S18-1163,W16-2502,0,0.061441,"Mikolov et al., 2013a,b; Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The"
S18-1163,L18-1286,1,0.827694,"Missing"
S18-1163,D14-1162,0,0.0818355,"ple times in different POSdependency combinations. However, we found that f eat rarely appears in the top N features to the elements of the triples as w1, w2 and f eat. A training and validation set have been provided for system development (figures in Table 2). 2.2 Methodology Embeddings and Graphs For the Discriminative Attributes task, we combined word embeddings, patterns and information extracted from a graph-based distributional model. Concerning the word embeddings, we used the vectors produced by two popular frameworks for word embeddings: Word2Vec (Mikolov et al., 2013a,b) and GloVe (Pennington et al., 2014).2 The Word2Vec Skip-Gram architecture is a singlelayer neural network, based on the dot-product between word vectors, in which the vector representation is optimized to predict the context of a target word given the word itself. The context generally consists of a word window of a fixed width around the target. The other framework, GloVe, is similar to traditional count models based on matrix factorization (Turney and Pantel, 2010; Baroni et al., 2014), in the sense that vectors are trained on global word-word co-occurrence counts. In the case of GloVe, the training objective is to learn word"
S18-1163,Y16-2021,1,0.930479,"4) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing discriminative attributes"
S18-1163,L16-1722,1,0.933311,"4) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing discriminative attributes"
S18-1163,W16-2506,0,0.0527279,"Levy and Goldberg, 2014) has led to an increase of the number of studies dedicated to lexical similarity and to remarkable performance improvements on related tasks (Baroni et al., 2014). However, the validity of similarity estimation as the only benchmark for semantic representations has been questioned, for several reasons. One for all, most evaluation datasets provide human-elicited similarity scores, with the consequences that the ratings are subjective and the performance of some automated systems is already above the upper bound of the inter-annotator agreement (Batchkarov et al., 2016; Faruqui et al., 2016; Santus et al., 2016a). Originally proposed as an alternative benchmark for Distributional Semantic Models (DSMs), the Discriminative Attributes task focuses instead on the extraction of semantic differences between lexical meanings (Krebs and Paperno, 2016): given two words and an attribute (i.e., a discrete semantic feature), a system has to predict whether the attribute describes a difference between the corresponding concepts or not (e.g. wing is an attribute of plane, but not of helicopter). 2 2.1 Capturing Discriminative Attributes Task and Dataset Description The task of capturing disc"
S18-1163,W16-2509,0,0.616184,"rsoni@gmail.com 1 Massachussetts Institute of Technology, 2 Universit¨at Hamburg, 3 Aix-Marseille University Abstract Since even related words may differ for some non-shared attributes (e.g. hypernyms and hyponyms), the ability of automatically recognize discriminative features would be an extremely useful addition for the creation of ontologies and other types of lexical resources and would make machine decisions interpretable, enabling human validation (Biemann et al., 2018). Moreover, one can think to applications to many other NLP domains, such as machine translation and dialogue systems (Krebs and Paperno, 2016). In the present contribution, we describe the BomJi classification system, which we used for the identification of discriminative features between concept pairs. According to the official evaluation results provided by the organizers1 , our system ranked second out of 26 participants. Our score, F 1 = 0.73 lags slightly behind the best score of 0.75. After the evaluation period, we run further experiments including all investigated features and found that the system can achieve up to 0.75 F1 score. This paper describes BomJi, a supervised system for capturing discriminative attributes in word"
S19-1014,J15-4004,0,0.0726856,"anguage data: the overwhelming part of these unique pairs are synsets not related to each other at all. For most tasks, it is useless to ‘know’ that, e.g., ‘ambulance’ and ‘general’ are less similar than ‘ambulance’ and ‘president’. While the distances between these node pairs are indeed different on the WordNet graph, we find it much more important for the model to be able to robustly tell really similar pairs from the unrelated ones so that they could benefit applications. As a more balanced and relevant test set, we use noun pairs (666 total) from the SimLex999 semantic similarity dataset (Hill et al., 2015). SimLex999 contains lemmas; as some lemmas may map to several WordNet synsets, for each word pair we choose the synset pair maximizing the WordNet similarity, following (Resnik, 1999). Then, we measure the Spearman rank correlation between these ‘gold’ scores and the similarities produced by the graph embedding models trained on the WordNet. Further on, we call this evaluation score the ‘correlation with WordNet similarities’. This evaluation method directly measures how well the model fits the training objective4 . We also would like to check whether our models generalize to extrinsic tasks."
S19-1014,O97-1002,0,0.941242,"consist of pairs of noun synsets and their ‘ground truth’ similarity values. There exist several methods to calculate synset similarities on the WordNet (Budanitsky and Hirst, 2006). We compile four datasets, with different similarity functions: LeacockChodorow similarities (LCH); Jiang-Conrath similarities calculated over the SemCor corpus ( JCNS); Wu-Palmer similarities (WuP); and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilistic data about the distributions of synsets (‘information content’). We employed the SemCor subset of the Brown corpus, manually annotated with word senses (Kucera and Francis, 1982). WuP similarities (Wu and Palmer, 1994) are based on the depth of the two nodes in the taxonomy and the depth of their most specific ancesRelation to Similar Models Our model is similar to the Skip-gram model (Mikolov et al., 2013), where pairs of words (vi , v"
S19-1014,J06-1003,0,0.269283,"ng Pairwise Similarities Selection of the Similarity Measures Our aim is to produce node embeddings that capture given similarities between nodes in a graph. In our case, the graph is WordNet, and the nodes are its 82,115 noun synsets. We focused on nouns since in WordNet and SimLex999 they are represented better than other parts of speech. Embeddings for synsets of different part of speech can be generated analogously. The training datasets consist of pairs of noun synsets and their ‘ground truth’ similarity values. There exist several methods to calculate synset similarities on the WordNet (Budanitsky and Hirst, 2006). We compile four datasets, with different similarity functions: LeacockChodorow similarities (LCH); Jiang-Conrath similarities calculated over the SemCor corpus ( JCNS); Wu-Palmer similarities (WuP); and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilist"
S19-1014,W04-0807,0,0.382845,"al., 2014) FSE (Subercaze et al., 2015) 0.386 0.462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Note that both the original graph-based"
S19-1014,Q17-1022,0,0.0279129,"Missing"
S19-1014,N09-2060,0,0.0444916,"Missing"
S19-1014,P15-1173,0,0.0428543,"Missing"
S19-1014,S01-1005,0,0.0859796,"kovec, 2016) Deepwalk (Perozzi et al., 2014) FSE (Subercaze et al., 2015) 0.386 0.462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Not"
S19-1014,D14-1162,0,0.115764,"vk and vl are randomly sampled nodes from V . Embeddings are initialized randomly and trained with the Adam optimizer (Kingma and Ba, 2014) with early stopping.2 Once the model is trained, the computation of node similarities is approximated with the dot product of the learned node vectors, making the computations efficient: sˆij = vi · vj . embedding matrix, with the number of rows equal to the number of nodes and column width set to the desired embedding dimensionality. Finally, unlike the Skip-gram, we do not use any non-linearities. Another closely related model is Global Vectors (GloVe) (Pennington et al., 2014), which approximates the co-occurrence probabilities in a given corpus. The objective function to be minimized in GloVe model is L = P ˜j − log sij + bi + bj )2 , (vi ,vj )∈B f (sij )(vi · v where sij counts the number of co-occurrence of words vi and vj , bi and bj are additional biases for each word, and f (sij ) is a weighting function to give appropriate weight for rare co-occurrences. Like the Skip-gram, GloVe also uses two embedding matrices, but it relies on global information. 4 4.1 Computing Pairwise Similarities Selection of the Similarity Measures Our aim is to produce node embeddin"
S19-1014,P15-2002,0,0.381203,"tor space with a much lower dimensionality than the number of nodes. The method described in this paper falls into the category of ‘shallow embeddings’, meaning that we do not attempt to embed entire communities or neighborhoods: our aim is to approximate distances or similarities between (single) nodes. Existing approaches to solving this task mostly use either factorization of the graph adjacency matrix (Cao et al., 2015; Ou et al., 2016) or random walks over the graph as in Deepwalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016). A completely different approach is taken by Subercaze et al. (2015), who directly embed the WordNet tree graph into Hamming hypercube binary representations. Their model is dubbed ‘Fast similarity embedding’ (FSE) and also optimizes one of our objectives, i.e. to provide a much quicker way of calculating semantic similarities based on WordNet knowledge. However, the FSE embeddings are not differentiable, limiting their use in many deep neural architectures, especially if fine-tuning is needed. TransE (Bordes et al., 2013) interprets entities as vectors in the low-dimensional embeddings space and relations as a translation operation between two entity vectors."
S19-1014,D16-1174,0,0.0379846,"Missing"
S19-1014,P94-1019,0,0.199846,"and Shortest path similarities (ShP). LCH similarity (Leacock and Chodorow, 1998) is based on the shortest path between two synsets in the WordNet hypernym/hyponym taxonomy and its maximum depth, while JCN similarity (Jiang and Conrath, 1997) uses the lowest common parent of two synsets in the same taxonomy. JCN is significantly faster but additionally requires a corpus as a source of probabilistic data about the distributions of synsets (‘information content’). We employed the SemCor subset of the Brown corpus, manually annotated with word senses (Kucera and Francis, 1982). WuP similarities (Wu and Palmer, 1994) are based on the depth of the two nodes in the taxonomy and the depth of their most specific ancesRelation to Similar Models Our model is similar to the Skip-gram model (Mikolov et al., 2013), where pairs of words (vi , vj ) from a training corpus are optimized to have their corresponding vectors dot product vi · v ˜j close to 1, while randomly generated pairs (‘negative samples’) are optimized to have their dot product close to 0. In the Skipgram model, the target is to minimize the log likelihood of the conditional probabilities of context words vj given current words vi , which is in the c"
S19-1014,E17-1010,0,0.0261646,"462 0.533 0.556 Raw WordNet JCN-S Raw WordNet LCH Raw WordNet ShP Raw WordNet WuP 0.487 0.513 0.513 0.474 path2vec JCN-S path2vec LCH path2vec ShP path2vec WuP 0.533 0.532 0.555 0.555 Table 3: Spearman correlations with human SimLex999 noun similarities (model synset selection). Discussion of Results Table 4 presents the WSD micro-F1 scores using raw WordNet similarities, 300D path2vec, Deepwalk and node2vec models, and the 128D FSE model. We evaluate on the following all-words English WSD test sets: Senseval-2 (Palmer et al., 2001), Senseval-3 (Mihalcea et al., 2004), and SemEval-15 Task 13 (Raganato et al., 2017). Raw WordNet similarities are still the best, but the path2vec models are consistently the second after them (and orders of magnitude faster), outperforming other graph embedding baselines. The largest drop between the original and vector-based measures is for JCN-S, which is also the only one which relies not only on graph but also on external information from a corpus, making it more difficult to approximate (see also Figure 2, where this measure distribution seems to be the most difficult to reproduce). Note that both the original graph-based measures and graph embeddings do not outperform"
S19-1014,W08-2006,0,0.0165187,"trained on the WordNet graph alone. ing it encodes paths (or other similarities) between graph nodes into dense vectors. Our first contribution is an effective and efficient approach to learn graph embeddings based on a user-defined custom similarity measure sim on a set of nodes V , e.g. the shortest path distance. The second contribution is an application of state-of-the-art graph embeddings to word sense disambiguation task. 2 Related Work Various methods have been employed in NLP to derive lexical similarity directly from geometrical properties of the WordNet graph, from random walks in (Rao et al., 2008) to kernels in ´ S´eaghdha, 2009). More recently, representa(O tion learning on graphs (Bordes et al., 2011) received much attention in various research communities; see (Hamilton et al., 2017a) for a thorough survey on the existing methods. All of them (including ours) are based on the idea of projecting graph nodes into a latent vector space with a much lower dimensionality than the number of nodes. The method described in this paper falls into the category of ‘shallow embeddings’, meaning that we do not attempt to embed entire communities or neighborhoods: our aim is to approximate distance"
S19-2018,P98-1013,0,0.915854,"mEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov"
S19-2018,J05-1004,0,0.383238,"he runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats li"
S19-2018,Q17-1010,0,0.0444771,"e frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fa"
S19-2018,W11-0110,0,0.0286894,"ns for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dataset contained phrasal verbs, such as fall back and buy out, we have considered only the first word in the phrase. If this word is not present in the model v"
S19-2018,D17-1070,0,0.0377806,"each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model is trained on individual words only and the SemEval dat"
S19-2018,N18-1202,0,0.0333785,"which is usually the predicate. The goal is to label each highlighted verb according to the frame evoked by the sentence. The gold standard for this subtask is based on the FrameNet (Baker et al., 1998) definitions for frames. 2.1 Method on TensorFlow Hub.3 Among all the layers of this model, we used the mean-pooling layer for word and context embeddings. Method Since sentences evoking the same frame should receive the same labels, we used a verb clustering approach and experimented with a number of pre-trained word and sentence embeddings models, namely Word2Vec (Mikolov et al., 2013), ELMo (Peters et al., 2018), Universal Sentence Embeddings (Conneau et al., 2017), and fastText (Bojanowski et al., 2017). This setup is similar to treating the frame induction task as a word sense disambiguation task (Brown et al., 2011). We experimented with embedding different lexical units, such as verb (V), its sentence (context, C ), subject-verb-object (SVO) triples, and verb arguments. Combination of context and word representations (C+W) from Word2Vec and ELMo turned out to be the best combination in our case. We used the standard Google News Word2Vec embedding model by Mikolov et al. (2013). Since this model i"
S19-2018,S18-2016,0,0.0742481,"ures. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verb"
S19-2018,S19-2003,0,0.200367,"e induction is similar to the word sense induction approach by Arefyev et al. (2018), which uses tf–idf-weighted context word embeddings for a shared task on word sense induction by Panchenko et al. (2018). In this unsupervised task, our approach for clustering mainly consists of exploring the effectiveness of already available pre-trained models.1 Main contributions of this paper are: We present our system for semantic frame induction that showed the best performance in Subtask B.1 and finished as the runner-up in Subtask A of the SemEval 2019 Task 2 on unsupervised semantic frame induction (QasemiZadeh et al., 2019). Our approach separates this task into two independent steps: verb clustering using word and their context embeddings and role labeling by combining these embeddings with syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to t"
S19-2018,E12-1003,0,0.059716,"Missing"
S19-2018,J93-2004,0,0.0648084,"Missing"
S19-2018,P18-2010,1,0.635096,"on of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embeddings) for grouping verbs to frame type cluster"
S19-2018,W12-1901,0,0.0329549,"th syntactical features. A simple combination of these steps shows very competitive results and can be extended to process other datasets and languages. 1 Introduction Recent years have seen a lot of interest in computational models of frame semantics, with the availability of annotated sources like PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998). Unfortunately, such annotated resources are very scarce due to their language and domain specificity. Consequently, there has been work that investigated methods for unsupervised frame acquisition and parsing (Lang and Lapata, 2010; Modi et al., 2012; Kallmeyer et al., 2018; Ustalov et al., 2018). Researchers have used different approaches to induce frames, including clustering verb-specific arguments as per their roles (Lang and Lapata, 2010), subject-verb-object triples (Ustalov et al., 2018), syntactic dependency representation using dependency formats like CoNLL (Modi et al., 2012; Titov and Klementiev, 2012), and latent-variable PCFG models (Kallmeyer et al., 2018). The SemEval 2019 task of semantic frame and role induction consists of three subtasks: (A) 1. a method that uses contextualized distributional word representations (embed"
S19-2137,Q17-1010,0,0.0531344,"Missing"
S19-2137,D14-1179,0,0.0114841,"Missing"
S19-2137,S19-2010,0,0.0274778,"lt. While the performance of the supervised trans3 Also, the official test set was not released yet, so we cannot report a systematic comparison at this point. 785 Confusion Matrix Confusion Matrix Confusion Matrix 0.7 0.8 TIN 0.6 154 0.2 7 0.6 0.4 UNT 19 8 Figure 2: Sub-task A, supervised UNT Predicted label Figure 3: Sub-task B, unsupervised 15 77 8 0.4 0.3 0.2 OTH 19 9 7 0.1 0.0 Predicted label Figure 4: Sub-task C, unsupervised • For unsupervised pre-training, the benefit of transfer learning compared to the baseline without transfer is larger the smaller the target training dataset gets. Zampieri et al. (2019b) for a detailed comparison of all submissions). Fig. 2 to 4 show confusion matrices for the three best runs. The ratio between false positives and false negatives for subtask A is fairly balanced. False positives mainly comprise hard cases where, for instance, swear words are used in a non-offensive manner. In the highly unbalanced annotations for sub-task B, more tweets were wrongly predicted as targeted insults than true yet unpredicted targeted insults. Here we observe many cases which contain offensive language and some mentioning of individuals or groups but both are not directly linked"
S19-2137,W18-4401,0,0.100224,"Missing"
S19-2137,malmasi-zampieri-2017-detecting,0,0.0352016,"present the results in Section 4. 2 Input (sub word embeddings) 1 2 Bi-GRU (100 units) CNN CNN CNN (200 units, kernel=3) (200 units, kernel=4) (200 units, kernel=5) Global MaxPooling Global MaxPooling Global MaxPooling Related Work Two recent survey papers, Schmidt and Wiegand (2017) and Fortuna and Nunes (2018), summarize the current state of the art in offensive language detection and related tasks such as hate speech or abusive language detection. Specifically for offensive language detection, the paper by Davidson et al. (2017) introduced a publicly available dataset which was reused in (Malmasi and Zampieri, 2017, 2018; ElSherief et al., 2018; Zhang et al., 2018) as well as in our approach of supervised pre-training. A predecessor of our transfer learning approach has already successfully been applied at GermEval 2018 (Wiegand et al., 2018), a shared task on offensive language detection in German language tweets. In our paper (Wiedemann et al., 2018), we tested different types of knowledge transfer and transfer learning strategies. We further found that latent semantic clusters of user handles in tweets (e.g. user accounts run by media companies or politicians) are a very helpful feature to predict of"
S19-2137,W17-0802,0,0.0518616,"Missing"
S19-2137,W17-1101,0,0.017762,"itask learning of parallel training of five different NLP tasks with some overlap to offensive language detection. Unsupervised transfer learning is performed with a thematic clustering of a large dataset of unlabeled tweets via LDA. After shortly referencing related work (Section 2), we introduce both approaches in detail in Section 3 and present the results in Section 4. 2 Input (sub word embeddings) 1 2 Bi-GRU (100 units) CNN CNN CNN (200 units, kernel=3) (200 units, kernel=4) (200 units, kernel=5) Global MaxPooling Global MaxPooling Global MaxPooling Related Work Two recent survey papers, Schmidt and Wiegand (2017) and Fortuna and Nunes (2018), summarize the current state of the art in offensive language detection and related tasks such as hate speech or abusive language detection. Specifically for offensive language detection, the paper by Davidson et al. (2017) introduced a publicly available dataset which was reused in (Malmasi and Zampieri, 2017, 2018; ElSherief et al., 2018; Zhang et al., 2018) as well as in our approach of supervised pre-training. A predecessor of our transfer learning approach has already successfully been applied at GermEval 2018 (Wiegand et al., 2018), a shared task on offensiv"
S19-2137,N19-1144,0,0.0417343,"lt. While the performance of the supervised trans3 Also, the official test set was not released yet, so we cannot report a systematic comparison at this point. 785 Confusion Matrix Confusion Matrix Confusion Matrix 0.7 0.8 TIN 0.6 154 0.2 7 0.6 0.4 UNT 19 8 Figure 2: Sub-task A, supervised UNT Predicted label Figure 3: Sub-task B, unsupervised 15 77 8 0.4 0.3 0.2 OTH 19 9 7 0.1 0.0 Predicted label Figure 4: Sub-task C, unsupervised • For unsupervised pre-training, the benefit of transfer learning compared to the baseline without transfer is larger the smaller the target training dataset gets. Zampieri et al. (2019b) for a detailed comparison of all submissions). Fig. 2 to 4 show confusion matrices for the three best runs. The ratio between false positives and false negatives for subtask A is fairly balanced. False positives mainly comprise hard cases where, for instance, swear words are used in a non-offensive manner. In the highly unbalanced annotations for sub-task B, more tweets were wrongly predicted as targeted insults than true yet unpredicted targeted insults. Here we observe many cases which contain offensive language and some mentioning of individuals or groups but both are not directly linked"
W05-1704,1996.amta-1.13,0,0.31023,"ces to a cross-lingual setting. Dictionaries are evaluated against gold standards and manually; the analysis accounts for frequency and corpus size effects. 1 Introduction While more and more parallel text resources become available due to common regulations amongst different countries and the internationalization of markets, there is a need for translational lexicons that can keep pace with the daily creation of new terminology. The standard procedure amongst the Machine Translation Community to extract translation pairs from sentence-aligned parallel corpora can be outlined as follows (cf. (Melamed, 1996), (Moore, 2001) for an overview): 1. Define a measure which yields association scores between words of different languages 2. Calculate the association scores between all possible pairs 3. To obtain translations for a given word, sort its trans-lingual associations by score and take the top N or apply a threshold. The crucial choice in the first step is to define a well-suited measure, the problem in step two is the effectiveness of the calculation, and in the last step the challenge is to cut the list at the right point. In (Melamed, 1996), the problem of selecting inappropriate translations"
W05-1704,W01-1411,0,0.0153576,"ngual setting. Dictionaries are evaluated against gold standards and manually; the analysis accounts for frequency and corpus size effects. 1 Introduction While more and more parallel text resources become available due to common regulations amongst different countries and the internationalization of markets, there is a need for translational lexicons that can keep pace with the daily creation of new terminology. The standard procedure amongst the Machine Translation Community to extract translation pairs from sentence-aligned parallel corpora can be outlined as follows (cf. (Melamed, 1996), (Moore, 2001) for an overview): 1. Define a measure which yields association scores between words of different languages 2. Calculate the association scores between all possible pairs 3. To obtain translations for a given word, sort its trans-lingual associations by score and take the top N or apply a threshold. The crucial choice in the first step is to define a well-suited measure, the problem in step two is the effectiveness of the calculation, and in the last step the challenge is to cut the list at the right point. In (Melamed, 1996), the problem of selecting inappropriate translations due to indirect"
W05-1704,J03-3002,0,0.101526,"Missing"
W05-1704,sahlgren-2004-automatic,0,0.0253427,"our approach works in a language-independent fashion. Moreover, results for the first candidate are much higher than for the second, indicating that the association measure is appropriate. For the remainder, we use German-English to determine the influence of word frequency and corpus size. sim1=1 Precision For manual evaluation, we presented about 200 randomly selected words for each pair and let our language experts chose amongst correct, partially (part of a correct multiword, e.g. &quot;Richter&quot; instead of &quot;Bundesrichter&quot;) and wrong. Table 2 depicts the results of manual evaluation. sim1>0.6 [Sahlgren 2004] 1 0,9 0,8 0,7 0,6 0,5 0,4 0,3 0,2 0,1 0 1 10 100 1000 10000 100000 1000000 Frequency Figure 1: Dictionary precision for different frequency bands for English-German in comparison to (Sahlgren, 2004). Our approach obtains a higher precision for Biemann & Quasthoff: Dictionary acquisition using parallel text and co-occurrence statistics 25 Proceedings of the 15th NODALIDA conference, Joensuu 2005 3.4 Influence of corpus size Another question we try to answer is the influence of corpus size. Of course, more parallel data means more evidence, so better results can be expected. But how little is"
W05-1704,C02-1002,0,0.0288801,"segmentation step is assumed in the preprocessing). We argue that a language independent approach like ours is a good baseline where more problem-specific methods (like using edit distances for cognates) can build upon. Usually, co-occurrence statistics is used for large monolingual texts. It returns pairs of words that significantly often occur together within a predefined window. Implementations differ in the significance measure and the window size used. For our experiments, we use a log-likelihood-measure (which has been noticed to be adequate for the translation relation, see e.g (Tufi¸ s, 2002)) and sentences as windows. This approach has proved to return pairs of seS. Werner (ed.), Proceedings of the 15th NODALIDA conference, Joensuu 2005, Ling@JoY 1, 2006, pp. 22–29 ISBN 952-458-771-8, ISSN 1796-1114. http://ling.joensuu.fi/lingjoy/ © by the authors Proceedings of the 15th NODALIDA conference, Joensuu 2005 mantically related words similar to human associations, cf. (Biemann et al., 2004). For dictionary acquisition, we slightly modify the setting: Starting from a parallel corpus, we build a corpus containing bilingual bi-sentences built from pairs of aligned sentences. For technic"
W05-1729,sjobergh-kann-2004-finding,0,0.0307404,"chine learning approach: We would like to train a classifier using a set of training examples. In application, this classifier uses regularities acquired in the training phase to split compounds that have not been necessarily contained in the training set. Generally, there are two ways to design a generic compound splitter: one is based on training on all possible breakpoints and using letter n-grams to both sides as features, e.g. used by (Yoon, 2000). Another way is to memorize possible prefixes and suffixes of compounds and match them during classification, a methodology conducted by e.g. (Sjöbergh and Kann, 2004). While n-gram splitters are capable of reaching comparatively high accuracy scores with small training sets, affix splitters need more training data but handle exceptions more naturally. Here, we present an affix compound splitter that uses Compact Patricia Tries (CPT) as a data structure, which can be extended to function as a classifier on affixes of words. 4.1 Classification with Compact Patricia Tries A trie is a tree data structure for storing strings, in which there is one node for every common prefix. The number of possible children is limited by the number of characters in the strings"
W05-1729,A00-1027,0,0.0235045,"ta structure possesses some very useful properties: 4 Compound splitting For setting up a compound splitting component, it is clearly desirable to use a machine learning approach: We would like to train a classifier using a set of training examples. In application, this classifier uses regularities acquired in the training phase to split compounds that have not been necessarily contained in the training set. Generally, there are two ways to design a generic compound splitter: one is based on training on all possible breakpoints and using letter n-grams to both sides as features, e.g. used by (Yoon, 2000). Another way is to memorize possible prefixes and suffixes of compounds and match them during classification, a methodology conducted by e.g. (Sjöbergh and Kann, 2004). While n-gram splitters are capable of reaching comparatively high accuracy scores with small training sets, affix splitters need more training data but handle exceptions more naturally. Here, we present an affix compound splitter that uses Compact Patricia Tries (CPT) as a data structure, which can be extended to function as a classifier on affixes of words. 4.1 Classification with Compact Patricia Tries A trie is a tree data"
W06-3812,J93-1003,0,0.124595,"30,000 nodes. It is not surprising that separating the two parts is more difficult for higher r. Results are not very 77 Speed issues NLP Experiments In this section, some experiments with graphs originating from natural language data are presented. First, we define the notion of cooccurrence graphs, which are used in sections 4.1 and 4.3: Two words co-occur if they can both be found in a certain unit of text, here a sentence. Employing a significance measure, we determine whether their co-occurrences are significant or random. In this case, we use the log-likelihood measure as described in (Dunning 1993). We use the words as nodes in the graph. The weight of an 1 defined for two random variables X and Y as (H(X)+H(Y)H(X,Y))/max(H(X),H(Y)) with H(X) entropy. A value of 0 denotes indepenence, 1 is perfect congruence. edge between two words is set to the significance value of their co-occurrence, if it exceeds a certain threshold. In the experiments, we used significances from 15 on. The entirety of words that are involved in at least one edge together with these edges is called co-occurrence graph (cf. Biemann et al. 2004). In general, CW produces a large number of clusters on real-world graphs"
W06-3812,E03-1020,0,0.0221068,"Missing"
W06-3812,W96-0103,0,0.0725516,"Missing"
W06-3812,E06-1018,0,\N,Missing
W06-3812,E95-1020,0,\N,Missing
W07-2425,P98-1013,0,0.0260972,"language texts. While syntactic parsers have been intensively studied for decades, broad coverage semantic parsing is a relatively recent research topic. Semantic parsing aims at constructing a semantic representation of a sentence, abstracting from the syntactic form and allowing queries for meaning rather than syntax. That is, semantic relations between concepts are in the focus of interest rather than syntactic relations between words and phrases. A major current line of research for extracting semantic structures from texts is concerned with semantic role labeling. The FrameNet database (Baker et al., 1998) provides an inventory of semantic frames together with a list of lexical units associated with these frames. Semantic parsing then means to choose appropriate semantic frames from the frame inventory depending on the lexical concepts present in the given sentence and to assign frame-specific roles to concepts. A related task has been defined as part of CoNLL 2004 (Carreras and M`arques, 2004). Here, machine learning methods are used to learn a semantic role labeler from an annotated text to extract a fixed set of semantic relations. If one aims at deep semantic parsing, a lexicon containing s"
W07-2425,W04-2412,0,0.0768556,"Missing"
W07-2425,P06-1045,0,0.0130491,"eira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a discussion on context combination for synonym acquisition. 2 Semantic Lexicon and Parser This section gives a brief outline of the semantic parsing framework with respect to which our learning task is set up. The task is to automatically extend a semantic lexicon used for semantic parsing by exploiting parses that have been generated on the basis of the already existing lexicon. From these parses we extract three types of syntactic-semantic noun contexts which are then employed to classify unknown nouns on the basis of classified nouns, as explained in more detail in Section 3. 2.1 The"
W07-2425,P98-2127,0,0.0407064,"IDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a"
W07-2425,P93-1024,0,0.171559,"Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al.,"
W07-2425,W97-0313,0,0.0424834,"or semi-automatically extending semantic lexicons are highly needed to overcome the bottleneck and to scale the lexicon to a size where satisfactory coverage can be reached. In this paper, we present a method that enlarges the number of noun entries in the lexicon of a semantic parser for German. 1.1 Related Work Extending a given lexicon with the help of a parser relying on this lexicon can be viewed as a step of a bootstrapping cycle: Lexicon entries of known words are used to obtain entries for previously unknown words by exploiting a parsed corpus. Early bootstrapping approaches such as (Riloff and Shepherd, 1997) were based on few seed words of a semantic category and their nearest neighbor contexts. Higher precision was achieved by separating extraction patterns into two groups by (Roark Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed word"
W07-2425,P98-2182,0,0.0718828,"Missing"
W07-2425,P99-1014,0,0.0218252,"nference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differs from these approaches in that we use a conceptual framework that covers all sorts of nouns rather than concentrating on a small set of domain-specific classes or leaving the definition of the classes to the method. Moreover, we combine three different context types; see (Hagiwara et al., 2006) for a discussion on contex"
W07-2425,W02-1028,0,0.0285347,"y bootstrapping approaches such as (Riloff and Shepherd, 1997) were based on few seed words of a semantic category and their nearest neighbor contexts. Higher precision was achieved by separating extraction patterns into two groups by (Roark Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek and Mare Koit (Eds.) NODALIDA 2007 Conference Proceedings, pp. 175–182 Richard Socher, Chris Biemann and Rainer Osswald and Charniak, 1998): Conjunctions, lists and appositives being one and noun compounds being the other. Since bootstrapping single categories often leads to category shifts in later steps, (Thelen and Riloff, 2002) use an un-annotated corpus, seed words and a large body of extraction patterns to discover multiple semantic categories like event or human simultaneously. Another related research line is distributional clustering to obtain semantic classes via similar contexts, e.g. (Pereira et al., 1993; Lin, 1998; Rooth et al., 1999). Here, semantic classes are created by the clustering method rather than assigned to predefined classes in the lexicon; these works also employ the distributional hypothesis, i.e. that similar semantic properties are reflected in similar (syntactic) contexts. Our setup differ"
W07-2425,C98-1013,0,\N,Missing
W07-2425,C98-2122,0,\N,Missing
W07-2445,J93-1003,0,0.0648689,"documents but only well-formed sentences extracted from these (see Section 3). Sentences are indexed by words, so it is possible to quickly access all sentences a word form occurs in. For each word form, we automatically extract the following information: frequency (which indicates common vs. uncommon words) and significant co-occurrences based on neighbouring words (mostly containing typical properties of the corresponding concept or idioms the word is a part of) and based on sentences (mostly containing semantically related word forms). As significance measure, the loglikelihood ratio (cf. Dunning, 1993) is used. Our storing scheme is open for additional, manually provided information, such as grammatical data, lemmatization, thesaurus entries and subject area assignments. Íslenskur Orðasjóður contains mappings from word forms to lemmas (source: Bjarnadóttir (2004)), see Appendix. 5 Example Data Frequency statistics can be used for measuring visibility of a concept and commonness of a word. In lexicography this can be an important criterion to explain judgments. For the language researcher this data can tell whether and to what extent a word was in use in the time interval covered by the corp"
W07-2445,quasthoff-etal-2006-corpus,1,0.772064,"We introduce an Icelandic corpus of more than 250 million running words and describe the methodology to build it. The resource is available for use free of charge. We provide automatically generated monolingual lexicon entries, comprising frequency statistics, samples of usage, cooccurring words and a graphical representation of the word’s semantic neighbourhood. 1 Introduction Corpora are important language resources for a variety of Natural Language Processing tasks, especially in semi-supervised settings, where corpora are used to build e.g. language models. In (Biemann et al., 2004) and (Quasthoff et al., 2006) design and implementation of an architecture capable of building numerous of large corpora for different languages with little manual effort have been introduced. This infrastructure has been used to produce an Icelandic corpus based on web pages. In this paper we present the steps undertaken to digest unstructured clutter of HTML pages into a ready for use linguistic resource that provides rapid access through search indices on different language units. ancient Icelandic language (cf. Kristmannsson, 2004). But, despite a politically initiated language technology campaign in order to strength"
W10-2309,W06-3814,0,0.188906,"Missing"
W10-2309,W06-3812,1,0.761652,"rget word, the overlap in words between the textual context and all clusters from the neighborhood graph is measured. The cluster ID of the cluster with the highest overlap is assigned as a feature. This can be viewed as a word sense induction system in its own right. At this, several clusterings from different parameterizations are used to form distinct features, which enables the machine learning algorithm to pick the most suitable cluster features per target word when building the classification model. 2.4 2.2 Feature Assignment in Context Graph Clustering Parameterization As described in (Biemann, 2006), the neighborhood graph is clustered with Chinese Whispers. This efficient graph clustering algorithm finds the numbers of clusters automatically and returns a partition of the nodes. It is initialized by assigning different classes to all nodes in the graph. Then, 1 full dataset available for download at http://aclweb.org/aclwiki/index.php?title=Image:TWSI397.zip 2 http://beam.to/biem/software/TinyCC2.html 56 Corpora for Cluster Features When incorporating features that are induced using large unlabeled corpora, it is important to ensure that the corpus for feature induction and the word sen"
W10-2309,N06-2015,0,0.0830726,"Missing"
W10-2309,S07-1016,0,0.134152,"0 Workshop on Graph-based Methods for Natural Language Processing, ACL 2010, pages 55–59, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics corpus alone for unsupervised topic signature acquisition using graph clustering, not relying on the existence of a WordNet. Unlike in previous evaluations like (Agirre et al., 2006), parameters for word sense induction are not optimized globally, but instead several parameter settings are offered as features to a Machine Learning setup. Experimental results are provided for two datasets: the Semeval-2007 lexical sample task (Pradhan et al., 2007) and the Turk bootstrap Word Sense Inventory (TWSI1 , (Biemann and Nygaard, 2010) ). 2 a number of local update steps are performed, in which a node inherits the predominant class in its neighborhood. At this, classes of adjacent nodes are weighted by edge weight and downweighted by the degree (number of adjacent nodes) of the neighboring node. This results in hard clusters of words per target, which represent different target usages. Downweighting nodes by degree is done according to the following intuition: nodes with high degrees are probably very universally used words and should be less i"
W10-2309,C02-1114,0,0.397112,"Missing"
W11-1304,W03-1809,0,0.123637,"s could play the role of a compositionality prior that could, e.g., be stored in a dictionary. There is a long-living tradition within the research 21 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 21–28, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics community working on multiword units (MWUs) to automatically classify MWUs into either compositional or non-compositional ones. However, it has been often noted that compositionality comes in degrees, and a binary classification is not valid enough in many cases (Bannard et al., 2003; Katz and Giesbrecht, 2006). To the best of our knowledge, this has been the first attempt to offer a dataset and a shared task that allows to explicitly evaluate the models of graded compositionality. 2 Shared Task Description For the shared task, we aimed to get compositionality scores for phrases frequently occurring in corpora. Since distributional models need large corpora to perform reliable statistics, and these statistics are more reliable for frequent items, we chose to restrict the candidate set to the most frequent phrases from the freely available WaCky1 web corpora (Baroni et al."
W11-1304,W06-1203,1,0.834188,"of a compositionality prior that could, e.g., be stored in a dictionary. There is a long-living tradition within the research 21 Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo’2011), pages 21–28, c Portland, Oregon, 24 June 2011. 2011 Association for Computational Linguistics community working on multiword units (MWUs) to automatically classify MWUs into either compositional or non-compositional ones. However, it has been often noted that compositionality comes in degrees, and a binary classification is not valid enough in many cases (Bannard et al., 2003; Katz and Giesbrecht, 2006). To the best of our knowledge, this has been the first attempt to offer a dataset and a shared task that allows to explicitly evaluate the models of graded compositionality. 2 Shared Task Description For the shared task, we aimed to get compositionality scores for phrases frequently occurring in corpora. Since distributional models need large corpora to perform reliable statistics, and these statistics are more reliable for frequent items, we chose to restrict the candidate set to the most frequent phrases from the freely available WaCky1 web corpora (Baroni et al., 2009). Those are currently"
W12-0703,A00-2004,0,0.0934689,"higher than cp . When using topics instead of words, it can be expected that sentences within one segment have many topics in common, which leads to cosine similarities close to 1. Further, using topic IDs instead of words greatly increases sparsity. A minimum in the curve indicates a change in topic distribution. Segment boundaries are set at the positions of the n highest depthscores, which is common practice in text segmentation algorithms. An alternative to a given n would be the selection of segments according to a depth score threshold. 4 Experimental Setup As dataset the Choi dataset (Choi, 2000) is used. This dataset is an artificially generated corpus that consists of 700 documents. Each document consists of 10 segments and each segment has 3– 11 sentences extracted from a document of the Brown corpus. For the first setup, we perform a 10-fold Cross Validation (CV) for estimating the TM (estimating on 630 documents at a time), for the other setups we use 600 documents for TM estimation and the remaining 100 documents for testing. While we aim to neglect using the same documents for training and testing, it is not guaranteed that all testing data is unseen, since the same source sent"
W12-0703,N09-1040,0,0.0730746,"DA since it is the most commonly used TM in the field of NLP. To evaluate the contribution of the TM, we choose the task of TS: this task has received considerable interest from the NLP community, standard datasets and evaluation measures are available for testing, and it 19 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19–27, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics has been shown that this task considerably benefits from the use of TMs, see (Misra et al., 2009; Sun et al., 2008; Eisenstein, 2009). This paper is organized as follows: In the next section, we present related work regarding text segmentation using topic models and topic model parameter evaluations. Section 3 defines the TopicTiling text segmentation algorithm, which is a simplified version of TextTiling (Hearst, 1994), and makes direct use of topic assignments. Its simplicity allows us to observe direct consequences of LDA parameter settings. Further, we describe the experimental setup, our applicationbased evaluation methodology including the data set and the LDA parameters we vary in Section 4. Results of our experiment"
W12-0703,P94-1002,0,0.733234,"13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19–27, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics has been shown that this task considerably benefits from the use of TMs, see (Misra et al., 2009; Sun et al., 2008; Eisenstein, 2009). This paper is organized as follows: In the next section, we present related work regarding text segmentation using topic models and topic model parameter evaluations. Section 3 defines the TopicTiling text segmentation algorithm, which is a simplified version of TextTiling (Hearst, 1994), and makes direct use of topic assignments. Its simplicity allows us to observe direct consequences of LDA parameter settings. Further, we describe the experimental setup, our applicationbased evaluation methodology including the data set and the LDA parameters we vary in Section 4. Results of our experiments in Section 5 indicate that a) there is an optimal range for the number of topics, b) there is considerable variance in performance for different runs for both model estimation and inference, c) increasing the number of sampling iterations stabilizes average performance but does not make"
W12-0703,P08-2068,0,0.635597,"or this, we pick LDA since it is the most commonly used TM in the field of NLP. To evaluate the contribution of the TM, we choose the task of TS: this task has received considerable interest from the NLP community, standard datasets and evaluation measures are available for testing, and it 19 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 19–27, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics has been shown that this task considerably benefits from the use of TMs, see (Misra et al., 2009; Sun et al., 2008; Eisenstein, 2009). This paper is organized as follows: In the next section, we present related work regarding text segmentation using topic models and topic model parameter evaluations. Section 3 defines the TopicTiling text segmentation algorithm, which is a simplified version of TextTiling (Hearst, 1994), and makes direct use of topic assignments. Its simplicity allows us to observe direct consequences of LDA parameter settings. Further, we describe the experimental setup, our applicationbased evaluation methodology including the data set and the LDA parameters we vary in Section 4. Result"
W12-0703,P01-1064,0,\N,Missing
W12-3307,W04-2214,0,0.0127827,"Missing"
W12-3307,A00-2004,0,0.554188,"ned by the LDA inference method, instead of words. As some of the topic IDs obtained by the inference method tend to change for different runs, we recommend to use the most probable topic ID assigned during the inference. We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment. These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID. Without parameter optimization we obtain state-of-the-art results based on the Choi dataset (Choi, 2000). We show that the mode assignment improves the results substantially and improves even more when parameterizing the size of sampled blocks using a window size parameter. Using these optimizations, we obtain significant improvements compared to other algorithms based on the Choi dataset and also on a more difficult Wall Street Journal (WSJ) corpus provided by Galley et al. (2003). Not only does TopicTiling deliver state-of-the-art segmentation results, it also performs the segmentation in linear time, as opposed to most other recent TS algorithms. The paper is organized as follows: The next se"
W12-3307,N09-1040,0,0.110026,"1) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. Related to our work are the DP approaches described in Misra et al. (2009) and Sun et al. (2008): here, topic modeling is used to alleviate the sparsity of word vectors. This approach was extended by (Misra et al., 2009) and (Sun et al., 2008) using topic information achieved from the LDA topic model. The first hierarchical algorithm was proposed by Yaari (1997), using the cosine similarity and agglomerative clustering approaches. A hierarchical Bayesian algorithm based on LDA is introduced with Eisenstein (2009). In our work, however, we focus on linear TS. LDA was introduced by Blei et al. (2003) and is a generative model that discovers topics based on a training corpus. Model training estimates two distributions: A topic-word distribution and a topicdocument distribution. As LDA is a generative probabilistic model, the creation process follows a generative story: First, for each document a topic distribution is sampled. Then, for each document, words are randomly chosen, following the previously sampled topic distribution. Using the Gibbs inference method, LDA is used to apply a trained model for u"
W12-3307,P03-1071,0,0.200972,"he cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID. Without parameter optimization we obtain state-of-the-art results based on the Choi dataset (Choi, 2000). We show that the mode assignment improves the results substantially and improves even more when parameterizing the size of sampled blocks using a window size parameter. Using these optimizations, we obtain significant improvements compared to other algorithms based on the Choi dataset and also on a more difficult Wall Street Journal (WSJ) corpus provided by Galley et al. (2003). Not only does TopicTiling deliver state-of-the-art segmentation results, it also performs the segmentation in linear time, as opposed to most other recent TS algorithms. The paper is organized as follows: The next section gives an overview of text segmentation algorithms. Section 3 introduces the TopicTiling TS algorithm. The Choi and the Galley datasets used to measure the performance of TopicTiling are described in Section 4. In the evaluation section, the results of TopicTiling are demonstrated on these datasets, followed by a conclusion and discussion. 2 Related Work TS can be divided in"
W12-3307,P94-1002,0,0.103992,"ve than other LDA-based segmentation methods. 1 Introduction The task tackled in this paper is Text Segmentation (TS), which is to be understood as the segmentation of texts into topically similar units. This implies, viewing the text as a sequence of subtopics, that a subtopic change marks a new segment. The challenge for a text segmentation algorithm is to find the sub-topical structure of a text. In this work, this semantic information is gained from Topic Models (TMs). We introduce a newly developed TS algorithm called TopicTiling. The core algorithm is a simplified version of TextTiling (Hearst, 1994), where blocks of text are compared via bag-of-word vectors. TopicTiling uses topic IDs, obtained by the LDA inference method, instead of words. As some of the topic IDs obtained by the inference method tend to change for different runs, we recommend to use the most probable topic ID assigned during the inference. We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment. These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID."
W12-3307,H94-1020,0,0.0756613,"n from different documents, are combined forming one doc39 ument. 400 documents consist of segments with a sentence length of 3-11 sentences and there are 100 documents each with sentence lengths of 3-5, 6-8 and 9-11. 4.2 Galley Dataset Galley et al. (2003) present two corpora for written language, each having 500 documents, which are also generated artificially. In comparison to Choi’s dataset, the segments in its ’documents’ vary from 4 to 22 segments, and are composed by concatenating full source documents. One dataset is generated based on WSJ documents of the Penn Treebank (PTB) project (Marcus et al., 1994) and the other is based on Topic Detection Track (TDT) documents (Wayne, 1998). As the WSJ dataset seems to be harder (consistently higher error rates across several works), we use this dataset for experimentation. 5 Evaluation The performance of TopicTiling is evaluated using two measures, commonly used in the TS task: The Pk measure and the WindowDiff (WD) measure (Beeferman et al., 1999; Pevzner and Hearst, 2002). Besides the training corpus, the following parameters need to be specified for LDA: The number of topics T , the number of sample iterations for the model m and two hyperparameter"
W12-3307,J02-1002,0,0.741659,"its ’documents’ vary from 4 to 22 segments, and are composed by concatenating full source documents. One dataset is generated based on WSJ documents of the Penn Treebank (PTB) project (Marcus et al., 1994) and the other is based on Topic Detection Track (TDT) documents (Wayne, 1998). As the WSJ dataset seems to be harder (consistently higher error rates across several works), we use this dataset for experimentation. 5 Evaluation The performance of TopicTiling is evaluated using two measures, commonly used in the TS task: The Pk measure and the WindowDiff (WD) measure (Beeferman et al., 1999; Pevzner and Hearst, 2002). Besides the training corpus, the following parameters need to be specified for LDA: The number of topics T , the number of sample iterations for the model m and two hyperparameters α and β, specifying the sparseness of the topic-document and the topic-word distribution. For the inference method, the number of sampling iterations i is required. In line with Griffiths and Steyvers (2004), the following standard parameters are used: T = 100, α = 50/T , β = 0.01, m = 500, i = 100. We use the JGibbsLDA implementation described in Phan and Nguyen (2007). 5.1 Evaluation of the Choi Dataset For the"
W12-3307,N12-1064,1,0.392789,"opic distribution is sampled. Then, for each document, words are randomly chosen, following the previously sampled topic distribution. Using the Gibbs inference method, LDA is used to apply a trained model for unseen documents. Here, words are annotated by topic IDs by assigning a topic ID sampled by the document-word and word-topic distribution. Note that the inference procedure, in particular, marks the difference between LDA and earlier dimensionality reduction techniques such as Latent Semantic Analysis. 3 TopicTiling This section introduces the TopicTiling algorithm, first introduced in (Riedl and Biemann, 2012a). 38 In contrast to the quite similar TextTiling algorithm, TopicTiling is not based on words, but on the last topic IDs assigned by the Bayesian Inference method of LDA. This increases sparsity since the word space is reduced to a topic space of much lower dimension. Therefore, the documents that are to be segmented have first to be annotated with topic IDs. For useful topic distinctions, however, the topic model must be trained on documents similar in content to the test documents. Preliminary experiments have shown that repeating the Bayesian inference, often leads to different topic dist"
W12-3307,W12-0703,1,0.659949,"opic distribution is sampled. Then, for each document, words are randomly chosen, following the previously sampled topic distribution. Using the Gibbs inference method, LDA is used to apply a trained model for unseen documents. Here, words are annotated by topic IDs by assigning a topic ID sampled by the document-word and word-topic distribution. Note that the inference procedure, in particular, marks the difference between LDA and earlier dimensionality reduction techniques such as Latent Semantic Analysis. 3 TopicTiling This section introduces the TopicTiling algorithm, first introduced in (Riedl and Biemann, 2012a). 38 In contrast to the quite similar TextTiling algorithm, TopicTiling is not based on words, but on the last topic IDs assigned by the Bayesian Inference method of LDA. This increases sparsity since the word space is reduced to a topic space of much lower dimension. Therefore, the documents that are to be segmented have first to be annotated with topic IDs. For useful topic distinctions, however, the topic model must be trained on documents similar in content to the test documents. Preliminary experiments have shown that repeating the Bayesian inference, often leads to different topic dist"
W12-3307,P08-2068,0,0.0703941,"as introduced by Hearst (1994): TextTiling segments texts in linear time by calculating the similarity between two blocks of words based on the cosine similarity. The calculation is accomplished by two vectors containing the number of occurring terms of each block. LcSeg (Galley et al., 2003), a TextTiling-based algorithm, uses tf-idf term weights and improved TS results compared to TextTiling. Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. Related to our work are the DP approaches described in Misra et al. (2009) and Sun et al. (2008): here, topic modeling is used to alleviate the sparsity of word vectors. This approach was extended by (Misra et al., 2009) and (Sun et al., 2008) using topic information achieved from the LDA topic model. The first hierarchical algorithm was proposed by Yaari (1997), using the cosine similarity and agglomerative clustering approaches. A hierarchical Bayesian algorithm based on LDA is introduced with Eisenstein (2009). In our work, however, we focus on linear TS. LDA was introduced by Blei et al. (2003) and is a generative model that discovers topics based on a training corpus. Model training"
W12-3307,P01-1064,0,0.126577,"2. 2012 Association for Computational Linguistics hierarchical segmentation is concerned with finding more fine grained subtopic structures in texts. One of the first unsupervised linear TS algorithms was introduced by Hearst (1994): TextTiling segments texts in linear time by calculating the similarity between two blocks of words based on the cosine similarity. The calculation is accomplished by two vectors containing the number of occurring terms of each block. LcSeg (Galley et al., 2003), a TextTiling-based algorithm, uses tf-idf term weights and improved TS results compared to TextTiling. Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00. Related to our work are the DP approaches described in Misra et al. (2009) and Sun et al. (2008): here, topic modeling is used to alleviate the sparsity of word vectors. This approach was extended by (Misra et al., 2009) and (Sun et al., 2008) using topic information achieved from the LDA topic model. The first hierarchical algorithm was proposed by Yaari (1997), using the cosine similarity and agglomerative clustering approaches. A hierarchical Bayesian algorithm based on LDA is introduced with Ei"
W13-1409,W12-2513,0,0.0117646,", 2004; Fischer-Starcke, 2010; Mahlberg, 2012) or one certain text, often compared to other texts of the same author or period, e.g. (Craig, 1999; McKenna and Antonia, 2001; Stubbs, 2005; Clement, 2008; Fischer-Starcke, 2009). Some studies focus on content-related questions such as the analysis of plot or characterization and the exploration of relations between and role of different characters, e.g. (Mahlberg, 2007; Culpeper, 2002; Culpeper, 2009), developing new ways of exploring these literary features, e.g. via the application of social network analysis (Elson et al., 2010; Moretti, 2011; Agarwal et al., 2012). Besides this area, there are numerous other approaches, like the attempt to investigate the phenomenon of “literary creativity” (Hoey, 2007) or ways for automatic recognition of literary genres (Allison et al., 2011). Major methodological approaches of this field are, according to Biber (2011), Mahlberg (2007) and Hoover (2008), the study of keywords and wordfrequencies, co-occurrences, lexical clusters (also called bundles or n-grams) and collocational as well as concordance analysis. Additionally, the need for cross-investigating and comparing the results with other corpora (be it a genera"
W13-1409,J93-1003,0,0.0686116,"s.com/sandbox/ network-diagrams/bin/walden/ 3 http://bncweb.lancs.ac.uk/bncwebSignup/ user/login.php 4 http://cwb.sourceforge.net 5 http://corpora.uni-leipzig.de/ 62 ried out for producing the contents of CoocViewer’s database. These steps consist of a fairly standard natural language processing pipeline, which we describe shortly. After tokenizing, part-of-speech tagging (Schmid, 1994) and indexing the input data by document, sentence and paragraph within the document, we compute signficant sentence-wide and paragraphwide co-occurrences, using the tinyCC6 tool. Here, the log-likelihood test (Dunning, 1993) is employed to determine the significance sig(A, B) of the cooccurrence of two tokens A and B. To support the significant concordance view (described in the next section), we have extended the tool to also produce positional significant co-occurrences, where sig(A, B, of f set) is computed by the loglikelihood significance of the co-occurrence of A and B in a token-distance of of f set. Since the significance measure requires the single frequencies of A and B, as well as their joint frequency per positional offset in this setup, this adds considerable overhead during preprocessing. To our kno"
W13-1409,P10-1015,0,0.0156035,"’s style, e.g. (Burrows, 1987; Hori, 2004; Fischer-Starcke, 2010; Mahlberg, 2012) or one certain text, often compared to other texts of the same author or period, e.g. (Craig, 1999; McKenna and Antonia, 2001; Stubbs, 2005; Clement, 2008; Fischer-Starcke, 2009). Some studies focus on content-related questions such as the analysis of plot or characterization and the exploration of relations between and role of different characters, e.g. (Mahlberg, 2007; Culpeper, 2002; Culpeper, 2009), developing new ways of exploring these literary features, e.g. via the application of social network analysis (Elson et al., 2010; Moretti, 2011; Agarwal et al., 2012). Besides this area, there are numerous other approaches, like the attempt to investigate the phenomenon of “literary creativity” (Hoey, 2007) or ways for automatic recognition of literary genres (Allison et al., 2011). Major methodological approaches of this field are, according to Biber (2011), Mahlberg (2007) and Hoover (2008), the study of keywords and wordfrequencies, co-occurrences, lexical clusters (also called bundles or n-grams) and collocational as well as concordance analysis. Additionally, the need for cross-investigating and comparing the resu"
W13-5002,J10-4006,0,0.099525,"ext, and splits these observations into a pair of two parts, which we call the “Jo” and the “Bim”2 . All JoBim pairs are maintained in the bipartite First-Order JoBim graph T C(T, C, E) with T set of terms (Jos), C set of contexts (Bims), and e(t, c, f ) ∈ E edges between t ∈ T , c ∈ C with frequency f . While these parts can be thought of as language elements referred to as terms, and their respective context features, splits over arbitrary structures are possible (including pairs of terms for Jos), which makes this formulation more general than similar formulations found e.g. in (Lin, 1998; Baroni and Lenci, 2010). These splits form the basis for the computation of global similarities and for their contextualization. A Holing System based on dependency parses is illustrated in Figure 1: for each dependency relation, two JoBim pairs are generated. 2.2 Distributed Distributional Thesaurus Computation We employ the Apache Hadoop MapReduce Framwork3 , and Apache Pig4 , for parallelizing and distributing the computation of the DT. We describe this computation in terms of graph transformations. 2 arbitrary names to emphasize the generality, should be thought of as ”term” and ”context” 3 http://hadoop.apache."
W13-5002,D10-1115,0,0.0529973,"Missing"
W13-5002,W10-2309,1,0.861363,"corpus and the general out-of-context similarity of each expanded term to the corresponding term in the original sentence. Therefore the probability associated with any expansion t for any position xi is given by Equation 1. Where Z is the partition function, a normalization constant. P (xi = t) = 1 Z X ew(x) (1) {x |xi =t} The balance between the plausibility of an expanded sentence according to the language model, and its per-term similarity to the original sentence is an application specific tuning parameter. 2.4 We cluster this graph using the Chinese Whispers graph clustering algorithm (Biemann, 2010), which finds the number of clusters automatically, to obtain induced word senses. Running shallow, partof-speech-based IS-A patterns (Hearst, 1992) over the text collection, we obtain a list of extracted ISA relationships between terms, and their frequency. For each of the word clusters, consisting of similar terms for the same target term sense, we aggregate the IS-A information by summing the frequency of hypernyms, and multiplying this sum by the number of words in the cluster that elicited this hypernym. This results in taxonomic information for labeling the clusters, which provides an ab"
W13-5002,de-marneffe-etal-2006-generating,0,0.0289943,"Missing"
W13-5002,J93-1003,0,0.142765,"tions in (Biemann and Riedl, 2013), but could be replaced by any other measure without interfering with the remainder of the system. 2.3 Contextualization with CRF While the distributional thesaurus provides the similarity between pairs of terms, the fidelity of a particular expansion depends on the context. From the term-context associations gathered in the construction of the distributional thesaurus we effectively have a language model, factorized according to the holing operation. As with any language model, smoothing is critical to performance. There may be 5 we use log-likelihood ratio (Dunning, 1993) or LMI (Evert, 2004) many JoBim (term-context) pairs that are valid and yet under represented in the corpus. Yet, there may be some similar term-context pair that is attested in the corpus. We can find similar contexts by expanding the term arguments with similar terms. However, again we are confronted with the fact that the similarity of these terms depends on the context. This suggests some technique of joint inference to expand terms in context. We use marginal inference in a conditional random field (CRF) (Lafferty et al., 2001). A particular world, x is defined as single, definite sequen"
W13-5002,D08-1094,0,0.113522,"Missing"
W13-5002,C92-2082,0,0.630311,"associated with any expansion t for any position xi is given by Equation 1. Where Z is the partition function, a normalization constant. P (xi = t) = 1 Z X ew(x) (1) {x |xi =t} The balance between the plausibility of an expanded sentence according to the language model, and its per-term similarity to the original sentence is an application specific tuning parameter. 2.4 We cluster this graph using the Chinese Whispers graph clustering algorithm (Biemann, 2010), which finds the number of clusters automatically, to obtain induced word senses. Running shallow, partof-speech-based IS-A patterns (Hearst, 1992) over the text collection, we obtain a list of extracted ISA relationships between terms, and their frequency. For each of the word clusters, consisting of similar terms for the same target term sense, we aggregate the IS-A information by summing the frequency of hypernyms, and multiplying this sum by the number of words in the cluster that elicited this hypernym. This results in taxonomic information for labeling the clusters, which provides an abstraction layer for terms in context6 . Table 1 shows an example of this labeling from the model described below. The most similar 200 terms for ”ja"
W13-5002,A83-1012,0,0.184331,"Missing"
W13-5002,N10-4001,0,0.0231793,"employed in other frameworks for distributional semantics such as LSA (Deerwester et al., 1990) or LDA (Blei et al., 2003). The main contribution of this paper is to describe how we operationalize semantic similarity in a graph-based framework and explore this semantic graph using an interactive visualization. We describe a scalable and flexible computation of a distributional thesaurus (DT), and the contextualization of distributional similarity for specific occurrences of language elements (i.e. terms). For related works on the computation of distributional similarity, see e.g. (Lin, 1998; Lin and Dyer, 2010). 2.1 Holing System To keep the framework flexible and abstract with respect to the pre-processing that identifies structure in language material, we introduce the holing operation, cf. (Biemann and Riedl, 2013). It is applied to observations over the structure of text, and splits these observations into a pair of two parts, which we call the “Jo” and the “Bim”2 . All JoBim pairs are maintained in the bipartite First-Order JoBim graph T C(T, C, E) with T set of terms (Jos), C set of contexts (Bims), and e(t, c, f ) ∈ E edges between t ∈ T , c ∈ C with frequency f . While these parts can be tho"
W13-5002,P98-2127,0,0.946274,"t. We construct a semantic analyzer able to self-adapt to new domains and languages by unsupervised learning of semantics from large corpora of raw text. At the moment, this analyzer encompasses contextualized similarity, sense clustering, and a mapping of senses to existing knowledge bases. While its primary target application is functional domain adaptation of Question Answering (QA) systems (Fer1 http://sf.net/projects/jobimtext/ Underlying Technologies While distributional semantics (de Saussure, 1959; Harris, 1951; Miller and Charles, 1991) and the computation of distributional thesauri (Lin, 1998) has been around for decades, its full potential has yet to be utilized in Natural Language Processing (NLP) tasks and applications. Structural semantics claims that meaning can be fully defined by semantic oppositions and relations between words. In order to perform a reliable knowledge acquisition process in this framework, we gather statistical information about word co-occurrences with syntactic contexts from very large corpora. To avoid the intrinsic quadratic complexity of the similarity computation, we have developed an optimized process based on MapReduce (Dean and Ghemawat, 2004) that"
W13-5002,C12-1109,1,0.898356,"Missing"
W13-5002,N13-1133,1,0.897977,"Missing"
W13-5002,C02-1114,0,0.0375749,"ilar terms higher. To model this more explicitly, and to give rise to linking senses to taxonomies and domain ontologies, we apply a word sense induction (WSI) technique and use information extracted by IS-A-patterns (Hearst, 1992) to label the clusters. Using the aggregated context features of the clusters, the word cluster senses are assigned in context. The DT entry for each term j as given in SOJO (J, E) induces an open neighborhood graph Nj (Vj , Ej ) with Vj = {j 0 |e(j, j 0 , s) ∈ E) and Ej the projection of E regarding Vj , consisting of similar terms to j and their similarities, cf. (Widdows and Dorow, 2002). 8 jaguar N.1 IS-A labels car, brand, company, automaker, manufacturer, vehicle animal, species, wildlife, team, wild animal, cat similar terms geely, lincoln-mercury, tesla, peugeot, ..., mitsubishi, cadillac, jag, benz, mclaren, skoda, infiniti, sable, thunderbird panther, cougar, alligator, tiger, elephant, bull, hippo, dragon, leopard, shark, bear, otter, lynx, lion Table 1: Word sense induction and cluster labeling example for “jaguar”. The shortened cluster for the car sense has 186 members. 3 Interactive Visualization 3.1 Open Domain Model The open domain model used in the current visu"
W13-5002,C98-2122,0,\N,Missing
W13-5006,J10-4006,0,0.0249383,"utomatically computed word similarities Related Work The need to model semantics just in the same way as local syntax is covered by the n-gram-model, i.e. trained from a background corpus sparked a large body of research on semantic modeling. This includes computational models for topicality (Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003), and language models that incorporate topical (as well as syntactic) information, see e.g. (BoydGraber and Blei, 2008; Tan et al., 2012). In the Computational Linguistics community, the vector space model (Sch¨utze, 1993; Turney and Pantel, 2010; Baroni and Lenci, 2010; Pucci et al., 2009; de Cruys et al., 2013) is the prevalent metaphor for representing word meaning. While the computation of semantic similarities on the basis of a background corpus produces a global model, which e.g. contains semantically similar words for different word senses, there are a number of works that aim at contextualizing the information held in the global model for particular occurrences. With his predication algorithm, Kintsch (2001) contextualizes LSA (Deerwester et al., 1990) for N-VP constructions by spreading activation over neighbourhood graphs in the latent space. In pa"
W13-5006,N13-1134,0,0.0405117,"Missing"
W13-5006,W10-2808,0,0.0599044,"Missing"
W13-5006,P99-1004,0,0.0309474,"more then 1000 words, as these features do not contribute enough to similarity to justify the increase of computation time (cf. (Rychl´y and Kilgarriff, 2007; Goyal et al., 2010)). The secondorder similarity graph between terms is defined as SO(T, E) for t1 , t2 ∈ T with the similarity score s = |{c|e(t1 , c) ∈ F O, e(t2 , c) ∈ F O}|, which is the number of salient features two terms share. SO defines a distributional thesaurus. In contrast to (Lin, 1998) we do not count how often a feature occurs with a term (we use significance ranking instead), and do not use cosine or other similarities (Lee, 1999) to calculate the similarity over the feature counts of each term, but only count significant common features per term. This constraint makes this approach more scalable to larger data, as we do not need to know the full list of features for a term pair at any time. Seemingly simplistic, we show in (Biemann and Riedl, 2013) that this measure outperforms other measures on large corpora in a semantic relatedness evaluation. 3.3 Contextual Similarity The contextualization is framed as a ranking problem: given a set of candidate expansions as provided by the SO graph, we aim at ranking them such t"
W13-5006,P98-2127,0,0.0936039,"consider two distinct viewpoints: syntagmatic relations consider the assignment of values to a linear sequence of terms, and the associative (also: paradigmatic) viewpoint assigns values according to the commonalities and differences to other terms in the reader’s memory. Based on these notions, we automatically expand terms in the linear sequence with their paradigmatically related terms. Using the distributional hypothesis (Harris, 1951), and operationalizing similarity of terms (Miller and Charles, 1991), it became possible to compute term similarities for a large vocabulary (Ruge, 1992). Lin (1998) computed a distributional thesaurus (DT) by comparing context features defined over grammatical dependencies with an appropriate similarity measure for all reasonably frequent words in a large collection of text, and evaluated these automatically computed word similarities Related Work The need to model semantics just in the same way as local syntax is covered by the n-gram-model, i.e. trained from a background corpus sparked a large body of research on semantic modeling. This includes computational models for topicality (Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003), and languag"
W13-5006,de-marneffe-etal-2006-generating,0,0.0427864,"Missing"
W13-5006,P08-1028,0,0.106779,"Missing"
W13-5006,P07-2011,0,0.0400495,"Missing"
W13-5006,J12-3007,0,0.021441,"es with an appropriate similarity measure for all reasonably frequent words in a large collection of text, and evaluated these automatically computed word similarities Related Work The need to model semantics just in the same way as local syntax is covered by the n-gram-model, i.e. trained from a background corpus sparked a large body of research on semantic modeling. This includes computational models for topicality (Deerwester et al., 1990; Hofmann, 1999; Blei et al., 2003), and language models that incorporate topical (as well as syntactic) information, see e.g. (BoydGraber and Blei, 2008; Tan et al., 2012). In the Computational Linguistics community, the vector space model (Sch¨utze, 1993; Turney and Pantel, 2010; Baroni and Lenci, 2010; Pucci et al., 2009; de Cruys et al., 2013) is the prevalent metaphor for representing word meaning. While the computation of semantic similarities on the basis of a background corpus produces a global model, which e.g. contains semantically similar words for different word senses, there are a number of works that aim at contextualizing the information held in the global model for particular occurrences. With his predication algorithm, Kintsch (2001) contextuali"
W13-5006,C98-2122,0,\N,Missing
W14-5117,J92-4003,0,0.486538,"chnique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008), and the datasets reported in (Biemann et al., 2007b) for Hindi and German. These datasets were used for unsupervised lexical acquisition. A Distributional Thesaurus (DT) is an automatically computed resource that relates words according to their similarity. A"
W14-5117,D10-1056,0,0.0151201,"ed on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical representations and showed performance gains in knowledge-based word sense disambiguation (WSD), the expansion technique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008),"
W14-5117,E03-1009,0,0.0503129,"uding NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008), and the datasets reported in (Biemann et al., 2007b) for Hindi and German. These datasets were used for unsupervised lexical acquisition. A Distributional Thesaurus (DT) is an automatically computed resource that relates words according to their similarity. A DT contains, for every sufficiently frequent word, the most"
W14-5117,P98-2127,0,0.369085,"sion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical representations and showed performance gains in knowledge-based word sense disambiguation (WSD), the expansion technique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to buil"
W14-5117,C12-1109,1,0.853279,"problems including NERC (Ekbal and Saha, 2011a; Sofianopoulos and Tambouratzis, 2010). 1.2 Unsupervised Lexical Acquisition One of the major problems in applying machine learning algorithms for solving information extraction problems is the availability of large annotated corpora. We explore possibilities arisD S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 107–112, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) ing from the use of unsupervised part-of-speech (PoS) induction (Biemann, 2009) and lexical expansion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller"
W14-5117,D13-1089,1,0.903679,"fianopoulos and Tambouratzis, 2010). 1.2 Unsupervised Lexical Acquisition One of the major problems in applying machine learning algorithms for solving information extraction problems is the availability of large annotated corpora. We explore possibilities arisD S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 107–112, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) ing from the use of unsupervised part-of-speech (PoS) induction (Biemann, 2009) and lexical expansion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical represent"
W16-1620,E09-1005,0,0.0100572,"systems were trained on the ukWaC corpus. ing the top-ranked SemEval submissions and AdaGram, were able to beat the most frequent sense baselines of the respective datasets (with the exception of the balanced version of TWSI). Similar results are observed for other unsupervised WSD methods (Nieto Pi˜na and Johansson, 2016). ambiguate word senses. The UoS system (Hope and Keller, 2013) is most similar to our approach: to induce senses it builds an ego-network of a word using dependency relations, which is subsequently clustered using a simple graph clustering algorithm. The La Sapienza system (Agirre and Soroa, 2009), relies on WordNet to get word senses and perform disambiguation. Table 5 shows a comparative evaluation of our method on the SemEval dataset. Like above, dependency-based (JBT) word similarities yield slightly better results than word embedding similarity (w2v) for inventory induction. In addition to these two configurations, we also built a model based on the TWSI sense inventory (only for nouns as the TWSI contains nouns only). This model significantly outperforms both JBT- and w2v-based models, thus precise sense inventories greatly influence WSD performance. As one may observe, performan"
W16-1620,P15-1072,0,0.0300438,"Missing"
W16-1620,J90-1003,0,0.346085,"ilarity of their respective vectors. For scalability reasons, we perform similarity computations via block matrix multiplications, using blocks of 1000 vectors. Similarities using JoBimText (JBT). In this unsupervised approach, every word is represented 1 https://code.google.com/p/word2vec We used an English Wikipedia dump of October 2015: http://panchenko.me/data/joint/ corpora/en59g/wikipedia.txt.gz 2 176 as a bag of sparse dependency-based features extracted using the Malt parser and collapsed using an approach similar to (Ruppert et al., 2015). Features are normalized using the LMI score (Church and Hanks, 1990) and further pruned down according to the recommended defaults: we keep 1000 features per word and 1000 words per feature. Similarity of two words is equal to the number of common features. Multiple alternatives exist for computation of semantic relatedness (Zhang et al., 2013). JBT has two advantages in our case: (1) accurate estimation of word similarities based on dependency features; (2) efficient computation of nearest neighbours for all words in a corpus. Besides, we observed that nearest neighbours of word embeddings often tend to belong to the dominant sense, even if minor senses have"
W16-1620,J10-4006,0,0.010358,"se to downstream applications. Experiments show that the performance of our method is comparable to state-of-the-art unsupervised WSD systems. 1 Introduction 2 Term representations in the form of dense vectors are useful for many natural language processing applications. First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi"
W16-1620,P14-1023,0,0.201214,"on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurate similarities (therefore, the stage (2) is changed). Second, we use a sense inventory constructed using crowdsourcing (thus, stages (2) and (3) are skipped). Below we describe each of the stages of our method in detail. 3.1 To learn word vectors, we use the word2vec toolkit (Mikolov et al., 2013), namely we train CBOW word embeddings with 100 or 300 dimensions, context window size of 3 and minimum word frequency of 5. We selected these parameters according to prior evaluations, e.g. (Baroni et al., 2014), and tested them on the development dataset (see Section 5.1). Initial experiments showed that this configuration is superior to others, e.g. the Skip-gram model, with respect to WSD performance. For training, we modified the standard implementation of word2vec1 so that it also saves context vectors needed for one of our WSD approaches. For experiments, we use two commonly used corpora for training distributional models: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is e"
W16-1620,P12-1092,0,0.0216553,"ssing applications. First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (20"
W16-1620,S13-2050,0,0.0250181,"gher performance (0.410 vs 0.390) on the balanced TWSI, indicating the importance of a precise sense inventory. Finally, using the ”gold” TWSI inventory significantly improves the performance on the balanced dataset outperforming models based on induced inventories. 5.2 Evaluation metrics. Performance is measured with three measures that require a mapping of sense inventories (Jaccard Index, Tau and WNDCG) and two cluster comparison measures (Fuzzy NMI and Fuzzy B-Cubed). Discussion of results. We compare our approach to SemEval participants and the AdaGram sense embeddings. The AI-KU system (Baskaya et al., 2013) directly clusters test contexts using the k-means algorithm based on lexical substitution features. The Unimelb system (Lau et al., 2013) uses a hierarchical topic model to induce and disEvaluation on SemEval-2013 Task 13 The goal of this evaluation is to compare the performance of our method to state-of-the-art unsupervised WSD systems. 180 Supervised Evaluation Jacc. Ind. Tau WNDCG Model Clustering Evaluation F.NMI F.B-Cubed Baselines One sense for all One sense per instance Most Frequent Sense (MFS) 0.171 0.000 0.579 0.627 0.953 0.583 0.302 0.000 0.431 0.000 0.072 – 0.631 0.000 – SemEval A"
W16-1620,S13-2049,0,0.0332809,"38 0.305 0.275 0.366 0.383 0.360 0.338 0.305 0.275 Table 3: Upper-bound and actual value of the WSD performance on the sense-balanced TWSI dataset, function of sense inventory used for unweighted pooling of word vectors. Figure 3: WSD performance of our method trained on the Wikipedia corpus on the full (on the left) and on the sense-balanced (on the right) TWSI dataset. The w2v models are based on the CBOW with 300 dimensions and context window size 3. The JBT models are computed using the Malt parser. Dataset. The SemEval-2013 task 13 “Word Sense Induction for Graded and Non-Graded Senses” (Jurgens and Klapaftis, 2013) provides 20 nouns, 20 verbs and 10 adjectives in WordNetsense-tagged contexts. It contains 20-100 contexts per word, and 4,664 contexts in total, which were drawn from the Open American National Corpus. Participants were asked to cluster these 4,664 instances into groups, with each group corresponding to a distinct word sense. to the mechanism based on probabilities. Indeed, cosine similarity between embeddings proved to be useful for semantic relatedness, yielding stateof-the-art results (Baroni et al., 2014), while there is less evidence about successful use-cases of the CBOW as a language"
W16-1620,W02-0811,0,0.0984888,"6) introduced AdaGram, a non-parametric method for learning sense embeddings based on a Bayesian extension of the Skipgram model. The granularity of learned sense embeddings is controlled by the parameter α. Comparisons of their approach to (Neelakantan et al., 2014) on three SemEval word sense induction and 2.2 Word Sense Disambiguation (WSD) Many different designs of WSD systems were proposed, see (Agirre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurat"
W16-1620,W06-3812,1,0.718622,"aC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and sense hubs in graph sense representation, as in (V´eronis, 2004). 3 Learning Word Vectors 3.2 Calculating Word Similarity Graph At this step, we build a graph of word similarities, such as (table, desk, 0.78)."
W16-1620,S13-2051,0,0.108765,"Missing"
W16-1620,W10-2309,1,0.835524,"ustered with the Chinese Whispers algorithm (Biemann, 2006). This method is parameter free, thus we make no assumptions about the number of word senses. The sense induction algorithm has three metaparameters: the ego-network size (N ) of the target ego word t; the ego-network connectivity (n) is the maximum number of connections the neighbour v is allowed to have within the ego-network; the minimum size of the cluster k. The n parameter regulates the granularity of the inventory. In our experiments, we set the N to 200, n to 50, 100 or 200 and k to 5 or 15 to obtain different granulates, cf. (Biemann, 2010). Each word in a sense cluster has a weight which is equal to the similarity score between this word and the ambiguous word t. Algorithm 1: Word sense induction. input : T – word similarity graph, N – ego-network size, n – ego-network connectivity, k – minimum cluster size output: for each term t ∈ T , a clustering St of its N most similar terms foreach t ∈ T do V ← N most similar terms of t from T G ← graph with V as nodes and no edges E foreach v ∈ V do V 0 ← n most similar terms of v from T foreach v 0 ∈ V 0 do if v 0 ∈ V then add edge (v, v 0 ) to E end end St ← ChineseWhispers(G) St ← {s"
W16-1620,W02-1006,0,0.0174832,"rtunov et al. (2016) introduced AdaGram, a non-parametric method for learning sense embeddings based on a Bayesian extension of the Skipgram model. The granularity of learned sense embeddings is controlled by the parameter α. Comparisons of their approach to (Neelakantan et al., 2014) on three SemEval word sense induction and 2.2 Word Sense Disambiguation (WSD) Many different designs of WSD systems were proposed, see (Agirre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to"
W16-1620,biemann-2012-turk,1,0.907374,"Missing"
W16-1620,D14-1162,0,0.109945,"Missing"
W16-1620,D15-1200,0,0.272475,"represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same num"
W16-1620,N10-1013,0,0.157269,"012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space. Sparse TF-IDF vectors are clustered using a parametric method fixing the same number of senses for all words. Sense vectors are centroids of the clusters. While most dense word vector models represent a word with a single vector and thus conflate senses (Mikolov et al., 2013; Pennington et al., 2014), there are several approaches that produce word sense embeddings. Huang et al. (2012) learn 174 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 174–183, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Ling"
W16-1620,P15-1173,0,0.0368148,"Missing"
W16-1620,Q14-1019,0,0.0655666,"girre and Edmonds, 2007; Navigli, 2009). Supervised approaches use an explicitly sense-labeled training corpus to construct a model, usually building one model per target word (Lee and Ng, 2002; Klein et al., 2002). These approaches demonstrate top performance in competitions, but require considerable amounts of senselabeled examples. Knowledge-based approaches do not learn a model per target, but rather derive sense representation from information available in a lexical resource, such as WordNet. Examples of such system include (Lesk, 1986; Banerjee and Pedersen, 2002; Pedersen et al., 2005; Moro et al., 2014) Unsupervised WSD approaches rely neither on hand-annotated sense-labeled corpora, nor on 175 the word similarity graph, which relies on dependency features and is expected to provide more accurate similarities (therefore, the stage (2) is changed). Second, we use a sense inventory constructed using crowdsourcing (thus, stages (2) and (3) are skipped). Below we describe each of the stages of our method in detail. 3.1 To learn word vectors, we use the word2vec toolkit (Mikolov et al., 2013), namely we train CBOW word embeddings with 100 or 300 dimensions, context window size of 3 and minimum wo"
W16-1620,D14-1113,0,0.156749,"enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a multiprototype vector space."
W16-1620,J98-1004,0,0.241594,"Missing"
W16-1620,R15-1061,0,0.0688527,"Missing"
W16-1620,C14-1016,0,0.131499,"First of all, they enable the computation of semantically related words. Besides, they can be used to represent other linguistic units, such as phrases and short texts, reducing the inherent sparsity of traditional vector-space representations (Salton et al., 1975). One limitation of most word vector models, including sparse (Baroni and Lenci, 2010) and dense (Mikolov et al., 2013) representations, is that they conflate all senses of a word into a single vector. Several architectures for learning multiprototype embeddings were proposed that try to address this shortcoming (Huang et al., 2012; Tian et al., 2014; Neelakantan et al., 2014; Nieto Pi˜na and Johansson, 2015; Bartunov et al., 2016). Li and Jurafsky (2015) provide indications that such sense vectors improve the performance of text proRelated Work Our method learns multi-prototype word embeddings and applies them to WSD. Below we briefly review both strains of research. 2.1 Multi-Prototype Word Vector Spaces In his pioneering work, Sch¨utze (1998) induced sparse sense vectors by clustering context vectors using the EM algorithm. This approach is fitted with a similarity-based WSD mechanism. Later, Reisinger and Mooney (2010) presented a mul"
W16-1620,W16-1401,0,0.0673895,"Missing"
W16-1620,C02-1114,0,0.211562,"odels: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and sense hubs in graph sense representation, as in (V´eronis, 2004). 3 Learning Word Vectors 3.2 Calculating Word Similarity Graph At this step, we build a graph of word similarities, such as (tabl"
W16-1620,W97-0322,0,0.563393,"he standard implementation of word2vec1 so that it also saves context vectors needed for one of our WSD approaches. For experiments, we use two commonly used corpora for training distributional models: Wikipedia2 and ukWaC (Ferraresi et al., 2008). Figure 2: Visualization of the ego-network of “table” with furniture and data sense clusters. Note that the target “table” is excluded from clustering. handcrafted lexical resources. Instead, they automatically induce a sense inventory from raw corpora. Such unsupervised sense induction methods fall into two categories: context clustering, such as (Pedersen and Bruce, 1997; Sch¨utze, 1998; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Bartunov et al., 2016) and word (egonetwork) clustering, such as (Lin, 1998; Pantel and Lin, 2002; Widdows and Dorow, 2002; Biemann, 2006; Hope and Keller, 2013). Unsupervised methods use disambiguation clues from the induced sense inventory for word disambiguation. Usually, the WSD procedure is determined by the design of sense inventory. It might be the highest overlap between the instance’s context words and the words of the sense cluster, as in (Hope and Keller, 2013) or the smallest distance between context words and"
W16-1620,S01-1021,0,\N,Missing
W16-1801,W02-2001,0,0.0506867,"Missing"
W16-1801,W11-1602,0,0.0669186,"Missing"
W16-1801,I05-5001,0,0.0382306,"ets and candidates, skip-grams longest common subsequences, POS tags and proper names. Connor and Roth (2007) develop a global classifier that takes a word v and its context, along with a candidate word u, and determines whether u can replace v in the given context while maintaining the original meaning. Their work focuses on verb paraphrasing. Notions of context include: being either subject or object of the verb, named entities that appear as subject or object, all dependency links connected to the target, all noun phrases in sentences containing the target, or all of the above. The work of Brockett and Dolan (2005) uses annotated datasets and Support Vector Machines (SVMs) to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features in2.2 Multi-word Expression Resources While there are some works on the extraction of multi-word expressions and on investigation of their impact on different NLP applications, as far as we know, there is no single work dedicated 2 on paraphrasing multi-word expressions. Various approaches exist for the extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora."
W16-1801,S07-1009,0,0.213274,"Missing"
W16-1801,W03-1810,0,0.0513713,"s gradient boosting Figure 1: Paraphrase targets (a) and paraphrase candidates (b). We select targets that have at least five candidates in our combined paraphrase resources. The paraphrase resources (S) for candidates generations are composed of collections from PPDB (Pavlick et al., 2015), WordNet and JoBimText distributional thesaurus (DT – only for single words). For MWE paraphrase targets, we have used different MWE resources. A total of 79,349 MWE are collected from WordNet, STREUSLE (Schneider and Smith, 2015; Schneider et al., 2014)4 , Wiki50 (Vincze et al., 2011) and the MWE project (McCarthy et al., 2003; Baldwin and Villavicencio, 2002)5 . We consider MWEs from this resources to be a paraphrase target when it is possible to generate paraphrase candidates from our paraphrase resources (S). Candidates paraphrases for a target (both single and MWE) are generated as follows. For each paraphrase target, we retrieve candidates from the 2 http://www.anc.org/ https://www.mturk.com/mturk/help? helpPage=overview 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.edu/˜vdang/ ranklib.html 7 http://sourceforge.net/projects/ lemur/ 4 Figure 2: User-interface for par"
W16-1801,W15-1501,0,0.0140988,"w 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.edu/˜vdang/ ranklib.html 7 http://sourceforge.net/projects/ lemur/ 4 Figure 2: User-interface for paraphrase selection. to directly optimize learning-to-rank specific cost functions such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). 3.4 by averaging the embeddings of their parts. We use the word embeddings of the target (F 1) and the candidate (F 2) phrases. Candidate-Target similarities: The dot product of the target and candidate embeddings (F 3), as described in (Melamud et al., 2015). Target-Sentence similarity: The dot product between a candidate and the sentence, i.e. the average embeddings of all words in the sentence (F 4). The following features use local context information: Target-Close context similarity: The dot product between the candidate and the left and right 3-gram (F 5) and 5-gram embedding (F 6) resp.. Ngram features: A normalized frequency for a 2-5-gram context with the target and candidate phrases (F 7) based on Google Web 1T 5-Grams10 . Language model score: A normalized language model score using a sentence as context with the target and candidate ph"
W16-1801,W11-0805,0,0.0427054,"Missing"
W16-1801,P11-1027,0,0.046514,"roduct between a candidate and the sentence, i.e. the average embeddings of all words in the sentence (F 4). The following features use local context information: Target-Close context similarity: The dot product between the candidate and the left and right 3-gram (F 5) and 5-gram embedding (F 6) resp.. Ngram features: A normalized frequency for a 2-5-gram context with the target and candidate phrases (F 7) based on Google Web 1T 5-Grams10 . Language model score: A normalized language model score using a sentence as context with the target and candidate phrases (F 8). An n-gram language model (Pauls and Klein, 2011) is built using the BNC and Wikipedia corpora. Also, we experimented with features that eventually did not improve results, such as the embeddings of the target’s n = 5 most similar words, length and length ratios between target and candidate, most similar words and number of shared senses among target and candidate phrases based JoBimText DT (Ruppert et al., 2015), and N-gram POS sequences and dependency labels of the tarFeatures We have modeled three types of features: a resource-based feature where feature values are taken from a lexical resource (F 0), four features based on global context"
W16-1801,P15-2070,0,0.144898,"Missing"
W16-1801,P16-1012,1,0.889804,"Missing"
W16-1801,ramisch-etal-2010-mwetoolkit,0,0.0239911,"extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora. They align the parallel corpus and focus on misalignment, which typically indicates expressions in the source language that are translated to the target in a non-compositional way. Frantzi et al. (2000) present a method to extract multi-word terms from English corpora, which combines linguistic and statistical information. The Multi-word Expression Toolkit (MWEtoolkit) extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillters ranging form simple count thresholds to a more complex cases such as Association Measures (AMs). The tool further supports indexing and searching of MWEs, validation, and annotation facilities. Schneider et al. (2014) developed a sequencetagging-based supervised approach to MWE identification. A rich set of features has been used in a linguistically-driven evaluation of the identification of heterogeneous MWEs. The work by Vincze et al. (2011) constructs a multi-word expression corpus annotated with different types of MWEs such as compound, idiom, verb-particle co"
W16-1801,N06-1058,0,0.0788485,"Missing"
W16-1801,P15-4018,1,0.771204,"of only single words. platform. In the first annotation task, a total of 171 sentences are selected from the British Academic Written English (BAWE) corpus1 (Alsop and Nesi, 2009), with five paraphrase targets. The targets are selected in such a way that a) include MWEs as targets when it is possible (see Subection 3.2 how we select targets), b) the candidates could bear more than one contextual meaning and, c) workers can select up to three paraphrases and have to supply their own paraphrase if none of the candidates match. To satisfy condition b), we have used the JoBimText DT database API (Ruppert et al., 2015) to obtain single word candidates with multiple senses according to automatic sense induction. We conduct this annotation setup twice, both with and without showing the original context (3– 8 sentences). For both setups, a task is assigned to 5 workers. We incorporate control questions with invalid candidate paraphrases in order to reject unreliable workers. In addition to the control questions, JavaScript functions are embedded to ensure that workers select or supply at least one paraphrase. The results are aggregated by summing the number of workers that agreed on candidates, for scores betw"
W16-1801,E14-1057,0,0.0632122,"Missing"
W16-1801,N15-1177,0,0.0253392,"roject7 . In this paper, we present the results obtained using LambdaMART. LambdaMART (Burges, 2010) uses gradient boosting Figure 1: Paraphrase targets (a) and paraphrase candidates (b). We select targets that have at least five candidates in our combined paraphrase resources. The paraphrase resources (S) for candidates generations are composed of collections from PPDB (Pavlick et al., 2015), WordNet and JoBimText distributional thesaurus (DT – only for single words). For MWE paraphrase targets, we have used different MWE resources. A total of 79,349 MWE are collected from WordNet, STREUSLE (Schneider and Smith, 2015; Schneider et al., 2014)4 , Wiki50 (Vincze et al., 2011) and the MWE project (McCarthy et al., 2003; Baldwin and Villavicencio, 2002)5 . We consider MWEs from this resources to be a paraphrase target when it is possible to generate paraphrase candidates from our paraphrase resources (S). Candidates paraphrases for a target (both single and MWE) are generated as follows. For each paraphrase target, we retrieve candidates from the 2 http://www.anc.org/ https://www.mturk.com/mturk/help? helpPage=overview 4 http://www.cs.cmu.edu/˜ark/LexSem/ 5 http://mwe.stanford.edu 3 6 https://people.cs.umass.e"
W16-1801,D13-1198,0,0.0486316,"Missing"
W16-1801,W09-2506,0,0.0368321,"Missing"
W16-1801,C10-2144,0,0.0185508,"ences containing the target, or all of the above. The work of Brockett and Dolan (2005) uses annotated datasets and Support Vector Machines (SVMs) to induce larger monolingual paraphrase corpora from a comparable corpus of news clusters found on the World Wide Web. Features in2.2 Multi-word Expression Resources While there are some works on the extraction of multi-word expressions and on investigation of their impact on different NLP applications, as far as we know, there is no single work dedicated 2 on paraphrasing multi-word expressions. Various approaches exist for the extraction of MWEs: Tsvetkov and Wintner (2010) present an approach to extract MWEs from parallel corpora. They align the parallel corpus and focus on misalignment, which typically indicates expressions in the source language that are translated to the target in a non-compositional way. Frantzi et al. (2000) present a method to extract multi-word terms from English corpora, which combines linguistic and statistical information. The Multi-word Expression Toolkit (MWEtoolkit) extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillt"
W16-1801,R11-1040,0,0.134303,"extracts MWE candidates based on flat n-grams or specific morphosyntactic patterns (of surface forms, lemmas, POS tags) (Ramisch et al., 2010) and apply different fillters ranging form simple count thresholds to a more complex cases such as Association Measures (AMs). The tool further supports indexing and searching of MWEs, validation, and annotation facilities. Schneider et al. (2014) developed a sequencetagging-based supervised approach to MWE identification. A rich set of features has been used in a linguistically-driven evaluation of the identification of heterogeneous MWEs. The work by Vincze et al. (2011) constructs a multi-word expression corpus annotated with different types of MWEs such as compound, idiom, verb-particle constructions, light verb constructions, and others. In our work, we have used a combination of many MWEs resources from different sources for both MWE target detection and candidate generation (see Subsection 3.2). 3 No context Context MWE (ρ) 0.25 0.23 Single (ρ) 0.36 0.32 Table 1: Spearman correlation of human judgment with PPDB2 default rankings. The column MWE shows the result of only MWEs and the column Single shows the result of only single words. platform. In the fir"
W16-1801,Q14-1016,0,\N,Missing
W16-1816,W02-0109,0,0.00705498,"erve consistent improvements over the baseline approach, which does not rely on any MWE resource (None). For manually constructed MWE resources, improvements of up to 3 points F1-measure on MWE labeling are observed, the most useful resource being WikiMe. The combination of manual resources does not yield improvements. Experimental Setting We perform the evaluation, using a 10-fold cross validation and use the crfsuite3 implementation of CRF as classifier. For retrieving POS tags, we apply the OpenNLP POS tagger4 . The lemmatization is performed using the WordNetLemmatizer, contained in nltk (Loper and Bird, 2002).5 For the computation of automatically generated MWEs lists, we use the raw text from an English Wikipedia dump, without considering any markup and annotations. For applying them as resources, we only consider word sequences in the resource that are also contained in the Wiki50 dataset, both training and test data. Based on these candidates, we select the n highest ranked MWE candidates. The previous filtering does not influence the performance of the algorithm but enables an easier filtering parameter. None SemCor WordNet EnWikt WikiMe All resources t-test 1,000 t-test 10,000 DRUID 1,000 DRU"
W16-1816,H93-1061,0,0.429755,"i + 1, i + 2} • tokenj & tokenj+1 with j ∈ {i−2, i−1, i, i+ 1, i + 2} • WikiMe: WikiMe (Hartmann et al., 2012) is a resource extracted from Wikipedia that consists of 356,467 MWEs from length two to four that have been extracted using markup information. • word shape of tokeni , as used by Constant and Sigogne (2011) • has tokeni digits 1 For this, we rely on the MWE resources that are provided here: http://www.cs.cmu.edu/˜ark/ LexSem/mwelex-1.0.zip. • has tokeni alphanumeric characters 108 7 • SemCor: This dataset consists of 16,512 MWE and was generated from the Semantic Concordance corpus (Miller et al., 1993). First, we show the overall performance for the Wiki50 dataset for recognizing labeled MWE and NE spans. We show the performance for training classifiers to predict solely NEs and MWEs and also the combination without the usage of any MWE resource. As can be observed (see Table 2), the detection of NE reaches higher scores than learning to predict MWE. Additionally, we select the best-performing measures for ranking word sequences according to their multiwordness as described in (Riedl and Biemann, 2015) that do not require any POS filtering: • DRUID: We use the DRUID implementation2 , which"
W16-1816,R11-1023,0,0.0695511,"Missing"
W16-1816,W02-1407,0,0.0122123,"Missing"
W16-1816,D15-1290,1,0.68569,"r Science Department Technische Universit¨at Darmstadt {riedl,biem}@cs.tu-darmstadt.de Abstract that rank word sequences according to their multiwordness automatically using information from corpora, mostly relying on frequencies. Many of these methods (e.g. C/NC-Value (Frantzi et al., 1998), GM-MF (Nakagawa and Mori, 2002)) require previous filters, which are based on Part-ofSpeech (POS) sequences. Such sequences, (e.g. Frantzi et al. (1998)) need to be defined and mostly do not cover all POS types of MWE. In this work we do not want to restrict to specific MWE types and thus will use DRUID (Riedl and Biemann, 2015) and the Student’s t-test as multiword ranking methods, which do not require any previous filtering. This paper focuses on the following research question: how do such lists generated from raw text compete against manually generated resources? Furthermore, we want to examine whether a combination of resources yields better performance. In this paper, we demonstrate the impact of Multiword Expression (MWE) resources in the task of MWE recognition in text. We present results based on the Wiki50 corpus for MWE resources, generated using unsupervised methods from raw text and resources that are ex"
W16-1816,W11-0809,0,0.32541,"Missing"
W16-1816,P06-4018,0,\N,Missing
W16-1816,Q14-1016,0,\N,Missing
W16-2613,W16-2606,0,0.0444459,"e Research Training Group AIPHES University of Applied Sciences, Darmstadt Heidelberg University and Technische Universität Darmstadt § www.aiphes.tu-darmstadt.de Abstract is the minimization of the propagation of errors by using methods that perform as accurate as possible in lower level tasks on a diversity of texts and genres. In this paper we present a simple, yet flexible and universally applicable system for tokenization and POS tagging German text. Our system participated in the EmpiriST shared task on “Automatic Linguistic Annotation of Computer-Mediated Communication / Social Media” (Beißwenger et al., 2016). For this task, we applied our solution to texts from two different genres: a) general, html-stripped web data and b) colloquial language from social media texts. The paper is organized as follows: We first describe the shared task and related work Section 2. Our systems for tokenization and POS tagging are laid out in Section 3 and evaluated in Section 4, which includes a detailed error analysis. Section 5 concludes. We present our system used for the AIPHES team submission in the context of the EmpiriST shared task on “Automatic Linguistic Annotation of ComputerMediated Communication / Soci"
W16-2613,A00-1031,0,0.454801,"r German primarily rely on the Stuttgart-Tübingen Tagset (STTS, Schiller et al. (1999)), which consists of 54 POS tags and distinguishes between eleven main parts of speech, which are further divided into various subcategories. The STTS tagset has become a de facto standard for German, as it is also used in major German treebanks, such as the Tiger treebank (Brants et al., 2004), called Tiger henceforth. Tiger consists of approx. 900,000 tokens of German newspaper text (taken from the Frankfurter Rundschau), and the POS annotations have been added semi-automatically. For this, the TnT tagger (Brants, 2000) was used, because it also outputs probabilities that can be used as confidence scores. Only POS tags with a low confidence score were checked for correctness by human annotators. As the basis for the development of the STTStagset were newspaper corpora, STTS only contains six POS tags that describe categories other than the standard grammatical word categories (e.g., non-words or punctuation marks). In contrast, the extended version of STTS used in the EmpiriST shared task contains 18 additional tags for elements that are specific for computermediated communication, for example, tags for emot"
W16-2613,J06-4003,0,0.0527837,"ne can have severe effects to higher level tasks and influence their performance by a large margin. Existing tokenizers can be organized into three categories: a) rule-based methods, b) supervised methods, c) unsupervised methods. Manning et al. (2014)1 , for example, internally use JFlex2 , which is a meta language for rules based on regular expressions and procedures to execute when a rule matches. In contrast, Jurish and Würzner (2013) present a supervised system for joint tokenization and sentence splitting, which employs a Hidden Markov Model on character features for boundary detection. Kiss and Strunk (2006) introduce Punkt, providing an unsupervised model for sentence splitting and tokenization. Kiss and Strunk (2006) use the fact that most ambiguous token or sentence boundaries happen around punctuation characters, such as periods/full stops. Punkt finds collocations of characters before and after punctuations, assuming that these collocations are typical abbreviations, initials, or ordinal numbers which can be maintained as a simple list of non-splittable tokens. 1 As of the current version v3.6 of the Stanford Core Utils, the default PTBTokenizer uses JFlex. 2 http://jflex.de/ 107 transitions"
W16-2613,P14-5010,0,0.00480579,"s common, since tokens – either directly or indirectly – are usually considered to bear the information in a text eventually. However, the importance of tokenization is often neglected, as simple methods like whitespace segmentation can yield acceptable accuracies for many languages at first sight (Webster and Kit, 1992). But errors in an early phase of an NLP pipeline can have severe effects to higher level tasks and influence their performance by a large margin. Existing tokenizers can be organized into three categories: a) rule-based methods, b) supervised methods, c) unsupervised methods. Manning et al. (2014)1 , for example, internally use JFlex2 , which is a meta language for rules based on regular expressions and procedures to execute when a rule matches. In contrast, Jurish and Würzner (2013) present a supervised system for joint tokenization and sentence splitting, which employs a Hidden Markov Model on character features for boundary detection. Kiss and Strunk (2006) introduce Punkt, providing an unsupervised model for sentence splitting and tokenization. Kiss and Strunk (2006) use the fact that most ambiguous token or sentence boundaries happen around punctuation characters, such as periods/"
W16-2613,C92-4173,0,0.704184,"dependencies of tags into account – as opTokenization is usually the first step in a NLP system. Even systems that do not follow the classical NLP pipeline architecture still mostly operate on the basis of tokens, including unified architectures starting from scratch (Collobert et al., 2011). This is common, since tokens – either directly or indirectly – are usually considered to bear the information in a text eventually. However, the importance of tokenization is often neglected, as simple methods like whitespace segmentation can yield acceptable accuracies for many languages at first sight (Webster and Kit, 1992). But errors in an early phase of an NLP pipeline can have severe effects to higher level tasks and influence their performance by a large margin. Existing tokenizers can be organized into three categories: a) rule-based methods, b) supervised methods, c) unsupervised methods. Manning et al. (2014)1 , for example, internally use JFlex2 , which is a meta language for rules based on regular expressions and procedures to execute when a rule matches. In contrast, Jurish and Würzner (2013) present a supervised system for joint tokenization and sentence splitting, which employs a Hidden Markov Model"
W16-4011,P98-1013,0,0.0649142,"syntactic structures. This includes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation usi"
W16-4011,burchardt-etal-2006-salto,1,0.893566,"Missing"
W16-4011,N13-3004,0,0.131331,"ject in Section 4. While joint WSD and SRL annotations are conveniently supported and facilitated using constraints, they can also be performed separately and independently of one another. 2 Related Work We briefly review presently available annotation tools that could be used for annotating semantic structures. Since we aim to support geographically distributed annotation teams, we consider recent generic webbased annotation tools. Additionally, we examine tools from earlier semantic annotation projects that are specialised for SRL but not web-based. 2.1 Web-based Annotation Tools Anafora by Chen and Styler (2013) is a recent web-based annotation tool for event-like structures. Specifically, it supports the annotation of spans and n-ary relations. Spans are anchored on text while relations exist independently from the text and consist of slots that can be filled with spans. Annotations are visualised using a coloured text background. Selecting a relation highlights the participating spans by placing boxes around them. Anafora is not suited for annotation tasks that require an alignment of the semantic structures with syntactic structures such as constituent or dependency parse trees. brat by Stenetorp"
W16-4011,N10-2004,0,0.0183683,"s with FrameNet categories. Once a frame for a predicate has been selected, applicable roles from FrameNet can be assigned to nodes in the parse tree by drag-and-drop. SALTO supports discontinuous annotations, multi-token annotations, and cross-sentence annotations. It also offers basic team management functionalities including workload assignment and curation. However, annotators cannot correct mistakes in the underlying treebank because the parse tree is not editable. This is problematic for automatically preprocessed input. The final release of SALTO was in 2012. Jubilee and Cornerstone by Choi et al. (2010) are tools for annotating PropBank. Jubilee supports the annotation of PropBank instances, while its sister tool Cornerstone allows editing the frameset XML files that provide the annotation scheme to Jubilee. The user interface (UI) of Jubilee displays a treebank view and allows annotating nodes in the parse tree with frameset senses and roles. Jubilee supports annotation and adjudication of annotations in small teams. It is a Java application that stores all data on the file system. Thus, the annotation team needs to be able to access a common file system, which does not meet our needs for a"
W16-4011,W97-0802,0,0.114672,"since 2014. 2.3 Requirements of Semantic Annotation The annotation of semantic structures imposes two main requirements on annotation tools: 1) the flexibility to support multiple layers of annotation including syntactic and semantic layers using freely configurable annotation schemes and 2) the ability to handle large, interdependent tagsets. Flexible multi-layer annotation. While the usage-driven design of dedicated SRL annotation tools allows for a very efficient annotation, users face a serious lack of flexibility when trying to combine different annotation schemes (e.g. GermaNet senses (Hamp and Feldweg, 1997) and VerbNet roles), or when trying to use data preprocessed in different ways (e.g. for a crowdsourcing approach, automatically pre-annotating predicate and argument spans can be helpful, while experts may find pre-annotated dependency relations beneficial). This is not supported by current web-based annotation tools. Handling rich annotation schemes. Tools need to specifically support rich semantic annotation schemes—like FrameNet—with interdependent labels (i.e. sense labels determine available argument roles). Manually typing sense and role labels is error-prone, and selecting them from a"
W16-4011,L16-1484,1,0.824737,"Missing"
W16-4011,J05-1004,0,0.026006,"ludes, but is not limited to the tools mentioned in Section 2. WebAnno 3 was developed and implemented in close coordination with users in the context of an annotation project (cf. M´ujdricza-Maydt et al. (2016)) for word sense disambiguation (WSD) and SRL on German texts and driven by its practical requirements. SRL is the task of identifying semantic predicates, their arguments, and assigning roles to these arguments. It is a difficult task usually performed by experts. Examples of well-known SRL schemes motivated by different linguistic theories are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and VerbNet (Kipper Schuler, 2005). SRL annotation is typically based on syntactic structures obtained from treebanks, such as the constituent-based Penn Treebank (for PropBank annotation), or the German TIGER treebank for FrameNet-style annotation (Burchardt et al., 2009). An argument is typically identified by the span of its syntactic head or syntactic constituent. For some annotation schemes (e.g. FrameNet), the task also includes WSD. In this case, the sense label typically determines the available argument slots. The example below shows an annotation using FrameNet; the predicate ask r"
W16-4011,E12-2021,0,0.0315475,"Missing"
W16-4011,P13-4001,1,0.642633,"Missing"
W16-4011,P14-5016,1,0.655131,"Missing"
W16-4011,C98-1013,0,\N,Missing
W16-5301,J10-4006,0,0.0381184,"pared and combined in many ways, they lack interpretability, provenance and robustness. On the other hand, graph-based sparse models have a more straightforward interpretation, handle sense distinctions more naturally and can straightforwardly be linked to knowledge bases, while lacking the ability to compare arbitrary lexical units and a compositionality operation. Since both representations have their merits, I opt for exploring their combination in the outlook. 1 Introduction Rooted in Structural Linguistics (de Saussure, 1966; Harris, 1951), Distributional Semantic Models (DSMs, see e.g. (Baroni and Lenci, 2010)) characterize the meaning of lexical units by the contexts they appear in, cf. (Wittgenstein, 1963; Firth, 1957). Using the duality of form and contexts, forms can be compared along their contexts (Miller and Charles, 1991), giving rise to the field of Statistical Semantics. A data-driven, unsupervised approach to representing word meaning is attractive as there is no need for laborious creation of lexical resources. Further, these approaches naturally adapt to the domain or even language at hand. Desirable, in general, is a model that provides a firm basis for a wider range of (semantic) tas"
W16-5301,W06-3812,1,0.749977,"p, it is not the case that the information is compressed in a vector of fixed dimensionality, since ’dimensions’, if one wants to call them such, are different for each represented item. Of course, it would be possible to represent item-contexts or the distributional thesaurus in sparse matrices of very high dimensionality, but this view would not take the inherent sparseness into account and might obscure possible optimizations. Using the JoBimText DSM as a core, we extend this model in several ways. First, we perform word sense induction (WSI) on the ego-networks of lexical items in the DT (Biemann, 2006), utilising the property of many graph clustering algorithms that do not require the number of clusters as input (like e.g. k-Means). Further, we add taxonomic links (hypernyms) from Hearst-pattern-like extractions (Hearst, 1992). WSI allows us to disambiguate the model, which results in what we call a Proto-conceptualization (PCZ) (Faralli et al., 2016), see Table 1. The PCZ consists of entries that correspond to word senses, a list of similar senses, a list of hypernyms and a list of contexts that are salient for the sense. Note that it is straightforward to add these and other typed, weight"
W16-5301,J93-1003,0,0.340772,"e Representations Since I am proposing to de-bias the discussion on DSMs from the domination of vectors towards a more balanced view, I am exemplifying a graph-based DSM in this section. The JoBimText (Biemann and Riedl, 2013) framework is a scalable graph-based DSM implementation, developed in cooperation with IBM Research (Gliozzo et al., 2013). It is defined rather straightforwardly: lexical items j ∈ J are represented by their p most salient contexts B j , where saliency is measured by frequency or a statistical measure that prefers frequent co-occurrence, such as LMI (Evert, 2004) or LL (Dunning, 1993). Similarity of lexical items is defined as the overlap count of their respective contexts: sim( jk , jl ) = |(x|x ∈ B jk &x ∈ B jl )|. We call the graph of all lexical items with edges weighted by this similarity a distributional thesaurus (DT). Despite its simplicity, or maybe because of that, this DSM compares favourably to other DSMs (Riedl, 2016), including Lin’s thesaurus (Lin, 1998), Curran’s measure (Curran, 2004), and word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014a) on word similarity tasks, especially for large data. It was further successfully used for word expansion"
W16-5301,D08-1094,0,0.0488642,"Missing"
W16-5301,R15-1027,1,0.841323,"onj and, white-footed:JJ:amod, ... click:NN:-prep of, click:NN:-nn, .... play:VB:-dobj, electric:JJ:amod, .. computer:NN:nn, qwerty:JJ:amod ... Table 1: Examples of PCZ entries for “mouse:NN” and “keyboard:NN” based on dependency contexts (cf. (Erk and Pad´o, 2008)) from a newspaper corpus. Trailing numbers indicate sense identifiers. Similarity and context scores are not shown for brevity. feature for lexical substitution (Szarvas et al., 2013), for multiword identification (Riedl and Biemann, 2015), decompounding (Riedl and Biemann, 2016) and for resolving bridging mentions in co-reference (Feuerbach et al., 2015). The key to a scalable implementation is rooted in the pruning parameter p (typically p=1000), which has two functions: it reduces noise in the representations by only keeping the most salient contexts, and it limits the size of the representation, which is a list of key-value-pairs of fixed length (as opposed to a vector of fixed length). In other words: While there is a maximum size of the representation, as given by p, it is not the case that the information is compressed in a vector of fixed dimensionality, since ’dimensions’, if one wants to call them such, are different for each represe"
W16-5301,W13-5002,1,0.773248,"archers state, if asked why they did not use a sparse representation, is a technical one: many machine learning and statistical libraries do not natively operate on sparse representations, thus run out of memory when trying to represent all those zeros. 2 Graph-based Sparse Representations Since I am proposing to de-bias the discussion on DSMs from the domination of vectors towards a more balanced view, I am exemplifying a graph-based DSM in this section. The JoBimText (Biemann and Riedl, 2013) framework is a scalable graph-based DSM implementation, developed in cooperation with IBM Research (Gliozzo et al., 2013). It is defined rather straightforwardly: lexical items j ∈ J are represented by their p most salient contexts B j , where saliency is measured by frequency or a statistical measure that prefers frequent co-occurrence, such as LMI (Evert, 2004) or LL (Dunning, 1993). Similarity of lexical items is defined as the overlap count of their respective contexts: sim( jk , jl ) = |(x|x ∈ B jk &x ∈ B jl )|. We call the graph of all lexical items with edges weighted by this similarity a distributional thesaurus (DT). Despite its simplicity, or maybe because of that, this DSM compares favourably to other"
W16-5301,C92-2082,0,0.172284,"t item-contexts or the distributional thesaurus in sparse matrices of very high dimensionality, but this view would not take the inherent sparseness into account and might obscure possible optimizations. Using the JoBimText DSM as a core, we extend this model in several ways. First, we perform word sense induction (WSI) on the ego-networks of lexical items in the DT (Biemann, 2006), utilising the property of many graph clustering algorithms that do not require the number of clusters as input (like e.g. k-Means). Further, we add taxonomic links (hypernyms) from Hearst-pattern-like extractions (Hearst, 1992). WSI allows us to disambiguate the model, which results in what we call a Proto-conceptualization (PCZ) (Faralli et al., 2016), see Table 1. The PCZ consists of entries that correspond to word senses, a list of similar senses, a list of hypernyms and a list of contexts that are salient for the sense. Note that it is straightforward to add these and other typed, weighted relationships in a graph-based framework, cf. (Hovy, 2010). Furthermore, it is straightforward to link this kind of structure to existing structured resources, such as lexical-semantic networks and ontologies, see (Pavel and E"
W16-5301,P14-2050,0,0.153393,"e dimensionality typically ranges from 200 to 10’000. Such a representation is dense: there are virtually no zero entries in these vectors. A range of more recent models, such as Latent Dirichlet Allocation (LDA), are characterised in the same way – variants are distinguished by the notion of context (document vs. window-based vs. structured by grammatical dependencies) and the mechanism for dimensionality reduction. With the advent of neural embeddings such as word2vec (Mikolov et al., 2013), a series of works showed modest but significant advances in semantic tasks over previous approaches. Levy and Goldberg (2014b), however, showed that there is no substantial representational advance in neural embeddings, as they approximate matrix factorisation, as used in LSA. The advantage of word2vec is rather its efficient and scalable implementation that enables the processing of larger text collections. Improvements on task performance can mostly be attributed to better tuning of hyperparameters1 – which however overfits the DSM to a task at hand, and defies the premise of unsupervised systems of not needing (hyper)supervision. But there is a problem with all of these approaches: the fallacy of dimensionality2"
W16-5301,N15-1098,1,0.830375,"til we pull in a lot of unrelated words into our subspace we use for clustering. This might be a reason why in word sense induction, graph-based algorithms are very popular while there are only few approaches that determine the number of sense embeddings per item automatically (as e.g. (Neelakantan et al., 2014)). Word analogy and other arithmetics Word analogy tasks are a classic use-case for word embeddings, and there are further works, which learn vector operations that represent semantic relations. While many of these approaches in fact learn prototypical heads of the respective relation (Levy et al., 2015), word analogy and relational arithmetics are much less straightforward in GDSMs. Compositionality This is another task where the VDSM representation is more suited than GDSMs. While in general, a scalable computation in GDSMs allows to compute representation and similarities for frequent multi-word units (Riedl and Biemann, 2015), the computation of compositional vectors from single vectors in VDSMs is more attractive since it generalises to unseen combinations, even phrases and sentences (Bentivogli et al., 2016). Interpretability and Robustness of Representation The lack of interpretability"
W16-5301,P98-2127,0,0.0752261,"l items j ∈ J are represented by their p most salient contexts B j , where saliency is measured by frequency or a statistical measure that prefers frequent co-occurrence, such as LMI (Evert, 2004) or LL (Dunning, 1993). Similarity of lexical items is defined as the overlap count of their respective contexts: sim( jk , jl ) = |(x|x ∈ B jk &x ∈ B jl )|. We call the graph of all lexical items with edges weighted by this similarity a distributional thesaurus (DT). Despite its simplicity, or maybe because of that, this DSM compares favourably to other DSMs (Riedl, 2016), including Lin’s thesaurus (Lin, 1998), Curran’s measure (Curran, 2004), and word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014a) on word similarity tasks, especially for large data. It was further successfully used for word expansion in word sense disambiguation (Miller et al., 2012), as a 1 ”If you want to get good results, you should tune your hyperparameters. And if you want to make good science, don’t forget to tune your baselines’ hyperparameters too!” - Omer Levy, pers. communication 2 not to be confused with the curse of dimensionality, which refers to adverse phenomena when representing problems in too high-di"
W16-5301,C12-1109,1,0.91523,"s defined as the overlap count of their respective contexts: sim( jk , jl ) = |(x|x ∈ B jk &x ∈ B jl )|. We call the graph of all lexical items with edges weighted by this similarity a distributional thesaurus (DT). Despite its simplicity, or maybe because of that, this DSM compares favourably to other DSMs (Riedl, 2016), including Lin’s thesaurus (Lin, 1998), Curran’s measure (Curran, 2004), and word embeddings (Mikolov et al., 2013; Levy and Goldberg, 2014a) on word similarity tasks, especially for large data. It was further successfully used for word expansion in word sense disambiguation (Miller et al., 2012), as a 1 ”If you want to get good results, you should tune your hyperparameters. And if you want to make good science, don’t forget to tune your baselines’ hyperparameters too!” - Omer Levy, pers. communication 2 not to be confused with the curse of dimensionality, which refers to adverse phenomena when representing problems in too high-dimensional spaces 2 entry mouse:NN:0 mouse:NN:1 keyboard:NN:0 keyboard:NN:1 similar terms rat:NN:0, rodent:NN:0, monkey:NN:0, ... keyboard:NN:1, computer:NN:0, printer:NN:0 ... piano:NN:1, synthesizer:NN:2, organ:NN:0 ... keypad:NN:0, mouse:NN:1, screen:NN:1 ."
W16-5301,C08-1076,0,0.0139423,"rovements on task performance can mostly be attributed to better tuning of hyperparameters1 – which however overfits the DSM to a task at hand, and defies the premise of unsupervised systems of not needing (hyper)supervision. But there is a problem with all of these approaches: the fallacy of dimensionality2 , following from a simplification that we should not apply without being aware of its consequences: there is no ’appropriate number’ of dimensions in natural language, because natural language follows a scale-free distribution on all levels (e.g. (Zipf, 1949; Steyvers and Tenenbaum, 2005; Mukherjee et al., 2008), inter al.). Thus, a representation with a fixed number of dimensions introduces a granularity – ’major’ dimensions encode the most important distinctions while ’minor’ distinctions in the data cannot be modelled if the granularity is too coarse. This is why the recommended number of dimensions depends on the task, the dataset’s size and even the domain. In principle, there are two conclusions from studies that vary the number of dimensions to optimise some sort of a score: (a) in one type of study, there is a sweet spot in the number of dimensions, typically between 50 and 2000. This means t"
W16-5301,D14-1113,0,0.0309262,"ctor for ”bank” is surrounded by other money-bank-terms (such as names of banks). The larger the underlying corpus, the higher is the amount of these terms, most of them rare (see e.g. (Pelevina et al., 2016)). We either do not find river-bank terms in the neighbourhood or we have to extend the neighbourhood until we pull in a lot of unrelated words into our subspace we use for clustering. This might be a reason why in word sense induction, graph-based algorithms are very popular while there are only few approaches that determine the number of sense embeddings per item automatically (as e.g. (Neelakantan et al., 2014)). Word analogy and other arithmetics Word analogy tasks are a classic use-case for word embeddings, and there are further works, which learn vector operations that represent semantic relations. While many of these approaches in fact learn prototypical heads of the respective relation (Levy et al., 2015), word analogy and relational arithmetics are much less straightforward in GDSMs. Compositionality This is another task where the VDSM representation is more suited than GDSMs. While in general, a scalable computation in GDSMs allows to compute representation and similarities for frequent multi"
W16-5301,W16-1620,1,0.839542,"lowing situation when trying to induce word senses: Suppose we hypothesise for a lexical item like ”bank” that it has more than one sense and we want to cluster the neighbourhood to get two sense representations. As for most ambiguous words, the sense distribution is biased: in our hypothetical collection, the monetary sense of bank is much better represented than the river bank sense. In this situation, the vector for ”bank” is surrounded by other money-bank-terms (such as names of banks). The larger the underlying corpus, the higher is the amount of these terms, most of them rare (see e.g. (Pelevina et al., 2016)). We either do not find river-bank terms in the neighbourhood or we have to extend the neighbourhood until we pull in a lot of unrelated words into our subspace we use for clustering. This might be a reason why in word sense induction, graph-based algorithms are very popular while there are only few approaches that determine the number of sense embeddings per item automatically (as e.g. (Neelakantan et al., 2014)). Word analogy and other arithmetics Word analogy tasks are a classic use-case for word embeddings, and there are further works, which learn vector operations that represent semantic"
W16-5301,D15-1290,1,0.896606,". device:NN:1, equipment:NN:3, ... instrument:NN:2, device:NN:3, ... device:NN:1, technology:NN:0 ... context rat::NN:conj and, white-footed:JJ:amod, ... click:NN:-prep of, click:NN:-nn, .... play:VB:-dobj, electric:JJ:amod, .. computer:NN:nn, qwerty:JJ:amod ... Table 1: Examples of PCZ entries for “mouse:NN” and “keyboard:NN” based on dependency contexts (cf. (Erk and Pad´o, 2008)) from a newspaper corpus. Trailing numbers indicate sense identifiers. Similarity and context scores are not shown for brevity. feature for lexical substitution (Szarvas et al., 2013), for multiword identification (Riedl and Biemann, 2015), decompounding (Riedl and Biemann, 2016) and for resolving bridging mentions in co-reference (Feuerbach et al., 2015). The key to a scalable implementation is rooted in the pruning parameter p (typically p=1000), which has two functions: it reduces noise in the representations by only keeping the most salient contexts, and it limits the size of the representation, which is a list of key-value-pairs of fixed length (as opposed to a vector of fixed length). In other words: While there is a maximum size of the representation, as given by p, it is not the case that the information is compressed i"
W16-5301,N16-1075,1,0.823261,"ment:NN:2, device:NN:3, ... device:NN:1, technology:NN:0 ... context rat::NN:conj and, white-footed:JJ:amod, ... click:NN:-prep of, click:NN:-nn, .... play:VB:-dobj, electric:JJ:amod, .. computer:NN:nn, qwerty:JJ:amod ... Table 1: Examples of PCZ entries for “mouse:NN” and “keyboard:NN” based on dependency contexts (cf. (Erk and Pad´o, 2008)) from a newspaper corpus. Trailing numbers indicate sense identifiers. Similarity and context scores are not shown for brevity. feature for lexical substitution (Szarvas et al., 2013), for multiword identification (Riedl and Biemann, 2015), decompounding (Riedl and Biemann, 2016) and for resolving bridging mentions in co-reference (Feuerbach et al., 2015). The key to a scalable implementation is rooted in the pruning parameter p (typically p=1000), which has two functions: it reduces noise in the representations by only keeping the most salient contexts, and it limits the size of the representation, which is a list of key-value-pairs of fixed length (as opposed to a vector of fixed length). In other words: While there is a maximum size of the representation, as given by p, it is not the case that the information is compressed in a vector of fixed dimensionality, since"
W16-5301,P16-1214,0,0.0284608,"ctic Dependencies model on www.jobimtext.org/jobimviz-web-demo/ 3 relatedness e.g. between rooster:voyage and asylum:fruit (from RG65; (Rubenstein and Goodenough, 1965)). On the other hand, the ability of the GDSM only return a set of minimally similar items has been experimentally shown to be advantageous when using DSM similarity for lexical expansion (cf. (Miller et al., 2012)). Similarity Computation and Semantic Neighbourhood Similarity computation in the metric space of the VDSM is computationally expensive and needs engineering solutions like K-d-trees (Bentley, 1990) or approximation (Sugawara et al., 2016) to make it feasible to return the top-n-similar list of items, which is a frequently used function in statistical semantics. In GDSMs, on the other hand, similarity is directly read off the representation. Pre-computation of all similarities in VDSMs is possible, but does not scale well, cf. (Panchenko et al., 2016). Word Sense Representations Another consequence of the metric space is that neighbourhoods of lexical items are populated with similar lexical items across all frequency bands. This leads to the following situation when trying to induce word senses: Suppose we hypothesise for a le"
W16-5301,N13-1133,1,0.858111,"reen:NN:1 ... hypernyms animal:NN:0, species:NN:1, ... device:NN:1, equipment:NN:3, ... instrument:NN:2, device:NN:3, ... device:NN:1, technology:NN:0 ... context rat::NN:conj and, white-footed:JJ:amod, ... click:NN:-prep of, click:NN:-nn, .... play:VB:-dobj, electric:JJ:amod, .. computer:NN:nn, qwerty:JJ:amod ... Table 1: Examples of PCZ entries for “mouse:NN” and “keyboard:NN” based on dependency contexts (cf. (Erk and Pad´o, 2008)) from a newspaper corpus. Trailing numbers indicate sense identifiers. Similarity and context scores are not shown for brevity. feature for lexical substitution (Szarvas et al., 2013), for multiword identification (Riedl and Biemann, 2015), decompounding (Riedl and Biemann, 2016) and for resolving bridging mentions in co-reference (Feuerbach et al., 2015). The key to a scalable implementation is rooted in the pruning parameter p (typically p=1000), which has two functions: it reduces noise in the representations by only keeping the most salient contexts, and it limits the size of the representation, which is a list of key-value-pairs of fixed length (as opposed to a vector of fixed length). In other words: While there is a maximum size of the representation, as given by p,"
W16-5301,C98-2122,0,\N,Missing
W16-5308,C12-1017,1,0.852364,"omic relations, such as co-hyponyms. The second-best resource in this evaluation is the word co-occurrence network, which outperforms WN on all metrics except the P@100 of MinRank scores. We also analyzed the differences qualitatively and looked at cue-target-pairs where the three networks perform very differently. As our findings show, different networks have different potentials with respect to the retrieval of ToT targets based on a given cue: • WordNet good, Co-occurrence poor: Synonyms or near-synonyms, like javelin – spear, cadaver – corpse. These do not co-occur in sentences, also cf. (Biemann et al., 2012). • WordNet poor, Co-occurrence good: associations, like hospital–doctor or hospital–sick. They are not encoded in WordNet, its associative relations are very spotty. Note that placing them first in the order of relations did not increase performance. • WordNet good, Similarity poor: meronyms/holonyms, such as door–knob, road–asphalt. These are not similar at all from a distributional point of view. • WordNet poor, Similarity good: relations that should be in WN, but for some reason are missing, e.g. torpedo–missile, calligraphy–art, gazebo–pavilion. • Co-occurrence good, Similarity poor: asso"
W16-5308,J93-1003,0,0.0594457,"tistically significant sentence-based word co-occurrences using the same corpus as here above, and following the methodology of (Quasthoff et al., 2006)10. We expect this resource to be suited for free associations, i.e. cue words whose link to the target cannot be specified. This resource has by far the highest rate of relations across different word classes, as they may occur in patterns like “With Xs, especially with Y ones, you can Z and W” (e.g. “with mochas, especially with iced ones, you can chill and have cookies”). Co-occurrences are ranked by the log-likelihood significance measure (Dunning, 1993). 6.2 Network Access Given the structural differences of our resources, our networks are accessed with different query strategies. The general setup is to query the resource via a cue and to insert then the retrieved terms into a ranking. As long as the system has not found all the desired words, it will keep going by querying with words according to their rank, inserting previously un-retrieved terms below the ranking. • WordNet: Having noticed that people tend to use hypernyms (flower) as cues to find the hyponym (rose, the target), we defined a heuristic supporting queries using this relati"
W16-5308,dutoit-nugues-2002-algorithm,0,0.0619055,"and various network-based lexical resources: WordNet, henceforth WN (Miller,1990), Framenet (Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by introducing a distance-based approach to compute word similarity. Given a small set of words they compare their approach with Onelook and with dense-vector similarity. While we adopt part of their methodology in our evaluation scheme, we are more reserved with respect to their architecture. Since it requires a fully computed similarity matrix for the entire vocabulary, their work cannot scale up: it is unreasonable to assume that the lexicon is stored in a fully connected"
W16-5308,quasthoff-etal-2006-corpus,0,0.0344205,"newswire corpus of 100 million sentences in English. We expect this resource to be suitable for most associative queries, that is to help us find words occurring in contexts like “X is somehow like a Y or a Z” (e.g. “a panda is somehow like a koala or a grizzly”). This example illustrates ‘co-hyponymy’, a relation not directly encoded in WordNet. Similarities (for example, panda/koala vs. panda/dog) are ranked by context overlap. • Word Co-occurrence: We compute statistically significant sentence-based word co-occurrences using the same corpus as here above, and following the methodology of (Quasthoff et al., 2006)10. We expect this resource to be suited for free associations, i.e. cue words whose link to the target cannot be specified. This resource has by far the highest rate of relations across different word classes, as they may occur in patterns like “With Xs, especially with Y ones, you can Z and W” (e.g. “with mochas, especially with iced ones, you can chill and have cookies”). Co-occurrences are ranked by the log-likelihood significance measure (Dunning, 1993). 6.2 Network Access Given the structural differences of our resources, our networks are accessed with different query strategies. The gen"
W16-5308,P98-2180,0,0.201234,"eir task (our goal). Lexicographers bridge this gap. Unfortunately, until recently most of their tools have been built for the language receiver. Nevertheless, nowadays there are also some tools for the language producer. For example, Roget’s thesaurus (Roget, 1852) or its modern incarnation built with the help of corpus linguistics (Dornseiff, 2003). There are also the Language Activator (Summers, 1993), the Oxford Learner’s Wordfinder Dictionary (Trappes-Lomax, 1997), and various network-based lexical resources: WordNet, henceforth WN (Miller,1990), Framenet (Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by in"
W16-5308,C16-1263,0,0.0131414,"(Fillmore et al. 2003); MindNet (Richardson et al., 1998), and HowNet (Dong & Dong, 2006;). Finally, there are collocation dictionaries (Benson et al., 2010), and web-based tools like Lexical FreeNet3 or Onelook (Beeferman, 2003), which, like BabelNet (Navigli & Ponzetto, 2012) combines a dictionary (WN) and an encyclopedia (Wikipedia), though putting the emphasis on onomasiological search, access by meaning. Reverse dictionaries have been built by hand (Bernstein, 1975) and with the help of machines (Dutoit and Nugues, 2002). In both cases, one draws on the words occurring in the definition. Thorat and Choudhari (2016) try to extend this idea by introducing a distance-based approach to compute word similarity. Given a small set of words they compare their approach with Onelook and with dense-vector similarity. While we adopt part of their methodology in our evaluation scheme, we are more reserved with respect to their architecture. Since it requires a fully computed similarity matrix for the entire vocabulary, their work cannot scale up: it is unreasonable to assume that the lexicon is stored in a fully connected similarity matrix, which grows quadratically in the size of the vocabulary. Note that while den"
W16-5308,W93-0310,0,0.840815,"n particular, association thesauri, they are too small to allow us to solve the ToT-problem. Projected resource would still have to be built, and while one could imagine the use of combined resources, like Babelnet (Navigli and Ponzetto, 2012), or the combination of WN with other resources like topic maps (Agirre et al. 2001), Roget’s Thesaurus (Mandala, 1999) or ConceptNet (Liu and Sing, 2004), it is not easy to tell which combination is best, all the more as besides encyclopedic knowledge, we also need episodic knowledge (Tulving, 1983). One straightforward solution might be co-occurrences (Wettler & Rapp, 1993; Lemaire & Denhière, 2004; Schulte im Walde & Melinger, 2008). While co-occurring words contain many appropriate clue – target pairs, they also contain many unrelated terms that hamper access – even after application of appropriate significance measures. More severely, there are no structural elements that generalize across queries. Another solution could be lexical functions (Mel'čuk, 1996) or semagrams (Moerdijk, 2008) which are reminiscent of the lexical-semantic networks produced by Fontenelle (1997) on the basis of the Collins-Robert dictionary enriched with Melcuk's lexical functions. S"
W16-5308,C98-2175,0,\N,Missing
W17-0202,P14-2133,0,0.0134169,"hich are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (2014) address the lexical gap by generalizing over OOV and other words in a feature role via feature embeddings. Another approach for replacing OOV words by known ones using word embeddings is introduced by Andreas and Klein (2014). All these approaches, however, require retraining the parser with these additional features and make the model more complex. We present a much simpler setup of replacing OOV words with similar words from the training set, which allows retrofitting any parser with our method. This work is related to Biemann and Riedl (2013), where OOV performance of fine-grained POS tagging has been improved in a similar fashion. Another similar work to ours is proposed by Huang et al. (2014), who replace OOV named entities with named entities from the same (finegrained) class for improving Chinese dependency"
W17-0202,P14-2131,0,0.0249007,"oceedings of the 21st Nordic Conference of Computational Linguistics, pages 11–19, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press The influence of all components is evaluated separately for POS tagging and dependency parsing in Section 5. dependency parsing in particular, see e.g. Wang et al. (2005). In order to transfer lexical knowledge from the training data to unseen words in the test data, Koo et al. (2008) improve dependency parsing with features based on Brown Clusters (Brown et al., 1992), which are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (2014) address the lexical gap by generalizing over OOV and other words in a feature role via feature embeddings. Another approach for replacing OOV words by known ones using word embeddings is introduced by Andreas and Klein (2014). All these approaches, however, require retraining the p"
W17-0202,J92-4003,0,0.0544434,"982) inter al.) that lexical information helps for parsing in general and for 11 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 11–19, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press The influence of all components is evaluated separately for POS tagging and dependency parsing in Section 5. dependency parsing in particular, see e.g. Wang et al. (2005). In order to transfer lexical knowledge from the training data to unseen words in the test data, Koo et al. (2008) improve dependency parsing with features based on Brown Clusters (Brown et al., 1992), which are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (2014) address the lexical gap by generalizing over OOV and other words in a feature role via feature embeddings. Another approach for replacing OOV words by known ones using word embeddings is introduced"
W17-0202,C10-2037,0,0.021499,"POS tagging has been improved in a similar fashion. Another similar work to ours is proposed by Huang et al. (2014), who replace OOV named entities with named entities from the same (finegrained) class for improving Chinese dependency parsing, which largely depends on the quality of the employed NER tagger and is restricted to named entities only. In contrast, we operate on all OOV words, and try to improve prediction on coarse universal POS classes and universal dependencies. On a related note, examples for a successful application of OOV replacements is demonstrated for Machine Translation (Gangadharaiah et al., 2010; Zhang et al., 2012). 3 3.1 Semantic Similarities In order to replace an OOV word by a similar invocabulary word, we use models that are based on the distributional hypothesis (Harris, 1951). For showing the impact of different models we use a graph-based approach that uses the left- and rightneighbored word as context, represented by the method proposed by Biemann and Riedl (2013), and is called distributional thesaurus (DT). Furthermore, we apply two dense numeric vectorspace approaches, using the skip-gram model (SKG) and CBOW model of the word2vec implementation of Mikolov et al. (2013)."
W17-0202,P13-1045,0,0.0472274,"rsing in Section 5. dependency parsing in particular, see e.g. Wang et al. (2005). In order to transfer lexical knowledge from the training data to unseen words in the test data, Koo et al. (2008) improve dependency parsing with features based on Brown Clusters (Brown et al., 1992), which are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (2014) address the lexical gap by generalizing over OOV and other words in a feature role via feature embeddings. Another approach for replacing OOV words by known ones using word embeddings is introduced by Andreas and Klein (2014). All these approaches, however, require retraining the parser with these additional features and make the model more complex. We present a much simpler setup of replacing OOV words with similar words from the training set, which allows retrofitting any parser with our method. This work is related to Biemann a"
W17-0202,W05-1516,0,0.0153891,"ble amount of work to address the OOV problem with continuous 2 Related Work While part-of-speech (POS) tags play a major role in detecting syntactic structure, it is well known (Kaplan and Bresnan (1982) inter al.) that lexical information helps for parsing in general and for 11 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 11–19, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press The influence of all components is evaluated separately for POS tagging and dependency parsing in Section 5. dependency parsing in particular, see e.g. Wang et al. (2005). In order to transfer lexical knowledge from the training data to unseen words in the test data, Koo et al. (2008) improve dependency parsing with features based on Brown Clusters (Brown et al., 1992), which are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (201"
W17-0202,huang-etal-2014-sentence,0,0.0291164,"feature embeddings. Another approach for replacing OOV words by known ones using word embeddings is introduced by Andreas and Klein (2014). All these approaches, however, require retraining the parser with these additional features and make the model more complex. We present a much simpler setup of replacing OOV words with similar words from the training set, which allows retrofitting any parser with our method. This work is related to Biemann and Riedl (2013), where OOV performance of fine-grained POS tagging has been improved in a similar fashion. Another similar work to ours is proposed by Huang et al. (2014), who replace OOV named entities with named entities from the same (finegrained) class for improving Chinese dependency parsing, which largely depends on the quality of the employed NER tagger and is restricted to named entities only. In contrast, we operate on all OOV words, and try to improve prediction on coarse universal POS classes and universal dependencies. On a related note, examples for a successful application of OOV replacements is demonstrated for Machine Translation (Gangadharaiah et al., 2010; Zhang et al., 2012). 3 3.1 Semantic Similarities In order to replace an OOV word by a s"
W17-0202,P08-1068,0,0.0540754,"major role in detecting syntactic structure, it is well known (Kaplan and Bresnan (1982) inter al.) that lexical information helps for parsing in general and for 11 Proceedings of the 21st Nordic Conference of Computational Linguistics, pages 11–19, c Gothenburg, Sweden, 23-24 May 2017. 2017 Link¨oping University Electronic Press The influence of all components is evaluated separately for POS tagging and dependency parsing in Section 5. dependency parsing in particular, see e.g. Wang et al. (2005). In order to transfer lexical knowledge from the training data to unseen words in the test data, Koo et al. (2008) improve dependency parsing with features based on Brown Clusters (Brown et al., 1992), which are known to be drawing syntacticsemantic distinctions. Bansal et al. (2014) show slight improvements over Koo et al. (2008)’s method by tailoring word embeddings for dependency parsing by inducing them on syntactic contexts, which presupposes the existence of a dependency parser. In more principled fashion, Socher et al. (2013) directly operate on vector representations. Chen et al. (2014) address the lexical gap by generalizing over OOV and other words in a feature role via feature embeddings. Anoth"
W17-0202,P09-1040,0,0.0250169,"mprovements (p &lt; 0.05) over the baseline without OOV replacement are marked with an asterisk (∗ ), significant performance drops with a hashmark (#) and the best result per experiment is marked in bold. 5.1 Results for POS Tagging In Table 2 we show overall and OOV-only POS tagging accuracies on the respective test set for seven languages using similarities extracted from the DT. Dependency Parser and POS Tagger LANG For the dependency parsing we use the implementation of the graph-based dependency parser provided in Mate-tools (Bohnet, 2010, version 3.6) and the transition-based Malt parser (Nivre, 2009, version 1.8.1). Graph-based parsers use global inference to construct the maximum spanning dependency tree for the input sequences. Contrary, the greedy algorithm in the transition-based parser uses local inference to predict the dependency tree. The parsing models for both parsers, Matetools and Malt parser, are optimized using crossvalidation on the training section of the treebank6 . We train the dependency parsers using POS tags (from the Mate-tools tagger) predicted using a 5fold cross-validation. The evaluation of the parser accuracies is carried out using MaltEval. We report labeled a"
W17-0202,C14-1078,0,\N,Missing
W17-0202,L16-1262,0,\N,Missing
W17-0213,S13-2049,0,0.0252126,"h newspaper sentences3 , using the JoBimText (Biemann and Riedl, 2013) implementation. In the case of co-occurrences, for a given word v we use a frequency-weighted version of pointwise mutual information called lexicographer’s mutual information (LMI) (Kilgarriff et al., 2004; Evert, 2004) to rank all the terms co-occurring with v in a sentence and to select those that will appear in its ego word graph. Edge weights are defined by LMI and the possible edge between two nodes u and w will be determined by the presence of u in the distribu1 As was also the case for task 13 of SemEval 2013, cf. (Jurgens and Klapaftis, 2013) 2 See for example the results at task 14 of SemEval 2010 (Manandhar et al., 2010), where adjusted mutual information was introduced to correct the bias: https://www.cs.york. ac.uk/semeval2010_WSI/task_14_ranking.html. 3A combination of the Leipzig Corpora Collection (LCC), http://corpora.uni-leipzig.de (Richter et al., 2006) and the Gigaword corpus (Parker et al., 2011). 106 tional thesaurus of w, or viceversa. The process is similar in the case of SSIMs, but here LMI is computed on term-context cooccurrences based on syntactic dependencies extracted from the corpus by means of the Stanford P"
W17-0213,W06-3812,1,0.754635,"holds. In this case one word is totally dominant over the other, and the pseudoword actually collapses onto one sense. As already mentioned, we decided to exclude collapsed pseudowords from evaluation. To compute the BCubed F-score and NMI, we compare the ground truth clustering T = {α, β} to the clustering C (γ ∪ δ) that we obtain from any 4 The Algorithms In our experimental setting we will compare four graph-based clustering algorithms commonly applied in, or especially developed for, the task of WSI . They are: the Markov cluster algorithm (MCL) (van Dongen, 2000); Chinese Whispers (CW) (Biemann, 2006); MaxMax (MM) (Hope and Keller, 2013); and the gangplank clustering algorithm (GP) (Cecchini and Fersini, 2015). They are detailed in the following subsections. We remark that none of these algorithms sets a predefined number of clusters to be found. This is a critical property of WSI algorithms, since it is not known a priori whether a word is ambiguous in the underlying data collection and how many senses it might have. 7 NMI is equivalent to V-measure, as shown by Remus and Biemann (2013). 108 4.1 Markov Cluster Algorithm 2013), each of which is represented by its root. Clusters can overlap"
W17-0213,E06-1018,0,0.0350313,"udowords. As homonymy is more clear-cut than generic polysemy, we deem that the efficacy of a WSI algorithm should be first measured in this case before being tested in a more fine-grained and ambiguous situation. Also, the task we defined does not depend on the arbitrary granularity of an external lexical resource1 , which might be too finegrained for our purpose. Further, the sense distinctions e.g. in WordNet might not be mirrored in the corpus, and conversely, some unforeseen senses might be observed. Instead, our work can be seen as an expansion of the pseudoword evaluation presented in (Bordag, 2006), albeit more focused in its goal and implementation. In our opinion, current WSI tasks present some shortcomings. A fundamental problem is the vagueness regarding the granularity (fine or coarse) of the senses that have to be determined. As a consequence, the definition of an adequate evaluation measure becomes difficult, as many of them have been showed to be biased towards few or many clusters2 . Further, small data sets often do not allow obtaining significant results. Pseudoword evaluation, on the contrary, presents an objective and self-contained framework where the classification task i"
W17-0213,J14-3007,0,0.0180731,"re always 0, as this measure heavily penalizes the asymmetry of having just one cluster in the output and two clusters in the ground truth. This, together with the fact that MaxMax, which is the most fine-grained among our examined algorithms, reaches NMI values that are on par with the other systems (or consistently better, in the case of co-occurrences) while regularly obtaining the lowest BC - F scores, leads us to claim that NMI is biased towards finegrained clusterings8 . On the opposite side of the spectrum, the more coarse-grained systems tend 8 This bias is discussed more at length by Li et al. (2014). 110 1 MCL CW MM GP BSL 2 3 4 5 BC - F NMI TOP 2 BC - F NMI TOP 2 BC - F NMI TOP 2 BC - F NMI TOP 2 BC - F NMI TOP 2 92.6 94.7 37.0 70.6 72.5 71.9 79.6 45.5 57.5 0.0 79.7 85.6 50.8 76.4 35.5 89.7 94.2 26.8 62.3 75.8 88.2 92.4 22.7 60.4 73.8 67.6 78.4 39.7 46.4 0.0 63.5 74.5 38.9 47.3 0.0 81.1 87.0 46.4 71.7 38.2 80.4 86.8 40.8 72.7 37.2 87.4 91.8 17.6 50.8 81.5 85.4 87.5 16.3 55.5 76.4 83.5 84.3 12.7 53.2 76.6 64.0 73.0 32.5 34.9 0.0 58.3 59.6 33.6 40.4 0.0 49.9 43.4 31.0 36.1 0.0 83.4 86.8 40.2 62.4 41.6 79.0 78.8 34.8 67.0 38.0 75.0 69.9 28.4 65.8 38.1 99.0 98.9 17.0 49.8 98.4 98.2 98.2 16."
W17-0213,S10-1011,0,0.0187974,"In the case of co-occurrences, for a given word v we use a frequency-weighted version of pointwise mutual information called lexicographer’s mutual information (LMI) (Kilgarriff et al., 2004; Evert, 2004) to rank all the terms co-occurring with v in a sentence and to select those that will appear in its ego word graph. Edge weights are defined by LMI and the possible edge between two nodes u and w will be determined by the presence of u in the distribu1 As was also the case for task 13 of SemEval 2013, cf. (Jurgens and Klapaftis, 2013) 2 See for example the results at task 14 of SemEval 2010 (Manandhar et al., 2010), where adjusted mutual information was introduced to correct the bias: https://www.cs.york. ac.uk/semeval2010_WSI/task_14_ranking.html. 3A combination of the Leipzig Corpora Collection (LCC), http://corpora.uni-leipzig.de (Richter et al., 2006) and the Gigaword corpus (Parker et al., 2011). 106 tional thesaurus of w, or viceversa. The process is similar in the case of SSIMs, but here LMI is computed on term-context cooccurrences based on syntactic dependencies extracted from the corpus by means of the Stanford Parser (De Marneffe et al., 2006). In both cases, the word v itself is removed from"
W17-0213,de-marneffe-etal-2006-generating,0,0.0671295,"Missing"
W17-0213,N03-2023,0,0.0626706,"analysis of their behaviours, we also define a new specific evaluation measure. As far as we know, this is the first large-scale systematic pseudoword evaluation dedicated to the induction of coarsegrained homonymous word senses. 1 Pseudowords were first proposed by Gale et al. (1992) and Sch¨utze (1992) as a way to create artificial ambiguous words by merging two (or more) random words. A pseudoword simulates homonymy, i.e. a word which possesses two (or more) semantically and etymologically unrelated senses, such as count as “nobleman” as opposed to “the action of enumerating”. The study of Nakov and Hearst (2003) shows that the performances of WSI algorithms on random pseudowords might represent an optimistic upper bound with respect to true polysemous words, as generic polysemy implies some kind of correlation between the categories and the distributions of the different senses of a word, which is absent from randomly generated ones. We are aware of the approaches proposed in (Otrusina and Smrˇz, 2010) and (Pilehvar and Navigli, 2013), used e.g. in (Bas¸kaya Introduction and Related Work Word Sense Induction (WSI) is the branch of Natural Language Processing (NLP) concerned with the unsupervised dete"
W17-0213,S07-1006,0,0.098679,"Missing"
W17-0213,C02-1114,0,0.445088,"semeval2010_WSI/task_14_ranking.html. 3A combination of the Leipzig Corpora Collection (LCC), http://corpora.uni-leipzig.de (Richter et al., 2006) and the Gigaword corpus (Parker et al., 2011). 106 tional thesaurus of w, or viceversa. The process is similar in the case of SSIMs, but here LMI is computed on term-context cooccurrences based on syntactic dependencies extracted from the corpus by means of the Stanford Parser (De Marneffe et al., 2006). In both cases, the word v itself is removed from G, since we are interested just in the relations between the words more similar to it, following (Widdows and Dorow, 2002). The clusters in which the node set of G will be subdivided will represent the possible senses of v. We remark that co-occurrences are first-order relations (i.e. inferred directly by data), whereas SSIMs are of second order, as they are computed on the base of cooccurrences4 . For this reason, two different kinds of distributional thesauri might have quite different entries even if they pertain to the same word. Further, the ensuing word graphs will show a complementary correlation: co-occurrences represent syntagmatic relations with the central word, while SSIM s paradigmatic ones5 , and th"
W17-0213,otrusina-smrz-2010-new,0,0.0635136,"Missing"
W17-0213,N13-1130,0,0.0324103,"Missing"
W17-0213,N13-1119,1,0.802878,"ed for, the task of WSI . They are: the Markov cluster algorithm (MCL) (van Dongen, 2000); Chinese Whispers (CW) (Biemann, 2006); MaxMax (MM) (Hope and Keller, 2013); and the gangplank clustering algorithm (GP) (Cecchini and Fersini, 2015). They are detailed in the following subsections. We remark that none of these algorithms sets a predefined number of clusters to be found. This is a critical property of WSI algorithms, since it is not known a priori whether a word is ambiguous in the underlying data collection and how many senses it might have. 7 NMI is equivalent to V-measure, as shown by Remus and Biemann (2013). 108 4.1 Markov Cluster Algorithm 2013), each of which is represented by its root. Clusters can overlap because a node could be the descendant of two roots at the same time. The algorithm’s complexity is linear in the number of the edges and its results are uniquely determined. The Markov cluster algorithm (van Dongen, 2000) uses the concept of random walk on a graph, or Markov chain: the more densely intra-connected a region in the graph, the higher the probability to remain inside it starting from one of its nodes and moving randomly to another one. The strategy of the algorithm is then to"
W17-0213,J98-1004,0,0.742661,"Missing"
W17-1909,E03-1020,0,0.0844048,"hese hybrid methods, our approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pag"
W17-1909,W06-3812,1,0.424857,"approach does not require access to web search engines, texts mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Val"
W17-1909,N15-1184,0,0.0163787,"based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using t"
W17-1909,N15-1059,0,0.154874,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N13-1092,0,0.0846982,"Missing"
W17-1909,P15-1072,0,0.0758306,"D) task. In particular, the contribution of this paper is a new unsupervised knowledge-based approach to WSD based on the hybrid aligned resource (HAR) introduced by Faralli et al. (2016). The key difference of our approach from prior hybrid methods based on sense embeddings, e.g. (Rothe and Sch¨utze, 2015), is that we rely on sparse lexical representations that make the sense representation readable and allow to straightforwardly use this representation for word sense disambiguation, as will be shown below. In contrast to hybrid approaches based on sparse interpretable representations, e.g. (Camacho-Collados et al., 2015a), our method requires no mapping of texts to a sense inventory and thus can be applied to larger text collections. By linking symbolic distributional sense representations to lexical resources, we are able to improve representations of senses, leading to performance gains in word sense disambiguation. We introduce a new method for unsupervised knowledge-based word sense disambiguation (WSD) based on a resource that links two types of sense-aware lexical networks: one is induced from a corpus using distributional semantics, the other is manually constructed. The combination of two networks re"
W17-1909,N15-1165,0,0.0236298,"l, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain related terms of several senses. The Chinese Whispers (Biemann, 2006) graph clustering is applied to the ego-network (Everett and Borgatti, 2005) of the each term, as defined by its related terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:"
W17-1909,P15-2003,0,0.0258444,"pproach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds by performing a more conventional context clustering, similar what is found to unsupervised methods such as (Neelakantan et al., 2014; Bartunov et al., 2016). Building a Distributional Thesaurus (DT). At this stage, a similarity graph over terms is induced from a corpus, where each entry consists of the most similar 200 terms for a given term using the JoBimText method (Biemann and Riedl, 2013). Word Sense Induction. In DTs, entries of polysemous terms are mixed, i.e. they contain relat"
W17-1909,C92-2082,0,0.455908,"lated terms and connections between then observed in the DT to derive word sense clusters. Rothe and Sch¨utze (2015) proposed a method that learns sense embedding using word embeddings and the sense inventory of WordNet. The approach was evaluated on the WSD tasks using features based on the learned sense embeddings. Goikoetxea et al. (2015) proposed a method for learning word embeddings using random walks on a graph of a lexical resource. Nieto Pi˜na and Johansson (2016) used a similar approach based on random walks on a WordNet to learn sense embeddings. Labeling Word Senses with Hypernyms. Hearst (1992) patterns are used to extract hypernyms from the corpus. These hypernyms are assigned to senses by aggregating hypernym 73 PCZ ID mouse:0 mouse:1 keyboard:0 keyboard:1 WordNet ID mouse:1 mouse:4 keyboard:1 keyboard:1 Related Terms rat:0, rodent:0, monkey:0, ... keyboard:1, computer:0, printer:0 ... piano:1, synthesizer:2, organ:0 ... keypad:0, mouse:1, screen:1 ... Hypernyms animal:0, species:1, ... device:1, equipment:3, ... instrument:2, device:3, ... device:1, technology:0 ... Context Clues rat:conj and, white-footed:amod, ... click:-prep of, click:-nn, .... play:-dobj, electric:amod, .. co"
W17-1909,S07-1015,0,0.22111,"n context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining WordNet with the eXtended WordNet (Mihalcea and"
W17-1909,C08-1021,0,0.0324758,"from the hybrid aligned resource. form WSD in context. For each test instance consisting of a target word and its context, we select the sense whose corresponding sense representation has the highest cosine similarity with the target word’s context. improvements in the results. Further expansion of the sense representation with context clues (cf. Table 1) provide a modest further improvement on the SemEval-2007 dataset and yield no further improvement on the case of the Senseval-3 dataset. 4 Comparison to the state-of-the-art. We compare our approach to four state-of-the-art systems: KnowNet (Cuadros and Rigau, 2008), BabelNet, WN+XWN (Cuadros and Rigau, 2007), and NASARI. KnowNet builds sense representations based on snippets retrieved with a web search engine. We use the best configuration reported in the original paper (KnowNet-20), which extends each sense with 20 keywords. BabelNet in its core relies on a mapping of WordNet synsets and Wikipedia articles to obtain enriched sense representations. The WN+XWN system is the topranked unsupervised knowledge-based system of Senseval-3 and SemEval-2007 datasets from the original competition (Cuadros and Rigau, 2007). It alleviates sparsity by combining Word"
W17-1909,W16-1401,0,0.0467827,"Missing"
W17-1909,N15-1070,0,0.0148651,"al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource proposed by Faralli et al. (2016) to perform WSD. We start with a short description of this resource and then discuss how it is used for WSD. 3.1 Construction of the Hybrid Aligned Resource (HAR) The hybrid aligned resource links two lexical semantic networks using the method of Faralli et al. (2016): a corpus-based distributionallyinduced network and a man"
W17-1909,W16-1620,1,0.89163,"Missing"
W17-1909,S10-1011,0,0.0341299,"original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense annotations. In all experi"
W17-1909,P15-2004,0,0.0159052,"on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into account lexical relations of a target word. Faruqui et al. (2015) proposed a related approach that performs a post-processing of word embeddings on the basis of lexical relations from the same resources. Pham et al. (2015) introduced another model that also aim at improving word vector representations by using lexical relations from WordNet. The method makes representations of synonyms closer than representations of antonyms of the given word. While these three models improve the performance on word relatedness evaluations, they do not model word senses. Jauhar et al. (2015) proposed two models that tackle this shortcoming, learning sense embeddings using the word sense inventory of WordNet. 3 Unsupervised Knowledge-based WSD using Hybrid Aligned Resource We rely on the hybrid aligned lexical semantic resource"
W17-1909,W04-0807,0,0.198258,"we use the scores reported in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only"
W17-1909,S07-1016,0,0.019055,"rted in the respective original publications. However, as NASARI was not evaluated on the datasets used in our study, we used the following procedure to obtain NASARI-based sense representations: Each WordNet-based sense representation was extended with all features from the lexical vectors of NASARI.2 Thus, we compare our method to three hybrid systems that induce sense representations on the Evaluation We perform an extrinsic evaluation and show the impact of the hybrid aligned resource on word sense disambiguation performance. While there exist many datasets for WSD (Mihalcea et al., 2004; Pradhan et al., 2007; Manandhar et al., 2010, inter alia), we follow Navigli and Ponzetto (2012) and use the SemEval-2007 Task 16 on the “Evaluation of wide-coverage knowledge resources” (Cuadros and Rigau, 2007). This task is specifically designed for evaluating the impact of lexical resources on WSD performance. The SemEval-2007 Task 16 is, in turn, based on two “lexical sample” datasets, from the Senseval-3 (Mihalcea et al., 2004) and SemEval-2007 Task 17 (Pradhan et al., 2007) evaluation campaigns. The first dataset has coarse- and fine-grained annotations, while the second contains only fine-grained sense an"
W17-1909,Q14-1019,0,0.0758595,"s of the lexical resource and relations between them (WordNet ID). Each sense in the PCZ network is subsequently linked to a sense of the knowledgebased network based on their similarity calculated on the basis of lexical representations of senses and their neighbors. The construction of the PCZ involves the following steps (Faralli et al., 2016): Iacobacci et al. (2015) proposed to learn sense embeddings on the basis of the BabelNet lexical ontology (Navigli and Ponzetto, 2012). Their approach is to train the standard skipgram model on a pre-disambiguated corpus using the Babelfy WSD system (Moro et al., 2014). NASARI (Camacho-Collados et al., 2015a) relies on Wikipedia and WordNet to produce vector representations of senses. In this approach, a sense is represented in lexical or sense-based feature spaces. The links between WordNet and Wikipedia are retrieved from BabelNet. MUFFIN (Camacho-Collados et al., 2015b) adapts several ideas from NASARI, extending the method to the multi-lingual case by using BabelNet synsets instead of monolingual WordNet synsets. The approach of Chen et al. (2015) to learning sense embeddings starts from initialization of sense vectors using WordNet glosses. It proceeds"
W17-1909,P15-1173,0,0.0342251,"Missing"
W17-1909,P14-2089,0,0.0231136,"igli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics All these diverse contributions indicate the benefits of hybrid knowledge sources for learning word and sense representations. combines the objective function of the skip-gram model (Mikolov et al., 2013) with a term that takes into ac"
W17-1909,D14-1113,0,0.0724977,"mapped to a sense inventory, or machine translation systems. 1 Introduction The representation of word senses and the disambiguation of lexical items in context is an ongoing long-established branch of research (Agirre and Edmonds, 2007; Navigli, 2009). Traditionally, word senses are defined and represented in lexical resources, such as WordNet (Fellbaum, 1998), while more recently, there is an increased interest in approaches that induce word senses from corpora using graph-based distributional approaches (Dorow and Widdows, 2003; Biemann, 2006; Hope and Keller, 2013), word sense embeddings (Neelakantan et al., 2014; Bartunov et al., 2 Related Work Several prior approaches combined distributional information extracted from text (Turney and Pantel, 2010) from text with information available in lexical resources, such as WordNet. Yu and Dredze (2014) proposed a model to learn word embeddings based on lexical relations of words from WordNet and PPDB (Ganitkevitch et al., 2013). The objective function of their model 72 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 72–78, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistic"
W17-1909,P15-1010,0,\N,Missing
W17-6933,P01-1005,0,0.162538,"jak (2016) performed another comparison of various semantic representation using both intrinsic and extrinsic evaluations. They compare the performance of their count-based method to dense representations and prediction-based methods using a manually crafted lexicon, SimLex and an information retrieval task. They show that their method performs better on the manually crafted lexicon than using word2vec. For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (Banko and Brill, 2001; Gorman and Curran, 2006; Riedl and Biemann, 2013). Based on the SimLex task and the extrinsic evaluation they show comparable performance to the word2vec model computed on a larger corpus. In this work, we do not focus on the best performing systems for each dataset, like e.g. retrofitting embeddings (Kiela et al., 2015; Rothe and Sch¨utze, 2015), but want to carve out the difference of existing methods for computing distributional similarities. 3 Methods for Distributional Semantics For the efficient and scalable similarity computation, we select SKIP and CBOW from word2vec as prediction-ba"
W17-6933,P14-1023,0,0.0574193,"efficiently computable methods and selected SKIP and CBOW from word2vec, GloVe and JoBimText. Based on these methods we want to explore different aspects: 1) which method performs the best global similarity scoring using word pair similarity datasets 2) which method performs the best local ranking of most similar terms for a query term 3) which context works best for the different methods and 4) are there differences in the performance when evaluating on verbs and nouns. 2 Related Work One of the first comparisons between count-based and prediction-based distributional models was performed by Baroni et al. (2014). For this, they consider various tasks and show that prediction-based word embeddings outperform sparse count-based methods and dense count-based methods used for computing distributional semantic models. The evaluation is performed on datasets for relatedness, analogy, concept categorization and selectional preferences. The majority of word pairs considered for the evaluation consists of noun pairs. However, Levy and Goldberg (2014b) showed that dense count-based methods, using PPMI weighted co-occurrences and SVD, approximates neural word embeddings. Levy et al. (2015) showed in an extensiv"
W17-6933,C16-1173,0,0.0168698,"rences. The majority of word pairs considered for the evaluation consists of noun pairs. However, Levy and Goldberg (2014b) showed that dense count-based methods, using PPMI weighted co-occurrences and SVD, approximates neural word embeddings. Levy et al. (2015) showed in an extensive study the impact of various parameters and show the best performing parameters for these methods. The study reports results for various datasets for word similarity and analogy. However, they do not evaluate the performance on local similarity ranking tasks and omit results for pure count-based semantic methods. Claveau and Kijak (2016) performed another comparison of various semantic representation using both intrinsic and extrinsic evaluations. They compare the performance of their count-based method to dense representations and prediction-based methods using a manually crafted lexicon, SimLex and an information retrieval task. They show that their method performs better on the manually crafted lexicon than using word2vec. For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (Banko and"
W17-6933,de-marneffe-etal-2006-generating,0,0.121229,"Missing"
W17-6933,D16-1235,0,0.0611887,"Missing"
W17-6933,P06-1046,0,0.0442865,"other comparison of various semantic representation using both intrinsic and extrinsic evaluations. They compare the performance of their count-based method to dense representations and prediction-based methods using a manually crafted lexicon, SimLex and an information retrieval task. They show that their method performs better on the manually crafted lexicon than using word2vec. For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (Banko and Brill, 2001; Gorman and Curran, 2006; Riedl and Biemann, 2013). Based on the SimLex task and the extrinsic evaluation they show comparable performance to the word2vec model computed on a larger corpus. In this work, we do not focus on the best performing systems for each dataset, like e.g. retrofitting embeddings (Kiela et al., 2015; Rothe and Sch¨utze, 2015), but want to carve out the difference of existing methods for computing distributional similarities. 3 Methods for Distributional Semantics For the efficient and scalable similarity computation, we select SKIP and CBOW from word2vec as prediction-based, GloVe as dense count"
W17-6933,J15-4004,0,0.018076,"nally, contexts are removed that co-occur with more than 1000 terms. The similarity score is only computed between terms that share at least one context and is based on the logarithmic sum of the reciprocal value of the number of terms a context co-occurs (log). Furthermore, we computed similarity scores by using solely the number of contexts two terms share (one). 4 Experimental Setting For performing the studies, we rely on two different evaluation methods. First, we show results based on datasets that contain averaged similarity scores for word pairs annotated by humans. We use SimLex-999 (Hill et al., 2015), which consists of 999 word pairs, formed by 666 noun, 222 verb and 111 adjective pairs and the SimVerb-3500 dataset (Gerz et al., 2016) which comprises of 3500 verb pairs. The evaluation scores are computed using the Spearman rank correlation coefficient between the gold standard scores and the similarity scores obtained with the semantic methods. These evaluations validate the ability of semantic methods to provide similarity scores that demonstrate the performance for a global ranking between word pairs scores. We name this task as a ‘global ranking task’ as the semantic models have to pro"
W17-6933,P90-1034,0,0.656755,"h as dependency relations. In summary, we highlight differences of popular distributional semantic representations and derive recommendations for their usage. 1 Introduction With the steady growth of textual data, NLP methods are required that are able to process the data efficiently. In this paper, we focus on efficient methods that are targeted to compute distributional models that are based on the distributional hypothesis of Harris (1951). This hypothesis claims that words occurring in similar contexts tend to have similar meanings. In order to implement this hypothesis, early approaches (Hindle, 1990; Grefenstette, 1994; Lin, 1997) represented words using count-based vectors of the context. However, such representations are very sparse, require a lot of memory and are not very efficient. In the last decades, methods have been developed that transform such sparse representations into dense representations mainly using matrix factorization. With word2vec (Mikolov et al., 2013), an efficient prediction-based method was introduced, which also represents words with a dense vector. However, also sparse and count-based methods have been proposed that allow an efficient computation, e.g. (Kilgarr"
W17-6933,D15-1242,0,0.0393855,"heir method performs better on the manually crafted lexicon than using word2vec. For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (Banko and Brill, 2001; Gorman and Curran, 2006; Riedl and Biemann, 2013). Based on the SimLex task and the extrinsic evaluation they show comparable performance to the word2vec model computed on a larger corpus. In this work, we do not focus on the best performing systems for each dataset, like e.g. retrofitting embeddings (Kiela et al., 2015; Rothe and Sch¨utze, 2015), but want to carve out the difference of existing methods for computing distributional similarities. 3 Methods for Distributional Semantics For the efficient and scalable similarity computation, we select SKIP and CBOW from word2vec as prediction-based, GloVe as dense count-based1 and JoBimText as sparse count-based method. Word2Vec We use the SKIP-gram model, which predicts for a word the neighboring words within a symmetric window of w. Considering the CBOW model, a word is predicted by its neighboring words. For the computation, we use the implementation by Mikol"
W17-6933,P14-2050,0,0.0833201,"Missing"
W17-6933,Q15-1016,0,0.0234551,"ls was performed by Baroni et al. (2014). For this, they consider various tasks and show that prediction-based word embeddings outperform sparse count-based methods and dense count-based methods used for computing distributional semantic models. The evaluation is performed on datasets for relatedness, analogy, concept categorization and selectional preferences. The majority of word pairs considered for the evaluation consists of noun pairs. However, Levy and Goldberg (2014b) showed that dense count-based methods, using PPMI weighted co-occurrences and SVD, approximates neural word embeddings. Levy et al. (2015) showed in an extensive study the impact of various parameters and show the best performing parameters for these methods. The study reports results for various datasets for word similarity and analogy. However, they do not evaluate the performance on local similarity ranking tasks and omit results for pure count-based semantic methods. Claveau and Kijak (2016) performed another comparison of various semantic representation using both intrinsic and extrinsic evaluations. They compare the performance of their count-based method to dense representations and prediction-based methods using a manual"
W17-6933,P97-1009,0,0.145646,"ary, we highlight differences of popular distributional semantic representations and derive recommendations for their usage. 1 Introduction With the steady growth of textual data, NLP methods are required that are able to process the data efficiently. In this paper, we focus on efficient methods that are targeted to compute distributional models that are based on the distributional hypothesis of Harris (1951). This hypothesis claims that words occurring in similar contexts tend to have similar meanings. In order to implement this hypothesis, early approaches (Hindle, 1990; Grefenstette, 1994; Lin, 1997) represented words using count-based vectors of the context. However, such representations are very sparse, require a lot of memory and are not very efficient. In the last decades, methods have been developed that transform such sparse representations into dense representations mainly using matrix factorization. With word2vec (Mikolov et al., 2013), an efficient prediction-based method was introduced, which also represents words with a dense vector. However, also sparse and count-based methods have been proposed that allow an efficient computation, e.g. (Kilgarriff et al., 2004; Biemann and Ri"
W17-6933,P98-2127,0,0.121956,"c, which was introduced by Levy and Goldberg (2014a)3 and allows to use arbitrary contexts for computing dense vector representations for similarity computations. Global Vectors (GloVe) As dense count-based approach, we select GloVe (Pennington et al., 2014).4 GloVe achieves its representation based on logarithmic co-occurrences between words and context. This representation is learned using matrix factorization methods. JoBimText (JBT) We consider JoBimText (Biemann and Riedl, 2013) as symbolic count-based method5 that produces word similarities encoded in a distributional thesaurus (DT, cf. Lin, 1998). The method is based on a term-context representation and can handle arbitrary contexts. For an efficient computation it considers 1 In this study, we consider GloVe as a dense count-based method. Although GloVe uses a classifier in order to optimize its cost function, it is based on co-occurrence statistics and does not predict contexts from words directly, as performed in word2vec. 2 https://code.google.com/archive/p/word2vec/ 3 https://bitbucket.org/yoavgo/word2vecf 4 https://nlp.stanford.edu/projects/glove/ 5 http://sf.net/p/jobimtext/ several pruning techniques and uses Lexicographer’s M"
W17-6933,N16-1118,0,0.01814,"all methods, the dependency parses can only be used with a modification of word2vec (Levy and Goldberg, 2014a) and JBT. 5 Word Similarity Evaluation In this section, we show the Spearman correlations for the different models using SimLex and SimVerb7 . First, we perform the computation of the models on raw text (see Table 1). Using various parameters for both word2vec models8 , we observe the best results for the SimLex dataset when computing both SKIP and CBOW with 500 dimensions, using random sampling (s = 1E −5 ), 10 negative examples and a word window size of 1 (W1). This is in line with Melamud et al. (2016), who mostly obtain the highest scores for word similarity tasks when using a comparably high number of dimensions. For GloVe we obtain the best results with the same parameters as for word2vec: we use a window size of 1 and 500 dimensions.9 The CBOW model performs best on the SimVerb dataset but does not yield the best scores for the verbs in SimLex. However, we could not detect much differences between the two sets, as we observe a correlation of 0.9177 for 90 verb pairs that are shared in both datasets. GloVe performs best on 6 We use the Stanford dependency parser (de Marneffe et al., 2006"
W17-6933,J07-2002,0,0.146311,"Missing"
W17-6933,D14-1047,0,0.0398402,"Missing"
W17-6933,N04-3012,0,0.0605434,"pairs scores. We name this task as a ‘global ranking task’ as the semantic models have to provide a score between two given word pairs and the evaluation score is computed by the correlation between similarity scores given by the model and averaged similarity scores given by humans. In a so-called local ranking task, we will evaluate how well semantic models can retrieve the most similar words for a given term. For this, we sample 1000 low-, middle- and high frequent nouns and verbs. In order to compute the semantic similarities between the most similar terms, we use the WordNet Path measure (Pedersen et al., 2004) and perform an evaluation that is similar to the one used by Biemann and Riedl (2013). This Path measure is the shortest reciprocal distance + 1 between two words based on the IS-A path. The computation of the various models is performed using a dump of English Wikipedia that comprises of 35 million sentences. The similarities are computed on raw tokenized text, then on lemmatized and POS-tagged tagged text and finally using dependency parses6 as context representation, which has been shown to work well for computing similarities (Lin, 1997; Biemann and Riedl, 2013; Levy and Goldberg, 2014a)."
W17-6933,D14-1162,0,0.0787017,"ount-based1 and JoBimText as sparse count-based method. Word2Vec We use the SKIP-gram model, which predicts for a word the neighboring words within a symmetric window of w. Considering the CBOW model, a word is predicted by its neighboring words. For the computation, we use the implementation by Mikolov et al. (2013)2 . In addition, we use the extension of word2vec, which was introduced by Levy and Goldberg (2014a)3 and allows to use arbitrary contexts for computing dense vector representations for similarity computations. Global Vectors (GloVe) As dense count-based approach, we select GloVe (Pennington et al., 2014).4 GloVe achieves its representation based on logarithmic co-occurrences between words and context. This representation is learned using matrix factorization methods. JoBimText (JBT) We consider JoBimText (Biemann and Riedl, 2013) as symbolic count-based method5 that produces word similarities encoded in a distributional thesaurus (DT, cf. Lin, 1998). The method is based on a term-context representation and can handle arbitrary contexts. For an efficient computation it considers 1 In this study, we consider GloVe as a dense count-based method. Although GloVe uses a classifier in order to optim"
W17-6933,D13-1089,1,0.888863,"us semantic representation using both intrinsic and extrinsic evaluations. They compare the performance of their count-based method to dense representations and prediction-based methods using a manually crafted lexicon, SimLex and an information retrieval task. They show that their method performs better on the manually crafted lexicon than using word2vec. For this task, they also show that a word2vec model computed on a larger dataset yields inferior results than models computed on a smaller corpus, which is contrary to previous findings, e.g. (Banko and Brill, 2001; Gorman and Curran, 2006; Riedl and Biemann, 2013). Based on the SimLex task and the extrinsic evaluation they show comparable performance to the word2vec model computed on a larger corpus. In this work, we do not focus on the best performing systems for each dataset, like e.g. retrofitting embeddings (Kiela et al., 2015; Rothe and Sch¨utze, 2015), but want to carve out the difference of existing methods for computing distributional similarities. 3 Methods for Distributional Semantics For the efficient and scalable similarity computation, we select SKIP and CBOW from word2vec as prediction-based, GloVe as dense count-based1 and JoBimText as s"
W17-6933,P15-1173,0,0.0612233,"Missing"
W17-6933,D15-1036,0,0.0770066,"we compute the average frequency for the top N = {1, 10, 100, 200} most similar words for the sampled candidates. In addition, we use the relative frequency in relation to the frequency of the queried word. Among all frequency bands and for verbs and nouns we observe a consistent pattern, as shown in Figure 1. For nouns, the SKIP and CBOW similar Figure 1: Normalized average frequency for the top N most similar words for 3000 nouns (left) and 3000 verbs (right) for the semantic models computed using lemmas and POS information. words are on average 3 times more frequent than the queried term. Schnabel et al. (2015) also describe that the frequency of the similarities stay in the same frequency region and attribute this effect to the cosine similarity. Using GloVe the similar nouns are on average 20 times more frequent and for JBT we retrieve words that are on average 100 times more frequent than the queried word. For verbs, we obtain consistently higher average similarities. However, the pattern is similar to the one observed with nouns. Keeping the same POS Next, we examine the stability of the most similar terms in respect to the POS of the candidate term. For this we use the lemmatized and dependency"
W17-6933,C98-2122,0,\N,Missing
W18-0507,W14-1206,0,0.0720755,"Missing"
W18-0507,W18-0519,0,0.153197,"Missing"
W18-0507,S16-1156,0,0.0850516,"Missing"
W18-0507,S16-1151,0,0.0753176,"Missing"
W18-0507,W18-0539,1,0.893727,"Missing"
W18-0507,W18-0538,0,0.100445,"lower log-probability for complex words. The systems submitted performed the best out of all systems for the cross-lingual task (the French dataset) both for the binary and probabilistic classification tasks, showing a promising direction in the creation of CWI dataset for new languages. 1 74 https://code.google.com/archive/p/word2vec/ tant features. Their best system shows an average performance compared to the other systems in the shared task for the monolingual English binary classification track. NLP-CIC present systems for the English and Spanish multilingual binary classification tasks (Aroyehun et al., 2018). The feature sets include morphological features such as frequency counts of target word on large corpora such as Wikipedia and Simple Wikipedia, syntactic and lexical features, psycholinguistic features from the MRC psycholinguistic database and entity features using the OpenNLP and CoreNLP tools, and word embedding distance as a feature which is computed between the target word and the sentence. Tree learners such as Random Forest, Gradient Boosted, and Tree Ensembles are used to train different classifiers. Furthermore, a deep learning approach based on 2D convolutional (CNN) and word embe"
W18-0507,S16-1148,0,0.0982594,"Missing"
W18-0507,W18-0518,0,0.065579,"on the ensemble techniques where AdaBoost classifier with 5000 estimators achieves the highest results, followed by the bootstrap aggregation classifier of Random Forest. All the features are used for the N EWS and W IKI N EWS datasets, but for the W IKIPEDIA dataset, MCR psycholinguistic features are excluded. For the probabilistic classification task, the same feature setups are used and the Linear Regression algorithm is used to estimate values of targets. CoastalCPH describe systems developed for multilingual and cross-lingual domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both"
W18-0507,W18-0520,0,0.193647,"l domains for the binary and probabilistic classification tasks (Bingel and Bjerva, 2018). Unlike most systems, they have focused mainly on German, Spanish, and French datasets in order to investigate if multitask learning can be applied to the cross-lingual CWI task. They have devised two models, using languageagnostic approach with an ensemble that comprises of Random Forests (random forest classifiers for the binary classification task and random forest regressors for the probabilistic classification tasks, with 100 trees) and feed-forward neural networks. Camb describes different systems (Gooding and Kochmar, 2018) they have developed for the monolingual English datasets both for the binary and probabilistic classification tasks. They have used features that are based on the insights of the CWI shared task 2016 (Paetzold and Specia, 2016a) such as lexical features (word length, number of syllables, WordNet features such as the number of synsets), word n-gram and POS tags, and dependency parse relations. In addition, they have used features such as the number of words grammatically related to the target word, psycholinguistic features from the MRC database, CEFR (Common European Framework of Reference fo"
W18-0507,S16-1160,0,0.102164,"Missing"
W18-0507,W18-0540,0,0.0627627,"Missing"
W18-0507,W18-0521,0,0.24579,"layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using th"
W18-0507,S16-1164,0,0.0425763,"Missing"
W18-0507,S16-1149,1,0.538636,"elines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second edition focused on multilingualism providing datasets containing four languages: English, German, French, and Spanish. • English monolingual CWI; • German monolingual CWI; • Spanish monolingual CWI; and • Multilingual CWI with a French test set. For the first three tracks, participants were provided with training and testing data for the same language. For French, participants were provided only with a French test set and no French training data. In the CWI 2016, the task was cast as binary classification. To be able t"
W18-0507,S16-1162,0,0.138871,"Missing"
W18-0507,S16-1158,0,0.0719992,"Missing"
W18-0507,S16-1163,0,0.176167,"Missing"
W18-0507,D14-1162,0,0.0887949,"earning methods, 2) using the average embedding of target words as an input to a neural network, and 3) modeling the context of the target words using an LSTM. For the feature engineering-based systems, features such as linguistic, psycholinguistic, and language model features were used to train different binary and probabilistic classifiers. Lexical features include word length, number of syllables, and number of senses, hypernyms, and hyponyms in WordNet. For N-gram features, probabilities of the n-gram containing the target words were For embedding-based systems, a pre-trained GloVe model (Pennington et al., 2014) was used to get the vector representations of target words. For MWE, the average of the vectors is used. In the first approach, the resulting vector is passed on to a neural network with two ReLu layers followed by a sigmoid layer, which predicted the probability of the target word being complex. Their experiments show that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For t"
W18-0507,S16-1154,1,0.847385,"Missing"
W18-0507,W18-0541,0,0.102296,"Missing"
W18-0507,S16-1153,1,0.879975,"Missing"
W18-0507,S16-1161,0,0.200613,"Missing"
W18-0507,S16-1147,0,0.032877,"Missing"
W18-0507,S16-1157,0,0.212408,"Missing"
W18-0507,I11-1017,0,0.0188214,"that the feature engineering approach achieved the best results using the XGBoost classifier for the binary classification task. They submitted four systems using XGBoost, average embeddings, LSTMs with transfer learning, and a voting system that combines the other three. For the probabilistic classification task, their LSTMs achieve the best results. TMU submitted multilingual and cross-lingual CWI systems for both of the binary and probabilistic classification tasks (Kajiwara and Komachi, 2018). The systems use two variants of frequency features from the learner corpus (Lang-8 corpus) from Mizumoto et al. (2011) and from the general domain corpus (Wikipedia and WikiNews). The list of features used in building the model include the number of characters in the target word, number of words in the target phrase, and frequency of the target word in learner corpus (Lang-8 corpus) and general domain corpus (Wikipedia and WikiNews). Random forest classifiers are used for the binary classification task while random forest regressors are used for the probabilistic classification task using the scikit-learn library. Feature ablation shows that both the length, frequency, and probability features (based on corpu"
W18-0507,P13-3015,0,0.247331,"ative speakers. To train their systems, participants received a labeled training set where words in context were annotated regarding their complexity. One month later, an unlabeled test set was provided and participating teams were required to upload their predictions for evaluation. More information about the data collection is presented in Section 3. Given the multilingual dataset provided, the CWI challenge was divided into four tracks: Introduction The most common first step in lexical simplification pipelines is identifying which words are considered complex by a given target population (Shardlow, 2013). This task is known as complex word identification (CWI) and it has been attracting attention from the research community in the past few years. In this paper we present the findings of the second Complex Word Identification (CWI) shared task organized as part of the thirteenth Workshop on Innovative Use of NLP for Building Educational Applications (BEA) co-located with NAACL-HLT’2018. The second CWI shared task follows a successful first edition featuring 21 teams organized at SemEval’2016 (Paetzold and Specia, 2016a). While the first CWI shared task targeted an English dataset, the second e"
W18-0507,S16-1152,0,0.262443,"Missing"
W18-0507,S16-1159,0,0.0576998,"Missing"
W18-0507,L16-1035,1,0.906663,"Missing"
W18-0507,W18-0522,0,0.230666,"n dependency relation, frequency features based on the BNC, Wikipedia, and Dale and Chall list corpora, number of synsets and senses in WordNet, and so on. The experiment is conducted using the Weka machine learning framework using the Support vector machine (with linear and radial basis function kernels), Na¨ıve Bayes, Logistic Regression, Random Tree, and Random Forest classification algorithms. The final experiments employ Support Vector Machines and Random Forest classifiers. CFILT IITB Developed ensemble-based classification systems for the English monolingual binary classification task (Wani et al., 2018). Lexical features based on WordNet for the target word are extracted as follows: 1) Degree of Polysemy: number of senses of the target word in WordNet, 2) Hyponym and Hypernym Tree Depth: the position of the word in WordNet’s hierarchical tree, and 3) Holonym and Meronym Counts: based on the relationship of the target word to its components (meronyms) or to the things it is contained in (Holonym’s). Additional feature classes include size-based features such as word count, word length, vowel counts, and syllable counts. They also use vocabulary-based features such as Ogden Basic (from Ogden’s"
W18-0507,S16-1146,0,0.112937,"Missing"
W18-0507,I17-2068,1,0.54282,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,yimam-etal-2017-multilingual,1,0.591517,"least 10 native English speakers and 10 non-native English speakers so that it is possible to investigate if native and non-native speakers have different CWI needs. 5) Complex words are not pre-highlighted, as in previous contributions, so that annotators are not biased to the pre-selection of the complex phrases. 6) In addition to single words, we allowed the annotation of multi-word expressions (MWE), up to a size of 50 characters. Table 2 shows the total, native, and non-native number of annotators that participated in the annotation task. Datasets We have used the CWIG3G2 datasets from (Yimam et al., 2017b,a) for the complex word identification (CWI) shared task 2018. The datasets are collected for multiple languages (English, German, Spanish). The English datasets cover different text genres, namely News (professionally written news), WikiNews (news written by amateurs), and Wikipedia articles. Below, we will briefly describe the annotation process and the statistics of collected datasets. For detail explanation of the datasets, please refer to the works of Yimam et al. (2017b,a) Furthermore, to bolster the cross-lingual CWI experiment, we have collected a CWI dataset for French. The French d"
W18-0507,W17-5910,1,0.857961,"Missing"
W18-0507,S16-1155,1,0.729801,"Missing"
W18-0507,S16-1150,0,\N,Missing
W19-0413,L16-1429,1,0.942898,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,C16-1047,1,0.934723,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,S16-1049,0,0.04135,"Missing"
W19-0413,baccianella-etal-2010-sentiwordnet,0,0.0144947,"* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et"
W19-0413,C12-2008,1,0.824775,"n detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA"
W19-0413,W13-5006,1,0.768299,"positive sentiment of the sentence. “It was used to be a horrible place to eat but not any more.” In contrast to A4, architecture A3 does not rely on the sequence information of the extracted features and allows the network to learn on its own. We use 300 dimension Word2Vec (Mikolov et al., 2013) word embeddings for the experiments. Each Bi-LSTM layer contains 100 neurons while two dense layers contain 100 and 50 neurons, respectively. Features As additional features, we extract the following information for each token in an instance. – Aspect term extraction: Distributional thesaurus (DT)1 (Biemann and Riedl, 2013) defines the lexicon expansion of a token based on a similar context. It is usually very effective for the handling of unseen text. If a token in the test set never appears in the training set, it becomes a non-trivial task for the classifier to make a correct prediction. By employing DT feature, the classifier can additionally utilize lexical expansion of the current token for mapping with the training set, thus minimize the chance of unseen text. For each token, we use its top 3 DT expansions as features. Language English Spanish French Dutch German Hindi #sent. 2,000 2,070 1,733 1,711 19,43"
W19-0413,S16-1044,0,0.270428,"m identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network (Graves et al"
W19-0413,S16-1054,0,0.0389318,"Missing"
W19-0413,P14-2063,0,0.0690449,"Missing"
W19-0413,D18-1382,1,0.797108,"Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achi"
W19-0413,S15-2080,0,0.0298934,"ral language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & D"
W19-0413,W06-0301,0,0.0146414,"evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Condition"
W19-0413,S14-2076,0,0.167189,"2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags"
W19-0413,S16-1174,1,0.942977,"n a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) ne"
W19-0413,S13-2053,0,0.0243541,"ing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & Dutch. System XRCE (Brun"
W19-0413,P05-1015,0,0.48177,"Missing"
W19-0413,W02-1011,0,0.029084,"or aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez"
W19-0413,S16-1002,0,0.0636458,"Missing"
W19-0413,S14-2004,0,0.525133,"timent towards food and service are positive and negative, respectively. Such analysis offers finegrained information to a user or an organization who seeks users opinion towards any specific entity. For example, based on the users’ feedback, an individual can draw a general perception about the specific attribute or aspect of a product or service, and he/she can make an informed decision about the product or service under observation. Similarly, an organization can utilize the feedback to refine its product/service or to take a decision in the business model. Aspect-based sentiment analysis (Pontiki et al., 2014, 2016) has two subproblems at its core, i.e., aspect term identification (or opinion target extraction) and aspect sentiment classification. Given a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and re"
W19-0413,S16-1045,0,0.0591637,"Missing"
W19-0413,P02-1053,0,0.0202289,"es (one each for aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA,"
W19-0413,waltinger-2010-germanpolarityclues,0,0.0321484,"9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et al., 2016) (Task 5), which contain user reviews across multiple languages. The datasets of English, Spanish, French and Dutch are related to the reviews of consumer electronics a"
W19-0413,H05-1044,0,0.101206,"65.6 65.7 64.2 23.1 22.0 22.4 24.0* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspe"
W19-0413,P18-1208,0,0.0149591,"n 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for"
W19-4516,D17-1070,0,0.0290688,"ll word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v), that is then fed into a fully connected layer and a softmax layer. We use the pre-trained embeddings in our experiments.7"
W19-4516,D17-1218,0,0.0347067,"Missing"
W19-4516,D17-1245,0,0.0216943,"domain-specific vocabulary) but also paths between comparison targets in dependency parses. More recently, Gupta et al. (2017) described a system for the biomedical domain that also combines manually collected patterns for lexical matches and dependency parses in order to identify comparison targets and comparison type using the as gradable, non-gradable, superlativetaxonomy of Jindal and Liu (2006). Developing a system for mining comparative sentences (with potential argumentation support for a preference) from the web might utilize specialized jargon like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structur"
W19-4516,W17-5112,0,0.018989,"like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structural features such as punctuation subsumed syntactic features in the above studies. The role of discourse markers in the identification of claims and premises was discussed by Eckle-Kohler et al. (2015), who found such markers to be moderately useful for identifying argumentative sentences. Also Daxenberger et al. (2017) noted that claims share lexical clues across different datasets. They also concluded from their experiments that typical argumentation mining datasets were too small to unleash the power of recent DNN-based classifiers; methods based o"
W19-4516,W07-1018,0,0.0547236,"o! Answers that contain a lot of “How does X compare to Y?”-questions with human answers but the web itself is an even larger source of textual comparisons. Mining and categorizing comparative sentences from the web could support search engines in answering comparative queries (with potential argumentation justifying the preference in the mined sentence itself or in its context) but also has opinion mining (Ganapathibhotla and Liu, 2008) as another important application. Still, previous work on recognizing comparative sentences has mostly been conducted in the biomedical domain. For instance, Fiszman et al. (2007) identify sentences explicitly comparing elements of drug therapy via manually developed comparative and direction patterns informed by a lot of domain knowledge. Later, Park and Blake (2012) trained a highprecision Bayesian Network classifier for toxicol1 2 3 Dataset As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or “wins”) / WORSE (the first item is worse or “looses”) or NONE (the sentence does not contain a comparison of the target items). The BETTERsente"
W19-4516,D15-1075,0,0.0171193,"ings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v), that is then fed into a fully connected layer and a softmax layer. We use the pre-trained embeddings in our experiments.7 LexNet (customized) To overcome the LexNet (original) coverage issue, we relaxed the restriction by extending the maximal path length to 16 and ignoring edge directions"
W19-4516,W17-2326,0,0.0719825,"m at providing comparative functionality across domains. Still, such pages and systems usually suffer from coverage issues relying on structured databases as the only source of information ignoring the rich textual content available on the web. 136 Proceedings of the 6th Workshop on Argument Mining, pages 136–145 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Our main contributions are two-fold: ogy publications that used lexical clues (comparatives and domain-specific vocabulary) but also paths between comparison targets in dependency parses. More recently, Gupta et al. (2017) described a system for the biomedical domain that also combines manually collected patterns for lexical matches and dependency parses in order to identify comparison targets and comparison type using the as gradable, non-gradable, superlativetaxonomy of Jindal and Liu (2006). Developing a system for mining comparative sentences (with potential argumentation support for a preference) from the web might utilize specialized jargon like hashtags for argumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): w"
W19-4516,D14-1162,0,0.0820342,"0 neurons, batch size of 128, RMSprop with learning rate 0.01 and 150 epochs, and max pooling with a pool size of 2. A Keras embedding layer is used to create word embeddings of length 100 for the string path components. In the original study, paths were restricted to a length of four with the first comparison target having to be reachable from the lowest common head of the two targets by following left edges only, the second one by following right edges. With this LexNet (original) restriction, a path was found for only 1,519 of our 5,759 training sentences. Word Embeddings We rely on GloVe (Pennington et al., 2014) embeddings of size 300 to create a dense, low-dimension vector representation of a sentence.6 We average all word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 20"
W19-4516,N16-1162,0,0.0197701,"n et al., 2014) embeddings of size 300 to create a dense, low-dimension vector representation of a sentence.6 We average all word vectors of a sentence, representing it by kind of a centroid word—a simple method shown to be effective for several tasks (Wieting et al., 2016). Sentence Embeddings Bags of words and average word embeddings lose sequence information, which intuitively should help for (directed) comparison extraction. Sentence embeddings aim to learn representations for spans of text instead of single words by taking sequence information into account. Several methods like FastSent (Hill et al., 2016) or SkipTought (Kiros et al., 2015) have been proposed to create sentence embeddings. We use InferSent (Conneau et al., 2017) that learns sentence embeddings similar to word embeddings. A neural network is trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015) containing 570,000 English sentence pairs (each labelled as entailment, contradiction, or neutral). InferSent combines the embeddings u and v of the two sentences from a sentence pair into one feature vector (containing the concatenation, the element-wise product, and the elementwise difference of u and v"
W19-4516,N18-1202,0,0.0493126,"Missing"
W19-4516,W16-5304,0,0.0146014,"ion experiments using several machine learning approaches and representations and analyse the results. We use common performance metrics: precision, recall and F1 per each class and micro-averaged when reporting overall results. Dependency-based Features The HypeNet method to detect hypernym relations between words (Shwartz et al., 2016) combines distributional and dependency path-based methods to create a vector representation for word pairs. The LexNet generalization of HypeNet encodes tries to capture multiple semantic relationships between two words also using dependency path information (Shwartz and Dagan, 2016). Since dependency paths have been one of the major sources for comparison extraction in related work from the biomedical domain (see Section 2), we also include two LexNet-based features in our experiments. 5.1 Using spaCy’s en core web lg spacy.io/models/en#section-en core web lg. 7 github.com/facebookresearch/InferSent Impact of Classification Models To identify the best classification algorithm, we used a fixed baseline set of feature representations: a sparse bag-of-words model with binary weights computed on the whole sentence (see Section 4.3). We used F1 score to measure the models per"
W19-4516,P16-1226,0,0.0387854,"Missing"
W19-4516,L18-1286,1,0.850117,"Missing"
W19-4516,D14-1006,0,0.0218616,"rgumentative tweets (Dusmanu et al., 2017) but at the same time faces the challenges recognized for general web ˇ argument mining (Snajder, 2017): web text is typically not well formulated, misses argument structures, and contains poorly formulated claims. In contrast to the use of dependency parses for mining comparative sentences in the biomedical domain, such syntactic features are often impossible to derive for noisy web text and were even shown to not really help in identifying argument structures from well-formulated texts like persuasive essays or Wikipedia articles (Aker et al., 2017; Stab and Gurevych, 2014); simpler structural features such as punctuation subsumed syntactic features in the above studies. The role of discourse markers in the identification of claims and premises was discussed by Eckle-Kohler et al. (2015), who found such markers to be moderately useful for identifying argumentative sentences. Also Daxenberger et al. (2017) noted that claims share lexical clues across different datasets. They also concluded from their experiments that typical argumentation mining datasets were too small to unleash the power of recent DNN-based classifiers; methods based on feature engineering stil"
W19-4516,W12-4301,0,0.38074,"e sentences from the web could support search engines in answering comparative queries (with potential argumentation justifying the preference in the mined sentence itself or in its context) but also has opinion mining (Ganapathibhotla and Liu, 2008) as another important application. Still, previous work on recognizing comparative sentences has mostly been conducted in the biomedical domain. For instance, Fiszman et al. (2007) identify sentences explicitly comparing elements of drug therapy via manually developed comparative and direction patterns informed by a lot of domain knowledge. Later, Park and Blake (2012) trained a highprecision Bayesian Network classifier for toxicol1 2 3 Dataset As there is no large publicly available crossdomain dataset for comparative argument mining, we create one composed of sentences annotated with markers BETTER (the first item is better or “wins”) / WORSE (the first item is worse or “looses”) or NONE (the sentence does not contain a comparison of the target items). The BETTERsentences represent a pro argument in favor of the first compared item (or a con argument for the seczenodo.org/record/3237552 github.com/uhh-lt/comparative 137 ond item) while the roles are excha"
yimam-etal-2017-entity,P14-5016,1,\N,Missing
yimam-etal-2017-multilingual,E09-1027,0,\N,Missing
